link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://dl.acm.org/doi/10.5555/3618408.3619888,Transparency & Explainability,Tilted Sparse Additive Models,"Additive models have been burgeoning in data analysis due to their flexible representation and desirable interpretability. However, most existing approaches are constructed under empirical risk minimization (ERM), and thus perform poorly in situations where average performance is not a suitable criterion for the problems of interest, e.g., data with complex non-Gaussian noise, imbalanced labels or both of them. In this paper, a novel class of sparse additive models is proposed under tilted empirical risk minimization (TERM), which addresses the deficiencies in ERM by imposing tilted impact on individual losses, and is flexibly capable of achieving a variety of learning objectives, e.g., variable selection, robust estimation, imbalanced classification and multiobjective learning. On the theoretical side, a learning theory analysis which is centered around the generalization bound and function approximation error bound (under some specific data distributions) is conducted rigorously. On the practical side, an accelerated optimization algorithm is designed by integrating Prox-SVRG and random Fourier acceleration technique. The empirical assessments verify the competitive performance of our approach on both synthetic and real data.",[],[],"['Yingjie Wang', 'Hong Chen', 'Weifeng Liu', 'Fengxiang He', 'Tieliang Gong', 'Youcheng Fu', 'Dacheng Tao']","['College of Control Science and Engineering,  University of Petroleum (East ), Qingdao,  and College of Informatics, Huazhong Agricultural University', 'College of Informatics, Huazhong Agricultural University,  and Engineering Research Center of Intelligent Technology for Agriculture, Ministry of Education, Wuhan', 'College of Control Science and Engineering,  University of Petroleum (East ), Qingdao', 'JD Explore Academy, JD.com, Inc., Beijing,  and Artificial Intelligence and its Applications Institute, School of Informatics, The University of Edinburgh, Edinburgh, U', ""School of Computer Science and Technology, Xi'an Jiaotong University"", 'College of Informatics, Huazhong Agricultural University', 'The University of Sydney, Sydney']","['China', 'China', 'China', 'China', 'China', 'China', 'Australia']"
https://dl.acm.org/doi/10.5555/3618408.3618765,Transparency & Explainability,Adaptive Whitening in Neural Populations with Gain-modulating Interneurons,"Statistical whitening transformations play a fundamental role in many computational systems, and may also play an important role in biological sensory systems. Existing neural circuit models of adaptive whitening operate by modifying synaptic interactions; however, such modifications would seem both too slow and insufficiently reversible. Motivated by the extensive neuroscience literature on gain modulation, we propose an alternative model that adaptively whitens its responses by modulating the gains of individual neurons. Starting from a novel whitening objective, we derive an online algorithm that whitens its outputs by adjusting the marginal variances of an overcomplete set of projections. We map the algorithm onto a recurrent neural network with fixed synaptic weights and gain-modulating interneurons. We demonstrate numerically that sign-constraining the gains improves robustness of the network to ill-conditioned inputs, and a generalization of the circuit achieves a form of local whitening in convolutional populations, such as those found throughout the visual or auditory systems.",[],[],"['Lyndon R. Duong', 'David Lipshutz', 'David J. Heeger', 'Dmitri B. Chklovskii', 'Eero P. Simoncelli']","['Center for Neural Science, New York University', 'Center for Computational Neuroscience, Flatiron Institute', 'Center for Neural Science, New York University', 'Center for Computational Neuroscience, Flatiron Institute and Neuroscience Institute, NYU School of Medicine', 'Center for Neural Science, New York University and Center for Computational Neuroscience, Flatiron Institute']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619251,Transparency & Explainability,Does a Neural Network Really Encode Symbolic Concepts?,"Recently, a series of studies have tried to extract interactions between input variables modeled by a DNN and define such interactions as concepts encoded by the DNN. However, strictly speaking, there still lacks a solid guarantee whether such interactions indeed represent meaningful concepts. Therefore, in this paper, we examine the trustworthiness of interaction concepts from four perspectives. Extensive empirical studies have verified that a well-trained DNN usually encodes sparse, transferable, and discriminative concepts, which is partially aligned with human intuition. The code is released at https://github.com/sjtu-xai-lab/interaction-concept.",[],[],"['Mingjie Li', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","[None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618683,Transparency & Explainability,Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation,"We propose a meta-ability decoupling (MAD) paradigm, which brings together various object navigation methods in an architecture system, allowing them to mutually enhance each other and evolve together. Based on the MAD paradigm, we design a multiple thinking (MT) model that leverages distinct thinking to abstract various meta-abilities. Our method decouples meta-abilities from three aspects: input, encoding, and reward while employing the multiple thinking collaboration (MTC) module to promote mutual cooperation between thinking. MAD introduces a novel qualitative and quantitative interpretability system for object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we demonstrate that our method outperforms state-of-the-art (SOTA) methods on both typical and zero-shot object navigation tasks.",[],[],"['Ronghao Dang', 'Lu Chen', 'Liuyi Wang', 'Zongtao He', 'Chengju Liu', 'Qijun Chen']","['Department of Control Science and Engineering, Tongji University, Shanghai', 'Department of Control Science and Engineering, Tongji University, Shanghai', 'Department of Control Science and Engineering, Tongji University, Shanghai', 'Department of Control Science and Engineering, Tongji University, Shanghai', 'Department of Control Science and Engineering, Tongji University, Shanghai', 'Department of Control Science and Engineering, Tongji University, Shanghai']","['China', 'China', 'China', 'China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3619911,Transparency & Explainability,Curriculum Co-disentangled Representation Learning across Multiple Environments for Social Recommendation,"There exist complex patterns behind the decision-making processes of different individuals across different environments. For instance, in a social recommender system, various user behaviors are driven by highly entangled latent factors from two environments, i.e., consuming environment where users consume items and social environment where users connect with each other. Uncovering the disentanglement of these latent factors for users can benefit in enhanced explainability and controllability for recommendation. However, in literature there has been no work on social recommendation capable of disentangling user representations across consuming and social environments. To solve this problem, we study co-disentangled representation learning across different environments via proposing the curriculum co-disentangled representation learning (CurCoDis) model to disentangle the hidden factors for users across both consuming and social environments. To co-disentangle joint representations for user-item consumption and user-user social graph simultaneously, we partition the social graph into equal-size sub-graphs with minimum number of edges being cut, and design a curriculum weighing strategy for subgraph training through measuring the complexity of subgraphs via Descartes' rule of signs. We further develop the prototype-routing optimization mechanism, which achieves co-disentanglement of user representations across consuming and social environments. Extensive experiments for social recommendation demonstrate that our proposed CurCoDis model can significantly outperform state-of-the-art methods on several real-world datasets.",[],[],"['Xin Wang', 'Zirui Pan', 'Yuwei Zhou', 'Hong Chen', 'Chendi Ge', 'Wenwu Zhu']","['Department of Computer Science and Technology, BNRist, Tsinghua University', 'Department of Computer Science and Technology, BNRist, Tsinghua University', 'Department of Computer Science and Technology, BNRist, Tsinghua University', 'Department of Computer Science and Technology, BNRist, Tsinghua University', 'Department of Computer Science and Technology, BNRist, Tsinghua University', 'Department of Computer Science and Technology, BNRist, Tsinghua University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618484,Transparency & Explainability,Interpretable Neural-Symbolic Concept Reasoning,"Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based models on challenging benchmarks (ii) discovers meaningful logic rules matching known ground truths even in the absence of concept supervision during training, and (iii), facilitates the generation of counterfactual examples providing the learnt rules as guidance.",[],[],"['Pietro Barbiero', 'Gabriele Ciravegna', 'Francesco Giannini', 'Mateo Espinosa Zarlenga', 'Lucie Charlotte Magister', 'Alberto Tonda', 'Pietro Lió', 'Frederic Precioso', 'Mateja Jamnik', 'Giuseppe Marra']","['University of Cambridge, Cambridge, UK', ""Université Côte d'Azur, Inria, CNRS, I3S, Maasai, Nice"", 'University of Siena, Siena', 'University of Cambridge, Cambridge, UK', 'University of Cambridge, Cambridge, UK', 'INRA, Université Paris- Saclay, Thiverval-Grignon', 'University of Cambridge, Cambridge, UK', ""Université Côte d'Azur, Inria, CNRS, I3S, Maasai, Nice"", 'University of Cambridge, Cambridge, UK', 'KU Leuven, Leuven']","[None, 'France', 'Italy', None, None, 'France', None, 'France', None, 'Belgium']"
https://dl.acm.org/doi/abs/10.5555/3586589.3586595,Transparency & Explainability,XAI Beyond Classification: Interpretable Neural Clustering,"In this paper, we study two challenging problems in explainable AI (XAI) and data clustering. The first is how to directly design a neural network with inherent interpretability, rather than giving post-hoc explanations of a black-box model. The second is implementing discrete $k$-means with a differentiable neural network that embraces the advantages of parallel computing, online clustering, and clustering-favorable representation learning. To address these two challenges, we design a novel neural network, which is a differentiable reformulation of the vanilla $k$-means, called inTerpretable nEuraL cLustering (TELL). Our contributions are threefold. First, to the best of our knowledge, most existing XAI works focus on supervised learning paradigms. This work is one of the few XAI studies on unsupervised learning, in particular, data clustering. Second, TELL is an interpretable, or the so-called intrinsically explainable and transparent model. In contrast, most existing XAI studies resort to various means for understanding a black-box model with post-hoc explanations. Third, from the view of data clustering, TELL possesses many properties highly desired by $k$-means, including but not limited to online clustering, plug-and-play module, parallel computing, and provable convergence. Extensive experiments show that our method achieves superior performance comparing with 14 clustering approaches on three challenging data sets. The source code could be accessed at www.pengxi.me.","['transparent neural networks', 'stochastic k-means clustering', 'differentiable programming']",[],"['Xi Peng', 'Yunfan Li', 'Ivor W. Tsang', 'Hongyuan Zhu', 'Jiancheng Lv', 'Joey Tianyi Zhou']","['College of Computer Science, Sichuan University, Chengdu', 'College of Computer Science, Sichuan University, Chengdu', 'Centre for Frontier Artificial Intelligence Research, A*STAR, Singapore and n Artificial Intelligence Institute, University of Technology, Sydne', 'Institute for Infocomm Research, A*STAR', 'College of Computer Science, Sichuan University, Chengdu', 'n Artificial Intelligence Institute, University of Technology, Sydney  and Institute of High Performance Computing, A*STAR, Singapor']","['China', 'China', 'Australia', 'Singapore', 'China', 'Australia']"
https://dl.acm.org/doi/10.5555/3618408.3619962,Transparency & Explainability,Effective Neural Topic Modeling with Embedding Clustering Regularization,"Topic models have been prevalent for decades with various applications. However, existing topic models commonly suffer from the notorious topic collapsing: discovered topics semantically collapse towards each other, leading to highly repetitive topics, insufficient topic discovery, and damaged model interpretability. In this paper, we propose a new neural topic model, Embedding Clustering Regularization Topic Model (ECRTM). Besides the existing reconstruction error, we propose a novel Embedding Clustering Regularization (ECR), which forces each topic embedding to be the center of a separately aggregated word embedding cluster in the semantic space. This enables each produced topic to contain distinct word semantics, which alleviates topic collapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics together with high-quality topic distributions of documents. Extensive experiments on benchmark datasets demonstrate that ECRTM effectively addresses the topic collapsing issue and consistently surpasses state-of-the-art baselines in terms of topic quality, topic distributions of documents, and downstream classification tasks.",[],[],"['Xiaobao Wu', 'Xinshuai Dong', 'Thong Nguyen', 'Anh Tuan Luu']","['Nanyang Technological University', 'Carnegie Mellon University', 'National University o', 'Nanyang Technological University']","[None, None, 'Singapore', None]"
https://dl.acm.org/doi/10.5555/3618408.3619982,Transparency & Explainability,Discover and Cure: Concept-aware Mitigation of Spurious Correlation,"Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure (DISC), to tackle the issue. With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task and a skin-lesion classification task by 7.5% and 9.6%, respectively. Additionally, we offer theoretical analysis and guarantees to understand the benefits of models trained by DISC. Code and data are available at https://github.com/Wuyxin/DISC.",[],[],"['Shirley Wu', 'Mert Yuksekgonul', 'Linjun Zhang', 'James Zou']","['Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Statistics, Rutgers University', 'Department of Computer Science, Stanford University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618857,Transparency & Explainability,Towards Reliable Neural Specifications,"Having reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). While existing specifications contribute to verifying adversarial robustness, a significant problem in many research domains, our empirical study shows that those verified regions are somewhat tight, and thus fail to allow verification of test set inputs, making them impractical for some real-world applications. To this end, we propose a new family of specifications called neural representation as specification. This form of specifications uses the intrinsic information of neural networks, specifically neural activation patterns (NAPs), rather than input data to specify the correctness and/or robustness of neural network predictions. We present a simple statistical approach to mining neural activation patterns. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no ambiguity between different NAPs. We show that by using NAP, we can verify a significant region of the input space, while still recalling 84% of the data on MNIST. Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark. Thus, we argue that NAPs can potentially be used as a more reliable and extensible specification for neural network verification.",[],[],"['Chuqin Geng', 'Nham Le', 'Xiaojie Xu', 'Zhaoyue Wang', 'Arie Gurfinkel', 'Xujie Si']","['McGill University and Mila Quebec AI Institute', 'University of Waterloo', 'McGill University and Mila Quebec AI Institute', 'McGill University and Mila Quebec AI Institute', 'University of Waterloo', 'Mila Quebec AI Institute and University of Toronto']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618696,Transparency & Explainability,Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling,"Despite the success of physics-informed neural networks (PINNs) in approximating partial differential equations (PDEs), PINNs can sometimes fail to converge to the correct solution in problems involving complicated PDEs. This is reflected in several recent studies on characterizing the ""failure modes"" of PINNs, although a thorough understanding of the connection between PINN failure modes and sampling strategies is missing. In this paper, we provide a novel perspective of failure modes of PINNs by hypothesizing that training PINNs relies on successful ""propagation"" of solution from initial and/or boundary condition points to interior points. We show that PINNs with poor sampling strategies can get stuck at trivial solutions if there are propagation failures, characterized by highly imbalanced PDE residual fields. To mitigate propagation failures, we propose a novel Retain-Resample-Release sampling (R3) algorithm that can incrementally accumulate collocation points in regions of high PDE residuals with little to no computational overhead. We provide an extension of R3 sampling to respect the principle of causality while solving time-dependent PDEs. We theoretically analyze the behavior of R3 sampling and empirically demonstrate its efficacy and efficiency in comparison with baselines on a variety of PDE problems.",[],[],"['Arka Daw', 'Jie Bu', 'Sifan Wang', 'Paris Perdikaris', 'Anuj Karpatne']","['Department of Computer Science, Virginia Tech, Blacksburg, Virginia', 'Department of Computer Science, Virginia Tech, Blacksburg, Virginia', 'University of Pennsylvania, Philadelphia, Pennsylvania', 'University of Pennsylvania, Philadelphia, Pennsylvania', 'Department of Computer Science, Virginia Tech, Blacksburg, Virginia']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618867,Transparency & Explainability,Generalized Disparate Impact for Configurable Fairness Solutions in ML,"We make two contributions in the field of AI fairness over continuous protected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR) indicator (the only one currently available for such a case) is valuable but subject to a few crucial limitations regarding semantics, interpretability, and robustness. Second, we introduce a family of indicators that are: 1) complementary to HGR in terms of semantics; 2) fully interpretable and transparent; 3) robust over finite samples; 4) configurable to suit specific applications. Our approach also allows us to define fine-grained constraints to permit certain types of dependence and forbid others selectively. By expanding the available options for continuous protected attributes, our approach represents a significant contribution to the area of fair artificial intelligence.",[],[],"['Luca Giuliani', 'Eleonora Misino', 'Michele Lombardi']","['Department of Computer Science and Engineering, University of Bologna, Bologna', 'Department of Computer Science and Engineering, University of Bologna, Bologna', 'Department of Computer Science and Engineering, University of Bologna, Bologna']","['Italy', 'Italy', 'Italy']"
https://dl.acm.org/doi/10.5555/3618408.3618909,Transparency & Explainability,On the Impact of Knowledge Distillation for Model Interpretability,"Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields. The code is available at https://github.com/Rok07/KD_XAI.git.",[],[],"['Hyeongrok Han', 'Siwon Kim', 'Hyun-Soo Choi', 'Sungroh Yoon']","['Department of Electrical and Computer Engineering, Seoul National University, Seoul, Republic of Korea', 'Department of Electrical and Computer Engineering, Seoul National University, Seoul, Republic of Korea', 'Department of Computer Science and Engineering, Seoul National University of Science and Technology, Seoul, Republic of Korea and ZIOVISION Inc., Chuncheon, Republic of Korea', 'Department of Electrical and Computer Engineering and Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Republic of Korea']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618865,Transparency & Explainability,"Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat","ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible, potentially underperforming than their Blackbox equivalents. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iterativelycarve outa mixture of interpretable models and aresidual network. The interpretable models identify a subset of samples and explain them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that ourroute, interpret, and repeatapproach (1) identifies a richer diverse set of instance-specific concepts with high concept completeness via interpretable models by specializing in various subsets of data without compromising in performance, (2) identifies the relatively ``harder'' samples to explain via residuals, (3) outperforms the interpretable by-design models by significant margins during test-time interventions, (4) can be used to fix the shortcut learned by the original Blackbox.",[],[],"['Shantanu Ghosh', 'Ke Yu', 'Forough Arabshahi', 'Kayhan Batmanghelich']","['Department of Electrical and Computer Engineering, Boston University, MA', 'Intelligent Systems Program, University of Pittsburgh, PA', 'MetaAI, MenloPark, CA', 'Department of Electrical and Computer Engineering, Boston University, MA']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618663,Transparency & Explainability,Task-specific experimental design for treatment effect estimation,"Understanding causality should be a core requirement of any attempt to build real impact through AI. Due to the inherent unobservability of counterfactuals, large randomised trials (RCTs) are the standard for causal inference. But large experiments are generically expensive, and randomisation carries its own costs, e.g. when suboptimal decisions are trialed. Recent work has proposed more sample-efficient alternatives to RCTs, but these are not adaptable to the downstream application for which the causal effect is sought. In this work, we develop a task-specific approach to experimental design and derive sampling strategies customised to particular downstream applications. Across a range of important tasks, real-world datasets, and sample sizes, our method outperforms other benchmarks, e.g. requiring an order-of-magnitude less data to match RCT performance on targeted marketing tasks.",[],[],"['Bethany Connolly', 'Kim Moore', 'Tobias Schwedes', 'Alexander Adam', 'Gary Willis', 'Ilya Feige', 'Christopher Frye']","['Faculty, London, UK', 'Faculty, London, UK', 'Faculty, London, UK', 'Faculty, London, UK', 'Faculty, London, UK', 'Faculty, London, UK', 'Faculty, London, UK']","[None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618734,Transparency & Explainability,On Data Manifolds Entailed by Structural Causal Models,"The geometric structure of data is an important inductive bias in machine learning. In this work, we characterize the data manifolds entailed by structural causal models. The strengths of the proposed framework are twofold: firstly, the geometric structure of the data manifolds is causally informed, and secondly, it enables causal reasoning about the data manifolds in an interventional and a counterfactual sense. We showcase the versatility of the proposed framework by applying it to the generation of causally-grounded counterfactual explanations for machine learning classifiers, measuring distances along the data manifold in a differential geometric-principled manner.",[],[],"['Ricardo Dominguez-Olmedo', 'Amir-Hossein Karimi', 'Georgios Arvanitidis', 'Bernhard Schölkopf']","['Max Planck Institute for Intelligent Systems, Tübingen', 'Max Planck Institute for Intelligent Systems, Tübingen, Germany and ETH Zürich, Zürich', 'Technical University of , Lyngby', 'Max Planck Institute for Intelligent Systems, Tübingen']","['Germany', 'Switzerland', 'Denmark', 'Germany']"
https://dl.acm.org/doi/10.5555/3618408.3619136,Transparency & Explainability,Variational Mixture of HyperGenerators for Learning Distributions over Functions,"Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VaMoH. VaMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VaMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VaMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VaMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better than previous approaches, while being less computationally demanding.",[],[],"['Batuhan Koyuncu', 'Pablo Sánchez-Martín', 'Ignacio Peis', 'Pablo M. Olmos', 'Isabel Valera']","['Saarland University, Saarbrücken', 'Max Planck Institute for Intelligent Systems, Tübingen', 'Universidad Carlos III de Madrid, Madrid', 'Universidad Carlos III de Madrid, Madrid', 'Saarland University, Saarbrücken']","['Germany', 'Germany', 'Spain', 'Spain', 'Germany']"
https://dl.acm.org/doi/10.5555/3618408.3619307,Transparency & Explainability,2D-Shapley: A Framework for Fragmented Data Valuation,"Data valuation—quantifying the contribution of individual data sources to certain predictive behaviors of a model—is of great importance to enhancing the transparency of machine learning and designing incentive systems for data sharing. Existing work has focused on evaluating data sources with the shared feature or sample space. How to valuate fragmented data sources of which each only contains partial features and samples remains an open question. We start by presenting a method to calculate the counterfactual of removing a fragment from the aggregated data matrix. Based on the counterfactual calculation, we further propose 2D-Shapley, a theoretical framework for fragmented data valuation that uniquely satisfies some appealing axioms in the fragmented data context. 2D-Shapley empowers a range of new use cases, such as selecting useful data fragments, providing interpretation for sample-wise data values, and fine-grained data issue diagnosis.",[],[],"['Zhihong Liu', 'Hoang Anh Just', 'Xiangyu Chang', 'Xi Chen', 'Ruoxi Jia']","[""Center for Intelligent Decision-Making and Machine Learning, Department of Information Systems and Intelligent Business, School of Management, Xi'an Jiaotong University, Xi'an"", 'Bradley Department of Electrical and Computer Engineering, Virginia Tech, Virginia', ""Center for Intelligent Decision-Making and Machine Learning, Department of Information Systems and Intelligent Business, School of Management, Xi'an Jiaotong University, Xi'an"", 'Bradley Department of Electrical and Computer Engineering, Virginia Tech, Virginia', 'Department of Technology, Operations, and Statistics, Stern School of Business, New York University, New York']","['China', None, 'China', None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619032,Transparency & Explainability,Automatically Auditing Large Language Models via Discrete Optimization,"Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to find a non-toxic input that starts with ``Barack Obama'' that a model maps to a toxic output. This optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, ARCA, that jointly and efficiently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g. ""Barack Obama is a legalized unborn"" --> ""child murderer""), produces French inputs that complete to English outputs, and finds inputs that generate a specific name. Our work offers a promising new tool to uncover models' failure-modes before deployment. Content Warning: This paper contains examples that may be offensive in nature.",[],[],"['Erik Jones', 'Anca Dragan', 'Aditi Raghunathan', 'Jacob Steinhardt']","['UC Berkeley', 'UC Berkeley', 'Carnegie Mellon University', 'UC Berkeley']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619676,Transparency & Explainability,Explainability as statistical inference,"A wide variety of model explanation approaches have been proposed in recent years, all guided by very different rationales and heuristics. In this paper, we take a new route and cast interpretability as a statistical inference problem. We propose a general deep probabilistic model designed to produce interpretable predictions. The model’s parameters can be learned via maximum likelihood, and the method can be adapted to any predictor network architecture, and any type of prediction problem. Our model is akin to amortized interpretability methods, where a neural network is used as a selector to allow for fast interpretation at inference time. Several popular interpretability methods are shown to be particular cases of regularized maximum likelihood for our general model. Using our framework, we identify imputation as a common issue of these models. We propose new datasets with ground truth selection which allow for the evaluation of the features importance map and show experimentally that multiple imputation provides more reasonable interpretations.",[],[],"['Hugo Henri Joseph Senetaire', 'Damien Garreau', 'Jes Frellsen', 'Pierre-Alexandre Mattei']","['Department of Applied Mathematics and Computer Science, Technical University of ', ""Université Côte d'Azur, Inria, Maasai, LJAD, CNRS, Nice"", 'Department of Applied Mathematics and Computer Science, Technical University of ', ""Université Côte d'Azur, Inria, Maasai, LJAD, CNRS, Nice""]","['Denmark', 'France', 'Denmark', 'France']"
https://dl.acm.org/doi/10.5555/3618408.3619779,Transparency & Explainability,Learning Prescriptive ReLU Networks,"We study the problem of learning optimal policy from a set of discrete treatment options using observational data. We propose a piecewise linear neural network model that can balance strong prescriptive performance and interpretability, which we refer to as the prescriptive ReLU network, or P-ReLU. We show analytically that this model (i) partitions the input space into disjoint polyhedra, where all instances that belong to the same partition receive the same treatment, and (ii) can be converted into an equivalent prescriptive tree with hyperplane splits for interpretability. We demonstrate the flexibility of the P-ReLU network as constraints can be easily incorporated with minor modifications to the architecture. Through experiments, we validate the superior prescriptive accuracy of P-ReLU against competing benchmarks. Lastly, we present examples of prescriptive trees extracted from trained P-ReLUs using a real-world dataset, for both the unconstrained and constrained scenarios.",[],[],"['Wei Sun', 'Asterios Tsiourvas']","['IBM Research, Yorktown Heights, NY', 'Operations Research Center, Massachusetts Institute of Technology, Cambridge, MA']","[None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619961,Transparency & Explainability,Causal Proxy Models for Concept-based Model Explanations,"Explainability methods for NLP systems encounter a version of the fundamental problem of causal inference: for a given ground-truth input text, we never truly observe the counterfactual texts necessary for isolating the causal effects of model representations on outputs. In response, many explainability methods make no use of counterfactual texts, assuming they will be unavailable. In this paper, we show that robust causal explainability methods can be created using approximate counterfactuals, which can be written by humans to approximate a specific counterfactual or simply sampled using metadata-guided heuristics. The core of our proposal is the Causal Proxy Model (CPM). A CPM explains a black-box model $\mathcal{N}$ because it is trained to have the same *actual* input/output behavior as $\mathcal{N}$ while creating neural representations that can be intervened upon to simulate the *counterfactual* input/output behavior of $\mathcal{N}$. Furthermore, we show that the best CPM for $\mathcal{N}$ performs comparably to $\mathcal{N}$ in making factual predictions, which means that the CPM can simply replace $\mathcal{N}$, leading to more explainable deployed models.",[],[],"['Zhengxuan Wu', ""Karel D'Oosterlinck"", 'Atticus Geiger', 'Amir Zur', 'Christopher Potts']","['Stanford University, Stanford, California', 'Ghent University - imec, Ghent', 'Stanford University, Stanford, California', 'Stanford University, Stanford, California', 'Stanford University, Stanford, California']","[None, 'Belgium', None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619629,Transparency & Explainability,"Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models","Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.",[],[],"['Phillip Rust', 'Anders Søgaard']","['Department of Computer Science, University of Copenhagen', 'Department of Computer Science, University of Copenhagen']","[None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619762,Transparency & Explainability,Lookahead When It Matters: Adaptive Non-causal Transformers for Streaming Neural Transducers,"Streaming speech recognition architectures are employed for low-latency, real-time applications. Such architectures are often characterized by their causality. Causal architectures emit tokens at each frame, relying only on current and past signal, while non-causal models are exposed to a window of future frames at each step to increase predictive accuracy. This dichotomy amounts to a trade-off for real-time Automatic Speech Recognition (ASR) system design: profit from the low-latency benefit of strictly-causal architectures while accepting predictive performance limitations, or realize the modeling benefits of future-context models accompanied by their higher latency penalty. In this work, we relax the constraints of this choice and present the Adaptive Non-Causal Attention Transducer (ANCAT). Our architecture is non-causal in the traditional sense, but executes in a low-latency, streaming manner by dynamically choosing when to rely on future context and to what degree within the audio stream. The resulting mechanism, when coupled with our novel regularization algorithms, delivers comparable accuracy to non-causal configurations while improving significantly upon latency, closing the gap with their causal counterparts. We showcase our design experimentally by reporting comparative ASR task results with measures of accuracy and latency on both publicly accessible and production-scale, voice-assistant datasets.",[],[],"['Grant P. Strimel', 'Yi Xie', 'Brian King', 'Martin Radfar', 'Ariya Rastrow', 'Athanasios Mouchtaris']","['Amazon Alexa', 'Amazon Alexa', 'Amazon Alexa', 'Amazon Alexa', 'Amazon Alexa', 'Amazon Alexa']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619609,Transparency & Explainability,Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts,"In this paper, we focus on mean-field variational Bayesian Neural Networks (BNNs) and explore the representation capacity of such BNNs by investigating which types of concepts are less likely to be encoded by the BNN. It has been observed and studied that a relatively small set of interactive concepts usually emerge in the knowledge representation of a sufficiently-trained neural network, and such concepts can faithfully explain the network output. Based on this, our study proves that compared to standard deep neural networks (DNNs), it is less likely for BNNs to encode complex concepts. Experiments verify our theoretical proofs. Note that the tendency to encode less complex concepts does not necessarily imply weak representation power, considering that complex concepts exhibit low generalization power and high adversarial vulnerability. The code is available at https://github.com/sjtu-xai-lab/BNN-concepts.",[],[],"['Qihan Ren', 'Huiqi Deng', 'Yunuo Chen', 'Siyu Lou', 'Quanshi Zhang']","['Shanghai Jiao Tong University, Shanghai', 'Shanghai Jiao Tong University, Shanghai', 'Shanghai Jiao Tong University, Shanghai', 'Shanghai Jiao Tong University, Shanghai', 'Shanghai Jiao Tong University, Shanghai,  and Department of Computer Science and Engineering, the John Hopcroft Cente']","['China', 'China', 'China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3619720,Transparency & Explainability,Probabilistic Attention-to-Influence Neural Models for Event Sequences,"Discovering knowledge about which types of events influence others, using datasets of event sequences without time stamps, has several practical applications. While neural sequence models are able to capture complex and potentially long-range historical dependencies, they often lack the interpretability of simpler models for event sequence dynamics. We provide a novel neural framework in such a setting - a probabilistic attention-to-influence neural model - which not only captures complex instance-wise interactions between events but also learns influencers for each event type of interest. Given event sequence data and a prior distribution on type-wise influence, we efficiently learn an approximate posterior for type-wise influence by an attention-to-influence transformation using variational inference. Our method subsequently models the conditional likelihood of sequences by sampling the above posterior to focus attention on influencing event types. We motivate our general framework and show improved performance in experiments compared to existing baselines on synthetic data as well as real-world benchmarks, for tasks involving prediction and influencing set identification.",[],[],"['Xiao Shou', 'Debarun Bhattacharjya', 'Tian Gao', 'Dharmashankar Subramanian', 'Oktie Hassanzadeh', 'Kristin P. Bennett']","['Rensselaer Polytechnic Institute, Troy, NY', 'IBM AI Research, Yorktown Heights, NY', 'IBM AI Research, Yorktown Heights, NY', 'IBM AI Research, Yorktown Heights, NY', 'IBM AI Research, Yorktown Heights, NY', 'Rensselaer Polytechnic Institute, Troy, NY']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619147,Transparency & Explainability,Towards Explaining Distribution Shifts,"A distribution shift can have fundamental consequences such as signaling a change in the operating environment or significantly reducing the accuracy of downstream models. Thus, understanding distribution shifts is critical for examining and hopefully mitigating the effect of such a shift. Most prior work has focused on merely detecting if a shift has occurred and assumes any detected shift can be understood and handled appropriately by a human operator. We hope to aid in these manual mitigation tasks by explaining the distribution shift using interpretable transportation maps from the original distribution to the shifted one. We derive our interpretable mappings from a relaxation of the optimal transport problem, where the candidate mappings are restricted to a set of interpretable mappings. We then use a wide array of quintessential examples of distribution shift in real-world tabular, text, and image cases to showcase how our explanatory mappings provide a better balance between detail and interpretability than baseline explanations by both visual inspection and our PercentExplained metric.",[],[],"['Sean Kulinski', 'David I. Inouye']","['Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN', 'Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN']","[None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619388,Transparency & Explainability,Applied Online Algorithms with Heterogeneous Predictors,"For many application domains, the integration of machine learning (ML) models into decision making is hindered by the poor explainability and theoretical guarantees of black box models. Although the emerging area of algorithms with predictions offers a way to leverage ML while enjoying worst-case guarantees, existing work usually assumes access to only one predictor. We demonstrate how to more effectively utilize historical datasets and application domain knowledge by intentionally using predictors ofdifferentquantities. By leveraging the heterogeneity in our predictors, we are able to achieve improved performance, explainability and computational efficiency over predictor-agnostic methods. Theoretical results are supplemented by large-scale empirical evaluations with production data demonstrating the success of our methods on optimization problems occurring in large distributed computing systems.",[],[],"['Jessica Maghakian', 'Russell Lee', 'Mohammad Hajiesmaili', 'Jian Li', 'Ramesh Sitaraman', 'Zhenhua Liu']","['Department of Applied Mathematics & Statistics, Stony Brook University, Stony Brook, NY', 'Manning College of Information and Computer Sciences, UMass Amherst, Amherst, MA', 'Manning College of Information and Computer Sciences, UMass Amherst, Amherst, MA', 'Department of Electrical and Computer Engineering, SUNY Binghamton, Binghamton, NY', 'Manning College of Information and Computer Sciences, UMass Amherst, Amherst, MA and Akamai Tech, Cambridge, MA', 'Department of Applied Mathematics & Statistics, Stony Brook University, Stony Brook, NY']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618922,Transparency & Explainability,Counterfactual Analysis in Dynamic Latent State Models,"We provide an optimization-based framework to perform counterfactual analysis in a dynamic model with hidden states. Our framework is grounded in the ``abduction, action, and prediction'' approach to answer counterfactual queries and handles two key challenges where (1) the states are hidden and (2) the model is dynamic. Recognizing the lack of knowledge on the underlying causal mechanism and the possibility of infinitely many such mechanisms, we optimize over this space and compute upper and lower bounds on the counterfactual quantity of interest. Our work brings together ideas from causality, state-space models, simulation, and optimization, and we apply it on a breast cancer case study. To the best of our knowledge, we are the first to compute lower and upper bounds on a counterfactual query in a dynamic latent-state model.",[],[],"['Martin Haugh', 'Raghav Singal']","['Imperial College', 'Dartmouth College']","[None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618561,Transparency & Explainability,LESS-VFL: Communication-Efficient Feature Selection for Vertical Federated Learning,"We propose LESS-VFL, a communication-efficient feature selection method for distributed systems with vertically partitioned data. We consider a system of a server and several parties with local datasets that share a sample ID space but have different feature sets. The parties wish to collaboratively train a model for a prediction task. As part of the training, the parties wish to remove unimportant features in the system to improve generalization, efficiency, and explainability. In LESS-VFL, after a short pre-training period, the server optimizes its part of the global model to determine the relevant outputs from party models. This information is shared with the parties to then allow local feature selection without communication. We analytically prove that LESS-VFL removes spurious features from model training. We provide extensive empirical evidence that LESS-VFL can achieve high accuracy and remove spurious features at a fraction of the communication cost of other feature selection approaches.",[],[],"['Timothy Castiglia', 'Yi Zhou', 'Shiqiang Wang', 'Swanand Kadhe', 'Nathalie Baracaldo', 'Stacy Patterson']","['Rensselaer Polytechnic Institute', 'IBM Research', 'IBM Research', 'IBM Research', 'IBM Research', 'Rensselaer Polytechnic Institute']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618816,Transparency & Explainability,Explainable Data-Driven Optimization: From Context to Decision and Back Again,"Data-driven optimization uses contextual information and machine learning algorithms to find solutions to decision problems with uncertain parameters. While a vast body of work is dedicated to interpreting machine learning models in the classification setting, explaining decision pipelines involving learning algorithms remains unaddressed. This lack of interpretability can block the adoption of data-driven solutions as practitioners may not understand or trust the recommended decisions. We bridge this gap by introducing a counterfactual explanation methodology tailored to explain solutions to data-driven problems. We introduce two classes of explanations and develop methods to find nearest explanations of random forest and nearest-neighbor predictors. We demonstrate our approach by explaining key problems in operations management such as inventory management and routing.",[],[],"['Alexandre Forel', 'Axel Parmentier', 'Thibaut Vidal']","['CIRRELT & SCALE-AI Chair in Data-Driven Supply Chaims, Department of Mathematical and Industrial Engineering, Polytechnique Montreal, Montreal', 'CERMICS, École des Ponts, Marne-la-Vallée', 'CIRRELT & SCALE-AI Chair in Data-Driven Supply Chaims, Department of Mathematical and Industrial Engineering, Polytechnique Montreal, Montreal']","['Canada', 'France', 'Canada']"
https://dl.acm.org/doi/10.5555/3618408.3620003,Transparency & Explainability,Relevant Walk Search for Explaining Graph Neural Networks,"Graph Neural Networks (GNNs) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for GNNs (GNN-LRP) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by GNN-LRP requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-$K$ relevant walks, which drastically reduces the computation and thus increases the applicability of GNN-LRP to large-scale problems. Our proposed algorithms are based on the max-product algorithm---a common tool for finding the maximum likelihood configurations in probabilistic graphical models---and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks. We provide our codes under github.com/xiong-ping/rel_walk_gnnlrp.",[],[],"['Ping Xiong', 'Thomas Schnake', 'Michael Gastegger', 'Grégoire Montavon', 'Klaus-Robert Müller', 'Shinichi Nakajima']","['Technische Universität Berlin and BIFOLD - Berlin Institute for the Foundations of Learning and Data', 'Technische Universität Berlin and BIFOLD - Berlin Institute for the Foundations of Learning and Data', 'Technische Universität Berlin', 'Freie Universität Berlin and BIFOLD - Berlin Institute for the Foundations of Learning and Data', 'Technische Universität Berlin and BIFOLD - Berlin Institute for the Foundations of Learning and Data and Department of Artificial Intelligence, Korea University, Seoul, Korea and Max Planck Institut für Informatik, Saarbrücken,  and Google Research, Brain team, Berli', 'Technische Universität Berlin and BIFOLD - Berlin Institute for the Foundations of Learning and Data and RIKEN Center for AIP']","[None, None, None, None, 'Germany', 'Japan']"
https://dl.acm.org/doi/10.5555/3618408.3618490,Transparency & Explainability,Individually Fair Learning with One-Sided Feedback,"We consider an online learning problem with one-sided feedback, in which the learner is able to observe the true label only for positively predicted instances. On each round, $k$ instances arrive and receive classification outcomes according to a randomized policy deployed by the learner, whose goal is to maximize accuracy while deploying individually fair policies. We first present a novel auditing scheme, capable of utilizing feedback from dynamically-selected panels of multiple, possibly inconsistent, auditors regarding fairness violations. In particular, we show how our proposed auditing scheme allows for algorithmically exploring the resulting accuracy-fairness frontier, with no need for additional feedback from auditors. We then present an efficient reduction from our problem of online learning with one-sided feedback and a panel reporting fairness violations to the contextual combinatorial semi-bandit problem (Cesa-Bianchi & Lugosi, 2009; Gyorgy et al., 2007), allowing us to leverage algorithms for contextual combinatorial semi-bandits to establish multi-criteria no regret guarantees in our setting, simultaneously for accuracy and fairness. Our results eliminate two potential sources of bias from prior work: the “hidden outcomes” that are not available to an algorithm operating in the full information setting, and human biases that might be present in any single human auditor, but can be mitigated by selecting a well-chosen panel.",[],[],"['Yahav Bechavod', 'Aaron Roth']","['Hebrew University and University of Pennsylvania', 'University of Pennsylvania']","[None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619080,Transparency & Explainability,"Trainability, Expressivity and Interpretability in Gated Neural ODEs","Understanding how the dynamics in biological and artificial neural networks implement the computations required for a task is a salient open question in machine learning and neuroscience. In particular, computations requiring complex memory storage and retrieval pose a significant challenge for these networks to implement or learn. Recently, a family of models described by neural ordinary differential equations (nODEs) has emerged as powerful dynamical neural network models capable of capturing complex dynamics. Here, we extend nODEs by endowing them with adaptive timescales using gating interactions. We refer to these as gated neural ODEs (gnODEs). Using a task that requires memory of continuous quantities, we demonstrate the inductive bias of the gnODEs to learn (approximate) continuous attractors. We further show how reduced-dimensional gnODEs retain their modeling power while greatly improving interpretability, even allowing explicit visualization of the structure of learned attractors. We introduce a novel measure of expressivity which probes the capacity of a neural network to generate complex trajectories. Using this measure, we explore how the phase-space dimension of the nODEs and the complexity of the function modeling the flow field contribute to expressivity. We see that a more complex function for modeling the flow field allows a lower-dimensional nODE to capture a given target dynamics. Finally, we demonstrate the benefit of gating in nODEs on several real-world tasks.",[],[],"['Timothy Doyeon Kim', 'Tankut Can', 'Kamesh Krishnamurthy']","['Princeton Neuroscience Institute, Princeton University, Princeton, NJ', 'School of Natural Sciences, Institute for Advanced Study, Princeton, NJ', 'Princeton Neuroscience Institute and Joseph Henry Laboratories of Physics, Princeton University, Princeton, NJ']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618616,Transparency & Explainability,How to address monotonicity for model risk management?,"In this paper, we study the problem of establishing the accountability and fairness of transparent machine learning models through monotonicity. Although there have been numerous studies on individual monotonicity, pairwise monotonicity is often overlooked in the existing literature. This paper studies transparent neural networks in the presence of three types of monotonicity: individual monotonicity, weak pairwise monotonicity, and strong pairwise monotonicity. As a means of achieving monotonicity while maintaining transparency, we propose the monotonic groves of neural additive models. As a result of empirical examples, we demonstrate that monotonicity is often violated in practice and that monotonic groves of neural additive models are transparent, accountable, and fair.",[],[],"['Dangxing Chen', 'Weicheng Ye']","['Zu Chongzhi Center for Mathematics and Computational Sciences, Duke Kunshan University, Kunshan, Jiangsu', 'Zu Chongzhi Center for Mathematics and Computational Sciences, Duke Kunshan University, Kunshan, Jiangsu']","['China', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3619205,Transparency & Explainability,GLOBE-CE: A Translation Based Approach for Global Counterfactual Explanations,"Counterfactual explanations have been widely studied in explainability, with a range of application dependent methods prominent in fairness, recourse and model understanding. The major shortcoming associated with these methods, however, is their inability to provide explanations beyond the local or instance-level. While many works touch upon the notion of a global explanation, typically suggesting to aggregate masses of local explanations in the hope of ascertaining global properties, few provide frameworks that are both reliable and computationally tractable. Meanwhile, practitioners are requesting more efficient and interactive explainability tools. We take this opportunity to propose Global & Efficient Counterfactual Explanations (GLOBE-CE), a flexible framework that tackles the reliability and scalability issues associated with current state-of-the-art, particularly on higher dimensional datasets and in the presence of continuous features. Furthermore, we provide a unique mathematical analysis of categorical feature translations, utilising it in our method. Experimental evaluation with publicly available datasets and user studies demonstrate that GLOBE-CE performs significantly better than the current state-of-the-art across multiple metrics (e.g., speed, reliability).",[],[],"['Dan Ley', 'Saumitra Mishra', 'Daniele Magazzeni']","['Harvard University', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618537,Transparency & Explainability,Emergence of Sparse Representations from Noise,"A hallmark of biological neural networks, which distinguishes them from their artificial counterparts, is the high degree of sparsity in their activations. This discrepancy raises three questions our work helps to answer: (i) Why are biological networks so sparse? (ii) What are the benefits of this sparsity? (iii) How can these benefits be utilized by deep learning models? Our answers to all of these questions center around training networks to handle random noise. Surprisingly, we discover that noisy training introduces three implicit loss terms that result in sparsely firing neurons specializing to high variance features of the dataset. When trained to reconstruct noisy-CIFAR10, neurons learn biological receptive fields. More broadly, noisy training presents a new approach to potentially increase model interpretability with additional benefits to robustness and computational efficiency.",[],[],"['Trenton Bricken', 'Rylan Schaeffer', 'Bruno Olshausen', 'Gabriel Kreiman']","['Systems, Synthetic and Quantitative Biology, Harvard University and Redwood Center for Theoretical Neuroscience, University of California, Berkeley', 'Computer Science, Stanford University', 'Redwood Center for Theoretical Neuroscience, University of California, Berkeley', 'Programs in Biophysics and Neuroscience, Harvard Medical School']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619935,Transparency & Explainability,On Heterogeneous Treatment Effects in Heterogeneous Causal Graphs,"Heterogeneity and comorbidity are two interwoven challenges associated with various healthcare problems that greatly hampered research on developing effective treatment and understanding of the underlying neurobiological mechanism. Very few studies have been conducted to investigate heterogeneous causal effects (HCEs) in graphical contexts due to the lack of statistical methods. To characterize this heterogeneity, we first conceptualize heterogeneous causal graphs (HCGs) by generalizing the causal graphical model with confounder-based interactions and multiple mediators. Such confounders with an interaction with the treatment are known as moderators. This allows us to flexibly produce HCGs given different moderators and explicitly characterize HCEs from the treatment or potential mediators on the outcome. We establish the theoretical forms of HCEs and derive their properties at the individual level in both linear and nonlinear models. An interactive structural learning is developed to estimate the complex HCGs and HCEs with confidence intervals provided. Our method is empirically justified by extensive simulations and its practical usefulness is illustrated by exploring causality among psychiatric disorders for trauma survivors. Code implementing the proposed algorithm is open-source and publicly available at: https://github.com/richard-watson/ISL.",[],[],"['Richard Watson', 'Hengrui Cai', 'Xinming An', 'Samuel Mclean', 'Rui Song']","['Department of Statistics, North Carolina State University', 'Department of Statistics, University of California Irvine', 'Department of Anesthesiology, University of North Carolina Chapel Hill', 'Department of Anesthesiology, University of North Carolina Chapel Hill', 'Department of Statistics, North Carolina State University']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618548,Transparency & Explainability,Extrapolated Random Tree for Regression,"In this paper, we propose a novel tree-based algorithm named *Extrapolated Random Tree for Regression* (ERTR) that adapts to arbitrary smoothness of the regression function while maintaining the interpretability of the tree. We first put forward the *homothetic random tree for regression* (HRTR) that converges to the target function as the homothetic ratio approaches zero. Then ERTR uses a linear regression model to extrapolate HRTR estimations with different ratios to the ratio zero. From the theoretical perspective, we for the first time establish the optimal convergence rates for ERTR when the target function resides in the general Hölder space $C^{k,\alpha}$ for $k\in \mathbb{N}$, whereas the lower bound of the convergence rate of the random tree for regression (RTR) is strictly slower than ERTR in the space $C^{k,\alpha}$ for $k\geq 1$. This shows that ERTR outperforms RTR for the target function with high-order smoothness due to the extrapolation. In the experiments, we compare ERTR with state-of-the-art tree algorithms on real datasets to show the superior performance of our model. Moreover, promising improvements are brought by using the extrapolated trees as base learners in the extension of ERTR to ensemble methods.",[],[],"['Yuchao Cai', 'Yuheng Ma', 'Yiwei Dong', 'Hanfang Yang']","['School of Statistics, Renmin University o', 'School of Statistics, Renmin University o', 'School of Statistics, Renmin University o', 'School of Statistics, Renmin University of  and Center for Applied Statistics, School of Statistics, Renmin University o']","['China', 'China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3618656,Transparency & Explainability,A Toy Model of Universality: Reverse Engineering how Networks Learn Group Operations,"Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small networks learn to implement group compositions. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that these networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks trained on various groups and architectures, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.",[],[],"['Bilal Chughtai', 'Lawrence Chan', 'Neel Nanda']","['Independent', 'UC Berkeley', 'Independent']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619045,Transparency & Explainability,Identifying Interpretable Subspaces in Image Representations,"We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.",[],[],"['Neha Kalibhat', 'Shweta Bhardwaj', 'Bayan Bruss', 'Hamed Firooz', 'Maziar Sanjabi', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park', 'Center for Machine Learning, CapitalOne', 'Meta AI', 'Meta AI', 'University of Maryland, College Park']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619826,Transparency & Explainability,PWSHAP: A Path-Wise Explanation Model for Targeted Variables,"Predictive black-box models can exhibit high-accuracy but their opaque nature hinders their uptake in safety-critical deployment environments. Explanation methods (XAI) can provide confidence for decision-making through increased transparency. However, existing XAI methods are not tailored towards models in sensitive domains where one predictor is of special interest, such as a treatment effect in a clinical model, or ethnicity in policy models. We introduce Path-Wise Shapley effects (PWSHAP), a framework for assessing the targeted effect of a binary (e.g. treatment) variable from a complex outcome model. Our approach augments the predictive model with a user-defined directed acyclic graph (DAG). The method then uses the graph alongside on-manifold Shapley values to identify effects along causal pathways whilst maintaining robustness to adversarial attacks. We establish error bounds for the identified path-wise Shapley effects and for Shapley values. We show PWSHAP can perform local bias and mediation analyses with faithfulness to the model. Further, if the targeted variable is randomised we can quantify local effect modification. We demonstrate the resolution, interpretability and true locality of our approach on examples and a real-world experiment.",[],[],"['Lucile Ter-Minassian', 'Oscar Clivio', 'Karla Diaz-Ordaz', 'Robin J. Evans', 'Chris Holmes']","['Department of Statistics, University of Oxford, Oxford, UK', 'Department of Statistics, University of Oxford, Oxford, UK', 'Department of Statistical Science, University College London, London, UK', 'Department of Statistics, University of Oxford, Oxford, UK', 'Department of Statistics, University of Oxford, Oxford, UK and The Alan Turing Institute, London, UK']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3618937,Transparency & Explainability,Generalized Teacher Forcing for Learning Chaotic Dynamics,"Chaotic dynamical systems (DS) are ubiquitous in nature and society. Often we are interested in reconstructing such systems from observed time series for prediction or mechanistic insight, where by reconstruction we mean learning geometrical and invariant temporal properties of the system in question (like attractors). However, training reconstruction algorithms like recurrent neural networks (RNNs) on such systems by gradient-descent based techniques faces severe challenges. This is mainly due to exploding gradients caused by the exponential divergence of trajectories in chaotic systems. Moreover, for (scientific) interpretability we wish to have as low dimensional reconstructions as possible, preferably in a model which is mathematically tractable. Here we report that a surprisingly simple modification of teacher forcing leads to provably strictly all-time bounded gradients in training on chaotic systems, and, when paired with a simple architectural rearrangement of a tractable RNN design, piecewise-linear RNNs (PLRNNs), allows for faithful reconstruction in spaces of at most the dimensionality of the observed system. We show on several DS that with these amendments we can reconstruct DS better than current SOTA algorithms, in much lower dimensions. Performance differences were particularly compelling on real world data with which most other methods severely struggled. This work thus led to a simple yet powerful DS reconstruction algorithm which is highly interpretable at the same time.",[],[],"['Florian Hess', 'Zahra Monfared', 'Manuel Brenner', 'Daniel Durstewitz']","['Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim and Faculty of Physics and Astronomy and Cluster of Excellence STRUCTURES, Heidelberg University, Heidelberg', 'Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim and 3Cluster of Excellence STRUCTURES, Heidelberg University, Heidelberg', 'Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim and Faculty of Physics and Astronomy, Heidelberg University, Heidelberg', 'Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim and Faculty of Physics and Interdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, German']","['Germany', 'Germany', 'Germany', None]"
https://dl.acm.org/doi/10.1145/3580305.3599363,Transparency & Explainability,Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting,"Conventional supervised learning methods typically assume i.i.d samples and are found to be sensitive to out-of-distribution (OOD) data. We propose Generative Causal Representation Learning (GCRL) which leverages causality to facilitate knowledge transfer under distribution shifts. While we evaluate the effectiveness of our proposed method in human trajectory prediction models, GCRL can be applied to other domains as well. First, we propose a novel causal model that explains the generative factors in motion forecasting datasets using features that are common across all environments and with features that are specific to each environment. Selection variables are used to determine which parts of the model can be directly transferred to a new environment without fine-tuning. Second, we propose an end-to-end variational learning paradigm to learn the causal mechanisms that generate observations from features. GCRL is supported by strong theoretical results that imply identifiability of the causal model under certain assumptions. Experimental results on synthetic and real-world motion forecasting datasets show the robustness and effectiveness of our proposed method for knowledge transfer under zero-shot and low-shot settings by substantially outperforming the prior motion forecasting models on out-of-distribution prediction.","['Spatio-Temporal Representation Learning',Generative Causal Model']",['Computing methodologies _ Knowledge representation and reasoning'],"['Yu Zhao', 'Pan Deng', 'Junting Liu', 'Xiaofeng Jia', 'Jianwei Zhang']","['Beihang University, Beijing', 'Beihang University, Beijing', 'Beihang University, Beijing', 'Beijing Big Data Centre, Beijing', 'Capinfo Company Limited, Beijing']","['China', 'China', 'China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3618704,Fairness & Bias,Scaling Vision Transformers to 22 Billion Parameters,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",[],[],"['Mostafa Dehghani', 'Josip Djolonga', 'Basil Mustafa', 'Piotr Padlewski', 'Jonathan Heek', 'Justin Gilmer', 'Andreas Steiner', 'Mathilde Caron', 'Robert Geirhos', 'Ibrahim Alabdulmohsin', 'Rodolphe Jenatton', 'Lucas Beyer', 'Michael Tschannen', 'Anurag Arnab', 'Xiao Wang', 'Carlos Riquelme', 'Matthias Minderer', 'Joan Puigcerver', 'Utku Evci', 'Manoj Kumar', 'Sjoerd Van Steenkiste', 'Gamaleldin F. Elsayed', 'Aravindh Mahendran', 'Fisher Yu', 'Avital Oliver', 'Fantine Huot', 'Jasmijn Bastings', 'Mark Patrick Collier', 'Alexey A. Gritsenko', 'Vighnesh Birodkar', 'Cristina Vasconcelos', 'Yi Tay', 'Thomas Mensink', 'Alexander Kolesnikov', 'Filip Pavetić', 'Dustin Tran', 'Thomas Kipf', 'Mario Lučić', 'Xiaohua Zhai', 'Daniel Keysers', 'Jeremiah Harmsen', 'Neil Houlsby']","['Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619397,Fairness & Bias,Differential Privacy has Bounded Impact on Fairness in Classification,"We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. This result is a consequence of a more general statement on accuracy conditioned on an arbitrary event (such as membership to a sensitive group), which may be of independent interest. We use this Lipschitz property to prove a non-asymptotic bound showing that, as the number of samples increases, the fairness level of private models gets closer to the one of their non-private counterparts. This bound also highlights the importance of the confidence margin of a model on the disparate impact of differential privacy.",[],[],"['Paul Mangold', 'Michaël Perrot', 'Aurélien Bellet', 'Marc Tommasi']","['Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189, CRIStAL, Lille', 'Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189, CRIStAL, Lille', 'Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189, CRIStAL, Lille', 'Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189, CRIStAL, Lille']","['France', 'France', 'France', 'France']"
https://dl.acm.org/doi/10.5555/3618408.3619213,Fairness & Bias,FAIRER: Fairness as Decision Rationale Alignment,"Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while ignoring intermediate neuron alignment. To address the issue, we formulate the fairness as a new task, i.e., decision rationale alignment that requires DNNs' neurons to have consistent responses on subgroups at both intermediate processes and the final prediction. To make this idea practical during optimization, we relax the naive objective function and propose gradient-guided parity alignment, which encourages gradient-weighted consistency of neurons across subgroups. Extensive experiments on a variety of datasets show that our method can significantly enhance fairness while sustaining a high level of accuracy and outperforming other approaches by a wide margin.",[],[],"['Tianlin Li', 'Qing Guo', 'Aishan Liu', 'Mengnan Du', 'Zhiming Li', 'Yang Liu']","['Nanyang Technological University', 'Institute of High Performance Computing (IHPC), Centre for Frontier AI Research (CFAR), A*STAR', 'Beihang University', 'New  Institute of Technolog', 'Zhejiang Sci-Tech University,  and Nanyang Technological University, Singapor', 'Nanyang Technological University']","['Singapore', 'Singapore', 'China', 'Jersey', 'China', 'Singapore']"
https://dl.acm.org/doi/10.5555/3618408.3618510,Fairness & Bias,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introducePythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intendPythiato facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at https://github.com/EleutherAI/pythia.",[],[],"['Stella Biderman', 'Hailey Schoelkopf', 'Quentin Anthony', 'Herbie Bradley', ""Kyle O'Brien"", 'Eric Hallahan', 'Mohammad Aflah Khan', 'Shivanshu Purohit', 'USVSN Sai Prashanth', 'Edward Raff', 'Aviya Skowron', 'Lintang Sutawika', 'Oskar Van Der Wal']","['EleutherAI and Booz Allen Hamilton, McLean', 'EleutherAI and Yale University, New Haven', 'EleutherAI', 'EleutherAI and University of Cambridge, UK', 'EleutherAI', 'EleutherAI', 'Indraprastha Institute of Information Technology Delhi', 'Stability AI and EleutherAI', 'EleutherAI', 'Booz Allen Hamilton, McLean', 'EleutherAI', 'EleutherAI and Datasaur.ai', 'Institute for Logic, Language and Computation, University of Amsterdam']","[None, None, None, None, None, None, 'India', None, None, None, None, None, 'Netherlands']"
https://dl.acm.org/doi/10.5555/3618408.3620174,Fairness & Bias,Men Also Do Laundry: Multi-Attribute Bias Amplification,"The phenomenon of $\textit{bias amplification}$ occurs when models amplify training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\texttt{computer}$). However, large-scale datasets typically consist of instances with multiple attribute annotations (e.g., $\{\texttt{computer}, \texttt{keyboard}\}$). We demonstrate models can learn to exploit correlations with respect to multiple attributes, which are not accounted for by current metrics. Moreover, we show that current metrics can give the erroneous impression that little to no bias amplification has occurred as they aggregate positive and negative bias scores. Further, these metrics lack an ideal value, making them difficult to interpret. To address these shortcomings, we propose a new metric: $\textit{Multi-Attribute Bias Amplification}$. We validate our metric's utility through a bias amplification analysis on the COCO, imSitu, and CelebA datasets. Finally, we benchmark bias mitigation methods using our proposed metric, suggesting possible avenues for future bias mitigation efforts.",[],[],"['Dora Zhao', 'Jerone T. A. Andrews', 'Alice Xiang']","['Sony AI, New York', 'Sony AI, Tokyo', 'Sony AI, New York']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3454287.3455674,Privacy & Data Governance,Differential Privacy has Bounded Impact on Fairness in Classification,"We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. This result is a consequence of a more general statement on accuracy conditioned on an arbitrary event (such as membership to a sensitive group), which may be of independent interest. We use this Lipschitz property to prove a non-asymptotic bound showing that, as the number of samples increases, the fairness level of private models gets closer to the one of their non-private counterparts. This bound also highlights the importance of the confidence margin of a model on the disparate impact of differential privacy.",[],[],"['Eugene Bagdasaryan', 'Omid Poursaeed', 'Vitaly Shmatikov']","['Cornell Tech', 'Cornell Tech', 'Cornell Tech']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619568,Privacy & Data Governance,Conformal Prediction for Federated Uncertainty Quantification Under Label Shift,"Federated Learning (FL) is a machine learning framework where many clients collaboratively train models while keeping the training data decentralized. Despite recent advances in FL, the uncertainty quantification topic (UQ) remains partially addressed. Among UQ methods, conformal prediction (CP) approaches provides distribution-free guarantees under minimal assumptions. We develop a new federated conformal prediction method based on quantile regression and take into account privacy constraints. This method takes advantage of importance weighting to effectively address the label shift between agents and provides theoretical guarantees for both valid coverage of the prediction sets and differential privacy. Extensive experimental studies demonstrate that this method outperforms current competitors.",[],[],"['Vincent Plassier', 'Mehdi Makni', 'Aleksandr Rubashevskii', 'Eric Moulines', 'Maxim Panov']","['Lagrange Mathematics and Computing Research Center, Paris,  and CMAP, Ecole Polytechnique, Paris', 'Lagrange Mathematics and Computing Research Center, Paris', 'Skolkovo Institute of Science and Technology, Moscow, Russia and Mohamed bin Zayed University of Artificial Intelligence, Masdar City, UAE', 'CMAP, Ecole Polytechnique, Paris,  and Mohamed bin Zayed University of Artificial Intelligence, Masdar City, UA', 'Technology Innovation Institute, Abu Dhabi, UAE']","['France', 'France', None, 'France', None]"
https://dl.acm.org/doi/10.5555/3618408.3619059,Privacy & Data Governance,Cocktail Party Attack: Breaking Aggregation-Based Privacy in Federated Learning Using Independent Component Analysis,"Federated learning (FL) aims to perform privacy-preserving machine learning on distributed data held by multiple data owners. To this end, FL requires the data owners to perform training locally and share the gradients or weight updates (instead of the private inputs) with the central server, which are then securely aggregated over multiple data owners. Although aggregation by itself does not offer provable privacy protection, prior work suggested that if the batch size is sufficiently large the aggregation may be secure enough. In this paper, we propose the Cocktail Party Attack (CPA) that, contrary to prior belief, is able to recover the private inputs from gradients/weight updates aggregated over as many as 1024 samples. CPA leverages the crucial insight that aggregate gradients from a fully connected (FC) layer is a linear combination of its inputs, which allows us to frame gradient inversion as a blind source separation (BSS) problem. We adapt independent component analysis (ICA)---a classic solution to the BSS problem---to recover private inputs for FC and convolutional networks, and show that CPA significantly outperforms prior gradient inversion attacks, scales to ImageNet-sized inputs, and works on large batch sizes of up to 1024.",[],[],"['Sanjay Kariyappa', 'Chuan Guo', 'Kiwan Maeng', 'Wenjie Xiong', 'G. Edward Suh', 'Moinuddin K Qureshi', 'Hsien-Hsin S. Lee']","['Institute of Technolog', 'Meta AI', 'Pennsylvania State University', 'Virginia Tech', 'Cornell University and Meta AI', 'Institute of Technolog', 'Intel']","['Georgia', None, None, None, None, 'Georgia', None]"
https://dl.acm.org/doi/10.5555/3618408.3619823,Privacy & Data Governance,Concurrent Shuffle Differential Privacy Under Continual Observation,"We introduce the concurrent shuffle model of differential privacy. In this model we have multiple concurrent shufflers permuting messages from different, possibly overlapping, batches of users. Similarly to the standard (single) shuffler model, the privacy requirement is that the concatenation of all shuffled messages should be differentially private. We study the private continual summation problem (a.k.a. the counter problem) and show that the concurrent shuffle model allows for significantly improved error compared to a standard (single) shuffler model. Specifically, we give a summation algorithm with error $\tilde{O}(n^{1/(2k+1)})$ with $k$ concurrent shufflers on a sequence of length $n$. Furthermore, we prove that this bound is tight for any $k$, even if the algorithm can choose the sizes of the batches adaptively. For $k=\log n$ shufflers, the resulting error is polylogarithmic, much better than $\tilde{\Theta}(n^{1/3})$ which we show is the smallest possible with a single shuffler. We use our online summation algorithm to get algorithms with improved regret bounds for the contextual linear bandit problem. In particular we get optimal $\tilde{O}(\sqrt{n})$ regret with $k= \tilde{\Omega}(\log n)$ concurrent shufflers.",[],[],"['Jay Tenenbaum', 'Haim Kaplan', 'Yishay Mansour', 'Uri Stemmer']","['Google Research', 'Blavatnik School of Computer Science, Tel Aviv University and Google Research', 'Blavatnik School of Computer Science, Tel Aviv University and Google Research', 'Blavatnik School of Computer Science, Tel Aviv University and Google Research']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619821,Privacy & Data Governance,Deep Regression Unlearning,"With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning",[],[],"['Ayush K Tarun', 'Vikram S Chundawat', 'Murari Mandal', 'Mohan Kankanhalli']","['Mavvex Labs', 'Mavvex Labs', 'School of Computer Engineering, Kalinga Institute of Industrial Technology Bhubaneswar', 'School of Computing, National University o']","['India', 'India', 'India', 'Singapore']"
https://dl.acm.org/doi/abs/10.1007/978-3-030-92681-6_34,Privacy & Data Governance,Streaming Submodular Maximization with Differential Privacy,"In this work, we study the problem of privately maximizing a submodular function in the streaming setting. Extensive work has been done on privately maximizing submodular functions in the general case when the function depends upon the private data of individuals. However, when the size of the data stream drawn from the domain of the objective function is large or arrives very fast, one must privately optimize the objective within the constraints of the streaming setting. We establish fundamental differentially private baselines for this problem and then derive better trade-offs between privacy and utility for the special case of decomposable submodular functions. A submodular function is decomposable when it can be written as a sum of submodular functions; this structure arises naturally when each summand function models the utility of an individual and the goal is to study the total utility of the whole population as in the well-known Combinatorial Public Projects Problem. Finally, we complement our theoretical analysis with experimental corroboration.",[],[],"['Di Xiao', 'Longkun Guo', 'Kewen Liao', 'Pei Yao']","['Fuzhou University, Fuzhou', 'Fuzhou University, Fuzhou', 'n Catholic University, Sydney', 'Fuzhou University, Fuzhou']","['China', 'China', 'Australia', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3619194,Privacy & Data Governance,HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption,"Transfer learning is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and fine-tuning new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in transfer learning in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based Transfer Learning algorithm, that protects the client's privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known benchmark datasets show total training times of 567--3442 seconds, which is less than an hour.",[],[],"['Seewoo Lee', 'Garam Lee', 'Jung Woo Kim', 'Junbum Shin', 'Mun-Kyu Lee']","['University of California, Berkeley', 'CryptoLab Inc., Seoul, South Korea', 'CryptoLab Inc., Seoul, South Korea', 'CryptoLab Inc., Seoul, South Korea', 'Inha University, Incheon, South Korea']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3620182,Security,Protecting Language Generation Models via Invisible Watermarking,"Language generation models have been an increasingly powerful enabler to many applications. Many such models offer free or affordable API access which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and make fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as ``synonym randomization''. To address this issue, we propose GINSW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.",[],[],"['Xuandong Zhao', 'Yu-Xiang Wang', 'Lei Li']","['Department of Computer Science, UC Santa Barbara', 'Department of Computer Science, UC Santa Barbara', 'Department of Computer Science, UC Santa Barbara']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3620178,Security,Addressing Budget Allocation and Revenue Allocation in Data Market Environments Using an Adaptive Sampling Algorithm,"High-quality machine learning models are dependent on access to high-quality training data. When the data are not already available, it is tedious and costly to obtain them. Data markets help with identifying valuable training data: model consumers pay to train a model, the market uses that budget to identify data and train the model (the budget allocation problem), and finally the market compensates data providers according to their data contribution (revenue allocation problem). For example, a bank could pay the data market to access data from other financial institutions to train a fraud detection model. Compensating data contributors requires understanding data’s contribution to the model; recent efforts to solve this revenue allocation problem based on the Shapley value are inefficient to lead to practical data markets. In this paper, we introduce a new algorithm to solve budget allocation and revenue allocation problems simultaneously in linear time. The new algorithm employs an adaptive sampling process that selects data from those providers who are contributing the most to the model. Better data means that the algorithm accesses those providers more often, and more frequent accesses corresponds to higher compensation. Furthermore, the algorithm can be deployed in both centralized and federated scenarios, boosting its applicability. We provide theoretical guarantees for the algorithm that show the budget is used efficiently and the properties of revenue allocation are similar to Shapley’s. Finally, we conduct an empirical evaluation to show the performance of the algorithm in practical scenarios and when compared to other baselines. Overall, we believe that the new algorithm paves the way for the implementation of practical data markets.",[],[],"['Boxin Zhao', 'Boxiang Lyu', 'Raul Castro Fernandez', 'Mladen Kolar']","['Booth School of Business, University of Chicago, Chicago, IL', 'Booth School of Business, University of Chicago, Chicago, IL', 'Department of Computer Science, University of Chicago, Chicago, IL', 'Booth School of Business, University of Chicago, Chicago, IL']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619357,Security,Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks,"Indiscriminate data poisoning attacks aim to decrease a model's test accuracy by injecting a small amount of corrupted training data. Despite significant interest, existing attacks remain relatively ineffective against modern machine learning (ML) architectures. In this work, we introduce the notion of model poisoning reachability as a technical tool to explore the intrinsic limits of data poisoning attacks towards target parameters (i.e., model-targeted attacks). We derive an easily computable threshold to establish and quantify a surprising phase transition phenomenon among popular ML models: data poisoning attacks can achieve certain target parameters only when the poisoning ratio exceeds our threshold. Building on existing parameter corruption attacks and refining the Gradient Canceling attack, we perform extensive experiments to confirm our theoretical findings, test the predictability of our transition threshold, and significantly improve existing indiscriminate data poisoning baselines over a range of datasets and models. Our work highlights the critical role played by the poisoning ratio, and sheds new insights on existing empirical results, attacks and mitigation strategies in data poisoning.",[],[],"['Yiwei Lu', 'Gautam Kamath', 'Yaoliang Yu']","['School of Computer Science, University of Waterloo,  and Vector Institut', 'School of Computer Science, University of Waterloo,  and Vector Institut', 'School of Computer Science, University of Waterloo,  and Vector Institut']","['Canada', 'Canada', 'Canada']"
https://dl.acm.org/doi/10.5555/3618408.3619059,Security,Cocktail Party Attack: Breaking Aggregation-Based Privacy in Federated Learning Using Independent Component Analysis,"Federated learning (FL) aims to perform privacy-preserving machine learning on distributed data held by multiple data owners. To this end, FL requires the data owners to perform training locally and share the gradients or weight updates (instead of the private inputs) with the central server, which are then securely aggregated over multiple data owners. Although aggregation by itself does not offer provable privacy protection, prior work suggested that if the batch size is sufficiently large the aggregation may be secure enough. In this paper, we propose the Cocktail Party Attack (CPA) that, contrary to prior belief, is able to recover the private inputs from gradients/weight updates aggregated over as many as 1024 samples. CPA leverages the crucial insight that aggregate gradients from a fully connected (FC) layer is a linear combination of its inputs, which allows us to frame gradient inversion as a blind source separation (BSS) problem. We adapt independent component analysis (ICA)---a classic solution to the BSS problem---to recover private inputs for FC and convolutional networks, and show that CPA significantly outperforms prior gradient inversion attacks, scales to ImageNet-sized inputs, and works on large batch sizes of up to 1024.",[],[],"['Sanjay Kariyappa', 'Chuan Guo', 'Kiwan Maeng', 'Wenjie Xiong', 'G. Edward Suh', 'Moinuddin K Qureshi', 'Hsien-Hsin S. Lee']","['Institute of Technolog', 'Meta AI', 'Pennsylvania State University', 'Virginia Tech', 'Cornell University and Meta AI', 'Institute of Technolog', 'Intel']","['Georgia', None, None, None, None, 'Georgia', None]"
https://dl.acm.org/doi/10.5555/3618408.3620122,Security,Graph Contrastive Backdoor Attacks,"Graph Contrastive Learning (GCL) has attracted considerable interest due to its impressive node representation learning capability. Despite the wide application of GCL techniques, little attention has been paid to the security of GCL. In this paper, we systematically study the vulnerability of GCL in the presence of malicious backdoor adversaries. In particular, we proposeGCBA, the first backdoor attack for graph contrastive learning. GCBA incorporates three attacks: poisoning, crafting, and natural backdoor, each targeting one stage of the GCL pipeline. We formulate our attacks as optimization problems and solve them with a novel discrete optimization technique to overcome the discrete nature of graph-structured data. By extensively evaluating GCBA on multiple datasets and GCL methods, we show that our attack can achieve high attack success rates while preserving stealthiness. We further consider potential countermeasures to our attack and conclude that existing defenses are insufficient to mitigate GCBA. We show that as a complex paradigm involving data and model republishing, GCL is vulnerable to backdoor attacks, and specifically designed defenses are needed to mitigate the backdoor attacks on GCL.",[],[],"['Hangfan Zhang', 'Jinghui Chen', 'Lu Lin', 'Jinyuan Jia', 'Dinghao Wu']","['Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619821,Security,Deep Regression Unlearning,"With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning",[],[],"['Ayush K Tarun', 'Vikram S Chundawat', 'Murari Mandal', 'Mohan Kankanhalli']","['Mavvex Labs', 'Mavvex Labs', 'School of Computer Engineering, Kalinga Institute of Industrial Technology Bhubaneswar', 'School of Computing, National University o']","['India', 'India', 'India', 'Singapore']"
https://dl.acm.org/doi/10.5555/3618408.3619525,Security,Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark,"Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce Machiavelli, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.",[],[],"['Alexander Pan', 'Jun Shern Chan', 'Andy Zou', 'Nathaniel Li', 'Steven Basart', 'Thomas Woodside', 'Hanlin Zhang', 'Scott Emmons', 'Dan Hendrycks']","['University of California, Berkeley', 'Center For AI Safety, San Francisco', 'Carnegie Mellon University, Pittsburgh', 'University of California, Berkeley', 'Center For AI Safety, San Francisco', 'Yale University, New Haven', 'Carnegie Mellon University, Pittsburgh', 'University of California, Berkeley', 'Center For AI Safety, San Francisco']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3618408.3619299,Security,Opponent-Limited Online Search for Imperfect Information Games,"In recent years, online search has been playing an increasingly important role in imperfect information games (IIGs). Previous online search is known as common-knowledge subgame solving, which has to consider all the states in a common-knowledge closure. This is only computationally tolerable for medium size games, such as poker. To handle larger games, order-1 Knowledge-Limited Subgame Solving (1-KLSS) only considers the states in a knowledge-limited closure, which results in a much smaller subgame. However, 1-KLSS is unsafe. In this paper, we first extend 1-KLSS to Safe-1-KLSS and prove its safeness. To make Safe-1-KLSS applicable to even larger games, we propose Opponent-Limited Subgame Solving (OLSS) to limit how the opponent reaches a subgame and how it acts in the subgame. Limiting the opponent's strategy dramatically reduces the subgame size and improves the efficiency of subgame solving while still preserving some safety in the limit. Experiments in medium size poker show that Safe-1-KLSS and OLSS are orders of magnitude faster than previous common-knowledge subgame solving. Also, OLSS significantly improves the online performance in a two-player Mahjong game, whose game size prohibits the use of previous common-knowledge subgame-solving methods.",[],[],"['Weiming Liu', 'Haobo Fu', 'Qiang Fu', 'Wei Yang']","['Tencent AI Lab, Shenzhen', 'Tencent AI Lab, Shenzhen', 'Tencent AI Lab, Shenzhen', 'Tencent AI Lab, Shenzhen']","['China', 'China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3618408.3620204,Security,NNSplitter: An Active Defense Solution for DNN Model via Automated Weight Obfuscation,"As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users with the support of the trusted execution environment. Experimental results demonstrate the effectiveness of NNSplitter, e.g., by only modifying 275 out of over 11 million (i.e., 0.002%) weights, the accuracy of the obfuscated ResNet-18 model on CIFAR-10 can drop to 10%. Moreover, NNSplitter is stealthy and resilient against norm clipping and fine-tuning attacks, making it an appealing solution for DNN model protection. The code is available at: https://github.com/Tongzhou0101/NNSplitter.",[],[],"['Tong Zhou', 'Yukui Luo', 'Shaolei Ren', 'Xiaolin Xu']","['Northeastern University, Boston, MA', 'Northeastern University, Boston, MA', 'UC Riverside, Riverside, CA', 'Northeastern University, Boston, MA']","[None, None, None, None]"