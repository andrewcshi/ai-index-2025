link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/2201.09932,Transparency & Explainability,"Learning Optimal Fair Decision Trees: Trade-offs Between Interpretability, Fairness, and Accuracy","The increasing use of machine learning in high-stakes domains -- where people's livelihoods are impacted -- creates an urgent need for interpretable, fair, and highly accurate algorithms. With these needs in mind, we propose a mixed integer optimization (MIO) framework for learning optimal classification trees -- one of the most interpretable models -- that can be augmented with arbitrary fairness constraints. In order to better quantify the ""price of interpretability"", we also propose a new measure of model interpretability called decision complexity that allows for comparisons across different classes of machine learning models. We benchmark our method against state-of-the-art approaches for fair classification on popular datasets; in doing so, we conduct one of the first comprehensive analyses of the trade-offs between interpretability, fairness, and predictive accuracy. Given a fixed disparity threshold, our method has a price of interpretability of about 4.2 percentage points in terms of out-of-sample accuracy compared to the best performing, complex models. However, our method consistently finds decisions with almost full parity, while other methods rarely do.",[],[],"['Nathanael Jo', 'Sina Aghaei', 'Andrés Gómez', 'Phebe Vayanos']","['USC Center for AI in Society, USA', 'USC Center for AI in Society, USA', 'USC Center for AI in Society, USA', 'University of Southern California, USA', 'USC Center for AI in Society, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2307.06941,Transparency & Explainability,On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations,"Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings.",[],[],"['Emanuele Albini', 'Shubham Sharma', 'Saumitra Mishra', 'Danial Dervovic', 'Daniele Magazzeni']","['J.P. Morgan AI Research, United Kingdom', 'J.P. Morgan AI Research, USA', 'J.P. Morgan AI Research, United Kingdom', 'J.P. Morgan AI Research, United Kingdom', 'J.P. Morgan AI Research, United Kingdom']","['United Kingdom', 'US', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2307.10223,Transparency & Explainability,Ethical Unaffordances: Collaboratively Developing Evaluation Frameworks for Queer AI Harms,"Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).",[],[],"['Organizers of QueerInAI', 'Nathan Dennler', 'Anaelia Ovalle', 'Ashwin Singh', 'Luca Soldaini', 'Arjun Subramonian', 'Huy Tu', 'William Agnew', 'Avijit Ghosh', 'Kyra Yee', 'Irene Font Peradejordi', 'Zeerak Talat', 'Mayra Russo', 'Jess de Jesus de Pinho Pinhal']",[],[]
https://arxiv.org/abs/2005.04702,Transparency & Explainability,Unpicking Epistemic Injustices in Digital Health: On Designing Data-Driven Technologies to Support the Self-Management of Long-Term Health Conditions,"This dissertation is focused on the role of objectivity in peer review. Through an examination of aspects of peer review including anonymity, trust, expertise, and the question of who has standing to evaluate research, we find that objectivity in peer review differs significantly from other uses of the term objectivity in science. In peer review it is not required for this objectivity to have correspondence to an outside world, instead it is enough for it to operate inside the ""rules"" of the community. Neither is the objectivity here empirical in the sense of using data about the scientific problem in question. Rather, the objectivity is one of judgment, cleaving to the epistemological standards of a community that are formed by background assumptions and beliefs. As a consequence, we highlight the role of subjectivity in what is usually taken as a practice of objectivity, and arrive at the insight that objectivity is not defined by one core value, but a balance of transparency, confidentiality, trust, representation, and living up to community standards. As such, objectivity in peer review is a highly specific sense of the term that is not reducible to that used in other aspects of scientific practice.",[],[],['Daniel Ucko'],[],[]
https://arxiv.org/abs/2310.06361,Transparency & Explainability,Effective Enforceability of EU Competition Law Under AI Development Scenarios: a Framework for Anticipatory Governance,"The tremendous rise of generative AI has reached every part of society - including the news environment. There are many concerns about the individual and societal impact of the increasing use of generative AI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of generative AI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of generative AI, as well as mitigation strategies to address these. Methodologically, we apply scenario writing and use participatory foresight in the context of a survey (n=119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of generative AI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on the (non-) presence of different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for generative AI impact assessment.",[],[],"['Kimon Kieslich', 'Nicholas Diakopoulos', 'Natali Helberger']",[],[]
https://arxiv.org/abs/2312.04745,Transparency & Explainability,Measures of Disparity and their Efficient Estimation,"In fairness audits, a standard objective is to detect whether a given algorithm performs substantially differently between subgroups. Properly powering the statistical analysis of such audits is crucial for obtaining informative fairness assessments, as it ensures a high probability of detecting unfairness when it exists. However, limited guidance is available on the amount of data necessary for a fairness audit, lacking directly applicable results concerning commonly used fairness metrics. Additionally, the consideration of unequal subgroup sample sizes is also missing. In this tutorial, we address these issues by providing guidance on how to determine the required subgroup sample sizes to maximize the statistical power of hypothesis tests for detecting unfairness. Our findings are applicable to audits of binary classification models and multiple fairness metrics derived as summaries of the confusion matrix. Furthermore, we discuss other aspects of audit study designs that can increase the reliability of audit results.",[],[],"['Harvineet Singh', 'Fan Xia', 'Mi-Ok Kim', 'Romain Pirracchio', 'Rumi Chunara', 'Jean Feng']","['Center for Data Science, New York University, USA', 'School of Global Public Health, New York University, USA']","['US', 'US']"
https://arxiv.org/abs/2309.02517,Transparency & Explainability,Towards User Guided Actionable Recourse,"Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.",[],[],"['Jayanth Yetukuri', 'Ian Hardy', 'Yang Liu']",[],[]
https://arxiv.org/abs/2304.09991,Transparency & Explainability,Designing for Human-AI Collaboration in Auditing LLMs with LLMs,"Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants auditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis formation and testing. Further, with our tool, participants identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown before in formal audits and also those previously under-reported.",[],[],"['Charvi Rastogi', 'Marco Tulio Ribeiro', 'Nicholas King', 'Harsha Nori', 'Saleema Amershi']",[],[]
https://arxiv.org/abs/2212.04997,Transparency & Explainability,Self-determination through explanation: an ethical perspective on the implementation of the transparency requirements for recommender systems set by the Digital Services Act of the European Union,"Artificial intelligence is not only increasingly used in business and administration contexts, but a race for its regulation is also underway, with the EU spearheading the efforts. Contrary to existing literature, this article suggests, however, that the most far-reaching and effective EU rules for AI applications in the digital economy will not be contained in the proposed AI Act - but have just been enacted in the Digital Markets Act. We analyze the impact of the DMA and related EU acts on AI models and their underlying data across four key areas: disclosure requirements; the regulation of AI training data; access rules; and the regime for fair rankings. The paper demonstrates that fairness, in the sense of the DMA, goes beyond traditionally protected categories of non-discrimination law on which scholarship at the intersection of AI and law has so far largely focused on. Rather, we draw on competition law and the FRAND criteria known from intellectual property law to interpret and refine the DMA provisions on fair rankings. Moreover, we show how, based on CJEU jurisprudence, a coherent interpretation of the concept of non-discrimination in both traditional non-discrimination and competition law may be found. The final part sketches specific proposals for a comprehensive framework of transparency, access, and fairness under the DMA and beyond.",[],[],"['Philipp Hacker', 'Johann Cordes', 'Janina Rochon']",[],[]
https://arxiv.org/abs/2305.12622,Transparency & Explainability,Evaluating the Impact of Social Determinants on Health Prediction,"Social determinants of health (SDOH) -- the conditions in which people live, grow, and age -- play a crucial role in a person's health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protective attributes. We hope the new integrated EHR-SDOH database will enable studies on the relationship between community health and individual outcomes and provide new benchmarks to study algorithmic biases beyond race, gender, and age.",[],[],"['Ming Ying Yang', 'Gloria Hyunjung Kwak', 'Tom Pollard', 'Leo Anthony Celi', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology, USA', 'Harvard Medical School, USA and Massachusetts General Hospital, USA', 'Massachusetts Institute of Technology, USA', 'Massachusetts Institute of Technology, USA and Beth Israel Deaconess Medical Center, USA', 'Massachusetts Institute of Technology, USA and Vector Institute, Canada']","['US', 'US', 'US', 'Israel', 'Canada']"
https://arxiv.org/abs/2201.11358,Fairness & Bias,Fairness implications of encoding protected categorical attributes,"Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g.\ support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: \emph{one-hot encoding} and \emph{target encoding}. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, \textit{irreducible bias}, is due to direct group category discrimination, and the second type, \textit{reducible bias}, is due to the large variance in statistically underrepresented groups. We investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. Furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.",[],[],"['Carlos Mougan', 'Jose M. Alvarez', 'Salvatore Ruggieri', 'Steffen Staab']","['Electronic and Computer Science, Univeristy of Southampton, United Kingdom', 'Computer Science, University of Pisa, Italy and Scuola Normale Superiore, Italy', 'Dipartimento di Informatica, Università di Pisa, Italy', 'Universität Stuttgart, Germany and Univeristy of Southampton, United Kingdom']","['United Kingdom', 'Italy', 'Italy', 'Germany']"
https://arxiv.org/abs/2308.08656,Fairness & Bias,"Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data","Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an ``othering'' phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to improve the applicability of computer vision models in a global context.",[],[],"['Keziah Naggita', 'Julienne LaChance', 'Alice Xiang']","['Computer Science, Toyota Technological Institute at Chicago, USA', 'Sony AI America, USA', 'Sony AI America, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2307.10749,Fairness & Bias,Mitigating Voter Attribute Bias for Fair Opinion Aggregation,"The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model. To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.",[],[],"['Ryosuke Ueda', 'Koh Takeuchi', 'Hisashi Kashima']","['Kyoto University, Japan', 'Kyoto University, Japan', 'Kyoto University, Japan']","['Japan', 'Japan', 'Japan']"
https://arxiv.org/abs/2305.00817,Fairness & Bias,Evaluation of targeted dataset collection on racial equity in face recognition,"Facial recognition is one of the most academically studied and industrially developed areas within computer vision where we readily find associated applications deployed globally. This widespread adoption has uncovered significant performance variation across subjects of different racial profiles leading to focused research attention on racial bias within face recognition spanning both current causation and future potential solutions. In support, this study provides an extensive taxonomic review of research on racial bias within face recognition exploring every aspect and stage of the face recognition processing pipeline. Firstly, we discuss the problem definition of racial bias, starting with race definition, grouping strategies, and the societal implications of using race or race-related groupings. Secondly, we divide the common face recognition processing pipeline into four stages: image acquisition, face localisation, face representation, face verification and identification, and review the relevant corresponding literature associated with each stage. The overall aim is to provide comprehensive coverage of the racial bias problem with respect to each and every stage of the face recognition processing pipeline whilst also highlighting the potential pitfalls and limitations of contemporary mitigation strategies that need to be considered within future research endeavours or commercial applications alike.",[],[],"['Seyma Yucer', 'Furkan Tektas', 'Noura Al Moubayed', 'Toby P. Breckon']","['University of Washington, USA', 'Paul G. Allen School of Computer Science & Engineering, University of Washington, USA', 'University of Washington, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2202.01351,Fairness & Bias,"No Justice, No Robots: From the Dispositions of Policing to an Abolitionist Robotics","This special issue interrogates the meaning and impacts of ""tech ethics"": the embedding of ethics into digital technology research, development, use, and governance. In response to concerns about the social harms associated with digital technologies, many individuals and institutions have articulated the need for a greater emphasis on ethics in digital technology. Yet as more groups embrace the concept of ethics, critical discourses have emerged questioning whose ethics are being centered, whether ""ethics"" is the appropriate frame for improving technology, and what it means to develop ""ethical"" technology in practice. This interdisciplinary issue takes up these questions, interrogating the relationships among ethics, technology, and society in action. This special issue engages with the normative and contested notions of ethics itself, how ethics has been integrated with technology across domains, and potential paths forward to support more just and egalitarian technology. Rather than starting from philosophical theories, the authors in this issue orient their articles around the real-world discourses and impacts of tech ethics--i.e., tech ethics in action.",[],[],['Ben Green'],[],[]
https://arxiv.org/abs/2307.03360,Fairness & Bias,Evaluating Biased Attitude Associations of Language Models in an Intersectional Context,"Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.",[],[],"['Shiva Omrani Sabbaghi', 'Robert Wolfe', 'Aylin Caliskan']",[],[]
https://arxiv.org/abs/2308.04346,Fairness & Bias,Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles,"We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader's perception of a country. These findings emphasize the critical role of public perception in shaping AI's impact on society and the need to correct biases in AI systems.",[],[],"['Pranav Narayanan Venkit', 'Sanjana Gautam', 'Ruchi Panchanadikar', ""Ting-Hao `Kenneth' Huang"", 'Shomir Wilson']","['Pennsylvania State University, USA', 'Pennsylvania State University, USA', 'Pennsylvania State University, USA', 'Pennsylvania State University, USA', 'Pennsylvania State University, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2305.12178,Fairness & Bias,Model Debiasing via Gradient-based Explanation on Representation,"Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicting sensitive attributes and 2) the other focus for predicting downstream task labels, and second, use them to perturb the latent code that guides the training of downstream task models towards fairness and utility goals. We show empirically that our framework works with both disentangled and non-disentangled representation learning methods and achieves better fairness-accuracy trade-off on unstructured and structured datasets than previous state-of-the-art approaches.",[],[],"['Jindi Zhang', 'Luning Wang', 'Dan Su', 'Yongxiang Huang', 'Caleb Chen Cao', 'Lei Chen']","['Hong Kong Research Center, Huawei, Hong Kong', 'Hong Kong Research Center, Huawei, Hong Kong', 'NVIDIA Research, Hong Kong', 'Hong Kong Research Center, Huawei, Hong Kong', 'Hong Kong University of Science and Technology, Hong Kong', 'Hong Kong University of Science and Technology, Hong Kong']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://arxiv.org/abs/2306.11964,Fairness & Bias,Sampling Individually-Fair Rankings that are Always Group Fair,"Rankings on online platforms help their end-users find the relevant information -- people, news, media, and products -- quickly. Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in the Algorithmic Fairness, Information Retrieval, and Machine Learning literature. Recent works, however, identify uncertainty in the utilities of items as a primary cause of unfairness and propose introducing randomness in the output. This randomness is carefully chosen to guarantee an adequate representation of each item (while accounting for the uncertainty). However, due to this randomness, the output rankings may violate group fairness constraints. We give an efficient algorithm that samples rankings from an individually-fair distribution while ensuring that every output ranking is group fair. The expected utility of the output ranking is at least $\alpha$ times the utility of the optimal fair solution. Here, $\alpha$ depends on the utilities, position-discounts, and constraints -- it approaches 1 as the range of utilities or the position-discounts shrinks, or when utilities satisfy distributional assumptions. Empirically, we observe that our algorithm achieves individual and group fairness and that Pareto dominates the state-of-the-art baselines.",[],[],"['Sruthi Gorantla', 'Anay Mehrotra', 'Amit Deshpande', 'Anand Louis']","['Computer Science and Automation, Indian Institute of Science, India', 'Yale University, USA', 'Microsoft, India', 'Indian Institute of Science, India']","['India', 'US', 'India', 'India']"
https://arxiv.org/abs/2305.10510,Fairness & Bias,ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages,"In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in such translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT's accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. We also observe ChatGPT completely failing to translate the English gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. While it does respect and provide appropriately gender-marked versions of Bengali words when prompted with gender information in English, ChatGPT appears to confer a higher respect to men than to women in the same occupation. We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AIs that perform language translation to better accommodate such low-resource languages.",[],[],"['Sourojit Ghosh', 'Aylin Caliskan']",[],[]
https://arxiv.org/abs/2303.03975,Fairness & Bias,GATE: A Challenge Set for Gender-Ambiguous Translation Examples,"Although recent years have brought significant progress in improving translation of unambiguously gendered sentences, translation of ambiguously gendered input remains relatively unexplored. When source gender is ambiguous, machine translation models typically default to stereotypical gender roles, perpetuating harmful bias. Recent work has led to the development of ""gender rewriters"" that generate alternative gender translations on such ambiguous inputs, but such systems are plagued by poor linguistic coverage. To encourage better performance on this task we present and release GATE, a linguistically diverse corpus of gender-ambiguous source sentences along with multiple alternative target language translations. We also provide tools for evaluation and system analysis when using GATE and use them to evaluate our translation rewriter system.",[],[],"['Spencer Rarrick', 'Ranjita Naik', 'Varun Mathur', 'Sundar Poudel', 'Vishal Chowdhary']","['Machine Translation, Microsoft, USA', 'Machine Translation, Microsoft, USA', 'Microsoft, USA', 'Machine Translation, Microsoft, USA', 'Machine Translation, Microsoft, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2103.12016,Fairness & Bias,Perceived Algorithmic Fairness using Organizational Justice Theory: An Empirical Case Study on Algorithmic Hiring,"Algorithmic decision-making (ADM) increasingly shapes people's daily lives. Given that such autonomous systems can cause severe harm to individuals and social groups, fairness concerns have arisen. A human-centric approach demanded by scholars and policymakers requires taking people's fairness perceptions into account when designing and implementing ADM. We provide a comprehensive, systematic literature review synthesizing the existing empirical insights on perceptions of algorithmic fairness from 39 empirical studies spanning multiple domains and scientific disciplines. Through thorough coding, we systemize the current empirical literature along four dimensions: (a) algorithmic predictors, (b) human predictors, (c) comparative effects (human decision-making vs. algorithmic decision-making), and (d) consequences of ADM. While we identify much heterogeneity around the theoretical concepts and empirical measurements of algorithmic fairness, the insights come almost exclusively from Western-democratic contexts. By advocating for more interdisciplinary research adopting a society-in-the-loop framework, we hope our work will contribute to fairer and more responsible ADM.",[],[],"['Christopher Starke', 'Janine Baleis', 'Birte Keller', 'Frank Marcinkowski']","['Utrecht University, Netherlands', 'DEUS, Netherlands', 'DEUS, Portugal', 'Utrecht University, Netherlands']","['Netherlands', 'Netherlands', 'Portugal', 'Netherlands']"
https://arxiv.org/abs/2308.02081,Fairness & Bias,"Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare","Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.",[],[],['Eran Tal'],"['McGill University, Canada']",['Canada']
https://arxiv.org/abs/2206.07635,Fairness & Bias,A sector-based approach to AI ethics: Understanding ethical issues of AI-related incidents within their sectoral context,"With the powerful performance of Artificial Intelligence (AI) also comes prevalent ethical issues. Though governments and corporations have curated multiple AI ethics guidelines to curb unethical behavior of AI, the effect has been limited, probably due to the vagueness of the guidelines. In this paper, we take a closer look at how AI ethics issues take place in real world, in order to have a more in-depth and nuanced understanding of different ethical issues as well as their social impact. With a content analysis of AI Incident Database, which is an effort to prevent repeated real world AI failures by cataloging incidents, we identified 13 application areas which often see unethical use of AI, with intelligent service robots, language/vision models and autonomous driving taking the lead. Ethical issues appear in 8 different forms, from inappropriate use and racial discrimination, to physical safety and unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI practitioners with a practical guideline when trying to deploy AI applications ethically.",[],[],"['Mengyi Wei', 'Zhixuan Zhou']","['Institute of Sociology, Technische Universität Berlin, Germany and Science of Intelligence, Research Cluster of Excellence, Germany', 'Volkswagen Group, Germany', 'Responsible Technology Hub, Germany', 'Volkswagen Consulting, Volkswagen Group, Germany', 'DFKI (German Research Center for Artificial Intelligence), Germany', 'Institute of Sociology, Technische Universität Berlin, Germany and Science of Intelligence, Research Cluster of Excellence, Germany', 'Machine Learning Research Lab, Volkswagen Group, Germany and Department of Computer Science, ELTE University Budapest, Hungary', 'Machine Learning Research Lab, Volkswagen Group, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2307.10223,Fairness & Bias,Ethical Unaffordances: Collaboratively Developing Evaluation Frameworks for Queer AI Harms,"Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).",[],[],"['Organizers of QueerInAI', 'Nathan Dennler', 'Anaelia Ovalle', 'Ashwin Singh', 'Luca Soldaini', 'Arjun Subramonian', 'Huy Tu', 'William Agnew', 'Avijit Ghosh', 'Kyra Yee', 'Irene Font Peradejordi', 'Zeerak Talat', 'Mayra Russo', 'Jess de Jesus de Pinho Pinhal']",[],[]
https://arxiv.org/abs/2005.04702,Fairness & Bias,Unpicking Epistemic Injustices in Digital Health: On Designing Data-Driven Technologies to Support the Self-Management of Long-Term Health Conditions,"This dissertation is focused on the role of objectivity in peer review. Through an examination of aspects of peer review including anonymity, trust, expertise, and the question of who has standing to evaluate research, we find that objectivity in peer review differs significantly from other uses of the term objectivity in science. In peer review it is not required for this objectivity to have correspondence to an outside world, instead it is enough for it to operate inside the ""rules"" of the community. Neither is the objectivity here empirical in the sense of using data about the scientific problem in question. Rather, the objectivity is one of judgment, cleaving to the epistemological standards of a community that are formed by background assumptions and beliefs. As a consequence, we highlight the role of subjectivity in what is usually taken as a practice of objectivity, and arrive at the insight that objectivity is not defined by one core value, but a balance of transparency, confidentiality, trust, representation, and living up to community standards. As such, objectivity in peer review is a highly specific sense of the term that is not reducible to that used in other aspects of scientific practice.",[],[],['Daniel Ucko'],[],[]
https://arxiv.org/abs/2307.06518,Fairness & Bias,Machine Learning practices and infrastructures,"Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers' have demonstrated to be particularly salient for the societal impact of deployed ML systems.",[],[],['Glen Berman'],[],[]
https://arxiv.org/abs/2203.08235,Fairness & Bias,A Deep Dive into Dataset Imbalance and Bias in Face Identification,"As the deployment of automated face recognition (FR) systems proliferates, bias in these systems is not just an academic question, but a matter of public concern. Media portrayals often center imbalance as the main source of bias, i.e., that FR models perform worse on images of non-white people or women because these demographic groups are underrepresented in training data. Recent academic research paints a more nuanced picture of this relationship. However, previous studies of data imbalance in FR have focused exclusively on the face verification setting, while the face identification setting has been largely ignored, despite being deployed in sensitive applications such as law enforcement. This is an unfortunate omission, as 'imbalance' is a more complex matter in identification; imbalance may arise in not only the training data, but also the testing data, and furthermore may affect the proportion of identities belonging to each demographic group or the number of images belonging to each identity. In this work, we address this gap in the research by thoroughly exploring the effects of each kind of imbalance possible in face identification, and discuss other factors which may impact bias in this setting.",[],[],"['Valeriia Cherepanova', 'Steven Reich', 'Samuel Dooley', 'Hossein Souri', 'Micah Goldblum', 'Tom Goldstein']","['University of Maryland, USA', 'University of Maryland, USA', 'University of Maryland, USA and Abacus.AI, USA', 'Johns Hopkins University, USA', 'University of Maryland, USA and ArthurAI, USA', 'New York University, USA', 'University of Maryland, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2310.06361,Fairness & Bias,Effective Enforceability of EU Competition Law Under AI Development Scenarios: a Framework for Anticipatory Governance,"The tremendous rise of generative AI has reached every part of society - including the news environment. There are many concerns about the individual and societal impact of the increasing use of generative AI, including issues such as disinformation and misinformation, discrimination, and the promotion of social tensions. However, research on anticipating the impact of generative AI is still in its infancy and mostly limited to the views of technology developers and/or researchers. In this paper, we aim to broaden the perspective and capture the expectations of three stakeholder groups (news consumers; technology developers; content creators) about the potential negative impacts of generative AI, as well as mitigation strategies to address these. Methodologically, we apply scenario writing and use participatory foresight in the context of a survey (n=119) to delve into cognitively diverse imaginations of the future. We qualitatively analyze the scenarios using thematic analysis to systematically map potential impacts of generative AI on the news environment, potential mitigation strategies, and the role of stakeholders in causing and mitigating these impacts. In addition, we measure respondents' opinions on a specific mitigation strategy, namely transparency obligations as suggested in Article 52 of the draft EU AI Act. We compare the results across different stakeholder groups and elaborate on the (non-) presence of different expected impacts across these groups. We conclude by discussing the usefulness of scenario-writing and participatory foresight as a toolbox for generative AI impact assessment.",[],[],"['Kimon Kieslich', 'Nicholas Diakopoulos', 'Natali Helberger']",[],[]
https://arxiv.org/abs/2207.07068,Fairness & Bias,Disambiguating Algorithmic Bias: From Neutrality to Justice,"This paper provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.",[],[],"['Max Hort', 'Zhenpeng Chen', 'Jie M. Zhang', 'Mark Harman', 'Federica Sarro']","['Department of Philosophy, Baruch College, The City University of New York, USA', 'Berkman Klein Center for Internet & Society, Harvard University, USA']","['US', 'US']"
https://arxiv.org/abs/2312.04745,Fairness & Bias,Measures of Disparity and their Efficient Estimation,"In fairness audits, a standard objective is to detect whether a given algorithm performs substantially differently between subgroups. Properly powering the statistical analysis of such audits is crucial for obtaining informative fairness assessments, as it ensures a high probability of detecting unfairness when it exists. However, limited guidance is available on the amount of data necessary for a fairness audit, lacking directly applicable results concerning commonly used fairness metrics. Additionally, the consideration of unequal subgroup sample sizes is also missing. In this tutorial, we address these issues by providing guidance on how to determine the required subgroup sample sizes to maximize the statistical power of hypothesis tests for detecting unfairness. Our findings are applicable to audits of binary classification models and multiple fairness metrics derived as summaries of the confusion matrix. Furthermore, we discuss other aspects of audit study designs that can increase the reliability of audit results.",[],[],"['Harvineet Singh', 'Fan Xia', 'Mi-Ok Kim', 'Romain Pirracchio', 'Rumi Chunara', 'Jean Feng']","['Center for Data Science, New York University, USA', 'School of Global Public Health, New York University, USA']","['US', 'US']"
https://arxiv.org/abs/1803.04383,Fairness & Bias,Not So Fair: The Impact of Presumably Fair Machine Learning Models,"Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",[],[],"['Lydia T. Liu', 'Sarah Dean', 'Esther Rolf', 'Max Simchowitz', 'Moritz Hardt']","[""King's College London, United Kingdom"", 'Universität Osnabrück, Germany', ""King's College London, United Kingdom"", 'Universitat Politecnica de Valencia, Spain', ""King's College London, United Kingdom""]","['United Kingdom', 'Germany', 'United Kingdom', 'Spain', 'United Kingdom']"
https://arxiv.org/abs/2106.03673,Fairness & Bias,The Bureaucratic Challenge to AI Governance: An Empirical Assessment of Implementation at U.S. Federal Agencies,"This article surveys the use of algorithmic systems to support decision-making in the public sector. Governments adopt, procure, and use algorithmic systems to support their functions within several contexts -- including criminal justice, education, and benefits provision -- with important consequences for accountability, privacy, social inequity, and public participation in decision-making. We explore the social implications of municipal algorithmic systems across a variety of stages, including problem formulation, technology acquisition, deployment, and evaluation. We highlight several open questions that require further empirical research.",[],[],"['Karen Levy', 'Kyla Chasalow', 'Sarah Riley']","['Stanford Law School, USA and Harvard Kennedy School, USA', 'Stanford Law School, USA', 'Stanford Law School, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2307.03306,Fairness & Bias,When Fair Classification Meets Noisy Protected Attributes,"The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all. To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-blind algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-blind and noise-tolerant fair classifiers can potentially achieve similar level of performance as attribute-reliant algorithms, even when protected attributes are noisy. However, implementing them in practice requires careful nuance. Our study provides insights into the practical implications of using fair classification algorithms in scenarios where protected attributes are noisy or partially available.",[],[],"['Avijit Ghosh', 'Pablo Kvitca', 'Christo Wilson']","['Khoury College of Computer Sciences, Northeastern University, USA', 'Khoury College of Computer Sciences, Northeastern University, USA and Amazon, USA', 'Khoury College of Computer Sciences, Northeastern University, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2309.02517,Fairness & Bias,Towards User Guided Actionable Recourse,"Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.",[],[],"['Jayanth Yetukuri', 'Ian Hardy', 'Yang Liu']",[],[]
https://arxiv.org/abs/1912.08189,Fairness & Bias,Learning from Discriminatory Training Data,"Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups' intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulations, and is computationally lightweight - it can be used with any supervised learning model to prevent discrimination via proxies while maximizing model accuracy for business necessity.",[],[],"['Przemyslaw A. Grabowicz', 'Nicholas Perello', 'Kenta Takatsu']","['University of Massachusetts Amherst, USA', 'University of Massachusetts Amherst, USA', 'Carnegie Mellon University, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2304.06034,Fairness & Bias,Social Biases through the Text-to-Image Generation Lens,"Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people from results for both models. Such biases can get mitigated by increasing the amount of specification in the prompt itself, although the prompting mitigation will not address discrepancies in image quality or other usages of the model or its representations in other scenarios. Further, we observe personality traits being associated with only a limited set of people at the intersection of race, gender, and age. Finally, an analysis of geographical location representations on everyday situations (e.g., park, food, weddings) shows that for most situations, images generated through default location-neutral prompts are closer and more similar to images generated for locations of United States and Germany.",[],[],"['Ranjita Naik', 'Besmira Nushi']","['Microsoft, USA', 'Microsoft, USA']","['US', 'US']"
https://arxiv.org/abs/2304.09991,Fairness & Bias,Designing for Human-AI Collaboration in Auditing LLMs with LLMs,"Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants auditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis formation and testing. Further, with our tool, participants identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown before in formal audits and also those previously under-reported.",[],[],"['Charvi Rastogi', 'Marco Tulio Ribeiro', 'Nicholas King', 'Harsha Nori', 'Saleema Amershi']",[],[]
https://arxiv.org/abs/2302.05906,Fairness & Bias,Stress-testing Bias Mitigation Algorithms to Understand Fairness Vulnerabilities,"In this paper, we consider a theoretical model for injecting data bias, namely, under-representation and label bias (Blum & Stangl, 2019). We empirically study the effect of varying data biases on the accuracy and fairness of fair classifiers. Through extensive experiments on both synthetic and real-world datasets (e.g., Adult, German Credit, Bank Marketing, COMPAS), we empirically audit pre-, in-, and post-processing fair classifiers from standard fairness toolkits for their fairness and accuracy by injecting varying amounts of under-representation and label bias in their training data (but not the test data). Our main observations are: 1. The fairness and accuracy of many standard fair classifiers degrade severely as the bias injected in their training data increases, 2. A simple logistic regression model trained on the right data can often outperform, in both accuracy and fairness, most fair classifiers trained on biased training data, and 3. A few, simple fairness techniques (e.g., reweighing, exponentiated gradients) seem to offer stable accuracy and fairness guarantees even when their training data is injected with under-representation and label bias. Our experiments also show how to integrate a measure of data bias risk in the existing fairness dashboards for real-world deployments.",[],[],"['Mohit Sharma', 'Amit Deshpande', 'Rajiv Ratn Shah']",[],[]
https://arxiv.org/abs/2212.04997,Fairness & Bias,Self-determination through explanation: an ethical perspective on the implementation of the transparency requirements for recommender systems set by the Digital Services Act of the European Union,"Artificial intelligence is not only increasingly used in business and administration contexts, but a race for its regulation is also underway, with the EU spearheading the efforts. Contrary to existing literature, this article suggests, however, that the most far-reaching and effective EU rules for AI applications in the digital economy will not be contained in the proposed AI Act - but have just been enacted in the Digital Markets Act. We analyze the impact of the DMA and related EU acts on AI models and their underlying data across four key areas: disclosure requirements; the regulation of AI training data; access rules; and the regime for fair rankings. The paper demonstrates that fairness, in the sense of the DMA, goes beyond traditionally protected categories of non-discrimination law on which scholarship at the intersection of AI and law has so far largely focused on. Rather, we draw on competition law and the FRAND criteria known from intellectual property law to interpret and refine the DMA provisions on fair rankings. Moreover, we show how, based on CJEU jurisprudence, a coherent interpretation of the concept of non-discrimination in both traditional non-discrimination and competition law may be found. The final part sketches specific proposals for a comprehensive framework of transparency, access, and fairness under the DMA and beyond.",[],[],"['Philipp Hacker', 'Johann Cordes', 'Janina Rochon']",[],[]
https://arxiv.org/abs/2305.12622,Fairness & Bias,Evaluating the Impact of Social Determinants on Health Prediction,"Social determinants of health (SDOH) -- the conditions in which people live, grow, and age -- play a crucial role in a person's health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protective attributes. We hope the new integrated EHR-SDOH database will enable studies on the relationship between community health and individual outcomes and provide new benchmarks to study algorithmic biases beyond race, gender, and age.",[],[],"['Ming Ying Yang', 'Gloria Hyunjung Kwak', 'Tom Pollard', 'Leo Anthony Celi', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology, USA', 'Harvard Medical School, USA and Massachusetts General Hospital, USA', 'Massachusetts Institute of Technology, USA', 'Massachusetts Institute of Technology, USA and Beth Israel Deaconess Medical Center, USA', 'Massachusetts Institute of Technology, USA and Vector Institute, Canada']","['US', 'US', 'US', 'Israel', 'Canada']"
https://arxiv.org/abs/2310.11867,Fairness & Bias,Evaluating the Fairness of Discriminative Foundation Models in Computer Vision,"We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.",[],[],"['Junaid Ali', 'Matthaeus Kleindessner', 'Florian Wenzel', 'Kailash Budhathoki', 'Volkan Cevher', 'Chris Russell']",[],[]
https://arxiv.org/abs/2307.05543,Fairness & Bias,Ethical and Social Risks of Generative Text-to-Image Models,"This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.",[],[],"['Charlotte Bird', 'Eddie L. Ungless', 'Atoosa Kasirzadeh']",[],[]
https://arxiv.org/abs/2005.04702,Privacy & Data Governance,Unpicking Epistemic Injustices in Digital Health: On Designing Data-Driven Technologies to Support the Self-Management of Long-Term Health Conditions,"This dissertation is focused on the role of objectivity in peer review. Through an examination of aspects of peer review including anonymity, trust, expertise, and the question of who has standing to evaluate research, we find that objectivity in peer review differs significantly from other uses of the term objectivity in science. In peer review it is not required for this objectivity to have correspondence to an outside world, instead it is enough for it to operate inside the ""rules"" of the community. Neither is the objectivity here empirical in the sense of using data about the scientific problem in question. Rather, the objectivity is one of judgment, cleaving to the epistemological standards of a community that are formed by background assumptions and beliefs. As a consequence, we highlight the role of subjectivity in what is usually taken as a practice of objectivity, and arrive at the insight that objectivity is not defined by one core value, but a balance of transparency, confidentiality, trust, representation, and living up to community standards. As such, objectivity in peer review is a highly specific sense of the term that is not reducible to that used in other aspects of scientific practice.",[],[],['Daniel Ucko'],[],[]
https://arxiv.org/abs/2401.15897,Security,From Preference Elicitation to Participatory ML: A Critical Survey & Guidelines for Future Research,"In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how precisely it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of the relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing a broad set of activities and attitudes aimed at improving the behavior of GenAI models, gestures towards red-teaming as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.",[],[],"['Michael Feffer', 'Anusha Sinha', 'Zachary C. Lipton', 'Hoda Heidari']","['Carnegie Mellon University, USA', 'Carnegie Mellon University, USA', 'Carnegie Mellon University, USA', 'Carnegie Mellon University, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2109.04083,Security,User Tampering in Reinforcement Learning Recommender Systems,"In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms -- 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.",[],[],"['Charles Evans', 'Atoosa Kasirzadeh']","['University of Edinburgh, United Kingdom and Australian National University, Australia', 'Australian National University, Australia']","['Australia', 'Australia']"
https://arxiv.org/abs/2106.11022,Security,"Artificial Intelligence, Radical Ignorance, and the Institutional Context of Consent","As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured.",[],[],"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']",[],[]
https://arxiv.org/abs/2206.07635,Security,A sector-based approach to AI ethics: Understanding ethical issues of AI-related incidents within their sectoral context,"With the powerful performance of Artificial Intelligence (AI) also comes prevalent ethical issues. Though governments and corporations have curated multiple AI ethics guidelines to curb unethical behavior of AI, the effect has been limited, probably due to the vagueness of the guidelines. In this paper, we take a closer look at how AI ethics issues take place in real world, in order to have a more in-depth and nuanced understanding of different ethical issues as well as their social impact. With a content analysis of AI Incident Database, which is an effort to prevent repeated real world AI failures by cataloging incidents, we identified 13 application areas which often see unethical use of AI, with intelligent service robots, language/vision models and autonomous driving taking the lead. Ethical issues appear in 8 different forms, from inappropriate use and racial discrimination, to physical safety and unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI practitioners with a practical guideline when trying to deploy AI applications ethically.",[],[],"['Mengyi Wei', 'Zhixuan Zhou']","['Institute of Sociology, Technische Universität Berlin, Germany and Science of Intelligence, Research Cluster of Excellence, Germany', 'Volkswagen Group, Germany', 'Responsible Technology Hub, Germany', 'Volkswagen Consulting, Volkswagen Group, Germany', 'DFKI (German Research Center for Artificial Intelligence), Germany', 'Institute of Sociology, Technische Universität Berlin, Germany and Science of Intelligence, Research Cluster of Excellence, Germany', 'Machine Learning Research Lab, Volkswagen Group, Germany and Department of Computer Science, ELTE University Budapest, Hungary', 'Machine Learning Research Lab, Volkswagen Group, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2307.10312,Security,Beyond the ML Model: Applying Safety Engineering Frameworks to Text-to-Image Development,"Identifying potential social and ethical risks in emerging machine learning (ML) models and their applications remains challenging. In this work, we applied two well-established safety engineering frameworks (FMEA, STPA) to a case study involving text-to-image models at three stages of the ML product development pipeline: data processing, integration of a T2I model with other models, and use. Results of our analysis demonstrate the safety frameworks - both of which are not designed explicitly examine social and ethical risks - can uncover failure and hazards that pose social and ethical risks. We discovered a broad range of failures and hazards (i.e., functional, social, and ethical) by analyzing interactions (i.e., between different ML models in the product, between the ML product and user, and between development teams) and processes (i.e., preparation of training data or workflows for using an ML service/product). Our findings underscore the value and importance of examining beyond an ML model in examining social and ethical risks, especially when we have minimal information about an ML model.",[],[],"['Shalaleh Rismani', 'Renee Shelby', 'Andrew Smart', 'Renelito Delos Santos', 'AJung Moon', 'Negar Rostamzadeh']","['McGill University, Canada', 'Google Research, USA', 'Google Research, USA', 'Google Research, USA', 'McGill University, Canada', 'Google Research, Canada']","['Canada', 'US', 'US', 'US', 'Canada', 'Canada']"
https://arxiv.org/abs/2211.14946,Security,Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models,"A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model's ability to perform profession classification.",[],[],"['Peter Henderson', 'Eric Mitchell', 'Christopher D. Manning', 'Dan Jurafsky', 'Chelsea Finn']","['Stanford University, USA', 'Stanford University, USA', 'Stanford University, USA', 'Stanford University, USA', 'Stanford University, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2303.12872,Security,Human Uncertainty in Concept-Based AI Systems,"Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks.",[],[],"['Katherine M. Collins', 'Matthew Barker', 'Mateo Espinosa Zarlenga', 'Naveen Raman', 'Umang Bhatt', 'Mateja Jamnik', 'Ilia Sucholutsky', 'Adrian Weller', 'Krishnamurthy Dvijotham']","['University of Cambridge, United Kingdom', 'Department of Engineering, University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'Princeton University, United Kingdom', 'University of Cambridge, United Kingdom', 'Google DeepMind, USA']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'US']"
https://arxiv.org/abs/2207.09506,Security,Protecting Children from Online Exploitation: Can a trained model detect harmful communication strategies?,"The explosion of global social media and online communication platforms has changed how we interact with each other and as a society, bringing with it new security and privacy challenges. Like all technologies, these platforms can be abused and they are routinely used to attempt to cause harm at scale. One of the most significant offence types that is enabled by these platforms is child sexual abuse - both scaling existing abuse and enabling entirely new types of online-only abuse where the impacts on the victim are equally catastrophic. Many platforms invest significantly in combating this crime, referring confirmed evidence of illegality to law enforcement. The introduction of end-to-end encryption and similar technologies breaks many of the mitigations in place today and this has led to a debate around the apparent dichotomy of good child safety and good general user privacy and security. This debate has concentrated on the problem of detecting offenders sharing known abuse imagery using a technique known as client side scanning. We will show that the real problem of online child sexual abuse is much more complex than offender image sharing, providing a new set of 'harm archetypes' to better group harms into categories that have similar technical characteristics and, as far as we are able, bring more clarity to the processes currently used by platforms and law enforcement in relation to child sexual abuse content and the real world impacts. We explore, at a high level, a variety of techniques that could be used as part of any potential solution and examine the benefits and disbenefits that may accrue in various use cases, and use a hypothetical service as an example of how various techniques could be brought together to provide both user privacy and security, while protecting child safety and enabling law enforcement action.",[],[],"['Ian Levy', 'Crispin Robinson']","['Dyson School of Design Engineering, Imperial College London, United Kingdom and Institute of Risk and Uncertainty, University of Liverpool, United Kingdom', 'Engineering, University of Cambridge, United Kingdom', 'University of Liverpool, United Kingdom', 'University of Liverpool, United Kingdom', 'University of Liverpool, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2312.04745,Security,Measures of Disparity and their Efficient Estimation,"In fairness audits, a standard objective is to detect whether a given algorithm performs substantially differently between subgroups. Properly powering the statistical analysis of such audits is crucial for obtaining informative fairness assessments, as it ensures a high probability of detecting unfairness when it exists. However, limited guidance is available on the amount of data necessary for a fairness audit, lacking directly applicable results concerning commonly used fairness metrics. Additionally, the consideration of unequal subgroup sample sizes is also missing. In this tutorial, we address these issues by providing guidance on how to determine the required subgroup sample sizes to maximize the statistical power of hypothesis tests for detecting unfairness. Our findings are applicable to audits of binary classification models and multiple fairness metrics derived as summaries of the confusion matrix. Furthermore, we discuss other aspects of audit study designs that can increase the reliability of audit results.",[],[],"['Harvineet Singh', 'Fan Xia', 'Mi-Ok Kim', 'Romain Pirracchio', 'Rumi Chunara', 'Jean Feng']","['Center for Data Science, New York University, USA', 'School of Global Public Health, New York University, USA']","['US', 'US']"
https://arxiv.org/abs/1911.09005,Security,Robust Artificial Moral Agents and Metanormativity,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",[],[],"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']","['Department of Philosophy, The Ohio State University, USA']",['US']
https://arxiv.org/abs/2309.02528,Security,Adaptive Adversarial Training Does Not Increase Recourse Costs,"Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.",[],[],"['Ian Hardy', 'Jayanth Yetukuri', 'Yang Liu']",[],[]
https://arxiv.org/abs/2304.09991,Security,Designing for Human-AI Collaboration in Auditing LLMs with LLMs,"Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants auditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis formation and testing. Further, with our tool, participants identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown before in formal audits and also those previously under-reported.",[],[],"['Charvi Rastogi', 'Marco Tulio Ribeiro', 'Nicholas King', 'Harsha Nori', 'Saleema Amershi']",[],[]
https://arxiv.org/abs/2305.12622,Security,Evaluating the Impact of Social Determinants on Health Prediction,"Social determinants of health (SDOH) -- the conditions in which people live, grow, and age -- play a crucial role in a person's health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protective attributes. We hope the new integrated EHR-SDOH database will enable studies on the relationship between community health and individual outcomes and provide new benchmarks to study algorithmic biases beyond race, gender, and age.",[],[],"['Ming Ying Yang', 'Gloria Hyunjung Kwak', 'Tom Pollard', 'Leo Anthony Celi', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology, USA', 'Harvard Medical School, USA and Massachusetts General Hospital, USA', 'Massachusetts Institute of Technology, USA', 'Massachusetts Institute of Technology, USA and Beth Israel Deaconess Medical Center, USA', 'Massachusetts Institute of Technology, USA and Vector Institute, Canada']","['US', 'US', 'US', 'Israel', 'Canada']"