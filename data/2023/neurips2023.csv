link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries,,,,,,,
https://nips.cc/virtual/2023/poster/72134,Transparency & Explainability,SHAP-IQ: Unified Approximation of any-order Shapley Interactions,"Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature attributions for any black box model. Shapley interaction indices extend the SV to define any-order feature interactions. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our approach reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP with a greatly simplified calculation. We illustrate the computational efficiency and effectiveness by explaining language, image classification and high-dimensional synthetic models.","['Explainable Artificial Intelligence', 'Feature Interaction', 'Shapley Interaction', 'Shapley Value']",[],"['Fabian Fumagalli', 'Maximilian Muschalik', 'Patrick Kolpaczki', 'Eyke Hüllermeier', 'Barbara Eva Hammer']","['Universität Bielefeld', 'Institute of Computer Science, Ludwig-Maximilians-Universität München', 'Universität Paderborn', 'Ludwig-Maximilians-Universität München', 'Universität Bielefeld']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74159,Transparency & Explainability,Reproducibility Study of ”Label-Free Explainability for Unsupervised Models”,"In this work, we present our reproducibility study of ""Label-Free Explainability for Unsupervised Models"", a paper that introduces two post‐hoc explanation techniques for neural networks: (1) label‐free feature importance and (2) label‐free example importance. Our study focuses on the reproducibility of the authors’ most important claims: (i) perturbing features with the highest importance scores causes higherlatent shift than perturbing random pixels, (ii) label‐free example importance scores help to identify training examples that are highly related to a given test example, (iii) unsupervised models trained on different tasks show moderate correlation among the highest scored features and (iv) low correlation in example scores measured on a fixed set of data points, and (v) increasing the disentanglement with β in a β‐VAE does not imply that latent units will focus on more different features. We reviewed the authors’ code, checked if the implementation of experiments matched with the paper, and also ran all experiments. The results are shown to be reproducible. Moreover, we extended the codebase in order to run the experiments on more datasets, and to test the claims with other experiments.","['Explainable AI', 'Unsupervised Learning', 'Feature Importance', 'Example Importance']",[],"['Julius Wagenbach', 'Gergely Papp', 'Niklas Mather', 'Laurens Jan de Vries']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam, University of Amsterdam']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70181,Transparency & Explainability,Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning,"In this paper, we propose a Disentangled Counterfactual Learning (DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects’ physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. To address these issues, our proposed DCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model’s reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. Our proposed method is a plug-and-play module that can be incorporated into any baseline. In experiments, we show that our proposed method improves baseline methods and achieves state-of-the-art performance. Our source code is available at https://github.com/Andy20178/DCL.",['Physical Audiovisual；Commonsense Reasoning'],[],"['Changsheng Lv', 'Shuai Zhang', 'Yapeng Tian', 'Mengshi Qi', 'Huadong Ma']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'University of Texas at Dallas', 'Beijing University of Posts and Telecommunications', 'Beijing University of Post and Telecommunication, Tsinghua University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73495,Transparency & Explainability,Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union,"Semantic segmentation datasets often exhibit two types of imbalance: \textit{class imbalance}, where some classes appear more frequently than others and \textit{size imbalance}, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards \textit{majority classes} (e.g. overall pixel-wise accuracy) and \textit{large objects} (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.",['Semantic Segmentation'],[],"['Zifu Wang', 'Maxim Berman', 'Amal Rannen-Triki', 'Philip Torr', 'Devis Tuia', 'Tinne Tuytelaars', 'Luc Van Gool', 'Jiaqian Yu', 'Matthew B. Blaschko']","['KU Leuven', 'Google', 'Google DeepMind', 'University of Oxford', 'EPFL - EPF Lausanne', 'KU Leuven', 'INSAIT - Sofia Un.', 'Samsung Research -Beijing', 'KU Leuven']","[None, None, None, None, None, None, None, 'China', None]",,,,,,,
https://nips.cc/virtual/2023/poster/70998,Transparency & Explainability,SmoothHess: ReLU Network Feature Interactions via Stein's Lemma,"Several recent methods for interpretability model feature interactions by looking at the Hessian of a neural network. This poses a challenge for ReLU networks, which are piecewise-linear and thus have a zero Hessian almost everywhere. We propose SmoothHess, a method of estimating second-order interactions through Stein's Lemma. In particular, we estimate the Hessian of the network convolved with a Gaussian through an efficient sampling algorithm, requiring only network gradient calls. SmoothHess is applied post-hoc, requires no modifications to the ReLU network architecture, and the extent of smoothing can be controlled explicitly. We provide a non-asymptotic bound on the sample complexity of our estimation procedure. We validate the superior ability of SmoothHess to capture interactions on benchmark datasets and a real-world medical spirometry dataset.","['Interpretability', 'Feature Interactions', ""Stein's Lemma""]",[],"['Max Torop', 'Aria Masoomi', 'Kivanc Kose', 'Stratis Ioannidis', 'Jennifer Dy']","['Northeastern University', 'Northeastern University', 'Memorial Sloan Kettering Cancer Centre', 'Northeastern University', 'Northeastern University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73478,Transparency & Explainability,FIND: A Function Description Benchmark for Evaluating Interpretability Methods,"Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure—acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, FIND also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.","['Interpretability', 'Explainability', 'Dataset', 'Benchmark', 'LLMs', 'LM Agents', 'Language Model']",[],"['Sarah Schwettmann', 'Tamar Rott Shaham', 'Joanna Materzynska', 'Neil Chowdhury', 'Jacob Andreas', 'David Bau', 'Antonio Torralba']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Northeastern University', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72785,Transparency & Explainability,Pairwise Causality Guided Transformers for Event Sequences,"Although pairwise causal relations have been extensively studied in observational longitudinal analyses across many disciplines, incorporating knowledge of causal pairs into deep learning models for temporal event sequences remains largely unexplored. In this paper, we propose a novel approach for enhancing the performance of transformer-based models in multivariate event sequences by injecting pairwise qualitative causal knowledge such as `event Z amplifies future occurrences of event Y'. We establish a new framework for causal inference in temporal event sequences using a transformer architecture, providing a theoretical justification for our approach, and show how to obtain unbiased estimates of the proposed measure. Experimental results demonstrate that our approach outperforms several state-of-the-art models in terms of prediction accuracy by effectively leveraging knowledge about causal pairs. We also consider a unique application where we extract knowledge around sequences of societal events by generating them from a large language model, and demonstrate how a causal knowledge graph can help with event prediction in such sequences. Overall, our framework offers a practical means of improving the performance of transformer-based models in multivariate event sequences by explicitly exploiting pairwise causal information.","['temporal event sequences', 'causal inference', 'transformer', 'causal knowledge graph']",[],"['Xiao Shou', 'Debarun Bhattacharjya', 'Tian Gao', 'Dharmashankar Subramanian', 'Oktie Hassanzadeh', 'Kristin Bennett']","['IBM, International Business Machines', 'Stanford University', 'International Business Machines', 'International Business Machines', 'International Business Machines', 'Rensselaer Polytechnic Institute']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71571,Transparency & Explainability,Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models,"Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity — i.e., (possibly non-linear) dependencies between the features — has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular data. Our experiments show that concurvity in GAMs can be reduced without significantly compromising prediction quality, improving interpretability and reducing variance in the feature importances.","['Interpretable Machine Learning', 'Generalized Additive Models', 'Concurvity', 'Multicollinearity', 'Regularization', 'Time-Series Forecasting', 'Interpretability']",[],"['Julien Niklas Siems', 'Konstantin Ditschuneit', 'Winfried Ripken', 'Alma Lindborg', 'Maximilian Schambach', 'Johannes Otterbach', 'Martin Genzel']","['University of Freiburg, Albert-Ludwigs-Universität Freiburg', 'Merantix Momentum GmbH', 'Merantix Momentum', 'Merantix Momentum', 'Merantix Momentum', 'nyonic', 'Merantix Momentum']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73882,Transparency & Explainability,Semantic HELM: A Human-Readable Memory for Reinforcement Learning,"Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, existing methods lack interpretability in the sense that it is not comprehensible for humans what the agent stores in its memory. In this regard, we propose a novel memory mechanism that represents past events in human language. Our method uses CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and human-readable representation of the past. We train our memory mechanism on a set of partially observable environments and find that it excels on tasks that require a memory component, while mostly attaining performance on-par with strong baselines on tasks that do not. On a challenging continuous recognition task, where memorizing the past is crucial, our memory mechanism converges two orders of magnitude faster than prior methods. Since our memory mechanism is human-readable, we can peek at an agent's memory and check whether crucial pieces of information have been stored. This significantly enhances troubleshooting and paves the way toward more interpretable agents.","['Reinforcement Learning', 'Language Models', 'History Compression', 'Partial Observability', 'Foundation Models', 'Interpretability', 'Explainable AI']",[],"['Fabian Paischer', 'Thomas Adler', 'Markus Hofmarcher', 'Sepp Hochreiter']","[', Johannes Kepler Universität Linz', 'Johannes Kepler University Linz', 'Johannes Kepler Universität Linz', 'Johannes Kepler University Linz']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72642,Transparency & Explainability,Mitigating the Effect of Incidental Correlations on Part-based Learning,"Intelligent systems possess a crucial characteristic of breaking complicated problems into smaller reusable components or parts and adjusting to new tasks using these part representations. However, current part-learners encounter difficulties in dealing with incidental correlations resulting from the limited observations of objects that may appear only in specific arrangements or with specific backgrounds. These incidental correlations may have a detrimental impact on the generalization and interpretability of learned part representations. This study asserts that part-based representations could be more interpretable and generalize better with limited data, employing two innovative regularization methods. The first regularization separates foreground and background information's generative process via a unique mixture-of-parts formulation. Structural constraints are imposed on the parts using a weakly-supervised loss, guaranteeing that the mixture-of-parts for foreground and background entails soft, object-agnostic masks. The second regularization assumes the form of a distillation loss, ensuring the invariance of the learned parts to the incidental background correlations. Furthermore, we incorporate sparse and orthogonal constraints to facilitate learning high-quality part representations. By reducing the impact of incidental background correlations on the learned parts, we exhibit state-of-the-art (SoTA) performance on few-shot learning tasks on benchmark datasets, including MiniImagenet, TieredImageNet, and FC100. We also demonstrate that the part-based representations acquired through our approach generalize better than existing techniques, even under domain shifts of the background and common data corruption on the ImageNet-9 dataset.","['part-based learning', 'interpretability', 'few-shot learning', 'vision transformers']",[],"['Gaurav Bhatt', 'Deepayan Das', 'Leonid Sigal', 'Vineeth N. Balasubramanian']","[', University of British Columbia', 'International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad', 'University of British Columbia', 'n Institute of Technology Hyderabad']","[None, None, None, 'India']",,,,,,,
https://nips.cc/virtual/2023/poster/73690,Transparency & Explainability,"$\mathcal{M}^4$: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models","While Explainable Artificial Intelligence (XAI) techniques have been widely studied to explain predictions made by deep neural networks, the way to evaluate the faithfulness of explanation results remains challenging, due to the heterogeneity of explanations for various models and the lack of ground-truth explanations. This paper introduces an XAI benchmark named $\mathcal{M}^4$, which allows evaluating various input feature attribution methods using the same set of faithfulness metrics across multiple data modalities (images and texts) and network structures (ResNets, MobileNets, Transformers). A taxonomy for the metrics has been proposed as well. We first categorize commonly used XAI evaluation metrics into three groups based on the ground truth they require. We then implement classic and state-of-the-art feature attribution methods using InterpretDL and conduct extensive experiments to compare methods and gain insights. Extensive experiments have been conducted to provide holistic evaluations as benchmark baselines. Several interesting observations are noticed for designing attribution algorithms. The implementation of state-of-the-art explanation methods and evaluation metrics of $\mathcal{M}^4$ is publicly available at \url{https://github.com/PaddlePaddle/InterpretDL}.","['explanation', 'attribution', 'XAI', 'faithfulness evaluation', 'benchmark']",[],"['Xuhong Li', 'Mengnan Du', 'Jiamin Chen', 'Yekun Chai', 'Himabindu Lakkaraju', 'Haoyi Xiong']","['Baidu', 'New  Institute of Technology', 'Beijing University of Aeronautics and Astronautics', 'Baidu', 'Harvard University', 'Baidu']","[None, 'Jersey', None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72697,Transparency & Explainability,Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis,"How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.","['Computational Neuroscience', 'Neural Data Analysis', 'Statistical Shape Metrics', 'Representational Similarity Analysis', 'Recurrent Neural Networks', 'Dynamical Systems']",[],"['Mitchell Ostrow', 'Adam Joseph Eisen', 'Leo Kozachkov', 'Ila R Fiete']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71905,Transparency & Explainability,Explainable and Efficient Randomized Voting Rules,"With a rapid growth in the deployment of AI tools for making critical decisions (or aiding humans in doing so), there is a growing demand to be able to explain to the stakeholders how these tools arrive at a decision. Consequently, voting is frequently used to make such decisions due to its inherent explainability. Recent work suggests that using randomized (as opposed to deterministic) voting rules can lead to significant efficiency gains measured via the distortion framework. However, rules that use intricate randomization can often become too complex to explain to the stakeholders; losing explainability can eliminate the key advantage of voting over black-box AI tools, which may outweigh the efficiency gains. We study the efficiency gains which can be unlocked by using voting rules that add a simple randomization step to a deterministic rule, thereby retaining explainability. We focus on two such families of rules, randomized positional scoring rules and random committee member rules, and show, theoretically and empirically, that they indeed achieve explainability and efficiency simultaneously to some extent.","['explainability', 'efficiency', 'voting', 'distortion', 'randomized decision-making']",[],"['Soroush Ebadian', 'Aris Filos-Ratsikas', 'Nisarg Shah']","['Department of Computer Science, University of Toronto', 'University of Edinburgh, University of Edinburgh', 'University of Toronto']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70129,Transparency & Explainability,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,"Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\times$ increase in inference throughput and even greater memory savings.","['Transformers', 'Context-pruning', 'Efficient Transformer']",[],"['Sotiris Anagnostidis', 'Dario Pavllo', 'Luca Biggio', 'Lorenzo Noci', 'Aurelien Lucchi', 'Thomas Hofmann']","['ETH Zurich', 'ETHZ - ETH Zurich', 'EPFL - EPF Lausanne', 'ETHZ - ETH Zurich', 'University of Basel', 'Swiss Federal Institute of Technology']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71029,Transparency & Explainability,SwiFT: Swin 4D fMRI Transformer,"Modeling spatiotemporal brain dynamics from high-dimensional data, such as functional Magnetic Resonance Imaging (fMRI), is a formidable task in neuroscience. Existing approaches for fMRI analysis utilize hand-crafted features, but the process of feature extraction risks losing essential information in fMRI scans. To address this challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer architecture that can learn brain dynamics directly from fMRI volumes in a memory and computation-efficient manner. SwiFT achieves this by implementing a 4D window multi-head self-attention mechanism and absolute positional embeddings. We evaluate SwiFT using multiple large-scale resting-state fMRI datasets, including the Human Connectome Project (HCP), Adolescent Brain Cognitive Development (ABCD), and UK Biobank (UKB) datasets, to predict sex, age, and cognitive intelligence. Our experimental outcomes reveal that SwiFT consistently outperforms recent state-of-the-art models. Furthermore, by leveraging its end-to-end learning capability, we show that contrastive loss-based self-supervised pre-training of SwiFT can enhance performance on downstream tasks. Additionally, we employ an explainable AI method to identify the brain regions associated with sex classification. To our knowledge, SwiFT is the first Swin Transformer architecture to process dimensional spatiotemporal brain functional data in an end-to-end fashion. Our work holds substantial potential in facilitating scalable learning of functional brain imaging in neuroscience research by reducing the hurdles associated with applying Transformer models to high-dimensional fMRI.","['fMRI', 'Swin Transformer', '4D', 'neuroscience']",[],"['Peter Yongho Kim', 'Junbeom Kwon', 'Sunghwan Joo', 'Sangyoon Bae', 'Donggyu Lee', 'Yoonho Jung', 'Shinjae Yoo', 'Jiook Cha', 'Taesup Moon']","['Seoul National University', 'Seoul National University', 'Sung Kyun Kwan University', 'Seoul National University', 'Sungkyunkwan University', 'Seoul National University', 'Brookhaven National Lab', 'Seoul National University', 'Seoul National University']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74164,Transparency & Explainability,[Re] Hierarchical Shrinkage: Improving the Accuracy and Interpretability of Tree-Based Methods,"Scope of Reproducibility The paper presents a novel post-hoc regularization technique for tree-based models, called Hierarchical Shrinkage (Agarwal 2022). Our main goal is to confirm the claims that it substantially increases the predictive performance of both decision trees and random forests, that it is faster than other regularization techniques, and that it makes the interpretation of random forests simpler. Methodology In our reproduction, we used the Hierarchical Shrinkage, provided by the authors in the Python package imodels. We also used their function for obtaining pre-cleaned data sets. While the algorithm code and clean datasets were provided we re-implemented the experiments as well as added additional experiments to further test the validity of the claims. The results were tested by applying Hierarchical Shrinkage to different tree models and comparing them to the authors' results. Results We managed to reproduce most of the results the authors get. The method works well and most of the claims are supported. The method does increase the predictive performance of tree-based models most of the time, but not always. When compared to other regularization techniques the Hierarchical Shrinkage outperforms them when used on decision trees, but not when used on random forests. Since the method is applied after learning, it is extremely fast. And it does simplify decision boundaries for random forests, making them easier to interpret. What was easy The use of the official code for Hierarchical Shrinkage was straightforward and used the same function naming convention as other machine learning libraries. The function for acquiring already clean data sets saved a lot of time. What was difficult The authors also provided the code for their experiments in a separate library, but the code did not run out of the box and we had no success reproducing the results with it. The code was inconsistent with the paper methodology. We had the most problems with hyperparameter tuning. The authors did not specify how they tuned the hyperparameters for the used RF regularizers. Communication with original authors We did not contact the authors of the original paper","['rescience c', 'machine learning', 'decision tree', 'random forest', 'post-hoc regularization', 'Python']",[],"['Domen Mohorčič', 'David Ocepek']","['University of Ljubljana', 'University of Ljubljana']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73597,Transparency & Explainability,Ethical Considerations for Responsible Data Curation,"Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.","['human-centric', 'datasets', 'computer vision', 'fairness', 'algorithmic bias', 'robustness', 'responsible AI']",[],"['Jerone Andrews', 'Dora Zhao', 'William Thong', 'Orestis Papakyriakopoulos']","['Sony AI', 'Stanford University', 'Sony AI', 'Sony AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73694,Transparency & Explainability,Into the LAION’s Den: Investigating Hate in Multimodal Datasets,"`Scale the model, scale the data, scale the compute' is the reigning sentiment in the world of generative AI today. While the impact of model scaling has been extensively studied, we are only beginning to scratch the surface of data scaling and its consequences. This is especially of critical importance in the context of vision-language datasets such as LAION. These datasets are continually growing in size and are built based on large-scale internet dumps such as the Common Crawl, which is known to have numerous drawbacks ranging from quality, legality, and content. The datasets then serve as the backbone for large generative models, contributing to the operationalization and perpetuation of harmful societal and historical biases and stereotypes. In this paper, we investigate the effect of scaling datasets on hateful content through a comparative audit of two datasets: LAION-400M and LAION-2B. Our results show that hate content increased by nearly **12%** with dataset scale, measured both qualitatively and quantitatively using a metric that we term as Hate Content Rate (HCR). We also found that filtering dataset contents based on Not Safe For Work (NSFW) values calculated based on images alone does not exclude all the harmful content in alt-text. Instead, we found that trace amounts of hateful, targeted, and aggressive text remain even when carrying out conservative filtering. We end with a reflection and a discussion of the significance of our results for dataset curation and usage in the AI community. Code and the meta-data assets curated in this paper are publicly available at https://github.com/vinayprabhu/hate_scaling. Content warning: This paper contains examples of hateful text that might be disturbing, distressing, and/or offensive.","['multimodal datasets', 'dataset audit', 'hate speech detection', 'toxic content detection']",[],[],[],[],,,,,,,
https://nips.cc/virtual/2023/poster/70858,Transparency & Explainability,Labeling Neural Representations with Inverse Recognition,"Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance. We demonstrate the applicability of INVERT in various scenarios, including the identification of representations affected by spurious correlations, and the interpretation of the hierarchical structure of decision-making within the models.","['Explainable AI', 'Mechanistic Interpretability', 'Machine Learning', 'Deep Neural Networks']",[],"['Kirill Bykov', 'Laura Kopf', 'Shinichi Nakajima', 'Marius Kloft', 'Marina MC Höhne']","['Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB)', 'Universität Potsdam', 'TU Berlin', 'RPTU Kaiserslautern-Landau', 'Universität Potsdam']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69867,Transparency & Explainability,Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization,"Adversarial contrastive learning (ACL) is a technique that enhances standard contrastive learning (SCL) by incorporating adversarial data to learn a robust representation that can withstand adversarial attacks and common corruptions without requiring costly annotations. To improve transferability, the existing work introduced the standard invariant regularization (SIR) to impose style-independence property to SCL, which can exempt the impact of nuisance style factors in the standard representation. However, it is unclear how the style-independence property benefits ACL-learned robust representations. In this paper, we leverage the technique of causal reasoning to interpret the ACL and propose adversarial invariant regularization (AIR) to enforce independence from style factors. We regulate the ACL using both SIR and AIR to output the robust representation. Theoretically, we show that AIR implicitly encourages the representational distance between different views of natural data and their adversarial variants to be independent of style factors. Empirically, our experimental results show that invariant regularization significantly improves the performance of state-of-the-art ACL methods in terms of both standard generalization and robustness on downstream tasks. To the best of our knowledge, we are the first to apply causal reasoning to interpret ACL and develop AIR for enhancing ACL-learned robust representations. Our source code is at https://github.com/GodXuxilie/Enhancing_ACL_via_AIR.","['robust pre-training', 'adversarial contrastive learning']",[],"['Xilie Xu', 'Jingfeng Zhang', 'Feng Liu', 'Masashi Sugiyama', 'Mohan Kankanhalli']","['National University of', 'RIKEN', 'University of Melbourne', 'RIKEN', 'National University of']","['Singapore', None, None, None, 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/70732,Transparency & Explainability,Similarity-based cooperative equilibrium,"As machine learning agents act more autonomously in the world, they will increasingly interact with each other. Unfortunately, in many social dilemmas like the one-shot Prisoner’s Dilemma, standard game theory predicts that ML agents will fail to cooperate with each other. Prior work has shown that one way to enable cooperative outcomes in the one-shot Prisoner’s Dilemma is to make the agents mutually transparent to each other, i.e., to allow them to access one another’s source code (Rubinstein, 1998; Tennenholtz, 2004) – or weights in the case of ML agents. However, full transparency is often unrealistic, whereas partial transparency is commonplace. Moreover, it is challenging for agents to learn their way to cooperation in the full transparency setting. In this paper, we introduce a more realistic setting in which agents only observe a single number indicating how similar they are to each other. We prove that this allows for the same set of cooperative outcomes as the full transparency setting. We also demonstrate experimentally that cooperation can be learned using simple ML methods.","['Program equilibrium', 'multi-agent learning', 'game theory', 'opponent shaping', 'superrationality', 'decision theory', 'cooperative AI', ""Newcomb's problem""]",[],"['Caspar Oesterheld', 'Johannes Treutlein', 'Roger Baker Grosse', 'Vincent Conitzer', 'Jakob Nicolaus Foerster']","['School of Computer Science, Carnegie Mellon University', 'University of California, Berkeley', 'Department of Computer Science, University of Toronto', 'University of Oxford', 'University of Oxford, University of Oxford']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71409,Transparency & Explainability,Orthogonal Non-negative Tensor Factorization based Multi-view Clustering,"Multi-view clustering (MVC) based on non-negative matrix factorization (NMF) and its variants have attracted much attention due to their advantages in clustering interpretability. However, existing NMF-based multi-view clustering methods perform NMF on each view respectively and ignore the impact of between-view. Thus, they can't well exploit the within-view spatial structure and between-view complementary information. To resolve this issue, we present orthogonal non-negative tensor factorization (Orth-NTF) and develop a novel multi-view clustering based on Orth-NTF with one-side orthogonal constraint. Our model directly performs Orth-NTF on the 3rd-order tensor which is composed of anchor graphs of views. Thus, our model directly considers the between-view relationship. Moreover, we use the tensor Schatten $p$-norm regularization as a rank approximation of the 3rd-order tensor which characterizes the cluster structure of multi-view data and exploits the between-view complementary information. In addition, we provide an optimization algorithm for the proposed method and prove mathematically that the algorithm always converges to the stationary KKT point. Extensive experiments on various benchmark datasets indicate that our proposed method is able to achieve satisfactory clustering performance.","['Multi-view clustering', 'tensor Schatten p-norm', 'non-negative matrix factorization.']",[],"['Jing Li', 'Quanxue Gao', 'QIANQIAN WANG', 'Ming Yang', 'Wei Xia']","['Xidian University', 'Xidian University', 'Xidian University', 'University of Evansville', 'Xidian University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71530,Transparency & Explainability,Topological RANSAC for instance verification and retrieval without fine-tuning,"This paper presents an innovative approach to enhancing explainable image retrieval, particularly in situations where a fine-tuning set is unavailable. The widely-used SPatial verification (SP) method, despite its efficacy, relies on a spatial model and the hypothesis-testing strategy for instance recognition, leading to inherent limitations, including the assumption of planar structures and neglect of topological relations among features. To address these shortcomings, we introduce a pioneering technique that replaces the spatial model with a topological one within the RANSAC process. We propose bio-inspired saccade and fovea functions to verify the topological consistency among features, effectively circumventing the issues associated with SP's spatial model. Our experimental results demonstrate that our method significantly outperforms SP, achieving state-of-the-art performance in non-fine-tuning retrieval. Furthermore, our approach can enhance performance when used in conjunction with fine-tuned features. Importantly, our method retains high explainability and is lightweight, offering a practical and adaptable solution for a variety of real-world applications.","['Landmarks retrieval', 'non-fine-tuning', 'spatial verification', 'explainable AI', 'hypothesis and test']",[],"['Guoyuan An', 'Ju-hyeong Seon', 'Inkyu An', 'Yuchi Huo', 'Sung-eui Yoon']","['KAIST', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'Zhejiang University', 'KAIST']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70432,Transparency & Explainability,How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model,"Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as ""The war lasted from the year 1732 to the year 17"", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we find related tasks that activate our circuit. Our results suggest that GPT-2 small computes greater-than using a complex but general mechanism that activates across diverse contexts.","['interpretability', 'language models', 'NLP']",[],"['Michael Hanna', 'Ollie Liu', 'Alexandre Variengien']","['University of Amsterdam', 'University of Southern California', 'Conjecture']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71179,Transparency & Explainability,Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement,"Humans use abstract *concepts* for understanding instead of hard features. Recent interpretability research has focused on human-centered concept explanations of neural networks. Concept Activation Vectors (CAVs) estimate a model's sensitivity and possible biases to a given concept. We extend CAVs from post-hoc analysis to ante-hoc training to reduce model bias through fine-tuning using an additional *Concept Loss*. Concepts are defined on the final layer of the network in the past. We generalize it to intermediate layers, including the last convolution layer. We also introduce *Concept Distillation*, a method to define rich and effective concepts using a pre-trained knowledgeable model as the teacher. Our method can sensitize or desensitize a model towards concepts. We show applications of concept-sensitive training to debias several classification problems. We also show a way to induce prior knowledge into a reconstruction problem. We show that concept-sensitive training can improve model interpretability, reduce biases, and induce prior knowledge.","['Human Centered Concepts', 'ML interpretability', 'XAI based Model Improvement', 'Debiasing']",[],"['Avani Gupta', 'Saurabh Saini', 'P J Narayanan']","['G42', 'International Institute of Information Technology Hyderabad', 'International Institute of Information Technology Hyderabad']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73533,Transparency & Explainability,HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models,"Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as output transcription. The proposed benchmark contains a novel dataset, ""HyPoradise"" (HP), encompassing more than 316,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt design can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new paradigm for ASR error correction with LLMs.","['Automatic speech recognition', 'language model rescoring', 'N-best Hypothesis list.']",[],"['CHEN CHEN', 'Yuchen Hu', 'Chao-Han Huck Yang', 'Sabato Marco Siniscalchi', 'Pin-Yu Chen', 'Ensiong Chng']","['Nanyang Technological University', 'Nanyang Technological University', 'NVIDIA Research', 'Norwegian Institute of Technology', 'International Business Machines', 'Nanyang Technological University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71665,Transparency & Explainability,Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning,"Human reasoning can be understood as a cooperation between the intuitive, associative ""System-1'' and the deliberative, logical ""System-2''. For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities. Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted and checked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation. Our method shows superiority in extensive activity understanding tasks. Code and data are available at https://mvig-rhos.com/symbol_llm.","['neuro-symbolic', 'visual reasoning', 'human activity understanding']",[],"['Xiaoqian Wu', 'Yong-Lu Li', 'Jianhua Sun', 'Cewu Lu']","['Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71105,Transparency & Explainability,Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture,"Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE—showing for the first time that it may be possible to match Transformer quality without attention or MLPs.","['structured matrices', 'transformers', 'efficiency']",[],"['Daniel Y Fu', 'Simran Arora', 'Jessica Grogan', 'Isys Johnson', 'Sabri Eyuboglu', 'Armin W Thomas', 'Benjamin Frederick Spector', 'Michael Poli', 'Atri Rudra']","['Stanford University', 'Stanford University', 'State University of New York at Buffalo', 'State University of New York, Buffalo', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'State University of New York, Buffalo']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69885,Transparency & Explainability,Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction,"Many security applications require unsupervised anomaly detection, as malicious data are extremely rare and often only unlabeled normal data are available for training (i.e., zero-positive). However, security operators are concerned about the high stakes of trusting black-box models due to their lack of interpretability. In this paper, we propose a post-hoc method to globally explain a black-box unsupervised anomaly detection model via rule extraction. First, we propose the concept of distribution decomposition rules that decompose the complex distribution of normal data into multiple compositional distributions. To find such rules, we design an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the boundary inference rules that estimate the decision boundary of the original model on each compositional distribution. By merging these two types of rules into a rule set, we can present the inferential process of the unsupervised black-box model in a human-understandable way, and build a surrogate rule-based model for online deployment at the same time. We conduct comprehensive experiments on the explanation of four distinct unsupervised anomaly detection models on various real-world datasets. The evaluation shows that our method outperforms existing methods in terms of diverse metrics including fidelity, correctness and robustness.","['unsupervised anomaly detection', 'global explanation', 'rule extraction']",[],"['Ruoyu Li', 'Qing Li', 'Yu Zhang', 'Dan Zhao', 'Yong Jiang']","['Tsinghua University, Tsinghua University', 'Peng Cheng Laboratory', 'Tsinghua University, Tsinghua University', 'Peng Cheng Laborotary', 'Tsinghua University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71753,Transparency & Explainability,Learning Transformer Programs,"Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the “circuits” used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.","['mechanistic interpretability', 'transformers']",[],"['Dan Friedman', 'Alexander Wettig', 'Danqi Chen']","['Princeton University', 'Princeton University', 'Department of Computer Science, Princeton University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72030,Transparency & Explainability,Invariant Learning via Probability of Sufficient and Necessary Causes,"Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of sufficiency and necessity conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose PNS risk and formulate an algorithm to learn representation with a high PNS value. We theoretically analyze and prove the generalizability of the PNS risk. Experiments on both synthetic and real-world benchmarks demonstrate the effectiveness of the proposed method. The detailed implementation can be found at the GitHub repository: https://github.com/ymy4323460/CaSN.","['OOD Generalization', 'Invariant Representation Learning']",[],"['Mengyue Yang', 'Zhen Fang', 'Yonggang Zhang', 'Furui Liu', 'Jean-Francois Ton', 'Jianhong Wang', 'Jun Wang']","['University College London', 'University of Technology Sydney', 'Baptist University', 'Zhejiang Lab & UCAS & Zhejiang University', 'Bytedance', 'University of Manchester', 'University College London']","[None, None, 'Hong Kong', None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70965,Transparency & Explainability,TRIAGE: Characterizing and auditing training data for improved regression,"Data quality is crucial for robust machine learning algorithms, with the recent interest in data-centric AI emphasizing the importance of training data characterization. However, current data characterization methods are largely focused on classification settings, with regression settings largely understudied. To address this, we introduce TRIAGE, a novel data characterization framework tailored to regression tasks and compatible with a broad class of regressors. TRIAGE utilizes conformal predictive distributions to provide a model-agnostic scoring method, the TRIAGE score. We operationalize the score to analyze individual samples' training dynamics and characterize samples as under-, over-, or well-estimated by the model. We show that TRIAGE's characterization is consistent and highlight its utility to improve performance via data sculpting/filtering, in multiple regression settings. Additionally, beyond sample level, we show TRIAGE enables new approaches to dataset selection and feature acquisition. Overall, TRIAGE highlights the value unlocked by data characterization in real-world regression applications.","['data-centric AI', 'data characterization', 'data quality']",[],"['Nabeel Seedat', 'Jonathan Crabbé', 'Zhaozhi Qian', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73475,Transparency & Explainability,REASONER: An Explainable Recommendation Dataset with Comprehensive Labeling Ground Truths,"Explainable recommendation has attracted much attention from the industry and academic communities. It has shown great potential to improve the recommendation persuasiveness, informativeness and user satisfaction. In the past few years, while a lot of promising explainable recommender models have been proposed, the datasets used to evaluate them still suffer from several limitations, for example, the explanation ground truths are not labeled by the real users, the explanations are mostly single-modal and around only one aspect. To bridge these gaps, in this paper, we build a new explainable recommendation dataset, which, to our knowledge, is the first contribution that provides a large amount of real user labeled multi-modal and multi-aspect explaination ground truths. In specific, we firstly develop a video recommendation platform, where a series of questions around the recommendation explainability are carefully designed. Then, we recruit about 3000 high-quality labelers with different backgrounds to use the system, and collect their behaviors and feedback to our questions. In this paper, we detail the construction process of our dataset and also provide extensive analysis on its characteristics. In addition, we develop a library, where ten well-known explainable recommender models are implemented in a unified framework. Based on this library, we build several benchmarks for different explainable recommendation tasks. At last, we present many new opportunities brought by our dataset, which are expected to promote the field of explainable recommendation. Our dataset, library and the related documents have been released at https://reasoner2023.github.io/.","['Explainable Recommendation', 'Recommendation Dataset', 'Labeling Ground Truths']",[],"['Xu Chen', 'Jingsen Zhang', 'Lei Wang', 'Quanyu Dai', 'Zhenhua Dong', 'Ruiming Tang', 'Xin Zhao', 'Ji-Rong Wen']","['Renmin University of', 'Renmin University of', 'Renmin University of', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Renmin University of', 'Renmin University of']","['China', 'China', 'China', None, None, None, 'China', 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/72388,Transparency & Explainability,Optimal Transport for Treatment Effect Estimation,"Estimating individual treatment effects from observational data is challenging due to treatment selection bias. Prevalent methods mainly mitigate this issue by aligning different treatment groups in the latent space, the core of which is the calculation of distribution discrepancy. However, two issues that are often overlooked can render these methods invalid: (1) mini-batch sampling effects (MSE), where the calculated discrepancy is erroneous in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), where the unobserved confounders are not considered in the discrepancy calculation. Both of these issues invalidate the calculated discrepancy, mislead the training of estimators, and thus impede the handling of treatment selection bias. To tackle these issues, we propose Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport technology in the context of causality. Specifically, based on the canonical optimal transport framework, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue. Extensive experiments demonstrate that ESCFR estimates distribution discrepancy accurately, handles the treatment selection bias effectively, and outperforms prevalent competitors significantly.","['treatment effect estimation', 'optimal transport', 'wasserstein', 'causal inference', 'counterfactual']",[],"['Hao Wang', 'Jiajun Fan', 'Zhichao Chen', 'Haoxuan Li', 'Weiming Liu', 'Tianqiao Liu', 'Quanyu Dai', 'Yichao Wang', 'Zhenhua Dong', 'Ruiming Tang']","['N/A', 'Tsinghua University, Tsinghua University', 'Zhejiang University', 'Pohang University of Science and Technology', 'Zhejiang University', 'Purdue University', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70046,Transparency & Explainability,Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models,"Pretrained vision-language models, such as CLIP, have demonstrated strong generalization capabilities, making them promising tools in the realm of zero-shot visual recognition. Visual relation detection (VRD) is a typical task that identifies relationship (or interaction) types between object pairs within an image. However, naively utilizing CLIP with prevalent class-based prompts for zero-shot VRD has several weaknesses, e.g., it struggles to distinguish between different fine-grained relation types and it neglects essential spatial information of two objects. To this end, we propose a novel method for zero-shot VRD: RECODE, which solves RElation detection via COmposite DEscription prompts. Specifically, RECODE first decomposes each predicate category into subject, object, and spatial components. Then, it leverages large language models (LLMs) to generate description-based prompts (or visual cues) for each component. Different visual cues enhance the discriminability of similar relation categories from different perspectives, which significantly boosts performance in VRD. To dynamically fuse different cues, we further introduce a chain-of-thought method that prompts LLMs to generate reasonable weights for different visual cues. Extensive experiments on four VRD benchmarks have demonstrated the effectiveness and interpretability of RECODE.","['Visual relation detection', 'Zero-short learning', 'Scene graph generation']",[],"['Lin Li', 'Jun Xiao', 'Guikun Chen', 'Jian Shao', 'Yueting Zhuang', 'Long Chen']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'University of Science and Technology']","[None, None, None, None, None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/72868,Transparency & Explainability,Learning to Receive Help: Intervention-Aware Concept Embedding Models,"Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly outperform state-of-the-art concept-interpretable models when provided with test-time concept interventions, demonstrating the effectiveness of our approach.","['Explainable Artificial Intelligence', 'Concept Bottleneck Models', 'Concept-based Explainability', 'Interpretability', 'XAI', 'Concept Interventions']",[],"['Mateo Espinosa Zarlenga', 'Katherine M. Collins', 'Krishnamurthy Dj Dvijotham', 'Adrian Weller', 'Mateja Jamnik']","['University of Cambridge', 'University of Cambridge', 'Google DeepMind', 'Alan Turing Institute', 'University of Cambridge']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71620,Transparency & Explainability,Graph Convolutional Kernel Machine versus Graph Convolutional Networks,"Graph convolutional networks (GCN) with one or two hidden layers have been widely used in handling graph data that are prevalent in various disciplines. Many studies showed that the gain of making GCNs deeper is tiny or even negative. This implies that the complexity of graph data is often limited and shallow models are often sufficient to extract expressive features for various tasks such as node classification. Therefore, in this work, we present a framework called graph convolutional kernel machine (GCKM) for graph-based machine learning. GCKMs are built upon kernel functions integrated with graph convolution. An example is the graph convolutional kernel support vector machine (GCKSVM) for node classification, for which we analyze the generalization error bound and discuss the impact of the graph structure. Compared to GCNs, GCKMs require much less effort in architecture design, hyperparameter tuning, and optimization. More importantly, GCKMs are guaranteed to obtain globally optimal solutions and have strong generalization ability and high interpretability. GCKMs are composable, can be extended to large-scale data, and are applicable to various tasks (e.g., node or graph classification, clustering, feature extraction, dimensionality reduction). The numerical results on benchmark datasets show that, besides the aforementioned advantages, GCKMs have at least competitive accuracy compared to GCNs.","['graph neural network', 'kernel method']",[],"['Zhihao Wu', 'Zhao Zhang']","['The Chinese University of , Shenzhen', 'Hefei University of Technology']","['Hong Kong', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71164,Transparency & Explainability,A case for reframing automated medical image classification as segmentation,"Image classification and segmentation are common applications of deep learning to radiology. While many tasks can be framed using either classification or segmentation, classification has historically been cheaper to label and more widely used. However, recent work has drastically reduced the cost of training segmentation networks. In light of this recent work, we reexamine the choice of training classification vs. segmentation models. First, we use an information theoretic approach to analyze why segmentation vs. classification models may achieve different performance on the same dataset and overarching task. We then implement multiple methods for using segmentation models to classify medical images, which we call *segmentation-for-classification*, and compare these methods against traditional classification on three retrospective datasets. We use our analysis and experiments to summarize the benefits of switching from segmentation to classification, including: improved sample efficiency, enabling improved performance with fewer labeled images (up to an order of magnitude lower), on low-prevalence classes, and on certain rare subgroups (up to 161.1\% improved recall); improved robustness to spurious correlations (up to 44.8\% improved robust AUROC); and improved model interpretability, evaluation, and error analysis.","['Medical imaging', 'segmentation', 'classification']",[],"['Mayee F Chen', 'Khaled Kamal Saab', 'Kush Bhatia', 'Curtis Langlotz']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71107,Transparency & Explainability,Explainable Brain Age Prediction using coVariance Neural Networks,"In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of ""brain age"" for an individual. Importantly, the discordance between brain age and chronological age (referred to as ""brain age gap"") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer’s disease (AD) and we make two important observations: (i) VNNs can assign anatomical interpretability to elevated brain age gap in AD by identifying contributing brain regions, (ii) the interpretability offered by VNNs is contingent on their ability to exploit specific eigenvectors of the anatomical covariance matrix. Together, these observations facilitate an explainable and anatomically interpretable perspective to the task of brain age prediction.","['graph neural networks', 'brain age', ""Alzheimer's disease"", 'interpretability', 'explainability', 'computational neuroscience']",[],"['Saurabh Sihag', 'Gonzalo Mateos', 'Corey McMillan', 'Alejandro Ribeiro']","['University of Pennsylvania', 'University of Rochester', 'University of Pennsylvania', 'University of Pennsylvania']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71808,Transparency & Explainability,Red Teaming Deep Neural Networks with Feature Synthesis Tools,"Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods that do not depend on a dataset. In this paper, we benchmark the usefulness of interpretability tools for model debugging. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when a user's interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation.","['interpretability', 'benchmarking', 'auditing', 'diagnostics', 'debugging', 'adversarial attacks', 'feature synthesis']",[],"['Stephen Casper', 'Tong Bu', 'Yuxiao Li', 'Jiawei Li', 'Kevin Zhang', 'Kaivalya Hariharan', 'Dylan Hadfield-Menell']","['Massachusetts Institute of Technology', 'Peking University', 'Department of Electronic Engineering, Tsinghua University', 'INSC, Tsinghua University', 'Peking University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70197,Transparency & Explainability,Tracr: Compiled Transformers as a Laboratory for Interpretability,"We show how to ""compile"" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study ""superposition"" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as _ground-truth_ for evaluating interpretability methods. Commonly, because the ""programs"" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr.","['interpretability', 'transformers', 'language models', 'RASP', 'Tracr', 'mechanistic interpretability']",[],"['David Lindner', 'Janos Kramar', 'Sebastian Farquhar', 'Matthew Rahtz', 'Thomas McGrath', 'Vladimir Mikulik']","['ETH Zurich', 'DeepMind', 'DeepMind', 'DeepMind', 'Google', 'DeepMind']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70760,Transparency & Explainability,A polar prediction model for learning to represent visual transformations,"All organisms make temporal predictions, and their evolutionary fitness level depends on the accuracy of these predictions. In the context of visual perception, the motions of both the observer and objects in the scene structure the dynamics of sensory signals, allowing for partial prediction of future signals based on past ones. Here, we propose a self-supervised representation-learning framework that extracts and exploits the regularities of natural videos to compute accurate predictions. We motivate the polar architecture by appealing to the Fourier shift theorem and its group-theoretic generalization, and we optimize its parameters on next-frame prediction. Through controlled experiments, we demonstrate that this approach can discover the representation of simple transformation groups acting in data. When trained on natural video datasets, our framework achieves better prediction performance than traditional motion compensation and rivals conventional deep networks, while maintaining interpretability and speed. Furthermore, the polar computations can be restructured into components resembling normalized simple and direction-selective complex cell models of primate V1 neurons. Thus, polar prediction offers a principled framework for understanding how the visual system represents sensory inputs in a form that simplifies temporal prediction.","['video prediction', 'neural coding', 'symmetry discovery', 'self-supervised representation-learning']",[],"['Pierre-Étienne H Fiquet', 'Eero P Simoncelli']","['New York University', 'New York University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70983,Transparency & Explainability,CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models,"The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating _commonsense_ causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined _formal rules_. To address this, we propose a new NLP task, _causal inference in natural language_, inspired by the _""causal inference engine""_ postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.","['Large Language Models', 'Causal Reasoning', 'Causal Inference', 'Benchmark Dataset', 'Natural Language Processing']",[],"['Zhijing Jin', 'Yuen Chen', 'Felix Leeb', 'Luigi Gresele', 'Ojasv Kamal', 'Zhiheng LYU', 'Kevin Blin', 'Max Kleiman-Weiner', 'Mrinmaya Sachan']","['Max-Planck Institute', 'University of Illinois at Urbana-Champaign ', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'n Institute of Technology Kharagpur', 'University of', 'ETHZ - ETH Zurich', 'Harvard University', 'Swiss Federal Institute of Technology']","[None, None, None, None, 'India', 'Hong Kong', None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71761,Transparency & Explainability,"Feature Learning for Interpretable, Performant Decision Trees","Decision trees are regarded for high interpretability arising from their hierarchical partitioning structure built on simple decision rules. However, in practice, this is not realized because axis-aligned partitioning of realistic data results in deep trees, and because ensemble methods are used to mitigate overfitting. Even then, model complexity and performance remain sensitive to transformation of the input, and extensive expert crafting of features from the raw data is common. We propose the first system to alternate sparse feature learning with differentiable decision tree construction to produce small, interpretable trees with good performance. We benchmark against conventional tree-based models and demonstrate several notions of interpretation of a model and its predictions.","['explainability', 'interpretability', 'decision tree', 'feature learning']",[],"['Jack Henry Good', 'Torin Kovach', 'Kyle Miller', 'Artur Dubrawski']","['Carnegie Mellon University', 'Alloy Therapeutics', 'CMU, Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71812,Transparency & Explainability,Scale Alone Does not Improve Mechanistic Interpretability in Vision Models,"In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We use a psychophysical paradigm to quantify one form of mechanistic interpretability for a diverse suite of nine models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 130'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset facilitates research on automated instead of human-based interpretability evaluations, which can ultimately be leveraged to directly optimize the mechanistic interpretability of models.","['feature visualization', 'interpretability', 'explainability', 'deep learning', 'neural networks', 'analysis', 'activation maximization', 'psychophysics']",[],"['Roland S. Zimmermann', 'Wieland Brendel']","['Eberhard-Karls-Universität Tübingen', 'ELLIS Institute Tübingen']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72666,Transparency & Explainability,Towards Automated Circuit Discovery for Mechanistic Interpretability,"Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: finding the connections between the abstract neural network units that form a circuit. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery","['Mechanistic Interpretability', 'Pruning', 'Science of Deep Learning', 'AI Safety']",[],"['Arthur Conmy', 'Augustine N. Mavor-Parker', 'Aengus Lynch', 'Adrià Garriga-Alonso']","['Google DeepMind', 'University College London', 'University College London, University of London', 'FAR']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70207,Transparency & Explainability,Emergent Communication in Interactive Sketch Question Answering,"Vision-based emergent communication (EC) aims to learn to communicate through sketches and demystify the evolution of human communication. Ironically, previous works neglect multi-round interaction, which is indispensable in human communication. To fill this gap, we first introduce a novel Interactive Sketch Question Answering (ISQA) task, where two collaborative players are interacting through sketches to answer a question about an image. To accomplish this task, we design a new and efficient interactive EC system, which can achieve an effective balance among three evaluation factors, including the question answering accuracy, drawing complexity and human interpretability. Our experimental results demonstrate that multi-round interactive mechanism facilitates tar- geted and efficient communication between intelligent agents. The code will be released.","['Emergent communication', 'Interactive', 'Question Answering']",[],"['Zixing Lei', 'Yiming Zhang', 'Yuxin Xiong', 'Siheng Chen']","['Shanghai Jiaotong University', 'Cornell University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72988,Transparency & Explainability,Topological Obstructions and How to Avoid Them,"Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder.","['representation learning', 'variational autoencoders', 'homeomorphism', 'topological', 'equivariant', 'lie groups', 'normalizing flows']",[],"['Babak Esmaeili', 'Robin Walters', 'Heiko Zimmermann', 'Jan-Willem van de Meent']","['Eindhoven University of Technology', 'Northeastern University ', 'University of Amsterdam', 'Northeastern University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71118,Transparency & Explainability,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs—e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always ""(A)""—which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.","['Natural language processing', 'large language models', 'XAI', 'explainability']",[],"['Miles Turpin', 'Julian Michael', 'Ethan Perez']","['New York University', 'New York University', 'New York University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70663,Transparency & Explainability,L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference,"Many recent works in simulation-based inference  (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation  over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce $\ell$-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret -- e.g. graphical -- diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, $\ell$-C2ST can be specialized to offer better statistical power, while being computationally more efficient. On standard SBI benchmarks, $\ell$-C2ST  provides comparable results to C2ST and outperforms alternative local approaches such as coverage tests based on highest predictive density (HPD). We further highlight the importance of local evaluation and the benefit of interpretability of $\ell$-C2ST on a challenging application from computational neuroscience.","['machine learning', 'calibration', 'simulation-based inference', 'neuroscience', 'normalizing flows', 'classifier two-sample tests']",[],"['Julia Linhart', 'Alexandre Gramfort', 'Pedro L. C. Rodrigues']","['INRIA', 'Meta', 'Inria']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70206,Transparency & Explainability,Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts,"Neuro-Symbolic (NeSy) predictive models hold the promise of improved compliance with given constraints, systematic generalization, and interpretability, as they allow to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. It was recently shown that NeSy predictors are affected by *reasoning shortcuts*: they can attain high accuracy but by leveraging concepts with \textit{unintended semantics}, thus coming short of their promised advantages. Yet, a systematic characterization of reasoning shortcuts and of potential mitigation strategies is missing. This work fills this gap by characterizing them as unintended optima of the learning objective and identifying four key conditions behind their occurrence. Based on this, we derive several natural mitigation strategies, and analyze their efficacy both theoretically and empirically. Our analysis shows reasoning shortcuts are difficult to deal with, casting doubts on the trustworthiness and interpretability of existing NeSy solutions.","['Neuro-Symbolic Integration', 'Trustworthy AI', 'Concept Learning', 'Learning Shortcuts', 'Mitigation Strategies']",[],"['Emanuele Marconato', 'Stefano Teso', 'Antonio Vergari', 'Andrea Passerini']","['University of Pisa', 'University of Trento', 'University of Edinburgh', 'University of Trento']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71885,Transparency & Explainability,A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation,"In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual ``concepts'' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that recast the first step -- concept extraction problem -- as a special case of **dictionary learning**, and we formalize the second step -- concept importance estimation -- as a more general form of **attribution method**. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii)  to derive theoretical guarantees regarding the optimality of such methods. We further leverage our framework to try to tackle a crucial question in explainability: how to *efficiently* identify clusters of data points that are classified based on a similar shared strategy. To illustrate these findings and to highlight the main strategies of a model, we introduce a visual representation called the strategic cluster graph. Finally, we present Lens, a dedicated website that offers a complete compilation of these visualizations for all classes of the ImageNet dataset.","['Explainable AI', 'Concept-based explainability', 'Interpretability', 'Concept extraction', 'Concept importance', 'Attribution methods']",[],"['Thomas FEL', 'Victor Boutin', 'Louis Béthune', 'Remi Cadene', 'Mazda Moayeri', 'Léo Andéol', 'Mathieu Chalvidal', 'Thomas Serre']","['Brown University', 'Brown University', 'Université Paul Sabatier', 'LIP6', 'University of Maryland, College Park', 'Institut de Mathématique de Toulouse', 'Brown University', 'Brown University']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72331,Transparency & Explainability,Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution,"Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability. To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features. We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare. Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available. Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively.","['Interpretability', 'Attribution Maps', 'Information Bottleneck', 'Multi-Modal Learning', 'Vision-Language Pretrained Models']",[],"['Ying Wang', 'Tim G. J. Rudner', 'Andrew Gordon Wilson']","['New York University', 'New York University', 'Cornell University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70504,Transparency & Explainability,Interpretability at Scale: Identifying Causal Mechanisms in Alpaca,"Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.",['Mechanistic Interpretability'],[],"['Zhengxuan Wu', 'Atticus Geiger', 'Thomas Icard', 'Christopher Potts', 'Noah Goodman']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72292,Transparency & Explainability,Transfer learning for atomistic simulations using GNNs and kernel mean embeddings,"Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, accurate models require large training datasets, while generating reference calculations is computationally demanding. To bypass this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) to represent chemical environments together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by incorporating into the kernel the chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression alone, as well as similar fine-tuning approaches.","['GNN', 'Mean Embedding', 'Kernels', 'Atomistic Simulations', 'OCP', 'Transfer Learning', 'Molecular Dynamics', 'Kernel Ridge Regression', 'Neural Networks']",[],"['John Isak Texas Falk', 'Luigi Bonati', 'Pietro Novelli', 'Michele Parrinello', 'massimiliano pontil']","['University College London', 'Istituto Italiano di Tecnologia,', 'Istituto Italiano di Tecnologia', 'Istituto Italiano di Tecnologia, Genova', 'Istituto Italiano di Tecnologia']","[None, 'Italy', None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70984,Transparency & Explainability,Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors,"As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. To do so, one naturally requires access to sensitive attributes, such as demographics, biological sex, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable. In this work we study the well known equalized odds (EOD) definition of fairness. In a setting without sensitive attributes, we first provide tight and computable upper bounds for the EOD violation of a predictor. These bounds precisely reflect the worst possible EOD violation. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes is -- and when is not -- optimal when it comes to controlling worst-case EOD. Our results hold under assumptions that are milder than previous works, and we illustrate these results with experiments on synthetic and real datasets.","['fairness', 'sensitive attributes', 'equalized odds', 'missing data', 'proxies']",[],"['Beepul Bharti', 'Paul Yi', 'Jeremias Sulam']","['Johns Hopkins University', 'University of Maryland, Baltimore', 'Johns Hopkins University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72517,Transparency & Explainability,Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model,"Counterfactual inference aims to answer retrospective ""what if"" questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual identification methods are special cases of our Curvature Sensitivity Model when the bound of the curvature is set to zero. We then propose an implementation of our Curvature Sensitivity Model in the form of a novel deep generative model, which we call Augmented Pseudo-Invertible Decoder. Our implementation employs (i) residual normalizing flows with (ii) variational augmentations. We empirically demonstrate the effectiveness of our Augmented Pseudo-Invertible Decoder. To the best of our knowledge, ours is the first partial identification model for Markovian structural causal models with continuous outcomes.","['causal inference', 'counterfactual inference', 'partial identification', 'sensitivity model', 'normalizing flows', 'causal machine learning']",[],"['Valentyn Melnychuk', 'Dennis Frauen', 'Stefan Feuerriegel']","['Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', 'LMU Munich']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72241,Transparency & Explainability,Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives,"Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives. While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives. Such representations are interpretable, easy to manipulate and suited for physics-based simulations. Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering. Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss. We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives. We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions. We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio. We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations. Code and video results are available at https://www.tmonnier.com/DBW.","['3D decomposition', '3D reconstruction', 'MVS', 'primitives', 'qualitative 3D']",[],"['Tom Monnier', 'Jake Austin', 'Angjoo Kanazawa', 'Alexei A Efros', 'Mathieu Aubry']","['Facebook', 'UC Berkeley, University of California, Berkeley', 'University of California, Berkeley', 'University of California Berkeley', 'ENPC']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71468,Transparency & Explainability,Auditing for Human Expertise,"High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (‘features’). A rejection of our test thus suggests that human experts may add value to any algorithm trained on the available data, and has direct implications for whether human-AI ‘complementarity’ is achievable in a given prediction task. We highlight the utility of our procedure using admissions data collected from the emergency department of a large academic hospital system, where we show that physicians’ admit/discharge decisions for patients with acute gastrointestinal bleeding (AGIB) appear to be incorporating information that is not available to a standard algorithmic screening tool. This is despite the fact that the screening tool is arguably more accurate than physicians’ discretionary decisions, highlighting that – even absent normative concerns about accountability or interpretability – accuracy is insufficient to justify algorithmic automation.","['hypothesis testing', 'human-AI complementarity', 'machine learning for healthcare']",[],"['Rohan Alur', 'Loren Laine', 'Darrick K Li', 'Manish Raghavan', 'Devavrat Shah', 'Dennis Shung']","['Massachusetts Institute of Technology', 'Yale University', 'Yale School of Medicine', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Yale University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73557,Transparency & Explainability,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,"We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.","['CommonCrawl', 'Low Resource Languages', 'Underrepresented Languages', 'Multilinguality', 'LLMs', 'Large Language Models', 'Machine Translation']",[],"['Sneha Kudugunta', 'Isaac Rayburn Caswell', 'Biao Zhang', 'Xavier Garcia', 'Derrick Xin', 'Aditya Kusupati', 'Ankur Bapna', 'Orhan Firat']","['Department of Computer Science', 'Google', 'Google Research', 'Google', 'Research, Google', 'Google Research', 'Google', 'Google']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71547,Transparency & Explainability,BERT Lost Patience Won't Be Robust to Adversarial Slowdown,"In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively. This result suggests that future work is needed for developing efficient yet robust multi-exit models. Our code is available at: https://github.com/ztcoalson/WAFFLE",['Efficient Methods for NLP; Multi-exit Language Models; Adversarial Slowdown'],[],"['Zachary Coalson', 'Rakesh B Bobba', 'Sanghyun Hong']","['Oregon State University', 'Oregon State University', 'Oregon State University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71213,Transparency & Explainability,Thought Cloning: Learning to Think while Acting by Imitating Human Thinking,"Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to $\textit{think like humans do}$. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, $\textit{but also the thoughts humans have as they perform these behaviors}$. While we expect Thought Cloning to truly shine at scale on internet-sized datasets (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent’s thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents $\textit{how to think}$ as well as behave, Thought Cloning creates safer, more powerful agents.","['Reinforcement learning', 'Imitation Learning', 'AI Safety', 'Interpretability']",[],['Shengran Hu'],['University of British Columbia'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/73519,Transparency & Explainability,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","Large language models have emerged as a promising approach towards achieving general-purpose AI agents. The thriving open-source LLM community has greatly accelerated the development of agents that support human-machine dialogue interaction through natural language processing. However, human interaction with the world extends beyond only text as a modality, and other modalities such as vision are also crucial. Recent works on multi-modal large language models, such as GPT-4V and Bard, have demonstrated their effectiveness in handling visual modalities. However, the transparency of these works is limited and insufficient to support academic research. To the best of our knowledge, we present one of the very first open-source endeavors in the field, LAMM, encompassing a Language-Assisted Multi-Modal instruction tuning dataset, framework, and benchmark. Our aim is to establish LAMM as a growing ecosystem for training and evaluating MLLMs, with a specific focus on facilitating AI agents capable of bridging the gap between ideas and execution, thereby enabling seamless human-AI interaction. Our main contribution is three-fold: 1) We present a comprehensive dataset and benchmark, which cover a wide range of vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We outline the detailed methodology of constructing multi-modal instruction tuning datasets and benchmarks for MLLMs, enabling rapid scaling and extension of MLLM research to diverse domains, tasks, and modalities. 3) We provide a primary but potential MLLM training framework optimized for modality extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Our baseline model is trained within 24 A100 GPU hours, framework supports training with V100 and RTX3090 is available thanks to the open-source society. Codes and data are now available at https://openlamm.github.io.","['Multi-modality', 'Large Language Model']",[],"['Zhenfei Yin', 'Jiong WANG', 'Jianjian Cao', 'Zhelun Shi', 'Dingning Liu', 'Mukai Li', 'Xiaoshui Huang', 'Zhiyong Wang', 'Lu Sheng', 'LEI BAI', 'Jing Shao', 'Wanli Ouyang']","['University of Sydney', 'Fudan University', 'Fudan University', 'Beijing University of Aeronautics and Astronautics', 'Dalian University of Technology', 'Shanghai AI Lab', 'Shanghai AI Laboratory', 'The University of Sydney', 'Beihang University', 'Shanghai AI Laboratory', 'Shanghai AI Laboratory', 'Shanghai AI Lab']","[None, None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69958,Transparency & Explainability,Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency,"Interpreting time series models is uniquely challenging because it requires identifying both the location of time series signals that drive model predictions and their matching to an interpretable temporal pattern. While explainers from other modalities can be applied to time series, their inductive biases do not transfer well to the inherently challenging interpretation of time series. We present TimeX, a time series consistency model for training explainers. TimeX trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It addresses the issue of model faithfulness by introducing model behavior consistency, a novel formulation that preserves relations in the latent space induced by the pretrained model with relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and, unlike existing interpretability methods, it learns a latent space of explanations that can be used in various ways, such as to provide landmarks to visually aggregate similar explanations and easily recognize temporal patterns. We evaluate TimeX on eight synthetic and real-world datasets and compare its performance against state-of-the-art interpretability methods. We also conduct case studies using physiological time series. Quantitative evaluations demonstrate that TimeX achieves the highest or second-highest performance in every metric compared to baselines across all datasets. Through case studies, we show that the novel components of TimeX show potential for training faithful, interpretable models that capture the behavior of pretrained time series models.","['Explainability', 'Interpretability', 'Time Series', 'Explanations', 'Temporal patterns', 'Model Understanding', 'Latent space', 'Self-supervised learning']",[],"['Owen Queen', 'Thomas Hartvigsen', 'Teddy Koker', 'Huan He', 'Theodoros Tsiligkaridis', 'Marinka Zitnik']","['Harvard University', 'University of Virginia, Charlottesville', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'University of Pennsylvania, University of Pennsylvania', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Harvard University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72817,Transparency & Explainability,A Spectral Theory of Neural Prediction and Alignment,"The representations of neural networks are often compared to those of biological systems by performing regression between the neural network responses and those measured from biological systems. Many different state-of-the-art deep neural networks yield similar neural predictions, but it remains unclear how to differentiate among models that perform equally well at predicting neural responses. To gain insight into this, we use a recent theoretical framework that relates the generalization error from regression to the spectral properties of the model and the target. We apply this theory to the case of regression between model activations and neural responses and decompose the neural prediction error in terms of the model eigenspectra, alignment of model eigenvectors and neural responses, and the training set size. Using this decomposition, we introduce geometrical measures to interpret the neural prediction error. We test a large number of deep neural networks that predict visual cortical activity and show that there are multiple types of geometries that result in low neural prediction error as measured via regression. The work demonstrates that carefully decomposing representational metrics can provide interpretability of how models are capturing neural activity and points the way towards improved models of neural activity.","['computational neuroscience', 'neural manifolds', 'neuro-AI', 'statistical physics', 'representational geometry']",[],"['Abdulkadir Canatar', 'Jenelle Feather', 'Albert Wakhloo', 'SueYeon Chung']","['Flatiron Institute', 'Flatiron Institute', 'Flatiron Institute', 'New York University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71732,Transparency & Explainability,Interpretable Graph Networks Formulate Universal Algebra Conjectures,"The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)---one of the fields laying the foundations of modern mathematics---is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.","['universal algebra', 'interpretability', 'graph neural networks', 'concept-based models']",[],"['Francesco Giannini', 'Stefano Fioravanti', 'Oguzhan Keskin', 'Alisia Maria Lupidi', 'Lucie Charlotte Magister', 'Pietro Lio', 'Pietro Barbiero']","['University of Siena', 'Johannes Kepler Universität Linz', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71655,Transparency & Explainability,BanditPAM++: Faster $k$-medoids Clustering,"Clustering is a fundamental task in data science with wide-ranging applications. In $k$-medoids clustering, cluster centers must be actual datapoints and arbitrary distance metrics may be used; these features allow for greater interpretability of the cluster centers and the clustering of exotic objects in $k$-medoids clustering, respectively. $k$-medoids clustering has recently grown in popularity due to the discovery of more efficient $k$-medoids algorithms. In particular, recent research has proposed BanditPAM, a randomized $k$-medoids algorithm with state-of-the-art complexity and clustering accuracy. In this paper, we present BanditPAM++, which accelerates BanditPAM via two algorithmic improvements, and is $O(k)$ faster than BanditPAM in complexity and substantially faster than BanditPAM in wall-clock runtime. First, we demonstrate that BanditPAM has a special structure that allows the reuse of clustering information $\textit{within}$ each iteration. Second, we demonstrate that BanditPAM has additional structure that permits the reuse of information $\textit{across}$ different iterations. These observations inspire our proposed algorithm, BanditPAM++, which returns the same clustering solutions as BanditPAM but often several times faster. For example, on the CIFAR10 dataset, BanditPAM++ returns the same results as BanditPAM but runs over 10$\times$ faster. Finally, we provide a high-performance C++ implementation of BanditPAM++, callable from Python and R, that may be of interest to practitioners at https://github.com/motiwari/BanditPAM. Auxiliary code to reproduce all of our experiments via a one-line script is available at https://github.com/ThrunGroup/BanditPAM_plusplus_experiments.","['multi-armed bandits', 'clustering', 'k-medoids', 'best-arm identification']",[],"['Mo Tiwari', 'Ryan Kang', 'Donghyun Lee', 'Sebastian Thrun', 'Ilan Shomorony', 'Martin Jinye Zhang']","['Stanford University', 'Stanford University', 'University College London, University of London', 'Stanford University', 'University of Illinois, Urbana Champaign', 'CMU, Carnegie Mellon University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70073,Transparency & Explainability,Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach,"A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Reward redistribution serves as a solution to re-assign credits for each time step from observed sequences.  While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable reward redistribution and preserving policy invariance. In this paper, we start by studying the role of causal generative models in reward redistribution by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then,  GRD makes use of the identified causal generative model to form a compact representation to train policy over the most favorable subspace of the state space of the agent. Theoretically, we show that the unobservable Markovian reward function is identifiable, as well as the underlying causal structure and causal models. Experimental results show that our method outperforms state-of-the-art methods and the provided visualization further demonstrates the interpretability of our method. The project page is located at [https://reedzyd.github.io/GenerativeReturnDecomposition/](https://reedzyd.github.io/GenerativeReturnDecomposition/).","['Reinforcement learning', 'sparse reward', 'return decomposition', 'causal modeling']",[],"['Yudi Zhang', 'Biwei Huang', 'Ziyan Wang', 'Jun Wang', 'Meng Fang', 'Mykola Pechenizkiy']","['Eindhoven University of Technology', 'University of California, San Diego', ""King's College London"", 'University College London', 'University of Liverpool', 'Eindhoven University of Technology']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69917,Transparency & Explainability,Thin and deep Gaussian processes,"Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values.However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis of both previous approaches: {Thin and Deep GP} (TDGP). Each TDGP layer defines locally linear transformations of the original input data maintaining the concept of latent embeddings while also retaining the interpretation of lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces non-pathological manifolds that admit learning lower-dimensional representations. We show with theoretical and experimental results that i) TDGP is, unlike previous models, tailored to specifically discover lower-dimensional manifolds in the input data, ii) TDGP behaves well when increasing the number of layers, and iii) TDGP performs well in standard benchmark datasets.","['Gaussian Processes', 'Deep Gaussian Processes', 'non-stationary kernels']",[],"['Daniel Augusto de Souza', 'Alexander V Nikitin', 'S. T. John', 'Magnus Ross', 'Mauricio A Álvarez', 'Marc Peter Deisenroth', 'João Paulo Pordeus Gomes', 'Diego Mesquita', 'César Lincoln Mattos']","['University College London', 'Aalto University', 'Aalto University', 'University of Manchester', 'University of Manchester', 'University College London', 'Universidade Federal do Ceará', 'Getulio Vargas Foundation', 'Federal University of Ceará']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69929,Transparency & Explainability,CaMP: Causal Multi-policy Planning for Interactive Navigation in  Multi-room Scenes,"Visual navigation has been widely studied under the assumption that there may be several clear routes to reach the goal. However, in more practical scenarios such as a house with several messy rooms, there may not. Interactive Navigation (InterNav) considers agents navigating to their goals more effectively with object interactions, posing new challenges of learning interaction dynamics and extra action space. Previous works learn single vision-to-action policy with the guidance of designed representations. However, the causality between actions and outcomes is prone to be confounded when the attributes of obstacles are diverse and hard to measure. Learning policy for long-term action planning in complex scenes also leads to extensive inefficient exploration. In this paper, we introduce a causal diagram of InterNav clarifying the confounding bias caused by obstacles. To address the problem, we propose a multi-policy model that enables the exploration of counterfactual interactions as well as reduces unnecessary exploration. We develop a large-scale dataset containing 600k task episodes in 12k multi-room scenes based on the ProcTHOR simulator and showcase the effectiveness of our method with the evaluations on our dataset.","['Embodied AI', 'Interactive Navigation', 'Causal Reinforcement Learning', 'Hierarchical Reinforcement Learning']",[],"['Xiaohan Wang', 'Yuehu Liu', 'Xinhang Song', 'Beibei Wang', 'Shuqiang Jiang']","[""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", 'Institute of Computing Technology, Chinese Academy of Sciences', ""Xi'an Jiaotong University"", 'Institute of Computing Technology, Chinese Academy of Sciences']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69930,Transparency & Explainability,TOA: Task-oriented Active VQA,"Knowledge-based visual question answering (VQA) requires external knowledge to answer the question about an image. Early methods explicitly retrieve knowledge from external knowledge bases, which often introduce noisy information. Recently large language models like GPT-3 have shown encouraging performance as implicit knowledge source and revealed planning abilities. However, current large language models can not effectively understand image inputs, thus it remains an open problem to extract the image information and input to large language models. Prior works have used image captioning and object descriptions to represent the image. However, they may either drop the essential visual information to answer the question correctly or involve irrelevant objects to the task-of-interest. To address this problem, we propose to let large language models make an initial hypothesis according to their knowledge, then actively collect the visual evidence required to verify the hypothesis. In this way, the model can attend to the essential visual information in a task-oriented manner. We leverage several vision modules from the perspectives of spatial attention (i.e., Where to look) and attribute attention (i.e., What to look), which is similar to human cognition. The experiments show that our proposed method outperforms the baselines on open-ended knowledge-based VQA datasets and presents clear reasoning procedure with better interpretability.","['knowledge-based visual question answering', 'task-oriented', 'active image understanding', 'large language model', 'visual reasoning', 'multi-round dialogue']",[],"['Xiaoying Xing', 'Mingfu Liang', 'Ying Wu']","['Northwestern University', 'Northwestern University', 'Northwestern University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71623,Transparency & Explainability,Towards Self-Interpretable Graph-Level Anomaly Detection,"Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.","['Anomaly Detection', 'Graph Neural Networks', 'Explanation', 'Self-Interpretation']",[],"['Yixin Liu', 'Kaize Ding', 'Qinghua Lu', 'Leo Yu Zhang', 'Shirui Pan']","['Monash University', 'Northwestern University', 'CSIRO', 'Griffith University', 'Griffith University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70090,Transparency & Explainability,TransHP: Image Classification with Hierarchical Prompting,"This paper explores a hierarchical prompting mechanism for the hierarchical image classification (HIC) task. Different from prior HIC methods, our hierarchical prompting is the first to explicitly inject ancestor-class information as a tokenized hint that benefits the descendant-class discrimination. We think it well imitates human visual recognition, i.e., humans may use the ancestor class as a prompt to draw focus on the subtle differences among descendant classes. We model this prompting mechanism into a Transformer with Hierarchical Prompting (TransHP). TransHP consists of three steps: 1) learning a set of prompt tokens to represent the coarse (ancestor) classes, 2) on-the-fly predicting the coarse class of the input image at an intermediate block, and 3) injecting the prompt token of the predicted coarse class into the intermediate feature. Though the parameters of TransHP maintain the same for all input images, the injected coarse-class prompt conditions (modifies) the subsequent feature extraction and encourages a dynamic focus on relatively subtle differences among the descendant classes. Extensive experiments show that TransHP improves image classification on accuracy (e.g., improving ViT-B/16 by +2.83% ImageNet classification accuracy), training data efficiency (e.g., +12.69% improvement under 10% ImageNet training data), and model explainability. Moreover, TransHP also performs favorably against prior HIC methods, showing that TransHP well exploits the hierarchical information. The code is available at: https://github.com/WangWenhao0716/TransHP.","['hierarchical image classification', 'hierarchical prompting', 'vision transformer']",[],"['Wenhao Wang', 'Yifan Sun', 'Wei Li', 'Yi Yang']","['University of Technology Sydney', 'Baidu', 'Zhejiang University', 'Zhejiang University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70381,Transparency & Explainability,Brain Dissection: fMRI-trained Networks Reveal Spatial Selectivity in the Processing of Natural Images,"The alignment between deep neural network (DNN) features and cortical responses currently provides the most accurate quantitative explanation for higher visual areas. At the same time, these model features have been critiqued as uninterpretable explanations, trading one black box (the human brain) for another (a neural network). In this paper, we train networks to directly predict, from scratch, brain responses to images from a large-scale dataset of natural scenes (Allen et. al., 2021). We then use ""network dissection"" (Bau et. al., 2017), an explainable AI technique used for enhancing neural network interpretability by identifying and localizing the most significant features in images for individual units of a trained network, and which has been used to study category selectivity in the human brain (Khosla & Wehbe, 2022). We adapt this approach to create a hypothesis-neutral model that is then used to explore the tuning properties of specific visual regions beyond category selectivity, which we call ""brain dissection"". We use brain dissection to examine a range of ecologically important, intermediate properties, including depth, surface normals, curvature, and object relations across sub-regions of the parietal, lateral, and ventral visual streams, and scene-selective regions. Our findings reveal distinct preferences in brain regions for interpreting visual scenes, with ventro-lateral areas favoring closer and curvier features, medial and parietal areas opting for more varied and flatter 3D elements, and the parietal region uniquely preferring spatial relations. Scene-selective regions exhibit varied preferences, as the retrosplenial complex prefers distant and outdoor features, while the occipital and parahippocampal place areas favor proximity, verticality, and in the case of the OPA, indoor elements. Such findings show the potential of using explainable AI to uncover spatial feature selectivity across the visual cortex, contributing to a deeper, more fine-grained understanding of the functional characteristics of human visual cortex when viewing natural scenes.","['Computational Neuroscience', 'Deep Neural Networks', 'Visual Neuroscience', 'Visual Streams', 'Scene Perception', 'Brain Imaging']",[],"['Gabriel Herbert Sarch', 'Michael J. Tarr', 'Katerina Fragkiadaki', 'Leila Wehbe']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70025,Transparency & Explainability,"Do Not Marginalize Mechanisms, Rather Consolidate!","Structural causal models (SCMs) are a powerful tool for understanding the complex causal relationships that underlie many real-world systems. As these systems grow in size, the number of variables and complexity of interactions between them does, too. Thus, becoming convoluted and difficult to analyze. This is particularly true in the context of machine learning and artificial intelligence, where an ever increasing amount of data demands for new methods to simplify and compress large scale SCM. While methods for marginalizing and abstracting SCM already exist today, they may destroy the causality of the marginalized model. To alleviate this, we introduce the concept of consolidating causal mechanisms to transform large-scale SCM while preserving consistent interventional behaviour. We show consolidation is a powerful method for simplifying SCM, discuss reduction of computational complexity and give a perspective on generalizing abilities of consolidated SCM.","['Structural Causal Models', 'Marginalization', 'Consolidation', 'Compression']",[],"['Moritz Willig', 'Matej Zečević', 'Devendra Singh Dhami', 'Kristian Kersting']","['CS Department, TU Darmstadt, Technische Universität Darmstadt', 'TU Darmstadt', 'Eindhoven University of Technology', 'German Research Center for AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70041,Transparency & Explainability,Granger Components Analysis: Unsupervised learning of latent temporal dependencies,"A new technique for unsupervised learning of time series data based on the notion of Granger causality is presented. The technique learns pairs of projections of a multivariate data set such that the resulting components -- ""driving"" and ""driven"" -- maximize the strength of the Granger causality between the latent time series (how strongly the past of the driving signal predicts the present of the driven signal). A coordinate descent algorithm that learns pairs of coefficient vectors in an alternating fashion is developed and shown to blindly identify the underlying sources (up to scale) on simulated vector autoregressive (VAR) data. The technique is tested on scalp electroencephalography (EEG) data from a motor imagery experiment where the resulting components lateralize with the side of the cued hand, and also on functional magnetic resonance imaging (fMRI) data, where the recovered components express previously reported resting-state networks.","['components analysis', 'unsupervised learning', 'Granger Causality']",[],['Jacek Dmochowski'],['City College of New York'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/70459,Transparency & Explainability,Activity Grammars for Temporal Action Segmentation,"Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level semantics beyond individual and contextual properties of parts. The task of temporal action segmentation remains challenging for the reason, aiming at translating an untrimmed activity video into a sequence of action segments. This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation. We propose a novel grammar induction algorithm, dubbed KARI, that extracts a powerful context-free grammar from action sequence data. We also develop an efficient generalized parser, dubbed BEP, that transforms frame-level probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure. Experimental results demonstrate that our method significantly improves temporal action segmentation in terms of both performance and interpretability on two standard benchmarks, Breakfast and 50 Salads.","['neuro-symbolic approach', 'Temporal action segmentation', 'grammar']",[],"['Dayoung Gong', 'Joonseok Lee', 'Deunsol Jung', 'Suha Kwak', 'Minsu Cho']","['Pohang University of Science and Technology', 'Pohang University of Science and Technology', 'POSTECH', 'POSTECH', 'POSTECH']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70539,Transparency & Explainability,Unleashing the Power of Randomization in Auditing Differentially Private ML,"We present a rigorous methodology for auditing differentially private machine learning by adding multiple carefully designed examples called canaries. We take a first principles approach based on three key components. First, we introduce Lifted Differential Privacy (LiDP) that expands the definition of differential privacy to handle randomized datasets. This gives us the freedom to design randomized canaries. Second, we audit LiDP by trying to distinguish between the model trained with $K$ canaries versus $K-1$ canaries in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDP can leverage the symmetry in the design and reuse each privately trained model to run multiple statistical tests, one for each canary. Third, we introduce novel confidence intervals that take advantage of the multiple test statistics by adapting to the empirical higher-order correlations. Together, this new recipe demonstrates significant improvements in sample complexity, both theoretically and empirically, using synthetic and real data. Further, recent advances in designing stronger canaries can be readily incorporated in the new framework.","['Differential privacy auditing', 'multiple canaries', 'randomization', 'lifting', 'adaptive confidence intervals']",[],"['Krishna Pillutla', 'Galen Andrew', 'Peter Kairouz', 'Hugh Brendan McMahan', 'Alina Oprea', 'Sewoong Oh']","['Google', 'Google', 'Google', 'Google', 'Northeastern University', 'University of Washington']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70605,Transparency & Explainability,An information-theoretic quantification of the content of communication between brain regions,"Quantifying the amount, content and direction of communication between brain regions is key to understanding brain function. Traditional methods to analyze brain activity based on the Wiener-Granger causality principle quantify the overall information propagated by neural activity between simultaneously recorded brain regions, but do not reveal the information flow about specific features of interest (such as sensory stimuli). Here, we develop a new information theoretic measure termed Feature-specific Information Transfer (FIT), quantifying how much information about a specific feature flows between two regions. FIT merges the Wiener-Granger causality principle with information-content specificity. We first derive FIT and prove analytically its key properties. We then illustrate and test them with simulations of neural activity, demonstrating that FIT identifies, within the total information propagated between regions, the information that is transmitted about specific features. We then analyze three neural datasets obtained with different recording methods, magneto- and electro-encephalography, and spiking activity, to demonstrate the ability of FIT to uncover the content and direction of information flow between brain regions beyond what can be discerned with traditional analytical methods. FIT can improve our understanding of how brain regions communicate by uncovering previously unaddressed feature-specific information flow.",['Information transmission; Brain data analysis; Sensory processing; Partial information decomposition'],[],"['Marco Celotto', 'Jan Bím', 'Alejandro Tlaie', 'Vito De Feo', 'Alessandro Toso', 'Stefan M Lemke', 'Daniel Chicharro', 'Hamed Nili', 'Ileana Livia Hanganu-Opatz', 'Tobias H. Donner', 'Andrea Brovelli', 'Stefano Panzeri']","['University of Bologna', 'Czech Technical University in Prague', 'Ernst Strungmann Institute for Neuroscience', 'University of Essex', 'University Medical Center Hamburg-Eppendorf', 'University of North Carolina at Chapel Hill', 'City, University of London', 'Universität Hamburg', 'University Medical Center Hamburg-Eppendorf', 'University Medical Center Hamburg-Eppendorf', 'CNRS', 'University Medical Center Hamburg-Eppendorf']","[None, None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70625,Transparency & Explainability,DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models,"A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: “keeping critical thinking” and “letting everyone do their jobs” in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.","['Chain-of-Thought Reasoning', 'Multimodal Science Question Answering', 'Vision and Langauge']",[],"['Ge Zheng', 'Bin Yang', 'Jiajin Tang', 'Hong-Yu Zhou', 'Sibei Yang']","['ShanghaiTech University', 'ShanghaiTech University', 'ShanghaiTech University', 'Harvard University', 'ShanghaiTech University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70264,Transparency & Explainability,DISCOVER: Making Vision Networks Interpretable via Competition and Dissection,"Modern deep networks are highly complex and their inferential outcome very hard to interpret. This is a serious obstacle to their transparent deployment in safety-critical or bias-aware applications. This work contributes to *post-hoc* interpretability, and specifically Network Dissection. Our goal is to present a framework that makes it easier to *discover* the individual functionality of each neuron in a network trained on a vision task; discovery is performed in terms of textual description generation. To achieve this objective, we leverage: (i) recent advances in multimodal vision-text models and (ii) network layers founded upon the novel concept of stochastic local competition between linear units. In this setting, only a *small subset* of layer neurons are activated *for a given input*, leading to extremely high activation sparsity (as low as only $\approx 4\%$). Crucially, our proposed method infers (sparse) neuron activation patterns that enables the neurons to activate/specialize to inputs with specific characteristics, diversifying their individual functionality. This capacity of our method supercharges the potential of dissection processes: human understandable descriptions are generated only for the very few active neurons, thus facilitating the direct investigation of the network's decision process. As we experimentally show, our approach: (i) yields Vision Networks that retain or improve classification performance, and (ii) realizes a principled framework for text-based description and examination of the generated neuronal representations.","['Interpretability', 'Explainability', 'Network Dissection', 'Competitive Networks', 'Sparsity', 'Multimodal Models']",[],"['Konstantinos P. Panousis', 'Sotirios Chatzis']","['INRIA', 'University of Technology']","[None, 'Cyprus']",,,,,,,
https://nips.cc/virtual/2023/poster/70267,Transparency & Explainability,D-Separation for Causal Self-Explanation,"Rationalization aims to strengthen the interpretability of NLP models by extracting a subset of human-intelligible pieces of their inputting texts. Conventional works generally employ the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are \emph{d-separated} by the causal rationale. By minimizing the dependence between the non-selected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure for dependence, specifically the KL-divergence, to validate our proposed MCD criterion.  Empirically, we demonstrate that MCD improves the F1 score by up to 13.7% compared to previous state-of-the-art MMI-based methods. Our code is in an anonymous repository: https://anonymous.4open.science/r/MCD-CE88.","['interpretability', 'causal inference', 'rationalization', 'self-explaining']",[],"['Jun Wang', 'Haozhao Wang', 'Ruixuan Li', 'Zhiying Deng', 'Yang Qiu']","['iWudao', 'National Technological University', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70271,Transparency & Explainability,A Measure-Theoretic Axiomatisation of Causality,"Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.","['Causality', 'probability theory', 'causal models']",[],"['Junhyung Park', 'Simon Buchholz', 'Krikamol Muandet']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Max-Planck Institute', 'CISPA - Helmholtz Center for Information Security']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70941,Transparency & Explainability,Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks,"Intrinsic interpretable graph neural networks aim to provide transparent predictions by identifying the influential fraction of the input graph that guides the model prediction, i.e., the explanatory subgraph. However, current interpretable GNNs mostly are dataset-specific and hard to generalize to different graphs. A more generalizable GNN interpretation model which can effectively distill the universal structural patterns of different graphs is until-now unexplored. Motivated by the great success of recent pre-training techniques, we for the first time propose the Pre-training Interpretable Graph Neural Network ($\pi$-GNN) to distill the universal interpretability of GNNs by pre-training over synthetic graphs with ground-truth explanations. Specifically, we introduce a structural pattern learning module to extract diverse universal structure patterns and integrate them together to comprehensively represent the graphs of different types. Next, a hypergraph refining module is proposed to identify the explanatory subgraph by incorporating the universal structure patterns with local edge interactions. Finally, the task-specific predictor is cascaded with the pre-trained $\pi$-GNN model and fine-tuned over downstream tasks. Extensive experiments demonstrate that $\pi$-GNN significantly surpasses the leading interpretable GNN baselines with up to 9.98\% interpretation improvement and 16.06\% classification accuracy improvement. Meanwhile, $\pi$-GNN pre-trained on graph classification task also achieves the top-tier interpretation performance on node classification task, which further verifies its promising generalization performance among different downstream tasks. Our code and datasets are available at https://anonymous.4open.science/r/PI-GNN-F86C","['Intrinsic Interpretability', 'Graph Neural Networks', 'Pre-training and Fine-tuning']",[],"['Jun Yin', 'Chaozhuo Li', 'Hao Yan', 'Jianxun Lian', 'Senzhang Wang']","['Central South University', 'Microsoft Research Asia', 'Central South University', 'Microsoft', 'Central South University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70975,Transparency & Explainability,Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis,"This work studies the evaluation of explaining graph neural networks (GNNs), which is crucial to the credibility of post-hoc explainability in practical usage. Conventional evaluation metrics, and even explanation methods -- which mainly follow the paradigm of feeding the explanatory subgraph and measuring output difference -- always suffer from the notorious out-of-distribution (OOD) issue. In this work, we endeavor to confront the issue by introducing a novel evaluation metric, termed **O**OD-resistant **A**dversarial **R**obustness (OAR). Specifically, we draw inspiration from the notion of adversarial robustness and evaluate post-hoc explanation subgraphs by calculating their robustness under attack. On top of that, an elaborate OOD reweighting block is inserted into the pipeline to confine the evaluation process to the original data distribution. For applications involving large datasets, we further devise a **Sim**plified version of **OAR** (SimOAR), which achieves a significant improvement in computational efficiency at the cost of a small amount of performance. Extensive empirical studies validate the effectiveness of our OAR and SimOAR.","['Post-hoc Explainability', 'Explanation Evaluation', 'Graph Neural Network', 'Robustness Analysis']",[],"['Junfeng Fang', 'Wei Liu', 'Yuan Gao', 'Zemin Liu', 'An Zhang', 'Xiang Wang', 'Xiangnan He']","['University of Science and Technology of', 'University of Science and Technology of', 'University of Science and Technology of', 'National University of', 'National University of', 'University of Science and Technology of', 'University of Science and Technology of']","['China', 'China', 'China', 'Singapore', 'Singapore', 'China', 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/70730,Transparency & Explainability,Interpretable Prototype-based Graph Information Bottleneck,"The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph that is important for the model prediction. This is the first work that incorporates prototype learning into the process of identifying the key subgraphs that have a critical impact on the prediction performance. Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability.","['Graph neural network', 'Explainable AI', 'Interpretability']",[],"['Sangwoo Seo', 'Sungwon Kim', 'Chanyoung Park']","['Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70735,Transparency & Explainability,On the Identifiability and Interpretability of Gaussian Process Models,"In this paper, we critically examine the prevalent practice of using additive mixtures of Mat\'ern kernels in single-output Gaussian process (GP) models and explore the properties of multiplicative mixtures of Mat\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\'ern. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well suited for multi-output tasks. Our findings are supported by extensive simulations and real applications for both single- and multi-output settings. This work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.","['Gaussian process', 'Identifiability', 'Interpretability', 'Mixture kernel', 'Separable kernel']",[],"['Jiawen Chen', 'Wancen Mu', 'Yun Li', 'Didong Li']","['University of North Carolina at Chapel Hill', 'University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70762,Transparency & Explainability,Analyzing Vision Transformers for Image Classification in Class Embedding Space,"Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.","['transformers', 'computer vision', 'image classification', 'mechanistic interpretability', 'explainability']",[],"['Martina G. Vilas', 'Timothy Schaumlöffel', 'Gemma Roig']","['Computer Science Department, Goethe University', 'Johann Wolfgang Goethe Universität Frankfurt am Main', 'Goethe University Frankfurt']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71073,Transparency & Explainability,Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection,"Unsupervised image Anomaly Detection (UAD) aims to learn robust and discriminative representations of normal samples. While separate solutions per class endow expensive computation and limited generalizability, this paper focuses on building a unified framework for multiple classes. Under such a challenging setting, popular reconstruction-based networks with continuous latent representation assumption always suffer from the ""identical shortcut"" issue, where both normal and abnormal samples can be well recovered and difficult to distinguish. To address this pivotal issue, we propose a hierarchical vector quantized prototype-oriented Transformer under a probabilistic framework. First, instead of learning the continuous representations, we preserve the typical normal patterns as discrete iconic prototypes, and confirm the importance of Vector Quantization in preventing the model from falling into the shortcut. The vector quantized iconic prototypes are integrated into the Transformer for reconstruction, such that the abnormal data point is flipped to a normal data point. Second, we investigate an exquisite hierarchical framework to relieve the codebook collapse issue and replenish frail normal patterns.  Third, a prototype-oriented optimal transport method is proposed to better regulate the prototypes and hierarchically evaluate the abnormal score. By evaluating on MVTec-AD and VisA datasets, our model surpasses the state-of-the-art alternatives and possesses good interpretability. The code is available at https://github.com/RuiyingLu/HVQ-Trans.","['Anomaly Detection', 'Transformer', 'Vector Quantization', 'Unsupervised Anomaly Detection']",[],"['Ruiying Lu', 'YuJie Wu', 'Long Tian', 'Dongsheng Wang', 'Bo Chen', 'Xiyang Liu', 'Ruimin Hu']","['Xidian University', ""Xi'an University of Electronic Science and Technology"", ""Xi'an University of Software Engineering Institute"", 'Xidian University', 'Xidian University', ""Xi'an University of Electronic Science and Technology"", 'Xidian University']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71075,Transparency & Explainability,A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference,"We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance.","['Neurosymbolic Learning', 'Generative Modeling', 'Approximate Inference']",[],"['Emile van Krieken', 'Thiviyan Thanapalasingam', 'Jakub M. Tomczak', 'Frank Van Harmelen', 'Annette Ten Teije']","['Edinburgh University, University of Edinburgh', 'University of Amsterdam', 'Eindhoven University of Technology', 'Vrije Universiteit Amsterdam', 'Vrije Universiteit Amsterdam']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71176,Transparency & Explainability,LEACE: Perfect linear concept erasure in closed form,"Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called concept scrubbing, which erases target concept information from _every_ layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Our code is available at https://github.com/EleutherAI/concept-erasure.","['interpretability', 'fairness', 'concept erasure', 'representation', 'adversarial', 'robustness']",[],"['Nora Belrose', 'David Schneider-Joseph', 'Shauli Ravfogel', 'Ryan Cotterell', 'Edward Raff', 'Stella Biderman']","['EleutherAI', 'EleutherAI', 'Bar-Ilan University', 'Swiss Federal Institute of Technology', 'University of Maryland, Baltimore County', 'EleutherAI']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71178,Transparency & Explainability,Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions,"We propose an interpretable model to uncover the behavioral patterns of human movements by analyzing their trajectories. Our approach is based on the belief that human actions are driven by intentions and are influenced by environmental factors such as spatial relationships with surrounding objects. To model this, we use a set of spatial-temporal logic rules that include intention variables as principles. These rules are automatically discovered and used to capture the dynamics of human actions. To learn the model parameters and rule content, we design an EM learning algorithm that treats the unknown rule content as a latent variable. In the E-step, we evaluate the posterior over the latent rule content, and in the M-step, we optimize the rule generator and model parameters by maximizing the expected log-likelihood. Our model has wide-ranging applications in areas such as sports analytics, robotics, and autonomous cars. We demonstrate the model's superior interpretability and prediction performance on both pedestrian and NBA basketball player datasets, achieving promising results.","['Logic rule', 'human actions', 'sports analyze']",[],"['Chengzhi Cao', 'Chao Yang', 'Ruimao Zhang', 'Shuang Li']","['University of Science and Technology of', 'The Chinese University of , Shenzhen', 'The Chinese University of  (Shenzhen)', 'The Chinese University of  (Shenzhen)']","['China', 'Hong Kong', 'Hong Kong', 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/71180,Transparency & Explainability,LICO: Explainable Models with Language-Image COnsistency,"Interpreting the decisions of deep learning models has been actively studied since the explosion of deep neural networks. One of the most convincing interpretation approaches is salience-based visual interpretation, such as Grad-CAM, where the generation of attention maps depends merely on categorical labels. Although existing interpretation methods can provide explainable decision clues, they often yield partial correspondence between image and saliency maps due to the limited discriminative information from one-hot labels. This paper develops a Language-Image COnsistency model for explainable image classification, termed LICO,  by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner. Specifically, we first establish a coarse global manifold structure alignment by minimizing the distance between the distributions of image and language features. We then achieve fine-grained saliency maps by applying optimal transport (OT) theory to assign local feature maps with class-specific prompts. Extensive experimental results on eight benchmark datasets demonstrate that the proposed LICO achieves a significant improvement in generating more explainable attention maps in conjunction with existing interpretation methods such as Grad-CAM. Remarkably, LICO improves the classification performance of existing  models without introducing any computational overhead during inference.","['Language-image consistency', 'prompt learning', 'image classification', 'CNN interpretation']",[],"['Yiming Lei', 'Zilong Li', 'Yangyang Li', 'Junping Zhang', 'Hongming Shan']","['Fudan University', 'Fudan University', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Fudan University', 'Fudan University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71608,Transparency & Explainability,Cognitive Model Discovery via Disentangled RNNs,"Computational cognitive models are a fundamental tool in behavioral neuroscience. They embody in software precise hypotheses about the cognitive mechanisms underlying a particular behavior. Constructing these models is typically a difficult iterative process that requires both inspiration from the literature and the creativity of an individual researcher. Here, we adopt an alternative approach to learn parsimonious cognitive models directly from data. We fit behavior data using a recurrent neural network that is penalized for carrying excess information between timesteps, leading to sparse, interpretable representations and dynamics. When fitting synthetic behavioral data from known cognitive models, our method recovers the underlying form of those models. When fit to choice data from rats performing a bandit task, our method recovers simple and interpretable models that make testable predictions about neural mechanisms.","['Cognitive modeling', 'neural networks', 'interpretability', 'disentangling', 'neuroscience', 'rodent behavior']",[],"['Kevin J Miller', 'Maria K Eckstein', 'Matthew Botvinick', 'Zeb Kurth-Nelson']","['University College London, University of London', 'Google DeepMind', 'Google DeepMind', 'DeepMind']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71802,Transparency & Explainability,Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars,"Transformer interpretability aims to understand the algorithm implemented by a learned Transformer by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be ""nearly randomized"", while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even with severe constraints to the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading.","['Transformer', 'Self Attention', 'Dyck Language', 'Context Free Grammar', 'Formal Language', 'Theory', 'Interpretability']",[],"['Kaiyue Wen', 'Yuchen Li', 'Bingbin Liu', 'Andrej Risteski']","['Tsinghua University, Tsinghua University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71806,Transparency & Explainability,The Transient Nature of Emergent In-Context Learning in Transformers,"Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it.  Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to ``overtrain'' transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits.","['in-context learning', 'transformers', 'emergence', 'transience']",[],"['Aaditya K Singh', 'Stephanie C.Y. Chan', 'Ted Moskovitz', 'Erin Grant', 'Andrew M Saxe', 'Felix Hill']","['University College London, University of London', 'DeepMind', 'Gatsby Computational Neuroscience Unit', 'University College London', 'University College London, University of London', 'Google']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71893,Transparency & Explainability,Trade-off Between Efficiency and Consistency for Removal-based Explanations,"In the current landscape of explanation methodologies, most predominant approaches, such as SHAP and LIME, employ removal-based techniques to evaluate the impact of individual features by simulating various scenarios with specific features omitted. Nonetheless, these methods primarily emphasize efficiency in the original context, often resulting in general inconsistencies. In this paper, we demonstrate that such inconsistency is an inherent aspect of these approaches by establishing the Impossible Trinity Theorem, which posits that interpretability, efficiency, and consistency cannot hold simultaneously. Recognizing that the attainment of an ideal explanation remains elusive, we propose the utilization of interpretation error as a metric to gauge inefficiencies and inconsistencies. To this end, we present two novel algorithms founded on the standard polynomial basis, aimed at minimizing interpretation error. Our empirical findings indicate that the proposed methods achieve a substantial reduction in interpretation error, up to 31.8 times lower when compared to alternative techniques.","['AI interpretability', 'explainable AI', 'deep learning theory']",[],"['Yifan Zhang', 'Haowei He', 'Zhiquan Tan', 'Yang Yuan']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71942,Transparency & Explainability,Non-autoregressive Machine Translation with Probabilistic Context-free Grammar,"Non-autoregressive Transformer(NAT) significantly accelerates the inference of neural machine translation. However, conventional NAT models suffer from limited expression power and performance degradation compared to autoregressive (AT) models due to the assumption of conditional independence among target tokens. To address these limitations, we propose a novel approach called PCFG-NAT, which leverages a specially designed Probabilistic Context-Free Grammar (PCFG) to enhance the ability of NAT models to capture complex dependencies among output tokens. Experimental results on major machine translation benchmarks demonstrate that PCFG-NAT further narrows the gap in translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a deeper understanding of the generated sentences, addressing the lack of satisfactory explainability in neural machine translation. Code is publicly available at https://github.com/ictnlp/PCFG-NAT.","['Machine translation', 'Non-autoregressive generation', 'Probabilistic Context-free Grammar']",[],"['Shangtong Gui', 'Chenze Shao', 'Zhengrui Ma', 'Xishan Zhang', 'Yunji Chen', 'Yang Feng']","['Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', ', Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72058,Transparency & Explainability,Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks,"Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes with dense updates.","['numerical methods', 'deep networks', 'evolution equations', 'scientific computing', 'partial differential equations', 'model reduction']",[],"['Jules Berman', 'Benjamin Peherstorfer']","['New York University', 'New York University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71676,Transparency & Explainability,The Utility of “Even if” Semifactual Explanation to Optimise Positive Outcomes,"When users receive either a positive or negative outcome from an automated system, Explainable AI (XAI) has almost exclusively focused on how to mutate negative outcomes into positive ones by crossing a decision boundary using counterfactuals (e.g., *""If you earn 2k more, we will accept your loan application""*). Here, we instead focus on positive outcomes, and take the novel step of using XAI to optimise them (e.g., *""Even if you wish to half your down-payment, we will still accept your loan application""*). Explanations such as these that employ ""even if..."" reasoning, and do not cross a decision boundary, are known as semifactuals. To instantiate semifactuals in this context, we introduce the concept of *Gain* (i.e., how much a user stands to benefit from the explanation), and consider the first causal formalisation of semifactuals. Tests on benchmark datasets show our algorithms are better at maximising gain compared to prior work, and that causality is important in the process. Most importantly however, a user study supports our main hypothesis by showing people find semifactual explanations more useful than counterfactuals when they receive the positive outcome of a loan acceptance.","['Semifactual Explanation', 'Counterfactual Explanation', 'Explainable AI', 'Recourse', 'User Study']",[],"['Eoin M. Kenny', 'Weipeng Fuzzy Huang']","['Massachusetts Institute of Technology', 'Tencent SBD Lab']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71711,Transparency & Explainability,Causal normalizing flows: from theory to practice,"In this work, we deepen on the use of normalizing flows for causal reasoning. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyze different design and learning choices for *causal normalizing flows* to capture the underlying causal data-generating process. Third, we describe how to implement the *do-operator* in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems—where the presence of mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be found at https://github.com/psanch21/causal-flows.","['causality', 'causal inference', 'normalizing flows', 'identifiability', 'interventions', 'counterfactuals']",[],"['Adrián Javaloy', 'Pablo Sanchez Martin', 'Isabel Valera']","['Saarland University, Saarland University', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Saarland University, Saarland University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71720,Transparency & Explainability,Unified 3D Segmenter As Prototypical Classifiers,"The task of point cloud segmentation, comprising semantic, instance, and panoptic segmentation, has been mainly tackled by designing task-specific network architectures, which often lack the flexibility to generalize across tasks, thus resulting in a fragmented research landscape. In this paper, we introduce ProtoSEG, a prototype-based model that unifies semantic, instance, and panoptic segmentation tasks. Our approach treats these three homogeneous tasks as a classification problem with different levels of granularity. By leveraging a Transformer architecture, we extract point embeddings to optimize prototype-class distances and dynamically learn class prototypes to accommodate the end tasks. Our prototypical design enjoys simplicity and transparency, powerful representational learning, and ad-hoc explainability. Empirical results demonstrate that ProtoSEG outperforms concurrent well-known specialized architectures on 3D point cloud benchmarks, achieving 72.3%, 76.4% and 74.2% mIoU for semantic segmentation on S3DIS, ScanNet V2 and SemanticKITTI, 66.8% mCov and 51.2% mAP for instance segmentation on S3DIS and ScanNet V2, 62.4% PQ for panoptic segmentation on SemanticKITTI, validating the strength of our concept and the effectiveness of our algorithm.  The code and models are available at https://github.com/zyqin19/PROTOSEG.","['Point Cloud Segmentation', 'Prototypical Classifier', 'Unified Framework']",[],"['Zheyun Qin', 'Cheng Han', 'Qifan Wang', 'Xiushan Nie', 'Yilong Yin', 'Xiankai Lu']","['Shandong University', 'Rochester Institute of Technology', 'Meta AI', 'Shandong Jianzhu University', 'Shandong University', 'Shandong University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71721,Transparency & Explainability,Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans,"Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named restoration gap for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a gap predictor which produces restoration gap guidance to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans.","['Offline Reinforcement Learning', 'Trajectory Optimization', 'Diffusion Models', 'Sequential Decision Making']",[],"['Kyowoon Lee', 'Seongun Kim', 'Jaesik Choi']","['Ulsan National Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71758,Transparency & Explainability,Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction,"The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behavior, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.","['Reinforcement Learning', 'First-Order-Logic', 'Symbolic Abstraction', 'Interpretable Reinforcement Learning', 'Logic Reinforcement Learning']",[],"['Quentin Delfosse', 'Hikaru Shindo', 'Devendra Singh Dhami', 'Kristian Kersting']","['CS Department, TU Darmstadt, TU Darmstadt', 'TU Darmstadt', 'Eindhoven University of Technology', 'German Research Center for AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72454,Transparency & Explainability,On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective,"Input gradients have a pivotal role in a variety of applications, including adversarial attack algorithms for evaluating model robustness, explainable AI techniques for generating saliency maps, and counterfactual explanations. However, saliency maps generated by traditional neural networks are often noisy and provide limited insights. In this paper, we demonstrate that, on the contrary, the saliency maps of 1-Lipschitz neural networks, learnt with the dual loss of an optimal transportation problem, exhibit desirable XAI properties: They are highly concentrated on the essential parts of the image with low noise, significantly outperforming state-of-the-art explanation approaches across various models and metrics. We also prove that these maps align unprecedentedly well with human explanations on ImageNet. To explain the particularly beneficial properties of the saliency map for such models, we prove this gradient encodes  both the direction of the transportation plan and the direction towards the nearest adversarial attack. Following the gradient down to the decision boundary is no longer considered an adversarial attack, but rather a counterfactual explanation that explicitly transports the input from one class to another.  Thus, Learning with such a loss jointly optimizes the classification objective and the alignment of the gradient , i.e. the saliency map, to the transportation plan direction. These networks were previously known to be certifiably robust by design, and we demonstrate that they scale well for large problems and models, and are tailored for explainability using a fast and straightforward method.","['1-Lipschitz neural network', 'explicability']",[],"['Mathieu Serrurier', 'Franck Mamalet', 'Thomas FEL', 'Louis Béthune', 'Thibaut Boissin']","['university Paul Sabatier', 'IRT Saint Exupery', 'Brown University', 'Université Paul Sabatier', 'IRT Saint exupéry']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72478,Transparency & Explainability,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader,"We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR also has the potential to serve as a unified model for tackling various extraction and classification tasks in the MRC formulation.","['Machine Reading Comprehension', 'Pre-training', 'Natural Language Understanding']",[],"['Weiwen Xu', 'Xin Li', 'Wenxuan Zhang', 'Meng Zhou', 'Wai Lam', 'Luo Si', 'Lidong Bing']","['The Chinese University of', 'Alibaba Group', 'Alibaba Group', 'CMU, Carnegie Mellon University', 'The Chinese University of', 'Alibaba Group', 'Alibaba Group']","['Hong Kong', None, None, None, 'Hong Kong', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72481,Transparency & Explainability,Passive learning of active causal strategies in agents and language models,"What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models.",['passive; causal; offline; agency; language models'],[],"['Andrew Kyle Lampinen', 'Stephanie C.Y. Chan', 'Ishita Dasgupta', 'Andrew Joo Hun Nam', 'Jane X Wang']","['Google DeepMind', 'DeepMind', 'DeepMind', 'Stanford University', 'DeepMind']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72483,Transparency & Explainability,Identifiable Contrastive Learning with Automatic Feature Importance Discovery,"Existing contrastive learning methods rely on pairwise sample contrast $z_x^\top z_{x'}$ to learn data representations, but the learned features often lack clear interpretability from a human perspective. Theoretically, it lacks feature identifiability and different initialization may lead to totally different features. In this paper, we study a new method named tri-factor contrastive learning (triCL) that involves a 3-factor contrast in the form of $z_x^\top S z_{x'}$, where $S=\text{diag}(s_1,\dots,s_k)$ is a learnable diagonal matrix that automatically captures the importance of each feature. We show that by this simple extension, triCL can not only obtain identifiable features that eliminate randomness but also obtain more interpretable features that are ordered according to the importance matrix $S$. We show that features with high importance have nice interpretability by capturing common classwise features, and obtain superior performance when evaluated for image retrieval using a few features. The proposed triCL objective is general and can be applied to different contrastive learning methods like SimCLR and CLIP. We believe that it is a better alternative to existing 2-factor contrastive learning by improving its identifiability and interpretability with minimal overhead. Code is available at https://github.com/PKU-ML/Tri-factor-Contrastive-Learning.","['Self-supervised Learning', 'Contrastive Learning', 'Identifiability', 'Representation Learning']",[],"['Qi Zhang', 'Yifei Wang', 'Yisen Wang']","['Peking University', 'Massachusetts Institute of Technology', 'Peking University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72505,Transparency & Explainability,An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient,"Restricting the variance of a policy’s return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.","['risk-averse RL', 'mean-variance RL']",[],"['Yudong Luo', 'Guiliang Liu', 'Pascal Poupart', 'Yangchen Pan']","['University of Waterloo', 'The Chinese University of , Shenzhen', 'University of Waterloo', 'University of Oxford']","[None, 'Hong Kong', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72760,Transparency & Explainability,TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery,"Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called **Temp**oral **M**otifs **E**xplainer (**TempME**), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs.  Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.","['Explainability', 'Temporal Graph Neural Network']",[],"['Jialin Chen', 'Zhitao Ying']","['Yale University', 'Yale University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72934,Transparency & Explainability,Estimating Propensity for Causality-based Recommendation without Exposure Data,"Causality-based recommendation systems focus on the causal effects of user-item interactions resulting from item exposure (i.e., which items are recommended or exposed to the user), as opposed to conventional correlation-based recommendation. They are gaining popularity due to their multi-sided benefits to users, sellers and platforms alike. However, existing causality-based recommendation methods require additional input in the form of exposure data and/or propensity scores (i.e., the probability of exposure) for training. Such data, crucial for modeling causality in recommendation, are often not available in real-world situations due to technical or privacy constraints. In this paper, we bridge the gap by proposing a new framework, called Propensity Estimation for Causality-based Recommendation (PropCare). It can estimate the propensity and exposure from a more practical setup, where only interaction data are available *without* any ground truth on exposure or propensity in training and inference. We demonstrate that, by relating the pairwise characteristics between propensity and item popularity, PropCare enables competitive causality-based recommendation given only the conventional interaction data. We further present a theoretical analysis on the bias of the causal effect under our model estimation.  Finally, we empirically evaluate PropCare through both quantitative and qualitative experiments.","['recommendation systems', 'causal effect', 'propensity score', 'propensity estimation']",[],"['Zhongzhou Liu', 'Yuan Fang', 'Min Wu']","['Management University', 'Management University', 'A*STAR']","['Singapore', 'Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/72274,Transparency & Explainability,Directed Cyclic Graph for Causal Discovery from Multivariate Functional Data,"Discovering causal relationship using multivariate functional data has received a significant amount of attention very recently. In this article, we introduce a functional linear structural equation model for causal structure learning when the underlying graph involving the multivariate functions may have cycles. To enhance interpretability, our model involves a low-dimensional causal embedded space such that all the relevant causal information in the multivariate functional data is preserved in this lower-dimensional subspace. We prove that the proposed model is causally identifiable under standard assumptions that are often made in the causal discovery literature. To carry out inference of our model, we develop a fully Bayesian framework with suitable prior specifications and uncertainty quantification through posterior summaries. We illustrate the superior performance of our method over existing methods in terms of causal graph estimation through extensive simulation studies. We also demonstrate the proposed method using a brain EEG dataset.","['Causal Embedding', 'Causal Discovery', 'Multivariate Functional Data', 'Directed Cyclic Graph', 'Causal Structure Learning', 'Bayesian Inference']",[],"['Saptarshi Roy', 'Raymond K. W. Wong', 'Yang Ni']","['Texas A&M University - College Station', 'Texas A&M University', 'Texas A&M University Main Campus']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72281,Transparency & Explainability,GRAND-SLAMIN’ Interpretable Additive Modeling with Structural Constraints,"Generalized Additive Models (GAMs) are a family of flexible and interpretable models with old roots in statistics. GAMs are often used with pairwise interactions to improve model accuracy while still retaining flexibility and interpretability but lead to computational challenges as we are dealing with order of $p^2$ terms. It is desirable to restrict the number of components (i.e., encourage sparsity) for easier interpretability, and better computational and statistical properties. Earlier approaches, considering sparse pairwise interactions, have limited scalability, especially when imposing additional structural interpretability constraints. We propose a flexible GRAND-SLAMIN framework that can learn GAMs with interactions under sparsity and additional structural constraints in a differentiable end-to-end fashion. We customize first-order gradient-based optimization to perform sparse backpropagation to exploit sparsity in additive effects for any differentiable loss function in a GPU-compatible manner. Additionally, we establish novel non-asymptotic prediction bounds for our estimators with tree-based shape functions. Numerical experiments on real-world datasets show that our toolkit performs favorably in terms of performance, variable selection and scalability when compared with popular toolkits to fit GAMs with interactions. Our work expands the landscape of interpretable modeling while maintaining prediction accuracy competitive with non-interpretable black-box models. Our code is available at https://github.com/mazumder-lab/grandslamin.","['Generalized additive models', 'component selection', 'hierarchy', 'interpretability']",[],"['Shibal Ibrahim', 'Gabriel Isaac Afriat', 'Kayhan Behdin', 'Rahul Mazumder']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72296,Transparency & Explainability,Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models,"Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior.","['localization', 'model editing', 'mechanistic interpretability', 'language models']",[],"['Peter Hase', 'Mohit Bansal', 'Been Kim', 'Asma Ghandeharioun']","['University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill', 'Google DeepMind', 'Google']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72323,Transparency & Explainability,AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models,"Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, \textbf{AUDIT} has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edited) audio as conditions and generating output (edited) audio; 2) it can automatically learn to only modify segments that need to be edited by comparing the difference between the input and output audio; 3) it only needs edit instructions instead of full target audio descriptions as text input. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution). Demo samples are available at https://audit-demopage.github.io/.","['audio editing', 'text-to-audio generation', 'diffusion models']",[],"['Yuancheng Wang', 'Zeqian Ju', 'Xu Tan', 'Lei He', 'Zhizheng Wu', 'Jiang Bian', 'sheng zhao']","['The Chinese University of , Shenzhen', 'University of Science and Technology of', 'Microsoft', 'Microsoft', 'Chinese University of , Shenzhen', 'Microsoft', 'Tsinghua University']","['Hong Kong', 'China', None, None, 'Hong Kong', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72328,Transparency & Explainability,Auditing Fairness by Betting,"We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a  probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics---the ``testing by betting'' framework in particular. These connections ensure that our methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach on three benchmark fairness datasets.","['fairness', 'auditing', 'sequential analysis', 'martingales', 'testing by betting']",[],"['Ben Chugg', 'Santiago Cortes-Gomez', 'Bryan Wilder', 'Aaditya Ramdas']","['Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72340,Transparency & Explainability,Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder,"Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. In order to measure perturbation-specific model-properties, we also introduce a framework for evaluation of perturbation models based on average treatment effects with links to posterior predictive checks. SAMS-VAE outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms. Our results suggest SAMS-VAE is an interesting addition to the modeling toolkit for machine learning-driven scientific discovery.","['Disentagled representation learning', 'VAE', 'generative models', 'sparse mechanism shift', 'perturbation modeling', 'cellular modeling']",[],['Theofanis Karaletsos'],['Insitro'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/72384,Transparency & Explainability,ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP,"Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. We employ ChatGPT, a state-of-the-art large language model, as our paraphraser and formulate the trigger-removal task as a prompt engineering problem. We adopt fuzzing, a technique commonly used for unearthing software vulnerabilities, to discover optimal paraphrase prompts that can effectively eliminate triggers while concurrently maintaining input semantics. Experiments on 4 types of backdoor attacks, including the subtle style backdoors, and 4 distinct datasets demonstrate that our approach surpasses baseline methods, including STRIP, RAP, and ONION, in precision and recall.","['NLP', 'backdoor attack', 'fuzzing']",[],"['Lu Yan', 'ZHUO ZHANG', 'Guanhong Tao', 'Kaiyuan Zhang', 'Guangyu Shen', 'Xiangyu Zhang']","['Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', ', Purdue University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72430,Transparency & Explainability,Unbiased learning of deep generative models with structured discrete representations,"By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges.  We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables.  Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work.  These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time series models, where the SVAE performs competitively while learning interpretable and structured discrete data representations.","['Generative Models', 'Graphical Models', 'Variational Inference', 'Amortized Inference']",[],"['Harry Bendekgey', 'Gabriel Hope', 'Erik B. Sudderth']","['Donald Bren School of Information and Computer Sciences, University of California, Irvine', 'University of California, Irvine', 'University of California, Irvine']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72951,Transparency & Explainability,Scaling laws for language encoding models in fMRI,"Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales logarithmically with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models showed that performance is nearing the theoretical maximum for brain areas such as the precuneus and higher auditory cortex. These results suggest that increasing scale in both models and data will yield incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding.","['Encoding Models', 'Language Models', 'Neuroscience', 'Scaling Laws']",[],"['Richard Antonello', 'Aditya Vaidya', 'Alexander Huth']","['University of Texas, Austin', 'University of Texas at Austin', 'The University of Texas at Austin']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72958,Transparency & Explainability,Gaussian Membership Inference Privacy,"We propose a novel and practical privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model.  Consequently, $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). In particular, we derive a parametric family of $f$-MIP guarantees that we refer to as $\mu$-Gaussian Membership Inference Privacy ($\mu$-GMIP) by theoretically analyzing likelihood ratio-based membership inference attacks on stochastic gradient descent (SGD). Our analysis highlights that models trained with standard SGD already offer an elementary level of MIP.  Additionally, we show how $f$-MIP can be amplified by adding noise to gradient updates. Our analysis further yields an analytical membership inference attack that offers two distinct advantages over previous approaches. First, unlike existing state-of-the-art attacks that require training hundreds of shadow models, our attack does not require any shadow model.  Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, we quantify how various hyperparameters (e.g., batch size, number of model parameters) and specific data characteristics determine an attacker's ability to accurately infer a point's membership in the training set. We demonstrate the effectiveness of our method on models trained on vision and tabular datasets.","['Privacy', 'Membership Inference Attacks']",[],"['Tobias Leemann', 'Martin Pawelczyk', 'Gjergji Kasneci']","['Technische Universität München', 'Harvard University', 'Technische Universität München']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73570,Transparency & Explainability,WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes,"The examination of blood samples at a microscopic level plays a fundamental role in clinical diagnostics. For instance, an in-depth study of White Blood Cells (WBCs), a crucial component of our blood, is essential for diagnosing blood-related diseases such as leukemia and anemia. While multiple datasets containing WBC images have been proposed, they mostly focus on cell categorization, often lacking the necessary morphological details to explain such categorizations, despite the importance of explainable artificial intelligence (XAI) in medical domains. This paper seeks to address this limitation by introducing comprehensive annotations for WBC images. Through collaboration with pathologists, a thorough literature review, and manual inspection of microscopic images, we have identified 11 morphological attributes associated with the cell and its components (nucleus, cytoplasm, and granules). We then annotated ten thousand WBC images with these attributes, resulting in 113k labels (11 attributes x 10.3k images). Annotating at this level of detail and scale is unprecedented, offering unique value to AI in pathology. Moreover, we conduct experiments to predict these attributes from cell images, and also demonstrate specific applications that can benefit from our detailed annotations. Overall, our dataset paves the way for interpreting WBC recognition models, further advancing XAI in the fields of pathology and hematology.","['white blood cells', 'morphological attributes', 'microscopic image', 'explainable AI', 'computer vision']",[],"['Winnie Pang', 'Bihan Wen']","['Nanyang Technological University', 'Nanyang Technological University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74184,Transparency & Explainability,[Re] On Explainability of Graph Neural Networks via Subgraph Explorations,"Yuan et al. claim their proposed method SubgraphX achieves (i) higher fidelity in explaining models for graph- and node classification tasks compared to other explanation techniques, namely GNNExplainer. Additionally, (ii) the computational effort of SubgraphX is at a 'reasonable level', which is not further specified by the original authors. We define this as at most ten times slower than GNNExplainer. We reimplemented the proposed algorithm in PyTorch. Then, we replicated the experiments performed by the authors on a smaller scale due to resource constraints. Additionally, we checked the performance on a new dataset and investigated the influence of hyperparameters. Lastly, we improved SubgraphX using greedy initialization and utilizing fidelity as a score function. We were able to reproduce the main claims on the MUTAG dataset, where SubgraphX has a better performance than GNNExplainer. Furthermore, SubgraphX has a reasonable runtime of about seven times longer than GNNExplainer. We successfully employed SubgraphX on the Karate Club dataset, where it outperforms GNNExplainer as well. The hyperparameter study revealed that the number of Monte-Carlo Tree search iterations and Monte-Carlo sampling steps are the most important hyperparameters and directly trade performance for runtime. Lastly, we show that our proposed improvements to SubgraphX significantly enhance fidelity and runtime. The authors' description of the algorithm was clear and concise. The original implementation is available in the DIG-library as a reference to our implementation. The authors performed extensive experiments, which we could not replicate in their full scale due to resource constraints. However, we were able to achieve similar results on a subset of the datasets used. Another issue was that despite the original code of the authors and datasets being publicly available, there were many compatibility issues. The original authors briefly reviewed our work and agreed with the findings.","['Rescience c', 'Rescience x', 'Explainable AI', 'Graph Neural Networks', 'SubgraphX', 'Python']",[],"['Yannik Mahlau', 'Lukas Berg', 'Leo Kayser']","['Universität Hannover', 'Universität Hannover', 'Max Planck Institute for Mathematics in the Sciences, Max-Planck Institute']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72687,Transparency & Explainability,"AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neuron Activity","Latent Variable Models (LVMs) propose to model the dynamics of neural populations by capturing low-dimensional structures that represent features involved in neural activity. Recent LVMs are based on deep learning methodology where a deep neural network is trained to reconstruct the same neural activity given as input and as a result to build the latent representation. Without taking past or future activity into account such a task is non-causal. In contrast, the task of forecasting neural activity based on given input extends the reconstruction task. LVMs that are trained on such a task could potentially capture temporal causality constraints within its latent representation. Forecasting has received less attention than reconstruction due to recording challenges such as limited neural measurements and trials. In this work, we address modeling neural population dynamics via the forecasting task and improve forecasting performance by including a prior, which consists of pairwise neural unit interaction as a multivariate dynamic system. Our proposed model---Additive, Multiplicative, and Adaptive Graph Neural Network (AMAG)---leverages additive and multiplicative message-passing operations analogous to the interactions in neuronal systems and adaptively learns the interaction among neural units to forecast their future activity. We demonstrate the advantage of AMAG compared to non-GNN based methods on synthetic data and multiple modalities of neural recordings (field potentials from penetrating electrodes or surface-level micro-electrocorticography) from four rhesus macaques. Our results show the ability of AMAG to recover ground truth spatial interactions and yield estimation for future dynamics of the neural population.","['Neuroscience and Cognitive Science', 'Neural Activity Forecasting', 'Graph Neural Network']",[],"['Jingyuan Li', 'Leo Scholl', 'Trung Le', 'Pavithra Rajeswaran', 'Amy L Orsborn', 'Eli Shlizerman']","['University of Washington, Seattle', 'University of Washington', 'University of Washington, Seattle', 'University of Washington', 'University of Washington, Seattle', 'University of Washington, University of Washington']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72733,Transparency & Explainability,Explaining Predictive Uncertainty with Information Theoretic Shapley Values,"Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the $\textit{uncertainty}$ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement efficient algorithms that perform well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-value acquisition.","['Explainable AI', 'interpretable ML', 'feature attributions', 'information theory', 'Shapley values']",[],"['David Watson', ""Joshua O'Hara"", 'Niek Tax', 'Ido Guy']","[""King's College London, University of London"", ""King's College London, University of London"", 'Facebook', 'Facebook']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72978,Transparency & Explainability,Beyond Normal: On the Evaluation of Mutual Information Estimators,"Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set.","['Mutual Information', 'Invariance', 'Benchmark', 'Geometric Machine Learning']",[],"['Frederic Grabowski', 'Julia E Vogt', 'Niko Beerenwinkel', 'Alexander Marx']","['Institute of Fundamental Technological Research, Polish Academy of Sciences', 'Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Department of Computer Science, ETHZ - ETH Zurich']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73036,Transparency & Explainability,Deciphering Spatio-Temporal Graph Forecasting: A Causal Lens and Treatment,"Spatio-Temporal Graph (STG) forecasting is a fundamental task in many real-world applications. Spatio-Temporal Graph Neural Networks have emerged as the most popular method for STG forecasting, but they often struggle with temporal out-of-distribution (OoD) issues and dynamic spatial causation. In this paper, we propose a novel framework called CaST to tackle these two challenges via causal treatments. Concretely, leveraging a causal lens, we first build a structural causal model to decipher the data generation process of STGs. To handle the temporal OoD issue, we employ the back-door adjustment by a novel disentanglement block to separate the temporal environments from input data. Moreover, we utilize the front-door adjustment and adopt edge-level convolution to model the ripple effect of causation. Experiments results on three real-world datasets demonstrate the effectiveness of CaST, which consistently outperforms existing methods with good interpretability. Our source code is available at https://github.com/yutong-xia/CaST.",['Spatio-temporal forecasting'],[],"['Yutong Xia', 'Yuxuan Liang', 'Haomin Wen', 'Xu Liu', 'Kun Wang', 'Zhengyang Zhou', 'Roger Zimmermann']","['National University of', 'The  University of Science and Technology (Guangzhou)', 'Beijing Jiaotong University', 'National University of', 'University of Science and Technology of', 'University of Science and Technology of', 'National University of']","['Singapore', 'Hong Kong', None, 'Singapore', 'China', 'China', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/73453,Transparency & Explainability,Holistic Evaluation of Text-to-Image Models,"The stunning qualitative improvement of text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on image-text alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase","['text-to-image', 'image generation', 'multimodal', 'holistic evaluation', 'benchmarking', 'human evaluation']",[],"['Tony Lee', 'Michihiro Yasunaga', 'Chenlin Meng', 'Yifan Mai', 'Joon Sung Park', 'Agrim Gupta', 'Yunzhi Zhang', 'Deepak Narayanan', 'Hannah Benita Teufel', 'Marco Bellagente', 'Minguk Kang', 'Taesung Park', 'Jure Leskovec', 'Jun-Yan Zhu', 'Li Fei-Fei', 'Jiajun Wu', 'Stefano Ermon', 'Percy Liang']","['Stanford University', 'Stanford University', 'Stanford University', 'Computer Science Department, Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'NVIDIA', 'Aleph Alpha GmbH', 'Aleph-Alpha gmbh', 'POSTECH', 'Adobe Systems', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71192,Transparency & Explainability,Anchor Data Augmentation,"We propose a novel algorithm for data augmentation in nonlinear over-parametrized regression. Our data augmentation algorithm borrows from the literature on causality. Contrary to the current state-of-the-art solutions that rely on modifications of Mixup algorithm, we extend the recently proposed distributionally robust Anchor regression (AR) method for data augmentation. Our Anchor Data Augmentation (ADA) uses several replicas of the modified samples in AR to provide more training examples, leading to more robust regression predictions. We apply ADA to linear and nonlinear regression problems using neural networks. ADA is competitive with state-of-the-art C-Mixup solutions.","['Data Augmentation', 'Regression', 'Deep Learning']",[],"['Nora Schneider', 'Fernando Perez-Cruz']","['ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73661,Transparency & Explainability,MedSat: A Public Health Dataset for England Featuring Medical Prescriptions and Satellite Imagery,"As extreme weather events become more frequent, understanding their impact on human health becomes increasingly crucial. However, the utilization of Earth Observation to effectively analyze the environmental context in relation to health remains limited. This limitation is primarily due to the lack of fine-grained spatial and temporal data in public and population health studies, hindering a comprehensive understanding of health outcomes. Additionally, obtaining appropriate environmental indices across different geographical levels and timeframes poses a challenge. For the years 2019 (pre-COVID) and 2020 (COVID), we collected spatio-temporal indicators for all Lower Layer Super Output Areas in England. These indicators included: i) 111 sociodemographic features linked to health in existing literature, ii) 43 environmental point features (e.g., greenery and air pollution levels), iii) 4 seasonal composite satellite images each with 11 bands, and iv) prescription prevalence associated with five medical conditions (depression, anxiety, diabetes, hypertension, and asthma), opioids and total prescriptions. We combined these indicators into a single MedSat dataset, the availability of which presents an opportunity for the machine learning community to develop new techniques specific to public health. These techniques would address challenges such as handling large and complex data volumes, performing effective feature engineering on environmental and sociodemographic factors, capturing spatial and temporal dependencies in the models, addressing imbalanced data distributions, developing novel computer vision methods for health modeling based on satellite imagery, ensuring model explainability, and achieving generalization beyond the specific geographical region.",['public health; population health; medical prescriptions; satellite imagery; satellite data; SDGs'],[],"['Sanja Scepanovic', 'Ivica Obadic', 'Sagar Joglekar', 'Laura GIUSTARINI', 'Cristiano Nattero', 'Daniele Quercia', 'Xiao Xiang Zhu']","['Nokia Bell Labs', 'Technische Universität München', 'Expedia Group', 'RSS-Hydro', 'Università degli Studi di Genova', 'University College London, University of London', 'Technical University Munich']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73544,Transparency & Explainability,Data Portraits: Recording Foundation Model Training Data,"Foundation models are trained on increasingly immense and opaque datasets. Even while these models are now key in AI system building, it can be  difficult to answer the straightforward question: has the model already encountered a given example during training? We therefore propose a widespread adoption of Data Portraits: artifacts that record training data and allow for downstream inspection. First we outline the properties of such an artifact and discuss how existing solutions can be used to increase transparency. We then propose and implement a solution based on data sketching, stressing fast and space efficient querying. Using our tools, we document a popular language modeling corpus (The Pile) and a recently released code modeling dataset (The Stack). We show that our solution enables answering questions about test set leakage and model plagiarism. Our tool is lightweight and fast, costing only 3% of the dataset size in overhead. We release a live interface of our tools at https://dataportraits.org/ and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.","['natural language processing', 'data documentation', 'dataset curation', 'documentation practices']",[],"['Marc Marone', 'Benjamin Van Durme']","['Johns Hopkins University', 'Johns Hopkins University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72075,Transparency & Explainability,Unlocking Feature Visualization for Deep Network with MAgnitude Constrained Optimization,"Feature visualization has gained significant popularity as an explainability method, particularly after the influential work by Olah et al. in 2017. Despite its success, its widespread adoption has been limited due to issues in scaling to deeper neural networks and the reliance on tricks to generate interpretable images. Here, we describe MACO, a simple approach to address these shortcomings. It consists in optimizing solely an image's phase spectrum while keeping its magnitude constant to ensure that the generated explanations lie in the space of natural images. Our approach yields significantly better results -- both qualitatively and quantitatively -- unlocking efficient and interpretable feature visualizations for state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing to augment feature visualizations with spatial importance. Furthermore, we enable quantitative evaluation of feature visualizations by introducing 3 metrics: transferability, plausibility, and alignment with natural images. We validate our method on various applications and we introduce a website featuring MACO visualizations for all classes of the ImageNet dataset, which will be made available upon acceptance. Overall, our study unlocks feature visualizations for the largest, state-of-the-art classification networks without resorting to any parametric prior image model, effectively advancing a field that has been stagnating since 2017 (Olah et al, 2017).","['explainable AI', 'feature visualization', 'interpretability', 'optimization']",[],"['Thomas FEL', 'Thibaut Boissin', 'Victor Boutin', 'Agustin Martin Picard', 'Paul Novello', 'Julien Colin', 'Drew Linsley', 'Tom ROUSSEAU', 'Remi Cadene', 'Lore Goetschalckx', 'Laurent Gardes', 'Thomas Serre']","['Brown University', 'IRT Saint exupéry', 'Brown University', 'IRT Saint-Exupery', 'IRT Saint Exupery', 'Universidad de Alicante', 'Brown University', 'Société Nationale des Chemins de fer Français', 'LIP6', 'Brown University', 'Télécom ParisTech', 'Brown University']","[None, None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71378,Transparency & Explainability,Explain Any Concept: Segment Anything Meets Concept-Based Explanation,"EXplainable AI (XAI) is an essential topic to improve human understanding of deep neural networks (DNNs) given their black-box internals. For computer vision tasks, mainstream pixel-based XAI methods explain DNN decisions by identifying important pixels, and emerging concept-based XAI explore forming explanations with concepts (e.g., a head in an image). However, pixels are generally hard to interpret and sensitive to the imprecision of XAI methods, whereas “concepts” in prior works require human annotation or are limited to pre-defined concept sets. On the other hand, driven by large-scale pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promotable framework for performing precise and comprehensive instance segmentation, enabling automatic preparation of concept sets from a given image. This paper for the first time explores using SAM to augment concept-based XAI. We offer an effective and flexible concept-based explanation method, namely Explain Any Concept (EAC), which explains DNN decisions with any concept. While SAM is highly effective and offers an “out-of-the-box” instance segmentation, it is costly when being integrated into defacto XAI pipelines. We thus propose a lightweight per-input equivalent (PIE) scheme, enabling efficient explanation with a surrogate model. Our evaluation  over two popular datasets (ImageNet and COCO) illustrate the highly encouraging performance of EAC over commonly-used XAI methods.","['EXplainable AI', 'Machine Learning', 'Computer Vision']",[],"['Ao Sun', 'Yuanyuan Yuan', 'Shuai Wang']","['University of Science and Technology', 'Department of Computer Science and Engineering, The  University of Science and Technology', 'University of Science and Technology']","['Hong Kong', 'Hong Kong', 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/70925,Transparency & Explainability,Privacy Auditing with One (1) Training Run,"We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting. We demonstrate the effectiveness of our framework by applying it to DP-SGD, where we can achieve meaningful empirical privacy lower bounds by training only one model. In contrast, standard methods would require training hundreds of models.","['Differential privacy', 'membership inference attacks', 'privacy auditing']",[],"['Thomas Steinke', 'Milad Nasr', 'Matthew Jagielski']","['Google DeepMind', 'Google', 'Google']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74161,Transparency & Explainability,"Reproducibility study of 'Proto2Proto: Can you recognise the car, the way I do?'","Scope of Reproducibility — This paper analyses the reproducibility of the study Proto2Proto: Can you recognize the car, the way I do? The main contributions and claims of the study are: 1) Using Proto2Proto, a shallower student model is more faithful to the teacher in terms of interpretability than a baseline student model while also showing the same or better accuracy; 2) Global Explanation loss forces student prototypes to be close to teacher prototypes; 3) Patch‐Prototype Correspondence loss enforces the local representations of the student to be similar to those of the teacher; 4) The proposed evaluation metrics determine the faithfulness of the student to the teacher in terms of interpretability. Methodology — A public code repository was available for the paper, which provided a working but incomplete and minimally documented codebase. With some modifications we were able to carry out the experiments that were best supported by the codebase. We spent a total of 60 computational GPU hours on reproduction. Results — The results we were able to produce support claim 1, albeit weakly. Further results are in line with the paper, but we found them to go against claim 3. In addition, we carried out a theoretical analysis which provides support for claim 4. Finally, we were unable to carry out our intended experiment to verify claim 2. What was easy — The original paper was clearly structured and understandable. The experiments for which configurations were provided were simple to conduct. What was difficult — The public codebase contained minimal documentation. Moreover, the use of variable names did not correspond between the code and the paper. Furthermore, the codebase lacked elements vital to reproducing some experiments. Another significant constraint were the computational requirements needed to reproduce the original experiments. Finally, the code required to reproduce one of the visualizations was not provided. Communication with original authors — We contacted the authors to ask for trained model weights and missing hyperparameters for several experiments. We did not receive aresponse.","['Reproducibility', 'Interpretability', 'Proto2Proto', 'ProtoPNet', 'Prototypes', 'Knowledge distillation']",[],"['Gerson de Kleuver', 'Wenhua Hu', 'Bram Veenman']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70246,Transparency & Explainability,HASSOD: Hierarchical Adaptive Self-Supervised Object Detection,"The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.","['self-supervised learning', 'object detection']",[],"['Shengcao Cao', 'Dhiraj Joshi', 'Liangyan Gui', 'Yu-Xiong Wang']","['University of Illinois at Urbana-Champaign', 'IBM Research', 'UIUC', 'School of Computer Science, Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70746,Transparency & Explainability,"MultiMoDN—Multimodal, Multi-Task, Interpretable Modular Networks","Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance.","['Deep Learning', 'Multimodal Learning', 'Multi-task learning', 'Missingness', 'Interpretability']",[],"['Vinitra Swamy', 'Malika Satayeva', 'Jibril Frej', 'Thierry Bossy', 'Thijs Vogels', 'Martin Jaggi', 'Tanja Käser', 'Mary-Anne Hartley']","['Swiss Federal Institute of Technology Lausanne', 'KTH Royal Institute of Technology', 'EPFL - EPF Lausanne', 'EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'EPFL', 'School of Computer and Communication Sciences, EPFL - EPF Lausanne', 'Yale University']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72800,Transparency & Explainability,Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance,"Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associated with various modalities and symmetry groups, we derive a set of 5 guidelines to allow users and developers of interpretability methods to produce robust explanations.","['interpretability', 'explainability', 'robustness', 'invariance', 'equivariance', 'geometric deep learning']",[],"['Jonathan Crabbé', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72111,Transparency & Explainability,Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization,"A hallmark property of explainable AI models is the ability to teach other agents, communicating knowledge of how to perform a task. While Large Language Models (LLMs) perform complex reasoning by generating explanations for their predictions, it is unclear whether they also make good teachers for weaker agents. To address this, we consider a student-teacher framework between two LLM agents and study if, when, and how the teacher should intervene with natural language explanations to improve the student’s performance. Since communication is expensive, we define a budget such that the teacher only communicates explanations for a fraction of the data, after which the student should perform well on its own. We decompose the teaching problem along four axes: (1) if teacher’s test time in- tervention improve student predictions, (2) when it is worth explaining a data point, (3) how the teacher should personalize explanations to better teach the student, and (4) if teacher explanations also improve student performance on future unexplained data. We first show that teacher LLMs can indeed intervene on student reasoning to improve their performance. Next, inspired by the Theory of Mind abilities of effective teachers, we propose building two few-shot mental models of the student. The first model defines an Intervention Function that simulates the utility of an intervention, allowing the teacher to intervene when this utility is the highest and improving student performance at lower budgets. The second model enables the teacher to personalize explanations for a particular student and outperform unpersonalized teachers. We also demonstrate that in multi-turn interactions, teacher explanations generalize and learning from explained data improves student performance on future unexplained data. Finally, we also verify that misaligned teachers can lower student performance to random chance by intentionally misleading them.","['Language Models', 'Reasoning', 'Explanations']",[],"['Swarnadeep Saha', 'Peter Hase', 'Mohit Bansal']","['Department of Computer Science, University of North Carolina, Chapel Hill', 'University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70668,Transparency & Explainability,Auxiliary Losses for Learning Generalizable Concept-based Models,"The increasing use of neural networks in various applications has lead to increasing apprehensions, underscoring the necessity to understand their operations beyond mere final predictions. As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction. CBMs essentially limit the latent space of a model to human-understandable high-level concepts.  While beneficial, CBMs have been reported to often learn irrelevant concept representations that consecutively damage model performance. To overcome the performance trade-off, we propose a cooperative-Concept Bottleneck Model (coop-CBM). The concept representation of our model is particularly meaningful when fine-grained concept labels are absent. Furthermore, we introduce the concept orthogonal loss (COL) to encourage the separation between the concept representations and to reduce the intra-concept distance. This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL. We also study the performance of coop-CBM models under various distributional shift settings. We show that our proposed method achieves higher accuracy in all distributional shift settings even compared to the black-box models with the highest concept accuracy.","['Interpretability', 'concept bottleneck models', 'explainability']",[],"['Ivaxi Sheth', 'Samira Ebrahimi Kahou']","['CISPA, saarland university, saarland informatics campus', 'École de technologie supérieure']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70379,Transparency & Explainability,Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability,"With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by identifying features critical to model predictions; however, prior work has shown that these explanations may not be faithful, in that they incorrectly attribute high importance to features that are unimportant or non-discriminative for the underlying task. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we identify a key reason for the lack of faithfulness of feature attributions: the lack of robustness of the underlying black-box models, especially the erasure of unimportant distractor features in the input. To address this issue, we propose Distractor Erasure Tuning (DiET), a method that adapts black-box models to be robust to distractor erasure, thus providing discriminative and faithful attributions. This strategy naturally combines the ease-of-use of post hoc explanations with the faithfulness of inherently interpretable models. We perform extensive experiments on semi-synthetic and real-world datasets, and show that DiET produces models that (1) closely approximate the original black-box models they are intended to explain, and (2) yield explanations that match approximate ground truths available by construction.","['Machine Learning Explainability', 'Machine Learning Interpretability']",[],"['Suraj Srinivas', 'Himabindu Lakkaraju']","['School of Engineering and Applied Sciences, Harvard University', 'Harvard University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73908,Transparency & Explainability,Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond,OpenReview link not found.,OpenReview link not found.,[],,,,,,,,,,
https://nips.cc/virtual/2023/poster/72918,Transparency & Explainability,Fairness Aware Counterfactuals for Subgroups,"In this work, we present Fairness Aware Counterfactuals for Subgroups (FACTS), a framework for auditing subgroup fairness through counterfactual explanations. We start with revisiting (and generalizing) existing notions and introducing new, more refined notions of subgroup fairness. We aim to (a) formulate different aspects of the difficulty of individuals in certain subgroups to achieve recourse, i.e. receive the desired outcome, either at the micro level, considering members of the subgroup individually, or at the macro level, considering the subgroup as a whole, and (b) introduce notions of subgroup fairness that are robust, if not totally oblivious, to the cost of achieving recourse. We accompany these notions with an efficient, model-agnostic, highly parameterizable, and explainable framework for evaluating subgroup fairness. We demonstrate the advantages, the wide applicability, and the efficiency of our approach through a thorough experimental evaluation on different benchmark datasets.","['subgroup fairness', 'recourse', 'counterfactual explanations']",[],"['Loukas Kavouras', 'Konstantinos Tsopelas', 'Giorgos Giannopoulos', 'Dimitris Sacharidis', 'Eleni Psaroudaki', 'Nikolaos Theologitis', 'Dimitrios Rontogiannis', 'Dimitris Fotakis', 'Ioannis Emiris']","['IMIS - ""Athena"" Research Center', 'National Technical University of Athens', 'Athena Research Center', 'ULB', 'IMIS - ""Athena"" Research Center', 'IMIS - ""Athena"" Research Center', 'IMIS - ""Athena"" Research Center', 'Archimedes/Athena RC ', 'Athena Research Center,']","[None, None, None, None, None, None, None, None, 'Greece']",,,,,,,
https://nips.cc/virtual/2023/poster/73502,Transparency & Explainability,DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection,"A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detection, called \textit{DeepfakeBench}, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evaluation metrics and protocols to promote transparency and reproducibility.  Featuring an extensible, modular-based codebase, \textit{DeepfakeBench} contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations.  Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (\eg, data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at \url{https://github.com/SCLBD/DeepfakeBench}.","['Deepfake Detection', 'Benchmark']",[],"['Zhiyuan Yan', 'Yong Zhang', 'Xinhang Yuan', 'Siwei Lyu', 'Baoyuan Wu']","['Tencent YouTu Lab', 'Tencent AI Lab', 'Tongji University', 'State University of New York, Buffalo', 'The Chinese University of , Shenzhen']","[None, None, None, None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/73480,Transparency & Explainability,LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting,"Road traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors in California with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract data insights, benchmark well-known baselines in terms of their performance and efficiency, and identify challenges as well as opportunities for future research. We release the datasets and baseline implementations at: https://github.com/liuxu77/LargeST.",['Traffic Forecasting Benchmark Dataset'],[],"['Xu Liu', 'Yutong Xia', 'Yuxuan Liang', 'Junfeng Hu', 'Yiwei Wang', 'LEI BAI', 'Chao Huang', 'Zhenguang Liu', 'Bryan Hooi', 'Roger Zimmermann']","['National University of', 'National University of', 'The  University of Science and Technology (Guangzhou)', 'National University of', 'national university of singaore, National University of', 'Shanghai AI Laboratory', 'University of', 'Zhejiang University', 'National University of', 'National University of']","['Singapore', 'Singapore', 'Hong Kong', 'Singapore', 'Singapore', None, 'Hong Kong', None, 'Singapore', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/72444,Transparency & Explainability,Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI,"Information Pursuit (IP) is a classical active testing algorithm for predicting an output by sequentially and greedily querying the input in order of information gain. However, IP is computationally intensive since it involves estimating mutual information in high-dimensional spaces. This paper explores Orthogonal Matching Pursuit (OMP) as an alternative to IP for greedily selecting the queries. OMP is a classical signal processing algorithm for sequentially encoding a signal in terms of dictionary atoms chosen in order of correlation gain. In each iteration, OMP selects the atom that is most correlated with the signal residual (the signal minus its reconstruction thus far). Our first contribution is to establish a fundamental connection between IP and OMP, where we prove that IP with random projections of dictionary atoms as queries ``almost'' reduces to OMP, with the difference being that IP selects atoms in order of normalized correlation gain. We call this version IP-OMP and present simulations indicating that this difference does not have any appreciable effect on the sparse code recovery rate of IP-OMP compared to that of OMP for random Gaussian dictionaries. Inspired by this connection, our second contribution is to explore the utility of IP-OMP for generating explainable predictions, an area in which IP has recently gained traction. More specifically, we propose a simple explainable AI algorithm which encodes an image as a sparse combination of semantically meaningful dictionary atoms that are defined as text embeddings of interpretable concepts. The final prediction is made using the weights of this sparse combination, which serve as an explanation. Empirically, our proposed algorithm is not only competitive with existing explainability methods but also computationally less expensive.","['Information Maximization', 'Sparse Coding', 'Orthogonal Matching Pursuit', 'Explainable AI', 'Information Pursuit']",[],"['Aditya Chattopadhyay', 'Rene Vidal']","['Johns Hopkins University', 'Johns Hopkins University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72499,Transparency & Explainability,Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization,"Discovering object-centric representations from images has the potential to greatly improve the robustness, sample efficiency and interpretability of machine learning algorithms. Current works on multi-object images typically follow a generative approach that optimizes for input reconstruction and fail to scale to real-world datasets despite significant increases in model capacity. We address this limitation by proposing a novel method that leverages feature connectivity to cluster neighboring pixels likely to belong to the same object. We further design two object-centric regularization terms to refine object representations in the latent space, enabling our approach to scale to complex real-world images. Experimental results on simulated, real-world, complex texture and common object images demonstrate a substantial improvement in the quality of discovered objects compared to state-of-the-art methods, as well as the sample efficiency and generalizability of our approach. We also show that the discovered object-centric representations can accurately predict key object properties in downstream tasks, highlighting the potential of our method to advance the field of multi-object representation learning.","['Object-Centric Learning', 'Multi-Object Representation Learning']",[],"['Alex Foo', 'Wynne Hsu', 'Mong-Li Lee']","['National University of', 'National University of', 'National University of']","['Singapore', 'Singapore', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/72212,Transparency & Explainability,D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion,"The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness.","['Explainability', 'Graph Neural Network', 'Diffusion Model']",[],"['Jialin Chen', 'Shirley Wu', 'Abhijit Gupta', 'Zhitao Ying']","['Yale University', 'Computer Science Department, Stanford University', 'Yale University', 'Yale University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74172,Transparency & Explainability,"Reproducibility Study of ""Label-Free Explainability for Unsupervised Models""","Scope of Reproducibility In this work, we evaluate the reproducibility of the paper Label-Free Explainability for Unsupervised Models by Crabbe and van der Schaar. Our goal is to reproduce the paper's four main claims in a label-free setting:(1) feature importance scores determine salient features of a model's input, (2) example importance scores determine salient training examples to explain a test example, (3) interpretability of saliency maps is hard for disentangled VAEs, (4) distinct pretext tasks don’t have interchangeable representations. Methodology The authors of the paper provide an implementation in PyTorch for their proposed techniques and experiments. We reuse and extend their code for our additional experiments. Our reproducibility study comes at a total computational cost of 110 GPU hours, using an NVIDIA Titan RTX. Results We reproduced the original paper's work through our experiments. We find that the main claims of the paper largely hold. We assess the robustness and generalizability of some of the claims, through our additional experiments. In that case, we find that one claim is not generalizable and another is not reproducible for the graph dataset. What was easy The original paper is well-structured. The code implementation is well-organized and with clear instructions on how to get started. This was helpful to understand the paper's work and begin experimenting with their proposed methods. What was difficult We found it difficult to extrapolate some of the authors' proposed techniques to datasets other than those used by them.  Also, we were not able to reproduce the results for one of the experiments. We couldn't find the exact reason for it by running explorative experiments due to time and resource constraints. Communication with original authors We reached out to the authors once about our queries regarding one experimental setup and to understand the assumptions and contexts of some sub-claims in the paper. We received a prompt response which satisfied most of our questions.","['Reproducibility', 'Feature Importance', 'Example Importance', 'Disentangled VAEs', 'Label-Free', 'Unsupervised', 'Post-Hoc Explainability']",[],"['Valentinos Pariza', 'Avik Pal', 'Madhura Pawar', 'Quim Serra Faber']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72338,Transparency & Explainability,VeriX: Towards Verified Explainability of Deep Neural Networks,"We present **VeriX** (**Veri**fied e**X**plainability), a system for producing *optimal robust explanations* and generating *counterfactuals* along decision boundaries of machine learning models. We build such explanations and counterfactuals iteratively using constraint solving techniques and a heuristic based on feature-level sensitivity ranking. We evaluate our method on image recognition benchmarks and a real-world scenario of autonomous aircraft taxiing.","['trustworthy machine learning', 'deep neural networks', 'explainability', 'interpretability', 'formal methods', 'automated verification']",[],"['Haoze Wu', 'Clark Barrett']","['Stanford University', 'Stanford University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71494,Transparency & Explainability,Multi-Agent First Order Constrained Optimization in Policy Space,"In the realm of multi-agent reinforcement learning (MARL), achieving high performance is crucial for a successful multi-agent system. Meanwhile, the ability to avoid unsafe actions is becoming an urgent and imperative problem to solve for real-life applications. Whereas, it is still challenging to develop a safety-aware method for multi-agent systems in MARL. In this work, we introduce a novel approach called Multi-Agent First Order Constrained Optimization in Policy Space (MAFOCOPS), which effectively addresses the dual objectives of attaining satisfactory performance and enforcing safety constraints. Using data generated from the current policy, MAFOCOPS first finds the optimal update policy by solving a constrained optimization problem in the nonparameterized policy space. Then, the update policy is projected back into the parametric policy space to achieve a feasible policy. Notably, our method is first-order in nature, ensuring the ease of implementation, and exhibits an approximate upper bound on the worst-case constraint violation. Empirical results show that our approach achieves remarkable performance while satisfying safe constraints on several safe MARL benchmarks.","['Safe Multi-agent Reinforcement Learning', 'constrained policy optimisation', 'first-order optimisation']",[],"['Youpeng Zhao', 'Yaodong Yang', 'Zhenbo Lu', 'Wengang Zhou', 'Houqiang Li']","['University of Science and Technology of', 'Peking University', 'Institute of Artificial Intelligence, Hefei Comprehensive National Science Center', 'University of Science and Technology of', 'University of Science and Technology of']","['China', None, None, 'China', 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/71516,Transparency & Explainability,Attacks on Online Learners: a Teacher-Student Analysis,"Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of our theoretical analysis. Our findings show that greedy attacks can be extremely efficient, especially when data stream in small batches.","['Adversarial attacks', 'data poisoning', 'online learning', 'optimal control', 'teacher-student setup', 'solvable model']",[],"['Sebastian Goldt', 'Guido Sanguinetti']","['SISSA', 'University of Edinburgh']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71053,Transparency & Explainability,Zero-Shot Anomaly Detection via Batch Normalization,"Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the ""new normal,"" has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.","['deep anomaly detection', 'zero-shot learning', 'batch normalization']",[],"['Aodong Li', 'Chen Qiu', 'Marius Kloft', 'Padhraic Smyth', 'Maja Rudolph', 'Stephan Mandt']","['University of California, Irvine', 'Robert Bosch LLC, A', 'RPTU Kaiserslautern-Landau', 'University of California, Irvine', 'Robert Bosch GmbH, Bosch', 'University of California, Irvine']","[None, 'US', None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71056,Transparency & Explainability,On the Role of Randomization in Adversarially Robust Classification,"Deep neural networks are known to be vulnerable to small adversarial perturbations in test data. To defend against adversarial attacks, probabilistic classifiers have been proposed as an alternative to deterministic ones. However, literature has conflicting findings on the effectiveness of probabilistic classifiers in comparison to deterministic ones. In this paper, we clarify the role of randomization in building adversarially robust classifiers. Given a base hypothesis set of deterministic classifiers, we show the conditions under which a randomized ensemble outperforms the hypothesis set in adversarial risk, extending previous results. Additionally, we show that for any probabilistic binary classifier (including randomized ensembles), there exists a deterministic classifier that outperforms it. Finally, we give an explicit description of the deterministic hypothesis set that contains such a deterministic classifier for many types of commonly used probabilistic classifiers, *i.e.* randomized ensembles and parametric/input noise injection.","['adversarial attacks', 'robustness', 'adversarial', 'attacks', 'deep learning', 'randomization', 'randomized ensembles']",[],"['Lucas Gnecco Heredia', 'Muni Sreenivas Pydi', 'Laurent Meunier', 'Yann Chevaleyre']","[', Université Paris-Dauphine (Paris IX)', 'Université Paris Dauphine - PSL', 'Payflows', 'Université Paris-Dauphine (Paris IX)']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71079,Transparency & Explainability,IBA: Towards Irreversible Backdoor Attacks in Federated Learning,"Federated learning (FL) is a distributed learning approach that enables machine learning models to be trained on decentralized data without compromising end devices' personal, potentially sensitive data. However, the distributed nature and uninvestigated data intuitively introduce new security vulnerabilities, including backdoor attacks. In this scenario, an adversary implants backdoor functionality into the global model during training, which can be activated to cause the desired misbehaviors for any input with a specific adversarial pattern. Despite having remarkable success in triggering and distorting model behavior, prior backdoor attacks in FL often hold impractical assumptions, limited imperceptibility, and durability. Specifically, the adversary needs to control a sufficiently large fraction of clients or know the data distribution of other honest clients. In many cases, the trigger inserted is often visually apparent, and the backdoor effect is quickly diluted if the adversary is removed from the training process. To address these limitations, we propose a novel backdoor attack framework in FL, the Irreversible Backdoor Attack (IBA), that jointly learns the optimal and visually stealthy trigger and then gradually implants the backdoor into a global model. This approach allows the adversary to execute a backdoor attack that can evade both human and machine inspections. Additionally, we enhance the efficiency and durability of the proposed attack by selectively poisoning the model's parameters that are least likely updated by the main task's learning process and constraining the poisoned model update to the vicinity of the global model. Finally, we evaluate the proposed attack framework on several benchmark datasets, including MNIST, CIFAR-10, and Tiny ImageNet, and achieved high success rates while simultaneously bypassing existing backdoor defenses and achieving a more durable backdoor effect compared to other backdoor attacks. Overall, IBA offers a more effective, stealthy, and durable approach to backdoor attacks in FL. The code associated with this paper is available on [GitHub](https://github.com/sail-research/iba).","['Backdoor Attacks', 'Federated Learning', 'Durability', 'Imperceptibility', 'Stealthiness']",[],"['Dung Thuy Nguyen', 'Tuan Minh Nguyen', 'Anh Tuan Tran', 'Khoa D Doan', 'KOK SENG WONG']","['Vanderbilt University', 'VinUniversity', 'VinAI Research', 'VinUniversity', 'VinUniversity']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71158,Transparency & Explainability,Asymmetric Certified Robustness via Feature-Convex Neural Networks,"Real-world adversarial attacks on machine learning models often feature an asymmetric structure wherein adversaries only attempt to induce false negatives (e.g., classify a spam email as not spam). We formalize the asymmetric robustness certification problem and correspondingly present the feature-convex neural network architecture, which composes an input-convex neural network (ICNN) with a Lipschitz continuous feature map in order to achieve asymmetric adversarial robustness. We consider the aforementioned binary setting with one ""sensitive"" class, and for this class we prove deterministic, closed-form, and easily-computable certified robust radii for arbitrary $\ell_p$-norms. We theoretically justify the use of these models by characterizing their decision region geometry, extending the universal approximation theorem for ICNN regression to the classification setting, and proving a lower bound on the probability that such models perfectly fit even unstructured uniformly distributed data in sufficiently high dimensions. Experiments on Malimg malware classification and subsets of the MNIST, Fashion-MNIST, and CIFAR-10 datasets show that feature-convex classifiers attain substantial certified $\ell_1$, $\ell_2$, and $\ell_{\infty}$-radii while being far more computationally efficient than competitive baselines.","['asymmetric certified robustness', 'input-convex neural networks']",[],"['Samuel Pfrommer', 'Brendon G. Anderson', 'Julien Piet', 'Somayeh Sojoudi']","['University of California, Berkeley', 'University of California, Berkeley', 'Electrical Engineering & Computer Science Department, University of California, Berkeley', 'University of California Berkeley']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71785,Transparency & Explainability,Rethinking the Backward Propagation for Adversarial Transferability,"Transfer-based attacks generate adversarial examples on the surrogate model, which can mislead other black-box models without access, making it promising to attack real-world applications. Recently, several works have been proposed to boost adversarial transferability, in which the surrogate model is usually overlooked. In this work, we identify that non-linear layers (e.g., ReLU, max-pooling, etc.) truncate the gradient during backward propagation, making the gradient w.r.t. input image imprecise to the loss function. We hypothesize and empirically validate that such truncation undermines the transferability of adversarial examples. Based on these findings, we propose a novel method called Backward Propagation Attack (BPA) to increase the relevance between the gradient w.r.t. input image and loss function so as to generate adversarial examples with higher transferability. Specifically, BPA adopts a non-monotonic function as the derivative of ReLU and incorporates softmax with temperature to smooth the derivative of max-pooling, thereby mitigating the information loss during the backward propagation of gradients. Empirical results on the ImageNet dataset demonstrate that not only does our method substantially boost the adversarial transferability, but it is also general to existing transfer-based attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.","['Adversarial examples', 'Convolutional neural networks', 'Adversarial transferability', 'Backward propagation']",[],"['Xiaosen Wang', 'Kangheng Tong', 'Kun He']","['Huawei Technologies Ltd.', 'Huazhong University of Science and Technology', 'Huazhong University of Sceince and Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71817,Transparency & Explainability,Are aligned neural networks adversarially aligned?,"Large language models are now tuned to align with the goals of their creators, namely to be ""helpful and harmless."" These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study adversarial alignment, and ask to what extent these models remain aligned when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.","['Adversarial examples', 'large language models', 'alignment']",[],"['Nicholas Carlini', 'Milad Nasr', 'Christopher A. Choquette-Choo', 'Matthew Jagielski', 'Irena Gao', 'Pang Wei Koh', 'Daphne Ippolito', 'Florian Tramèr', 'Ludwig Schmidt']","['Google', 'Google', 'Google Research, Brain team', 'Google', 'Stanford University', 'University of Washington', 'School of Engineering and Applied Science, University of Pennsylvania', 'ETHZ - ETH Zurich', 'University of Washington']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71818,Transparency & Explainability,Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks,"Contrastive vision-language representation learning has achieved state-of-the-art performance for zero-shot classification, by learning from millions of image-caption pairs crawled from the internet. However, the massive data that powers large multimodal models such as CLIP, makes them extremely vulnerable to various types of targeted data poisoning and backdoor attacks. Despite this vulnerability, robust contrastive vision-language pre-training against such attacks has remained unaddressed. In this work, we propose RoCLIP, the first effective method for robust pre-training multimodal vision-language models against targeted data poisoning and backdoor attacks. RoCLIP effectively breaks the association between poisoned image-caption pairs by considering a relatively large and varying pool of random captions, and matching every image with the text that is most similar to it in the pool instead of its own caption, every few epochs.It also leverages image and text augmentations to further strengthen the defense and improve the performance of the model. Our extensive experiments show that RoCLIP renders state-of-the-art targeted data poisoning and backdoor attacks ineffective during pre-training CLIP models. In particular, RoCLIP decreases the success rate for targeted data poisoning attacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while improving the model's linear probe performance by 10% and maintains a similar zero shot performance compared to CLIP. By increasing the frequency of matching, RoCLIP is able to defend strong attacks, which add up to 1% poisoned examples to the data, and successfully maintain a low attack success rate of 12.5%, while trading off the performance on some tasks.","['Contrastive Learning', 'Adversarial Learning', 'Model Robustness']",[],"['Wenhan Yang', 'Jingdong Gao', 'Baharan Mirzasoleiman']","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71827,Transparency & Explainability,Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities,"We investigate safe multi-agent reinforcement learning, where agents seek to collectively maximize an aggregate sum of local objectives while satisfying their own safety constraints. The objective and constraints are described by general utilities, i.e., nonlinear functions of the long-term state-action occupancy measure, which encompass broader decision-making goals such as risk, exploration, or imitations. The exponential growth of the state-action space size with the number of agents presents challenges for global observability, further exacerbated by the global coupling arising from agents' safety constraints. To tackle this issue, we propose a primal-dual method utilizing shadow reward and $\kappa$-hop neighbor truncation under a form of correlation decay property, where $\kappa$ is the communication radius. In the exact setting, our algorithm converges to a first-order stationary point (FOSP) at the rate of $\mathcal{O}\left(T^{-2/3}\right)$. In the sample-based setting, we demonstrate that, with high probability, our algorithm requires $\widetilde{\mathcal{O}}\left(\epsilon^{-3.5}\right)$ samples to achieve an $\epsilon$-FOSP with an approximation error of $\mathcal{O}(\phi_0^{2\kappa})$, where $\phi_0\in (0,1)$. Finally, we demonstrate the effectiveness of our model through extensive numerical experiments.","['Reinforcement Learning Theory', 'Safe reinforcement learning', 'Multi-agent reinforcement learning']",[],"['Donghao Ying', 'Yunkai Zhang', 'Yuhao Ding', 'Alec Koppel']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California Berkeley', 'J.P. Morgan Chase']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71835,Transparency & Explainability,Label-Only Model Inversion Attacks via Knowledge Transfer,"In a model inversion (MI) attack, an adversary abuses access to a machine learning (ML) model to infer and reconstruct private training data. Remarkable progress has been made in the white-box and black-box setups, where the adversary has access to the complete model or the model's soft output respectively. However, there is very limited study in the most challenging but practically important setup: Label-only MI attacks, where the adversary only has access to the model's predicted  label (hard label) without confidence scores nor any other model information. In this work, we propose LOKT, a novel approach for label-only MI attacks. Our idea is based on transfer of knowledge from the opaque target model to  surrogate models. Subsequently, using these surrogate models, our approach can harness advanced white-box attacks. We propose knowledge transfer based on generative modelling, and introduce a new model, Target model-assisted ACGAN (T-ACGAN), for effective knowledge transfer. Our method casts the challenging label-only MI into the more tractable white-box setup. We provide analysis to support that surrogate models based on our approach serve as effective proxies for the target model for MI. Our experiments show that our method significantly outperforms existing SOTA Label-only MI attack by more than 15% across all MI benchmarks. Furthermore, our method compares favorably in terms of query budget. Our study highlights rising privacy threats for  ML models even when minimal information (i.e.,  hard labels) is exposed. Our study highlights rising privacy threats for  ML models even when minimal information (i.e.,  hard labels) is exposed. Our code, demo, models and reconstructed data are available at our project page: https://ngoc-nguyen-0.github.io/lokt/","['Model Inversion attacks', 'Generative models', 'Surrogate models', 'Knowledge transfer']",[],"['Ngoc-Bao Nguyen', 'Keshigeyan Chandrasegaran', 'Milad Abdollahzadeh', 'Ngai-man Cheung']","['University of Technology and Design', 'Stanford University', 'University of Technology and Design', 'University of Technology and Design']","['Singapore', None, 'Singapore', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/71841,Transparency & Explainability,IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers,"Data augmentation has been proven effective for training high-accuracy convolutional neural network classifiers by preventing overfitting. However, building deep neural networks in real-world scenarios requires not only high accuracy on clean data but also robustness when data distributions shift. While prior methods have proposed that there is a trade-off between accuracy and robustness, we propose IPMix, a simple data augmentation approach to improve robustness without hurting clean accuracy. IPMix integrates three levels of data augmentation (image-level, patch-level, and pixel-level) into a coherent and label-preserving technique to increase the diversity of training data with limited computational overhead. To further improve the robustness, IPMix introduces structural complexity at different levels to generate more diverse images and adopts the random mixing method for multi-scale information fusion. Experiments demonstrate that IPMix outperforms state-of-the-art corruption robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also significantly improves the other safety measures, including robustness to adversarial perturbations, calibration, prediction consistency, and anomaly detection, achieving state-of-the-art or comparable results on several benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.","['data augmentation', 'robustness', 'safety']",[],"['Zhenglin Huang', 'Xiaoan Bao', 'Na Zhang', 'Qingqi Zhang', 'Xiao mei Tu', 'Biao Wu', 'Xi Yang']","['Zhejiang Sci-Tech University', 'Zhejiang Sci-Tech University', 'Zhejiang Sci-Tech University', 'Yamaguchi University', 'Zhejiang Guangsha Vocational and Technical University of construction', 'Zhejiang Sci-Tech University', 'University of Science and Technology of']","[None, None, None, None, None, None, 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/71879,Transparency & Explainability,Understanding and Improving Ensemble Adversarial Defense,"The strategy of ensemble has become popular in adversarial defense, which trains multiple base classifiers to defend against adversarial attacks in a cooperative manner. Despite the empirical success, theoretical explanations on why an ensemble of adversarially trained classifiers is more robust than single ones remain unclear. To fill in this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense,  demonstrating a provable 0-1 loss reduction on challenging sample sets in adversarial defense scenarios. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates to different base classifiers adversarial examples that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. Being tested over various existing ensemble adversarial defense techniques,  iGAT is capable of boosting their performance by up to 17\%  evaluated using  CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks.","['adversarial defense', 'ensemble diversity', 'robustness', 'curvature']",[],"['Yian Deng', 'Tingting Mu']","['University of Manchester', 'University of Manchester']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71024,Transparency & Explainability,Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms,"Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian process to ensure safety with a deep RL algorithm to maximize the reward. Finally, we demonstrate that our proposed algorithm achieves better performance than state-of-the-art algorithms on grid-world and Safety Gym benchmarks without violating any safety constraints, even during training.","['Reinforcement Learning', 'Safety Exploration']",[],"['Akifumi Wachi', 'Wataru Hashimoto', 'Xun Shen', 'Kazumune Hashimoto']","['LINE', 'Osaka University', 'Osaka University', 'Osaka University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71372,Transparency & Explainability,GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection,"Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data---analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose GAIA, a simple and effective approach that incorporates Gradient Abnormality Inspection and Aggregation.  The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to advanced post-hoc methods.","['out-of-distribution detection', 'distribution shifts', 'attribution gradients']",[],"['Junjie Li', 'Xiaoyang Qu', 'Jianzong Wang', 'Jing Xiao']","['Pingan Technology', 'Pingan Technology', 'Pingan Technology', 'Pingan Group']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71380,Transparency & Explainability,"Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly","The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 10 popular substitute/victim models on ImageNet. New insights about the effectiveness of these methods are gained and guidelines for future evaluations are provided.","['adversarial examples', 'adversarial transferability', 'black-box attack']",[],"['Qizhang Li', 'Yiwen Guo', 'Wangmeng Zuo', 'Hao Chen']","['Harbin Institute of Technology', 'ByteDance', 'Harbin Institute of Technology', 'University of California, Davis']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71384,Transparency & Explainability,Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning,"Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments.","['deep reinforcement learning', 'model uncertainty', 'safety', 'risk-averse', 'distributionally robust']",[],"['James Queeney', 'Mouhacine Benosman']","['Mitsubishi Electric Research Labs', 'Mitsubishi Electric Research Labs']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72056,Transparency & Explainability,Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability,"Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberately mislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic adversarial samples. By exploiting a gradient guided by a diffusion model, Diff-PGD ensures that adversarial samples remain close to the original data distribution while maintaining their effectiveness. Moreover, our framework can be easily customized for specific tasks such as digital attacks, physical-world attacks, and style-based attacks. Compared with existing methods for generating natural-style adversarial samples, our framework enables the separation of optimizing adversarial loss from other surrogate losses (e.g. content/smoothness/style loss), making it more stable and controllable. Finally, we demonstrate that the samples generated using Diff-PGD have better transferability and anti-purification power than traditional gradient-based methods.","['Robustness', 'Adversarial Samples', 'Diffusion Model']",[],"['Haotian Xue', 'Alexandre Araujo', 'Bin Hu', 'Yongxin Chen']","['Institute of Technology', 'New York University', 'University of Illinois, Urbana Champaign', 'Institute of Technology']","['Georgia', None, None, 'Georgia']",,,,,,,
https://nips.cc/virtual/2023/poster/72092,Transparency & Explainability,Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,"We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset. We demonstrate efficacy of our attack when unlearning is performed via retraining from scratch, the idealized setting of machine unlearning which other efficient methods attempt to emulate, as well as against the approximate unlearning approach of Graves et al. (2021).","['Machine unlearning', 'new attack vector', 'Camouflaging poisoning attacks']",[],"['Jimmy Z. Di', 'Jack Douglas', 'Jayadev Acharya', 'Gautam Kamath', 'Ayush Sekhari']","['University of Waterloo', 'University of Waterloo', 'Cornell University', 'University of Waterloo', 'Massachusetts Institute of Technology']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72102,Transparency & Explainability,Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability,"The transferability of adversarial perturbations provides an effective shortcut for black-box attacks. Targeted perturbations have greater practicality but are more difficult to transfer between models. In this paper, we experimentally and theoretically demonstrated that neural networks trained on the same dataset have more consistent performance in High-Sample-Density-Regions (HSDR) of each class instead of low sample density regions. Therefore, in the target setting, adding perturbations towards HSDR of the target class is more effective in improving transferability. However, density estimation is challenging in high-dimensional scenarios. Further theoretical and experimental verification demonstrates that easy samples with low loss are more likely to be located in HSDR. Perturbations towards such easy samples in the target class can avoid density estimation for HSDR location. Based on the above facts, we verified that adding perturbations to easy samples in the target class improves targeted adversarial transferability of existing attack methods. A generative targeted attack strategy named Easy Sample Matching Attack (ESMA) is proposed, which has a higher success rate for targeted attacks and outperforms the SOTA generative method. Moreover, ESMA requires only $5\%$ of the storage space and much less computation time comparing to the current SOTA, as ESMA attacks all classes with only one model instead of seperate models for each class. Our code is available at https://github.com/gjq100/ESMA",['Adversarial Attacks; Generative Attack; Transferable Targeted Attack'],[],"['Junqi Gao', 'Biqing Qi', 'Yao Li', 'Zhichang Guo', 'Dong Li', 'Yuming Xing', 'Dazhi Zhang']","['Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72104,Transparency & Explainability,On the Adversarial Robustness of Out-of-distribution Generalization Models,"Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. Interestingly, we find that existing OOD generalization methods are vulnerable to adversarial attacks. This motivates us to study OOD adversarial robustness. We first present theoretical analyses of OOD adversarial robustness in two different complementary settings. Motivated by the theoretical results, we design two algorithms to improve the OOD adversarial robustness. Finally, we conduct experiments to validate the effectiveness of our proposed algorithms.","['Adversarial Robustness', 'Out-of-distribution Generalization']",[],"['Xin Zou', 'Weiwei Liu']","['Wuhan University', 'Wuhan University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72158,Transparency & Explainability,Alignment with human representations supports robust few-shot learning,"Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both natural adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.","['representation learning', 'supervised learning', 'human alignment', 'few-shot learning']",[],"['Ilia Sucholutsky', 'Thomas L. Griffiths']","['Princeton University', 'Princeton University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72193,Transparency & Explainability,Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks,"Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using benchmark datasets and representative attacks validates the efficacy of MDP. The code of MDP is publicly available.","['few-shot learning', 'prompt learning', 'language model', 'backdoor defense']",[],"['Zhaohan Xi', 'Tianyu Du', 'Changjiang Li', 'Ren Pang', 'Shouling Ji', 'Jinghui Chen', 'Fenglong Ma', 'Ting Wang']","['Pennsylvania State University', 'Zhejiang University', 'State University of New York at Stony Brook', 'Pennsylvania State University', 'Zhejiang University', 'Pennsylvania State University', 'Pennsylvania State University', 'State University of New York at Stony Brook']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71643,Transparency & Explainability,Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective,"Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared with several state-of-the-art approaches.","['action recognition', 'unsupervised domain adaptation', 'video analysis']",[],"['Pengfei Wei', 'Lingdong Kong', 'Xinghua Qu', 'Yi Ren', 'zhiqiang xu', 'Jing Jiang', 'Xiang Yin']","['AI LAB Bytedance', 'National University of', 'Bytedance AI Lab', 'ByteDance', 'Mohamed bin Zayed University of Artificial Intelligence', 'University of Technology Sydney', 'University of Science and Technology of']","[None, 'Singapore', None, None, None, None, 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/71700,Transparency & Explainability,Adaptive Data Analysis in a Balanced Adversarial Model,"In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $\cal{D}$, and is required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to $\cal{D}$. Hardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in general, it is computationally hard to answer more than $\Theta(n^2)$ adaptive queries, assuming the existence of one-way functions. However, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution $\cal{D}$. This imbalance raises questions with respect to the applicability of the obtained hardness results -- an analyst who has complete knowledge of the underlying distribution $\cal{D}$ would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from $\cal{D}$. We consider more restricted adversaries, called \emph{balanced}, where each such adversary consists of two separated algorithms: The \emph{sampler} who is the entity that chooses the distribution and provides the samples to the mechanism, and the \emph{analyst} who chooses the adaptive queries, but has no prior knowledge of the underlying distribution (and hence has no a priori advantage with respect to the mechanism). We improve the quality of previous lower bounds by revisiting them using an efficient \emph{balanced} adversary, under standard public-key cryptography assumptions. We show that these stronger hardness assumptions are unavoidable in the sense that any computationally bounded \emph{balanced} adversary that has the structure of all known attacks, implies the existence of public-key cryptography.","['Adaptive Data Analysis', 'Differential Privacy', 'Statistical Queries']",[],"['Kobbi Nissim', 'Uri Stemmer', 'Eliad Tsfadia']","['Georgetown University', 'Tel Aviv University', 'Georgetown University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71713,Transparency & Explainability,RADAR: Robust AI-Text Detection via Adversarial Learning,"Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a $\underline{r}$obust  $\underline{A}$I-text  $\underline{d}$etector via  $\underline{a}$dversarial lea$\underline{r}$ning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.","['Large Language Models', 'Text Detection', 'Adversarial Learning', 'Paraphrase']",[],"['Xiaomeng Hu', 'Pin-Yu Chen', 'Tsung-Yi Ho']","['Department of Computer Science and Engineering, The Chinese University of', 'International Business Machines', 'Department of Computer Science and Engineering, The Chinese University of']","['Hong Kong', None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/71718,Transparency & Explainability,DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification,"Diffusion-based purification defenses leverage diffusion models to remove crafted perturbations of adversarial examples and achieve state-of-the-art robustness. Recent studies show that even advanced attacks cannot break such defenses effectively, since the purification process induces an extremely deep computational graph which poses the potential problem of gradient obfuscation, high memory cost, and unbounded randomness. In this paper, we propose a unified framework DiffAttack to perform effective and efficient attacks against diffusion-based purification defenses, including both DDPM and score-based approaches. In particular, we propose a deviated-reconstruction loss at intermediate diffusion steps to induce inaccurate density gradient estimation to tackle the problem of vanishing/exploding gradients. We also provide a segment-wise forwarding-backwarding algorithm, which leads to memory-efficient gradient backpropagation. We validate the attack effectiveness of DiffAttack compared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that DiffAttack decreases the robust accuracy of models compared with SOTA attacks by over 20\% on CIFAR-10 under $\ell_\infty$ attack $(\epsilon=8/255)$, and over 10\% on ImageNet under $\ell_\infty$ attack $(\epsilon=4/255)$. We conduct a series of ablations studies, and we find 1) DiffAttack with the deviated-reconstruction loss added over uniformly sampled time steps is more effective than that added over only initial/final steps, and 2) diffusion-based purification with a moderate diffusion length is more robust under DiffAttack.","['adversarial attack', 'adversarial purification', 'adversarial robustness', 'diffusion model']",[],"['Mintong Kang', 'Dawn Song', 'Bo Li']","['University of Illinois at Urbana-Champaign', 'University of California Berkeley', 'University of Illinois, Urbana Champaign']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71728,Transparency & Explainability,Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks,"No-Reference Video Quality Assessment (NR-VQA) plays an essential role in improving the viewing experience of end-users. Driven by deep learning, recent NR-VQA models based on Convolutional Neural Networks (CNNs) and Transformers have achieved outstanding performance. To build a reliable and practical assessment system, it is of great necessity to evaluate their robustness. However, such issue has received little attention in the academic community. In this paper, we make the first attempt to evaluate the robustness of NR-VQA models against adversarial attacks, and propose a patch-based random search method for black-box attack. Specifically, considering both the attack effect on quality score and the visual quality of adversarial video, the attack problem is formulated as misleading the estimated quality score under the constraint of just-noticeable difference (JND). Built upon such formulation, a novel loss function called Score-Reversed Boundary Loss is designed to push the adversarial video’s estimated quality score far away from its ground-truth score towards a specific boundary, and the JND constraint is modeled as a strict $L_2$ and $L_\infty$ norm restriction. By this means, both white-box and black-box attacks can be launched in an effective and imperceptible manner. The source code is available at https://github.com/GZHU-DVL/AttackVQA.","['video quality assessment', 'adversarial attack', 'black-box', 'just noticeable difference']",[],"['Aoxiang Zhang', 'Yu Ran', 'Weixuan Tang', 'Yuan-Gen Wang']","['Guangzhou University', 'Guangzhou University', 'Guangzhou University', 'Guangzhou University,']","[None, None, None, 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/71757,Transparency & Explainability,"Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models","While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR-10, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\ell_\infty$-threat model, but even more so improve generalization to unseen $\ell_1/\ell_2$-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust $\ell_\infty$-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models.","['adversarial robustness', 'deep learning', 'vision transformers', 'convnext']",[],"['Naman Deep Singh', 'Francesco Croce', 'Matthias Hein']","['Eberhard-Karls-Universität Tübingen', 'EPFL - EPF Lausanne', 'University of Tübingen']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72253,Transparency & Explainability,Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning,"Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications.","['Safe Reinforcement Learning', 'Conditioned Reinforcement Learning', 'Multi-task Reinforcement Learning']",[],"['Yihang Yao', 'Zuxin Liu', 'Zhepeng Cen', 'Jiacheng Zhu', 'Wenhao Yu', 'Tingnan Zhang', 'Ding Zhao']","['Carnegie Mellon University', 'Carnegie Mellon University', 'CMU, Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Google', 'Google', 'Carnegie Mellon University']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72463,Transparency & Explainability,Designing Robust Transformers using Robust Kernel Density Estimation,"Transformer-based architectures have recently exhibited remarkable successes across different domains beyond just powering large language models. However, existing approaches typically focus on predictive accuracy and computational cost, largely ignoring certain other practical issues such as robustness to contaminated samples. In this paper, by re-interpreting the self-attention mechanism as a non-parametric kernel density estimator, we adapt classical robust kernel density estimation methods to develop novel classes of transformers that are resistant to adversarial attacks and data contamination. We first propose methods that down-weight outliers in RKHS when computing the self-attention operations. We empirically show that these methods produce improved performance over existing state-of-the-art methods, particularly on image data under adversarial attacks. Then we leverage the median-of-means principle to obtain another efficient approach that results in noticeably enhanced performance and robustness on language modeling and time series classification tasks. Our methods can be combined with existing transformers to augment their robust properties, thus promising to impact a wide variety of applications.","['Transformers', 'Kernel Density Estimation', 'Robustness']",[],"['Xing Han', 'Tongzheng Ren', 'Tan Minh Nguyen', 'Khai Nguyen', 'Joydeep Ghosh', 'Nhat Ho']","['Department of Computer Science, Whiting School of Engineering', 'University of Texas, Austin', 'University of California, Los Angeles', 'University of Texas, Austin', 'University of Texas, Austin', 'University of Texas, Austin']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72515,Transparency & Explainability,Counterfactually Comparing Abstaining Classifiers,"Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stakes decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions matter when they can eventually be utilized, either directly or as a backup option in a failure mode. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions under which the counterfactual score is identifiable: if the abstentions are stochastic, and if the evaluation data is independent of the training data (ensuring that the predictions are missing at random), then the score is identifiable. Note that, if abstentions are deterministic, then the score is unidentifiable because the classifier can perform arbitrarily poorly on its abstentions. Leveraging tools from observational causal inference, we then develop nonparametric and doubly robust methods to efficiently estimate this quantity under identification. Our approach is examined in both simulated and real data experiments.","['abstaining classifiers', 'black-box model evaluation', 'causal inference', 'missing data']",[],"['Yo Joong Choe', 'Aditya Gangrade', 'Aaditya Ramdas']","['University of Chicago', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72535,Transparency & Explainability,A Unified Solution for Privacy and Communication Efficiency in Vertical Federated Learning,"Vertical Federated Learning (VFL) is a collaborative machine learning paradigm that enables multiple participants to jointly train a model on their private data without sharing it. To make VFL practical, privacy security and communication efficiency should both be satisfied. Recent research has shown that Zero-Order Optimization (ZOO) in VFL can effectively conceal the internal information of the model without adding costly privacy protective add-ons, making it a promising approach for privacy and efficiency. However, there are still two key problems that have yet to be resolved. First, the convergence rate of ZOO-based VFL is significantly slower compared to gradient-based VFL, resulting in low efficiency in model training and more communication round, which hinders its application on large neural networks. Second, although ZOO-based VFL has demonstrated resistance to state-of-the-art (SOTA) attacks, its privacy guarantee lacks a theoretical explanation. To address these challenges, we propose a novel cascaded hybrid optimization approach that employs a zeroth-order (ZO) gradient on the most critical output layer of the clients, with other parts utilizing the first-order (FO) gradient. This approach preserves the privacy protection of ZOO while significantly enhancing convergence. Moreover, we theoretically prove that applying ZOO to the VFL is equivalent to adding Gaussian Mechanism to the gradient information, which offers an implicit differential privacy guarantee. Experimental results demonstrate that our proposed framework achieves similar utility as the Gaussian mechanism under the same privacy budget, while also having significantly lower communication costs compared with SOTA communication-efficient VFL frameworks.","['Vertical Federated Learning', 'Zeroth Order Optimization', 'Communication Efficiency', 'Privacy']",[],"['Ganyu Wang', 'Bin Gu', 'Qingsong Zhang', 'Xiang Li', 'Boyu Wang', 'Charles Ling']","['Western University', 'Mohamed bin Zayed University of Artificial Intelligence', 'Xidian University', 'University of Western Ontario', 'University of Western Ontario', 'Western University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71886,Fairness & Bias,Mitigating Test-Time Bias for Fair Image Retrieval,"We address the challenge of generating fair and unbiased image retrieval results given neutral textual queries (with no explicit gender or race connotations), while maintaining the utility (performance) of the underlying vision-language (VL) model. Previous methods aim to disentangle learned representations of images and text queries from gender and racial characteristics. However, we show these are inadequate at alleviating bias for the desired equal representation result, as there usually exists test-time bias in the target retrieval set. So motivated, we introduce a straightforward technique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from the pre-trained vision-language model. We evaluate our algorithm on real-world image search datasets, Occupation 1 and 2, as well as two large-scale image-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest bias, compared with various existing bias-mitigation methods, in text-based image retrieval result while maintaining satisfactory retrieval performance. The source code is publicly available at \url{https://github.com/timqqt/Fair_Text_based_Image_Retrieval}.","['Vision-language', 'Fairness', 'Text-based Image Retrieval', 'Deep Learning', 'Application']",[],"['Fanjie Kong', 'Shuai Yuan', 'Weituo Hao', 'Ricardo Henao']","['Duke University', 'Meta', 'TikTok Inc.', 'Duke University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71683,Fairness & Bias,Equal Opportunity of Coverage in Fair Regression,"We study fair machine learning (ML) under predictive uncertainty to enable reliable and trustworthy decision-making. The seminal work of 'equalized coverage' proposed an uncertainty-aware fairness notion. However, it does not guarantee equal coverage rates across more fine-grained groups (e.g., low-income females) conditioning on the true label and is biased in the assessment of uncertainty. To tackle these limitations, we propose a new uncertainty-aware fairness -- Equal Opportunity of Coverage (EOC) -- that aims to achieve two properties: (1) coverage rates for different groups with similar outcomes are close, and (2) the coverage rate for the entire population remains at a predetermined level. Further, the prediction intervals should be narrow to be informative. We propose Binned Fair Quantile Regression (BFQR), a distribution-free post-processing method to improve EOC with reasonable width for any trained ML models. It first calibrates a hold-out set to bound deviation from EOC, then leverages conformal prediction to maintain EOC on a test set, meanwhile optimizing prediction interval width. Experimental results demonstrate the effectiveness of our method in improving EOC.",['Equal Opportunity; Fair Machine Learning; Conformal Prediction; Uncertainty Quantification'],[],"['Fangxin Wang', 'Lu Cheng', 'Ruocheng Guo', 'Kay Liu', 'Philip S. Yu']","['University of Illinois at Chicago', 'University of Illinois at Chicago', 'Bytedance Research', 'University of Illinois Chicago', 'University of Illinois, Chicago']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71862,Fairness & Bias,Fairly Recommending with Social Attributes: A Flexible and Controllable Optimization Approach,"Item-side group fairness (IGF) requires a recommendation model to treat different item groups similarly, and has a crucial impact on information diffusion, consumption activity, and market equilibrium. Previous IGF notions only focus on the direct utility of the item exposures, i.e., the exposure numbers across different item groups. Nevertheless, the item exposures also facilitate utility gained from the neighboring users via social influence, called social utility, such as information sharing on the social media. To fill this gap, this paper introduces two social attribute-aware IGF metrics, which require similar user social attributes on the exposed items across the different item groups. In light of the trade-off between the direct utility and social utility, we formulate a new multi-objective optimization problem for training recommender models with flexible trade-off while ensuring controllable accuracy. To solve this problem, we develop a gradient-based optimization algorithm and theoretically show that the proposed algorithm can find Pareto optimal solutions with varying trade-off and guaranteed accuracy. Extensive experiments on two real-world datasets validate the effectiveness of our approach.","['Recommender System', 'Fairness']",[],"['Jinqiu Jin', 'Haoxuan Li', 'Fuli Feng', 'Sihao Ding', 'Peng Wu', 'Xiangnan He']","['University of Science and Technology of', 'Pohang University of Science and Technology', 'University of Science and Technology of', 'University of Science and Technology of', 'Beijing Technology and Business University', 'University of Science and Technology of']","['China', None, 'China', 'China', None, 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/72485,Fairness & Bias,Fast Model DeBias with Machine Unlearning,"Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing method (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset.  Experiments on the Colored MNIST, CelebA, and Adult Income datasets demonstrate that our method achieves superior or competing classification accuracies compared with state-of-the-art retraining-based methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.","['Model Debias', 'Bias Mitigation', 'Machine Unlearning', 'Counterfactual Fairness']",[],"['Ruizhe Chen', 'Jianfei Yang', 'Huimin Xiong', 'Jianhong Bai', 'Tianxiang Hu', 'Jin Hao', 'YANG FENG', 'Joey Tianyi Zhou', 'Jian Wu', 'Zuozhu Liu']","['Zhejiang University', 'Nanyang Technological University', 'Zhejiang University', 'Zhejiang University', ' Zhejiang University-University of Illinois Urbana-Champaign Institute, Zhejiang University', 'Stanford University', 'Angelalign Tech.', 'National University of', 'Zhejiang University', 'Zhejiang University']","[None, None, None, None, None, None, None, 'Singapore', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73438,Fairness & Bias,Building Socio-culturally Inclusive Stereotype Resources with Community Engagement,"With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in India. The resultant resource increases the number of stereotypes known for and in the Indian context by over 1000 stereotypes across many unique identities. We also demonstrate the utility and effectiveness of such expanded resources for evaluations of language models. CONTENT WARNING: This paper contains examples of stereotypes that may be offensive.","['stereotype', 'dataset', 'evaluation', 'language models', 'cross cultural']",[],"['Sunipa Dev', 'Jaya Goyal', 'Dinesh Tewari', 'Shachi Dave', 'Vinodkumar Prabhakaran']","['Google', 'British Council', 'Jawaharlal Nehru University', 'Research, Google', 'Google']","[None, 'India', None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73597,Fairness & Bias,Ethical Considerations for Responsible Data Curation,"Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.","['human-centric', 'datasets', 'computer vision', 'fairness', 'algorithmic bias', 'robustness', 'responsible AI']",[],"['Jerone Andrews', 'Dora Zhao', 'William Thong', 'Orestis Papakyriakopoulos']","['Sony AI', 'Stanford University', 'Sony AI', 'Sony AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70219,Fairness & Bias,Doubly Constrained Fair Clustering,"The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness ($\textbf{GF}$), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection ($\textbf{DS}$), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for one constraint ($\textbf{GF}$ or $\textbf{DS}$ only) we can obtain a constant approximation solution that satisfies both constraints simultaneously. Interestingly, we prove that any given solution that satisfies the $\textbf{GF}$ constraint can always be post-processed at a bounded degradation to the clustering cost to additionally satisfy the $\textbf{DS}$ constraint while the same statement is not true given a solution that satisfies $\textbf{DS}$ instead. Furthermore, we show that both $\textbf{GF}$ and $\textbf{DS}$ are incompatible (having an empty feasibility set in the worst case) with a collection of other distance-based fairness notions. Finally, we carry experiments to validate our theoretical findings.","['Fairness', 'Clustering', 'Approximation Algorithms']",[],"['John P Dickerson', 'Seyed A. Esmaeili', 'Jamie Heather Morgenstern', 'Claire Jie Zhang']","['University of Maryland, College Park', 'Simons Laufer Mathematical Sciences Institute', 'University of Washington, Seattle', 'Department of Computer Science, University of Washington']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72770,Fairness & Bias,Causal Fairness for Outcome Control,"As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called outcome control in which an automated system aims to optimize an outcome variable $Y$ while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of benefit, which captures how much a specific individual would benefit from a positive decision, counterfactually speaking, when contrasted with an alternative, negative one. We introduce the notion of benefit fairness, which can be seen as the minimal fairness requirement in decision-making, and develop an algorithm for satisfying it. We then note that the benefit itself may be influenced by the protected attribute, and propose causal tools which can be used to analyze this. Finally, if some of the variations of the protected attribute in the benefit are considered as discriminatory, the notion of benefit fairness may need to be strengthened, which leads us to articulating a notion of causal benefit fairness. Using this notion, we develop a new optimization procedure capable of maximizing $Y$ while ascertaining causal fairness in the decision process.","['Fair Machine Learning', 'Causal Inference', 'Decision-Making']",[],"['Drago Plecko', 'Elias Bareinboim']","['Columbia University', 'Purdue University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73486,Fairness & Bias,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,"Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance – where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives – including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.","['trustworthiness evaluation', 'GPT models', 'GPT-3.5', 'GPT-4', 'toxicity', 'stereotypes', 'bias', 'adversarial robustness', 'out-of-distribution robustness', 'privacy', 'ethics', 'fairness']",[],"['Boxin Wang', 'Weixin Chen', 'Chulin Xie', 'Mintong Kang', 'Chenhui Zhang', 'Chejian Xu', 'Zidi Xiong', 'Ritik Dutta', 'Rylan Schaeffer', 'Sang T. Truong', 'Simran Arora', 'Mantas Mazeika', 'Dan Hendrycks', 'Zinan Lin', 'Yu Cheng', 'Sanmi Koyejo', 'Dawn Song', 'Bo Li']","['NVIDIA', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'University of Illinois at Urbana-Champaign', 'Massachusetts Institute of Technology', 'University of Illinois at Urbana-Champaign', 'Department of Computer Science', 'IIT Gandhinagar, Dhirubhai Ambani Institute Of Information and Communication Technology', 'Computer Science Department, Stanford University', 'Stanford University', 'Stanford University', 'University of Illinois, Urbana-Champaign', 'UC Berkeley', 'Microsoft', 'Microsoft Research', 'Stanford University', 'University of California Berkeley', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71358,Fairness & Bias,Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions,"Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination.","['information theory', 'fair machine learning']",[],"['Hao Wang', 'Luxi He', 'Rui Gao', 'Flavio Calmon']","['MIT-IBM Watson AI Lab', 'Department of Computer Science, Princeton University', 'University of Texas, Austin', 'Harvard University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70480,Fairness & Bias,Individual Arbitrariness and Group Fairness,"Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples---a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered  when deploying models to aid decision-making in applications of individual-level impact. To address this challenge, we propose an ensemble  algorithm applicable to any fairness intervention that provably ensures  more consistent predictions.","['predictive multiplicity', 'fairness in machine learning', 'Rashomon effect']",[],"['Carol Xuan Long', 'Hsiang Hsu', 'Wael Alghamdi', 'Flavio Calmon']","['Harvard University, Harvard University', 'JP Morgan & Chase Bank', 'Harvard University', 'Harvard University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72984,Fairness & Bias,Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition,"Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race.  Conventional wisdom dictates that model biases arise from biased training data.  As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition.  In our work, we discover that biases are actually inherent to neural network architectures themselves.  Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairness, often by large margins, on the two most widely used datasets for face identification, CelebA and VGGFace2. Furthermore, these models generalize to other datasets and sensitive attributes. We release our code, models and raw data files at https://github.com/dooleys/FR-NAS.","['Bias Mitigation', 'Fairness', 'Facial Recognition']",[],"['Rhea Sanjay Sukthanker', 'John P Dickerson', 'Colin White', 'Frank Hutter', 'Micah Goldblum']","['University of Freiburg, Albert-Ludwigs-Universität Freiburg', 'University of Maryland, College Park', 'California Institute of Technology', 'University of Freiburg & Bosch', 'New York University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71109,Fairness & Bias,"Fair, Polylog-Approximate Low-Cost Hierarchical Clustering","Research in fair machine learning, and particularly clustering, has been crucial in recent years given the many ethical controversies that modern intelligent systems have posed. Ahmadian et al. [2020] established the study of fairness in hierarchical clustering, a stronger, more structured variant of its well-known flat counterpart, though their proposed algorithm that optimizes for Dasgupta's [2016] famous cost function was highly theoretical. Knittel et al. [2023] then proposed the first practical fair approximation for cost, however they were unable to break the polynomial-approximate barrier they posed as a hurdle of interest. We break this barrier, proposing the first truly polylogarithmic-approximate low-cost fair hierarchical clustering, thus greatly bridging the gap between the best fair and vanilla hierarchical clustering approximations.","['Fair machine learning', 'hierarchical clustering', 'clustering']",[],"['Marina Knittel', 'Max Springer', 'John P Dickerson', 'MohammadTaghi Hajiaghayi']","['Department of Computer Science, University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72365,Fairness & Bias,Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach,"Fairness in machine learning has attracted increasing attention in recent years. The fairness methods improving algorithmic fairness for in-distribution data may not perform well under distribution shifts. In this paper, we first theoretically demonstrate the inherent connection between distribution shift,  data perturbation, and model weight perturbation. Subsequently, we analyze the sufficient conditions to guarantee fairness (i.e., low demographic parity) for the target dataset, including fairness for the source dataset, and low prediction difference between the source and target datasets for each sensitive attribute group. Motivated by these sufficient conditions, we propose robust fairness regularization (RFR) by considering the worst case within the model weight perturbation ball for each sensitive attribute group. We evaluate the effectiveness of our proposed RFR algorithm on synthetic and real distribution shifts across various datasets. Experimental results demonstrate that RFR achieves better fairness-accuracy trade-off performance compared with several baselines. The source code is available at \url{https://github.com/zhimengj0326/RFR_NeurIPS23}.","['Model Weight Perturbation', 'fairness', 'distribution shift']",[],"['Zhimeng Jiang', 'Xiaotian Han', 'Hongye Jin', 'Guanchu Wang', 'Rui Chen', 'Na Zou', 'Xia Hu']","['Texas A&M University', 'Texas A&M University', 'Texas A&M', 'Rice University', 'Samsung', 'Texas A&M University - College Station', 'Rice University']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71118,Fairness & Bias,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs—e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always ""(A)""—which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.","['Natural language processing', 'large language models', 'XAI', 'explainability']",[],"['Miles Turpin', 'Julian Michael', 'Ethan Perez']","['New York University', 'New York University', 'New York University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69927,Fairness & Bias,A Unifying Perspective on Multi-Calibration: Game Dynamics for Multi-Objective Learning,"We provide a unifying framework for the design and analysis of multi-calibrated predictors. By placing the multi-calibration problem in the general setting of multi-objective learning---where learning guarantees must hold simultaneously over a set of distributions and loss functions---we exploit connections to game dynamics to achieve state-of-the-art guarantees for a diverse set of multi-calibration learning problems. In addition to shedding light on existing multi-calibration guarantees and greatly simplifying their analysis, our approach also yields improved guarantees, such as error tolerances that scale with the square-root of group size versus the constant tolerances guaranteed by prior works, and improving the complexity of $k$-class multi-calibration by an exponential factor of $k$ versus Gopalan et al.. Beyond multi-calibration, we use these game dynamics to address emerging considerations in the study of group fairness and multi-distribution learning.","['multicalibration', 'multi-objective learning', 'learning theory', 'calibration', 'fairness', 'games']",[],"['Nika Haghtalab', 'Michael Jordan', 'Eric Zhao']","['University of California Berkeley', 'University of California, Berkeley', 'University of California Berkeley']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70223,Fairness & Bias,Understanding Deep Gradient Leakage via Inversion Influence Functions,"Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, modalities, attack implementations, and perturbation-based defenses. With this novel tool, we provide insights into effective gradient perturbation directions, the unfairness of privacy protection, and privacy-preferred model initialization. Our codes are provided in https://github.com/illidanlab/inversion-influence-function.","['Deep Learning', 'Privacy', 'Federated Learning', 'Influence Function']",[],"['Junyuan Hong', 'Yuyang Deng', 'Mehrdad Mahdavi', 'Jiayu Zhou']","['University of Texas at Austin', 'Pennsylvania State University', 'Pennsylvania State University', 'Michigan State University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69881,Fairness & Bias,Scalable Fair Influence Maximization,"Given a graph $G$, a community structure $\mathcal{C}$, and a budget $k$, the fair influence maximization problem aims to select a seed set $S$ ($|S|\leq k$) that maximizes the influence spread while narrowing the influence gap between different communities. While various fairness notions exist, the welfare fairness notion, which balances fairness level and influence spread, has shown promising effectiveness. However, the lack of efficient algorithms for optimizing the welfare fairness objective function restricts its application to small-scale networks with only a few hundred nodes. In this paper, we adopt the objective function of welfare fairness to maximize the exponentially weighted summation over the influenced fraction of all communities. We first introduce an unbiased estimator for the fractional power of the arithmetic mean. Then, by adapting the reverse influence sampling (RIS) approach, we convert the optimization problem to a weighted maximum coverage problem. We also analyze the number of reverse reachable sets needed to approximate the fair influence at a high probability. Further, we present an efficient algorithm that guarantees $1-1/e - \varepsilon$ approximation.","['influence maximization', 'approximation algorithm', 'social fairness']",[],"['Xiaobin Rui', 'Zhixiao Wang', 'Jiayu Zhao', 'Lichao Sun', 'Wei Chen']","['University of Mining Technology - Xuzhou', 'University of Mining Technology - Xuzhou', 'University of Mining Technology - Xuzhou', 'Lehigh University', 'Microsoft Research']","['China', 'China', 'China', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69984,Fairness & Bias,Calibrating “Cheap Signals” in Peer Review without a Prior,"Peer review lies at the core of the academic process, but even well-intentioned reviewers can still provide noisy ratings. While ranking papers by average ratings may reduce noise, varying noise levels and systematic biases stemming from ``cheap'' signals (e.g. author identity, proof length) can lead to unfairness.  Detecting and correcting bias is challenging, as ratings are subjective and unverifiable. Unlike previous works relying on prior knowledge or historical data, we propose a one-shot noise calibration process without any prior information. We ask reviewers to predict others' scores and use these predictions for calibration. Assuming reviewers adjust their predictions according to the noise, we demonstrate that the calibrated score results in a more robust ranking compared to average ratings, even with varying noise levels and biases. In detail, we show that the error probability of the calibrated score approaches zero as the number of reviewers increases and is significantly lower compared to average ratings when the number of reviewers is small.","['Peer prediction', 'Peer review', 'Calibration']",[],['Yuqing Kong'],['Peking University'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/70161,Fairness & Bias,FairLISA: Fair User Modeling with Limited Sensitive Attributes Information,"User modeling techniques profile users' latent characteristics (e.g., preference) from their observed behaviors, and play a crucial role in decision-making. Unfortunately, traditional user models may unconsciously capture biases related to sensitive attributes (e.g., gender) from behavior data, even when this sensitive information is not explicitly provided. This can lead to unfair issues and discrimination against certain groups based on these sensitive attributes.  Recent studies have been proposed to improve fairness by explicitly decorrelating user modeling results and sensitive attributes. However, most existing approaches assume that fully sensitive attribute labels are available in the training set, which is unrealistic due to collection limitations like privacy concerns, and hence bear the limitation of performance. In this paper, we focus on a practical situation with limited sensitive data and propose a novel FairLISA framework, which can efficiently utilize data with known and unknown sensitive attributes to facilitate fair model training. We first propose a novel theoretical perspective to build the relationship between data with both known and unknown sensitive attributes with the fairness objective.  Then, based on this, we provide a general adversarial framework to effectively leverage the whole user data for fair user modeling. We conduct experiments on representative user modeling tasks including recommender system and cognitive diagnosis. The results demonstrate that our FairLISA can effectively improve fairness while retaining high accuracy in scenarios with different ratios of missing sensitive attributes.","['fairness', 'user modeling']",[],"['Zheng Zhang', 'Qi Liu', 'Fei Wang', 'Yan Zhuang', 'Le Wu', 'Weibo Gao', 'Enhong Chen']","['University of Science and Technology of', 'University of Science and Technology of', 'University of Science and Technology of', 'University of Science and Technology of', 'Hefei University of Technology', 'University of Science and Technology of', 'University of Science and Technology of']","['China', 'China', 'China', 'China', None, 'China', 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/70002,Fairness & Bias,Fair Graph Distillation,"As graph neural networks (GNNs) struggle with large-scale graphs due to high computational demands, data distillation for graph data promises to alleviate this issue by distilling a large real graph into a smaller distilled graph while maintaining comparable prediction performance for GNNs trained on both graphs. However, we observe that GNNs trained on distilled graphs may exhibit more severe group fairness problems than those trained on real graphs. Motivated by this observation, we propose \textit{fair graph distillation} (\Algnameabbr), an approach for generating small distilled \textit{fair and informative} graphs based on the graph distillation method. The challenge lies in the deficiency of sensitive attributes for nodes in the distilled graph, making most debiasing methods (e.g., regularization and adversarial debiasing) intractable for distilled graphs. We develop a simple yet effective bias metric, called coherence, for distilled graphs. Based on the proposed coherence metric, we introduce a framework for fair graph distillation using a bi-level optimization algorithm. Extensive experiments demonstrate that the proposed algorithm can achieve better prediction performance-fairness trade-offs across various datasets and GNN architectures.","['Graph Distillation', 'Algorithmic Fairness']",[],"['Qizhang Feng', 'Zhimeng Jiang', 'Ruiquan Li', 'Yicheng Wang', 'Na Zou', 'Jiang Bian', 'Xia Hu']","['Texas A&M', 'Texas A&M University', 'University of Science and Technology of', 'Texas A&M', 'Texas A&M University - College Station', 'University of Florida', 'Rice University']","[None, None, 'China', None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70497,Fairness & Bias,Mitigating Source Bias for Fairer Weak Supervision,"Weak supervision enables efficient development of training sets by reducing the need for ground truth labels. However, the techniques that make weak supervision attractive---such as integrating any source of signal to estimate unknown labels---also entail the danger that the produced pseudolabels are highly biased. Surprisingly, given everyday use and the potential for increased bias, weak supervision has not been studied from the point of view of fairness. We begin such a study, starting with the observation that even when a fair model can be built from a dataset with access to ground-truth labels, the corresponding dataset labeled via weak supervision can be arbitrarily unfair. To address this, we propose and empirically validate a model for source unfairness in weak supervision, then introduce a simple counterfactual fairness-based technique that can mitigate these biases. Theoretically, we show that it is possible for our approach to simultaneously improve both accuracy and fairness---in contrast to standard fairness approaches that suffer from tradeoffs. Empirically, we show that our technique improves accuracy on weak supervision baselines by as much as 32\% while reducing demographic parity gap by 82.5\%. A simple extension of our method aimed at maximizing performance produces state-of-the-art performance in five out of ten datasets in the WRENCH benchmark.","['Weak supervision', 'fairness']",[],"['Changho Shin', 'Sonia Cromp', 'Dyah Adila', 'Frederic Sala']","['University of Wisconsin, Madison', 'Department of Computer Science, University of Wisconsin - Madison', 'University of Wisconsin, Madison', 'University of Wisconsin, Madison']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70498,Fairness & Bias,Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices,"Federated learning (FL) is usually performed on resource-constrained edge devices, e.g., with limited memory for the computation. If the required memory to train a model exceeds this limit, the device will be excluded from the training. This can lead to a lower accuracy as valuable data and computation resources are excluded from training, also causing bias and unfairness. The FL training process should be adjusted to such constraints. The state-of-the-art techniques propose training subsets of the FL model at constrained devices, reducing their resource requirements for training. However, these techniques largely limit the co-adaptation among parameters of the model and are highly inefficient, as we show: it is actually better to train a smaller (less accurate) model by the system where all the devices can train the model end-to-end than applying such techniques. We propose a new method that enables successive freezing and training of the parameters of the FL model at devices, reducing the training’s resource requirements at the devices while still allowing enough co-adaptation between parameters. We show through extensive experimental evaluation that our technique greatly improves the accuracy of the trained model (by 52.4 p.p. ) compared with the state of the art, efficiently aggregating the computation capacity available on distributed devices.","['Federated Learning', 'Memory', 'Resource Constraints']",[],"['Ramin Khalili', 'Joerg Henkel']","['Huawei Technologies Ltd., Munich research center', 'Karlsruhe Institute of Technology']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71301,Fairness & Bias,On Measuring Fairness in Generative Models,"Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on fairness measurement, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., 4.98%→0.62% for StyleGAN2 w.r.t. Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. Code and more resources: https://sutd-visual-computing-group.github.io/CLEAM/.","['Fairness', 'Generative models', 'GAN', 'Calibration']",[],"['Christopher T.H Teo', 'Milad Abdollahzadeh', 'Ngai-man Cheung']","['University of Technology and Design', 'University of Technology and Design', 'University of Technology and Design']","['Singapore', 'Singapore', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/71080,Fairness & Bias,Max-Sliced Mutual Information,"Quantifying dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast computation and scalable estimation from samples. We show that mSMI retains favorable structural properties of Shannon's mutual information, like variational forms and identification of independence. We then study statistical estimation of mSMI, propose an efficiently computable neural estimator, and couple it with formal non-asymptotic error bounds. We present experiments that demonstrate the utility of mSMI for several tasks, encompassing independence testing, multi-view representation learning, algorithmic fairness, and generative modeling. We observe that mSMI consistently outperforms competing methods with little-to-no computational overhead.","['CCA', 'dimensionality reduction', 'information theory', 'mutual information', 'neural estimation', 'slicing']",[],"['Dor Tsur', 'Ziv Goldfeld', 'Kristjan Greenewald']","['Ben-Gurion University of the Negev', 'Cornell University', 'MIT-IBM Watson AI Lab, IBM Research']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71176,Fairness & Bias,LEACE: Perfect linear concept erasure in closed form,"Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called concept scrubbing, which erases target concept information from _every_ layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Our code is available at https://github.com/EleutherAI/concept-erasure.","['interpretability', 'fairness', 'concept erasure', 'representation', 'adversarial', 'robustness']",[],"['Nora Belrose', 'David Schneider-Joseph', 'Shauli Ravfogel', 'Ryan Cotterell', 'Edward Raff', 'Stella Biderman']","['EleutherAI', 'EleutherAI', 'Bar-Ilan University', 'Swiss Federal Institute of Technology', 'University of Maryland, Baltimore County', 'EleutherAI']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71423,Fairness & Bias,Fair Canonical Correlation Analysis,"This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables CCA to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing correlation disparity error without compromising CCA accuracy.","['Fairness', 'Canonical Correlation Analysis', 'Riemannian Optimization', 'Pareto Optimization']",[],"['Zhuoping Zhou', 'Davoud Ataee Tarzanagh', 'Bojian Hou', 'Boning Tong', 'Jia Xu', 'Yanbo Feng', 'Qi Long', 'Li Shen']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71701,Fairness & Bias,Counterfactually Fair Representation,"The use of machine learning models in high-stake applications (e.g., healthcare, lending, college admission) has raised growing concerns due to potential biases against protected social groups. Various fairness notions and methods have been proposed to mitigate such biases. In this work, we focus on Counterfactual Fairness (CF), a fairness notion that is dependent on an underlying causal graph and first proposed by Kusner $\textit{et al.}$; it requires that the outcome an individual perceives is the same in the real world as it would be in a ""counterfactual"" world, in which the individual belongs to another social group.  Learning fair models satisfying CF can be challenging. It was shown in (Kusner $\textit{et al.}$) that a sufficient condition for satisfying CF is to $\textbf{not}$ use features that are descendants of sensitive attributes in the causal graph. This implies a simple method that learns CF models only using non-descendants of sensitive attributes while eliminating all descendants. Although several subsequent works proposed methods that use all features for training CF models, there is no theoretical guarantee that they can satisfy CF. In contrast, this work proposes a new algorithm that trains models using all the available features. We theoretically and empirically show that models trained with this method can satisfy CF.","['Counterfactual fairness', 'Representation learning']",[],"['Zhiqun Zuo', 'Mohammad Mahdi Khalili', 'Xueru Zhang']","['Ohio State University, Columbus', 'Yahoo! Research', 'Ohio State University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72525,Fairness & Bias,Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness,"Counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use \textit{causal context} to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts—measurement error, selection on label, and selection on predictors—counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics.","['causal graphs', 'causality', 'counterfactual fairness', 'domain generalization', 'fairness', 'robustness', 'machine learning', 'artificial intelligence']",[],"['Jacy Reese Anthis', 'Victor Veitch']","['University of Chicago', 'University of Chicago']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73666,Fairness & Bias,VisoGender:  A dataset for benchmarking gender bias in image-text pronoun resolution,"We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related biases within a hegemonic system of binary gender, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between pronoun resolution accuracies for image subjects with gender presentations perceived as masculine versus feminine by human annotators and ii) retrieval bias, where we compare ratios of professionals perceived to have masculine and feminine gender presentations retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they demonstrate bias in resolving binary gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models are generally less biased than Vision-Language Encoders.","['fairness', 'vision-language', 'bias', 'benchmark']",[],"['Siobhan Mackenzie Hall', 'Fernanda Gonçalves Abrantes', 'Hanwen Zhu', 'Grace Sodunke', 'Aleksandar Shtedritski', 'Hannah Rose Kirk']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73700,Fairness & Bias,WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit Courts,"Machine learning based decision-support tools in criminal justice systems are subjects of intense discussions and academic research. There are important open questions about the utility and fairness of such tools. Academic researchers often rely on a few small datasets that are not sufficient to empirically study various real-world aspects of these questions. In this paper, we contribute WCLD, a curated large dataset of 1.5 million criminal cases from circuit courts in the U.S. state of Wisconsin. We used reliable public data from 1970 to 2020 to curate attributes like prior criminal counts and recidivism outcomes. The dataset contains large number of samples from five racial groups, in addition to information like sex and age (at judgment and first offense). Other attributes in this dataset include neighborhood characteristics obtained from census data, detailed types of offense, charge severity, case decisions, sentence lengths, year of filing etc. We also provide pseudo-identifiers for judge, county and zipcode. The dataset will not only enable researchers to more rigorously study algorithmic fairness in the context of criminal justice, but also relate algorithmic challenges with various systemic issues. We also discuss in detail the process of constructing the dataset and provide a datasheet. The WCLD dataset is available at https://clezdata.github.io/wcld/.","['Algorithmic Fairness', 'Machine Learning', 'Dataset', 'Criminal Justice']",[],"['Elliott Ash', 'Naman Goel', 'Nianyun Li', 'Claudia Marangon', 'Peiyao Sun']","['Swiss Federal Institute of Technology', 'University of Oxford', 'University of Zurich', 'ETHZ - ETH Zurich', 'ETH-Zurich']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74144,Fairness & Bias,Reproducibility study of the Fairness-enhanced Node Representation Learning,"""CrossWalk: Fairness-Enhanced Node Representation Learning"" is set to be reproduced and reviewed. It presents an extension to existing graph algorithms that incorporate the idea of biased random walks for obtaining node embeddings. CrossWalk incorporates fairness by up-weighting edges of nodes located near group boundaries. The authors claim that their approach outperforms baseline algorithms, such as DeepWalk and FairWalk, in terms of reducing the disparity between different classes within a graph network. The authors accompanied their paper with the publication of an open GitHub page, which includes the source code and relevant data sets. The limited size of the data sets in combination with the efficient algorithms enables the experiments to be conducted without significant difficulties and is computable on standard CPUs without the need for additional resources. In this reproducibility report, the outcomes of the experiments are in agreement with the results presented in the original paper. However, the inherent randomness of the random walks makes it difficult to quantify the extent of similarity between the reproduced results and the results as stated in the original paper. However, it can be concluded that CrossWalk results in a decreased disparity between groups in graph networks. The authors effectively conveyed the underlying concept of their proposed method, rendering it both intriguing and straightforward to comprehend the key ideas. Furthermore, the authors successfully incorporated a range of methods and baseline algorithms into the paper. In contrast, the source code may not have been optimally constructed with reproducibility in mind. Certain sections of the code appear to be unfinished or inadequately executed. Additionally, the authors neglected to specify key hyperparameters, resulting in the unidentifiability of certain results. This presents challenges in drawing conclusions based on the available sources. The authors were unable to respond in time for elaborating on certain implementation details. However, we did receive additional data which was crucial to obtaining certain results.","['Crosswalk', 'Fairness', 'Graph Networks', 'Graph representation learning', 'Moonwalk', 'Random Walk', 'Fairwalk', 'Deepwalk']",[],"['Gijs Joppe Moens', 'Meggie Anna Antwanette van den Oever']","['University of Amsterdam', 'University of Amsterdam']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74146,Fairness & Bias,RELIC: Reproducibility and Extension on LIC metric in quantifying bias in captioning models,"Scope of Reproducibility In this work we reproduce and extend the results presented in “Quantifying Societal Bias Amplification in Image Captioning” by Hirota et al. This paper introduces LIC, a metric to quantify bias amplification by image captioning models, which is tested for gender and racial bias amplification. The original paper claims that this metric is robust, and that all models amplify both gender and racial bias. It also claims that gender bias is more apparent than racial bias, and the Equalizer variation of the NIC+ model increases gender but not racial bias. We repeat the measurements to confirm these claims. We extend the analysis to whether the method can be generalized to other attributes such as bias in age. Methodology The authors of the paper provided a repository containing the necessary code. We had to modify it and add several scripts to be able to run all the experiments. The results were reproduced using the same subset of COCO [3] as in the original paper. Additionally, we manually labeled images according to age for our specific experiments. All experiments were ran on GPUs for a total of approximately 100 hours. Results All claims made by the paper seem to hold, as the results we obtained follow the same trends as those presented in the original paper even if they do not match exactly. However, the same cannot always be said of the additional experiments. What was easy The paper was clear and matched the implementation. The code was well organized and was easy to run using the command interface provided by the authors. This also made it easy to replicate and expand upon it by adding our own new features. The data was also readily available and could be easily downloaded with no need for preprocessing. What was difficult We had to run several iterations of the same code, using different seeds and models, to get the results with the same conditions as in the original paper, which made use of time and resources. Our own experiments required additional time to hand-annotate data due to lack of data for new features. Communication with original authors There was no contact with the authors, since the code and the experiments were clear and did not need any additional explanation.","['rescience c', 'rescience x', 'machine learning', 'python', 'bias', 'caption', 'metrics', 'fairness']",[],"['Martijn van Raaphorst', 'Egoitz Gonzales', 'Marta Grasa', 'Paula Antequera']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72696,Fairness & Bias,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","['Fairness', 'Individual Fairness', 'Deep Learning', 'Certification', 'Trustworthy ML']",[],"['Matthew Robert Wicker', 'Vihari Piratla', 'Adrian Weller']","['Department of Computing, Imperial College London', 'University of Cambridge', 'Alan Turing Institute']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72721,Fairness & Bias,Language Model Tokenizers Introduce Unfairness Between Languages,"Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, there are concerns about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist even for tokenizers that are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the models. Therefore, we make the case that we should train future language models using multilingually fair subword tokenizers.","['LLM', 'language model', 'tokenizer', 'multilingual', 'language', 'fairness']",[],"['Aleksandar Petrov', 'Emanuele La Malfa', 'Philip Torr', 'Adel Bibi']","['University of Oxford', 'Department of Computer Science, University of Oxford', 'University of Oxford', 'University of Oxford']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73437,Fairness & Bias,Synthcity: a benchmark framework for diverse use cases of tabular synthetic data,"Accessible high-quality data is the bread and butter of machine learning research, and the demand for data has exploded as larger and more advanced ML models are built across different domains. Yet, real data often contain sensitive information, are subject to various biases, and are costly to acquire, which compromise their quality and accessibility. Synthetic data have thus emerged as a complement to, sometimes even a replacement for, real data for ML training. However, the landscape of synthetic data research has been fragmented due to the diverse range of data modalities, such as tabular, time series, and images, and the wide array of use cases, including privacy preservation, fairness considerations, and data augmentation. This fragmentation poses practical challenges when comparing and selecting synthetic data generators in for different problem settings. To this end, we develop Synthcity, an open-source Python library that allows researchers and practitioners to perform one-click benchmarking of synthetic data generators across data modalities and use cases. Beyond benchmarking, Synthcity serves as a centralized toolkit for accessing cutting-edge data generators. In addition, Synthcity’s flexible plug-in style API makes it easy to incorporate additional data generators into the framework. Using examples of tabular data generation and data augmentation, we illustrate the general applicability of Synthcity, and the insight one can obtain.","['Synthetic data', 'generative models', 'data augmentation', 'privacy']",[],"['Zhaozhi Qian', 'Rob Davis', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73453,Fairness & Bias,Holistic Evaluation of Text-to-Image Models,"The stunning qualitative improvement of text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on image-text alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase","['text-to-image', 'image generation', 'multimodal', 'holistic evaluation', 'benchmarking', 'human evaluation']",[],"['Tony Lee', 'Michihiro Yasunaga', 'Chenlin Meng', 'Yifan Mai', 'Joon Sung Park', 'Agrim Gupta', 'Yunzhi Zhang', 'Deepak Narayanan', 'Hannah Benita Teufel', 'Marco Bellagente', 'Minguk Kang', 'Taesung Park', 'Jure Leskovec', 'Jun-Yan Zhu', 'Li Fei-Fei', 'Jiajun Wu', 'Stefano Ermon', 'Percy Liang']","['Stanford University', 'Stanford University', 'Stanford University', 'Computer Science Department, Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'NVIDIA', 'Aleph Alpha GmbH', 'Aleph-Alpha gmbh', 'POSTECH', 'Adobe Systems', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73455,Fairness & Bias,Stable Bias: Evaluating Societal Representations in Diffusion Models,"As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems’ outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall·E 2 , Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for this work, as well as the necessary tools to similarly evaluate additional TTI systems.","['text-to-image models', 'diffusion models', 'bias and fairness', 'data exploration']",[],[],[],[],,,,,,,
https://nips.cc/virtual/2023/poster/71068,Fairness & Bias,Group Fairness in Peer Review,"Large conferences such as NeurIPS and AAAI serve as crossroads  of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a  notion of group fairness, called the core, which requires that every possible community (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by  withdrawing from a large conference. We study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment.  We use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics.",['peer review; group fairness; core; stable'],[],"['Haris Aziz', 'Evi Micha', 'Nisarg Shah']","['University of New South Wales', 'University of Toronto', 'University of Toronto']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72484,Fairness & Bias,Uncovering and Quantifying Social Biases in Code Generation,"With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias.","['Social Bias', 'Code Fairness']",[],"['Xiaokang Chen', 'Zhe Su', 'Fengji Zhang', 'Daoguang Zan', 'Jian-Guang Lou', 'Pin-Yu Chen', 'Tsung-Yi Ho']","['Peking University', 'Zhejiang University', 'City University of', 'University of the Chinese Academy of Sciences', 'Microsoft', 'International Business Machines', 'Department of Computer Science and Engineering, The Chinese University of']","[None, None, 'Hong Kong', None, None, None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/69883,Fairness & Bias,Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions,"Similarity functions measure how comparable pairs of elements are, and play a key role in a wide variety of applications, e.g., notions of Individual Fairness abiding by the seminal paradigm of Dwork et al., as well as Clustering problems. However, access to an accurate similarity function should not always be considered guaranteed, and this point was even raised by Dwork et al. For instance, it is reasonable to assume that when the elements to be compared are produced by different distributions, or in other words belong to different ``demographic'' groups, knowledge of their true similarity might be very difficult to obtain. In this work, we present an efficient sampling framework that learns these across-groups similarity functions, using only a limited amount of experts' feedback. We show analytical results with rigorous theoretical bounds, and empirically validate our algorithms via a large suite of experiments.",['individual fairness; similarity learning; active learning'],[],"['Leonidas Tsepenekas', 'Ivan Brugere', 'Freddy Lecue', 'Daniele Magazzeni']","['J.P. Morgan Chase', 'University of Illinois at Chicago', 'INRIA', ""King's College London""]","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70039,Fairness & Bias,Adapting Fairness Interventions to Missing Values,"Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification—a procedure referred to as ""impute-then-classify""—can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving information encoded within the missing patterns. Numerical experiments with state-of-the-art fairness interventions demonstrate that our adaptive algorithms consistently achieve higher fairness and accuracy than impute-then-classify across different datasets.","['algorithmic fairness', 'discrimination', 'missing values', 'machine learning']",[],"['Raymond Feng', 'Flavio Calmon', 'Hao Wang']","['Harvard University', 'Harvard University', 'MIT-IBM Watson AI Lab']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72714,Fairness & Bias,Long-Term Fairness with Unknown Dynamics,"While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness as an online reinforcement learning problem for a policy affecting human populations. This formulation accommodates dynamical control objectives, such as achieving equitable population states, that cannot be incorporated into static formulations of fairness. We demonstrate that algorithmic solutions to the proposed fairness problem can adapt to unknown dynamics and, by sacrificing short-term incentives, drive the policy-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning and prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness. In the classification setting subject to group fairness, we compare our proposed algorithm to several baselines, including the repeated retraining of myopic or distributionally robust classifiers, and to a deep reinforcement learning algorithm that lacks fairness guarantees. Our experiments model human populations according to evolutionary game theory and integrate real-world datasets.","['Long-term Fairness', 'Dynamics', 'Reinforcement Learning']",[],"['Tongxin Yin', 'Reilly Raab', 'Mingyan Liu', 'Yang Liu']","['University of Michigan - Ann Arbor', 'University of California, Santa Cruz', 'University of Michigan', 'University of California, Santa Cruz']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/74156,Fairness & Bias,Reproducibility Study of “Quantifying Societal Bias Amplification in Image Captioning”,"Scope of reproducibility - We study the reproducibility of the paper ""Quantifying Societal Bias Amplification in Image Captioning"" by Hirota et al. In this paper, the authors propose a new metric to measure bias amplification, called LIC, and evaluate it on multiple image captioning models. Based on this evaluation, they make the following main claims which we aim to verify: (1) all models amplify gender bias, (2) all models amplify racial bias, (3) LIC is robust against encoders, and (4) the NIC+Equalizer model increases gender bias with respect to the baseline. We also extend upon the original work by evaluating LIC for age bias. Methodology - For our reproduction, we were able to run the code provided by the authors without any modifications. For our extension, we automatically labelled the images in the dataset with age annotations and adjusted the code to work with this dataset. In total, 38 GPU hours were needed to perform all experiments. Results - The reproduced results are close to the original results and support all four main claims. Furthermore, our additional results show that only a subset of the models amplifies age bias, while they strengthen the claim that LIC is robust against encoders. However, we acknowledge that our extension to age bias has its limitations. What was easy - The author's code and the data needed to run it are publicly available. The code required no modification to run and the scripts were provided with an extensive argument parser, allowing us to quickly set up our experiments. Moreover, the details of the original experiments were clearly stated in the appendix. What was difficult - We found that it was difficult to interpret the author's code as the provided documentation contained room for improvement. Also, the scripts contained repetitive code. While the authors retrained all image captioning models, they did not share the model weights, making it difficult to extend upon their work. Communication with original authors - No (attempt at) communication with the original authors was performed.","['bias', 'gender bias', 'racial bias', 'age bias', 'societal bias', 'bias amplification', 'leakage', 'image captioning', 'leakage in image captioning', 'bert', 'lstm', 'machine learning', 'deep learning', 'rescience c', 'rescience x', 'reproducibility', 'replication']",[],"['Farrukh Baratov', 'Göksenin Yüksel', 'Darie Petcu']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72918,Fairness & Bias,Fairness Aware Counterfactuals for Subgroups,"In this work, we present Fairness Aware Counterfactuals for Subgroups (FACTS), a framework for auditing subgroup fairness through counterfactual explanations. We start with revisiting (and generalizing) existing notions and introducing new, more refined notions of subgroup fairness. We aim to (a) formulate different aspects of the difficulty of individuals in certain subgroups to achieve recourse, i.e. receive the desired outcome, either at the micro level, considering members of the subgroup individually, or at the macro level, considering the subgroup as a whole, and (b) introduce notions of subgroup fairness that are robust, if not totally oblivious, to the cost of achieving recourse. We accompany these notions with an efficient, model-agnostic, highly parameterizable, and explainable framework for evaluating subgroup fairness. We demonstrate the advantages, the wide applicability, and the efficiency of our approach through a thorough experimental evaluation on different benchmark datasets.","['subgroup fairness', 'recourse', 'counterfactual explanations']",[],"['Loukas Kavouras', 'Konstantinos Tsopelas', 'Giorgos Giannopoulos', 'Dimitris Sacharidis', 'Eleni Psaroudaki', 'Nikolaos Theologitis', 'Dimitrios Rontogiannis', 'Dimitris Fotakis', 'Ioannis Emiris']","['IMIS - ""Athena"" Research Center', 'National Technical University of Athens', 'Athena Research Center', 'ULB', 'IMIS - ""Athena"" Research Center', 'IMIS - ""Athena"" Research Center', 'IMIS - ""Athena"" Research Center', 'Archimedes/Athena RC ', 'Athena Research Center,']","[None, None, None, None, None, None, None, None, 'Greece']",,,,,,,
https://nips.cc/virtual/2023/poster/73502,Fairness & Bias,DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection,"A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detection, called \textit{DeepfakeBench}, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evaluation metrics and protocols to promote transparency and reproducibility.  Featuring an extensible, modular-based codebase, \textit{DeepfakeBench} contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations.  Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (\eg, data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at \url{https://github.com/SCLBD/DeepfakeBench}.","['Deepfake Detection', 'Benchmark']",[],"['Zhiyuan Yan', 'Yong Zhang', 'Xinhang Yuan', 'Siwei Lyu', 'Baoyuan Wu']","['Tencent YouTu Lab', 'Tencent AI Lab', 'Tongji University', 'State University of New York, Buffalo', 'The Chinese University of , Shenzhen']","[None, None, None, None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/70386,Privacy & Data Governance,StateMask: Explaining Deep Reinforcement Learning through State Mask,"Despite the promising performance of deep reinforcement learning (DRL) agents in many challenging scenarios, the black-box nature of these agents greatly limits their applications in critical domains. Prior research has proposed several explanation techniques to understand the deep learning-based policies in RL. Most existing methods explain why an agent takes individual actions rather than pinpointing the critical steps to its final reward. To fill this gap, we propose StateMask, a novel method to identify the states most critical to the agent's final reward. The high-level idea of StateMask is to learn a mask net that blinds a target agent and forces it to take random actions at some steps without compromising the agent's performance. Through careful design, we can theoretically ensure that the masked agent performs similarly to the original agent. We evaluate StateMask in various popular RL environments and show its superiority over existing explainers in explanation fidelity. We also show that StateMask  has better utilities, such as launching adversarial attacks and patching policy errors.","['deep reinforcement learning', 'interpretation', 'explanation']",[],"['Zelei Cheng', 'Xian Wu', 'Jiahao Yu', 'Wenbo Guo', 'Xinyu Xing']","['Northwestern University', 'Northwestern University', 'Northwestern University', 'University of California, Santa Barbara', 'Northwestern University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71582,Privacy & Data Governance,Topological Parallax: A Geometric Specification for Deep Perception Models,"For safety and robustness of AI systems, we introduce _topological parallax_ as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between ""overfitting""' and ""generalization'' in applications of deep-learning. In typical deep-learning applications, an explicit geometric description of the model is impossible, but parallax can estimate topological features (components, cycles, voids, etc.) in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset. Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset. Parallax presents theoretically via topological data analysis [TDA] as a bi-filtered persistence module, and the key properties of this module are stable under perturbation of the reference dataset.","['topological data analysis', 'persistent homology', 'convexity', 'AI safety', 'interpolation']",[],"['Abraham David Smith', 'Michael J. Catanzaro', 'Gabrielle Angeloro', 'Nirav Patel', 'Paul Bendich']","['University of Wisconsin - Stout', 'Geometric Data Analytics', 'Iowa State University', 'Geometric Data Analytics', 'Duke University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72301,Privacy & Data Governance,Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning,"Many problems in Reinforcement Learning (RL) seek an optimal policy with large discrete multidimensional yet unordered action spaces; these include problems in randomized allocation of resources such as placements of multiple security resources and emergency response units, etc. A challenge in this setting is that the underlying action space is categorical (discrete and unordered) and large, for which existing RL methods do not perform well. Moreover, these problems require validity of the realized action (allocation); this validity constraint is often difficult to express compactly in a closed mathematical form. The allocation nature of the problem also prefers stochastic optimal policies, if one exists. In this work, we address these challenges by (1) applying a (state) conditional normalizing flow to compactly represent the stochastic policy — the compactness arises due to the network only producing one sampled action and the corresponding log probability of the action, which is then used by an actor-critic method; and (2) employing an invalid action rejection method (via a valid action oracle) to update the base policy. The action rejection is enabled by a modified policy gradient that we derive. Finally, we conduct extensive experiments to show the scalability of our approach compared to prior methods and the ability to enforce arbitrary state-conditional constraints on the support of the distribution of actions in any state.","['Action constrained reinforcement learning', 'Normalizing flow', 'Generative modelling']",[],"['Ramesha Karunasena', 'Thanh Hong Nguyen', 'Arunesh Sinha', 'Pradeep Varakantham']","['Management University', 'University of Oregon', 'Rutgers University', 'Management University']","['Singapore', None, None, 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/73642,Privacy & Data Governance,DICES Dataset: Diversity in Conversational AI Evaluation for Safety,"Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This requirement overly simplifies the natural subjectivity present in many tasks, and obscures the inherent diversity in human perceptions and opinions about many content items. Preserving the variance in content and diversity in human perceptions in datasets is often quite expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is socio-culturally situated in this context. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographics information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different aggregation strategies. The DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of safety for conversational AI. We further describe a set of metrics that show how rater diversity influences safety perception across different geographic regions, ethnicity groups, age groups, and genders. The goal of the DICES dataset is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.","['conversational AI', 'human evaluation', 'human annotation', 'safety task', 'disagreement', 'variance in human annotations', 'diversity of rater pool']",[],[],[],[],,,,,,,
https://nips.cc/virtual/2023/poster/73531,Privacy & Data Governance,AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator,"Designing robust machine learning systems remains an open problem, and there is a need for benchmark problems that cover both environmental changes and evaluation on a downstream task. In this work, we introduce AVOIDDS, a realistic object detection benchmark for the vision-based aircraft detect-and-avoid problem. We provide a labeled dataset consisting of 72,000 photorealistic images of intruder aircraft with various lighting conditions, weather conditions, relative geometries, and geographic locations.  We also provide an interface that evaluates trained models on slices of this dataset to identify changes in performance with respect to changing environmental conditions. Finally, we implement a fully-integrated, closed-loop simulator of the vision-based detect-and-avoid problem to evaluate trained models with respect to the downstream collision avoidance task. This benchmark will enable further research in the design of robust machine learning systems for use in safety-critical applications. The AVOIDDS dataset and code are publicly available at https://purl.stanford.edu/hj293cv5980 and https://github.com/sisl/VisionBasedAircraftDAA, respectively.","['aviation', 'distribution shift', 'closed-loop evaluation', 'object detection']",[],"['Elysia Quinn Smyers', 'Sydney Michelle Katz', 'Anthony Corso', 'Mykel Kochenderfer']","['Computer Science Department, Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73567,Privacy & Data Governance,Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark,"Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and responsible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium.","['Safe Reinforcement Learning', 'SafeRL', 'RL Simulator']",[],"['Jiaming Ji', 'Borong Zhang', 'Jiayi Zhou', 'Xuehai Pan', 'Weidong Huang', 'Ruiyang Sun', 'Yiran Geng', 'Yifan Zhong', 'Josef Dai', 'Yaodong Yang']","['Peking University', 'Jilin University', ""Xi'an Jiaotong University"", 'Peking University', 'Peking University', 'Peking University', 'Peking University', 'Beijing Institute for General Artificial Intelligence', 'Peking University', 'Peking University']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71849,Privacy & Data Governance,Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders,"Machine Learning as a Service (MLaaS) APIs provide ready-to-use and high-utility encoders that generate vector representations for given inputs. Since these encoders are very costly to train, they become lucrative targets for model stealing attacks during which an adversary leverages query access to the API to replicate the encoder locally at a fraction of the original training costs. We propose *Bucks for Buckets (B4B)*, the first *active defense* that prevents stealing while the attack is happening without degrading representation quality for legitimate API users. Our defense relies on the observation that the representations returned to adversaries who try to steal the encoder's functionality cover a significantly larger fraction of the embedding space than representations of legitimate users who utilize the encoder to solve a particular downstream task. B4B leverages this to adaptively adjust the utility of the returned representations according to a user's coverage of the embedding space. To prevent adaptive adversaries from eluding our defense by simply creating multiple user accounts (sybils), B4B also individually transforms each user's representations. This prevents the adversary from directly aggregating representations over multiple accounts to create their stolen encoder copy. Our active defense opens a new path towards securely sharing and democratizing encoders over public APIs.","['model stealing', 'model defenses', 'self-supervised learning']",[],"['Jan Dubiński', 'Stanisław Pawlak', 'Tomasz Trzcinski']","['Warsaw University of Technology', 'Warsaw University of Technology', 'Warsaw University of Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70169,Privacy & Data Governance,MADG: Margin-based Adversarial Learning for Domain Generalization,"Domain Generalization (DG) techniques have emerged as a popular approach to address the challenges of domain shift in Deep Learning (DL), with the goal of generalizing well to the target domain unseen during the training. In recent years, numerous methods have been proposed to address the DG setting, among which one popular approach is the adversarial learning-based methodology. The main idea behind adversarial DG methods is to learn domain-invariant features by minimizing a discrepancy metric. However, most adversarial DG methods use 0-1 loss based $\mathcal{H}\Delta\mathcal{H}$ divergence metric. In contrast, the margin loss-based discrepancy metric has the following advantages: more informative, tighter, practical, and efficiently optimizable. To mitigate this gap, this work proposes a novel adversarial learning DG algorithm, $\textbf{MADG}$, motivated by a margin loss-based discrepancy metric. The proposed $\textbf{MADG}$ model learns domain-invariant features across all source domains and uses adversarial training to generalize well to the unseen target domain. We also provide a theoretical analysis of the proposed $\textbf{MADG}$ model based on the unseen target error bound. Specifically, we construct the link between the source and unseen domains in the real-valued hypothesis space and derive the generalization bound using margin loss and Rademacher complexity. We extensively experiment with the $\textbf{MADG}$ model on popular real-world DG datasets, VLCS, PACS, OfficeHome, DomainNet, and TerraIncognita. We evaluate the proposed algorithm on DomainBed's benchmark and observe consistent performance across all the datasets.","['Domain Generalization', 'Margin Loss', 'Adversarial Learning', 'Domain Adaptation']",[],"['Aveen Dayal', 'Vimal K B', 'Linga Reddy Cenkeramaddi', 'C Krishna Mohan', 'Abhinav Kumar', 'Vineeth N. Balasubramanian']","['n Institute of Technology, Hyderabad', 'n Institute of Technology, Hyderabad', 'University of Agder', 'n Institute of Technology Hyderabad', 'n Institute of Technology, Hyderabad', 'n Institute of Technology Hyderabad']","['India', 'India', None, 'India', 'India', 'India']",,,,,,,
https://nips.cc/virtual/2023/poster/71051,Privacy & Data Governance,On the Gini-impurity Preservation For Privacy Random Forests,"Random forests have been one successful ensemble algorithms in machine learning. Various techniques have been utilized to preserve the privacy of random forests from anonymization, differential privacy, homomorphic encryption, etc., whereas it rarely takes into account some crucial ingredients of learning algorithm. This work presents a new encryption to preserve data's Gini impurity, which plays a crucial role during the construction of random forests. Our basic idea is to modify the structure of binary search tree to store several examples in each node,  and encrypt data features by incorporating label and order information. Theoretically, we prove that our scheme preserves the minimum Gini impurity in ciphertexts without decrypting, and present the security guarantee for encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels  due to their importance and privacy. We  conduct extensive experiments to show the effectiveness, efficiency and security of our proposed method.","['classification', 'random forests', 'privacy-preserving machine learng', 'data encrytion']",[],"['XinRan Xie', 'Xuetong Bai', 'Wei Gao', 'Zhi-Hua Zhou']","['Nanjing University', 'nanjing university', 'Nanjing University', 'Nanjing University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71628,Privacy & Data Governance,A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning,"Federated Learning (FL) is a distributed machine learning paradigm that allows multiple clients to train a global model collaboratively without sharing their local training data. Due to its distributed nature, many studies have shown that it is vulnerable to backdoor attacks. However, existing studies usually used a predetermined, fixed backdoor trigger or optimized it based solely on the local data and model without considering the global training dynamics. This leads to sub-optimal and less durable attack effectiveness, i.e., their attack success rate is low when the attack budget is limited and decreases quickly if the attacker can no longer perform attacks anymore. To address these limitations, we propose A3FL, a new backdoor attack which adversarially adapts the backdoor trigger to make it less likely to be removed by the global training dynamics. Our key intuition is that the difference between the global model and the local model in FL makes the local-optimized trigger much less effective when transferred to the global model. We solve this by optimizing the trigger to even survive the worst-case scenario where the global model was trained to directly unlearn the trigger. Extensive experiments on benchmark datasets are conducted for twelve existing defenses to comprehensively evaluate the effectiveness of our A3FL. Our code is available at https://github.com/hfzhang31/A3FL.","['Backdoor Attack', 'Federated Learning']",[],"['Hangfan Zhang', 'Jinyuan Jia', 'Jinghui Chen', 'Lu Lin', 'Dinghao Wu']","['Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70289,Privacy & Data Governance,Private Federated Frequency Estimation: Adapting to the Hardness of the Instance,"In federated frequency estimation (FFE), multiple clients work together to estimate the frequency of their local data by communicating with a server, while maintaining the security constraint of $\mathtt{secsum}$ where the server can only access the sum of client-held vectors. For FFE with a single communication round, it is known that count sketch is nearly information-theoretically optimal [Chen et al., 2022]. However, when multiple communication rounds are allowed, we propose a new sketch algorithm that is provably more accurate than a naive adaptation of count sketch. Furthermore, we show that both our sketch algorithm and count sketch can achieve better accuracy when the problem instance is simpler. Therefore, we propose a two-phase approach to enable the use of a smaller sketch size for simpler problems. Finally, we provide mechanisms to make our proposed algorithm differentially private. We verify the performance of our methods through experiments conducted on real datasets.","['sketch', 'federated analytics', 'privacy']",[],"['Jingfeng Wu', 'Wennan Zhu', 'Peter Kairouz', 'Vladimir Braverman']","['University of California, Berkeley', 'Google', 'Google', 'Rice University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71209,Privacy & Data Governance,Students Parrot Their Teachers: Membership Inference on Model Distillation,"Model distillation is frequently proposed as a technique to reduce the privacy leakage of machine learning. These empirical privacy defenses rely on the intuition that distilled ``student'' models protect the privacy of training data, as they only interact with this data indirectly through a ``teacher'' model. In this work, we design membership inference attacks to systematically study the privacy provided by knowledge distillation to both the teacher and student training sets. Our new attacks show that distillation alone provides only limited privacy across a number of domains. We explain the success of our attacks on distillation by showing that membership inference attacks on a private dataset can succeed even if the target model is never queried on any actual training points, but only on inputs whose predictions are highly influenced by training data. Finally, we show that our attacks are strongest when student and teacher sets are similar, or when the attacker can poison the teacher set.","['model distillation', 'membership inference', 'privacy', 'dark knowledge']",[],"['Matthew Jagielski', 'Milad Nasr', 'Katherine Lee', 'Christopher A. Choquette-Choo', 'Nicholas Carlini', 'Florian Tramèr']","['Google', 'Google', 'Cornell University', 'Google Research, Brain team', 'Google', 'ETHZ - ETH Zurich']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73597,Privacy & Data Governance,Ethical Considerations for Responsible Data Curation,"Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.","['human-centric', 'datasets', 'computer vision', 'fairness', 'algorithmic bias', 'robustness', 'responsible AI']",[],"['Jerone Andrews', 'Dora Zhao', 'William Thong', 'Orestis Papakyriakopoulos']","['Sony AI', 'Stanford University', 'Sony AI', 'Sony AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70814,Privacy & Data Governance,Resilient Constrained Learning,"When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems that adapt to disruptions by modifying their operation. We show conditions under which this balance can be achieved and introduce a practical algorithm to compute it, for which we derive approximation and generalization guarantees. We showcase the advantages of this resilient learning method in image classification tasks involving multiple potential invariances and in federated learning under distribution shift.","['Constrained Learning', 'Relaxation', 'Lagrangian duality', 'Primal-Dual', 'Machine Learning', 'Federated Learning', 'Invariance']",[],"['Ignacio Hounie', 'Alejandro Ribeiro', 'Luiz F. O. Chamon']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'Universität Stuttgart']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71031,Privacy & Data Governance,Causal-structure Driven Augmentations for Text OOD Generalization,"The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic data, we demonstrate that our method for simulating interventions improves out-of-distribution (OOD) accuracy compared to baseline invariant learning algorithms.","['Counterfactually Augmented Data', 'Invariant Learning', 'Out-of-distribution Generalization', 'Clinical NLP']",[],"['Amir Feder', 'Yoav Wald', 'Claudia Shi', 'Suchi Saria', 'David Blei']","['Columbia University', 'Johns Hopkins University', 'Columbia University', 'Johns Hopkins University', 'Columbia University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70160,Privacy & Data Governance,Exploring Geometry of Blind Spots in Vision models,"Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of “equi-confidence” level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary images from other classes. We further observe that the source image is linearly connected by a high-confidence path to these inputs, uncovering a star-like structure for level sets of deep networks. Furthermore, we attempt to identify and estimate the extent of these connected higher-dimensional regions over which the model maintains a high degree of confidence.","['Neural networks', 'Vision models', 'blind spots', 'undersensitivity', 'invariance', 'level set geometry', 'input connectivity']",[],"['Sriram Balasubramanian', 'Gaurang Sriramanan', 'Vinu Sankar Sadasivan', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70886,Privacy & Data Governance,Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection,"Adversarial contrastive learning (ACL) does not require expensive data annotations but outputs a robust representation that withstands adversarial attacks and also generalizes to a wide range of downstream tasks. However, ACL needs tremendous running time to generate the adversarial variants of all training data, which limits its scalability to large datasets. To speed up ACL, this paper proposes a robustness-aware coreset selection (RCS) method. RCS does not require label information and searches for an informative subset that minimizes a representational divergence, which is the distance of the representation between natural data and their virtual adversarial variants. The vanilla solution of RCS via traversing all possible subsets is computationally prohibitive. Therefore, we theoretically transform RCS into a surrogate problem of submodular maximization, of which the greedy search is an efficient solution with an optimality guarantee for the original problem. Empirically, our comprehensive results corroborate that RCS can speed up ACL by a large margin without significantly hurting the robustness transferability. Notably, to the best of our knowledge, we are the first to conduct ACL efficiently on the large-scale ImageNet-1K dataset to obtain an effective robust representation via RCS. Our source code is at https://github.com/GodXuxilie/Efficient_ACL_via_RCS.","['robust pre-training', 'adversarial contrastive learning', 'coreset selection']",[],"['Xilie Xu', 'Jingfeng Zhang', 'Feng Liu', 'Masashi Sugiyama', 'Mohan Kankanhalli']","['National University of', 'RIKEN', 'University of Melbourne', 'RIKEN', 'National University of']","['Singapore', None, None, None, 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/69867,Privacy & Data Governance,Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization,"Adversarial contrastive learning (ACL) is a technique that enhances standard contrastive learning (SCL) by incorporating adversarial data to learn a robust representation that can withstand adversarial attacks and common corruptions without requiring costly annotations. To improve transferability, the existing work introduced the standard invariant regularization (SIR) to impose style-independence property to SCL, which can exempt the impact of nuisance style factors in the standard representation. However, it is unclear how the style-independence property benefits ACL-learned robust representations. In this paper, we leverage the technique of causal reasoning to interpret the ACL and propose adversarial invariant regularization (AIR) to enforce independence from style factors. We regulate the ACL using both SIR and AIR to output the robust representation. Theoretically, we show that AIR implicitly encourages the representational distance between different views of natural data and their adversarial variants to be independent of style factors. Empirically, our experimental results show that invariant regularization significantly improves the performance of state-of-the-art ACL methods in terms of both standard generalization and robustness on downstream tasks. To the best of our knowledge, we are the first to apply causal reasoning to interpret ACL and develop AIR for enhancing ACL-learned robust representations. Our source code is at https://github.com/GodXuxilie/Enhancing_ACL_via_AIR.","['robust pre-training', 'adversarial contrastive learning']",[],"['Xilie Xu', 'Jingfeng Zhang', 'Feng Liu', 'Masashi Sugiyama', 'Mohan Kankanhalli']","['National University of', 'RIKEN', 'University of Melbourne', 'RIKEN', 'National University of']","['Singapore', None, None, None, 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/73512,Privacy & Data Governance,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,"In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.","['Reinforcement Learning with Human Feedback (RLHF)', 'Large Language Model', 'Safety Alignment', 'Human-Preference Data', 'Harmlessness-Helpfulness Tension']",[],"['Jiaming Ji', 'Mickel Liu', 'Juntao Dai', 'Xuehai Pan', 'Chi Zhang', 'Ce Bian', 'Boyuan Chen', 'Ruiyang Sun', 'Yizhou Wang', 'Yaodong Yang']","['Peking University', 'University of Washington', 'Peking University', 'Peking University', 'Peking University', 'University College London, University of London', 'Peking University', 'Peking University', 'Peking University', 'Peking University']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72371,Privacy & Data Governance,Off-Policy Evaluation for Human Feedback,"Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and are only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we develop an immediate human reward (IHR) reconstruction approach, regularized by environmental knowledge distilled in a latent space that captures the underlying dynamics of state transitions as well as issuing HF signals. Our approach has been tested over *two real-world experiments*, adaptive *in-vivo* neurostimulation and intelligent tutoring, and a simulation environment (visual Q&A). Results show that our approach significantly improves the performance toward estimating HF signals accurately, compared to directly applying (variants of) existing OPE methods.","['Off-policy evaluation (OPE)', 'Variational latent model for trajectory representation learning', 'Reinforcement learning and OPE for adaptive neurostimulation']",[],"['Qitong Gao', 'Ge Gao', 'Min Chi', 'Miroslav Pajic']","['Duke University', 'North Carolina State University', 'North Carolina State University', 'Duke University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73579,Privacy & Data Governance,Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis,"We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.","['Graph Neural Networks', 'Road Safety', 'Transportation Networks']",[],"['Abhinav Nippani', 'Dongyue Li', 'Haotian Ju', 'Haris Koutsopoulos', 'Hongyang R. Zhang']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'Computer Science, Northeastern University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71476,Privacy & Data Governance,Lockdown: Backdoor Defense for Federated Learning  with Isolated Subspace Training,"Federated learning (FL) is vulnerable to backdoor attacks due to its distributed computing nature. Existing defense solution usually requires larger amount of computation in either the training or testing phase, which limits their practicality in the resource-constrain scenarios. A more practical defense, i.e., neural network (NN) pruning based defense has been proposed in centralized backdoor setting. However, our empirical study shows that traditional pruning-based solution suffers \textit{poison-coupling} effect in FL, which significantly degrades the defense performance.This paper presents Lockdown, an isolated subspace training method to mitigate the poison-coupling effect. Lockdown follows three key procedures. First, it modifies the training protocol by isolating the training subspaces for different clients. Second, it utilizes randomness in initializing isolated subspacess, and performs subspace pruning and subspace recovery to segregate the subspaces between malicious and benign clients. Third, it introduces quorum consensus to cure the global model by purging malicious/dummy parameters. Empirical results show that Lockdown achieves \textit{superior} and \textit{consistent} defense performance compared to existing representative approaches against backdoor attacks. Another value-added property of Lockdown is the communication-efficiency and model complexity reduction, which are both critical for resource-constrain FL scenario. Our code is available at \url{https://github.com/git-disl/Lockdown}.","['Federated learning', 'backdoor defense', 'isolated subspace training.']",[],"['Tiansheng Huang', 'Sihao Hu', 'Ka-Ho Chow', 'Fatih Ilhan', 'Selim Furkan Tekin', 'Ling Liu']","['Institute of Technology', 'Institute of Technology', 'International Business Machines', 'Institute of Technology', 'College of Computing,  Institute of Technology', 'Institute of Technology']","['Georgia', 'Georgia', None, 'Georgia', 'Georgia', 'Georgia']",,,,,,,
https://nips.cc/virtual/2023/poster/70499,Privacy & Data Governance,FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning,"Federated learning (FL) provides a distributed training paradigm where multiple clients can jointly train a global model without sharing their local data. However, recent studies have shown that FL offers an additional surface for backdoor attacks. For instance, an attacker can compromise a subset of clients and thus corrupt the global model to misclassify an input with a backdoor trigger as the adversarial target. Existing defenses for FL against backdoor attacks usually detect and exclude the corrupted information from the compromised clients based on a static attacker model. However, such defenses are inadequate against dynamic attackers who strategically adapt their attack strategies. To bridge this gap, we model the strategic interactions between the defender and dynamic attackers as a minimax game. Based on the analysis of the game, we design an interactive defense mechanism FedGame. We prove that under mild assumptions, the global model trained with FedGame under backdoor attacks is close to that trained without attacks. Empirically, we compare FedGame with multiple state-of-the-art baselines on several benchmark datasets under various attacks. We show that FedGame can effectively defend against strategic attackers and achieves significantly higher robustness than baselines. Our code is available at: https://github.com/AI-secure/FedGame.","['backdoor defense', 'federated learning', 'game theory']",[],"['Jinyuan Jia', 'Zhuowen Yuan', 'Dinuka Sahabandu', 'Luyao Niu', 'Arezoo Rajabi', 'Bhaskar Ramasubramanian', 'Bo Li', 'Radha Poovendran']","['Pennsylvania State University', 'University of Illinois at Urbana-Champaign', 'University of Washington, Seattle', 'University of Washington', 'University of Washington', 'Western Washington University', 'University of Illinois, Urbana Champaign', 'University of Washington, Seattle']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72293,Privacy & Data Governance,Honesty Is the Best Policy: Defining and Mitigating AI Deception,"Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being evaluated as truthful). There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a formal definition of deception in structural causal games, grounded in the philosophy literature, and applicable to real-world machine learning systems. Several examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception. Our main technical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models.","['Deception', 'Causality', 'Game Theory']",[],"['Francis Rhys Ward', 'Francesca Toni', 'Francesco Belardinelli', 'Tom Everitt']","['Imperial College London', 'Imperial College London', 'Imperial College London', 'DeepMind']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71637,Privacy & Data Governance,"Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities","Variational inequalities are a broad and flexible class of problems that includes minimization, saddle point, and fixed point problems as special cases. Therefore, variational inequalities are used in various applications ranging from equilibrium search to adversarial learning. With the increasing size of data and models, today's instances demand parallel and distributed computing for real-world machine learning problems, most of which can be represented as variational inequalities. Meanwhile, most distributed approaches have a significant bottleneck -- the cost of communications. The three main techniques to reduce the total number of communication rounds and the cost of one such round are the similarity of local functions, compression of transmitted information, and local updates. In this paper, we combine all these approaches. Such a triple synergy did not exist before for variational inequalities and saddle problems, nor even for minimization problems. The methods presented in this paper have the best theoretical guarantees of communication complexity and are significantly ahead of other methods for distributed variational inequalities. The theoretical results are confirmed by adversarial learning experiments on synthetic and real datasets.","['convex optimization', 'variational inequalities', 'similarity', 'local methods', 'compression', 'partial participation']",[],"['Aleksandr Beznosikov', 'Martin Takáč', 'Alexander Gasnikov']","['Moscow Institute of Physics and Technology', 'Mohamed bin Zayed University of Artificial Intelligence', 'Moscow Institute of Physics and Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72180,Privacy & Data Governance,CBD: A Certified Backdoor Detector Based on Local Dominant Probability,"Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves comparable or even higher detection accuracy than state-of-the-art detectors, and it in addition provides detection certification. Notably, for backdoor attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\% (84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, with low false positive rates.","['backdoor', 'Trojan', 'certification', 'adversarial learning', 'deep neural network', 'conformal prediction']",[],"['Zhen Xiang', 'Zidi Xiong', 'Bo Li']","['University of Illinois Urbana-Champaign', 'Department of Computer Science', 'University of Illinois, Urbana Champaign']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70434,Privacy & Data Governance,FlowPG: Action-constrained Policy Gradient with Normalizing Flows,"Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learned normalizing flow with the DDPG algorithm. By design, a well-trained normalizing flow will transform policy output into a valid action without requiring an optimization solver. Empirically, our approach results in significantly fewer constraint violations (upto an order-of-magnitude for several instances) and is multiple times faster on a variety of continuous control tasks.","['action-constrained reinforcement learning', 'decision making']",[],"['Janaka Chathuranga Brahmanage', 'Jiajing Ling', 'Akshat Kumar']","['Management University', 'Management University', 'Management University']","['Singapore', 'Singapore', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/69885,Privacy & Data Governance,Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction,"Many security applications require unsupervised anomaly detection, as malicious data are extremely rare and often only unlabeled normal data are available for training (i.e., zero-positive). However, security operators are concerned about the high stakes of trusting black-box models due to their lack of interpretability. In this paper, we propose a post-hoc method to globally explain a black-box unsupervised anomaly detection model via rule extraction. First, we propose the concept of distribution decomposition rules that decompose the complex distribution of normal data into multiple compositional distributions. To find such rules, we design an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the boundary inference rules that estimate the decision boundary of the original model on each compositional distribution. By merging these two types of rules into a rule set, we can present the inferential process of the unsupervised black-box model in a human-understandable way, and build a surrogate rule-based model for online deployment at the same time. We conduct comprehensive experiments on the explanation of four distinct unsupervised anomaly detection models on various real-world datasets. The evaluation shows that our method outperforms existing methods in terms of diverse metrics including fidelity, correctness and robustness.","['unsupervised anomaly detection', 'global explanation', 'rule extraction']",[],"['Ruoyu Li', 'Qing Li', 'Yu Zhang', 'Dan Zhao', 'Yong Jiang']","['Tsinghua University, Tsinghua University', 'Peng Cheng Laboratory', 'Tsinghua University, Tsinghua University', 'Peng Cheng Laborotary', 'Tsinghua University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71993,Privacy & Data Governance,Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings,"Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on tabular data. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain categorical features, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training). In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via universal robust embeddings tailored to categorical data. These embeddings, created using a bilevel alternating minimization framework, can be transferred to boosted trees or random forests making them robust without the need for adversarial training while preserving their high accuracy on tabular data. We show that our methods outperform existing techniques within a practical threat model suitable for tabular data.","['Tabular data', 'Categorical data', 'Robust ML', 'Adversarial Robustness']",[],"['Maksym Andriushchenko', 'Carmela Troncoso', 'Nicolas Flammarion']","['Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70409,Privacy & Data Governance,RETVec: Resilient and Efficient Text Vectorizer,"This paper describes RETVec, an efficient, resilient, and multilingual text vectorizer designed for neural-based text processing. RETVec combines a novel character encoding with an optional small embedding model to embed words into a 256-dimensional vector space. The RETVec embedding model is pre-trained using pair-wise metric learning to be robust against typos and character-level adversarial attacks. In this paper, we evaluate and compare RETVec to state-of-the-art vectorizers and word embeddings on popular model architectures and datasets. These comparisons demonstrate that RETVec leads to competitive, multilingual models that are significantly more resilient to typos and adversarial text attacks. RETVec is available under the Apache 2 license at https://github.com/google-research/retvec.","['language modeling', 'text embedding', 'adversarial text attack', 'text vectorization']",[],"['Elie Bursztein', 'Marina Zhang', 'Owen Skipper Vallis', 'Alexey Kurakin']","['Google', 'Google', 'Google', 'Research, Google']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72546,Privacy & Data Governance,Fantastic Robustness Measures: The Secrets of Robust Generalization,"Adversarial training has become the de-facto standard method for improving the robustness of models against adversarial examples. However, robust overfitting remains a significant challenge, leading to a large gap between the robustness on the training and test datasets. To understand and improve robust generalization, various measures have been developed, including margin, smoothness, and flatness-based measures. In this study, we present a large-scale analysis of robust generalization to empirically verify whether the relationship between these measures and robust generalization remains valid in diverse settings. We demonstrate when and how these measures effectively capture the robust generalization gap by comparing over 1,300 models trained on CIFAR-10 under the $L_\infty$ norm and further validate our findings through an evaluation of more than 100 models from RobustBench across CIFAR-10, CIFAR-100, and ImageNet. We hope this work can help the community better understand adversarial robustness and motivate the development of more robust defense methods against adversarial attacks.","['Adversarial Robustness', 'Generalization', 'Measures']",[],"['Hoki Kim', 'Jinseong Park', 'Yujin Choi', 'Jaewook Lee']","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71755,Privacy & Data Governance,Responsible AI (RAI) Games and Ensembles,"Several recent works have studied the societal effects of AI; these include issues such as fairness, robustness, and safety.  In many of these objectives, a learner seeks to minimize its worst-case loss over a set of predefined distributions (known as uncertainty sets), with usual examples being perturbed versions of the empirical distribution. In other words, the aforementioned problems can be written as min-max problems over these uncertainty sets. In this work, we provide a general framework for studying these problems, which we refer to as Responsible AI (RAI) games. We provide two classes of algorithms for solving these games:  (a) game-play based algorithms, and (b) greedy stagewise estimation algorithms. The former class is motivated by online learning and game theory, whereas the latter class is motivated by the classical statistical literature on boosting, and regression. We empirically demonstrate the applicability and competitive performance of our techniques for solving several RAI problems, particularly around subpopulation shift.","['Responsible AI', 'fairness', 'DRO', 'robustness']",[],"['Yash Gupta', 'Runtian Zhai', 'Arun Suggala', 'Pradeep Kumar Ravikumar']","['School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'Google', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72984,Privacy & Data Governance,Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition,"Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race.  Conventional wisdom dictates that model biases arise from biased training data.  As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition.  In our work, we discover that biases are actually inherent to neural network architectures themselves.  Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairness, often by large margins, on the two most widely used datasets for face identification, CelebA and VGGFace2. Furthermore, these models generalize to other datasets and sensitive attributes. We release our code, models and raw data files at https://github.com/dooleys/FR-NAS.","['Bias Mitigation', 'Fairness', 'Facial Recognition']",[],"['Rhea Sanjay Sukthanker', 'John P Dickerson', 'Colin White', 'Frank Hutter', 'Micah Goldblum']","['University of Freiburg, Albert-Ludwigs-Universität Freiburg', 'University of Maryland, College Park', 'California Institute of Technology', 'University of Freiburg & Bosch', 'New York University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71912,Privacy & Data Governance,Enhancing Adversarial Robustness via Score-Based Optimization,"Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data  in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustness performance and inference speed.","['Adversarial Defense', 'Adversarial Attack', 'Score-based Models', 'Diffusion Models']",[],"['Boya Zhang', 'Weijian Luo', 'Zhihua Zhang']","['Peking University', 'Peking University', 'Shanghai Jiao Tong University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71747,Privacy & Data Governance,One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning,"Densely structured pruning methods utilizing simple pruning heuristics can deliver immediate compression and acceleration benefits with acceptable benign performances. However, empirical findings indicate such naively pruned networks are extremely fragile under simple adversarial attacks. Naturally, we would be interested in knowing if such a phenomenon also holds for carefully designed modern structured pruning methods. If so, then to what extent is the severity? And what kind of remedies are available? Unfortunately, both the questions and the solution remain largely unaddressed: no prior art is able to provide a thorough investigation on the adversarial performance of modern structured pruning methods (spoiler: it is not good), yet the few works that attempt to provide mitigation often do so at various extra costs with only to-be-desired performance. In this work, we answer both questions by fairly and comprehensively investigating the adversarial performance of 10+ popular structured pruning methods. Solution-wise, we take advantage of *Grouped Kernel Pruning (GKP)*'s recent success in pushing densely structured pruning freedom to a more fine-grained level. By mixing up kernel smoothness — a classic robustness-related kernel-level metric — into a modified GKP procedure, we present a one-shot-post-train-weight-dependent GKP method capable of advancing SOTA performance on both the benign and adversarial scale, while requiring no extra (in fact, often less) cost than a standard pruning procedure. Please refer to our [GitHub repository](https://github.com/henryzhongsc/adv_robust_gkp) for code implementation, tool sharing, and model checkpoints.","['pruning', 'structured pruning', 'adversarial robustness', 'grouped kernel pruning', 'CNN', 'one-shot']",[],"['Shaochen Zhong', 'Zaichuan You', 'Jiamu Zhang', 'Sebastian Zhao', 'Zachary LeClaire', 'Zirui Liu', 'Daochen Zha', 'Vipin Chaudhary', 'Shuai Xu', 'Xia Hu']","['Rice University', 'Case Western Reserve University', 'Case Western Reserve University', 'UC Berkeley, University of California, Berkeley', 'Case Western Reserve University', 'Rice University', 'Rice University', 'Case Western Reserve University', 'Case Western Reserve University', 'Rice University']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71801,Privacy & Data Governance,Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization,"Single-step adversarial training (SSAT) has demonstrated the potential to achieve both efficiency and robustness. However, SSAT suffers from catastrophic overfitting (CO), a phenomenon that leads to a severely distorted classifier, making it vulnerable to multi-step adversarial attacks. In this work, we observe that some adversarial examples generated on the SSAT-trained network exhibit anomalous behaviour, that is, although these training samples are generated by the inner maximization process, their associated loss decreases instead, which we named abnormal adversarial examples (AAEs). Upon further analysis, we discover a close relationship between AAEs and classifier distortion, as both the number and outputs of AAEs undergo a significant variation with the onset of CO. Given this observation, we re-examine the SSAT process and uncover that before the occurrence of CO, the classifier already displayed a slight distortion, indicated by the presence of few AAEs. Furthermore, the classifier directly optimizing these AAEs will accelerate its distortion, and correspondingly, the variation of AAEs will sharply increase as a result. In such a vicious circle, the classifier rapidly becomes highly distorted and manifests as CO within a few iterations. These observations motivate us to eliminate CO by hindering the generation of AAEs. Specifically, we design a novel method, termed Abnormal Adversarial Examples Regularization (AAER), which explicitly regularizes the variation of AAEs to hinder the classifier from becoming distorted. Extensive experiments demonstrate that our method can effectively eliminate CO and further boost adversarial robustness with negligible additional computational overhead. Our implementation can be found at https://github.com/tmllab/2023_NeurIPS_AAER.","['adversarial training', 'catastrophic overfitting']",[],"['Chaojian Yu', 'Tongliang Liu']","['The University of Sydney, University of Sydney', 'University of Sydney']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71808,Privacy & Data Governance,Red Teaming Deep Neural Networks with Feature Synthesis Tools,"Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods that do not depend on a dataset. In this paper, we benchmark the usefulness of interpretability tools for model debugging. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when a user's interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation.","['interpretability', 'benchmarking', 'auditing', 'diagnostics', 'debugging', 'adversarial attacks', 'feature synthesis']",[],"['Stephen Casper', 'Tong Bu', 'Yuxiao Li', 'Jiawei Li', 'Kevin Zhang', 'Kaivalya Hariharan', 'Dylan Hadfield-Menell']","['Massachusetts Institute of Technology', 'Peking University', 'Department of Electronic Engineering, Tsinghua University', 'INSC, Tsinghua University', 'Peking University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70702,Privacy & Data Governance,Jailbroken: How Does LLM Safety Training Fail?,"Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of “jailbreak” attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model’s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI’s GPT-4 and Anthropic’s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models’ red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity—that safety mechanisms should be as sophisticated as the underlying model—and argues against the idea that scaling alone can resolve these safety failure modes.","['red teaming', 'safety', 'RLHF', 'large language models']",[],"['Alexander Wei', 'Nika Haghtalab', 'Jacob Steinhardt']","['University of California Berkeley', 'University of California Berkeley', 'University of California Berkeley']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71578,Privacy & Data Governance,Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management,"Reinforcement learning (RL) has shown great promise for developing agents for dialogue management (DM) that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite the advancements in RL and language models (LMs), employing RL to drive conversational chatbots still poses significant challenges. A primary issue stems from RL’s dependency on online exploration for effective learning, a process that can be costly. Moreover, engaging in online interactions with humans during the training phase can raise safety concerns, as the LM can potentially generate unwanted outputs. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop various RL algorithms, specialized in dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs)---models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting the MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM. We evaluate our methods in open-domain dialogue to demonstrate their effectiveness with respect to the diversity of intent in generated utterances and overall DM performance.","['Reinforcement Learning', 'Mixture of Experts', 'Dialogue Management']",[],"['Dhawal Gupta', 'Yinlam Chow', 'Azamat Tulepbergenov', 'Mohammad Ghavamzadeh', 'Craig Boutilier']","['College of Information and Computer Science, University of Massachusetts at Amherst', 'Google Research', 'Google', 'Amazon', 'Google']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70392,Privacy & Data Governance,Label Poisoning is All You Need,"In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.","['security', 'backdoor attack']",[],"['Rishi Dev Jha', 'Jonathan Hayase', 'Sewoong Oh']","['Cornell University', 'University of Washington', 'University of Washington']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71421,Privacy & Data Governance,Black-box Backdoor Defense via Zero-shot Image Purification,"Backdoor attacks inject poisoned samples into the training data, resulting in the misclassification of the poisoned input during a model's deployment. Defending against such attacks is challenging, especially for real-world black-box models where only query access is permitted. In this paper, we propose a novel defense framework against backdoor attacks through Zero-shot Image Purification (ZIP). Our framework can be applied to poisoned models without requiring internal information about the model or any prior knowledge of the clean/poisoned samples. Our defense framework involves two steps. First, we apply a linear transformation (e.g., blurring) on the poisoned image to destroy the backdoor pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process by using the transformed image to guide the generation of high-fidelity purified images, which works in zero-shot settings. We evaluate our ZIP framework on multiple datasets with different types of attacks. Experimental results demonstrate the superiority of our ZIP framework compared to state-of-the-art backdoor defense baselines. We believe that our results will provide valuable insights for future defense methods for black-box models. Our code is available at https://github.com/sycny/ZIP.","['backdoor defense', 'black-box defense', 'diffusion model']",[],"['Yucheng Shi', 'Mengnan Du', 'Xuansheng Wu', 'Zihan Guan', 'Jin Sun', 'Ninghao Liu']","['University of', 'New  Institute of Technology', 'University of', 'University of Virginia, Charlottesville', 'University of', 'University of']","['Georgia', 'Jersey', 'Georgia', None, 'Georgia', 'Georgia']",,,,,,,
https://nips.cc/virtual/2023/poster/70063,Privacy & Data Governance,A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP),"Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety measures beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related objectives is less-explored. Driven by the above, this work comprehensively investigates the safety measures of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study $83$ CLIP models and $127$ ImageNet classifiers. They are diverse in architecture (pre)training distribution and training strategies. We consider $10$ visual factors (\emph{e.g.}, shape and pattern), $5$ types of out-of-distribution data, and $8$ natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three key properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models.",['CLIP'],[],"['Weijie Tu', 'Weijian Deng', 'Tom Gedeon']","['n National University', 'n National University', 'Curtin University of Technology']","['Australia', 'Australia', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71118,Privacy & Data Governance,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs—e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always ""(A)""—which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.","['Natural language processing', 'large language models', 'XAI', 'explainability']",[],"['Miles Turpin', 'Julian Michael', 'Ethan Perez']","['New York University', 'New York University', 'New York University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72028,Privacy & Data Governance,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.",['adversarial training; regularization; linear models'],[],"['Antonio H. Ribeiro', 'Dave Zachariah', 'Francis Bach', 'Thomas B. Schön']","['Uppsala University', 'Uppsala University', 'Ecole Normale Superieure', 'Uppsala University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73006,Privacy & Data Governance,Reliable learning in challenging environments,"The problem of designing learners that provide guarantees that their predictions are provably correct is of increasing importance in machine learning. However, learning theoretic guarantees have only been considered in very specific settings.  In this work, we consider the design and analysis of reliable learners in challenging test-time environments as encountered in modern machine learning problems: namely adversarial test-time attacks (in several variations) and natural distribution shifts.  In this work, we provide a reliable learner with provably optimal guarantees in such settings. We discuss computationally feasible implementations of the learner and further show that our algorithm achieves strong positive performance guarantees on several natural examples: for example, linear separators under log-concave distributions or smooth boundary classifiers under smooth probability distributions.","['Reliable machine learning', 'adversarial robustness', 'distribution shift', 'theory']",[],"['Nina Balcan', 'Steve Hanneke', 'Rattana Pukdee', 'Dravyansh Sharma']","['Carnegie Mellon University', 'Purdue University', 'CMU, Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70914,Privacy & Data Governance,Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents,"In this paper, we introduce a generalization of the standard Stackelberg Games (SGs) framework: _Calibrated Stackelberg Games_. In CSGs, a principal repeatedly interacts with an agent who (contrary to standard SGs) does not have direct access to the principal's action but instead best responds to _calibrated forecasts_ about it. CSG is a powerful modeling tool that goes beyond assuming that agents use ad hoc and highly specified algorithms for interacting in strategic settings to infer the principal's actions  and thus more robustly addresses real-life applications that SGs were originally intended to capture. Along with CSGs, we also introduce a stronger notion of calibration, termed _adaptive calibration_, that provides fine-grained any-time calibration guarantees against adversarial sequences. We give a general approach for obtaining adaptive calibration algorithms and specialize them for finite CSGs. In our main technical result, we show that in CSGs, the principal can achieve utility that converges to the optimum Stackelberg value of the game both in _finite_ and _continuous_ settings and that no higher utility is achievable. Two prominent and immediate applications of our results are the settings of learning in Stackelberg Security Games and strategic classification, both against _calibrated_ agents.","['calibration', 'Stackelberg games', 'learning in repeated games', 'strategic agents', 'best response', 'strategic classification', 'Stackelberg Security Games']",[],"['Nika Haghtalab', 'Chara Podimata', 'Kunhe Yang']","['University of California Berkeley', 'Massachusetts Institute of Technology', 'University of California, Berkeley']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72402,Privacy & Data Governance,SALSA VERDE: a machine learning attack on LWE with sparse small secrets,"Learning with Errors (LWE) is a hard math problem used in post-quantum cryptography. Homomorphic Encryption (HE) schemes rely on the hardness of the LWE problem for their security, and two LWE-based cryptosystems were recently standardized by NIST for digital signatures and key exchange (KEM).  Thus, it is critical to continue assessing the security of LWE and specific parameter choices. For example, HE uses secrets with small entries, and the HE community has considered standardizing small sparse secrets to improve efficiency and functionality.  However, prior work, SALSA and PICANTE, showed that ML attacks can recover sparse binary secrets. Building on these, we propose VERDE, an improved ML attack that can recover sparse binary, ternary, and narrow Gaussian secrets. Using improved preprocessing and secret recovery techniques, VERDE can attack LWE with larger dimensions ($n=512$) and smaller moduli ($\log_2 q=12$ for $n=256$), using less time and power. We propose novel architectures for scaling. Finally, we develop a theory that explains the success of ML LWE attacks.","['machine learning', 'cryptography', 'cryptanalysis']",[],"['Cathy Yuanchen Li', 'Emily Wenger', 'Francois Charton', 'Kristin E. Lauter']","['University of Chicago', 'Meta AI', 'Facebook', 'Facebook']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73048,Privacy & Data Governance,Model Sparsity Can Simplify Machine Unlearning,"In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although exact unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. We show in both theory and practice that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This leads to a new MU paradigm,    termed prune first, then unlearn, which infuses a sparse prior to the unlearning process. Building on this insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of approximate unlearning. Extensive experiments show that our proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) when using our proposed sparsity-aware unlearning method. Furthermore, we showcase the practical impact of our proposed MU methods through two specific use cases: defending against backdoor attacks, and enhancing transfer learning through source class removal. These applications demonstrate the versatility and effectiveness of our approaches in addressing a variety of machine learning challenges beyond unlearning for data privacy. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse.","['Machine unlearning', 'model pruning']",[],"['Jinghan Jia', 'Jiancheng Liu', 'Parikshit Ram', 'Yuguang Yao', 'Gaowen Liu', 'Yang Liu', 'Pranay Sharma', 'Sijia Liu']","['Michigan State University', 'Michigan State University', 'International Business Machines', 'Michigan State University', 'Cisco Systems', 'University of California, Santa Cruz', 'Carnegie Mellon University', 'Michigan State University']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72331,Privacy & Data Governance,Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution,"Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability. To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features. We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare. Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available. Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively.","['Interpretability', 'Attribution Maps', 'Information Bottleneck', 'Multi-Modal Learning', 'Vision-Language Pretrained Models']",[],"['Ying Wang', 'Tim G. J. Rudner', 'Andrew Gordon Wilson']","['New York University', 'New York University', 'Cornell University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70504,Privacy & Data Governance,Interpretability at Scale: Identifying Causal Mechanisms in Alpaca,"Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.",['Mechanistic Interpretability'],[],"['Zhengxuan Wu', 'Atticus Geiger', 'Thomas Icard', 'Christopher Potts', 'Noah Goodman']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73623,Privacy & Data Governance,Degraded Polygons Raise Fundamental Questions of Neural Network Perception,"It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize and recover such degraded shapes when initialized with different priors. Ultimately, we find that neural networks’ behavior on this simple task conflicts with human behavior, raising a fundamental question of the robustness and learning capabilities of modern computer vision models","['Geometry', 'Cognitive Science', 'Psychology', 'Vision', 'Robustness', 'Safety']",[],"['Leonard Tang', 'Dan Ley']","['Harvard University', 'Harvard University, Harvard University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70793,Privacy & Data Governance,Face Reconstruction from Facial Templates by Learning Latent Space of a Generator Network,"In this paper, we focus on the template inversion attack against face recognition systems and propose a new method to reconstruct face images from facial templates. Within a generative adversarial network (GAN)-based framework, we learn a mapping from facial templates to the intermediate latent space of a pre-trained face generation network, from which we can generate high-resolution realistic reconstructed face images. We show that our proposed method can be applied in whitebox and blackbox attacks against face recognition systems. Furthermore, we evaluate the transferability of our attack when the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another face recognition system. Considering the adversary's knowledge and the target face recognition system, we define five different attacks and evaluate the vulnerability of state-of-the-art face recognition systems. Our experiments show that our proposed method achieves high success attack rates in whitebox and blackbox scenarios. Furthermore, the reconstructed face images are transferable and can be used to enter target face recognition systems with a different feature extractor model. We also explore important areas in the reconstructed face images that can fool the target face recognition system.","['Face Recognition (FR)', 'Face reconstruction', 'Generative Adversarial Network (GAN)', 'Privacy', 'Security', 'Template Inversion (TI) attack', 'Transferability']",[],"['Hatef Otroshi Shahreza', 'Sébastien Marcel']","['EPFL - EPF Lausanne', 'Université de Lausanne']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70163,Privacy & Data Governance,Static and Sequential Malicious Attacks in the Context of Selective Forgetting,"With the growing demand for the right to be forgotten, there is an increasing need for machine learning models to forget sensitive data and its impact. To address this, the paradigm of selective forgetting (a.k.a machine unlearning) has been extensively studied, which aims to remove the impact of requested data from a well-trained model without retraining from scratch. Despite its significant success, limited attention has been given to the security vulnerabilities of the unlearning system concerning malicious data update requests. Motivated by this, in this paper, we explore the possibility and feasibility of malicious data update requests during the unlearning process. Specifically, we first propose a new class of malicious selective forgetting attacks, which involves a static scenario where all the malicious data update requests are provided by the adversary at once. Additionally, considering the sequential setting where the data update requests arrive sequentially, we also design a novel framework for sequential forgetting attacks, which is formulated as a stochastic optimal control problem. We also propose novel optimization algorithms that can find the effective malicious data update requests. We perform theoretical analyses for the proposed selective forgetting attacks, and extensive experimental results validate the effectiveness of our proposed selective forgetting attacks. The source code is available in the supplementary material.","['Selective forgetting', 'static setting', 'sequential setting', 'security and robustness']",[],"['CHENXU ZHAO', 'Wei Qian', 'Zhitao Ying', 'Mengdi Huai']","['Iowa State University', 'Iowa State University', 'Yale University', 'Iowa State University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71143,Privacy & Data Governance,Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation,"Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and causal structure, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in avoiding learning spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks.","['reinforcement learning', 'robustness', 'causality', 'spurious correlation']",[],"['Wenhao Ding', 'Laixi Shi', 'Yuejie Chi', 'Ding Zhao']","['Carnegie Mellon University', 'CMU, Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71547,Privacy & Data Governance,BERT Lost Patience Won't Be Robust to Adversarial Slowdown,"In this paper, we systematically evaluate the robustness of multi-exit language models against adversarial slowdown. To audit their robustness, we design a slowdown attack that generates natural adversarial text bypassing early-exit points. We use the resulting WAFFLE attack as a vehicle to conduct a comprehensive evaluation of three multi-exit mechanisms with the GLUE benchmark against adversarial slowdown. We then show our attack significantly reduces the computational savings provided by the three methods in both white-box and black-box settings. The more complex a mechanism is, the more vulnerable it is to adversarial slowdown. We also perform a linguistic analysis of the perturbed text inputs, identifying common perturbation patterns that our attack generates, and comparing them with standard adversarial text attacks. Moreover, we show that adversarial training is ineffective in defeating our slowdown attack, but input sanitization with a conversational model, e.g., ChatGPT, can remove perturbations effectively. This result suggests that future work is needed for developing efficient yet robust multi-exit models. Our code is available at: https://github.com/ztcoalson/WAFFLE",['Efficient Methods for NLP; Multi-exit Language Models; Adversarial Slowdown'],[],"['Zachary Coalson', 'Rakesh B Bobba', 'Sanghyun Hong']","['Oregon State University', 'Oregon State University', 'Oregon State University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71627,Privacy & Data Governance,Posterior Sampling for Competitive RL: Function Approximation and Partial Observation,"This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.","['Markov game', 'Partial observation', 'Function approximation', 'Posterior sampling', 'Reinforcement Learning']",[],"['Shuang Qiu', 'Ziyu Dai', 'Han Zhong', 'Zhaoran Wang', 'Zhuoran Yang', 'Tong Zhang']","['University of Science and Technology', 'Fudan University', 'Peking University', 'Northwestern University', 'Yale University', 'UIUC']","['Hong Kong', None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72871,Privacy & Data Governance,On the Exploitability of Instruction Tuning,"Instruction tuning is an effective technique to align large language models (LLMs) with human intent. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose \textit{AutoPoison}, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs.","['Trustworthy machine learning', 'Large language models', 'Supervised fine-tuning', 'instruction tuning']",[],"['Manli Shu', 'Jiongxiao Wang', 'Chen Zhu', 'Jonas Geiping', 'Chaowei Xiao', 'Tom Goldstein']","['Department of Computer Science, University of Maryland, College Park', 'University of Wisconsin - Madison', 'NVIDIA', 'ELLIS Institute Tübingen', 'University of Wisconsin - Madison', 'University of Maryland, College Park']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71213,Privacy & Data Governance,Thought Cloning: Learning to Think while Acting by Imitating Human Thinking,"Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to $\textit{think like humans do}$. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, $\textit{but also the thoughts humans have as they perform these behaviors}$. While we expect Thought Cloning to truly shine at scale on internet-sized datasets (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent’s thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents $\textit{how to think}$ as well as behave, Thought Cloning creates safer, more powerful agents.","['Reinforcement learning', 'Imitation Learning', 'AI Safety', 'Interpretability']",[],['Shengran Hu'],['University of British Columbia'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/72945,Privacy & Data Governance,Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots,"In the field of natural language processing, the prevalent approach involves fine-tuning pretrained language models (PLMs) using local samples. Recent research has exposed the susceptibility of PLMs to backdoor attacks, wherein the adversaries can embed malicious prediction behaviors by manipulating a few training samples. In this study, our objective is to develop a backdoor-resistant tuning procedure that yields a backdoor-free model, no matter whether the fine-tuning dataset contains poisoned samples. To this end, we propose and integrate an \emph{honeypot module} into the original PLM, specifically designed to absorb backdoor information exclusively. Our design is motivated by the observation that lower-layer representations in PLMs carry sufficient backdoor features while carrying minimal information about the original tasks. Consequently, we can impose penalties on the information acquired by the honeypot module to inhibit backdoor creation during the fine-tuning process of the stem network. Comprehensive experiments conducted on benchmark datasets substantiate the effectiveness and robustness of our defensive strategy. Notably, these results indicate a substantial reduction in the attack success rate ranging from 10\% to 40\% when compared to prior state-of-the-art methods.","['Backdoor Defense', 'Honeypot']",[],"['Ruixiang Tang', 'Jiayi Yuan', 'Yiming Li', 'Zirui Liu', 'Rui Chen', 'Xia Hu']","['Rice University', 'Rice University', 'Zhejiang University', 'Rice University', 'Samsung', 'Rice University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73439,Privacy & Data Governance,ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling,"Large-scale driving datasets such as Waymo Open Dataset and nuScenes substantially accelerate autonomous driving research, especially for perception tasks such as 3D detection and trajectory forecasting. Since the driving logs in these datasets contain HD maps and detailed object annotations which accurately reflect the real-world complexity of traffic behaviors, we can harvest a massive number of complex traffic scenarios and recreate their digital twins in simulation. Compared to the hand-crafted scenarios often used in existing simulators, data-driven scenarios collected from the real world can facilitate many research opportunities in machine learning and autonomous driving. In this work, we present ScenarioNet, an open-source platform for large-scale traffic scenario modeling and simulation. ScenarioNet defines a unified scenario description format and collects a large-scale repository of real-world traffic scenarios from the heterogeneous data in various driving datasets including Waymo, nuScenes, Lyft L5, and nuPlan datasets. These scenarios can be further replayed and interacted with in multiple views from Bird-Eye-View layout to realistic 3D rendering in MetaDrive simulator. This provides a benchmark for evaluating the safety of autonomous driving stacks in simulation before their real-world deployment. We further demonstrate the strengths of ScenarioNet on large-scale scenario generation, imitation learning, and reinforcement learning in both single-agent and multi-agent settings. Code, demo videos, and website are available at https://github.com/metadriverse/scenarionet","['Autonomous Driving', 'Real-world Scenarios', 'Reinforcement Learning', 'Traffic Simulation', 'AD Stack Testing']",[],"['Quanyi Li', 'Zhenghao Peng', 'Lan Feng', 'Zhizheng Liu', 'Chenda Duan', 'Wenjie Mo', 'Bolei Zhou']","['University of Edinburgh', 'University of California, Los Angeles', 'EPFL - EPF Lausanne', 'UCLA Computer Science Department, University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70375,Privacy & Data Governance,Learning from Active Human Involvement through Proxy Value Propagation,"Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety and AI alignment to the learning process. In this work, we propose a new reward-free active human involvement method called Proxy Value Propagation for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state- action pairs in the human demonstration are labeled with high values, while those agents’ actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents’ exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human- in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: https://metadriverse.github.io/pvp.","['Machine Learning', 'Human-in-the-loop Reinforcement Learning', 'Safety', 'Sample Efficiency', 'Reward-free']",[],"['Zhenghao Peng', 'Wenjie Mo', 'Chenda Duan', 'Quanyi Li', 'Bolei Zhou']","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles', 'University of Edinburgh', 'University of California, Los Angeles']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71922,Privacy & Data Governance,Adversarial Attacks on Online Learning to Rank with Click Feedback,"Online learning to rank (OLTR) is a sequential decision-making problem where a learning agent selects an ordered list of items and receives feedback through user clicks. Although potential attacks against OLTR algorithms may cause serious losses in real-world applications, there is limited knowledge about adversarial attacks on OLTR. This paper studies attack strategies against multiple variants of OLTR. Our first result provides an attack strategy against the UCB algorithm on classical stochastic bandits with binary feedback, which solves the key issues caused by bounded and discrete feedback that previous works cannot handle. Building on this result, we design attack algorithms against UCB-based OLTR algorithms in position-based and cascade models. Finally, we propose a general attack strategy against any algorithm under the general click model. Each attack algorithm manipulates the learning agent into choosing the target attack item $T-o(T)$ times, incurring a cumulative cost of $o(T)$. Experiments on synthetic and real data further validate the effectiveness of our proposed attack algorithms.","['online learning to rank', 'adversarial attack', 'click model']",[],"['Jinhang Zuo', 'Zhiyong Wang', 'Shuai Li', 'Mohammad Hajiesmaili', 'Adam Wierman']","['University of Massachusetts at Amherst', 'Department of Computer Science and Engineering, The Chinese University of', 'John Hopcroft Center, Shanghai Jiao Tong University', 'College of Information and Computer Science, University of Massachusetts, Amherst', 'California Institute of Technology']","[None, 'Hong Kong', None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72611,Privacy & Data Governance,Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion,"Automated creation of synthetic traffic scenarios is a key part of scaling the safety validation of autonomous vehicles (AVs). In this paper, we propose Scenario Diffusion, a novel diffusion-based architecture for generating traffic scenarios that enables controllable scenario generation. We combine latent diffusion, object detection and trajectory regression to generate distributions of synthetic agent poses, orientations and trajectories simultaneously. This distribution is conditioned on the map and sets of tokens describing the desired scenario to provide additional control over the generated scenario. We show that our approach has sufficient expressive capacity to model diverse traffic patterns and generalizes to different geographical regions.","['Deep Learning', '(Other) Applications', '(Other) Machine Learning Topics']",[],"['Ethan Pronovost', 'Meghana Reddy Ganesina', 'Noureldin Hendy', 'Zeyu Wang', 'Andres Morales', 'Kai Wang', 'Nicholas Roy']","['Zoox', 'Zoox Inc.', 'Work', 'Work', 'Computer Science Department, Stanford University', 'Work', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72827,Privacy & Data Governance,A Unified Detection Framework for Inference-Stage Backdoor Defenses,"Backdoor attacks involve inserting poisoned samples during training, resulting in a model containing a hidden backdoor that can trigger specific behaviors without impacting performance on normal samples. These attacks are challenging to detect, as the backdoored model appears normal until activated by the backdoor trigger, rendering them particularly stealthy. In this study, we devise a unified inference-stage detection framework to defend against backdoor attacks. We first rigorously formulate the inference-stage backdoor detection problem, encompassing various existing methods, and discuss several challenges and limitations. We then propose a framework with provable guarantees on the false positive rate or the probability of misclassifying a clean sample.  Further, we derive the most powerful detection rule to maximize the detection power, namely the rate of accurately identifying a backdoor sample, given a false positive rate under classical learning scenarios. Based on the theoretically optimal detection rule, we suggest a practical and effective approach for real-world applications based on the latent representations of backdoored deep nets. We extensively evaluate our method on 14 different backdoor attacks using Computer Vision (CV) and Natural Language Processing (NLP) benchmark datasets. The experimental findings align with our theoretical results. We significantly surpass the state-of-the-art methods, e.g., up to 300\% improvement on the detection power as evaluated by AUCROC, over the state-of-the-art defense against advanced adaptive backdoor attacks.","['Backdoor attacks', 'Backdoor Defense', 'Security for AI']",[],"['Xun Xian', 'Ganghua Wang', 'Jayanth Srinivasa', 'Ashish Kundu', 'Xuan Bi', 'Mingyi Hong', 'Jie Ding']","['University of Minnesota, Minneapolis', 'University of Minnesota, Minneapolis', 'Cisco', 'Cisco Research', 'University of Minnesota - Twin Cities', 'University of Minnesota, Minneapolis', 'University of Minnesota, Minneapolis']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71402,Privacy & Data Governance,"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense","The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.","['AI-generated text detection', 'text detection', 'paraphrasing', 'attacks', 'retrieval', 'defenses', 'large language models', 'LLMs']",[],"['Kalpesh Krishna', 'Yixiao Song', 'John Frederick Wieting', 'Mohit Iyyer']","['Google', 'University of Massachusetts at Amherst', 'Google DeepMind', 'University of Massachusetts Amherst']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71455,Privacy & Data Governance,Batchnorm Allows Unsupervised Radial Attacks,"The construction of adversarial examples usually requires the existence of soft or hard labels for each instance, with respect to which a loss gradient provides the signal for construction of the example. We show that for batch normalized deep image recognition architectures, intermediate latents that are produced after a batch normalization step by themselves suffice to produce adversarial examples using an intermediate loss solely utilizing angular deviations, without relying on any label. We motivate our loss through the geometry of batch normed representations and their concentration of norm on a hypersphere and distributional proximity to Gaussians. Our losses expand intermediate latent based attacks that usually require labels. The success of our method implies that leakage of intermediate representations may create a security breach for deployed models, which persists even when the model is transferred to downstream usage. Removal of batch norm weakens our attack, indicating it contributes to this vulnerability. Our attacks also succeed against LayerNorm empirically, thus being relevant for transformer architectures, most notably vision transformers which we analyze.","['Adversarial', 'Batch normalization', 'Robustness', 'Geometric', 'radial']",[],"['Amur Ghose', 'Apurv Gupta', 'Yaoliang Yu', 'Pascal Poupart']","['Huawei Technologies Ltd.', 'IBM Consulting', 'University of Waterloo', 'University of Waterloo']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70911,Privacy & Data Governance,Reduced Policy Optimization for Continuous Control with Hard Constraints,"Recent advances in constrained reinforcement learning (RL) have endowed reinforcement learning with certain safety guarantees. However, deploying existing constrained RL algorithms in continuous control tasks with general hard constraints remains challenging, particularly in those situations with non-convex hard constraints. Inspired by the generalized reduced gradient (GRG) algorithm, a classical constrained optimization technique, we propose a reduced policy optimization (RPO) algorithm that combines RL with GRG to address general hard constraints. RPO partitions actions into basic actions and nonbasic actions following the GRG method and outputs the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints using the obtained basic actions. The policy network is then updated by implicitly differentiating nonbasic actions with respect to basic actions. Additionally, we introduce an action projection procedure based on the reduced gradient and apply a modified Lagrangian relaxation technique to ensure inequality constraints are satisfied. To the best of our knowledge, RPO is the first attempt that introduces GRG to RL as a way of efficiently handling both equality and inequality hard constraints. It is worth noting that there is currently a lack of RL environments with complex hard constraints, which motivates us to develop three new benchmarks: two robotics manipulation tasks and a smart grid operation control task. With these benchmarks, RPO achieves better performance than previous constrained RL algorithms in terms of both cumulative reward and constraint violation. We believe RPO, along with the new benchmarks, will open up new opportunities for applying RL to real-world problems with complex constraints.","['Reinforcement Learning', 'Hard Constraint', 'Generalized Reduced Gradient']",[],"['Shutong Ding', 'Jingya Wang', 'Ye Shi']","['ShanghaiTech University', 'ShanghaiTech University', 'ShanghaiTech University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70223,Privacy & Data Governance,Understanding Deep Gradient Leakage via Inversion Influence Functions,"Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products.   We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, modalities, attack implementations, and perturbation-based defenses. With this novel tool, we provide insights into effective gradient perturbation directions, the unfairness of privacy protection, and privacy-preferred model initialization. Our codes are provided in https://github.com/illidanlab/inversion-influence-function.","['Deep Learning', 'Privacy', 'Federated Learning', 'Influence Function']",[],"['Junyuan Hong', 'Yuyang Deng', 'Mehrdad Mahdavi', 'Jiayu Zhou']","['University of Texas at Austin', 'Pennsylvania State University', 'Pennsylvania State University', 'Michigan State University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72925,Privacy & Data Governance,A Diffusion-Model of Joint Interactive Navigation,"Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN -- a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing.","['Diffusion Models', 'Trajecotry Forecasting', 'Autonomous Vehicles', 'Motion Forecasting', 'Simulation']",[],"['Matthew Niedoba', 'Jonathan Wilder Lavington', 'Yunpeng Liu', 'Vasileios Lioutas', 'Justice Sefas', 'Xiaoxuan Liang', 'Dylan Green', 'Berend Zwartsenberg', 'Adam Scibior', 'Frank Wood']","['University of British Columbia', 'University of British Columbia', 'University of British Columbia', 'University of British Columbia', 'University of British Columbia', ', University of British Columbia', 'University of British Columbia', 'Inverted AI', 'University of British Columbia', 'University of British Columbia']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70232,Privacy & Data Governance,Scalable Membership Inference Attacks via Quantile Regression,"Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \emph{shadow models}---i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. \footnotetext[0]{Martin and Shuai are the lead authors, and other authors are ordered alphabetically. \{maberlop,shuat\}@amazon.com} We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box"". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures. Our code is available at \href{https://github.com/amazon-science/quantile-mia}{github.com/amazon-science/quantile-mia.}","['machine learning', 'privacy', 'membership inference']",[],"['Martin Andres Bertran', 'Shuai Tang', 'Aaron Roth', 'Michael Kearns', 'Jamie Heather Morgenstern', 'Steven Wu']","['Duke University', 'Amazon Web Services', 'Amazon', 'University of Pennsylvania', 'University of Washington, Seattle', 'Carnegie Mellon University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69874,Privacy & Data Governance,Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples,"Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense. The code is available at https://github.com/SCLBD/BackdoorBench (PyTorch) and https://github.com/shawkui/MindTrojan (MindSpore).","['Backdoor Attack', 'Trustworthy AI', 'Backdoor Learning']",[],"['Shaokui Wei', 'Mingda Zhang', 'Hongyuan Zha', 'Baoyuan Wu']","['The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/69920,Privacy & Data Governance,What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?,"We study indiscriminate poisoning for linear learners where an adversary injects a few  crafted examples into the training data with the goal of forcing the induced model to incur higher test error. Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners. For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget. Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks.",['poisoning attacks; adversarial machine learning; machine learning security'],[],"['Fnu Suya', 'Xiao Zhang', 'Yuan Tian', 'David Evans']","['University of Maryland, College Park', 'CISPA Helmholtz Center for Information Security', 'University of Virginia', 'University of Virginia']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69981,Privacy & Data Governance,Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach,"Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments  is available at \url{https://github.com/zknus/NeurIPS-2023-HANG-Robustness}.","['adversarial robustness', 'graph neural networks']",[],"['Kai Zhao', 'Qiyu Kang', 'Yang Song', 'Rui She', 'Sijie Wang', 'Wee Peng Tay']","['Nanyang Technological University', 'Nanyang Technological University', 'C3 AI', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70122,Privacy & Data Governance,Improving Adversarial Robustness via Information Bottleneck Distillation,"Previous studies have shown that optimizing the information bottleneck can significantly improve the robustness of deep neural networks. Our study closely examines the information bottleneck principle and proposes an Information Bottleneck Distillation approach. This specially designed, robust distillation technique utilizes prior knowledge obtained from a robust pre-trained model to boost information bottlenecks.  Specifically, we propose two distillation strategies that align with the two optimization processes of the information bottleneck. Firstly, we use a robust soft-label distillation method to increase the mutual information between latent features and output prediction. Secondly, we introduce an adaptive feature distillation method that automatically transfers relevant knowledge from the teacher model to the student model, thereby reducing the mutual information between the input and latent features. We conduct extensive experiments to evaluate our approach's robustness against state-of-the-art adversarial attackers such as PGD-attack and AutoAttack. Our experimental results demonstrate the effectiveness of our approach in significantly improving adversarial robustness. Our code is available at https://github.com/SkyKuang/IBD.","['Information Bottleneck', 'Adversarial training', 'Adversarial  robustness', 'Knowledge distillation']",[],"['Huafeng Kuang', 'Hong Liu', 'YONGJIAN WU', ""Shin'ichi Satoh"", 'Rongrong Ji']","['Xiamen University', 'Osaka University, Tokyo Institute of Technology', 'Wuhan University', 'National Institute of Informatics', 'Xiamen University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70177,Privacy & Data Governance,Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense,"Federated learning algorithms enable neural network models to be trained across multiple decentralized edge devices without sharing private data. However, they are susceptible to backdoor attacks launched by malicious clients. Existing robust federated aggregation algorithms heuristically detect and exclude suspicious clients based on their parameter distances, but they are ineffective on Natural Language Processing (NLP) tasks. The main reason is that, although text backdoor patterns are obvious at the underlying dataset level, they are usually hidden at the parameter level, since injecting backdoors into texts with discrete feature space has less impact on the statistics of the model parameters. To settle this issue, we propose to identify backdoor clients by explicitly modeling the data divergence among clients in federated NLP systems. Through theoretical analysis, we derive the f-divergence indicator to estimate the client data divergence with aggregation updates and Hessians. Furthermore, we devise a dataset synthesization method with a Hessian reassignment mechanism guided by the diffusion theory to address the key challenge of inaccessible datasets in calculating clients' data Hessians. We then present the novel Federated F-Divergence-Based Aggregation~(\textbf{Fed-FA}) algorithm, which leverages the f-divergence indicator to detect and discard suspicious clients. Extensive empirical results show that Fed-FA outperforms all the parameter distance-based methods in defending against backdoor attacks among various natural language backdoor attack scenarios.","['federated learning', 'backdoor learning', 'robust federated aggregation', 'data divergence']",[],"['Zhiyuan Zhang', 'Deli Chen', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Xu Sun']","['Peking University', 'DeepSeek AI', 'Tencent, Wechat AI', 'WeChat AI, Tencent Inc.', 'WeChat AI, Tencent Inc.', 'Peking University']","[None, None, None, None, None, None] https://nips.cc/virtual/2023/poster/70180",3,Secure Out-of-Distribution Task Generalization with Energy-Based Models,"The success of meta-learning on out-of-distribution (OOD) tasks in the wild has proved to be hit-and-miss. To safeguard the generalization capability of the meta-learned prior knowledge to OOD tasks, in particularly safety-critical applications, necessitates detection of an OOD task followed by adaptation of the task towards the prior.  Nonetheless, the reliability of estimated uncertainty on OOD tasks by existing Bayesian meta-learning methods is restricted by incomplete coverage of the feature distribution shift and insufficient expressiveness of the meta-learned prior.  Besides, they struggle to adapt an OOD task, running parallel to the line of cross-domain task adaptation solutions which are vulnerable to overfitting. To this end, we build a single coherent framework that supports both detection and adaptation of OOD tasks, while remaining compatible with off-the-shelf meta-learning backbones.  The proposed Energy-Based Meta-Learning (EBML) framework learns to characterize any arbitrary meta-training task distribution with the composition of two expressive neural-network-based energy functions. We deploy the sum of the two energy functions, being proportional to the joint distribution of a task, as a reliable score for detecting OOD tasks; during meta-testing, we adapt the OOD task to in-distribution tasks by energy minimization. Experiments on four regression and classification datasets  demonstrate the effectiveness of our proposal.",,"['Shengzhuang Chen', 'Long-Kai Huang', 'Jonathan Richard Schwarz', 'Yilun Du', 'Ying Wei']","['City University of', 'Tencent AI Lab', 'Harvard University', 'Massachusetts Institute of Technology', 'Nanyang Technological University']","['Hong Kong', None, None, None, None]"
https://nips.cc/virtual/2023/poster/70184,Privacy & Data Governance,SafeDICE: Offline Safe Imitation Learning with Non-Preferred Demonstrations,"We consider offline safe imitation learning (IL), where the agent aims to learn the safe policy that mimics preferred behavior while avoiding non-preferred behavior from non-preferred demonstrations and unlabeled demonstrations. This problem setting corresponds to various real-world scenarios, where satisfying safety constraints is more important than maximizing the expected return. However, it is very challenging to learn the policy to avoid constraint-violating (i.e. non-preferred) behavior, as opposed to standard imitation learning which learns the policy to mimic given demonstrations. In this paper, we present a hyperparameter-free offline safe IL algorithm, SafeDICE, that learns safe policy by leveraging the non-preferred demonstrations in the space of stationary distributions. Our algorithm directly estimates the stationary distribution corrections of the policy that imitate the demonstrations excluding the non-preferred behavior. In the experiments, we demonstrate that our algorithm learns a more safe policy that satisfies the cost constraint without degrading the reward performance, compared to baseline algorithms.","['Imitation learning', 'Preference-based learning', 'Safe imitation learning']",[],"['Youngsoo Jang', 'Geon-Hyeong Kim', 'Jongmin Lee', 'Sungryull Sohn', 'Byoungjip Kim', 'Honglak Lee', 'Moontae Lee']","['LG AI Research', 'LG AI Research', 'University of California, Berkeley', 'LG AI Research', 'LG AI Research', 'University of Michigan', 'University of Illinois, Chicago']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70194,Privacy & Data Governance,Stable Diffusion is Unstable,"Recently, text-to-image models have been thriving. Despite their powerful generative capacity, our research has uncovered a lack of robustness in this generation process. Specifically, the introduction of small perturbations to the text prompts can result in the blending of primary subjects with other categories or their complete disappearance in the generated images. In this paper, we propose **Auto-attack on Text-to-image Models (ATM)**, a gradient-based approach, to effectively and efficiently generate such perturbations. By learning a Gumbel Softmax distribution, we can make the discrete process of word replacement or extension continuous, thus ensuring the differentiability of the perturbation generation. Once the distribution is learned, ATM can sample multiple attack samples simultaneously. These attack samples can prevent the generative model from generating the desired subjects without tampering with the category keywords in the prompt. ATM has achieved a 91.1\% success rate in short-text attacks and an 81.2\% success rate in long-text attacks. Further empirical analysis revealed three attack patterns based on: 1) variability in generation speed, 2) similarity of coarse-grained characteristics, and 3) polysemy of words. The code is available at https://github.com/duchengbin8/Stable_Diffusion_is_Unstable","['Adversarial Attack', 'Generative Model', 'Diffusion Model', 'Latent Diffusion Model', 'Conditional Latent Diffusion Model']",[],"['Chengbin Du', 'Yanxi Li', 'Zhongwei Qiu', 'Chang Xu']","['University of Sydney', 'University of Sydney, University of Sydney', 'University of Sydney', 'University of Sydney']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70218,Privacy & Data Governance,Safety Verification of Decision-Tree Policies in Continuous Time,"Decision trees have gained popularity as interpretable surrogate models for learning-based control policies. However, providing safety guarantees for systems controlled by decision trees is an open challenge. We show that the problem is undecidable even for systems with the simplest dynamics, and PSPACE-complete for finite-horizon properties. The latter can be verified for discrete-time systems via bounded model checking. However, for continuous-time systems, such an approach requires discretization, thereby weakening the guarantees for the original system. This paper presents the first algorithm to directly verify decision-tree controlled system in continuous time. The key aspect of our method is exploiting the decision-tree structure to propagate a set-based approximation through the decision nodes. We demonstrate the effectiveness of our approach by verifying safety of several decision trees distilled to imitate neural-network policies for nonlinear systems.","['safety verification', 'decision tree', 'reinforcement learning', 'controller', 'continuous time']",[],"['Christian Schilling', 'Anna Lukina', 'Emir Demirović', 'Kim Guldstrand Larsen']","['Aalborg University', 'Delft University of Technology', 'Delft University of Technology', 'Aalborg University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70225,Privacy & Data Governance,Uncertainty Estimation for Safety-critical Scene Segmentation via Fine-grained Reward Maximization,"Uncertainty estimation plays an important role for future reliable deployment of deep segmentation models in safety-critical scenarios such as medical applications. However, existing methods for uncertainty estimation have been limited by the lack of explicit guidance for calibrating the prediction risk and model confidence. In this work, we propose a novel fine-grained reward maximization (FGRM) framework, to address uncertainty estimation by directly utilizing an uncertainty metric related reward function with a reinforcement learning based model tuning algorithm. This would benefit the model uncertainty estimation with direct optimization guidance for model calibration. Specifically, our method designs a new uncertainty estimation reward function using the calibration metric, which is maximized to fine-tune an evidential learning pre-trained segmentation model for calibrating prediction risk. Importantly, we innovate an effective fine-grained parameter update scheme, which imposes fine-grained reward-weighting of each network parameter according to the parameter importance quantified by the fisher information matrix. To the best of our knowledge, this is the first work exploring reward optimization for model uncertainty estimation in safety-critical vision tasks. The effectiveness of our method is demonstrated on two large safety-critical surgical scene segmentation datasets under two different uncertainty estimation settings. With real-time one forward pass at inference, our method outperforms state-of-the-art methods by a clear margin on all the calibration metrics of uncertainty estimation, while maintaining a high task accuracy for the segmentation results. Code is available at https://github.com/med-air/FGRM.","['uncertainty estimation', 'semantic segmentation', 'medical application']",[],"['Hongzheng Yang', 'Cheng Chen', 'Markus Scheppach', 'Hon Chi Yip', 'Qi Dou']","['Department of Computer Science and Engineering, The Chinese University of', 'Harvard Medical School', 'University Hospital of Augsburg, Augsburg,', 'The Chinese University of', 'The Chinese University of']","['Hong Kong', None, 'Germany', 'Hong Kong', 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/70476,Privacy & Data Governance,Certifiably Robust Graph Contrastive Learning,"Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any GCL model. The source code of RES is available at https://github.com/ventr1c/RES-GCL.","['Certifiable Robustness', 'Graph Contrastive Learning']",[],"['Minhua Lin', 'Teng Xiao', 'Enyan Dai', 'Suhang Wang']","['Pennsylvania State University', 'The Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70325,Privacy & Data Governance,TexQ: Zero-shot Network Quantization with Texture Feature Distribution Calibration,"Quantization is an effective way to compress neural networks. By reducing the bit width of the parameters, the processing efficiency of neural network models at edge devices can be notably improved. Most conventional quantization methods utilize real datasets to optimize quantization parameters and fine-tune. Due to the inevitable privacy and security issues of real samples, the existing real-data-driven methods are no longer applicable. Thus, a natural method is to introduce synthetic samples for zero-shot quantization (ZSQ). However, the conventional synthetic samples fail to retain the detailed texture feature distributions, which severely limits the knowledge transfer and performance of the quantized model. In this paper, a novel ZSQ method, TexQ is proposed to address this issue. We first synthesize a calibration image and extract its calibration center for each class with a texture feature energy distribution calibration method. Then, the calibration centers are used to guide the generator to synthesize samples. Finally, we introduce the mixup knowledge distillation module to diversify synthetic samples for fine-tuning. Extensive experiments on CIFAR10/100 and ImageNet show that TexQ is observed to perform state-of-the-art in ultra-low bit width quantization. For example, when ResNet-18 is quantized to 3-bit, TexQ achieves a 12.18% top-1 accuracy increase on ImageNet compared to state-of-the-art methods. Code at https://github.com/dangsingrue/TexQ.","['Zero-shot quantization', 'Texture feature calibration', 'Post-training quantization', 'low bit width', 'Neural network compression']",[],"['Xinrui Chen', 'Yizhi Wang', 'Renao Yan', 'Yiqing Liu', 'Tian Guan', 'Yonghong He']","['Tsinghua University', 'Tsinghua University, Tsinghua University', 'Graduate School at Shenzhen, Tsinghua Univ. ()', 'Biomedical Engineering, Tsinghua University', 'Graduate School at Shenzhen, Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, None, 'China', None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70351,Privacy & Data Governance,One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning,"Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be *risk-averse*. An additional challenge of offline RL is avoiding *distributional shift*, i.e. ensuring that  state-action pairs visited by the policy remain near those in the dataset. Previous offline RL algorithms that consider risk combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address *both* of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered by the dataset have high epistemic uncertainty. Risk-aversion to aleatoric uncertainty discourages actions that are risky due to environment stochasticity. Thus, by considering epistemic uncertainty via a model ensemble and introducing risk-aversion, our algorithm (1R2R) avoids distributional shift in addition to achieving risk-aversion to aleatoric risk. Our experiments show that 1R2R achieves strong performance on deterministic benchmarks, and outperforms existing approaches for risk-sensitive objectives in stochastic domains.","['offline reinforcement learning', 'model-based reinforcement learning', 'risk', 'uncertainty']",[],"['Marc Rigter', 'Bruno Lacerda', 'Nick Hawes']","['University of Oxford', 'University of Oxford', 'University of Oxford']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70371,Privacy & Data Governance,VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models,"Vision-Language (VL) pre-trained models have shown their superiority on many multimodal tasks. However, the adversarial robustness of such models has not been fully explored. Existing approaches mainly focus on exploring the adversarial robustness under the white-box setting, which is unrealistic. In this paper, we aim to investigate a new yet practical task to craft image and text perturbations using pre-trained VL models to attack black-box fine-tuned models on different downstream tasks. Towards this end, we propose VLATTACK to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multi-modal levels. At the single-modal level, we propose a new block-wise similarity attack (BSA) strategy to learn image perturbations for disrupting universal representations. Besides, we adopt an existing text attack strategy to generate text perturbations independent of the image-modal attack. At the multi-modal level, we design a novel iterative cross-search attack (ICSA) method to update adversarial image-text pairs periodically, starting with the outputs from the single-modal level.  We conduct extensive experiments to attack three widely-used VL pretrained models for six tasks on eight datasets. Experimental results show that the proposed VLATTACK framework achieves the highest attack success rates on all tasks compared with state-of-the-art baselines, which reveals a significant blind spot in the deployment of pre-trained VL models.","['vision-language', 'adversarial attacks', 'pre-trained model', 'fine-tuned model']",[],"['Ziyi Yin', 'Muchao Ye', 'Tianrong Zhang', 'Tianyu Du', 'Jinguo Zhu', 'Han Liu', 'Jinghui Chen', 'Ting Wang', 'Fenglong Ma']","['Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Zhejiang University', ""Xi'an Jiaotong University"", 'Dalian University of Technology', 'Pennsylvania State University', 'State University of New York at Stony Brook', 'Pennsylvania State University']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70373,Privacy & Data Governance,Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints,"In safety-critical robotic tasks, potential failures must be reduced, and multiple constraints must be met, such as avoiding collisions, limiting energy consumption, and maintaining balance. Thus, applying safe reinforcement learning (RL) in such robotic tasks requires to handle multiple constraints and use risk-averse constraints rather than risk-neutral constraints. To this end, we propose a trust region-based safe RL algorithm for multiple constraints called a safe distributional actor-critic (SDAC). Our main contributions are as follows: 1) introducing a gradient integration method to manage infeasibility issues in multi-constrained problems, ensuring theoretical convergence, and 2) developing a TD($\lambda$) target distribution to estimate risk-averse constraints with low biases. We evaluate SDAC through extensive experiments involving multi- and single-constrained robotic tasks. While maintaining high scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in multi-constrained tasks and 1.78 times fewer constraint violations in single-constrained tasks compared to safe RL baselines. Code is available at: https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.","['Reinforcement learning', 'Safety', 'Multiple Constraints', 'Distributional Critic']",[],"['Dohyeong Kim', 'Kyungjae Lee', 'Songhwai Oh']","['Seoul National University', 'ChungAng University', 'Seoul National University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70388,Privacy & Data Governance,Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection,"Anti-spoofing detection has become a necessity for face recognition systems due to the security threat posed by spoofing attacks. Despite great success in traditional attacks, most deep-learning-based methods perform poorly in 3D masks, which can highly simulate real faces in appearance and structure, suffering generalizability insufficiency while focusing only on the spatial domain with single frame input. This has been mitigated by the recent introduction of a biomedical technology called rPPG (remote photoplethysmography). However, rPPG-based methods are sensitive to noisy interference and require at least one second (> 25 frames) of observation time, which induces high computational overhead. To address these challenges, we propose a novel 3D mask detection framework, called FASTEN (Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the network for focusing more on fine-grained details in large movements, which can eliminate redundant spatio-temporal feature interference and quickly capture splicing traces of 3D masks in fewer frames. Our proposed network contains three key modules: 1) a facial optical flow network to obtain non-RGB inter-frame flow information; 2) flow attention to assign different significance to each frame; 3) spatio-temporal aggregation to aggregate high-level spatial features and temporal transition features. Through extensive experiments, FASTEN only requires five frames of input and outperforms eight competitors for both intra-dataset and cross-dataset evaluations in terms of multiple detection metrics. Moreover, FASTEN has been deployed in real-world mobile devices for practical 3D mask detection.","['3D mask detection', 'spatio-temporal aggregation', 'optical flow', 'deep learning']",[],"['Yuxin Cao', 'Yian Li', 'Yumeng Zhu']","['Tsinghua University', 'ShanghaiTech University', 'Pingan Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69997,Privacy & Data Governance,On Evaluating Adversarial Robustness of Large Vision-Language Models,"Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Our project page: https://yunqing-me.github.io/AttackVLM/.","['Large Vision-Language Models', 'Adversarial Robustness']",[],"['Yunqing Zhao', 'Tianyu Pang', 'Chao Du', 'Xiao Yang', 'Chongxuan Li', 'Ngai-man Cheung', 'Min Lin']","['University of Technology and Design', 'Sea AI Lab', 'Sea AI Lab', 'Tsinghua University, Tsinghua University', 'Renmin University of', 'University of Technology and Design', 'Sea AI Lab']","['Singapore', None, None, None, 'China', 'Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/70011,Privacy & Data Governance,Proximity-Informed Calibration for Deep Neural Networks,"Confidence calibration is central to providing accurate and interpretable uncertainty estimates, especially under safety-critical scenarios. However, we find that existing calibration algorithms often overlook the issue of proximity bias, a phenomenon where models tend to be more overconfident in low proximity data (i.e., data lying in the sparse region of the data distribution) compared to high proximity samples, and thus suffer from inconsistent miscalibration across different proximity samples. We examine the problem over $504$ pretrained ImageNet models and observe that: 1) Proximity bias exists across a wide variety of model architectures and sizes; 2) Transformer-based models are relatively more susceptible to proximity bias than CNN-based models; 3) Proximity bias persists even after performing popular calibration algorithms like temperature scaling; 4) Models tend to overfit more heavily on low proximity samples than on high proximity samples. Motivated by the empirical findings, we propose ProCal, a plug-and-play algorithm with a theoretical guarantee to adjust sample confidence based on proximity. To further quantify the effectiveness of calibration algorithms in mitigating proximity bias, we introduce proximity-informed expected calibration error (PIECE) with theoretical analysis. We show that ProCal is effective in addressing proximity bias and improving calibration on balanced, long-tail, and distribution-shift settings under four metrics over various model architectures. We believe our findings on proximity bias will guide the development of fairer and better-calibrated} models, contributing to the broader pursuit of trustworthy AI.","['Calibration', 'Uncertainty Estimation', 'Trustworthiness', 'Fairness', 'Multicalibration']",[],"['Miao Xiong', 'Ailin Deng', 'Pang Wei Koh', 'Jiaying Wu', 'Shen Li', 'Bryan Hooi']","['National University of', 'National University of', 'University of Washington', 'National University of', 'national university of singaore, National University of', 'National University of']","['Singapore', 'Singapore', None, 'Singapore', 'Singapore', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/70044,Privacy & Data Governance,Maximization of Average Precision for Deep Learning with Adversarial Ranking Robustness,"This paper seeks to address a gap in optimizing Average Precision (AP) while ensuring adversarial robustness, an area that has not been extensively explored to the best of our knowledge. AP maximization for deep learning has widespread applications, particularly when there is a significant imbalance between positive and negative examples. Although numerous studies have been conducted on adversarial training, they primarily focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. However, this type of adversarial robustness is insufficient for many applications, as minor perturbations on a single example can significantly impact AP  while not greatly influencing the accuracy of the prediction system. To tackle this issue, we introduce a novel formulation that combines an AP surrogate loss with a regularization term representing adversarial ranking robustness, which maintains the consistency between ranking of clean data and that of perturbed data. We then devise an efficient stochastic optimization algorithm to optimize the resulting objective. Our empirical studies, which compare our method to current leading adversarial training baselines and other robust AP maximization strategies, demonstrate the effectiveness of the proposed approach. Notably, our methods outperform a state-of-the-art method (TRADES) by more than 4\% in terms of robust AP against PGD attacks while achieving 7\% higher AP on clean data simultaneously on CIFAR10 and CIFAR100.The code is available at: <https://github.com/GangLii/Adversarial-AP>","['Adversarial Average Precision Maximization', 'Robust Average Precision', 'Adversarial Ranking Robustness', 'Adversarial Training']",[],"['Gang Li', 'Wei Tong', 'Tianbao Yang']","['Texas A&M University - College Station', 'General Motors R&D', 'University of Iowa']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70045,Privacy & Data Governance,VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models,"Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.","['backdoor', 'diffusion model', 'trustworthy']",[],"['Sheng-Yen Chou', 'Pin-Yu Chen', 'Tsung-Yi Ho']","['The Chinese University of', 'International Business Machines', 'Department of Computer Science and Engineering, The Chinese University of']","['Hong Kong', None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/70463,Privacy & Data Governance,When Demonstrations meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning,"Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations from an expert agent. Accurate models of expertise in executing a task has applications in safety-sensitive applications such as clinical decision making and autonomous driving. However, the structure of an expert's preferences implicit in observed actions is closely linked to the expert's model of the environment dynamics (i.e. the ``world''). Thus, inaccurate models of the world obtained from finite data with limited coverage could compound inaccuracy in estimated rewards. To address this issue, we propose a bi-level optimization formulation of the estimation task wherein the upper level is likelihood maximization based upon a conservative model of the expert's policy (lower level). The policy model is conservative in that it maximizes reward subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We propose a new algorithmic framework to solve the bi-level optimization problem formulation and provide statistical and computational guarantees of performance for the associated optimal reward estimator. Finally,  we demonstrate that the proposed algorithm outperforms the state-of-the-art offline IRL and imitation learning benchmarks by a large margin, over the continuous control tasks in MuJoCo and different datasets in the D4RL benchmark.","['Inverse Reinforcement Learning', 'Model-based Offline Inverse Reinforcement Learning']",[],"['Siliang Zeng', 'Chenliang Li', 'Alfredo Garcia', 'Mingyi Hong']","['University of Minnesota, Twin Cities', 'Texas A&M University - College Station', 'Texas A&M University - College Station', 'University of Minnesota, Minneapolis']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70578,Privacy & Data Governance,A General Framework for Robust G-Invariance in G-Equivariant Networks,"We introduce a general method for achieving robust group-invariance in group-equivariant convolutional neural networks ($G$-CNNs), which we call the $G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the triple-correlation on groups, which is the unique, lowest-degree polynomial invariant map that is also \textit{complete}. Many commonly used invariant maps\textemdash such as the \texttt{max}\textemdash are incomplete: they remove both group and signal structure. A complete invariant, by contrast, removes only the variation due to the actions of the group, while preserving all information about the structure of the signal. The completeness of the triple correlation endows the $G$-TC layer with strong robustness, which can be observed in its resistance to invariance-based adversarial attacks. In addition, we observe that it yields measurable improvements in classification accuracy over standard Max $G$-Pooling in $G$-CNN architectures. We provide a general and efficient implementation of the method for any discretized group, which requires only a table defining the group's product structure. We demonstrate the benefits of this method for $G$-CNNs defined on both commutative and non-commutative groups\textemdash $SO(2)$, $O(2)$, $SO(3)$, and $O(3)$ (discretized as the cyclic $C8$, dihedral $D16$, chiral octahedral $O$ and full octahedral $O_h$ groups)\textemdash acting on $\mathbb{R}^2$ and $\mathbb{R}^3$ on both $G$-MNIST and $G$-ModelNet10 datasets.","['equivariance', 'group-equivariant cnns', 'invariance', 'pooling', 'convolutional neural networks']",[],"['Sophia Sanborn', 'Nina Miolane']","['University of California, Santa Barbara', 'University of California, Santa Barbara']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70607,Privacy & Data Governance,Adversarial Learning for Feature Shift Detection and Correction,"Data shift is a phenomenon present in many real-world applications, and while there are multiple methods attempting to detect shifts, the task of localizing and correcting the features originating such shifts has not been studied in depth. Feature shifts can occur in many datasets, including in multi-sensor data, where some sensors are malfunctioning, or in tabular and structured data, including biomedical, financial, and survey data, where faulty standardization and data processing pipelines can lead to erroneous features. In this work, we explore using the principles of adversarial learning, where the information from several discriminators trained to distinguish between two distributions is used to both detect the corrupted features and fix them in order to remove the distribution shift between datasets. We show that mainstream supervised classifiers, such as random forest or gradient boosting trees, combined with simple iterative heuristics, can localize and correct feature shifts, outperforming current statistical and neural network-based techniques. The code is available at https://github.com/AI-sandbox/DataFix.","['feature shift detection', 'distribution shift', 'shift', 'data-centric AI']",[],"['Míriam Barrabés', 'Daniel Mas Montserrat', 'Margarita Geleta', 'Xavier Giró-i-Nieto', 'Alexander G Ioannidis']","['Cork Institute of Technology', 'Stanford University', 'University of California, Berkeley', 'Amazon', 'Stanford University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70618,Privacy & Data Governance,BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning,"Backdoor attacks pose a severe threat to the supply chain management of deep reinforcement learning (DRL) policies. Despite initial defenses proposed in recent studies, these methods have very limited generalizability and scalability. To address this issue, we propose BIRD, a technique to detect and remove backdoors from a pretrained DRL policy in a clean environment without requiring any knowledge about the attack specifications and accessing its training process. By analyzing the unique properties and behaviors of backdoor attacks, we formulate trigger restoration as an optimization problem and design a novel metric to detect backdoored policies. We also design a finetuning method to remove the backdoor, while maintaining the agent's performance in the clean environment. We evaluate BIRD against three backdoor attacks in ten different single-agent or multi-agent environments. Our results verify the effectiveness, efficiency, and generalizability of BIRD, as well as its robustness to different attack variations and adaptions.","['Backdoor Defense', 'Deep Reinforcement Learning']",[],"['Wenbo Guo', 'Guanhong Tao', 'Xiangyu Zhang', 'Dawn Song']","['University of California, Santa Barbara', 'Purdue University', ', Purdue University', 'University of California Berkeley']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70624,Privacy & Data Governance,Computing Optimal Nash Equilibria in Multiplayer Games,"Designing efficient algorithms to compute a Nash Equilibrium (NE) in multiplayer games is still an open challenge. In this paper, we focus on computing an NE that optimizes a given objective function. For example, when there is a team of players independently playing against an adversary in a game (e.g., several groups in a forest trying to interdict illegal loggers in green security games), these team members may need to find an NE minimizing the adversary’s utility. Finding an optimal NE in multiplayer games can be formulated as a mixed-integer bilinear program by introducing auxiliary variables to represent bilinear terms, leading to a huge number of bilinear terms, making it hard to solve. To overcome this challenge, we first propose a general framework for this formulation based on a set of correlation plans. We then develop a novel algorithm called CRM based on this framework, which uses correlation plans with their relations to strictly reduce the feasible solution space after the convex relaxation of bilinear terms while minimizing the number of correlation plans to significantly reduce the number of bilinear terms. We show that our techniques can significantly reduce the time complexity and CRM can be several orders of magnitude faster than the state-of-the-art baseline.","['Algorithmic game theory', 'Optimal Nash equilibrium']",[],"['Youzhi Zhang', 'Bo An', 'Venkatramanan Siva Subrahmanian']","['Centre for Artificial Intelligence and Robotics,  Institute of Science & Innovation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Maryland - College Park']","['Hong Kong', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70264,Privacy & Data Governance,DISCOVER: Making Vision Networks Interpretable via Competition and Dissection,"Modern deep networks are highly complex and their inferential outcome very hard to interpret. This is a serious obstacle to their transparent deployment in safety-critical or bias-aware applications. This work contributes to *post-hoc* interpretability, and specifically Network Dissection. Our goal is to present a framework that makes it easier to *discover* the individual functionality of each neuron in a network trained on a vision task; discovery is performed in terms of textual description generation. To achieve this objective, we leverage: (i) recent advances in multimodal vision-text models and (ii) network layers founded upon the novel concept of stochastic local competition between linear units. In this setting, only a *small subset* of layer neurons are activated *for a given input*, leading to extremely high activation sparsity (as low as only $\approx 4\%$). Crucially, our proposed method infers (sparse) neuron activation patterns that enables the neurons to activate/specialize to inputs with specific characteristics, diversifying their individual functionality. This capacity of our method supercharges the potential of dissection processes: human understandable descriptions are generated only for the very few active neurons, thus facilitating the direct investigation of the network's decision process. As we experimentally show, our approach: (i) yields Vision Networks that retain or improve classification performance, and (ii) realizes a principled framework for text-based description and examination of the generated neuronal representations.","['Interpretability', 'Explainability', 'Network Dissection', 'Competitive Networks', 'Sparsity', 'Multimodal Models']",[],"['Konstantinos P. Panousis', 'Sotirios Chatzis']","['INRIA', 'University of Technology']","[None, 'Cyprus']",,,,,,,
https://nips.cc/virtual/2023/poster/70278,Privacy & Data Governance,VOCE: Variational Optimization with Conservative Estimation for Offline Safe Reinforcement Learning,"Offline safe reinforcement learning (RL) algorithms promise to learn policies that satisfy safety constraints directly in offline datasets without interacting with the environment. This arrangement is particularly important in scenarios with high sampling costs and potential dangers, such as autonomous driving and robotics. However, the influence of safety constraints and out-of-distribution (OOD) actions have made it challenging for previous methods to achieve high reward returns while ensuring safety. In this work, we propose a Variational Optimization with Conservative Eestimation algorithm (VOCE) to solve the problem of optimizing safety policies in the offline dataset. Concretely, we reframe the problem of offline safe RL using probabilistic inference, which introduces variational distributions to make the optimization of policies more flexible. Subsequently, we utilize pessimistic estimation methods to estimate the Q-value of cost and reward, which mitigates the extrapolation errors induced by OOD actions. Finally, extensive experiments demonstrate that the VOCE algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety.","['Offline safe reinforcement learning', 'Pessimistic conservative estimation', 'Variational optimization', 'Reinforcement Learning']",[],"['Jiayi Guan', 'Guang Chen', 'Jiaming Ji', 'Long Yang', 'Ao Zhou', 'Zhijun Li', 'changjun jiang']","['Tongji University', 'Tongji University', 'Peking University', 'Peking University', 'Tongji University', 'University of Science and Technology of , Tsinghua University', 'Tongji University']","[None, None, None, None, None, 'China', None]",,,,,,,
https://nips.cc/virtual/2023/poster/70854,Privacy & Data Governance,Content-based Unrestricted Adversarial Attack,"Unrestricted adversarial attacks typically manipulate the semantic content of an image (e.g., color or texture) to create adversarial examples that are both effective and photorealistic, demonstrating their ability to deceive human perception and deep neural networks with stealth and success. However, current works usually sacrifice unrestricted degrees and subjectively select some image content to guarantee the photorealism of unrestricted adversarial examples, which limits its attack performance. To ensure the photorealism of adversarial examples and boost attack performance, we propose a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack. By leveraging a low-dimensional manifold that represents natural images, we map the images onto the manifold and optimize them along its adversarial direction. Therefore, within this framework, we implement Adversarial Content Attack (ACA) based on Stable Diffusion and can generate high transferable unrestricted adversarial examples with various adversarial contents. Extensive experimentation and visualization demonstrate the efficacy of ACA, particularly in surpassing state-of-the-art attacks by an average of 13.3-50.4\% and 16.8-48.0\% in normally trained models and defense methods, respectively.","['unrestricted attack', 'adversarial example', 'diffusion model', 'black-box attack', 'adversarial transferability']",[],"['Zhaoyu Chen', 'Bo Li', 'Shuang Wu', 'Kaixun Jiang', 'Shouhong Ding', 'Wenqiang Zhang']","['Fudan University', 'Tencent Youtu Lab', 'Tencent YouTu Lab', 'Fudan University', 'Tencent Youtu Lab', 'Fudan University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70867,Privacy & Data Governance,Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective,"For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth labels, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose $\texttt{ARCO}$, a semi-supervised contrastive learning (CL) framework with stratified group theory for medical image segmentation. In particular, we first propose building $\texttt{ARCO}$ through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in pixel/voxel-level segmentation tasks with extremely limited labels. Furthermore, we theoretically prove these sampling techniques are universal in variance reduction. Finally, we experimentally validate our approaches on eight benchmarks, i.e., five 2D/3D medical and three semantic segmentation datasets, with different label settings, and our methods consistently outperform state-of-the-art semi-supervised methods. Additionally, we augment the CL frameworks with these sampling techniques and demonstrate significant gains over previous methods. We believe our work is an important step towards semi-supervised medical image segmentation by quantifying the limitation of current self-supervision objectives for accomplishing such challenging safety-critical tasks.","['Long-tailed Medical Image Segmentation', 'Contrastive Learning', 'Variance Reduction', 'Imbalanced Learning', 'Semi-Supervised Learning']",[],"['Chenyu You', 'Weicheng Dai', 'Yifei Min', 'Fenglin Liu', 'David A. Clifton', 'S Kevin Zhou', 'Lawrence Hamilton Staib', 'James s Duncan']","['Yale University', 'Monash University', 'Yale University', 'University of Oxford', 'University of Oxford', 'University of Science and Technology of', 'Yale University', 'Yale University']","[None, None, None, None, None, 'China', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70909,Privacy & Data Governance,In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer,"Enabling machine learning classifiers to defer their decision to a downstream expert when the expert is more accurate will ensure improved safety and performance. This objective can be achieved with the learning-to-defer framework which aims to jointly learn how to classify and how to defer to the expert. In recent studies, it has been theoretically shown that popular estimators for learning to defer parameterized with softmax provide unbounded estimates for the likelihood of deferring which makes them uncalibrated. However, it remains unknown whether this is due to the widely used softmax parameterization and if we can find a softmax-based estimator that is both statistically consistent and possesses a valid probability estimator. In this work, we first show that the cause of the miscalibrated and unbounded estimator in prior literature is due to the symmetric nature of the surrogate losses used and not due to softmax. We then propose a novel statistically consistent asymmetric softmax-based surrogate loss that can produce valid estimates without the issue of unboundedness. We further analyze the non-asymptotic properties of our proposed method and empirically validate its performance and calibration on benchmark datasets.","['Classification', 'Learning to Defer', 'Probability Estimation']",[],"['Yuzhou Cao', 'Hussein Mozannar', 'Lei Feng', 'Hongxin Wei', 'Bo An']","['Nanyang Technological University', 'Massachusetts Institute of Technology', 'Nanyang Technological University', 'Southern University of Science and Technology', 'Nanyang Technological University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70926,Privacy & Data Governance,Iterative Reachability Estimation for Safe Reinforcement Learning,"Ensuring safety is important for the practical deployment of reinforcement learning (RL). Various challenges must be addressed, such as handling stochasticity in the environments, providing rigorous guarantees of persistent state-wise safety satisfaction, and avoiding overly conservative behaviors that sacrifice performance. We propose a new framework, Reachability Estimation for Safe Policy Optimization (RESPO), for safety-constrained RL in general stochastic settings. In the feasible set where there exist violation-free policies, we optimize for rewards while maintaining persistent safety. Outside this feasible set, our optimization produces the safest behavior by guaranteeing entrance into the feasible set whenever possible with the least cumulative discounted violations. We introduce a class of algorithms using our novel reachability estimation function to optimize in our proposed framework and in similar frameworks such as those concurrently handling multiple hard and soft constraints. We theoretically establish that our algorithms almost surely converge to locally optimal policies of our safe optimization framework. We evaluate the proposed methods on a diverse suite of safe RL environments from Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both reward performance and safety compared with state-of-the-art baselines.","['Constraints', 'Safety', 'Hamilton Jacobi Reachability', 'Deep Reinforcement Learning', 'Robotics']",[],"['Milan Ganai', 'Zheng Gong', 'Chenning Yu', 'Sylvia Lee Herbert', 'Sicun Gao']","['University of California, San Diego', 'University of California, San Diego', 'University of California, San Diego', 'University of California, San Diego', 'University of California, San Diego']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70490,Privacy & Data Governance,Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense,"Recent advancements in masked image modeling (MIM) have made it a prevailing framework for self-supervised visual representation learning. The MIM pretrained models, like most deep neural network methods, remain vulnerable to adversarial attacks, limiting their practical application, and this issue has received little research attention. In this paper, we investigate how this powerful self-supervised learning paradigm can provide adversarial robustness to downstream classifiers. During the exploration, we find that noisy image modeling (NIM), a simple variant of MIM that adopts denoising as the pre-text task, reconstructs noisy images surprisingly well despite severe corruption. Motivated by this observation, we propose an adversarial defense method, referred to as De^3, by exploiting the pretrained decoder for denoising. Through De^3, NIM is able to enhance adversarial robustness beyond providing pretrained features. Furthermore, we incorporate a simple modification, sampling the noise scale hyperparameter from random distributions, and enable the defense to achieve a better and tunable trade-off between accuracy and robustness. Experimental results demonstrate that, in terms of adversarial robustness, NIM is superior to MIM thanks to its effective denoising capability. Moreover, the defense provided by NIM achieves performance on par with adversarial training while offering the extra tunability advantage. Source code and models are available at https://github.com/youzunzhi/NIM-AdvDef.","['self-supervised learning', 'adversarial robustness']",[],"['Zunzhi You', 'Daochang Liu', 'Bohyung Han', 'Chang Xu']","['University of Sydney', 'University of Sydney', 'POSTECH', 'University of Sydney']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70992,Privacy & Data Governance,Characterizing Out-of-Distribution Error via Optimal Transport,"Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models, so methods of predicting a model's performance on OOD data without labels are important for machine learning safety. While a number of methods have been proposed by prior work, they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks. In this work, we identify *pseudo-label shift*, or the difference between the predicted and true OOD label distributions, as a key indicator of this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual transport costs and further improves the accuracy of COT's error estimates. We evaluate COT and COTT on a variety of standard benchmarks that induce various types of distribution shift -- synthetic, novel subpopulation, and natural -- and show that our approaches significantly outperform existing state-of-the-art methods with up to 3x lower prediction errors.","['Distribution Shift', 'OOD Error Prediction', 'Optimal Transport', 'Deep Learning']",[],"['Yuzhe Lu', 'Yilong Qin', 'Runtian Zhai', 'Andrew Shen', 'Zhenlin Wang', 'Soheil Kolouri', 'Simon Stepputtis', 'Joseph Campbell', 'Katia P. Sycara']","['School of Computer Science, Carnegie Mellon University', 'OpenAI', 'Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'Vanderbilt University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71009,Privacy & Data Governance,Improving Adversarial Transferability via Intermediate-level Perturbation Decay,"Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneously. In-depth discussion verifies the effectiveness of our method. Experimental results show that it outperforms state-of-the-arts by large margins in attacking various victim models on ImageNet (+10.07% on average) and CIFAR-10 (+3.88% on average). Our code is at https://github.com/qizhangli/ILPD-attack.","['adversarial examples', 'black-box attack', 'adversarial transferability']",[],"['Qizhang Li', 'Yiwen Guo', 'Wangmeng Zuo', 'Hao Chen']","['Harbin Institute of Technology', 'ByteDance', 'Harbin Institute of Technology', 'University of California, Davis']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71193,Privacy & Data Governance,Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests,"Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a ""bag"" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to ""positive"" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and other works derived from these models will share the same issue. In any context in which these models are being used, this creates the potential for learning incorrect models, which creates risk of operational failure.  We identify and demonstrate this problem via a proposed ``algorithmic unit test'', where we create synthetic datasets that can be solved by a MIL respecting model, and which clearly reveal learning that violates MIL assumptions. The five evaluated methods each fail one or more of these tests. This provides a model-agnostic way to identify violations of modeling assumptions, which we hope will be useful for future development and evaluation of MIL models.",['reproducibility; multiple instance learning'],[],"['Edward Raff', 'James Holt']","['University of Maryland, Baltimore County', 'Laboratory for Physical Sciences']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71230,Privacy & Data Governance,Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations,"Recent neural architecture search (NAS) frameworks have been successful in finding optimal architectures for given conditions (e.g., performance or latency). However, they search for optimal architectures in terms of their performance on clean images only, while robustness against various types of perturbations or corruptions is crucial in practice. Although there exist several robust NAS frameworks that tackle this issue by integrating adversarial training into one-shot NAS, however, they are limited in that they only consider robustness against adversarial attacks and require significant computational resources to discover optimal architectures for a single task, which makes them impractical in real-world scenarios. To address these challenges, we propose a novel lightweight robust zero-cost proxy that considers the consistency across features, parameters, and gradients of both clean and perturbed images at the initialization state. Our approach facilitates an efficient and rapid search for neural architectures capable of learning generalizable features that exhibit robustness across diverse perturbations. The experimental results demonstrate that our proxy can rapidly and efficiently search for neural architectures that are consistently robust against various perturbations on multiple benchmark datasets and diverse search spaces, largely outperforming existing clean zero-shot NAS and robust NAS with reduced search cost.","['neural architecture search', 'generalization', 'efficiency', 'zero-cost proxy']",[],"['Hyeonjeong Ha', 'Minseon Kim', 'Sung Ju Hwang']","['Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71256,Privacy & Data Governance,NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks,"While multi-exit neural networks are regarded as a promising solution for making efficient inference via early exits, combating adversarial attacks remains a challenging problem. In multi-exit networks, due to the high dependency among different submodels, an adversarial example targeting a specific exit not only degrades the performance of the target exit but also reduces the performance of all other exits concurrently. This makes multi-exit networks highly vulnerable to simple adversarial attacks. In this paper, we propose NEO-KD, a knowledge-distillation-based adversarial training strategy that tackles this fundamental challenge based on two key contributions. NEO-KD first resorts to neighbor knowledge distillation to guide the output of the adversarial examples to tend to the ensemble outputs of neighbor exits of clean data. NEO-KD also employs exit-wise orthogonal knowledge distillation for reducing adversarial transferability across different submodels. The result is a significantly improved robustness against adversarial attacks. Experimental results on various datasets/models show that our method achieves the best adversarial accuracy with reduced computation budgets, compared to the baselines relying on existing adversarial training or knowledge distillation techniques for multi-exit networks.","['Multi-exit Neural Network', 'Adversarial Training', 'Knowledge Distillation', 'Adversarial Transferability']",[],"['Seokil Ham', 'Jungwuk Park', 'Dong-Jun Han', 'Jaekyun Moon']","['KAIST', 'Korea Advanced Institute of Science and Technology', 'Purdue University', 'KAIST']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71290,Privacy & Data Governance,GAN You See Me? Enhanced Data Reconstruction Attacks against Split Inference,"Split Inference (SI) is an emerging deep learning paradigm that addresses computational constraints on edge devices and preserves data privacy through collaborative edge-cloud approaches. However, SI is vulnerable to Data Reconstruction Attacks (DRA), which aim to reconstruct users' private prediction instances. Existing attack methods suffer from various limitations. Optimization-based DRAs do not leverage public data effectively, while Learning-based DRAs depend heavily on auxiliary data quantity and distribution similarity. Consequently, these approaches yield unsatisfactory attack results and are sensitive to defense mechanisms. To overcome these challenges, we propose a GAN-based LAtent Space Search attack (GLASS) that harnesses abundant prior knowledge from public data using advanced StyleGAN technologies. Additionally, we introduce GLASS++ to enhance reconstruction stability. Our approach represents the first GAN-based DRA against SI, and extensive evaluation across different split points and adversary setups demonstrates its state-of-the-art performance. Moreover, we thoroughly examine seven defense mechanisms, highlighting our method's capability to reveal private information even in the presence of these defenses.","['deep learning', 'split inference', 'data reconstruction attack']",[],"['Ziang Li', 'Mengda Yang', 'Yaxin Liu', 'Juan Wang', 'Hongxin Hu', 'Wenzhe Yi', 'Xiaoyang Xu']","['Wuhan University', 'Wuhan University', 'Wuhan University', 'Wuhan University', 'State University of New York, Buffalo', 'Wuhan University', 'Wuhan University']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70720,Privacy & Data Governance,Punctuation-level Attack: Single-shot and Single Punctuation Can Fool Text Models,"The adversarial attacks have attracted increasing attention in various fields including natural language processing. The current textual attacking models primarily focus on fooling models by adding character-/word-/sentence-level perturbations, ignoring their influence on human perception. In this paper, for the first time in the community, we propose a novel mode of textual attack, punctuation-level attack. With various types of perturbations, including insertion, displacement, deletion, and replacement, the punctuation-level attack achieves promising fooling rates against SOTA models on typical textual tasks and maintains minimal influence on human perception and understanding of the text by mere perturbation of single-shot single punctuation. Furthermore, we propose a search method named Text Position Punctuation Embedding and Paraphrase (TPPEP) to accelerate the pursuit of optimal position to deploy the attack, without exhaustive search, and we present a mathematical interpretation of TPPEP. Thanks to the integrated Text Position Punctuation Embedding (TPPE), the punctuation attack can be applied at a constant cost of time. Experimental results on public datasets and SOTA models demonstrate the effectiveness of the punctuation attack and the proposed TPPE. We additionally apply the single punctuation attack to summarization, semantic-similarity-scoring, and text-to-image tasks, and achieve encouraging results.","['Punctuation-level Attack', 'Textual Adversarial attack', 'Natural Language Processing']",[],"['Wenqiang Wang', 'Chongyang Du', 'Tao Wang', 'Wenhan Luo', 'Lin Ma', 'Wei Liu', 'Xiaochun Cao']","['SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY', 'Nanjing University', 'SUN YAT-SEN UNIVERSITY', 'Meituan', 'Tencent', 'SUN YAT-SEN UNIVERSITY']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71360,Privacy & Data Governance,Mirror Diffusion Models for Constrained and Watermarked Generation,"Modern successes of diffusion models in learning complex, high-dimensional data distributions are attributed, in part, to their capability to construct diffusion processes with analytic transition kernels and score functions. The tractability results in a simulation-free framework with stable regression losses, from which reversed, generative processes can be learned at scale. However, when data is confined to a constrained set as opposed to a standard Euclidean space, these desirable characteristics appear to be lost based on prior attempts. In this work, we propose Mirror Diffusion Models (MDM), a new class of diffusion models that generate data on convex constrained sets without losing any tractability. This is achieved by learning diffusion processes in a dual space constructed from a mirror map, which, crucially, is a standard Euclidean space. We derive efficient computation of mirror maps for popular constrained sets, such as simplices and $\ell_2$-balls, showing significantly improved performance of MDM over existing methods. For safety and privacy purposes, we also explore constrained sets as a new mechanism to embed invisible but quantitative information (i.e., watermarks) in generated data, for which MDM serves as a compelling approach. Our work brings new algorithmic opportunities for learning tractable diffusion on complex domains.","['diffusion models', 'constrained generation', 'constrained manifold', 'mirror map', 'watermarked generation', 'generation privacy']",[],"['Guan-Horng Liu', 'Tianrong Chen', 'Evangelos Theodorou', 'Molei Tao']","['Institute of Technology', 'Institute of Technology', 'Institute of Technology', 'Institute of Technology']","['Georgia', 'Georgia', 'Georgia', 'Georgia']",,,,,,,
https://nips.cc/virtual/2023/poster/71467,Privacy & Data Governance,Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,"Recent studies have demonstrated the susceptibility of deep neural networks to backdoor attacks. Given a backdoored model, its prediction of a poisoned sample with trigger will be dominated by the trigger information, though trigger information and benign information coexist. Inspired by the mechanism of the optical polarizer that a polarizer could pass light waves with particular polarizations while filtering light waves with other polarizations, we propose a novel backdoor defense method by inserting a learnable neural polarizer into the backdoored model as an intermediate layer, in order to purify the poisoned sample via filtering trigger information while maintaining benign information. The neural polarizer is instantiated as one lightweight linear transformation layer, which is learned through solving a well designed bi-level optimization problem, based on a limited clean dataset. Compared to other fine-tuning-based defense methods which often adjust all parameters of the backdoored model, the proposed method only needs to learn one additional layer, such that it is more efficient and requires less clean data. Extensive experiments demonstrate the effectiveness and efficiency of our method in removing backdoors across various neural network architectures and datasets, especially in the case of very limited clean data. Codes are available at \href{https://github.com/SCLBD/BackdoorBench}{https://github.com/SCLBD/BackdoorBench} (PyTorch) and \href{https://github.com/JulieCarlon/NPD-MindSpore}{https://github.com/JulieCarlon/NPD-MindSpore} (MindSpore).","['Backdoor Defense', 'Backdoor Learning', 'Trustworthy AI']",[],"['Mingli Zhu', 'Shaokui Wei', 'Hongyuan Zha', 'Baoyuan Wu']","['The Chinese University of (Shen Zhen)', 'The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/71478,Privacy & Data Governance,Strategic Behavior in Two-sided Matching Markets with Prediction-enhanced Preference-formation,"Two-sided matching markets have long existed to pair agents in the absence of regulated exchanges.  A common example is school choice, where a matching mechanism uses student and school preferences to assign students to schools. In such settings, forming preferences is both difficult and critical. Prior work has suggested various prediction mechanisms that help agents make decisions about their preferences. Although often deployed together, these matching and prediction mechanisms are almost always analyzed separately. The present work shows that at the intersection of the two lies a previously unexplored type of strategic behavior: agents returning to the market (e.g., schools) can attack future predictions by interacting short-term non-optimally with their matches. Here, we first introduce this type of strategic behavior, which we call an adversarial interaction attack. Next, we construct a formal economic model that captures the feedback loop between prediction mechanisms designed to assist agents and the matching mechanism used to pair them. Finally, in a simplified setting, we prove that returning agents can benefit from using adversarial interaction attacks and gain progressively more as the trust in and accuracy of predictions increases. We also show that this attack increases inequality in the student population.","['matching markets', 'strategic behaviour', 'ML-based forecasting', 'recommender systems', 'adversarial attacks', 'agent-based modelling']",[],"['Stefania Ionescu', 'Yuhao Du', 'Kenneth Joseph', 'Aniko Hannak']","['Department of Informatics, University of Zurich, University of Zurich', 'State University of New York, Buffalo', 'State University of New York at Buffalo', 'Northeastern University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72630,Privacy & Data Governance,Towards Stable Backdoor Purification through Feature Shift Tuning,"It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based defenses. Therefore, it is necessary to disentangle the backdoor and clean features in order to improve backdoor purification. To address this, we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor purification. Specifically, FST encourages feature shifts by actively deviating the classifier weights from the originally compromised weights. Extensive experiments demonstrate that our FST provides consistently stable performance under different attack settings. Without complex parameter adjustments, FST also achieves much lower tuning costs, only $10$ epochs. Our codes are available at https://github.com/AISafety-HKUST/stable_backdoor_purification.","['Backdoor Defense', 'Model-tuning']",[],"['Rui Min', 'Zeyu Qin', 'Li Shen', 'Minhao Cheng']","['University of Science and Technology', 'The  University of Science and Technology', 'JD Explore Academy', 'Pennsylvania State University']","['Hong Kong', 'Hong Kong', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71998,Privacy & Data Governance,Wasserstein distributional robustness of neural networks,"Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multistep version. Our attack algorithms include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) as special cases. Furthermore, we provide a new asymptotic estimate of the adversarial accuracy against distributional threat models. The bound is fast to compute and first-order accurate, offering new insights even for the pointwise AA. It also naturally yields out-of-sample performance guarantees. We conduct numerical experiments on CIFAR-10, CIFAR-100, ImageNet datasets using DNNs on RobustBench to illustrate our theoretical results. Our code is available at https://github.com/JanObloj/W-DRO-Adversarial-Methods.","['adversarial attack', 'adversarial robustness of DNN', 'adversarial training', 'Wasserstein distance', 'distributionally robust optimization', 'sensitivity analysis', 'asymptotic bounds']",[],"['Xingjian Bai', 'Guangyi He', 'Yifan Jiang', 'Jan Obloj']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72011,Privacy & Data Governance,Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments,"Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper,  we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.",['Fairness Continual Learning; Semantic Segmentation; Contrastive Clustering;'],[],"['Thanh-Dat Truong', 'Hoang-Quan Nguyen', 'Bhiksha Raj', 'Khoa Luu']","['University of Arkansas, Fayetteville', 'University of Arkansas - Fayetteville', 'Carnegie Mellon University', 'University of Arkansas, Fayetteville']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72659,Privacy & Data Governance,Efficient Subgame Refinement for Extensive-form Games,"Subgame solving is an essential technique in addressing large imperfect information games, with various approaches developed to enhance the performance of refined strategies in the abstraction of the target subgame. However, directly applying existing subgame solving techniques may be difficult, due to the intricate nature and substantial size of many real-world games. To overcome this issue, recent subgame solving methods allow for subgame solving on limited knowledge order subgames, increasing their applicability in large games; yet this may still face obstacles due to extensive information set sizes. To address this challenge, we propose a generative subgame solving (GS2) framework, which utilizes a generation function to identify a subset of the earliest-reached nodes, reducing the size of the subgame. Our method is supported by a theoretical analysis and employs a diversity-based generation function to enhance safety. Experiments conducted on medium-sized games as well as the challenging large game of GuanDan demonstrate a significant improvement over the blueprint.","['Subgame solving', 'extensive-form game', 'imperfect information']",[],"['Zhenxing Ge', 'Zheng Xu', 'Tianyu Ding', 'Wenbin Li', 'Yang Gao']","['Nanjing University', 'Nanjing University', 'Microsoft', 'Nanjing University', 'Nanjing University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72757,Privacy & Data Governance,Robust Lipschitz Bandits to Adversarial Corruptions,"Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effectiveness of our algorithms against two classic kinds of attacks.",['bandits'],[],"['Yue Kang', 'Cho-Jui Hsieh']","['University of California, Davis', 'Google']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72858,Privacy & Data Governance,"CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography","Current image steganography techniques are mainly focused on cover-based methods, which commonly have the risk of leaking secret images and poor robustness against degraded container images. Inspired by recent developments in diffusion models, we discovered that two properties of diffusion models, the ability to achieve translation between two images without training, and robustness to noisy data, can be used to improve security and natural robustness in image steganography tasks. For the choice of diffusion model, we selected Stable Diffusion, a type of conditional diffusion model, and fully utilized the latest tools from open-source communities, such as LoRAs and ControlNets, to improve the controllability and diversity of container images. In summary, we propose a novel image steganography framework, named Controllable, Robust and Secure Image Steganography (CRoSS), which has significant advantages in controllability, robustness, and security compared to cover-based image steganography methods. These benefits are obtained without additional training. To our knowledge, this is the first work to introduce diffusion models to the field of image steganography. In the experimental section, we conducted detailed experiments to demonstrate the advantages of our proposed CRoSS framework in controllability, robustness, and security.","['Diffusion models', 'image steganography', 'Stable Diffusion', 'coverless steganography']",[],"['Jiwen Yu', 'Xuanyu Zhang', 'Youmin Xu', 'Jian Zhang']","['Peking University', 'Peking University', 'Peking University', 'Peking University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72885,Privacy & Data Governance,RECESS Vaccine for Federated Learning: Proactive Defense Against Model Poisoning Attacks,"Model poisoning attacks greatly jeopardize the application of federated learning (FL). The effectiveness of existing defenses is susceptible to the latest model poisoning attacks, leading to a decrease in prediction accuracy. Besides, these defenses are intractable to distinguish benign outliers from malicious gradients, which further compromises the model generalization. In this work, we propose a novel defense including detection and aggregation, named RECESS, to serve as a “vaccine” for FL against model poisoning attacks. Different from the passive analysis in previous defenses, RECESS proactively queries each participating client with a delicately constructed aggregation gradient, accompanied by the detection of malicious clients according to their responses with higher accuracy. Further, RECESS adopts a newly proposed trust scoring based mechanism to robustly aggregate gradients. Rather than previous methods of scoring in each iteration, RECESS takes into account the correlation of clients’ performance over multiple iterations to estimate the trust score, bringing in a significant increase in detection fault tolerance. Finally, we extensively evaluate RECESS on typical model architectures and four datasets under various settings including white/black-box, cross-silo/device FL, etc. Experimental results show the superiority of RECESS in terms of reducing accuracy loss caused by the latest model poisoning attacks over five classic and two state-of-the-art defenses.","['Federated Learning', 'Model Poisoning Attacks', 'Proactive Detection', 'Robust Aggregation', 'Benign Outlier Identification']",[],"['Wenjing Zhang', 'Qian Chen', 'Xiaoguang Li', 'HUI LI', 'Xiaodong Lin']","['University of Guelph', 'Xidian University', 'Xidian University', 'xidian university', 'University of Guelph']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72912,Privacy & Data Governance,Posthoc privacy guarantees for collaborative inference with modified Propose-Test-Release,"Cloud-based machine learning inference is an emerging paradigm where users query by sending their data through a service provider who runs an ML model on that data and returns back the answer. Due to increased concerns over data privacy, recent works have proposed Collaborative Inference (CI) to learn a privacy-preserving encoding of sensitive user data before it is shared with an untrusted service provider. Existing works so far evaluate the privacy of these encodings through empirical reconstruction attacks. In this work, we develop a new framework that provides formal privacy guarantees for an arbitrarily trained neural network by linking its local Lipschitz constant with its local sensitivity. To guarantee privacy using local sensitivity, we extend the Propose-Test-Release (PTR) framework to make it tractable for neural network queries. We verify the efficacy of our framework experimentally on real-world datasets and elucidate the role of Adversarial Representation Learning (ARL) in improving the privacy-utility trade-off.","['privacy', 'deep learning', 'neural networks', 'adversarial learning', 'reconstruction guarantees', 'collaborative inference', 'MLaaS']",[],"['Abhishek Singh', 'Praneeth Vepakomma', 'Vivek Sharma', 'Ramesh Raskar']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts General Hospital, Harvard University', 'Massachusetts Institute of Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72927,Privacy & Data Governance,Training on Foveated Images Improves Robustness to Adversarial Attacks,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle,  perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25% higher accuracy on perturbed data.","['adversarial robustness', 'computer vision', 'biologically-inspired', 'retina', 'blurring']",[],"['Muhammad A Shah', 'Aqsa Kashaf', 'Bhiksha Raj']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72263,Privacy & Data Governance,BCDiff: Bidirectional Consistent Diffusion for Instantaneous Trajectory Prediction,"The objective of pedestrian trajectory prediction is to estimate the future paths of pedestrians by leveraging historical observations, which plays a vital role in ensuring the safety of self-driving vehicles and navigation robots. Previous works usually rely on a sufficient amount of observation time to accurately predict future trajectories. However, there are many real-world situations where the model lacks sufficient time to observe, such as when pedestrians abruptly emerge from blind spots, resulting in inaccurate predictions and even safety risks. Therefore, it is necessary to perform trajectory prediction based on instantaneous observations, which has rarely been studied before. In this paper, we propose a Bi-directional Consistent Diffusion framework tailored for instantaneous trajectory prediction, named BCDiff. At its heart, we develop two coupled diffusion models by designing a mutual guidance mechanism which can bidirectionally and consistently generate unobserved historical trajectories and future trajectories step-by-step,  to utilize the complementary information between them. Specifically, at each step, the predicted unobserved historical trajectories and limited observed trajectories guide one diffusion model to generate future trajectories, while the predicted future trajectories and observed trajectories guide the other diffusion model to predict unobserved historical trajectories. Given the presence of relatively high noise in the generated trajectories during the initial steps, we introduce a gating mechanism to learn the weights between the predicted trajectories and the limited observed trajectories for automatically balancing their contributions. By means of this iterative and mutually guided generation process, both the future and unobserved historical trajectories undergo continuous refinement, ultimately leading to accurate predictions. Essentially, BCDiff is an encoder-free framework that can be compatible with existing trajectory prediction models in principle. Experiments show that our proposed BCDiff significantly improves the accuracy of instantaneous trajectory prediction on the ETH/UCY and Stanford Drone datasets, compared to related approaches.","['Trajectory prediction', 'instantaneous observation']",[],"['Rongqing Li', 'Changsheng Li', 'Dongchun Ren', 'Guangyi Chen', 'Guoren Wang']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'ALLRIDE.AI', 'Mohamed bin Zayed University of Artificial Intelligence', 'Beijing Institute of Technology']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72322,Privacy & Data Governance,Byzantine-Tolerant Methods for Distributed Variational Inequalities,"Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work [Abidi et al., 2022] addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.","['byzantine robustness', 'variational inequalities', 'min-max problems']",[],"['Nazarii Tupitsa', 'Abdulla Jasem Almansoori', 'Yanlin Wu', 'Martin Takáč', 'Karthik Nandakumar', 'Samuel Horváth', 'Eduard Gorbunov']","['Moscow Institute of Physics and Technology', 'Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed Bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72384,Privacy & Data Governance,ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP,"Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. We employ ChatGPT, a state-of-the-art large language model, as our paraphraser and formulate the trigger-removal task as a prompt engineering problem. We adopt fuzzing, a technique commonly used for unearthing software vulnerabilities, to discover optimal paraphrase prompts that can effectively eliminate triggers while concurrently maintaining input semantics. Experiments on 4 types of backdoor attacks, including the subtle style backdoors, and 4 distinct datasets demonstrate that our approach surpasses baseline methods, including STRIP, RAP, and ONION, in precision and recall.","['NLP', 'backdoor attack', 'fuzzing']",[],"['Lu Yan', 'ZHUO ZHANG', 'Guanhong Tao', 'Kaiyuan Zhang', 'Guangyu Shen', 'Xiangyu Zhang']","['Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', ', Purdue University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72434,Privacy & Data Governance,A Theory of Transfer-Based Black-Box Attacks: Explanation and Implications,"Transfer-based attacks are a practical method of black-box adversarial attacks, in which the attacker aims to craft adversarial examples from a source (surrogate) model that is transferable to the target model. A wide range of empirical works has tried to explain the transferability of adversarial examples from different angles. However, these works only provide ad hoc explanations without quantitative analyses. The theory behind transfer-based attacks remains a mystery. This paper studies transfer-based attacks under a unified theoretical framework. We propose an explanatory model, called the manifold attack model, that formalizes popular beliefs and explains the existing empirical results. Our model explains why adversarial examples are transferable even when the source model is inaccurate. Moreover, our model implies that the existence of transferable adversarial examples depends on the “curvature” of the data manifold, which quantitatively explains why the success rates of transfer-based attacks are hard to improve. We also discuss the expressive power and the possible extensions of our model in general applications.",['Learning Theory'],[],"['Yanbo Chen', 'Weiwei Liu']","['Wuhan University', 'Wuhan University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72938,Privacy & Data Governance,Effective Targeted Attacks for Adversarial Self-Supervised Learning,"Recently, unsupervised adversarial training (AT) has been highlighted as a means of achieving robustness in models without any label information. Previous studies in unsupervised AT have mostly focused on implementing self-supervised learning (SSL) frameworks, which maximize the instance-wise classification loss to generate adversarial examples. However, we observe that simply maximizing the self-supervised training loss with an untargeted adversarial attack often results in generating ineffective adversaries that may not help improve the robustness of the trained model, especially for non-contrastive SSL frameworks without negative examples. To tackle this problem, we propose a novel positive mining for targeted adversarial attack to generate effective adversaries for adversarial SSL frameworks. Specifically, we introduce an algorithm that selects the most confusing yet similar target example for a given instance based on entropy and similarity, and subsequently perturbs the given instance towards the selected target. Our method demonstrates significant enhancements in robustness when applied to non-contrastive SSL frameworks, and less but consistent robustness improvements with contrastive SSL frameworks, on the benchmark datasets.","['Adversarial self supervised learning', 'targeted attack', 'self supervised learning', 'contrastive learning', 'positive mining']",[],"['Minseon Kim', 'Hyeonjeong Ha', 'Sung Ju Hwang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72958,Privacy & Data Governance,Gaussian Membership Inference Privacy,"We propose a novel and practical privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model.  Consequently, $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). In particular, we derive a parametric family of $f$-MIP guarantees that we refer to as $\mu$-Gaussian Membership Inference Privacy ($\mu$-GMIP) by theoretically analyzing likelihood ratio-based membership inference attacks on stochastic gradient descent (SGD). Our analysis highlights that models trained with standard SGD already offer an elementary level of MIP.  Additionally, we show how $f$-MIP can be amplified by adding noise to gradient updates. Our analysis further yields an analytical membership inference attack that offers two distinct advantages over previous approaches. First, unlike existing state-of-the-art attacks that require training hundreds of shadow models, our attack does not require any shadow model.  Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, we quantify how various hyperparameters (e.g., batch size, number of model parameters) and specific data characteristics determine an attacker's ability to accurately infer a point's membership in the training set. We demonstrate the effectiveness of our method on models trained on vision and tabular datasets.","['Privacy', 'Membership Inference Attacks']",[],"['Tobias Leemann', 'Martin Pawelczyk', 'Gjergji Kasneci']","['Technische Universität München', 'Harvard University', 'Technische Universität München']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73563,Privacy & Data Governance,VisAlign: Dataset for Measuring the Alignment between AI and Humans in Visual Perception,"AI alignment refers to models acting towards human-intended goals, preferences, or ethical principles. Analyzing the similarity between models and humans can be a proxy measure for ensuring AI safety. In this paper, we focus on the models' visual perception alignment with humans, further referred to as AI-human visual alignment. Specifically, we propose a new dataset for measuring AI-human visual alignment in terms of image classification. In order to evaluate AI-human visual alignment, a dataset should encompass samples with various scenarios and have gold human perception labels. Our dataset consists of three groups of samples, namely Must-Act (i.e., Must-Classify), Must-Abstain, and Uncertain, based on the quantity and clarity of visual information in an image and further divided into eight categories. All samples have a gold human perception label; even Uncertain (e.g., severely blurry) sample labels were obtained via crowd-sourcing. The validity of our dataset is verified by sampling theory, statistical theories related to survey design, and experts in the related fields. Using our dataset, we analyze the visual alignment and reliability of five popular visual perception models and seven abstention methods. Our code and data is available at https://github.com/jiyounglee-0523/VisAlign.","['visual perception', 'visual perception alignment', 'reliability']",[],"['Jiyoung Lee', 'Seungho Kim', 'Seunghyun Won', 'Joonseok Lee', 'Marzyeh Ghassemi', 'James Thorne', 'Jaeseok Choi', 'Edward Choi']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Seoul National University Bundang Hospital', 'Seoul National University', 'Massachusetts Institute of Technology', 'KAIST', 'Kangwon National University', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72683,Privacy & Data Governance,ExPT: Synthetic Pretraining for Few-Shot Experimental Design,"Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at https://github.com/tung-nd/ExPT.git.","['experimental design', 'few-shot', 'black-box optimization', 'synthetic pretraining', 'in-context learning', 'transformer']",[],"['Tung Nguyen', 'Sudhanshu Agrawal', 'Aditya Grover']","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72701,Privacy & Data Governance,Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information,"Privacy-preserving instance encoding aims to encode raw data into feature vectors without revealing their privacy-sensitive information. When designed properly, these encodings can be used for downstream ML applications such as training and inference with limited privacy risk. However, the vast majority of existing schemes do not theoretically justify that their encoding is non-invertible, and their privacy-enhancing properties are only validated empirically against a limited set of attacks. In this paper, we propose a theoretically-principled measure for the invertibility of instance encoding based on Fisher information that is broadly applicable to a wide range of popular encoders. We show that dFIL can be used to bound the invertibility of encodings both theoretically and empirically, providing an intuitive interpretation of the privacy of instance encoding.","['privacy', 'instance encoding', 'split learning']",[],"['Kiwan Maeng', 'Chuan Guo', 'Sanjay Kariyappa', 'G. Edward Suh']","['Pennsylvania State University', 'Facebook AI Research', 'J.P. Morgan Chase', 'Meta AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73007,Privacy & Data Governance,Exact Verification of ReLU Neural Control Barrier Functions,"Control Barrier Functions (CBFs) are a popular approach for safe control of nonlinear systems. In CBF-based control, the desired safety properties of the system are mapped to nonnegativity of a CBF, and the control input is chosen to ensure that the CBF remains nonnegative for all time. Recently, machine learning methods that represent CBFs as neural networks (neural control barrier functions, or NCBFs) have shown great promise due to the universal representability of neural networks. However, verifying that a learned CBF guarantees safety remains a challenging research problem. This paper presents novel exact conditions and algorithms for verifying safety of feedforward NCBFs with ReLU activation functions. The key challenge in doing so is that, due to the piecewise linearity of the ReLU function, the NCBF will be nondifferentiable at certain points, thus invalidating traditional safety verification methods that assume a smooth barrier function. We resolve this issue by leveraging a generalization of Nagumo's theorem for proving invariance of sets with nonsmooth boundaries to derive necessary and sufficient conditions for safety. Based on this condition, we propose an algorithm for safety verification of NCBFs that first decomposes the NCBF into piecewise linear segments and then solves a nonlinear program to verify safety of each segment as well as the intersections of the linear segments. We  mitigate the complexity by only considering the boundary of the safe region and by pruning  the segments with Interval Bound Propagation (IBP) and linear relaxation. We evaluate our approach through numerical studies with comparison to state-of-the-art SMT-based methods. Our code is available at https://github.com/HongchaoZhang-HZ/exactverif-reluncbf-nips23.","['Safety', 'Neural Barrier Function', 'Verification']",[],"['Hongchao Zhang', 'Junlin Wu', 'Yevgeniy Vorobeychik', 'Andrew Clark']","['Washington University, Saint Louis', 'Washington University, Saint Louis', 'Washington University, St. Louis', 'Washington University, Saint Louis']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73425,Privacy & Data Governance,How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources,"In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, safety, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tülu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources. Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tülu, along with our code, data, and evaluation framework to facilitate future research.","['Instruction tuning', 'large language models', 'open-source', 'systematic evaluation']",[],"['Yizhong Wang', 'Hamish Ivison', 'Pradeep Dasigi', 'Jack Hessel', 'Tushar Khot', 'David Wadden', 'Kelsey MacMillan', 'Noah A. Smith', 'Iz Beltagy', 'Hannaneh Hajishirzi']","['Department of Computer Science, University of Washington', 'University of Washington', 'Allen Institute for Artificial Intelligence', 'Samaya AI', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'University of Washington', 'Allen Institute for Artificial Intelligence', 'University of Washington']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73444,Privacy & Data Governance,NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding,"The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert-level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing procedure activity understanding. NurViD consists of over 1.5k videos totaling 144 hours, making it approximately four times longer than the existing largest nursing activity datasets. Notably, it encompasses 51 distinct nursing procedures and 177 action steps, providing a much more comprehensive coverage compared to existing datasets that primarily focus on limited procedures. To evaluate the efficacy of current deep learning methods on nursing activity understanding, we establish three benchmarks on NurViD: procedure recognition on untrimmed videos, procedure and action recognition on trimmed videos, and action detection. Our benchmark and code will be available at https://github.com/minghu0830/NurViD-benchmark.",['Activity understanding; Action recognition; Nursing procedure; Standard level grading'],[],"['Ming Hu', 'Lin Wang', 'Siyuan Yan', 'Don Ma', 'Qingli Ren', 'Peng Xia', 'Wei Feng', 'Peibo Duan', 'Lie Ju', 'Zongyuan Ge']","['Monash University', 'Harbin Engineering University', 'Monash University', 'University of Wisconsin - Madison', 'Shanxi Medical University', 'Monash University', 'Monash University', 'Monash University', 'Monash University', 'Monash University']","[None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71780,Privacy & Data Governance,Temporal Robustness against Data poisoning,"Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples when the attacks are temporally bounded. We present a benchmark with an evaluation protocol simulating continuous data collection and periodic deployments of updated models, thus enabling empirical evaluation of temporal robustness. Lastly, we develop and also empirically verify a baseline defense, namely temporal aggregation, offering provable temporal robustness and highlighting the potential of our temporal threat model for data poisoning.","['Robustness', 'Data Poisoning', 'Security', 'Machine Learning', 'Backdoor', 'Adversarial']",[],"['Wenxiao Wang', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71514,Privacy & Data Governance,Provably Safe Reinforcement Learning with Step-wise Violation Constraints,"We investigate a novel safe reinforcement learning problem with step-wise violation constraints. Our problem differs from existing works in that we focus on stricter step-wise violation constraints and do not assume the existence of safe actions, making our formulation more suitable for safety-critical applications that need to ensure safety in all decision steps but may not always possess safe actions, e.g., robot control and autonomous driving. We propose an efficient algorithm SUCBVI, which guarantees $\widetilde{\mathcal{O}}(\sqrt{ST})$ or gap-dependent $\widetilde{\mathcal{O}}(S/\mathcal{C}_{\mathrm{gap}} + S^2AH^2)$ step-wise violation and $\widetilde{\mathcal{O}}(\sqrt{H^3SAT})$ regret. Lower bounds are provided to validate the optimality in both violation and  regret performance with respect to the number of states $S$ and the total number of steps $T$. Moreover, we further study an innovative safe reward-free exploration problem with step-wise violation constraints. For this problem, we design algorithm SRF-UCRL to find a near-optimal safe policy, which achieves nearly state-of-the-art  sample complexity $\widetilde{\mathcal{O}}((\frac{S^2AH^2}{\varepsilon}+\frac{H^4SA}{\varepsilon^2})(\log(\frac{1}{\delta})+S))$, and guarantees $\widetilde{\mathcal{O}}(\sqrt{ST})$ violation during exploration.  Experimental results demonstrate the  superiority of our algorithms in safety performance and corroborate our theoretical results.","['safe reinforcement learning', 'step-wise violation', 'reinforcement learning theory']",[],"['Nuoya Xiong', 'Yihan Du', 'Longbo Huang']","['Tsinghua University, Tsinghua University', 'University of Illinois at Urbana-Champaign', 'Tsinghua University, Tsinghua University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71533,Privacy & Data Governance,Tools for Verifying Neural Models' Training Data,"It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a ""Proof-of-Training-Data"": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature.","['Large Scale Learning', 'ML Security', 'AI Governance']",[],"['Dami Choi', 'Yonadav G Shavit', 'David Duvenaud']","['Department of Computer Science, University of Toronto', 'Harvard University', 'Department of Computer Science, University of Toronto']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71163,Privacy & Data Governance,Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack,"We study design of black-box model extraction attacks that can *send minimal number of queries from* a *publicly available dataset* to a target ML model through a predictive API with an aim *to create an informative and distributionally equivalent replica* of the target. First, we define *distributionally equivalent* and *Max-Information model extraction* attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to *an active sampling-based query selection algorithm*, Marich, which is *model-oblivious*. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\sim 60-95\%$ of true model's accuracy and uses $\sim 1,000 - 8,500$ queries from the publicly available datasets, which are different from the private training datasets. Models extracted by Marich yield prediction distributions, which are $\sim2-4\times$ closer to the target's distribution in comparison to the existing active sampling-based attacks. The extracted models also lead to 84-96$\%$ accuracy under membership inference attacks. Experimental results validate that Marich is *query-efficient*, and capable of performing task-accurate, high-fidelity, and informative model extraction.","['Differential Privacy', 'Model Extraction Attacks', 'Active Sampling', 'Max-Information Attack']",[],"['Pratik Karmakar', 'Debabrota Basu']","['National University of', 'INRIA']","['Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/72584,Privacy & Data Governance,Django: Detecting Trojans in Object Detection Models via Gaussian Focus Calibration,"Object detection models are vulnerable to backdoor or trojan attacks, where an attacker can inject malicious triggers into the model, leading to altered behavior during inference. As a defense mechanism, trigger inversion leverages optimization to reverse-engineer triggers and identify compromised models. While existing trigger inversion methods assume that each instance from the support set is equally affected by the injected trigger, we observe that the poison effect can vary significantly across bounding boxes in object detection models due to its dense prediction nature, leading to an undesired optimization objective misalignment issue for existing trigger reverse-engineering methods. To address this challenge, we propose the first object detection backdoor detection framework Django (Detecting Trojans in Object Detection Models via Gaussian Focus Calibration). It leverages a dynamic Gaussian weighting scheme that prioritizes more vulnerable victim boxes and assigns appropriate coefficients to calibrate the optimization objective during trigger inversion. In addition, we combine Django with a novel label proposal pre-processing technique to enhance its efficiency. We evaluate Django on 3 object detection image datasets, 3 model architectures, and 2 types of attacks, with a total of 168 models. Our experimental results show that Django outperforms 6 state-of-the-art baselines, with up to 38% accuracy improvement and 10x reduced overhead. The code is available at https://github.com/PurduePAML/DJGO.",['backdoor detection; object detection;'],[],"['Guangyu Shen', 'Siyuan Cheng', 'Guanhong Tao', 'Kaiyuan Zhang', 'Yingqi Liu', 'Shengwei An', 'Xiangyu Zhang']","['Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', 'Microsoft', 'Purdue University', ', Purdue University']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71613,Privacy & Data Governance,Adversarial Robustness through Random Weight Sampling,"Deep neural networks have been found to be vulnerable in a variety of tasks. Adversarial attacks can manipulate network outputs, resulting in incorrect predictions. Adversarial defense methods aim to improve the adversarial robustness of networks by countering potential attacks. In addition to traditional defense approaches, randomized defense mechanisms have recently received increasing attention from researchers. These methods introduce different types of perturbations during the inference phase to destabilize adversarial attacks. Although promising empirical results have been demonstrated by these approaches, the defense performance is quite sensitive to the randomness parameters, which are always manually tuned without further analysis. On the contrary, we propose incorporating random weights into the optimization to fully exploit the potential of randomized defense. To perform better optimization of randomness parameters, we conduct a theoretical analysis of the connections between randomness parameters and gradient similarity as well as natural performance. From these two aspects, we suggest imposing theoretically-guided constraints on random weights during optimizations, as these weights play a critical role in balancing natural performance and adversarial robustness. We derive both the upper and lower bounds of random weight parameters by considering prediction bias and gradient similarity. In this study, we introduce the Constrained Trainable Random Weight (CTRW), which adds random weight parameters to the optimization and includes a constraint guided by the upper and lower bounds to achieve better trade-offs between natural and robust accuracy. We evaluate the effectiveness of CTRW on several datasets and benchmark convolutional neural networks. Our results indicate that our model achieves a robust accuracy approximately 16% to 17% higher than the baseline model under PGD-20 and 22% to 25% higher on Auto Attack.",['Adversarial robustness; Randomized defense; Random parameters optimization'],[],"['Yanxiang Ma', 'Minjing Dong', 'Chang Xu']","['University of Sydney', 'City University of', 'University of Sydney']","[None, 'Hong Kong', None]",,,,,,,
https://nips.cc/virtual/2023/poster/72856,Privacy & Data Governance,Cal-DETR: Calibrated Detection Transformer,"Albeit revealing impressive predictive performance for several computer vision tasks, deep neural networks (DNNs) are prone to making overconfident predictions. This limits the adoption and wider utilization of DNNs in many safety-critical applications. There have been recent efforts toward calibrating DNNs, however, almost all of them focus on the classification task. Surprisingly, very little attention has been devoted to calibrating modern DNN-based object detectors, especially detection transformers, which have recently demonstrated promising detection performance and are influential in many decision-making systems. In this work, we address the problem by proposing a mechanism for calibrated detection transformers (Cal-DETR), particularly for Deformable-DETR, UP-DETR, and DINO. We pursue the train-time calibration route and make the following contributions. First, we propose a simple yet effective approach for quantifying uncertainty in transformer-based object detectors. Second, we develop an uncertainty-guided logit modulation mechanism that leverages the uncertainty to modulate the class logits. Third, we develop a logit mixing approach that acts as a regularizer with detection-specific losses and is also complementary to the uncertainty-guided logit modulation technique to further improve the calibration performance. Lastly, we conduct extensive experiments across three in-domain and four out-domain scenarios. Results corroborate the effectiveness of Cal-DETR against the competing train-time methods in calibrating both in-domain and out-domain detections while maintaining or even improving the detection performance. Our codebase and pre-trained models can be accessed at \url{https://github.com/akhtarvision/cal-detr}.","['Model Calibration', 'Object Detection', 'Detection Transformers', 'Uncertainty']",[],"['Muhammad Akhtar Munir', 'Salman Khan', 'Muhammad Haris Khan', 'Mohsen Ali', 'Fahad Khan']","['Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence', 'MBZUAI', 'Information Technology University', 'Inception Institute of Artificial Intelligence']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72503,Privacy & Data Governance,CamoPatch: An Evolutionary Strategy for Generating Camoflauged Adversarial Patches,"Deep neural networks (DNNs) have demonstrated vulnerabilities to adversarial examples, which raises concerns about their reliability in safety-critical applications. While the majority of existing methods generate adversarial examples by making small modifications to the entire image, recent research has proposed a practical alternative known as adversarial patches. Adversarial patches have shown to be highly effective in causing DNNs to misclassify by distorting a localized area (patch) of the image. However, existing methods often produce clearly visible distortions since they do not consider the visibility of the patch. To address this, we propose a novel method for constructing adversarial patches that approximates the appearance of the area it covers. We achieve this by using a set of semi-transparent, RGB-valued circles, drawing inspiration from the computational art community. We utilize an evolutionary strategy to optimize the properties of each shape, and employ a simulated annealing approach to optimize the patch's location. Our approach achieves better or comparable performance to state-of-the-art methods on ImageNet DNN classifiers while achieving a lower $l_2$ distance from the original image. By minimizing the visibility of the patch, this work further highlights the vulnerabilities of DNNs to adversarial patches.","['Evolutionary Strategy', 'Adversarial Attack', 'Adversarial Patches', 'Computational Art', 'Computer Vision']",[],"['Phoenix Neale Williams', 'Ke Li']","['University of Exeter', 'University of Exeter']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72545,Privacy & Data Governance,Boosting Adversarial Transferability by Achieving Flat Local Maxima,"Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the observation that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt a first-order procedure to approximate the curvature of the second-order Hessian matrix, which makes computing more efficient by interpolating two Jacobian matrices. Meanwhile, in order to obtain a more stable gradient direction, we randomly sample multiple examples and average the gradients of these examples to reduce the variance due to random sampling during the iterative process. Extensive experimental results on the ImageNet-compatible dataset show that the proposed method can generate adversarial examples at flat local regions, and significantly improve the adversarial transferability on either normally trained models or adversarially trained models than the state-of-the-art attacks. Our codes are available at: https://github.com/Trustworthy-AI-Group/PGN.","['Adversarial attack', 'Adversarial transferability', 'Black-box Attack']",[],"['Zhijin Ge', 'Hongying Liu', 'Xiaosen Wang', 'Fanhua Shang', 'Yuanyuan Liu']","[""Xi'an University of Electronic Science and Technology"", 'Tianjin University', 'Huawei Technologies Ltd.', 'Tianjin University', 'Xidian University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72646,Privacy & Data Governance,Learning Shared Safety Constraints from Multi-task Demonstrations,"Regardless of the particular task we want to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task setting to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks.","['constraints', 'inverse reinforcement learning', 'safe reinforcement learning']",[],"['Konwoo Kim', 'Gokul Swamy', 'Zuxin Liu', 'Ding Zhao', 'Sanjiban Choudhury', 'Steven Wu']","['School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Cornell University', 'Carnegie Mellon University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73079,Privacy & Data Governance,Robustness Guarantees for Adversarially Trained Neural Networks,"We study robust adversarial training of two-layer neural networks as a bi-level optimization problem. In particular, for the inner loop that implements the adversarial attack during training using projected gradient descent (PGD), we propose maximizing a \emph{lower bound} on the $0/1$-loss by reflecting a surrogate loss about the origin. This allows us to give a convergence guarantee for the inner-loop PGD attack. Furthermore, assuming the data is linearly separable, we provide precise iteration complexity results for end-to-end adversarial training, which holds for any width and initialization. We provide empirical evidence to support our theoretical results.","['Adversarial training', 'neural networks', 'robustness', 'guarantees']",[],"['Poorya Mianjy', 'Raman Arora']","['Citadel Securities', 'Johns Hopkins University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71224,Privacy & Data Governance,TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models,"Large Language Models (LLMs) are progressively being utilized as machine learning services and interface tools for various applications. However, the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined. In this paper, we propose TrojLLM, an automatic and black-box framework to effectively generate universal and stealthy triggers. When these triggers are incorporated into the input data, the LLMs' outputs can be maliciously manipulated.   Moreover, the framework also supports embedding Trojans within discrete prompts, enhancing the overall effectiveness and precision of the triggers' attacks.  Specifically, we propose a trigger discovery algorithm for generating universal triggers for various inputs by querying victim LLM-based APIs using few-shot data samples. Furthermore, we introduce a novel progressive Trojan poisoning algorithm designed to generate poisoned prompts that retain efficacy and transferability across a diverse range of models. Our experiments and results demonstrate TrojLLM's capacity to effectively insert Trojans into text prompts in real-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining exceptional performance on clean test sets. Our work sheds light on the potential security risks in current models and offers a potential defensive approach. The source code of TrojLLM is available at https://github.com/UCF-ML-Research/TrojLLM.","['Large Language Model', 'Trojan Attack', 'Adversary Attack', 'Prompt Injection', 'GPT-4', 'Black-box']",[],"['Jiaqi Xue', 'Mengxin Zheng', 'Ting Hua', 'Yilin Shen', 'Ladislau Bölöni', 'Qian Lou']","['University of Central Florida', 'na University, Bloomington', 'Samsung', 'Samsung Research America', 'University of Central Florida', 'University of Central Florida']","[None, 'India', None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72788,Privacy & Data Governance,LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference,"The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The  deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a model with minimized multiplication depth. (2) A compact node-wise polynomial replacement policy with a second-order trainable activation function, steered towards superior convergence by a two-level distillation approach from an all-ReLU based teacher model. (3) an enhanced HE solution that enables finer-grained operator fusion for node-wise activation functions, further reducing multiplication level consumption in HE-based inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that LinGCN excels in latency, accuracy, and scalability for homomorphically encrypted inference, outperforming solutions such as CryptoGCN. Remarkably, LinGCN achieves a 14.2× latency speedup relative to CryptoGCN, while preserving an inference accuracy of ~75\% and notably reducing multiplication depth. Additionally, LinGCN proves scalable for larger models, delivering a substantial 85.78\% accuracy with 6371s latency, a 10.47\% accuracy improvement over CryptoGCN.","['Privacy-Preserving Machine Learning', 'efficient private inference', 'machine learning as a service', 'homomorphic encryption', 'non-linear pruning', 'ST-GCN']",[],"['Hongwu Peng', 'Ran Ran', 'Yukui Luo', 'Jiahui Zhao', 'Shaoyi Huang', 'Kiran Thorat', 'Tong Geng', 'Chenghong Wang', 'Wujie Wen', 'Caiwen Ding']","['University of Connecticut', 'North Carolina State University', 'University of Massachusetts at Dartmouth', 'University of Connecticut', 'University of Connecticut', 'University of Connecticut', 'University of Rochester', 'na University', 'North Carolina State University', 'University of Connecticut']","[None, None, None, None, None, None, None, 'India', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70628,Privacy & Data Governance,SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data,"The problem of urban event ranking aims at predicting the top-$k$ most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. Due to the common assumption that items are independent. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named SpatialRank. SpatialRank features  adaptive graph convolution layers that dynamically learn the spatiotemporal dependencies across locations from data. In addition, the model optimizes through surrogates a hybrid NDCG loss with a spatial component to better rank neighboring spatial locations. We design an importance-sampling with a spatial filtering algorithm to effectively evaluate the loss during training. Comprehensive experiments on three real-world datasets demonstrate that SpatialRank can effectively identify the top riskiest locations of crimes and traffic accidents and outperform state-of-art methods in terms of NDCG by up to 12.7%.","['urban event', 'NDCG optimization', 'ranking', 'traffic accident', 'crime', 'spatiotemporal data']",[],"['BANG AN', 'Xun Zhou', 'Yongjian Zhong', 'Tianbao Yang']","['University of Iowa', 'Harbin Institute of Technology (Shenzhen)', 'University of Iowa', 'University of Iowa']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69944,Privacy & Data Governance,PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization,"Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a different perspective on understanding robust generalization: The mismatch terms between standard and robust generalization bounds shown in previous studies do not contribute to the poor robust generalization. Instead, these disparities solely due to mathematical issues. Finally, we extend the main result to adversarial robustness against general non-$\ell_p$ attacks and other neural network architectures.","['Pac-Bayes', 'Adversarial Robustness', 'Generalization']",[],"['Jiancong Xiao', 'Ruoyu Sun', 'Zhi-Quan Luo']","['University of Pennsylvania', 'University of Illinois, Urbana-Champaign', 'The Chinese University of , Shenzhen']","[None, None, 'Hong Kong']",,,,,,,
https://nips.cc/virtual/2023/poster/71059,Privacy & Data Governance,Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?,"Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used to evaluate model privacy risk under reconstruction attacks. Under these metrics, reconstructed images that are determined to resemble the original one generally indicate more privacy leakage. Images determined as overall dissimilar, on the other hand, indicate higher robustness against attack. However, there is no guarantee that these metrics well reflect human opinions, which offers trustworthy judgement for model privacy leakage.  In this paper, we comprehensively study the faithfulness of these hand-crafted metrics to human perception of privacy information from the reconstructed images. On 5 datasets ranging from natural images, faces, to fine-grained classes, we use 4 existing attack methods to reconstruct images from many different classification models and, for each reconstructed image, we ask multiple human annotators to assess whether this image is recognizable. Our studies reveal that the hand-crafted metrics only have a weak correlation with the human evaluation of privacy leakage and that even these metrics themselves often contradict each other. These observations suggest risks of current metrics  in the community. To address this potential risk, we propose a learning-based measure called SemSim to evaluate the Semantic Similarity between the original and reconstructed images. SemSim is trained with a standard triplet loss, using an original image as an anchor, one of its recognizable reconstructed images as a positive sample, and an unrecognizable one as a negative. By training on human annotations, SemSim exhibits a greater reflection of privacy leakage on the semantic level. We show that SemSim has a significantly higher correlation with human judgment compared with existing metrics. Moreover, this strong correlation generalizes to unseen datasets, models and attack methods. We envision this work as a milestone for image quality evaluation closer to the human level. The project webpage can be accessed at https://sites.google.com/view/semsim.","['Privacy Assessment', 'Reconstructed Images', 'Evaluation Metrics', 'Human Perception']",[],"['Xiaoxiao Sun', 'Nidham Gazagnadou', 'Vivek Sharma', 'Lingjuan Lyu', 'Hongdong Li', 'Liang Zheng']","['n National University', 'Sony AI', 'Massachusetts General Hospital, Harvard University', 'Sony Research', 'n National University', 'n National University']","['Australia', None, None, None, 'Australia', 'Australia']",,,,,,,
https://nips.cc/virtual/2023/poster/72711,Privacy & Data Governance,Bounding training data reconstruction in DP-SGD,"Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against a training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this question further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Finally, we use our methods to demonstrate that different settings of the DP-SGD parameters leading to same DP guarantees can results in significantly different success rates for reconstruction, indicating that the DP guarantee alone might not be a good proxy for controlling the protection against reconstruction attacks.","['Differential privacy', 'reconstruction']",[],"['Jamie Hayes', 'Borja Balle', 'Saeed Mahloujifar']","['DeepMind', 'DeepMind', 'Meta']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73022,Privacy & Data Governance,Blurred-Dilated Method for Adversarial Attacks,"Deep neural networks (DNNs) are vulnerable to adversarial attacks, which lead to incorrect predictions. In black-box settings, transfer attacks can be conveniently used to generate adversarial examples. However, such examples tend to overfit the specific architecture and feature representations of the source model, resulting in poor attack performance against other target models. To overcome this drawback, we propose a novel model modification-based transfer attack: Blurred-Dilated method (BD) in this paper. In summary, BD works by reducing downsampling while introducing BlurPool and dilated convolutions in the source model. Then BD employs the modified source model to generate adversarial samples. We think that BD can more comprehensively preserve the feature information than the original source model. It thus enables more thorough destruction of the image features, which can improve the transferability of the generated adversarial samples. Extensive experiments on the ImageNet dataset show that adversarial examples generated by BD achieve significantly higher transferability than the state-of-the-art baselines. Besides, BD can be conveniently combined with existing black-box attack techniques to further improve their performance.",['Transferable adversarial example'],[],"['Yang Deng', 'Weibin Wu', 'Jianping Zhang', 'Zibin Zheng']","['SUN YAT-SEN UNIVERSITY', 'Sun Yat-sen University', 'The Chinese University of', 'SUN YAT-SEN UNIVERSITY']","[None, None, 'Hong Kong', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71142,Privacy & Data Governance,Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents,"Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To alleviate this bias, a reset method has been proposed, which involves periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of this method can result in performance collapses after executing the reset, raising concerns from the perspective of safe RL and regret minimization. In this paper, we propose a novel reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The effectiveness of the proposed method is validated through various experiments including those in the domain of safe RL. Numerical results demonstrate its potential for real-world applications requiring high sample efficiency and safety considerations.","['deep reinforcement learning', 'primacy bais', 'reset', 'deep ensemble learning']",[],"['Woojun Kim', 'Yongjae Shin', 'Jongeui Park', 'Youngchul Sung']","['Carnegie Mellon University', 'KAIST', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70495,Privacy & Data Governance,Uncertainty Quantification via Neural Posterior Principal Components,"Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. Yet, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap around a pre-trained model that was trained to minimize the mean square error (MSE), or can be trained from scratch to output both a predicted image and the posterior PCs. We showcase our method on multiple inverse problems in imaging, including denoising, inpainting, super-resolution, and biological image-to-image translation. Our method reliably conveys instance-adaptive uncertainty directions, achieving uncertainty quantification comparable with posterior samplers while being orders of magnitude faster. Code and examples are available on our [webpage](https://eliasnehme.github.io/NPPC/).","['Uncertainty Quantification', 'Inverse Problems', 'Probabilistic Modelling', 'Principal Components Analysis', 'Deep Learning']",[],"['Elias Nehme', 'Omer Yair', 'Tomer Michaeli']","['Electrical Engineering Department, Technion –  Institute of Technology, Technion -  Institute of Technology', 'Technion, Technion', 'Technion, Technion']","['Israel', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73701,Privacy & Data Governance,"Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses","To generate evidence regarding the safety and efficacy of artificial intelligence (AI) enabled medical devices, AI models need to be evaluated on a diverse population of patient cases, some of which may not be readily available. We propose an evaluation approach for testing medical imaging AI models that relies on in silico imaging pipelines in which stochastic digital models of human anatomy (in object space) with and without pathology are imaged using a digital replica imaging acquisition system to generate realistic synthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with four breast fibroglandular density distributions imaged at different exposure levels using Monte Carlo x-ray simulations with the publicly available Virtual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize the synthetic dataset to analyze AI model performance and find that model performance decreases with increasing breast density and increases with higher mass density, as expected. As exposure levels decrease, AI model performance drops with the highest performance achieved at exposure levels lower than the nominal recommended dose for the breast type.",['synthetic data; medical imaging; AI testing; digital twins'],[],"['Elena Sizikova', 'Niloufar Saharkhiz', 'Diksha Sharma', 'Miguel Lago', 'Berkman Sahiner', 'Jana Gut Delfino', 'Aldo Badano']","['Food and Drug Administration', 'Food and Drug Administration', 'FDA', 'Universidad Politécnica de Valencia', 'Food and Drug Administration', 'Institute of Technology', 'Food and Drug Administration']","[None, None, None, None, 'US', 'Georgia', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71708,Privacy & Data Governance,Adversarial Self-Training Improves Robustness and Generalization for Gradual Domain Adaptation,"Gradual Domain Adaptation (GDA), in which the learner is provided with additional intermediate domains, has been theoretically and empirically studied in many contexts. Despite its vital role in security-critical scenarios, the adversarial robustness of the GDA model remains unexplored. In this paper, we adopt the effective gradual self-training method and replace vanilla self-training with adversarial self-training (AST). AST first predicts labels on the unlabeled data and then adversarially trains the model on the pseudo-labeled distribution. Intriguingly, we find that gradual AST improves not only adversarial accuracy but also clean accuracy on the target domain. We reveal that this is because adversarial training (AT) performs better than standard training when the pseudo-labels contain a portion of incorrect labels. Accordingly, we first present the generalization error bounds for gradual AST in a multiclass classification setting. We then use the optimal value of the Subset Sum Problem to bridge the standard error on a real distribution and the adversarial error on a pseudo-labeled distribution. The result indicates that AT may obtain a tighter bound than standard training on data with incorrect pseudo-labels. We further present an example of a conditional Gaussian distribution to provide more insights into why gradual AST can improve the clean accuracy for GDA.",['learning theory'],[],['Weiwei Liu'],['Wuhan University'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/72072,Privacy & Data Governance,Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents,"Recent progress in large language models (LLMs) has demonstrated the ability to learn and leverage Internet-scale knowledge through pre-training with autoregressive models. Unfortunately, applying such models to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. On the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Thus, if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realizable according to grounded models of the environment. We frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. We demonstrate how such grounded models can be obtained across three simulation and real-world domains, and that the proposed decoding strategy is able to solve complex, long-horizon embodiment tasks in a robotic setting by leveraging the knowledge of both models.","['robotics', 'language models', 'embodied agents']",[],"['Wenlong Huang', 'Fei Xia', 'Dhruv Shah', 'Danny Driess', 'Andy Zeng', 'Yao Lu', 'Pete Florence', 'Igor Mordatch', 'Sergey Levine', 'Karol Hausman', 'brian ichter']","['Stanford University', 'Google', 'UC Berkeley', 'TU Berlin', 'Google', 'Google', 'Google', 'Research, Google', 'Google', 'Stanford University', 'Google']","[None, None, None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72574,Privacy & Data Governance,Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning,"Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of the manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents’ algorithms.",['adversarial attacks; multi agent reinforcement learning;'],[],"['Guanlin Liu', 'Lifeng Lai']","['University of California, Davis', 'University of California, Davis']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72155,Privacy & Data Governance,Diffusion-Based Probabilistic Uncertainty Estimation for Active Domain Adaptation,"Active Domain Adaptation (ADA) has emerged as an attractive technique for assisting domain adaptation by actively annotating a small subset of target samples. Most ADA methods focus on measuring the target representativeness beyond traditional active learning criteria to handle the domain shift problem, while leaving the uncertainty estimation to be performed by an uncalibrated deterministic model. In this work, we introduce a probabilistic framework that captures both data-level and prediction-level uncertainties beyond a point estimate. Specifically, we use variational inference to approximate the joint posterior distribution of latent representation and model prediction. The variational objective of labeled data can be formulated by a variational autoencoder and a latent diffusion classifier, and the objective of unlabeled data can be implemented in a knowledge distillation framework. We utilize adversarial learning to ensure an invariant latent space. The resulting diffusion classifier enables efficient sampling of all possible predictions for each individual to recover the predictive distribution. We then leverage a t-test-based criterion upon the sampling and select informative unlabeled target samples based on the p-value, which encodes both prediction variability and cross-category ambiguity. Experiments on both ADA and Source-Free ADA settings show that our method provides more calibrated predictions than previous ADA methods and achieves favorable performance on three domain adaptation datasets.","['diffusion-based models', 'active learning', 'domain adaptation', 'source-free domain adaptation', 'uncertainty estimation']",[],"['Zhekai Du', 'Jingjing Li']","['University of Electronic Science and Technology of', 'University of Electronic Science and Technology of']","['China', 'China']",,,,,,,
https://nips.cc/virtual/2023/poster/72122,Privacy & Data Governance,HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text,"Black-box hard-label adversarial attack on text is a practical and challenging task, as the text data space is inherently discrete and non-differentiable, and only the predicted label is accessible. Research on this problem is still in the embryonic stage and only a few methods are available. Nevertheless, existing methods rely on the complex heuristic algorithm or unreliable gradient estimation strategy, which probably fall into the local optimum and inevitably consume numerous queries, thus are difficult to craft satisfactory adversarial examples with high semantic similarity and low perturbation rate in a limited query budget. To alleviate above issues, we propose a simple yet effective framework to generate high quality textual adversarial examples under the black-box hard-label attack scenarios, named HQA-Attack. Specifically, after initializing an adversarial example randomly, HQA-attack first constantly substitutes original words back as many as possible, thus shrinking the perturbation rate. Then it leverages the synonym set of the remaining changed words to further optimize the adversarial example with the direction which can improve the semantic similarity and satisfy the adversarial condition simultaneously. In addition, during the optimizing procedure, it searches a transition synonym word for each changed word, thus avoiding traversing the whole synonym set and reducing the query number to some extent. Extensive experimental results on five text classification datasets, three natural language inference datasets and two real-world APIs have shown that the proposed HQA-Attack method outperforms other strong baselines significantly.","['High-quality adversarial example', 'Black-box hard-label textual adversarial attack']",[],"['Han Liu', 'Zhi Xu', 'Xiaotong Zhang', 'Fenglong Ma', 'Hongyang Chen', 'Xianchao Zhang']","['Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Pennsylvania State University', 'Zhejiang Lab,', 'Dalian University of Technology']","[None, None, None, None, 'China', None]",,,,,,,
https://nips.cc/virtual/2023/poster/70231,Privacy & Data Governance,QuadAttac$K$: A Quadratic Programming Approach to Learning Ordered Top-$K$ Adversarial Attacks,"The adversarial vulnerability of Deep Neural Networks (DNNs) has been well-known and widely concerned, often under the context of learning top-$1$ attacks (e.g., fooling a DNN to classify a cat image as dog). This paper shows that the concern is much more serious by learning significantly more aggressive ordered top-$K$ clear-box targeted attacks proposed in~\citep{zhang2020learning}. We propose a novel and rigorous quadratic programming (QP) method of learning ordered top-$K$ attacks with low computing cost, dubbed as \textbf{QuadAttac$K$}. Our QuadAttac$K$ directly solves the QP to satisfy the attack constraint in the feature embedding space (i.e., the input space to the final linear classifier), which thus exploits the semantics of the feature embedding space (i.e., the principle of class coherence). With the optimized feature embedding  vector perturbation, it then computes the adversarial perturbation in the data space via the vanilla one-step back-propagation. In experiments, the proposed QuadAttac$K$ is tested in the ImageNet-1k  classification using ResNet-50, DenseNet-121, and Vision Transformers (ViT-B and DEiT-S). It successfully pushes the boundary of successful ordered top-$K$ attacks from $K=10$ up to $K=20$ at a cheap budget ($1\times 60$) and further improves attack success rates for $K=5$ for all tested models, while retaining the performance for $K=1$.","['Ordered Top-K Clear-Box Targeted Adversarial Attack', 'Deep Neural Networks', 'Quadratic Programming', 'Robustness']",[],"['Thomas Paniagua', 'Ryan Grainger', 'Tianfu Wu']","['North Carolina State University', 'North Carolina State University', 'North Carolina State University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73588,Privacy & Data Governance,RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions,"Depth estimation from monocular images is pivotal for real-world visual perception systems. While current learning-based depth estimation models train and test on meticulously curated data, they often overlook out-of-distribution (OoD) situations. Yet, in practical settings -- especially safety-critical ones like autonomous driving -- common corruptions can arise. Addressing this oversight, we introduce a comprehensive robustness test suite, RoboDepth, encompassing 18 corruptions spanning three categories: i) weather and lighting conditions; ii) sensor failures and movement; and iii) data processing anomalies. We subsequently benchmark 42 depth estimation models across indoor and outdoor scenes to assess their resilience to these corruptions. Our findings underscore that, in the absence of a dedicated robustness evaluation framework, many leading depth estimation models may be susceptible to typical corruptions. We delve into design considerations for crafting more robust depth estimation models, touching upon pre-training, augmentation, modality, model capacity, and learning paradigms. We anticipate our benchmark will establish a foundational platform for advancing robust OoD depth estimation.","['benchmark', 'monocular depth estimation', 'common corruptions', 'robustness']",[],"['Lingdong Kong', 'Shaoyuan Xie', 'Hanjiang Hu', 'Lai Xing Ng', 'Benoit R Cottereau', 'Wei Tsang Ooi']","['National University of', 'University of California, Irvine', 'Carnegie Mellon University', 'Institute for Infocomm Research (I2R), A*STAR', 'CNRS', 'National University of']","['Singapore', None, None, None, None, 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/70708,Security,Resolving the Tug-of-War: A Separation of Communication and Learning in Federated Learning,"Federated learning (FL) is a promising privacy-preserving machine learning paradigm over distributed data. In this paradigm, each client trains the parameter of a model locally and the server aggregates the parameter from clients periodically. Therefore, we perform the learning and communication over the same set of parameters. However, we find that learning and communication have fundamentally divergent requirements for parameter selection, akin to two opposite teams in a tug-of-war game. To mitigate this discrepancy, we introduce FedSep, a novel two-layer federated learning framework. FedSep consists of separated communication and learning layers for each client and the two layers are connected through decode/encode operations. In particular, the decoding operation is formulated as a minimization problem. We view FedSep as a federated bilevel optimization problem and propose an efficient algorithm to solve it. Theoretically, we demonstrate that its convergence matches that of the standard FL algorithms. The separation of communication and learning in FedSep offers innovative solutions to various challenging problems in FL, such as Communication-Efficient FL and Heterogeneous-Model FL. Empirical validation shows the superior performance of FedSep over various baselines in these tasks.",['Federated Learning'],[],"['Junyi Li', 'Heng Huang']","['University of Maryland, College Park', 'University of Maryland, College Park']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71205,Security,Optimal Unbiased Randomizers for Regression with Label Differential Privacy,"We propose a new family of label randomizers for  training _regression_ models under the constraint of label differential privacy (DP). In particular, we leverage the trade-offs between bias and variance to construct better label randomizers depending on a privately estimated prior distribution over the labels. We demonstrate that these randomizers achieve state-of-the-art privacy-utility trade-offs on several datasets, highlighting the importance of reducing bias when training neural networks with label DP. We also provide theoretical results shedding light on the structural properties of the optimal unbiased randomizers.",['label differential privacy'],[],"['Ashwinkumar Badanidiyuru', 'Badih Ghazi', 'Pritish Kamath', 'Ravi Kumar', 'Ethan Jacob Leeman', 'Pasin Manurangsi', 'Avinash V Varadarajan', 'Chiyuan Zhang']","['Google', 'Google', 'Google Research', 'Google', 'Google', 'Google', 'Google', 'Google']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72902,Security,DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization,"Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best-performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize ""adaptive"" hyperparameter optimization methods such as Gaussian Process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for ""adaptive"" private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets.","['Differential Privacy', 'Hyperparameter Tuning', 'Deep Learning']",[],"['Hua Wang', 'Sheng Gao', 'Huanyu Zhang', 'Weijie J Su', 'Milan Shen']","['The Wharton School, University of Pennsylvania', 'The Wharton School, University of Pennsylvania', 'Meta', 'The Wharton School, University of Pennsylvania', 'Research, Facebook']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71321,Security,Minimax Risks and Optimal Procedures for Estimation under Functional Local Differential Privacy,"As concerns about data privacy continue to grow, differential privacy (DP) has emerged as a fundamental concept that aims to guarantee privacy by ensuring individuals' indistinguishability in data analysis. Local differential privacy (LDP) is a rigorous type of DP that requires individual data to be privatized before being sent to the collector, thus removing the need for a trusted third party to collect data. Among the numerous (L)DP-based approaches, functional DP has gained considerable attention in the DP community because it connects DP to statistical decision-making by formulating it as a hypothesis-testing problem and also exhibits Gaussian-related properties. However, the utility of privatized data is generally lower than that of non-private data, prompting research into optimal mechanisms that maximize the statistical utility for given privacy constraints. In this study, we investigate how functional LDP preserves the statistical utility by analyzing minimax risks of univariate mean estimation as well as nonparametric density estimation. We leverage the contraction property of functional LDP mechanisms and classical information-theoretical bounds to derive private minimax lower bounds. Our theoretical study reveals that it is possible to establish an interpretable, continuous balance between the statistical utility and privacy level, which has not been achieved under the $\epsilon$-LDP framework. Furthermore, we suggest minimax optimal mechanisms based on Gaussian LDP (a type of functional LDP) that achieve the minimax upper bounds and show via a numerical study that they are superior to the counterparts derived under $\epsilon$-LDP. The theoretical and empirical findings of this work suggest that Gaussian LDP should be considered a reliable standard for LDP.","['Data privacy', 'Functional local differential privacy', 'Gaussian mechanism', 'Minimax risks', 'Statistical utility']",[],"['Bonwoo Lee', 'Cheolwoo Park']","['Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71051,Security,On the Gini-impurity Preservation For Privacy Random Forests,"Random forests have been one successful ensemble algorithms in machine learning. Various techniques have been utilized to preserve the privacy of random forests from anonymization, differential privacy, homomorphic encryption, etc., whereas it rarely takes into account some crucial ingredients of learning algorithm. This work presents a new encryption to preserve data's Gini impurity, which plays a crucial role during the construction of random forests. Our basic idea is to modify the structure of binary search tree to store several examples in each node,  and encrypt data features by incorporating label and order information. Theoretically, we prove that our scheme preserves the minimum Gini impurity in ciphertexts without decrypting, and present the security guarantee for encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels  due to their importance and privacy. We  conduct extensive experiments to show the effectiveness, efficiency and security of our proposed method.","['classification', 'random forests', 'privacy-preserving machine learng', 'data encrytion']",[],"['XinRan Xie', 'Xuetong Bai', 'Wei Gao', 'Zhi-Hua Zhou']","['Nanjing University', 'nanjing university', 'Nanjing University', 'Nanjing University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71710,Security,An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions,"Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms. We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly run out of memory, even for relatively small datasets).","['differential privacy', 'marginals', 'matrix mechanism', 'scalability']",[],"['Yingtai Xiao', 'Guanlin He', 'Danfeng Zhang', 'Daniel Kifer']","['Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72485,Security,Fast Model DeBias with Machine Unlearning,"Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing method (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets demonstrate that our method achieves superior or competing classification accuracies compared with state-of-the-art retraining-based methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.","['Model Debias', 'Bias Mitigation', 'Machine Unlearning', 'Counterfactual Fairness']",[],"['Ruizhe Chen', 'Jianfei Yang', 'Huimin Xiong', 'Jianhong Bai', 'Tianxiang Hu', 'Jin Hao', 'YANG FENG', 'Joey Tianyi Zhou', 'Jian Wu', 'Zuozhu Liu']","['Zhejiang University', 'Nanyang Technological University', 'Zhejiang University', 'Zhejiang University', ' Zhejiang University-University of Illinois Urbana-Champaign Institute, Zhejiang University', 'Stanford University', 'Angelalign Tech.', 'National University of', 'Zhejiang University', 'Zhejiang University']","[None, None, None, None, None, None, None, 'Singapore', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71365,Security,Incentives in Private Collaborative Machine Learning,"Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on  shared data or model parameters but neglect the privacy risks involved. To address this, we introduce _differential privacy_ (DP) as an incentive. Each party can select its required DP guarantee and perturb its _sufficient statistic_ (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a _privacy-valuation trade-off_, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets.","['Incentives', 'Privacy', 'Shapley fairness', 'Collaborative machine learning', 'data valuation', 'reward', 'sufficient statistics']",[],"['Rachael Hwee Ling Sim', 'Yehong Zhang', 'Trong Nghia Hoang', 'Xinyi Xu', 'Bryan Kian Hsiang Low', 'Patrick Jaillet']","['National University of', 'Peng Cheng Laboratory', 'Washington State University', 'National University of', 'National University of', 'Massachusetts Institute of Technology']","['Singapore', None, None, 'Singapore', 'Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/69906,Security,(Amplified) Banded Matrix Factorization: A unified approach to private training,"Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as $\epsilon$ becomes small). In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices (lower-triangular matrices with at most $\hat{b}$ nonzero bands including the main diagonal). For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment).  In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as the ubiquitous DP-SGD algorithm, but can provide strictly better performance  in most scenarios---this lets us always at least match DP-SGD, and often outperform it","['Machine Learning', 'Differential Privacy', 'Optimization', 'Private Machine Learning', 'Federated Learning', 'Privacy Amplification', 'Matrix Factorization']",[],"['Christopher A. Choquette-Choo', 'Arun Ganesh', 'Ryan McKenna', 'Hugh Brendan McMahan', 'J Keith Rush', 'Abhradeep Guha Thakurta', 'Zheng Xu']","['Google Research, Brain team', 'Google', 'Department of Computer Science, University of Massachusetts, Amherst', 'Google', 'Google', 'Google', 'Google']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70387,Security,Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy,"Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration  differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an inequality for trade-off functions introduced in this paper. This inequality implies the joint convexity of $F$-divergences. Finally, we study an $f$-DP analog of the advanced joint convexity of the hockey-stick divergence related to $(\epsilon,\delta)$-DP  and apply it to analyze the privacy of mixture mechanisms.","['Differential privacy', '$f$-DP', 'mixture mechanisms', 'shuffling', 'differentially private gradient descent']",[],"['Chendi Wang', 'Buxin Su', 'Jiayuan Ye', 'Reza Shokri', 'Weijie J Su']","['The Wharton School, University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', 'National University of', 'National University of', 'The Wharton School, University of Pennsylvania']","[None, None, 'Singapore', 'Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71639,Security,Dynamic Personalized Federated Learning with Adaptive Differential Privacy,"Personalized federated learning with differential privacy has been considered a feasible solution to address non-IID distribution of data and privacy leakage risks. However, current personalized federated learning methods suffer from inflexible personalization and convergence difficulties due to two main factors: 1) Firstly, we observe that the prevailing personalization methods mainly achieve this by personalizing a fixed portion of the model, which lacks flexibility. 2) Moreover, we further demonstrate that the default gradient calculation is sensitive to the widely-used clipping operations in differential privacy, resulting in difficulties in convergence. Considering that Fisher information values can serve as an effective measure for estimating the information content of parameters by reflecting the model sensitivity to parameters, we aim to leverage this property to address the aforementioned challenges. In this paper, we propose a novel federated learning method with Dynamic Fisher Personalization and Adaptive Constraint (FedDPA) to handle these challenges. Firstly, by using layer-wise Fisher information to measure the information content of local parameters, we retain local parameters with high Fisher values during the personalization process, which are considered informative, simultaneously prevent these parameters from noise perturbation. Secondly, we introduce an adaptive approach by applying differential constraint strategies to personalized parameters and shared parameters identified in the previous for better convergence.  Our method boosts performance through flexible personalization while mitigating the slow convergence caused by clipping operations. Experimental results on CIFAR-10, FEMNIST and SVHN dataset demonstrate the effectiveness of our approach in achieving better performance and robustness against clipping, under personalized federated learning with differential privacy.","['federated learning', 'differential privacy', 'personalization']",[],"['Xiyuan Yang', 'Wenke Huang', 'Mang Ye']","['Wuhan University', 'Wuhan University', 'Wuhan University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70950,Security,Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference,"The marriage of Graph Convolutional Network (GCN) and Homomorphic Encryption (HE) enables the inference of graph data on the cloud with significantly enhanced client data privacy. However, the tremendous computation and memory overhead associated with HE operations challenges the practicality of HE-based GCN inference. GCN inference involves a sequence of expensive matrix-matrix multiplications, and we observe that directly applying the state-of-the-art HE-based secure matrix-matrix multiplication solutions to accelerate HE-GCN inference is far less efficient as it does not exploit the unique aggregation mechanism of two-dimension graph node-features in GCN layer computation. As a result, in this paper, we propose a novel HE-based ciphertext packing technique, i.e., Penguin, that can take advantage of the unique computation pattern during the HE-GCN inference to significantly reduce the computation and memory overhead associated with HE operations. Specifically, Penguin employs (i) an effective two-dimension parallel packing technique for feature ciphertext with optimal graph node partitioning and graph feature interleaving, and (ii) an interleaved assembly technique that can effectively make use of the blank slots to merge ciphertexts after feature reduction and significantly reduce the costly rotation operation. We provide theoretical analysis and experimental validation to demonstrate the speedup achieved by Penguin in accelerating GCN inference using popular GCN models and datasets. Our results show that Penguin can achieve up to $\sim10\times$ speedup and around $\sim79$% reduction in computational memory overhead, significantly outperforming state-of-the-art solutions. To the best of our knowledge, this is the first work that can ensure the protection of both graph structure and features when accelerating HE-GCN inference on encrypted data. Our code is publicly available at https://github.com/ranran0523/Penguin.","['Cryptographic inference', 'Graph Convolutional Network', 'Parallel Packing']",[],"['Ran Ran', 'Nuo Xu', 'Wei Wang', 'Gang Quan', 'Wujie Wen']","['North Carolina State University', 'Lehigh University', 'Microsoft', 'Florida International University', 'North Carolina State University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71789,Security,Towards Unbounded Machine Unlearning,"Deep machine unlearning is the problem of 'removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their 'right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for 'forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure model utility (i.e. accuracy on retained data and generalization), and is more efficient than previous work. The above are substantiated through a comprehensive empirical evaluation against previous state-of-the-art.","['machine unlearning', 'deep learning']",[],"['Meghdad Kurmanji', 'Peter Triantafillou', 'Jamie Hayes', 'Eleni Triantafillou']","['University of Warwick', 'University of Warwick', 'DeepMind', 'Google']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70518,Security,Private Distribution Learning with Public Data: The View from Sample Compression,"We study the problem of private distribution learning with access to public data. In this setup, which we refer to as *public-private learning*, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples. We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as \emph{list learning}. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.","['differential privacy', 'distribution learning', 'gaussians', 'mixture of gaussians', 'compression schemes', 'robust compression schemes', 'privacy']",[],"['Shai Ben-David', 'Alex Bie', 'Clement Louis Canonne', 'Gautam Kamath', 'Vikrant Singhal']","['University of Waterloo', 'University of Waterloo', 'University of Sydney', 'University of Waterloo', 'School of Engineering and Applied Sciences, Harvard University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71807,Security,Practical Differentially Private Hyperparameter Tuning with Subsampling,"Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms often requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algorithms, where the number of random search samples is randomized. Commonly, these algorithms still considerably increase the DP privacy parameter $\varepsilon$ over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the compute cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by appropriately extrapolating the optimal values to a larger dataset. We carry out a Rényi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseline method by Papernot and Steinke.","['differential privacy', 'hyperparameter tuning', 'Rényi differential privacy', 'computational efficiency', 'DP-SGD']",[],"['Antti Koskela', 'Tejas Kulkarni']","['Nokia Bell Labs', 'Nokia Bell Labs']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72242,Security,Faster approximate subgraph counts with privacy,"One of the most common problems studied in the context of differential privacy for graph data is counting the number of non-induced embeddings of a subgraph in a given graph. These counts have very high global sensitivity. Therefore, adding noise based on powerful alternative techniques, such as smooth sensitivity and higher-order local sensitivity have been shown to give significantly better accuracy. However, all these alternatives to global sensitivity become computationally very expensive, and to date efficient polynomial time algorithms are known only for few selected subgraphs, such as triangles, $k$-triangles, and $k$-stars. In this paper, we show that good approximations to these sensitivity metrics can be still used to get private algorithms. Using this approach, we much faster algorithms for privately counting the number of triangles in real-world social networks, which can be easily parallelized. We also give a private polynomial time algorithm for counting any constant size subgraph using less noise than the global sensitivity; we show this can be improved significantly for counting paths in special classes of graphs.","['differential privacy', 'subgraph counting', 'smooth sensitivity', 'local sensitivity']",[],"['Mahantesh M Halappanavar', 'Anil Vullikanti']","['Pacific Northwest National Laboratory', 'University of Virginia']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71656,Security,Federated Spectral Clustering via Secure Similarity Reconstruction,"Federated learning has a significant advantage in protecting information privacy. Many scholars proposed various secure learning methods within the framework of federated learning but the study on secure federated unsupervised learning especially clustering is limited. We in this work propose a secure kernelized factorization method for federated spectral clustering on distributed dataset. The method is non-trivial because the kernel or similarity matrix for spectral clustering is computed by data pairs, which violates the principle of privacy protection. Our method implicitly constructs an approximation for the kernel matrix on distributed data such that we can perform spectral clustering under the constraint of privacy protection. We provide a convergence guarantee of the optimization algorithm, reconstruction error bounds of the Gaussian kernel matrix, and the sufficient condition of correct clustering of our method. We also present some results of differential privacy. Numerical results on synthetic and real datasets demonstrate that the proposed method is efficient and accurate in comparison to the baselines.","['clustering', 'federated learning', 'privacy']",[],"['Dong Qiao', 'Chris Ding']","['The Chinese University of , Shenzhen', 'University of Texas at Arlington']","['Hong Kong', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71878,Security,Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency,"We study the canonical problem of linear regression under $(\varepsilon,\delta)$-differential privacy when the datapoints are sampled i.i.d.~from a distribution and a fraction of response variables are adversarially corrupted. We provide the first provably efficient -- both computationally and statistically -- method for this problem, assuming standard assumptions on the data distribution. Our algorithm is a variant of the popular differentially private stochastic gradient descent (DP-SGD) algorithm with two key innovations: a full-batch gradient descent to improve sample complexity and a novel adaptive clipping to guarantee robustness. Our method requires only linear time in input size, and still matches the information theoretical optimal sample complexity up to a data distribution dependent condition number factor.  Interestingly, the same algorithm, when applied to a setting where there is no adversarial corruption, still improves upon the existing state-of-the-art and achieves a near optimal sample complexity.",['Differential Privacy; Private Estimation'],[],"['Xiyang Liu', 'Prateek Jain', 'Weihao Kong', 'Sewoong Oh', 'Arun Suggala']","['Department of Computer Science, University of Washington', 'Google', 'Google', 'University of Washington', 'Google']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72183,Security,Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation,"The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE) despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplace distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.","['Variational AutoEncoder', 'distributional learning', 'synthetic data generation', 'CRPS', 'asymmetric Laplace distribution']",[],"['Seunghwan An', 'Jong-June Jeon']","['University of Seoul', 'University of Seoul']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71784,Security,Differentially Private Image Classification by Learning Priors from Random Processes,"In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition. A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data. In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, MedMNIST and ImageNet for a range of privacy budgets $\\varepsilon \\in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \\%$ to $72.3 \\%$ for $\\varepsilon=1$.","['Differential privacy', 'image classification', 'deep learning']",[],"['Xinyu Tang', 'Ashwinee Panda', 'Vikash Sehwag', 'Prateek Mittal']","['Princeton University', 'Princeton University', 'Princeton University', 'Princeton University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73048,Security,Model Sparsity Can Simplify Machine Unlearning,"In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although exact unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. We show in both theory and practice that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This leads to a new MU paradigm,    termed prune first, then unlearn, which infuses a sparse prior to the unlearning process. Building on this insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of approximate unlearning. Extensive experiments show that our proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) when using our proposed sparsity-aware unlearning method. Furthermore, we showcase the practical impact of our proposed MU methods through two specific use cases: defending against backdoor attacks, and enhancing transfer learning through source class removal. These applications demonstrate the versatility and effectiveness of our approaches in addressing a variety of machine learning challenges beyond unlearning for data privacy. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse.","['Machine unlearning', 'model pruning']",[],"['Jinghan Jia', 'Jiancheng Liu', 'Parikshit Ram', 'Yuguang Yao', 'Gaowen Liu', 'Yang Liu', 'Pranay Sharma', 'Sijia Liu']","['Michigan State University', 'Michigan State University', 'International Business Machines', 'Michigan State University', 'Cisco Systems', 'University of California, Santa Cruz', 'Carnegie Mellon University', 'Michigan State University']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72570,Security,Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation,"Many fine-grained classification tasks, like rare animal identification, have limited training data and consequently classifiers trained on these datasets often fail to  generalize to variations in the domain like changes in weather or location.  As such, we explore how natural language descriptions of the domains seen in training data can be used with large vision models trained on diverse pretraining datasets to generate useful variations of the training data. We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes large vision and language models to automatically generate natural language descriptions of a dataset's domains and augment the training data via language-guided image editing. To maintain data integrity, a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. The resulting dataset is visually consistent with the original training data and offers significantly enhanced diversity. We show that ALIA is able to surpasses traditional data augmentation and text-to-image generated data on fine-grained classification tasks, including cases of domain generalization and contextual bias. Code is available at https://github.com/lisadunlap/ALIA.","['data augmentation', 'diffusion', 'vision and language']",[],"['Lisa Dunlap', 'Alyssa Umino', 'Han Zhang', 'Jiezhi Yang', 'Joseph E. Gonzalez', 'Trevor Darrell']","['University of California, Berkeley', 'University of California, Berkeley', 'Stanford University', 'Harvard University', 'University of California - Berkeley', 'Electrical Engineering & Computer Science Department']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72848,Security,Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy,"We consider a federated data analytics problem in which a server coordinates the collaborative data analysis of multiple users with privacy concerns and limited communication capability. The commonly adopted compression schemes introduce information loss into local data while improving communication efficiency, and it remains an open problem whether such discrete-valued mechanisms provide any privacy protection. In this paper, we study the local differential privacy guarantees of discrete-valued mechanisms with finite output space through the lens of $f$-differential privacy (DP). More specifically, we advance the existing literature by deriving tight $f$-DP guarantees for a variety of discrete-valued mechanisms, including the binomial noise and the binomial mechanisms that are proposed for privacy preservation, and the sign-based methods that are proposed for data compression, in closed-form expressions. We further investigate the amplification in privacy by sparsification and propose a ternary stochastic compressor. By leveraging compression for privacy amplification, we improve the existing methods by removing the dependency of accuracy (in terms of mean square error) on communication cost in the popular use case of distributed mean estimation, therefore breaking the three-way tradeoff between privacy, communication, and accuracy.","['Differential privacy', 'federated data analytics', 'discrete valued-mechanism', 'distributed mean estimation']",[],"['Richeng Jin', 'Caijun Zhong', 'Zhaoyang Zhang', 'Tony Quek', 'Huaiyu Dai']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'University of Technology and Design', 'North Carolina State University']","[None, None, None, 'Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/71215,Security,Differentially Private Approximate Near Neighbor Counting in High Dimensions,"Range counting (e.g., counting the number of data points falling into a given query ball) under differential privacy has been studied extensively. However, the current algorithms for this problem are subject to the following dichotomy. One class of algorithms suffers from an additive error that is a fixed polynomial in the number of points. Another class of algorithms allows for polylogarithmic additive error, but the error grows exponentially in the dimension. To achieve the latter, the problem is relaxed to allow a “fuzzy” definition of the range boundary, e.g., a count of the points in a ball of radius $r$ might also include points in a ball of radius $cr$ for some $c>1$. In this paper we present an efficient algorithm that offers a sweet spot between these two classes. The algorithm has an additive error that is an arbitrary small power of the data set size, depending on how fuzzy the range boundary is, as well as a small ($1+o(1)$) multiplicative error. Crucially, the amount of noise added has no dependence on the dimension. Our algorithm introduces a variant of Locality-Sensitive Hashing, utilizing it in a novel manner.","['Differential Privacy', 'Near Neighbor Search', 'Locality Sensitive Hashing', 'Data Structures', 'Range Query']",[],"['Alexandr Andoni', 'Piotr Indyk', 'Sepideh Mahabadi', 'Shyam Narayanan']","['Columbia University', 'Massachusetts Institute of Technology', 'Microsoft Research', 'Massachusetts Institute of Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70163,Security,Static and Sequential Malicious Attacks in the Context of Selective Forgetting,"With the growing demand for the right to be forgotten, there is an increasing need for machine learning models to forget sensitive data and its impact. To address this, the paradigm of selective forgetting (a.k.a machine unlearning) has been extensively studied, which aims to remove the impact of requested data from a well-trained model without retraining from scratch. Despite its significant success, limited attention has been given to the security vulnerabilities of the unlearning system concerning malicious data update requests. Motivated by this, in this paper, we explore the possibility and feasibility of malicious data update requests during the unlearning process. Specifically, we first propose a new class of malicious selective forgetting attacks, which involves a static scenario where all the malicious data update requests are provided by the adversary at once. Additionally, considering the sequential setting where the data update requests arrive sequentially, we also design a novel framework for sequential forgetting attacks, which is formulated as a stochastic optimal control problem. We also propose novel optimization algorithms that can find the effective malicious data update requests. We perform theoretical analyses for the proposed selective forgetting attacks, and extensive experimental results validate the effectiveness of our proposed selective forgetting attacks. The source code is available in the supplementary material.","['Selective forgetting', 'static setting', 'sequential setting', 'security and robustness']",[],"['CHENXU ZHAO', 'Wei Qian', 'Zhitao Ying', 'Mengdi Huai']","['Iowa State University', 'Iowa State University', 'Yale University', 'Iowa State University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72566,Security,k-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy,"In clustering algorithms, the choice of initial centers is crucial for the quality of the learned clusters. We propose a new initialization scheme for the $k$-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. We propose a novel and efficient search algorithm, for good initial centers that can be used subsequently for the local search algorithm. The so-called HST initialization method can produce initial centers achieving lower error than those from another popular method $k$-median++, also with higher efficiency when $k$ is not too small. Our HST initialization can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. We show that the error of applying DP local search followed by our private HST initialization improves previous results on the approximation error, and approaches the lower bound within a small factor. Experiments demonstrate the effectiveness of our proposed methods.","['privacy', 'clustering']",[],"['Chenglin Fan', 'Ping Li', 'Xiaoyun Li']","['UT Dallas', 'Rutgers University', 'LinkedIn']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72222,Security,Smooth Flipping Probability for Differential Private Sign Random Projection Methods,"We develop a series of differential privacy (DP) algorithms from a family of random projection (RP) and sign random projection (SignRP) methods. We first show how to improve the previous DP-RP approach using the ``optimal Gaussian mechanism''. Then, we propose a series of DP-SignRP algorithms that leverage the robustness of the ``sign flipping probability'' of random projections. That is, given $x = \sum_{i=1}^p u_i w_{i}$ where $u$ is a $p$-dimensional data vector and $w$ is a symmetric random vector,  $sign(x)$ only has a fairly small probability to be flipped if there is a small modification on data $u$, depending on the specific distribution of $w$. This robustness leads to our novel design of ``smooth flipping probability'' for SignRP-type algorithms with better utility than using the standard randomized response mechanism. Retrieval and classification experiments demonstrate that,  among the presented DP-RP  algorithms,  \textbf{DP-SignOPORP} (where OPORP  is an improvement over the celebrated  count-sketch algorithms), performs the best in general. In the industrial practice, DP methods were not very popular for machine learning or search, largely because the performance typically would  drop substantially if DP is applied. Since our proposed new DP algorithms have significantly improved the  performance, it is anticipated that our work will motivate a wide adoption of DP in practice. Finally, we   stress that, since our  methods are applied to the original data (i.e., feature vectors), the privacy of  downstream tasks is naturally protected.","['Differential Privacy', 'Random Projection']",[],"['Ping Li', 'Xiaoyun Li']","['Rutgers University', 'LinkedIn']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69956,Security,What Can We Learn from Unlearnable Datasets?,"In an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. But in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. First, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. In contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. Unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. To emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal projection attack which allows learning from unlearnable datasets published in ICML 2021 and ICLR 2023. Our proposed attack is significantly less complex than recently proposed techniques.","['data poisoning', 'poisons', 'unlearnable dataset', 'data protection', 'imperceptible perturbations', 'adversarial machine learning']",[],"['Pedro Sandoval-Segura', 'Vasu Singla', 'Jonas Geiping', 'Micah Goldblum', 'Tom Goldstein']","['University of Maryland, College Park', 'University of Maryland, College Park', 'ELLIS Institute Tübingen', 'New York University', 'University of Maryland, College Park']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70247,Security,Sparsity-Preserving Differentially Private Training of Large Embedding Models,"As the use of large embedding models in recommendation systems and language applications increases, concerns over user data privacy have also risen.  DP-SGD, a training algorithm that combines differential privacy with stochastic gradient descent, has been the workhorse in protecting user privacy without compromising model accuracy by much. However, applying DP-SGD naively to embedding models can destroy gradient sparsity, leading to reduced training efficiency. To address this issue, we present two new algorithms, DP-FEST and DP-AdaFEST, that preserve gradient sparsity during the private training of large embedding models.  Our algorithms achieve substantial reductions ($10^6 \times$) in gradient size, while maintaining comparable levels of accuracy, on benchmark real-world datasets.","['Differential Privacy', 'Recommendation Systems', 'Embedding Models', 'Efficient Machine Learning']",[],"['Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Pasin Manurangsi', 'Amer Sinha', 'Chiyuan Zhang']","['Google', 'Princeton University', 'Google Research', 'Google', 'Google', 'Research, Google', 'Google']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71772,Security,User-Level Differential Privacy With Few Examples Per User,"Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS 2021, Bun et al. STOC 2023] obtained generic algorithms that work for various learning tasks. However, their focus was on the *example-rich* regime, where the users have so many examples that each user could themselves solve the problem. In this work we consider the *example-scarce* regime, where each user has only a few examples, and obtain the following results: * For approximate-DP, we give a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a (multiplicative) savings of $O_{\varepsilon,\delta}(\sqrt{m})$ in terms of the number of users required for achieving the same utility, where $m$ is the number of examples per user. This algorithm, while recovering most known bounds for specific problems, also gives new bounds, e.g., for PAC learning. * For pure-DP, we present a simple technique for adapting the exponential mechanism [McSherry & Talwar, FOCS 2007] to the user-level setting. This gives new bounds for a variety of tasks, such as private PAC learning, hypothesis selection, and distribution learning. For some of these problems, we show that our bounds are near-optimal.","['differential privacy', 'user-level privacy', 'PAC learning']",[],"['Badih Ghazi', 'Pritish Kamath', 'Ravi Kumar', 'Pasin Manurangsi', 'Raghu Meka', 'Chiyuan Zhang']","['Google', 'Google Research', 'Google', 'Google', 'University of California, Los Angeles', 'Google']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70370,Security,Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy,"We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise *linearly correlated* over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretically and empirically.","['optimization', 'machine learning', 'differential privacy']",[],"['Anastasia Koloskova', 'Ryan McKenna', 'Zachary Charles', 'J Keith Rush', 'Hugh Brendan McMahan']","['Swiss Federal Institute of Technology Lausanne', 'Department of Computer Science, University of Massachusetts, Amherst', 'Google', 'Google', 'Google']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72741,Security,Adversarially Robust Distributed Count Tracking via Partial Differential Privacy,"We study the distributed tracking model, also known as distributed functional monitoring. This model involves $k$ sites each receiving a stream of items and communicating with the central server. The server's task is to track a function of all items received thus far continuously, with minimum communication cost. For count tracking, it is known that there is a $\sqrt{k}$ gap in communication between deterministic and randomized algorithms. However, existing randomized algorithms assume an ""oblivious adversary"" who constructs the entire input streams before the algorithm starts. Here we consider adaptive adversaries who can choose new items based on previous answers from the algorithm. Deterministic algorithms are trivially robust to adaptive adversaries, while randomized ones may not. Therefore, we investigate whether the $\sqrt{k}$ advantage of randomized algorithms is from randomness itself or the oblivious adversary assumption. We provide an affirmative answer to this question by giving a robust algorithm with optimal communication. Existing robustification techniques do not yield optimal bounds due to the inherent challenges of the distributed nature of the problem. To address this, we extend the differential privacy framework by introducing ""partial differential privacy"" and proving a new generalization theorem. This theorem may have broader applications beyond robust count tracking, making it of independent interest.","['Distributed Tracking', 'Adaptive Robustness', 'Differential Privacy', 'Generalization']",[],"['Zhongzheng Xiong', 'Xiaoyi Zhu', 'Zengfeng Huang']","['Fudan University', 'Fudan University', 'Fudan University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70167,Security,Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models,"Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with $(\varepsilon=0.147, \delta=10^{-6})$-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial~APIs.","['differential privacy', 'in-context learning', 'trustworthy ML']",[],"['Haonan Duan', 'Nicolas Papernot']","['Department of Computer Science, University of Toronto', 'University of Toronto']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70179,Security,The Bayesian Stability Zoo,"We show that many definitions of stability found in the learning theory literature are equivalent to one another. We distinguish between two families of definitions of stability: distribution-dependent and distribution-independent Bayesian stability. Within each family, we establish equivalences between various definitions, encompassing approximate differential privacy, pure differential privacy, replicability, global stability, perfect generalization, TV stability, mutual information stability, KL-divergence stability, and Rényi-divergence stability. Along the way, we prove boosting results that enable the amplification of the stability of a learning rule. This work is a step towards a more systematic taxonomy of stability notions in learning theory,  which can promote clarity and an improved understanding of an array of stability concepts that have emerged in recent years.","['Algorithmic stability', 'Replicability', 'Differential Privacy', 'KL Stability', 'Mutual Information Stability', 'Global Stability', 'Perfect Generalization', 'PAC Learning', 'Littlestone Dimension', 'Clique Dimension', 'PAC Bayes']",[],"['Shay Moran', 'Hilla Schefler']","['Technion, Technion', 'Technion -  Institute of Technology, Technion -  Institute of Technology']","[None, 'Israel']",,,,,,,
https://nips.cc/virtual/2023/poster/70015,Security,Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks,"Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks.","['language model', 'distillation', 'reasoning', 'knowledge augmentation']",[],"['Minki Kang', 'Seanie Lee', 'Jinheon Baek', 'Kenji Kawaguchi', 'Sung Ju Hwang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'National University of', 'Korea Advanced Institute of Science and Technology']","[None, None, None, 'Singapore', None]",,,,,,,
https://nips.cc/virtual/2023/poster/70539,Security,Unleashing the Power of Randomization in Auditing Differentially Private ML,"We present a rigorous methodology for auditing differentially private machine learning by adding multiple carefully designed examples called canaries. We take a first principles approach based on three key components. First, we introduce Lifted Differential Privacy (LiDP) that expands the definition of differential privacy to handle randomized datasets. This gives us the freedom to design randomized canaries. Second, we audit LiDP by trying to distinguish between the model trained with $K$ canaries versus $K-1$ canaries in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDP can leverage the symmetry in the design and reuse each privately trained model to run multiple statistical tests, one for each canary. Third, we introduce novel confidence intervals that take advantage of the multiple test statistics by adapting to the empirical higher-order correlations. Together, this new recipe demonstrates significant improvements in sample complexity, both theoretically and empirically, using synthetic and real data. Further, recent advances in designing stronger canaries can be readily incorporated in the new framework.","['Differential privacy auditing', 'multiple canaries', 'randomization', 'lifting', 'adaptive confidence intervals']",[],"['Krishna Pillutla', 'Galen Andrew', 'Peter Kairouz', 'Hugh Brendan McMahan', 'Alina Oprea', 'Sewoong Oh']","['Google', 'Google', 'Google', 'Google', 'Northeastern University', 'University of Washington']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70844,Security,On Private and Robust Bandits,"We study private and robust multi-armed bandits (MABs), where the agent receives Huber's contaminated heavy-tailed rewards and meanwhile needs to ensure differential privacy. We consider both the finite $k$-th raw moment and the finite $k$-th central moment settings for heavy-tailed rewards distributions with $k\ge 2$. We first present its minimax lower bound, characterizing the information-theoretic limit of regret with respect to privacy budget, contamination level, and heavy-tailedness. Then, we propose a meta-algorithm that builds on a private and robust mean estimation sub-routine \texttt{PRM} that essentially relies on reward truncation and the Laplace mechanism.  For the above two different heavy-tailed settings, we give corresponding schemes of \texttt{PRM}, which enable us to achieve nearly-optimal regrets.  Moreover, our two proposed truncation-based or histogram-based \texttt{PRM} schemes achieve the optimal trade-off between estimation accuracy, privacy and robustness. Finally, we support our theoretical results and show the effectiveness of our algorithms with experimental studies.","['Bandits', 'privacy', 'robustness']",[],"['Yulian Wu', 'Xingyu Zhou', 'Youming Tao', 'Di Wang']","['KAT', 'Wayne State University', 'Technische Universität Berlin', 'KAT']","['US', None, None, 'US']",,,,,,,
https://nips.cc/virtual/2023/poster/70859,Security,Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer,"Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution drift caused by data heterogeneity during the model training process and obtain a global model with better performance on the minority classes while maintaining the performance of the majority classes. Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achieves state-of-the-art performance on representative datasets such as CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist.","['Federated learning', 'Long-tailed learning', 'Data heterogeneity']",[],"['Zikai Xiao', 'Zihan Chen', 'Songshang Liu', 'Hualiang Wang', 'YANG FENG', 'Jin Hao', 'Joey Tianyi Zhou', 'Jian Wu', 'Howard Hao Yang', 'Zuozhu Liu']","['Zhejiang University', 'University of Technology and Design', 'University of Science and Technology', 'University of Science and Technology', 'Angelalign Tech.', 'Stanford University', 'National University of', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']","[None, 'Singapore', 'Hong Kong', 'Hong Kong', None, None, 'Singapore', None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70940,Security,Black-Box Differential Privacy for Interactive ML,"In this work we revisit an interactive variant of joint differential privacy, recently introduced by Naor et al. [2023], and generalize it towards handling online processes in which existing privacy definitions seem too restrictive. We study basic properties of this definition and demonstrate that it satisfies (suitable variants) of group privacy, composition, and post processing. In order to demonstrate the advantages of this privacy definition compared to traditional forms of differential privacy, we consider the basic setting of online classification. We show that any (possibly non-private) learning rule can be effectively transformed to a private learning rule with only a polynomial overhead in the mistake bound. This demonstrates a stark difference with traditional forms of differential privacy, such as the one studied  by Golowich and Livni [2021], where only a double exponential overhead in the mistake bound is known (via an information theoretic upper bound).","['Differential privacy', 'online learning']",[],"['Haim Kaplan', 'Yishay Mansour', 'Shay Moran', 'Kobbi Nissim', 'Uri Stemmer']","['Tel Aviv University', 'School of Computer Science, Tel Aviv University', 'Technion, Technion', 'Georgetown University', 'Tel Aviv University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70990,Security,Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks,"We reconsider the challenge of non-convex optimization under differential privacy constraint. Building upon the previous variance-reduced algorithm SpiderBoost, we propose a novel framework that employs two types of gradient oracles: one that estimates the gradient at a single point and a more cost-effective option that calculates the gradient difference between two points. Our framework can ensure continuous accuracy of gradient estimations and subsequently enhances the rates of identifying second-order stationary points. Additionally, we consider a more challenging task by attempting to locate the global minima of a non-convex objective via the exponential mechanism without almost any assumptions. Our preliminary results suggest that the regularized exponential mechanism can effectively emulate previous empirical and population risk bounds, negating the need for smoothness assumptions for algorithms with polynomial running time. Furthermore, with running time factors excluded, the exponential mechanism demonstrates promising population risk bound performance, and we provide a nearly matching lower bound.","['Differential Privacy', 'Non-convex optimization', 'Stationary points', 'Exponential Mechanism']",[],"['Daogao Liu', 'Arun Ganesh', 'Sewoong Oh', 'Abhradeep Guha Thakurta']","['University of Washington, Seattle', 'Google', 'University of Washington', 'Google']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71012,Security,Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.","['Graph Neural Networks', 'Differential Privacy', 'Multigranular Topology Protection']",[],"['Eli Chien', 'Wei-Ning Chen', 'Chao Pan', 'Pan Li', 'Ayfer Ozgur', 'Olgica Milenkovic']","['Institute of Technology', 'Stanford University', 'University of Illinois, Urbana Champaign', 'Institute of Technology', 'Stanford University', 'University of Illinois, Urbana Champaign']","['Georgia', None, None, 'Georgia', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71290,Security,GAN You See Me? Enhanced Data Reconstruction Attacks against Split Inference,"Split Inference (SI) is an emerging deep learning paradigm that addresses computational constraints on edge devices and preserves data privacy through collaborative edge-cloud approaches. However, SI is vulnerable to Data Reconstruction Attacks (DRA), which aim to reconstruct users' private prediction instances. Existing attack methods suffer from various limitations. Optimization-based DRAs do not leverage public data effectively, while Learning-based DRAs depend heavily on auxiliary data quantity and distribution similarity. Consequently, these approaches yield unsatisfactory attack results and are sensitive to defense mechanisms. To overcome these challenges, we propose a GAN-based LAtent Space Search attack (GLASS) that harnesses abundant prior knowledge from public data using advanced StyleGAN technologies. Additionally, we introduce GLASS++ to enhance reconstruction stability. Our approach represents the first GAN-based DRA against SI, and extensive evaluation across different split points and adversary setups demonstrates its state-of-the-art performance. Moreover, we thoroughly examine seven defense mechanisms, highlighting our method's capability to reveal private information even in the presence of these defenses.","['deep learning', 'split inference', 'data reconstruction attack']",[],"['Ziang Li', 'Mengda Yang', 'Yaxin Liu', 'Juan Wang', 'Hongxin Hu', 'Wenzhe Yi', 'Xiaoyang Xu']","['Wuhan University', 'Wuhan University', 'Wuhan University', 'Wuhan University', 'State University of New York, Buffalo', 'Wuhan University', 'Wuhan University']","[None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71354,Security,Have it your way: Individualized Privacy Assignment for DP-SGD,"When training a machine learning model with differential privacy, one sets a privacy budget. This uniform budget represents an overall maximal privacy violation that any user is willing to face by contributing their data to the training set. We argue that this approach is limited because different users may have different privacy expectations. Thus, setting a uniform privacy budget across all points may be overly conservative for some users or, conversely, not sufficiently protective for others. In this paper, we capture these preferences through individualized privacy budgets. To demonstrate their practicality, we introduce a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which supports such individualized budgets. DP-SGD is the canonical approach to training models with differential privacy. We modify its data sampling and gradient noising mechanisms to arrive at our approach, which we call Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees tailored to the preferences of individual users and their data points, we empirically find it to improve privacy-utility trade-offs.","['privacy', 'machine learning', 'differential privacy', 'DP-SGD', 'individualized privacy']",[],"['Christopher Mühl', 'Roy Rinberg', 'Nicolas Papernot']","['Freie Universität Berlin', 'Columbia University', 'University of Toronto']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71459,Security,Federated Learning via Meta-Variational Dropout,"Federated Learning (FL) aims to train a global inference model from remotely distributed clients, gaining popularity due to its benefit of improving data privacy. However, traditional FL often faces challenges in practical applications, including model overfitting and divergent local models due to limited and non-IID data among clients. To address these issues, we introduce a novel Bayesian meta-learning approach called meta-variational dropout (MetaVD). MetaVD learns to predict client-dependent dropout rates via a shared hypernetwork, enabling effective model personalization of FL algorithms in limited non-IID data settings. We also emphasize the posterior adaptation view of meta-learning and the posterior aggregation view of Bayesian FL via the conditional dropout posterior. We conducted extensive experiments on various sparse and non-IID FL datasets. MetaVD demonstrated excellent classification accuracy and uncertainty calibration performance, especially for out-of-distribution (OOD) clients. MetaVD compresses the local model parameters needed for each client, mitigating model overfitting and reducing communication costs. Code is available at https://github.com/insujeon/MetaVD.","['Personalized Federated Learning', 'Variational Dropout', 'Meta-Learning', 'Bayesian Neural Network']",[],"['Insu Jeon', 'Minui Hong', 'Junhyeog Yun', 'Gunhee Kim']","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71798,Security,Federated Multi-Objective Learning,"In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates to significantly reduce communication costs, while achieving the {\em same} convergence rates as those of their algorithmic counterparts in the single-objective federated learning. Our extensive experiments also corroborate the efficacy of our proposed FMOO algorithms.","['Multi-Objective Learning', 'Federated Learning']",[],"['Haibo Yang', 'Zhuqing Liu', 'Jia Liu', 'Chaosheng Dong', 'Michinari Momma']","['Rochester Institute of Technology', 'Ohio State University', 'The Ohio State University', 'University of Pittsburgh', 'Amazon']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71890,Security,An Efficient Dataset Condensation Plugin and Its Application to Continual Learning,"Dataset condensation (DC) distills a large real-world dataset into a small synthetic dataset, with the goal of training a network from scratch on the latter that performs similarly to the former. State-of-the-art (SOTA) DC methods have achieved satisfactory results through techniques such as accuracy, gradient, training trajectory, or distribution matching. However, these works all perform matching in the high-dimension pixel spaces, ignoring that natural images are usually locally connected and have lower intrinsic dimensions, resulting in low condensation efficiency.  In this work, we propose a simple-yet-efficient dataset condensation plugin that matches the raw and synthetic datasets in a low-dimensional manifold. Specifically, our plugin condenses raw images into two low-rank matrices instead of parameterized image matrices. Our plugin can be easily incorporated into existing DC methods, thereby containing richer raw dataset information at limited storage costs to improve the downstream applications' performance.  We verify on multiple public datasets that when the proposed plugin is combined with SOTA DC methods, the performance of the network trained on synthetic data is significantly improved compared to traditional DC methods. Moreover, when applying the DC methods as a plugin to continual learning tasks, we observed that our approach effectively mitigates catastrophic forgetting of old tasks under limited memory buffer constraints and avoids the problem of raw data privacy leakage.","['Data Condensation', 'Continual Learning', 'Few-shot Learning']",[],"['Enneng Yang', 'Li Shen', 'Zhenyi Wang', 'Tongliang Liu', 'Guibing Guo']","['Northeastern University,', 'JD Explore Academy', 'University of Maryland, College Park', 'University of Sydney', 'Nanyang Technological University']","['China', None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71928,Security,Multiply Robust Federated Estimation of Targeted Average Treatment Effects,"Federated or multi-site studies have distinct advantages over single-site studies, including increased generalizability, the ability to study underrepresented populations, and the opportunity to study rare exposures and outcomes. However, these studies are complicated by the need to preserve the privacy of each individual's data, heterogeneity in their covariate distributions, and different data structures between sites. We propose a novel federated approach to derive valid causal inferences for a target population using multi-site data. We adjust for covariate shift and accommodate covariate mismatch between sites by developing a multiply-robust and privacy-preserving nuisance function estimation approach. Our methodology incorporates transfer learning to estimate ensemble weights to combine information from source sites. We show that these learned weights are efficient and optimal under different scenarios. We showcase the finite sample advantages of our approach in terms of efficiency and robustness compared to existing state-of-the-art approaches. We apply our approach to study the treatment effect of percutaneous coronary intervention (PCI) on the duration of hospitalization for patients experiencing acute myocardial infarction (AMI) with data from the Centers for Medicare \& Medicaid Services (CMS).","['Causal inference', 'Covariate mismatch', 'Federated learning', 'Multiple robustness', 'Transportation']",[],"['Larry Han', 'Zhu Shen', 'Jose R Zubizarreta']","['Northeastern University', 'Harvard University, Harvard University', 'Harvard University']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72051,Security,Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition,"Privacy-Preserving Action Recognition (PPAR) aims to transform raw videos into anonymous ones to prevent privacy leakage while maintaining action clues, which is an increasingly important problem in intelligent vision applications. Despite recent efforts in this task, it is still challenging to deal with novel privacy attributes and novel privacy attack models that are unavailable during the training phase. In this paper, from the perspective of meta-learning (learning to learn), we propose a novel Meta Privacy-Preserving Action Recognition (MPPAR) framework to improve both generalization abilities above (i.e., generalize to *novel privacy attributes* and *novel privacy attack models*) in a unified manner. Concretely, we simulate train/test task shifts by constructing disjoint support/query sets w.r.t. privacy attributes or attack models. Then, a virtual training and testing scheme is applied based on support/query sets to provide feedback to optimize the model's learning toward better generalization. Extensive experiments demonstrate the effectiveness and generalization of the proposed framework compared to state-of-the-arts.","['Privacy Preservation', 'Action Recognition', 'Meta-Learning']",[],"['Duo Peng', 'Li Xu', 'Qiuhong Ke', 'Ping Hu', 'Jun Liu']","['University of Technology and Design', 'University of Technology and Design', 'Monash University', 'University of Electronic Science and Technology of', 'University of Technology and Design']","['Singapore', 'Singapore', None, 'China', 'Singapore']",,,,,,,
https://nips.cc/virtual/2023/poster/72092,Security,Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks,"We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset. We demonstrate efficacy of our attack when unlearning is performed via retraining from scratch, the idealized setting of machine unlearning which other efficient methods attempt to emulate, as well as against the approximate unlearning approach of Graves et al. (2021).","['Machine unlearning', 'new attack vector', 'Camouflaging poisoning attacks']",[],"['Jimmy Z. Di', 'Jack Douglas', 'Jayadev Acharya', 'Gautam Kamath', 'Ayush Sekhari']","['University of Waterloo', 'University of Waterloo', 'Cornell University', 'University of Waterloo', 'Massachusetts Institute of Technology']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72097,Security,Improving the Privacy and Practicality of Objective Perturbation for Differentially Private Linear Learners,"In the arena of privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) has outstripped the objective perturbation mechanism in popularity and interest. Though unrivaled in versatility, DP-SGD requires a non-trivial privacy overhead (for privately tuning the model’s hyperparameters) and a computational complexity which might be extravagant for simple models such as linear and logistic regression. This paper revamps the objective perturbation mechanism with tighter privacy analyses and new computational tools that boost it to perform competitively with DP-SGD on unconstrained convex generalized linear problems.","['differential privacy', 'empirical risk minimization', 'objective perturbation']",[],"['Rachel Emily Redberg', 'Antti Koskela', 'Yu-Xiang Wang']","['UC Santa Barbara', 'Nokia Bell Labs', 'UC Santa Barbara']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71717,Security,Nearly Tight Bounds For Differentially Private Multiway Cut,"Finding min $s$-$t$ cuts in graphs is a basic algorithmic tool, with applications in image segmentation, community detection, reinforcement learning, and data clustering. In this problem, we are given two nodes as terminals and the goal is to remove the smallest number of edges from the graph so that these two terminals are disconnected. We study the complexity of differential privacy for the min $s$-$t$ cut problem and show nearly tight lower and upper bounds where we achieve privacy at no cost for running time efficiency. We also develop a differentially private algorithm for the multiway $k$-cut problem, in which we are given $k$ nodes as terminals that we would like to disconnect. As a function of $k$, we obtain privacy guarantees that are exponentially more efficient than applying the advanced composition theorem to known algorithms for multiway $k$-cut. Finally, we empirically evaluate the approximation of our differentially private min $s$-$t$ cut algorithm and show that it almost matches the quality of the output of non-private ones.","['Differential Privacy', 'clustering', 'multiway cut', 'min cut', 'graph partitioning']",[],"['Mina Dalirrooyfard', 'Slobodan Mitrovic', 'Yuriy Nevmyvaka']","['Morgan Stanley', 'University of California, Davis', 'Morgan Stanley']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72502,Security,Sampling from Structured Log-Concave Distributions via a Soft-Threshold Dikin Walk,"Given a Lipschitz or smooth convex function $f:K \to \mathbb{R}^d$ for a bounded polytope $K:=${ $\theta \in \mathbb{R}^d: A\theta \leq b$}, where $A\in  \mathbb{R}^{m\times d}$ and $b \in \mathbb{R}^m$, we consider the problem of sampling from the log-concave distribution $\pi(\theta) \propto e^{-f(\theta)}$ constrained to $K$. Interest in this problem derives from its applications to  Bayesian inference and differential privacy. We present  a generalization of the Dikin walk  to this setting that requires at most $O((md + d L^2 R^2) \times md^{\omega-1} \log(\frac{w}{\delta}))$ arithmetic operations to sample from $\pi$ within error $\delta>0$ in the total variation distance from a $w$-warm start. Here $L$ is the Lipschitz constant of $f$, $K$ is contained in a ball of radius $R$ and contains a ball of smaller radius $r$, and $\omega \approx 2.37$ is the matrix-multiplication constant. This improves on the running time of prior works for a range of structured settings important for the aforementioned inference and privacy applications. Technically, we depart from previous Dikin walks by adding a soft-threshold regularizer derived from the Lipschitz or smoothness properties of $f$  to a barrier function for $K$ that allows our version of the Dikin walk to propose updates  that have a high Metropolis acceptance ratio for $f$, while at the same time remaining inside the polytope $K$.","['Logconcave sampling', 'Dikin walk', 'Markov chain Monte Carlo', 'interior point methods']",[],"['Oren Mangoubi', 'Nisheeth K Vishnoi']","['Worcester Polytechnic Institute', 'Yale University']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72535,Security,A Unified Solution for Privacy and Communication Efficiency in Vertical Federated Learning,"Vertical Federated Learning (VFL) is a collaborative machine learning paradigm that enables multiple participants to jointly train a model on their private data without sharing it. To make VFL practical, privacy security and communication efficiency should both be satisfied. Recent research has shown that Zero-Order Optimization (ZOO) in VFL can effectively conceal the internal information of the model without adding costly privacy protective add-ons, making it a promising approach for privacy and efficiency. However, there are still two key problems that have yet to be resolved. First, the convergence rate of ZOO-based VFL is significantly slower compared to gradient-based VFL, resulting in low efficiency in model training and more communication round, which hinders its application on large neural networks. Second, although ZOO-based VFL has demonstrated resistance to state-of-the-art (SOTA) attacks, its privacy guarantee lacks a theoretical explanation. To address these challenges, we propose a novel cascaded hybrid optimization approach that employs a zeroth-order (ZO) gradient on the most critical output layer of the clients, with other parts utilizing the first-order (FO) gradient. This approach preserves the privacy protection of ZOO while significantly enhancing convergence. Moreover, we theoretically prove that applying ZOO to the VFL is equivalent to adding Gaussian Mechanism to the gradient information, which offers an implicit differential privacy guarantee. Experimental results demonstrate that our proposed framework achieves similar utility as the Gaussian mechanism under the same privacy budget, while also having significantly lower communication costs compared with SOTA communication-efficient VFL frameworks.","['Vertical Federated Learning', 'Zeroth Order Optimization', 'Communication Efficiency', 'Privacy']",[],"['Ganyu Wang', 'Bin Gu', 'Qingsong Zhang', 'Xiang Li', 'Boyu Wang', 'Charles Ling']","['Western University', 'Mohamed bin Zayed University of Artificial Intelligence', 'Xidian University', 'University of Western Ontario', 'University of Western Ontario', 'Western University']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72662,Security,Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling,"Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling  scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation that is generally applicable without requiring changes to the underlying model and consistently learns the data generating parameter. We show that $\beta$D-Bayes produces more precise inference estimation for the same privacy guarantees, and further facilitates differentially private estimation of complex classifiers, and continuous regression models such as neural networks, which goes beyond what has been currently possible with private posterior sampling.","['differential privacy', 'beta-divergence', 'posterior sampling', 'generalised Bayesian inference']",[],"['Jack Jewson', 'Christopher C. Holmes']","['Universitat Pompeu Fabra', 'University of Oxford']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72860,Security,Generalization in the Face of Adaptivity: A Bayesian Perspective,"Repeated use of a data sample via adaptively chosen queries can rapidly lead to overfitting, wherein the empirical evaluation of queries on the sample significantly deviates from their mean with respect to the underlying data distribution. It turns out that simple noise addition algorithms suffice to prevent this issue, and differential privacy-based analysis of these algorithms shows that they can handle an asymptotically optimal number of queries.  However, differential privacy's worst-case nature entails scaling such noise to the range of the queries even for highly-concentrated queries, or introducing more complex algorithms. In this paper, we prove that straightforward noise-addition algorithms already provide variance-dependent guarantees that also extend to unbounded queries. This improvement stems from a novel characterization that illuminates the core problem of adaptive data analysis. We show that the harm of adaptivity results from the covariance between the new query and a Bayes factor-based measure of how much information about the data sample was encoded in the responses given to past queries. We then leverage this characterization to introduce a new data-dependent stability notion that can bound this covariance.","['Differential Privacy', 'Adaptive Data Analysis']",[],"['Moshe Shenfeld', 'Katrina Ligett']","['Hebrew University of Jerusalem', 'California Institute of Technology']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72912,Security,Posthoc privacy guarantees for collaborative inference with modified Propose-Test-Release,"Cloud-based machine learning inference is an emerging paradigm where users query by sending their data through a service provider who runs an ML model on that data and returns back the answer. Due to increased concerns over data privacy, recent works have proposed Collaborative Inference (CI) to learn a privacy-preserving encoding of sensitive user data before it is shared with an untrusted service provider. Existing works so far evaluate the privacy of these encodings through empirical reconstruction attacks. In this work, we develop a new framework that provides formal privacy guarantees for an arbitrarily trained neural network by linking its local Lipschitz constant with its local sensitivity. To guarantee privacy using local sensitivity, we extend the Propose-Test-Release (PTR) framework to make it tractable for neural network queries. We verify the efficacy of our framework experimentally on real-world datasets and elucidate the role of Adversarial Representation Learning (ARL) in improving the privacy-utility trade-off.","['privacy', 'deep learning', 'neural networks', 'adversarial learning', 'reconstruction guarantees', 'collaborative inference', 'MLaaS']",[],"['Abhishek Singh', 'Praneeth Vepakomma', 'Vivek Sharma', 'Ramesh Raskar']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts General Hospital, Harvard University', 'Massachusetts Institute of Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72277,Security,A Privacy-Friendly Approach to Data Valuation,"Data valuation, a growing field that aims at quantifying the usefulness of individual data sources for training machine learning (ML) models, faces notable yet often overlooked privacy challenges. This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and demonstrate the significant technical challenges in adapting KNN-Shapley to accommodate differential privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a superior privacy-utility tradeoff compared to naively privatized KNN-Shapley. Moreover, even non-private TKNN-Shapley matches KNN-Shapley's performance in discerning data quality. Overall, our findings suggest that TKNN-Shapley is a promising alternative to KNN-Shapley, particularly for real-world applications involving sensitive data.","['Data Valuation', 'Differential Privacy']",[],"['Jiachen T. Wang', 'Yuqing Zhu', 'Yu-Xiang Wang', 'Ruoxi Jia', 'Prateek Mittal']","['Princeton University', 'UC Santa Barbara', 'UC Santa Barbara', 'Virginia Tech', 'Princeton University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72283,Security,GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces,"We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synthetic glucose traces with strong privacy guarantees.","['Synthetic Data', 'Time Series', 'Generative Adversarial Networks', 'Differential Privacy', 'Glucose', 'Diabetes']",[],"['Josephine Lamp', 'Mark Derdzinski', 'Joost Van der Linden', 'Lu Feng', 'Tianhao Wang', 'David Evans']","['University of Virginia, Charlottesville', 'Dexcom, Inc.', 'Dexcom, Inc.', 'University of Virginia, Charlottesville', 'University of Virginia, Charlottesville', 'University of Virginia']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72347,Security,Gaussian Differential Privacy on Riemannian Manifolds,"We develop an advanced approach for extending Gaussian Differential Privacy (GDP) to general Riemannian manifolds. The concept of GDP stands out as a prominent privacy definition that strongly warrants extension to manifold settings, due to its central limit properties. By harnessing the power of the renowned Bishop-Gromov theorem in geometric analysis, we propose a Riemannian Gaussian distribution that integrates the Riemannian distance, allowing us to achieve GDP in Riemannian manifolds with bounded Ricci curvature. To the best of our knowledge, this work marks the first instance of extending the GDP framework to accommodate general Riemannian manifolds, encompassing curved spaces, and circumventing the reliance on tangent space summaries. We provide a simple algorithm to evaluate the privacy budget $\mu$ on any one-dimensional manifold and introduce a versatile Markov Chain Monte Carlo (MCMC)-based algorithm to calculate $\mu$ on any Riemannian manifold with constant curvature. Through simulations on one of the most prevalent manifolds in statistics, the unit sphere $S^d$, we demonstrate the superior utility of our Riemannian Gaussian mechanism in comparison to the previously proposed Riemannian Laplace mechanism for implementing GDP.","['Differential Privacy', 'Gaussian Differential Privacy', 'Differential Geometry', 'Riemannian Manifold', 'Homogeneous Riemannian Manifold', 'Frechet Mean']",[],"['Yangdi Jiang', 'Xiaotian Chang', 'Yi Liu', 'Lei Ding', 'Linglong Kong', 'Bei Jiang']","['University of Alberta', 'University of Alberta', 'University of Alberta', 'University of Alberta', 'University of Alberta', 'University of Alberta']","[None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72391,Security,A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment,"The assignment of papers to reviewers is a crucial part of the peer review processes of large publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform automated paper assignment. As such, a major challenge for the organizers of these processes is to specify paper assignment algorithms that find appropriate assignments with respect to various desiderata. Although the main objective when choosing a good paper assignment is to maximize the expertise of each reviewer for their assigned papers, several other considerations make introducing randomization into the paper assignment desirable: robustness to malicious behavior, the ability to evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is unclear in what way one should randomize the paper assignment in order to best satisfy all of these considerations simultaneously. In this work, we present a practical, one-size-fits-all method for randomized paper assignment intended to perform well across different motivations for randomness. We show theoretically and experimentally that our method outperforms currently-deployed methods for randomized paper assignment on several intuitive randomness metrics, demonstrating that the randomized assignments produced by our method are general-purpose.","['peer review', 'randomized paper assignment', 'mitigating malicious behavior', 'convex optimization']",[],"['Yixuan Even Xu', 'Steven Jecmen', 'Zimeng Song', 'Fei Fang']","['Tsinghua University, Tsinghua University', 'School of Computer Science, Carnegie Mellon University', 'Institute for Interdisciplinary Information Sciences, Tsinghua University', 'Carnegie Mellon University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72431,Security,Counting Distinct Elements in the Turnstile Model with Differential Privacy under Continual Observation,"Privacy is a central challenge for systems that learn from sensitive data sets, especially when a system's outputs must be continuously updated to reflect changing data. We consider the achievable error for differentially private continual release of a basic  statistic---the number of distinct items---in a stream where items may be both inserted and deleted (the turnstile model). With only insertions, existing algorithms have additive error just polylogarithmic in the length of the stream $T$. We uncover a much richer landscape in the turnstile model, even without considering memory restrictions. We show that every differentially private mechanism that handles insertions and deletions has worst-case additive error  at least $T^{1/4}$ even under  a relatively weak, event-level privacy definition. Then, we identify a parameter of the input stream, its maximum flippancy, that is low for natural data streams and for which we give tight parameterized error guarantees. Specifically, the maximum flippancy is the largest number of times that the contribution of a single item to the distinct elements count changes over the course of the stream. We present an item-level differentially private mechanism that, for all turnstile streams with  maximum flippancy  $w$, continually outputs the number of distinct elements with an $O(\sqrt{w} \cdot \mathsf{poly}\log T)$ additive error,  without requiring prior knowledge of $w$. We prove that this is the best achievable error bound  that depends only on $w$, for a large range of values of $w$. When $w$ is small, the error of our mechanism is similar to the polylogarithmic in $T$ error in the insertion-only setting, bypassing the hardness in the turnstile model.","['distinct elements', 'differential privacy', 'continual release', 'turnstile streams']",[],"['Palak Jain', 'Sofya Raskhodnikova', 'Satchit Sivakumar', 'Adam Smith']","['Boston University, Boston University', 'Boston University, Boston University', 'Boston University', 'Boston University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72958,Security,Gaussian Membership Inference Privacy,"We propose a novel and practical privacy notion called $f$-Membership Inference Privacy ($f$-MIP), which explicitly considers the capabilities of realistic adversaries under the membership inference attack threat model.  Consequently, $f$-MIP offers interpretable privacy guarantees and improved utility (e.g., better classification accuracy). In particular, we derive a parametric family of $f$-MIP guarantees that we refer to as $\mu$-Gaussian Membership Inference Privacy ($\mu$-GMIP) by theoretically analyzing likelihood ratio-based membership inference attacks on stochastic gradient descent (SGD). Our analysis highlights that models trained with standard SGD already offer an elementary level of MIP.  Additionally, we show how $f$-MIP can be amplified by adding noise to gradient updates. Our analysis further yields an analytical membership inference attack that offers two distinct advantages over previous approaches. First, unlike existing state-of-the-art attacks that require training hundreds of shadow models, our attack does not require any shadow model.  Second, our analytical attack enables straightforward auditing of our privacy notion $f$-MIP. Finally, we quantify how various hyperparameters (e.g., batch size, number of model parameters) and specific data characteristics determine an attacker's ability to accurately infer a point's membership in the training set. We demonstrate the effectiveness of our method on models trained on vision and tabular datasets.","['Privacy', 'Membership Inference Attacks']",[],"['Tobias Leemann', 'Martin Pawelczyk', 'Gjergji Kasneci']","['Technische Universität München', 'Harvard University', 'Technische Universität München']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73719,Security,Diverse Community Data for Benchmarking Data Privacy Algorithms,"The Collaborative Research Cycle (CRC) is a National Institute of Standards and Technology (NIST) benchmarking program intended to strengthen understanding of tabular data deidentification technologies. Deidentification algorithms are vulnerable to the same bias and privacy issues that impact other data analytics and machine learning applications, and it can even amplify those issues by contaminating downstream applications. This paper summarizes four CRC contributions: theoretical work on the relationship between diverse populations and challenges for equitable deidentification; public benchmark data focused on diverse populations and challenging features; a comprehensive open source suite of evaluation metrology for deidentified datasets; and an archive of more than 450 deidentified data samples from a broad range of techniques. The initial set of evaluation results demonstrate the value of the CRC tools for investigations in this field.","['privacy', 'data deidentification', 'synthetic data', 'benchmarks', 'data evaluation']",[],"['Aniruddha Sen', 'Christine Task', 'Dhruv Kapur', 'Gary Stanley Howarth', 'Karan Bhagat']","['College of Information and Computer Science, University of Massachusetts Amherst', 'Knexus Research Corporation', 'University of Michigan - Ann Arbor', 'National Institute of Standards and Technology', 'Rochester Institute of Technology']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72701,Security,Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information,"Privacy-preserving instance encoding aims to encode raw data into feature vectors without revealing their privacy-sensitive information. When designed properly, these encodings can be used for downstream ML applications such as training and inference with limited privacy risk. However, the vast majority of existing schemes do not theoretically justify that their encoding is non-invertible, and their privacy-enhancing properties are only validated empirically against a limited set of attacks. In this paper, we propose a theoretically-principled measure for the invertibility of instance encoding based on Fisher information that is broadly applicable to a wide range of popular encoders. We show that dFIL can be used to bound the invertibility of encodings both theoretically and empirically, providing an intuitive interpretation of the privacy of instance encoding.","['privacy', 'instance encoding', 'split learning']",[],"['Kiwan Maeng', 'Chuan Guo', 'Sanjay Kariyappa', 'G. Edward Suh']","['Pennsylvania State University', 'Facebook AI Research', 'J.P. Morgan Chase', 'Meta AI']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72717,Security,Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation,"We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed order-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), exact optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the exact-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several conditions for exact optimality. We prove that one of the conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the properties of the exact-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be exact-optimal for the randomly rotated simplex codebook.","['distributed mean estimation', 'privacy', 'compression', 'communication', 'federated analytics.']",[],"['Berivan Isik', 'Wei-Ning Chen', 'Ayfer Ozgur', 'Tsachy Weissman', 'Albert No']","['Google', 'Stanford University', 'Stanford University', 'Stanford University', 'Yonsei University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72617,Security,UltraRE: Enhancing RecEraser for Recommendation Unlearning via Error Decomposition,"With growing concerns regarding privacy in machine learning models, regulations have committed to granting individuals the right to be forgotten while mandating companies to develop non-discriminatory machine learning systems, thereby fueling the study of the machine unlearning problem. Our attention is directed toward a practical unlearning scenario, i.e., recommendation unlearning. As the state-of-the-art framework, i.e., RecEraser, naturally achieves full unlearning completeness, our objective is to enhance it in terms of model utility and unlearning efficiency. In this paper, we rethink RecEraser from an ensemble-based perspective and focus on its three potential losses, i.e., redundancy, relevance, and combination. Under the theoretical guidance of the above three losses, we propose a new framework named UltraRE, which simplifies and powers RecEraser for recommendation tasks. Specifically, for redundancy loss, we incorporate transport weights in the clustering algorithm to optimize the equilibrium between collaboration and balance while enhancing efficiency; for relevance loss, we ensure that sub-models reach convergence on their respective group data; for combination loss, we simplify the combination estimator without compromising its efficacy. Extensive experiments on three real-world datasets demonstrate the effectiveness of UltraRE.","['recommendation unlearning', 'machine unlearning', 'recommender systems', 'ensemble learning']",[],"['Yuyuan Li', 'Chaochao Chen', 'Yizhao Zhang', 'Weiming Liu', 'Lingjuan Lyu', 'Xiaolin Zheng', 'Dan Meng', 'Jun Wang']","['Hangzhou Dianzi University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Sony Research', 'Zhejiang University', 'OPPO', 'OPPO Research Institute ']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73011,Security,Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints,"We consider distributed parameter estimation using interactive protocols subject to local information constraints such as bandwidth limitations, local differential privacy, and restricted measurements. We provide a unified framework enabling us to derive a variety of (tight) minimax lower bounds for different parametric families of distributions, both continuous and discrete, under any $\ell_p$ loss. Our lower bound framework is versatile and yields “plug-and-play” bounds that are widely applicable to a large range of estimation problems, and, for the prototypical case of the Gaussian family, circumvents limitations of previous techniques. In particular, our approach recovers bounds obtained using data processing inequalities and Cramér–Rao bounds, two other alternative approaches for proving lower bounds in our setting of interest. Further, for the families considered, we complement our lower bounds with matching upper bounds.",['statistical estimation; interactivity; local differential privacy; communication constraint'],[],"['Jayadev Acharya', 'Clement Louis Canonne', 'Ziteng Sun', 'Himanshu Tyagi']","['Cornell University', 'University of Sydney', 'Google', 'n Institute of Science']","[None, None, None, 'India']",,,,,,,
https://nips.cc/virtual/2023/poster/73025,Security,On Computing Pairwise Statistics with Local Differential Privacy,"We study the problem of computing pairwise statistics, i.e., ones of the form $\binom{n}{2}^{-1} \sum_{i \ne j} f(x_i, x_j)$, where $x_i$ denotes the input to the $i$th user, with differential privacy (DP) in the local model. This formulation captures important metrics such as Kendall's $\tau$ coefficient, Area Under Curve, Gini's mean difference, Gini's entropy, etc. We give several novel and generic algorithms for the problem, leveraging techniques from DP algorithms for linear queries.","['differential privacy', 'local differential privacy', 'pairwise statistics']",[],"['Badih Ghazi', 'Pritish Kamath', 'Ravi Kumar', 'Pasin Manurangsi', 'Adam Sealfon']","['Google', 'Google Research', 'Google', 'Google', 'Google']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/73581,Security,Learning Human Action Recognition Representations Without Real Humans,"Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the {\em transferability} of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with {\em humans removed} and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5\% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA.","['(Application) Computer Vision', '(Application) Privacy', 'Anonymity', 'and Security', 'Multi-task and Transfer Learning']",[],"['Howard Zhong', 'Samarth Mishra', 'Donghyun Kim', 'SouYoung Jin', 'Rameswar Panda', 'Hilde Kuehne', 'Leonid Karlinsky', 'Venkatesh Saligrama', 'Rogerio Feris']","['Massachusetts Institute of Technology', 'Boston University', 'Korea University', 'Dartmouth College', 'MIT-IBM Watson AI Lab', 'Rheinische Friedrich-Wilhelms-Universität Bonn, Rheinische Friedrich-Wilhelms Universität Bonn', 'IBM Research AI', 'Amazon', 'International Business Machines']","[None, None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72324,Security,Neural Graph Generation from Graph Statistics,"We describe a new setting for learning a deep graph generative model (GGM) from aggregate graph statistics, rather than from the graph adjacency matrix. Matching the statistics of observed training graphs is the main approach for learning traditional GGMs (e.g, BTER, Chung-Lu, and  Erdos-Renyi models). Privacy researchers have proposed learning from graph statistics as a way to protect privacy. We develop an architecture for training a deep GGM  to match statistics while preserving local differential privacy guarantees. Empirical evaluation on 8 datasets indicates that our deep GGM model generates more realistic graphs than the traditional GGMs when both are learned from graph statistics only. We also benchmark our deep GGM trained on statistics only, against state-of-the-art deep GGM models that are trained on the entire adjacency matrix. The results show that graph statistics are often sufficient to build a competitive deep GGM that generates realistic graphs while protecting local privacy.","['Graph Generation', 'Local Differential Privacy', 'Graph Statistics', 'Latent Adjacency Matrix']",[],"['Kiarash Zahirnia', 'Yaochen Hu', 'Mark Coates', 'Oliver Schulte']","['Simon Fraser University', 'Huawei Technologies Ltd.', 'McGill University', 'Simon Fraser University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71838,Security,On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence,"Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence under $\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive a lower bound on the sample complexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the Total Variation Characteristic Time. In the low-privacy regime (large $\epsilon$), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an $\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in *arm-dependent adaptive episodes* and adds *Laplace noise* to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.","['Differential Privacy', 'Multi-armed Bandits', 'Best Arm Identification', 'Fixed Confidence']",[],"['Achraf Azize', 'Marc Jourdan', 'Aymen Al Marjani', 'Debabrota Basu']","['INRIA', 'INRIA', 'ENS Lyon', 'INRIA']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71330,Security,A Randomized Approach to Tight Privacy Accounting,"Bounding privacy leakage over compositions, i.e., privacy accounting, is a key challenge in differential privacy (DP). However, the privacy parameter ($\varepsilon$ or $\delta$) is often easy to estimate but hard to bound. In this paper, we propose a new differential privacy paradigm called estimate-verify-release (EVR), which tackles the challenges of providing a strict upper bound for the privacy parameter in DP compositions by converting an *estimate* of privacy parameter into a formal guarantee. The EVR paradigm first verifies whether the mechanism meets the *estimated* privacy guarantee, and then releases the query output based on the verification result. The core component of the EVR is privacy verification. We develop a randomized privacy verifier using Monte Carlo (MC) technique. Furthermore, we propose an MC-based DP accountant that outperforms existing DP accounting techniques in terms of accuracy and efficiency. MC-based DP verifier and accountant is applicable to an important and commonly used class of DP algorithms, including the famous DP-SGD. An empirical evaluation shows the proposed EVR paradigm improves the utility-privacy tradeoff for privacy-preserving machine learning.",['Differential Privacy; Privacy Accounting'],[],"['Jiachen T. Wang', 'Saeed Mahloujifar', 'Tong Wu', 'Ruoxi Jia', 'Prateek Mittal']","['Princeton University', 'Meta', 'Princeton University', 'Virginia Tech', 'Princeton University']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71174,Security,Greedy Poisson Rejection Sampling,"One-shot channel simulation is a fundamental data compression problem concerned with encoding a single sample from a target distribution $Q$ using a coding distribution $P$ using as few bits as possible on average. Algorithms that solve this problem find applications in neural data compression and differential privacy and can serve as a more efficient and natural alternative to quantization-based methods. Unfortunately, existing solutions are too slow or have limited applicability, preventing their widespread adaptation. In this paper, we conclusively solve one-shot channel simulation for one-dimensional problems where the target-proposal density ratio is unimodal by describing an algorithm with optimal runtime. We achieve this by constructing a rejection sampling procedure equivalent to greedily searching over the points of a Poisson process. Hence, we call our algorithm greedy Poisson rejection sampling (GPRS) and analyze the correctness and time complexity of several of its variants. Finally, we empirically verify our theorems, demonstrating that GPRS significantly outperforms the current state-of-the-art method, A* coding.","['channel simulation', 'relative entropy coding', 'reverse channel coding', 'rejection sampling', 'Poisson process']",[],['Gergely Flamich'],['University of Cambridge'],[None],,,,,,,
https://nips.cc/virtual/2023/poster/70925,Security,Privacy Auditing with One (1) Training Run,"We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting. We demonstrate the effectiveness of our framework by applying it to DP-SGD, where we can achieve meaningful empirical privacy lower bounds by training only one model. In contrast, standard methods would require training hundreds of models.","['Differential privacy', 'membership inference attacks', 'privacy auditing']",[],"['Thomas Steinke', 'Milad Nasr', 'Matthew Jagielski']","['Google DeepMind', 'Google', 'Google']","[None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72304,Security,Training Private Models That Know What They Don’t Know,"Training reliable deep learning models which avoid making overconfident but incorrect predictions is a longstanding challenge. This challenge is further exacerbated when learning has to be differentially private: protection provided to sensitive data comes at the price of injecting additional randomness into the learning process. In this work, we conduct a thorough empirical investigation of selective classifiers---that can abstain under uncertainty---under a differential privacy constraint. We find that some popular selective prediction approaches are ineffective in a differentially private setting because they increase the risk of privacy leakage. At the same time, we identify that a recent approach that only uses checkpoints produced by an off-the-shelf private learning algorithm stands out as particularly suitable under DP. Further, we show that differential privacy does not just harm utility but also degrades selective classification performance. To analyze this effect across privacy levels, we propose a novel evaluation mechanism which isolates selective prediction performance across model utility levels at full coverage. Our experimental results show that recovering the performance level attainable by non-private models is possible but comes at a considerable coverage cost as the privacy budget decreases.","['differential privacy', 'selective classification', 'selective prediction', 'abstain option', 'reject option', 'uncertainty quantification', 'misclassification detection']",[],"['Stephan Rabanser', 'Anvith Thudi', 'Abhradeep Guha Thakurta', 'Krishnamurthy Dj Dvijotham', 'Nicolas Papernot']","['University of Toronto', 'University of Toronto', 'Google', 'Google DeepMind', 'University of Toronto']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72788,Security,LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference,"The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The  deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a model with minimized multiplication depth. (2) A compact node-wise polynomial replacement policy with a second-order trainable activation function, steered towards superior convergence by a two-level distillation approach from an all-ReLU based teacher model. (3) an enhanced HE solution that enables finer-grained operator fusion for node-wise activation functions, further reducing multiplication level consumption in HE-based inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that LinGCN excels in latency, accuracy, and scalability for homomorphically encrypted inference, outperforming solutions such as CryptoGCN. Remarkably, LinGCN achieves a 14.2× latency speedup relative to CryptoGCN, while preserving an inference accuracy of ~75\% and notably reducing multiplication depth. Additionally, LinGCN proves scalable for larger models, delivering a substantial 85.78\% accuracy with 6371s latency, a 10.47\% accuracy improvement over CryptoGCN.","['Privacy-Preserving Machine Learning', 'efficient private inference', 'machine learning as a service', 'homomorphic encryption', 'non-linear pruning', 'ST-GCN']",[],"['Hongwu Peng', 'Ran Ran', 'Yukui Luo', 'Jiahui Zhao', 'Shaoyi Huang', 'Kiran Thorat', 'Tong Geng', 'Chenghong Wang', 'Wujie Wen', 'Caiwen Ding']","['University of Connecticut', 'North Carolina State University', 'University of Massachusetts at Dartmouth', 'University of Connecticut', 'University of Connecticut', 'University of Connecticut', 'University of Rochester', 'na University', 'North Carolina State University', 'University of Connecticut']","[None, None, None, None, None, None, None, 'India', None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71136,Security,Anonymous and Copy-Robust Delegations for Liquid Democracy,"Liquid democracy with ranked delegations is a novel voting scheme that unites the practicability of representative democracy with the idealistic appeal of direct democracy: Every voter decides between casting their vote on a question at hand or delegating their voting weight to some other, trusted agent. Delegations are transitive, and since voters may end up in a delegation cycle, they are encouraged to indicate not only a single delegate, but a set of potential delegates and a ranking among them. Based on the delegation preferences of all voters, a delegation rule selects one representative per voter. Previous work has revealed a trade-off between two properties of delegation rules called anonymity and copy-robustness. To overcome this issue we study two fractional delegation rules: Mixed Borda branching, which generalizes a rule satisfying copy-robustness, and the random walk rule, which satisfies anonymity. Using the Markov chain tree theorem, we show that the two rules are in fact equivalent, and simultaneously satisfy generalized versions of the two properties. Combining the same theorem with Fulkerson's algorithm, we develop  a polynomial-time algorithm for computing the outcome of the studied delegation rule. This algorithm is of independent interest, having applications in semi-supervised learning and graph theory.","['liquid democracy', 'directed trees', 'parameterized markov chain', 'matrix tree theorem', 'axiomatic method']",[],"['Markus Utke', 'Ulrike Schmidt-Kraepelin']","['University of Amsterdam', 'Universidad de']","[None, 'Chile']",,,,,,,
https://nips.cc/virtual/2023/poster/72765,Security,Certified Minimax Unlearning with Generalization Rates and Deletion Capacity,"We study the problem of $(\epsilon,\delta)$-certified machine unlearning for minimax models. Most of the existing works focus on unlearning from standard statistical learning models that have a single variable and their unlearning steps hinge on the direct Hessian-based conventional Newton update. We develop a new $(\epsilon,\delta)$-certified machine unlearning algorithm for minimax models. It proposes a minimax unlearning step consisting of a total Hessian-based complete Newton update and the Gaussian mechanism borrowed from differential privacy. To obtain the unlearning certification, our method injects calibrated Gaussian noises by carefully analyzing the ''sensitivity'' of the minimax unlearning step (i.e., the closeness between the minimax unlearning variables and the retraining-from-scratch variables). We derive the generalization rates in terms of population strong and weak primal-dual risk for three different cases of loss functions, i.e., (strongly-)convex-(strongly-)concave losses. We also provide the deletion capacity to guarantee that a desired population risk can be maintained as long as the number of deleted samples does not exceed the derived amount. With training samples $n$ and model dimension $d$, it yields the order $\mathcal O(n/d^{1/4})$, which shows a strict gap over the baseline method of differentially private minimax learning that has $\mathcal O(n/d^{1/2})$. In addition, our rates of generalization and deletion capacity match the state-of-the-art rates derived previously for standard statistical learning models.","['machine unlearning', 'machin learning privacy', 'minimax learning', 'certified removal']",[],"['Jiaqi Liu', 'Jian Lou', 'Zhan Qin', 'Kui Ren']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72966,Security,Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Approach for Object Detection,"Federated Learning (FL) has emerged as a potent framework for training models across distributed data sources while maintaining data privacy. Nevertheless, it faces challenges with limited high-quality labels and non-IID client data, particularly in applications like autonomous driving. To address these hurdles, we navigate the uncharted waters of Semi-Supervised Federated Object Detection (SSFOD). We present a pioneering SSFOD framework, designed for scenarios where labeled data reside only at the server while clients possess unlabeled data. Notably, our method represents the inaugural implementation of SSFOD for clients with 0% labeled non-IID data, a stark contrast to previous studies that maintain some subset of labels at each client. We propose FedSTO, a two-stage strategy encompassing Selective Training followed by Orthogonally enhanced full-parameter training, to effectively address data shift (e.g. weather conditions) between server and clients. Our contributions include selectively refining the backbone of the detector to avert overfitting, orthogonality regularization to boost representation divergence, and local EMA-driven pseudo label assignment to yield high-quality pseudo labels. Extensive validation on prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M) attests to the efficacy of our approach, demonstrating state-of-the-art results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as well as fully-supervised centralized training methods.","['Federated Learning', 'Semi-Supervised Learning', 'Object Detection']",[],"['Taehyeon Kim', 'Eric Lin', 'Christian Lau', 'Vaikkunth Mugunthan']","['Korea Advanced Institute of Science and Technology', 'DynamoFL', 'DynamoFL', 'Massachusetts Institute of Technology']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70711,Security,Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation,"Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and $(\varepsilon, \delta)$-differential privacy (DP) constraints. We consider both the central and the multi-message shuffled DP models. We show that in order to achieve the optimal $\ell_2$ error under $(\varepsilon, \delta)$-DP, it is sufficient for each client to send $\Theta\left( n \min\left(\varepsilon, \varepsilon^2\right)\right)$ bits for FL %{\color{blue}(assuming the dimension $d \gg n \min\left(\varepsilon, \varepsilon^2\right)$)} and $\Theta\left(\log\left( n\min\left(\varepsilon, \varepsilon^2\right) \right)\right)$ bits for FA to the server, where $n$ is the number of participating clients.  Without compression, each client needs $O(d)$ bits and $O\left(\log d\right)$ bits for the mean and frequency estimation problems respectively (where $d$ corresponds to the number of trainable parameters in FL or the domain size in FA), meaning that we can get significant savings in the regime $ n \min\left(\varepsilon, \varepsilon^2\right)  = o(d)$, which is often the relevant regime in practice. We propose two different ways to leverage compression for privacy amplification and achieve the optimal privacy-communication-accuracy trade-offs. In both cases, each client communicates only partial information about its sample and we show that privacy is amplified by randomly selecting the part contributed by each client. In the first method, the random selection is revealed to the server, which results in a central DP guarantee with optimal privacy-communication-accuracy trade-offs.  In the second method, the random data parts from the clients are  shuffled by a secure shuffler resulting in a multi-message shuffling scheme with the same optimal trade-offs. As a result, we establish the optimal three-way trade-offs between privacy, communication, and accuracy for both the central DP and multi-message shuffling frameworks.","['Differential Privacy', 'Federated Learning', 'Communication']",[],"['Wei-Ning Chen', 'Dan Song', 'Ayfer Ozgur', 'Peter Kairouz']","['Stanford University', 'Stanford University', 'Stanford University', 'Google']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71399,Security,Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization,"While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a top concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called ""look-alike clustering"", which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the  Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster centers acts as a regularization and improves generalization error of the trained models. Finally, we corroborate our asymptotic theory with finite-sample numerical experiments where we observe a perfect match when the sample size is only of order of a few hundreds.","['high-dimensional regression', 'generalization error', 'asymptotic analysis', 'Convex Gaussian Minimax Theorem', 'regularization']",[],"['Adel Javanmard', 'Vahab Mirrokni']","['University of Southern California', 'Google Research']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72182,Security,Functional Renyi Differential Privacy for Generative Modeling,"Differential privacy (DP) has emerged as a rigorous notion to quantify data privacy.  Subsequently, Renyi differential privacy (RDP) becomes an alternative to the ordinary DP notion in both theoretical and empirical studies, for its convenient compositional rules and flexibility. However, most mechanisms with DP (RDP) guarantees are essentially based on randomizing a fixed, finite-dimensional vector output. In this work, following Hall et al. (2013) we further extend RDP to functional outputs, where the output space can be infinite-dimensional, and develop all necessary tools, *e.g.*, (subsampled) Gaussian mechanism, composition, and post-processing rules, to facilitate its practical adoption. As an illustration, we apply functional RDP (f-RDP) to functions in the reproducing kernel Hilbert space (RKHS) to develop a differentially private generative model (DPGM), where training can be interpreted as iteratively releasing loss functions (in an RKHS) with DP (RDP) guarantees. Empirically, the new training paradigm achieves a  significant improvement in privacy-utility trade-off compared to existing alternatives, especially when $\epsilon=0.2$. Our code is available at https://github.com/dihjiang/DP-kernel.","['Renyi differential privacy', 'RKHS', 'MMD', 'Gaussian process', 'generative model']",[],"['Dihong Jiang', 'Sun Sun', 'Yaoliang Yu']","['University of Waterloo', 'National Research Council', 'University of Waterloo']","[None, 'Canada', None]",,,,,,,
https://nips.cc/virtual/2023/poster/73636,Security,Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition,"The prevalence of violence in daily life poses significant threats to individuals' physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deployment.To address the problem, we leverage Dynamic Vision Sensors (DVS) cameras to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, encompassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10,000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valuable resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains.","['violence detection', 'privacy protection', 'event-based dataset', 'Dynamic Vision Sensors (DVS)', 'bullying']",[],"['Yiting Dong', 'Yang Li', 'Dongcheng Zhao', 'Guobin Shen', 'Yi Zeng']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences']","[None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70796,Security,Creating a Public Repository for Joining Private Data,"How can one publish a dataset with sensitive attributes in a way that both preserves privacy and enables joins with other datasets on those same sensitive attributes? This problem arises in many contexts, e.g., a hospital and an airline may want to jointly determine whether people who take long-haul flights are more likely to catch respiratory infections. If they join their data by a common keyed user identifier such as email address, they can determine the answer, though it breaks privacy.  This paper shows how the hospital can generate a private sketch and how the airline can privately join with the hospital's sketch by email address. The proposed solution satisfies pure differential privacy and gives approximate answers to linear queries and optimization problems over those joins. Whereas prior work such as secure function evaluation requires sender/receiver interaction, a distinguishing characteristic of the proposed approach is that it is non-interactive. Consequently, the sketch can be published to a repository for any organization to join with, facilitating data discovery. The accuracy of the method is demonstrated through both theoretical analysis and extensive empirical evidence.","['Differential privacy', 'machine learning', 'linear queries']",[],"['James Cook', 'Milind Shyani']","['Amazon', 'Amazon']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/72236,Security,On Differentially Private Sampling from Gaussian and Product Distributions,"We study the problem, where given a dataset of $n$ i.i.d. samples from an unknown distribution $P$, we seek to generate a sample from a distribution that is close to $P$ in total variation distance, under the constraint of differential privacy. We study the settings where $P$ is a multi-dimensional Gaussian distribution with different assumptions: known covariance, unknown bounded covariance, and unknown unbounded covariance. We present new differentially private sampling algorithms, and show that they achieve near-optimal sample complexity in the first two settings. Moreover, when $P$ is a product distribution on the binary hypercube, we obtain a pure-DP algorithm whereas only an approximate-DP algorithm (with slightly worse sample complexity) was previously known.","['privacy', 'sampling', 'Gaussian distribution', 'product distributions']",[],"['Badih Ghazi', 'Xiao Hu', 'Ravi Kumar', 'Pasin Manurangsi']","['Google', 'Google', 'Google', 'Google']","[None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/71294,Security,Offline Reinforcement Learning with Differential Privacy,"The offline reinforcement learning (RL) problem is often motivated by the need to learn data-driven decision policies in financial, legal and healthcare applications.  However, the learned policy could retain sensitive information of individuals in the training data (e.g., treatment and outcome of patients), thus susceptible to various privacy risks. We design offline RL algorithms with differential privacy guarantees which provably prevent such risks. These algorithms also enjoy strong instance-dependent learning bounds under both tabular and linear Markov Decision Process (MDP) settings. Our theory and simulation suggest that the privacy guarantee comes at (almost) no drop in utility comparing to the non-private counterpart for a medium-size dataset.","['Differential privacy', 'offline reinforcement learning', 'reinforcement learning theory']",[],"['Dan Qiao', 'Yu-Xiang Wang']","[', University of California, Santa Barbara', 'UC Santa Barbara']","[None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/70506,Security,Eliminating Domain Bias for Federated Learning in Representation Space,"Recently, federated learning (FL) is popular for its privacy-preserving and collaborative learning abilities. However, under statistically heterogeneous scenarios, we observe that biased data domains on clients cause a representation bias phenomenon and further degenerate generic representations during local training, i.e., the representation degeneration phenomenon. To address these issues, we propose a general framework Domain Bias Eliminator (DBE) for FL. Our theoretical analysis reveals that DBE can promote bi-directional knowledge transfer between server and client, as it reduces the domain discrepancy between server and client in representation space. Besides, extensive experiments on four datasets show that DBE can greatly improve existing FL methods in both generalization and personalization abilities. The DBE-equipped FL method can outperform ten state-of-the-art personalized FL methods by a large margin. Our code is public at https://github.com/TsingZ0/DBE.","['Federated Learning', 'Personalized Federated Learning', 'Representation', 'Knowledge Transfer']",[],"['Jianqing Zhang', 'Yang Hua', 'Jian Cao', 'Hao Wang', 'Tao Song', 'Zhengui XUE', 'Ruhui Ma', 'Haibing Guan']","['Shanghai Jiaotong University', ""Queen's University Belfast"", 'Shanghai Jiaotong University', 'Louisiana State University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiaotong University']","[None, None, None, None, None, None, None, None]",,,,,,,
https://nips.cc/virtual/2023/poster/69886,Security,Counting Distinct Elements Under Person-Level Differential Privacy,"We study the problem of counting the number of distinct elements in a dataset subject to the constraint of differential privacy. We consider the challenging setting of person-level DP (a.k.a. user-level DP) where each person may contribute an unbounded number of items and hence the sensitivity is unbounded. Our approach is to compute a bounded-sensitivity version of this query, which reduces to solving a max-flow problem. The sensitivity bound is optimized to balance the noise we must add to privatize the answer against the error of the approximation of the bounded-sensitivity query to the true number of unique elements.","['differential privacy', 'user-level privacy', 'person-level privacy', 'sensitivity']",[],['Thomas Steinke'],['Google DeepMind'],[None],,,,,,,