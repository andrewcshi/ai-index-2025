link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://doi.org/10.1145/3593013.3593994,Transparency & Explainability,Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study,"Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users’ attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users’ trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users’ preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.","['AI', 'Audit', 'Documentation', 'Label', 'Seal', 'Certification', 'Trust', 'Trustworthy', 'User study']",['Human-centered computing _ Empirical studies in HCI'],"['Nicolas Scharowski', 'Michaela Benk', 'Swen J. Kühne', 'Léane Wettstein', 'Florian Brühlmann']","['University of Basel, Center for General Psychology and Methodology', 'ETH Zurich, Mobiliar Lab for Analytics', 'Zurich University of Applied Sciences, School of Applied Psychology', 'University of Basel, Center for General Psychology and Methodology', 'University of Basel, Center for General Psychology and Methodology']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://doi.org/10.1145/3593013.3594054,Transparency & Explainability,On the Impact of Explanations on Understanding of Algorithmic Decision-Making,"Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by ""high-risk"" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a ""high-risk"" ADM system to participants and analyse their responses both inductively and deductively, using the ""six facets of understanding"" framework by Wiggins & McTighe [63]. Our findings indicate that the ""six facets"" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the ""dialogue"" modality as a valid explanation approach to increase participant engagement and interaction with the ""explainer"", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the ""six facets"" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.","['XAI', 'learning Sciences', 'algorithmic decision-making', 'algorithmic fairness', 'qualitative methods']",['Human-centered computing _ Field studies'],"['Timothée Schmude', 'Laura Koesten', 'Torsten Möller', 'Sebastian Tschiatschek']","['Faculty of Computer Science, Research Network Data Science, UniVie Doctoral School Computer Science DoCS, University of Vienna', 'Faculty of Computer Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Data Mining and Machine Learning, University of Vienna']","['Austria', 'Austria', 'Austria', 'Austria']"
https://doi.org/10.1145/3593013.3594003,Transparency & Explainability,Simplicity Bias Leads to Amplified Performance Disparities,"Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.","['neural networks', 'simplicity bias', 'performance disparities', 'fairness']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation']","['Samuel James Bell', 'Levent Sagun']","['FAIR, Meta AI', 'FAIR, Meta AI']","['France', 'France']"
https://doi.org/10.1145/3593013.3594084,Transparency & Explainability,A Sociotechnical Audit: Assessing Police Use of Facial Recognition,"Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.","['algorithmic audits', 'accountability', 'ethical and legal considerations', 'facial recognition technology']","['Social and professional topics _ Surveillance', 'Social and professional topics _ Technology audits', 'Security and privacy _ Human and societal aspects of security and privacy']","['Evani Radiya-Dixit', 'Gina Neff']","['Minderoo Centre for Technology and Democracy, University of Cambridge', 'Minderoo Centre for Technology and Democracy, University of Cambridge']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594090,Transparency & Explainability,Navigating the Audit Landscape: A Framework for Developing Transparent and Auditable XR,"“Extended reality” (XR) systems work to blend the physical and digital worlds. This means that XR is highly contextual: its functionality, operation and therefore consequences are driven by a tight, run-time coupling of the technology, the user, and their physical environment. It follows that XR brings particular challenges regarding transparency and accountability, given that it can be difficult to foresee and mitigate all potential issues that might arise from using such systems, given their many potential contexts of use. Further, the physicality of XR can directly result in injury, property damage, or worse, in addition to the more traditionally discussed harms arising from algorithmic systems. Therefore the ability to audit the operation of XR systems is paramount – where information revealing and enabling some reconstruction of an XR system’s use, run-time behaviour, and surrounding context is important for understanding and scrutinising what happens/happened, and why.  Towards this, we present a framework to support those involved in developing XR systems to make them more auditable. The framework focuses on supporting the building and instrumentation of an XR system for transparency aims, elaborating key considerations regarding the capture and management of audit data during system operation. We demonstrate the framework’s efficacy with expert XR developers, who indicate the utility and need for such in practice. In all, we provide practical ways forward on, as well as seek to draw attention to, XR transparency and accountability.","['audit', 'transparency', 'accountability', 'responsibility', 'auditability', 'reviewability', 'trust', 'augmented reality', 'virtual reality', 'mixed reality']","['Human-centered computing _ Ubiquitous and mobile computing design and evaluation methods', 'Human-centered computing _ Mixed / augmented reality', 'Human-centered computing _ Virtual reality', 'Social and professional topics _ Technology audits', 'Human-centered computing _ Ubiquitous and mobile computing systems and tools']","['Chris Norval', 'Richard Cloete', 'Jatinder Singh']","['University of Cambridge', 'University of Harvard', 'University of Cambridge']","['United Kingdom', 'USA', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594079,Transparency & Explainability,AI Regulation Is (Not) All You Need,"The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems.","['AI regulation', 'algorithmic auditing', 'machine learning', 'ethical AI']","['Social and professional topics _ Governmental regulations', 'Human-centered computing']","['Laura Lucaj', 'Patrick van der Smagt', 'Djalel Benbouzid']","['Machine Learning Research Lab, Volkswagen Group', 'Machine Learning Research Lab, Volkswagen Group', 'Machine Learning Research Lab, Volkswagen Group']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594104,Transparency & Explainability,Interrogating the T in FAccT,"Fairness, accountability, and transparency are the three conceptual foundations of the FAccT conference. Transparency, however, has yet to be scrutinized to the same degree as accountability and fairness. As a result, we don't know: What does this community mean when it talks about transparency? How are we doing transparency? And to what ends? What commitments does (or should) the T in FAccT signify? This paper interrogates the T in FAccT using perspectives from critical transparency literature. Subsequently, we argue that FAccT might be better off dropping the T from its title for two reasons: (1) transparency can often be counterproductive to FAccT's primary objectives and (2) it is misleading as FAccT is mainly preoccupied with explainability rather than actual transparency. If we want to keep the T, we need to reframe how we think about and do transparency by making transparency contingent, reclaiming it from explainability, and bringing people into transparency processes.","['Transparency', 'Explainability', 'Interpretability', 'Critical Transparency Studies']","['Human-centered computing', 'Human computer interaction (HCI)', 'HCI theory', 'concepts and models', 'Transparency', 'Explainability', 'Interpretability', 'Critical Transparency Studies']","['Eric Corbett', 'Emily Denton']","['Google Research', 'Google Research']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3594019,Transparency & Explainability,Representation in AI Evaluations,"Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with ""representation"" or ""representativeness"" generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be ""representative""? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities.","['datasets', 'responsible AI', 'machine learning evaluation']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence']","['A. Stevie Bergman', 'Lisa Anne Hendricks', 'Maribeth Rauh', 'Boxi Wu', 'William Agnew', 'Markus Kunesch', 'Isabella Duan', 'Iason Gabriel', 'William Isaac']","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'University of Washington', 'DeepMind', 'University of Chicago', 'DeepMind', 'DeepMind']","['USA', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'USA', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594082,Transparency & Explainability,A Systematic Review of Ethics Disclosures in Predictive Mental Health Research,"Applied machine learning (ML) has not yet coalesced on standard practices for research ethics. For ML that predicts mental illness using social media data, ambiguous ethical standards can impact peoples’ lives because of the area’s sensitivity and material consequences on health. Transparency of current ethics practices in research is important to document decision-making and improve research practice. We present a systematic literature review of 129 studies that predict mental illness using social media data and ML, and the ethics disclosures they make in research publications. Rates of disclosure are going up over time, but this trend is slow moving – it will take another eight years for the average paper to have coverage on 75% of studied ethics categories. Certain practices are more readily adopted, or ""stickier"", over time, though we found prioritization of data-driven disclosures rather than human-centered. These inconsistently reported ethical considerations indicate a gap between what ML ethicists believe ought to be and what actually is done. We advocate for closing this gap through increased transparency of practice and formal mechanisms to support disclosure.","['ethics', 'mental health', 'systematic literature review', 'social media']","['Human-centered computing _ HCI theory', 'concepts and models', 'General and reference _ Surveys and overviews', 'Computing methodologies _ Machine learning']","['Leah Hope Ajmani', 'Stevie Chancellor', 'Bijal Mehta', 'Casey Fiesler', 'Michael Zimmer', 'Munmun De Choudhury']","['GroupLens, University of Minnesota', 'GroupLens, University of Minnesota', 'Georgetown University', 'University of Colorado Boulder', 'Marquette University', 'Georgia Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593985,Transparency & Explainability,"WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?","Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world’s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems’ fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.",[],"['Social and professional topics', 'Computing methodologies', 'Software and its engineering', 'Human-centered computing _ Human computer interaction (HCI)']","['Ali Akbar Septiandri', 'Marios Constantinides', 'Mohammad Tahaei', 'Daniele Quercia']","['Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3593972,Transparency & Explainability,How to Explain and Justify Almost Any Decision: Potential Pitfalls for Accountability in AI Decision-Making,"Discussion of the “right to an explanation” has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.","['explainable AI', 'right to an explanation', 'adversarial explanations']","['Social and professional topics _ Computing / technology policy', 'Human-centered computing _ Interactive systems and tools', 'Information systems _ Decision support systems']","['Joyce Zhou', 'Thorsten Joachims']","['Cornell University', 'Cornell University']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3594053,Transparency & Explainability,Questioning the Ability of Feature-Based Explanations to Empower Non-Experts in Robo-Advised Financial Decision-Making,"Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a “fully informed decision”. In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either “dialogic”—more textual—or “graphical”—more visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users’ trust in the recommendations of the robo-advisor, sometimes to the users’ detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations.","['explainability', 'intelligibility', 'AI regulation', 'financial inclusion']",['Human-centered computing _ Empirical studies in HCI'],"['Astrid Bertrand', 'James R. Eagan', 'Winston Maxwell']","['LTCI, Télécom Paris, Institut Polytechnique de Paris', 'LTCI, Télécom Paris, Institut Polytechnique de Paris', 'i3, CNRS, Télécom Paris, Institut Polytechnique de Paris']","['France', 'France', 'France']"
https://doi.org/10.1145/3593013.3594001,Transparency & Explainability,"Explainable AI is Dead, Long Live Explainable AI! Hypothesis-Driven Decision Support Using Evaluative AI","In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.",[],"['Computing methodologies _ Artificial intelligence', 'Human-centered computing _ HCI theory', 'concepts and models']",['Tim Miller'],['The University of Melbourne'],['Australia']
https://doi.org/10.1145/3593013.3594074,Transparency & Explainability,"Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK","Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.","['Explainable AI', 'AI policy', 'social epistemology']","['Applied computing _ Law', 'Social and professional topics _ Governmental regulations']","['Luca Nannini', 'Agathe Balayn', 'Adam Leon Smith']","['Minsait - Indra Sistemas, Spain and CiTIUS (Centro Singular de Investigación en Tecnoloxías Intelixentes), Universidade de Santiago de Compostela', 'Delft University of Technology', 'Dragonfly']","['Spain', 'Netherlands', 'Spain']"
https://doi.org/10.1145/3593013.3594031,Transparency & Explainability,Care and Coordination in Algorithmic Systems: An Economies of Worth Approach,"Algorithmic decision-making has permeated health and care domains (e.g., automated diagnoses, fall detection, caregiver staffing). Researchers have raised concerns about how these algorithms are built and how they shape fair and ethical care practices. To investigate algorithm development and understand its impact on people who provide and coordinate care, we conducted a case study of a U.S.-based senior care network and platform. We interviewed 14 technologists, 9 paid caregivers, and 7 care coordinators to explore their interactions with the platform’s algorithms. We find that technologists draw on a multitude of moral frameworks to navigate complex and contradictory demands and expectations. Despite technologists’ espoused commitments to fairness, accountability, and transparency, the platform reassembles problematic aspects of care labor. By analyzing how technologists justify their work, the problems that they claim to solve, the solutions they present, and caregivers’ and coordinators’ experiences, we advance fairness research that focuses on agency and power asymmetries in algorithmic platforms. We (1) make an empirical contribution, revealing tensions when developing and implementing algorithms and (2) provide insight into the social processes that reproduce power asymmetries in algorithmic decision-making.","['algorithms', 'care', 'coordination', 'morality', 'qualitative study']","['Human-centered computing _ Empirical studies in HCI', 'Human-centered computing _ Field studies']","['John Rudnik', 'Robin Brewer']","['School of Information, University of Michigan', 'School of Information, University of Michigan']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3593975,Transparency & Explainability,Two Reasons for Subjecting Medical AI Systems to Lower Standards than Humans,"This paper concerns the double standard debate in the ethics of AI literature. This debate revolves around the question of whether we should subject AI systems to different normative standards than humans. So far, the debate has centered around transparency. That is, the debate has focused on whether AI systems must be more transparent than humans in their decision-making processes in order for it to be morally permissible to use such systems. Some have argued that the same standards of transparency should be applied to AI systems and humans. Others have argued that we should hold AI systems to higher standards than humans in terms of transparency. In this paper, we first highlight that debates concerning double standards, which have a similar structure to those related to transparency, exist in relation to other values such as predictive accuracy. Second, we argue that when we focus on predictive accuracy, there are at least two reasons for holding AI systems to a lower standard than humans.","['Double Standard', 'Predictive Accuracy', 'Opacity', 'Cost-effectiveness', 'Speed']",[],"['Jakob Mainz', 'Lauritz Munch', 'Jens Christian Bjerring']","['Novo Nordisk', 'Aarhus University', 'Aarhus University']","['Denmark', 'Denmark', 'Denmark']"
https://doi.org/10.1145/3593013.3594038,Transparency & Explainability,Your Browsing History May Cost You: A Framework for Discovering Differential Pricing in Non-Transparent Markets,"In many online markets we “shop alone” — there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents’ attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers’ pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is $ 0.44 in the flight market and $ 0.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as $ 6.00 for flights and $ 3.00 for hotels (i.e., 15 × and 33 × the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.",[],"['General and reference _ Empirical studies', 'Information systems _ E-commerce infrastructure', 'Theory of computation _ Bayesian analysis']","['Aditya Karan', 'Naina Balepur', 'Hari Sundaram']","['Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594017,Transparency & Explainability,A Theory of Auditability for Allocation and Social Choice Mechanisms,"In centralized market mechanisms individuals may not fully observe other participants' type reports. Hence, the mechanism designer may deviate from the promised mechanism without the individuals being able to detect these deviations. In this paper, we develop a theory of auditability for allocation and social choice problems. Namely, we measure a mechanism's auditabilty by the smallest number of individuals that can jointly detect any deviation. Our theory reveals stark contrasts between prominent mechanisms' auditabilities in various applications. For priority-based allocation problems, we find that the Immediate Acceptance mechanism is maximally auditable, in a sense that any deviation can always be detected by just two individuals, whereas, on the other extreme, the Deferred Acceptance mechanism is minimally auditable, in a sense that some deviations may go undetected unless some individuals possess full information about everyone's reports. For the auctions setup, we find a similar contrast between the first-price and the second-price auction mechanisms. For voting problems, we characterize the majority voting rule as the unique most auditable anonymous voting mechanism. And finally, for the choice with affirmative action setting, we compare the auditability indices of prominent reserves mechanisms.","['auditability', 'mechanisms', 'auctions', 'allocation', 'affirmative action']","['Accountability', 'Auditing', 'Economics', 'Explainability', 'Mechanism Design', 'Social Choice']","['Aram Grigoryan', 'Markus Möller']","['University of California, San Diego', 'University of Bonn']","['USA', 'Germany']"
https://doi.org/10.1145/3593013.3594060,Transparency & Explainability,Towards Labor Transparency in Situated Computational Systems Impact Research,"Researchers seeking to examine and prevent technology-mediated harms have emphasized the importance of directly engaging with community stakeholders through participatory approaches to computational systems research. However, recent transformations in strategies of corporate capture within the tech industry pose significant challenges to established participatory practices. In this paper we extend existing critical participatory design scholarship to highlight the exploitative potential of labor relationships in community collaborations between researchers and participants. Drawing on a reflexive approach to our own experiences conducting agonistic participatory research on emerging technologies at a large technology company, we highlight the limitations of doing participatory work within such contexts by empirically illustrating how and when these relationships threaten to appropriate and alienate participant labor. We argue that a labor-conscious approach to computational systems impact research is critical for countering the commodification of inclusion and invite fellow researchers to more actively investigate such dynamics. To this end, we provide (1) a framework for documenting divisions of labor within participatory research, design, and data practices, and (2) a series of short provocations that help locate and inventory sites of extraction within participatory engagements.","['impact', 'inclusion', 'labor', 'participatory design', 'agonism', 'documentation', 'transparency']","['Human-centered computing _ Participatory design', 'Human-centered computing _ Collaborative and social computing design and evaluation methods']","['Felicia S. Jing', 'Sara E. Berger', 'Juana Catalina Becerra Sandoval']","['Responsible & Inclusive Technologies, IBM Research, USA and Department of Political Science, Johns Hopkins University', 'Responsible & Inclusive Technologies, IBM Research', 'Responsible & Inclusive Technologies, IBM Research']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594070,Transparency & Explainability,"The Dimensions of Data Labor: A Road Map for Researchers, Activists, and Policymakers to Empower Data Producers","Many recent technological advances (e.g. ChatGPT and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). However, data producers have little say in what data is captured, how it is used, or who it benefits. Organizations with the ability to access and process this data, e.g. OpenAI and Google, possess immense power in shaping the technology landscape. By synthesizing related literature that reconceptualizes the production of data for computing as “data labor”, we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data’s revenue more broadly. In doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.","['user-generated data', 'empowerment', 'data leverage']","['Human-centered computing _ HCI theory', 'concepts and models']","['Hanlin Li', 'Nicholas Vincent', 'Stevie Chancellor', 'Brent Hecht']","['University of California, Berkeley', 'University of California, Davis', 'University of Minnesota', 'Northwestern University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594039,Transparency & Explainability,Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness,"Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.","['machine learning', 'fairness', 'ethics', 'bias mitigation']",[],"['Brianna Richardson', 'Prasanna Sattigeri', 'Dennis Wei', 'Karthikeyan Natesan Ramamurthy', 'Kush Varshney', 'Amit Dhurandhar', 'Juan E. Gilbert']","['University of Florida', 'IBM', 'IBM', 'IBM', 'IBM', 'IBM', 'University of Florida']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594008,Transparency & Explainability,Domain Adaptive Decision Trees: Implications for Accuracy and Fairness,"In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.","['Decision Trees', 'Information Gain', 'Domain Adaptation', 'Covariate Shift', 'Fairness', 'folktables']","['Computing methodologies _ Classification and regression trees', 'Computing methodologies _ Learning under covariate shift', 'Computing methodologies _ Transfer learning']","['Jose M. Alvarez', 'Kristen M. Scott', 'Bettina Berendt', 'Salvatore Ruggieri']","['Scuola Normale Superiore, University of Pisa', 'KU Leuven', 'TU Berlin, Weizenbaum Institute, Germany and KU Leuven', 'University of Pisa']","['Italy', 'Belgium', 'Belgium', 'Italy']"
https://doi.org/10.1145/3593013.3594088,Transparency & Explainability,(Anti)-Intentional Harms: The Conceptual Pitfalls of Emotion AI in Education,"‘Emotion AI’ is a subset of artificial intelligence (AI) technologies that claim to be able to detect the inner emotional states of individuals by collecting biometric information such as face scans, voice recordings, and traces of physical movement. Despite their growing popularity in education, these systems have the potential to produce serious harm. In this paper, we argue that a major concern with emotion AI technologies has to do with the theories of emotion that undergird them. Most emotion AI technologies are built on the foundations of anti-intentionalist theories of human emotion, which claim that emotions can be understood as discrete, universal states that arise as automatic physiological responses. Anti-intentionalists suggest that emotions are not directed at any object, or subject to cognitive reasons. In our work, we focus on the increasing use of these technologies in education to illustrate the ways in which these anti-intentionalist systems are problematic, as they dissolve the space for pushback against the judgements they make. We argue that their use thereby contributes to harms towards children broadly centered around student disempowerment, surveillance, and classification. We then consider three alternative policy approaches to emotion AI use in schools in light of their role with this political agenda of emotion commodification, assessing each of these options—interpretability, technical reform, and non-use—for their desirability and feasibility. In doing so, we underscore the conceptual harms produced by emotion AI systems in the context of education, and the criteria by which these technologies should be judged by educators and policymakers.","['emotion', 'affect', 'emotion AI', 'education', 'socio-emotional learning (SEL)', 'Basic Emotion Theory (BET)', 'anti-intentionalism', 'affective computing', 'artificial intelligence', 'theories of emotion', 'intentionalism', 'human capital', 'digital classroom']","['Social and professional topics _ Professional topics _ Computing education _ K-12 education', 'Applied computing _ Education _ Learning management systems', 'Applied computing _ Education _ Computer-managed instruction', 'Applied computing _ Education _ E-learning']","['Nathalie DiBerardino', 'Luke Stark']","['Department of Philosophy, Western University', 'Faculty of Information and Media Studies, Western University']","['Canada', 'Canada']"
https://doi.org/10.1145/3593013.3594011,Transparency & Explainability,On the Praxes and Politics of AI Speech Emotion Recognition,"There is no scientific consensus on what is meant by “emotion” – researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment – as opposed to scientific rigor – with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.","['Emotion AI', 'speech emotion recognition', 'affective computing', 'critical study of AI', 'disability and AI', 'social critique of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Human-centered computing _ HCI theory', 'concepts and models', 'Social and professional topics _ Computing profession']",['Edward B. Kang'],"['Annenberg School for Communication and Journalism, University of Southern California']",['USA']
https://doi.org/10.1145/3593013.3594063,Transparency & Explainability,Ethical Considerations in the Early Detection of Alzheimer's Disease Using Speech and AI,"While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.","['ethics', '""Alzheimers disease""', 'speech', 'language', 'digital biomarkers', 'autonomy', 'privacy', 'welfare', 'transparency', 'fairness']","['Applied computing _ Health informatics', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Medical technologies']","['Ulla Petti', 'Rune Nyrup', 'Jeffrey M. Skopek', 'Anna Korhonen']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594014,Transparency & Explainability,Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits,"This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",[],"['Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management']","['Bogdana Rakova', 'Roel Dobbe']","['Mozilla Foundation', 'Delft University of Technology']","['USA', 'Netherlands']"
https://doi.org/10.1145/3593013.3594107,Transparency & Explainability,"Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity","Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions.  We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1","['Algorithmic Fairness', 'Education', 'Dropout Prediction', 'Transfer Learning', 'Intersectionality']","['Applied computing _ Law', 'social and behavioral sciences', 'Applied computing _ Education', 'Computing methodologies _ Machine learning']","['Joshua Gardner', 'Renzhe Yu', 'Quan Nguyen', 'Christopher Brooks', 'Rene Kizilcec']","['University of Washington', 'Teachers College, Columbia University', 'University of British Columbia', 'University of Michigan', 'Cornell University']","['USA', 'USA', 'Canada', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594065,Transparency & Explainability,"More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies","Collecting personally identifiable information (PII) on data subjects has become big business. Data brokers and data processors are part of a multi-billion-dollar industry that profits from collecting, buying, and selling consumer data. Yet there is little transparency in the data collection industry which makes it difficult to understand what types of data are being collected, used, and sold, and thus the risk to individual data subjects. In this study, we examine a large textual dataset of privacy policies from 1997-2019 in order to investigate the data collection activities of data brokers and data processors. We also develop an original lexicon of PII-related terms representing PII data types curated from legislative texts. This mesoscale analysis looks at privacy policies over time on the word, topic, and network levels to understand the stability, complexity, and sensitivity of privacy policies over time. We find that (1) privacy legislation may be correlated with changes in stability and turbulence of PII data types in privacy policies; (2) the complexity of privacy policies decreases over time and becomes more regularized; (3) sensitivity rises over time and shows spikes that appear to be correlated with events when new privacy legislation is introduced.","['Privacy', 'NLP', 'Data Privacy', 'Data Ethics', 'Privacy Policies', 'Data Science', 'Networks']","['Social and professional topics _ Privacy policies', 'Networks _ Network dynamics', 'Security and privacy _ Privacy protections']","['Juniper Lovato', 'Philip Mueller', 'Parisa Suchdev', 'Peter Dodds']","['Vermont Complex Systems Center, University of Vermont', 'Vermont Complex Systems Center, University of Vermont', 'Computer Science, University of Vermont', 'Vermont Complex Systems Center, Computer Science, University of Vermont']","['USA', 'Canada', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594086,Transparency & Explainability,Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).","['algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.']","['Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols']","['Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta']","['Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593974,Transparency & Explainability,Fairness in Machine Learning from the Perspective of Sociology of Statistics: How Machine Learning is Becoming Scientific by Turning Its Back on Metrological Realism,"We argue in this article that the integration of fairness into machine learning, or FairML, is a valuable exemplar of the politics of statistics and their ongoing transformations. Classically, statisticians sought to eliminate any trace of politics from their measurement tools. But data scientists who are developing predictive machines for social applications – are inevitably confronted with the problem of fairness. They thus face two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations. We begin by socially localizing the formation of FairML as a field of research and describing the associated epistemological framework. We then examine how researchers simultaneously think the mathematical and social construction of approaches to machine learning, following controversies around fairness metrics and their status. Thirdly and finally, we show that FairML approaches tend towards a specific form of objectivity, “trained judgement,” which is based on a reasonably partial justification from the designer of the machine – which itself comes to be politically situated as a result.","['controversy mapping', 'epistemic virtues', 'objectivity', 'social epistemology', 'sociology of quantification', 'sociology of sciences and technologies', 'situated knowledge', 'fairness in machine learning']",['Social and professional topics'],['Bilel Benbouzid'],['Université Gustave Eiffel'],['France']
https://doi.org/10.1145/3593013.3594098,Transparency & Explainability,Auditing Cross-Cultural Consistency of Human-Annotated Labels for Recommendation Systems,"Recommendation systems increasingly depend on massive human-labeled datasets; however, the human annotators hired to generate these labels increasingly come from homogeneous backgrounds. This poses an issue when downstream predictive models—based on these labels—are applied globally to a heterogeneous set of users. We study this disconnect with respect to the labels themselves, asking whether they are “consistently conceptualized” across annotators of different demographics. In a case study of video game labels, we conduct a survey on 5,174 gamers, identify a subset of inconsistently conceptualized game labels, perform causal analyses, and suggest both cultural and linguistic reasons for cross-country differences in label annotation. We further demonstrate that predictive models of game annotations perform better on global train sets as opposed to homogeneous (single-country) train sets. Finally, we provide a generalizable framework for practitioners to audit their own data annotation processes for consistent label conceptualization, and encourage practitioners to consider global inclusivity in recommendation systems starting from the early stages of annotator recruitment and data-labeling.","['label bias', 'human annotations', 'cultural bias', 'linguistic bias', 'poststratification', 'causal inference', 'recommendation systems']","['Social and professional topics _ Cultural characteristics', 'Human-centered computing _ Social recommendation']","['Rock Yuren Pang', 'Jack Cenatempo', 'Franklyn Graham', 'Bridgette Kuehn', 'Maddy Whisenant', 'Portia Botchway', 'Katie Stone Perez', 'Allison Koenecke']","['University of Washington', 'Microsoft Research', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Cornell University']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594043,Transparency & Explainability,“I Think You Might Like This”: Exploring Effects of Confidence Signal Patterns on Trust in and Reliance on Conversational Recommender Systems,"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like...” or “I think you will like...,” but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence, which is accurate, random, always low, or always high. Through semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and perceived anthropomorphism.","['human-AI interaction', 'conversational recommender systems', 'trust', 'reliance', 'confidence']","['Human-centered computing _ Empirical studies in HCI', 'Computing methodologies _ Artificial intelligence', 'Information systems _ Recommender systems']","['Marissa Radensky', 'Julie Anne Séguin', 'Jang Soo Lim', 'Kristen Olson', 'Robert Geiger']","['Paul G. Allen Center for Computer Science & Engineering, University of Washington', 'Google', 'Creative Circle', 'Google', 'Google']","['USA', 'USA', 'USA', 'United Kingdom', 'USA']"
https://doi.org/10.1145/3593013.3594009,Transparency & Explainability,Algorithmic Transparency and Accountability through Crowdsourcing: A Study of the NYC School Admission Lottery,"Algorithms are used to aid decision-making for a wide range of public policy decisions. Yet, the details of the algorithmic processes and how to interact with their systems are often inadequately communicated to stakeholders, leaving them frustrated and distrusting of the outcomes of the decisions. Transparency and accountability are critical prerequisites for building trust in the results of decisions and guaranteeing fair and equitable outcomes. Unfortunately, organizations and agencies do not have strong incentives to explain and clarify their decision processes; however, stakeholders are not powerless and can strategically combine their efforts to push for more transparency.  In this paper, I discuss the results and lessons learned from such an effort: a parent-led crowdsourcing campaign to increase transparency in the New York City school admission process. NYC famously uses a deferred-acceptance matching algorithm to assign students to schools, but families are given very little, and often wrong, information on the mechanisms of the system in which they have to participate. Furthermore, the odds of matching to specific schools depend on a complex set of priority rules and tie-breaking random (lottery) numbers, whose impact on the outcome is not made clear to students and their families, resulting in many “wasted choices” on students’ ranked lists and a high rate of unmatched students. Using the results of a crowdsourced survey of school application results, I was able to explain how random tie-breakers factored in the admission, adding clarity and transparency to the process. The results highlighted several issues and inefficiencies in the match and made the case for the need for more accountability and verification in the system.","['school matching', 'crowdsourcing', 'accountability', 'transparency']","['Social and professional topics _ Government technology policy', 'Human-centered computing _ User studies']",['Amelie Marian'],"['Computer Science, Rutgers University, New Brunswick']",['USA']
https://doi.org/10.1145/3593013.3594081,Transparency & Explainability,The Devil is in the Details: Interrogating Values Embedded in the Allegheny Family Screening Tool,"The design decisions of developers and researchers in creating algorithmic tools — like constructing variables, performing feature selection, and binning model outputs — are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST’s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as “risky” by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes “making decisions based on as much information as possible,” even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.","['Algorithm', 'audit', 'policy', 'design', 'values', 'accountability']","['Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy']","['Marissa Gerchick', 'Tobi Jegede', 'Tarak Shah', 'Ana Gutierrez', 'Sophie Beiers', 'Noam Shemtov', 'Kath Xu', 'Anjana Samant', 'Aaron Horowitz']","['American Civil Liberties Union', 'American Civil Liberties Union', 'Human Rights Data Analysis Group', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594050,Transparency & Explainability,"To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards","The EU’s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act’s hierarchy of risks, the AI systems that are likely to incur “high-risk” to health, safety, and fundamental rights are subject to the majority of the Act’s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the “core concepts” whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act’s application.","['AI Act', 'high-risk AI', 'harmonised standards', 'taxonomy', 'semantic web']","['Social and professional topics _ Governmental regulations', 'Computing methodologies _ Knowledge representation and reasoning', 'Information systems _ Resource Description Framework (RDF)']","['Delaram Golpayegani', 'Harshvardhan J. Pandit', 'Dave Lewis']","['ADAPT Centre, Trinity College Dublin', 'ADAPT Centre, Dublin City University', 'ADAPT Centre, Trinity College Dublin']","['Ireland', 'Ireland', 'Ireland']"
https://doi.org/10.1145/3593013.3594069,Transparency & Explainability,The Role of Explainable AI in the Context of the AI Act,"The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.","['explainable artificial intelligence', 'XAI', 'AI Act', 'EU regulation', 'trustworthy AI', 'transparency', 'human oversight']","['Computing methodologies _ Artificial intelligence', 'Applied computing _ Law']","['Cecilia Panigutti', 'Ronan Hamon', 'Isabelle Hupont', 'David Fernandez Llorca', 'Delia Fano Yela', 'Henrik Junklewitz', 'Salvatore Scalzo', 'Gabriele Mazzini', 'Ignacio Sanchez', 'Josep Soler Garrido', 'Emilia Gomez']","['European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission', 'European Commission', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla']","['Italy', 'Italy', 'Spain', 'Spain', 'Spain', 'Italy', 'Belgium', 'Belgium', 'Italy', 'Spain', 'Spain']"
https://doi.org/10.1145/3593013.3594067,Transparency & Explainability,Regulating ChatGPT and Other Large Generative AI Models,"Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",[],"['Social and professional topics_Computing / technology policy_Government / technology policy_Governmental regulations', 'Additional Keywords and Phrases: LGAIMs', 'LGAIM regulation', 'general-purpose AI systems', 'GPAIS', 'foundation models', 'large language models', 'LLMs', 'AI regulation', 'AI Act', 'direct AI regulation', 'data protection', 'GDPR', 'Digital Services Act', 'content moderation']","['Philipp Hacker', 'Andreas Engel', 'Marco Mauer']","['European New School of Digital Studies, European University Viadrina', 'Faculty of Law, Heidelberg University', 'Faculty of Law, Humboldt-University of Berlin, Germany and European New School of Digital Studies, European University Viadrina']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594118,Transparency & Explainability,Fairness Auditing in Urban Decisions Using LP-Based Data Combination,"Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.","['data combination', 'fairness auditing', 'urban data', 'disparate impact', 'proxy variables', 'linear programming']","['Theory of computation _ Linear programming', 'Mathematics of computing _ Distribution functions', 'Applied computing _ Multi-criterion optimization and decision-making', 'Theory of computation _ Sample complexity and generalization bounds', 'Social and professional topics _ Technology audits']","['Jingyi Yang', 'Joel Miller', 'Mesrob Ohannessian']","['Electrical and Computer Engineering, University of Illinois Chicago', 'Computer Science, University of Illinois Chicago', 'Electrical and Computer Engineering, University of Illinois Chicago']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594010,Transparency & Explainability,Rethinking Transparency as a Communicative Constellation,"In this paper we make the case for an expanded understanding of transparency. Within the now extensive FAccT literature, transparency has largely been understood in terms of explainability. While this approach has proven helpful in many contexts, it falls short of addressing some of the more fundamental issues in the development and application of machine learning, such as the epistemic limitations of predictions and the political nature of the selection of fairness criteria. In order to render machine learning systems more democratic, we argue, a broader understanding of transparency is needed. We therefore propose to view transparency as a communicative constellation that is a precondition for meaningful democratic deliberation. We discuss four perspective expansions implied by this approach and present a case study illustrating the interplay of heterogeneous actors involved in producing this constellation. Drawing from our conceptualization of transparency, we sketch implications for actor groups in different sectors of society.","['transparency', 'explainability', 'science communication', 'deliberation', 'prediction']","['Computing methodologies _ Machine learning', 'Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence']","['Florian Eyert', 'Paola Lopez']","['WZB Berlin Social Science Center, Germany and Weizenbaum Institute', 'University of Vienna, Austria and Weizenbaum Institute']","['Germany', 'Germany']"
https://doi.org/10.1145/3593013.3593991,Transparency & Explainability,Algorithmic Transparency from the South: Examining the State of Algorithmic Transparency in Chile's Public Administration Algorithms,"This paper presents the results and conclusions of the study on algorithmic transparency in public Administration and the use of automated decision systems within the State of Chile, carried out by the Public Innovation Laboratory of the Universidad Adolfo Ibáñez in alliance with the Chilean Transparency Council. In the first part we delimit the concept of algorithmic transparency, and the different considerations that can derive from this concept. We detail the information gathering procedure carried out on the use of automated decision systems in the public administration and evaluate its status according to a defined transparency framework. It then examines the state of administrative regulation and access to public information in Chile and how algorithmic transparency could be included within the current legal norms in Chile. The results of this study show that there is a use of automated decision systems in critical operations in the Chilean public Administration and that the current legal framework enables the implementation of an algorithmic transparency standard for the public administration, in a flexible, scaled way and with criteria that allow citizens to evaluate their interaction with these systems. Building on the results of this research, in 2022 the Transparency Council piloted a draft algorithmic transparency standard with seven algorithms from four public agencies. A public consultation and the publication of the final standard is expected in 2023.","['Algorithmic Transparency', 'Public Administration', 'Chile']","['Computers and Society', 'Public Policy issues', 'decision-support', 'public Administration', 'Computing in government']","['José Pablo Lapostol Piderit', 'Romina Garrido Iglesias', 'María Paz Hermosilla Cornejo']","['Universidad Adolfo Ibañez', 'Universidad Adolfo Ibañez', 'Universidad Adolfo Ibañez']","['Chile', 'Chile', 'Chile']"
https://doi.org/10.1145/3593013.3594064,Transparency & Explainability,Co-Design Perspectives on Algorithm Transparency Reporting: Guidelines and Prototypes,"Recommendation algorithms by and large determine what people see on social media. Users know little about how these algorithms work or what information they use to make their recommendations. But what exactly should platforms share with users about recommendation algorithms that would be meaningful to them? Research has looked into frameworks for explainability of algorithms as well as design features across social media platforms that can contribute to their transparency and accountability. We build on these prior efforts to explore what a recommendation algorithm transparency report may include and how it should present information to users. Through a human-centered co-design research process we result in: (1) A set of guidelines for recommendation algorithm transparency reports; (2) initial suggestions, in the form of prototypes, for more engaging and interactive forms of transparency; (3) an evaluation of these prototypes’ strengths and weaknesses, and areas of exploration for future work.",[],"['Human-centered computing _ HCI design and evaluation methods', 'User studies', 'Human-centered computing _ User centered design', 'Human-centered computing _ Participatory design', 'Human-centered computing _ Interface design prototyping']",['Michal Luria'],['Center for Democracy & Technology'],['USA']
https://doi.org/10.1145/3593013.3593977,Transparency & Explainability,Welfarist Moral Grounding for Transparent AI,"As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple – even simplistic – move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us.","['Transparency', 'Welfarism', 'Moral Theory', 'AI Ethics']","['Human-centered computing _ Collaborative and social computing theory', 'concepts and paradigms', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ Computing / technology policy']",['Devesh Narayanan'],['National University of Singapore'],['Singapore']
https://doi.org/10.1145/3593013.3594093,Transparency & Explainability,Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain,"Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.","['Co-Design', 'Document Organization', 'User-Centered Design', 'Collaborative Design']","['Human-centered computing _ Interaction paradigms', 'Human-centered computing _ User studies', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning']","['Hellina Hailu Nigatu', 'Lisa Pickoff-White', 'John Canny', 'Sarah Chasins']","['EECS, UC Berkeley', 'KQED', 'EECS, UC Berkeley', 'EECS, UC Berkeley']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594103,Transparency & Explainability,Arbitrary Decisions Are a Hidden Cost of Differentially Private Training,"Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze—both theoretically and through extensive experiments—the predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.",[],"['Computing methodologies _ Machine learning', 'Neural networks', 'Computing methodologies _ Model verification and validation', 'Security and privacy _ Privacy protections', 'Security and privacy _ Social aspects of security and privacy']","['Bogdan Kulynych', 'Hsiang Hsu', 'Carmela Troncoso', 'Flavio P. Calmon']","['SPRING Lab, EPFL', 'Harvard University', 'SPRING Lab, EPFL', 'Harvard University']","['Switzerland', 'USA', 'Switzerland', 'USA']"
https://doi.org/10.1145/3593013.3593986,Transparency & Explainability,Trustworthy AI and the Logics of Intersectional Resistance,"Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for ‘trustworthy AI’ and, from these, objective indicators of ‘trustworthiness’ for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly ‘trustworthy’ AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as ‘undeserving’, and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI—fairness, accountability, and transparency—that substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust.","['Trust', 'distrust', 'artificial intelligence', 'fairness', 'bias', 'inequality', 'intersectionality', 'accountability', 'transparency']","['Human-centered computing _ HCI theory', 'concepts and models', 'Social and professional topics _ Computing profession']","['Bran Knowles', 'Jasmine Fledderjohann', 'John T. Richards', 'Kush R. Varshney']","['Lancaster University', 'Lancaster University', 'TJ Watson Research Center, IBM', 'TJ Watson Research Center, IBM']","['United Kingdom', 'United Kingdom', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593997,Transparency & Explainability,Saliency Cards: A Framework to Characterize and Compare Saliency Methods,"Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model’s output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the saliency is calculated; sensitivity, or the relationship between the saliency and the underlying model and data; and, perceptibility, or how an end user ultimately interprets the result. By collating this information, saliency cards allow users to more holistically assess and compare the implications of different methods. Through nine semi-structured interviews with users from various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed vocabulary for discussing individual methods and allow for a more systematic selection of task-appropriate methods. Moreover, with saliency cards, we are able to analyze the research landscape in a more structured fashion to identify opportunities for new methods and evaluation metrics for unmet user needs.","['saliency cards', 'transparency', 'interpretability', 'documentation', 'saliency']","['Computing methodologies _ Machine learning', 'Software and its engineering _ Documentation', 'General and reference _ Evaluation', 'Software and its engineering _ Software evolution', 'Human-centered computing _ User studies']","['Angie Boggust', 'Harini Suresh', 'Hendrik Strobelt', 'John Guttag', 'Arvind Satyanarayan']","['CSAIL, Massachusetts Institute of Technology', 'CSAIL, Massachusetts Institute of Technology', 'IBM Research, USA and MIT-IBM Watson AI Lab', 'CSAIL, Massachusetts Institute of Technology', 'CSAIL, Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594054,Fairness & Bias,On the Impact of Explanations on Understanding of Algorithmic Decision-Making,"Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by ""high-risk"" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a ""high-risk"" ADM system to participants and analyse their responses both inductively and deductively, using the ""six facets of understanding"" framework by Wiggins & McTighe [63]. Our findings indicate that the ""six facets"" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the ""dialogue"" modality as a valid explanation approach to increase participant engagement and interaction with the ""explainer"", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the ""six facets"" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.","['XAI', 'learning Sciences', 'algorithmic decision-making', 'algorithmic fairness', 'qualitative methods']",['Human-centered computing _ Field studies'],"['Timothée Schmude', 'Laura Koesten', 'Torsten Möller', 'Sebastian Tschiatschek']","['Faculty of Computer Science, Research Network Data Science, UniVie Doctoral School Computer Science DoCS, University of Vienna', 'Faculty of Computer Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Visualization and Data Analysis, University of Vienna', 'Faculty of Computer Science, Research Network Data Science, Research Group Data Mining and Machine Learning, University of Vienna']","['Austria', 'Austria', 'Austria', 'Austria']"
https://doi.org/10.1145/3593013.3594087,Fairness & Bias,Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies,"AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",[],[],"['Vivian Lai', 'Chacha Chen', 'Alison Smith-Renner', 'Q. Vera Liao', 'Chenhao Tan']","['University of Colorado Boulder', 'University of Chicago', 'Dataminr Inc.', 'Microsoft Research', 'University of Chicago']","['USA', 'USA', 'USA', 'Canada', 'USA']"
https://doi.org/10.1145/3593013.3594007,Fairness & Bias,The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice,"The “impossibility theorem” — which is considered foundational in algorithmic fairness literature — asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner’s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when — and to what degree — fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.","['machine learning', 'fairness', 'public policy', 'responsible AI']","['Computing methodologies _ Machine learning', 'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Socio-technical systems']","['Andrew Bell', 'Lucius Bynum', 'Nazarii Drushchak', 'Tetiana Zakharchenko', 'Lucas Rosenblatt', 'Julia Stoyanovich']","['New York University', 'New York University', 'Ukrainian Catholic University', 'Ukrainian Catholic University', 'New York University', 'New York University']","['USA', 'USA', 'Ukraine', 'Ukraine', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594003,Fairness & Bias,Simplicity Bias Leads to Amplified Performance Disparities,"Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.","['neural networks', 'simplicity bias', 'performance disparities', 'fairness']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation']","['Samuel James Bell', 'Levent Sagun']","['FAIR, Meta AI', 'FAIR, Meta AI']","['France', 'France']"
https://doi.org/10.1145/3593013.3594094,Fairness & Bias,Examining Risks of Racial Biases in NLP Tools for Child Protective Services,"Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.","['NLP', 'bias', 'race', 'child protection system', 'CPS', 'text processing']",[],"['Anjalie Field', 'Amanda Coston', 'Nupoor Gandhi', 'Alexandra Chouldechova', 'Emily Putnam-Hornstein', 'David Steier', 'Yulia Tsvetkov']","['Computer Science Department, Johns Hopkins University, USA and Language Technologies Institute, Carnegie Mellon University', 'Heinz College of Information Systems and Public Policy and Machine Learning Department, Carnegie Mellon University', 'Heinz College of Information Systems and Public Policy and Machine Learning Department, Carnegie Mellon University', 'Microsoft Research NYC, USA and Heinz College of Information Systems and Public Policy, Carnegie Mellon University', 'School of Social Work, University of North Carolina at Chapel Hill', 'Heinz College of Information Systems and Public Policy, Carnegie Mellon University', 'Paul G. Allen School of Computer Science & Engineering, University of Washington']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593985,Fairness & Bias,"WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?","Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world’s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems’ fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.",[],"['Social and professional topics', 'Computing methodologies', 'Software and its engineering', 'Human-centered computing _ Human computer interaction (HCI)']","['Ali Akbar Septiandri', 'Marios Constantinides', 'Mohammad Tahaei', 'Daniele Quercia']","['Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs', 'Nokia Bell Labs']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594083,Fairness & Bias,An Empirical Analysis of Racial Categories in the Algorithmic Fairness Literature,"Recent work in algorithmic fairness has highlighted the challenge of defining racial categories for the purposes of anti-discrimination. These challenges are not new but have previously fallen to the state, which enacts race through government statistics, policies, and evidentiary standards in anti-discrimination law. Drawing on the history of state race-making, we examine how longstanding questions about the nature of race and discrimination appear within the algorithmic fairness literature. Through a content analysis of 60 papers published at FAccT between 2018 and 2020, we analyze how race is conceptualized and formalized in algorithmic fairness frameworks. We note that differing notions of race are adopted inconsistently, at times even within a single analysis. We also explore the institutional influences and values associated with these choices. While we find that categories used in algorithmic fairness work often echo legal frameworks, we demonstrate that values from academic computer science play an equally important role in the construction of racial categories. Finally, we examine the reasoning behind different operationalizations of race, finding that few papers explicitly describe their choices and even fewer justify them. We argue that the construction of racial categories is a value-laden process with significant social and political consequences for the project of algorithmic fairness. The widespread lack of justification around the operationalization of race reflects institutional norms that allow these political decisions to remain obscured within the backstage of knowledge production.","['racial categories', 'algorithmic fairness', 'state race-making']",[],"['Amina A. Abdu', 'Irene V. Pasquetto', 'Abigail Z. Jacobs']","['University of Michigan', 'University of Michigan', 'University of Michigan']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594047,Fairness & Bias,Datafication Genealogies beyond Algorithmic Fairness: Making Up Racialised Subjects,"A growing scholarship has discussed how datafication is grounded on algorithmic discrimination. However, these debates only marginally address how racialised classification or race categories are enforced through quantification and neglect its political and historical conceptualisation. In this work, we argue that literature partially fails to show that datafication reinforces racial profiling beyond the creation of racial categories as features. This article casts a new light on datafication by retracing its genealogy focusing on identification procedures in the colony and at the border. Such a genealogy foregrounds how datafication enforces racialised profiles by showing that it is part of a longer historical trajectory of modes of racialising individuals beyond algorithms and racial categories. Building on archival material, it develops this argument through two case studies. First, it focuses on the study of datafication of colonised bodies through biometrics by Francis Galton during the 19th-century. Second, it takes into account police identification procedures about unauthorised migrants, enforced by the French police at the Italian border in the 20th-century. These two cases show that although race categories as variables have been historically used to translate individuals into data, datafication processes as such also produce racialised profiles. A genealogical approach highlights continuities as well as quantitative and qualitative shifts between analogue and digital datafication. The article concludes arguing that datafication mechanisms have historically enforced legal and political measures by states in the name of science and objectivity and debates around algorithmic fairness should bring this key aspect back to the core of their critiques.","['datafication', 'genealogies', 'racialised subjects', 'classification', 'borders']","['Social and professional topics _ History of computing', 'Applied computing _ Sociology']","['Ana Valdivia', 'Martina Tazzioli']","['Oxford Internet Institute, University of Oxford', 'Politics and International Relations, Goldsmiths, University of London']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594034,Fairness & Bias,How Redundant Are Redundant Encodings? Blindness in the Wild and Racial Disparity When Race is Unobserved,"We address two emerging concerns in algorithmic fairness: (i) redundant encodings of race – the notion that machine learning models encode race with probability nearing one as the feature set grows – which is widely noted in theory, with little empirical evidence; and (ii) the lack of race and ethnicity data in many domains, where state-of-the-art remains (Naive) Bayesian Improved Surname Geocoding (BISG) that relies on name and geographic information. We leverage a novel and highly granular dataset of over 7.7 million patients’ electronic health records to provide one of the first empirical studies of redundant encodings in a realistic health care setting and examine the ability to assess health care disparities when race may be missing. First, we show that machine learning (random forest) applied to name and geographic information can improve on BISG, driven primarily by better performance in identifying minority groups. Second, contrary to theoretical concerns about redundant encodings as undercutting anti-discrimination law’s anti-classification principle, additional electronic health information provides little marginal information about race and ethnicity: race still remains measured with substantial noise. Third, we show how machine learning can enable the disaggregation of racial categories, responding to longstanding critiques of the government race reporting standard. Fourth, we show that an increasing feature set can differentially impact performance on majority and minority groups. Our findings address important questions for fairness in machine learning and algorithmic decision-making, enabling the assessment of disparities, tempering concerns about redundant encodings in one important setting, and demonstrating how bigger data can shape the accuracy of race imputations in nuanced ways.",[],[],"['Lingwei Cheng', 'Isabel O Gallegos', 'Derek Ouyang', 'Jacob Goldin', 'Dan Ho']","['Carnegie Mellon University', 'Regulation, Evaluation, and Governance Lab, Stanford University', 'Regulation, Evaluation, and Governance Lab, Stanford University', 'University of Chicago', 'Regulation, Evaluation, and Governance Lab, Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594005,Fairness & Bias,Envisioning Equitable Speech Technologies for Black Older Adults,"There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.",[],[],"['Robin N. Brewer', 'Christina Harrington', 'Courtney Heldreth']","['University of Michigan', 'Carnegie Mellon University', 'Google']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593971,Fairness & Bias,Broadening AI Ethics Narratives: An Indic Art View,"Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction–that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the ‘Natyashastra’), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability","['Indian arts', 'AI ethics']","['Social and professional topics', 'Computing methodologies _ Artificial intelligence']","['Ajay Divakaran', 'Aparna Sridhar', 'Ramya Srinivasan']","['SRI International', 'Independent Researcher', 'Fujitsu Research of America']","['USA', 'India', 'USA']"
https://doi.org/10.1145/3593013.3594024,Fairness & Bias,Invigorating Ubuntu Ethics in AI for Healthcare: Enabling Equitable Care,"The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of “I am because we are” in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly.","['algorithmic fairness', 'algorithmic trust', 'equitable care', 'relational ethics']","['Relational theory _ Ubuntu ethics', 'Relational theory~Artificial Intelligence', 'Artificial intelligence~Healthcare']","['Lameck Mbangula Amugongo', 'Nicola J. Bidwell', 'Caitlin C. Corrigan']","['Institute of Ethics in Artificial Intelligence (IEAI), Technical University of Munich, School of Social Sciences and Technology, Germany and Software Engineering, Namibia University of Science & Technology', 'International University of Management, Namibia, Namibia and University of Melbourne', 'Institute of Ethics in Artificial Intelligence (IEAI), Technical University of Munich, School of Social Sciences and Technology']","['Namibia', 'Australia', 'Germany']"
https://doi.org/10.1145/3593013.3594096,Fairness & Bias,"What's Fair Is… Fair? Presenting JustEFAB, an Ethical Framework for Operationalizing Medical Ethics and Social Justice in the Integration of Clinical Machine Learning: JustEFAB","The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.","['algorithmic bias', 'fairness', 'clinical machine learning', 'ethics', 'organizational ethics', 'justice', 'accountability', 'healthcare', 'health policy', 'safe deployment']","['Social and professional topics', 'Computing/technology policy', 'Medical information policy', 'Medical technologies']","['Melissa Mccradden', 'Oluwadara Odusi', 'Shalmali Joshi', 'Ismail Akrout', 'Kagiso Ndlovu', 'Ben Glocker', 'Gabriel Maicas', 'Xiaoxuan Liu', 'Mjaye Mazwi', 'Tee Garnett', 'Lauren Oakden-Rayner', 'Myrtede Alfred', 'Irvine Sihlahla', 'Oswa Shafei', 'Anna Goldenberg']","['The Hospital for Sick Children', 'The University of Sheffield Medical School', 'Columbia University', 'The Hospital for Sick Children', 'University of Botswana', 'Imperial College London', 'Australian Institute for Machine Learning', 'University of Birmingham', 'The Hospital for Sick Children', 'The Hospital for Sick Children', 'Australian Institute for Machine Learning', 'University of Toronto', 'University of Cape Town', 'The Hospital for Sick Children', 'The Hospital for Sick Children']","['Canada', 'United Kingdom', 'USA', 'Canada', 'Botswana', 'United Kingdom', 'Australia', 'United Kingdom', 'Canada', 'Canada', 'Australia', 'Canada', 'South Africa', 'Canada', 'Canada']"
https://doi.org/10.1145/3593013.3594102,Fairness & Bias,Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods,"Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models’ overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method’s superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.","['Federated Learning', 'Algorithmic Fairness', 'Adversarial Fairness']",[],"['Raphael Poulain', 'Mirza Farhan Bin Tarek', 'Rahmatollah Beheshti']","['University of Delaware', 'University of Delaware', 'University of Delaware']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593976,Fairness & Bias,Optimization’s Neglected Normative Commitments,"Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents’ divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.","['Optimization', 'ethics', 'modeling assumptions', 'values']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Codes of ethics', 'Social and professional topics~Socio-technical systems', 'Theory of computation~Mathematical optimization']","['Benjamin Laufer', 'Thomas Gilbert', 'Helen Nissenbaum']","['Cornell Tech', 'Cornell Tech', 'Cornell Tech']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594035,Fairness & Bias,On the Site of Predictive Justice,"Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these ‘cheap’ predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. The full version of this paper is available at http://mintresearch.org/pj.","['Philosophy', 'Algorithmic fairness', 'Predictive justice', 'Justice', 'Epistemic ethics']","['Computing methodologies _ Machine learning', 'Applied computing _ Law', 'social and behavioral sciences']","['Seth Lazar', 'Jake Stone']","['Australian National University', 'Australian National University']","['Australia', 'Australia']"
https://doi.org/10.1145/3593013.3594105,Fairness & Bias,Reducing Access Disparities in Networks Using Edge Augmentation,"In social networks, a node’s position is, in and of itself, a form of social capital. Better-positioned members not only benefit from (faster) access to diverse information, but innately have more potential influence on information spread. Structural biases often arise from network formation, and can lead to significant disparities in information access based on position. Further, processes such as link recommendation can exacerbate this inequality by relying on network structure to augment connectivity.  In this paper, we argue that one can understand and quantify this social capital through the lens of information flow in the network. In contrast to prior work, we consider the setting where all nodes may be sources of distinct information, and a node’s (dis)advantage takes into account its ability to access all information available on the network, not just that from a single source. We introduce three new measures of advantage (broadcast, influence, and control), which are quantified in terms of position in the network using access signatures – vectors that represent a node’s ability to share information with each other node in the network. We then consider the problem of improving equity by making interventions to increase the access of the least-advantaged nodes. Since all nodes are already sources of information in our model, we argue that edge augmentation is most appropriate for mitigating bias in the network structure, and frame a budgeted intervention problem for maximizing broadcast (minimum pairwise access) over the network.  Finally, we propose heuristic strategies for selecting edge augmentations and empirically evaluate their performance on a corpus of real-world social networks. We demonstrate that a small number of interventions can not only significantly increase the broadcast measure of access for the least-advantaged nodes (over 5 times more than random), but also simultaneously improve the minimum influence. Additional analysis shows that edge augmentations targeted at improving minimum pairwise access can also dramatically shrink the gap in advantage between nodes (over ) and reduce disparities between their access signatures.",['algorithmic fairness; information access; social networks; edge interventions'],"['Theory of computation _ Graph algorithms analysis', 'Information systems _ Social networks', 'Information systems~Social recommendation']","['Ashkan Bashardoust', 'Sorelle Friedler', 'Carlos Scheidegger', 'Blair D. Sullivan', 'Suresh Venkatasubramanian']","['University of Utah', 'Haverford College', 'Posit PBC', 'University of Utah', 'Brown University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594091,Fairness & Bias,Group Fairness without Demographics Using Social Networks,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, and unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering any form of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. We theoretically justify our measure by showing it is commensurate with the notion of additive decomposability in the economic inequality literature and also bound the impact of non-sensitive confounding attributes. Furthermore, we apply our measure to develop fair algorithms for classification, maximizing information access, and recommender systems. Our experimental results show that the proposed approach can reduce inequality among protected classes without knowledge of sensitive attribute labels. We conclude with a discussion of the limitations of our approach when applied in real-world settings.","['group fairness', 'social networks', 'homophily']","['Computing methodologies _ Artificial intelligence', 'Theory of computation _ Social networks']","['David Liu', 'Virginie Do', 'Nicolas Usunier', 'Maximilian Nickel']","['Northeastern University', 'FAIR, Meta AI, France and LAMSADE, PSL, Université Paris Dauphine', 'FAIR, Meta AI', 'FAIR, Meta AI']","['USA', 'France', 'France', 'USA']"
https://doi.org/10.1145/3593013.3594021,Fairness & Bias,Delayed and Indirect Impacts of Link Recommendations,"The impacts of link recommendations on social networks are challenging to evaluate, due to feedback loops between algorithmic recommendations and underlying network dynamics. Observational studies have limitations in answering causal questions; naive A/B experiments often result in biased evaluations due to unaccounted network interference and finally, existing simulations primarily employ static network models that do not take into account dynamics. Departing from existing approaches, we employ simulations to study dynamic impacts of link recommendations. Specifically, we propose an extension to the Jackson-Rogers network evolution model and investigate how link recommendations affect network evolution over time. Our experiments demonstrate that link recommendations can have surprising delayed and indirect effects on the structural properties of networks. Effects of recommendations vary in the short-term and long-term, such as the immediate reduction in degree inequality but eventual increase in degree inequality through friend-of-friend recommendations. Furthermore, even after recommendations are discontinued, their impacts can persist in the network, in part by altering natural network evolution dynamics. These results provide valuable insights into the interplay between algorithmic interventions and natural network dynamics and highlight the limitations of current evaluation paradigms.","['recommender systems', 'link recommendation', 'dynamic graphs']","['Computing methodologies _ Network science', 'Theory of computation _ Random network models', 'Mathematics of computing _ Graph algorithms']","['Han Zhang', 'Shangen Lu', 'Yixin Wang', 'Mihaela Curmei']","['University of California, Berkeley', 'Wharton School University of Pennsylvania', 'University of Michigan', 'University of California, Berkeley']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594115,Fairness & Bias,Discrimination through Image Selection by Job Advertisers on Facebook,"Targeted advertising platforms are widely used by job advertisers to reach potential employees; thus issues of discrimination due to targeting that have surfaced have received widespread attention. Advertisers could misuse targeting tools to exclude people based on gender, race, location and other protected attributes from seeing their job ads. In response to legal actions, Facebook disabled the ability for explicit targeting based on many attributes for some ad categories, including employment. Although this is a step in the right direction, prior work has shown that discrimination can take place not just due to the explicit targeting tools of the platforms, but also due to the impact of the biased ad delivery algorithm. Thus, one must look at the potential for discrimination more broadly, and not merely through the lens of the explicit targeting tools.  In this work, we propose and investigate the prevalence of a new means for discrimination in job advertising, that combines both targeting and delivery – through the disproportionate representation or exclusion of people of certain demographics in job ad images. We use the Facebook Ad Library to demonstrate the prevalence of this practice through: (1) evidence of advertisers running many campaigns using ad images of people of only one perceived gender, (2) systematic analysis for gender representation in all current ad campaigns for truck drivers and nurses, (3) longitudinal analysis of ad campaign image use by gender and race for select advertisers. After establishing that the discrimination resulting from a selective choice of people in job ad images, combined with algorithmic amplification of skews by the ad delivery algorithm, is of immediate concern, we discuss approaches and challenges for addressing it.",[],"['Social and professional topics _ Technology audits', 'Social and professional topics _ Employment issues', 'Human-centered computing _ Social media', 'Social and professional topics _ Socio-technical systems', 'Social and professional topics _ Race and ethnicity', 'Social and professional topics _ Gender', 'Information systems _ Online advertising', 'Information systems _ Display advertising']","['Varun Nagaraj Rao', 'Aleksandra Korolova']","['Princeton University', 'Princeton University']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3594080,Fairness & Bias,Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content Moderation,"In recent years, social media companies have grappled with defining and enforcing content moderation policies surrounding political content on their platforms, due in part to concerns about political bias, disinformation, and polarization. These policies have taken many forms, including disallowing political advertising, limiting the reach of political topics, fact-checking political claims, and enabling users to hide political content altogether. However, implementing these policies requires human judgement to label political content, and it is unclear how well human labelers perform at this task, or whether biases affect this process. Therefore, in this study we experimentally evaluate the feasibility and practicality of using crowd workers to identify political content, and we uncover biases that make it difficult to identify this content. Our results problematize crowds composed of seemingly interchangeable workers, and provide preliminary evidence that aggregating judgements from heterogeneous workers may help mitigate political biases. In light of these findings, we identify strategies to achieving fairer labeling outcomes, while also better supporting crowd workers at this task and potentially mitigating biases.",[],[],"['Jacob Thebault-Spieker', 'Sukrit Venkatagiri', 'Naomi Mine', 'Kurt Luther']","['Information School, University of Wisconsin - Madison', 'Center for an Informed Public, University of Washington', 'University of Wisconsin - Madison', 'Virginia Tech']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593984,Fairness & Bias,Preventing Discriminatory Decision-Making in Evolving Data Streams,"Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.","['Fairness', 'Data Stream', 'Concept Drift', 'Fairness Drift']",[],"['Zichong Wang', 'Nripsuta Saxena', 'Tongjia Yu', 'Sneha Karki', 'Tyler Zetty', 'Israat Haque', 'Shan Zhou', 'Dukka Kc', 'Ian Stockwell', 'Xuyu Wang', 'Albert Bifet', 'Wenbin Zhang']","['Michigan Technological University', 'University of Southern California', 'Columbia University', 'Michigan Technological University', 'Michigan Technological University', 'Dalhousie University', 'Michigan Technological University', 'Michigan Technological University', 'University of Maryland, Baltimore County', 'Florida International University', 'University of Waikato', 'Michigan Technological University']","['USA', 'USA', 'USA', 'USA', 'USA', 'Canada', 'USA', 'USA', 'USA', 'USA', 'New Zealand', 'USA']"
https://doi.org/10.1145/3593013.3594006,Fairness & Bias,Group-Fair Classification with Strategic Agents,"The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be “fair” under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.",[],[],"['Andrew Estornell', 'Sanmay Das', 'Yang Liu', 'Yevgeniy Vorobeychik']","['Washington University in St Louis', 'George Mason University', 'University of California Santa Cruz', 'Washington University in Saint Louis']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594039,Fairness & Bias,Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness,"Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.","['machine learning', 'fairness', 'ethics', 'bias mitigation']",[],"['Brianna Richardson', 'Prasanna Sattigeri', 'Dennis Wei', 'Karthikeyan Natesan Ramamurthy', 'Kush Varshney', 'Amit Dhurandhar', 'Juan E. Gilbert']","['University of Florida', 'IBM', 'IBM', 'IBM', 'IBM', 'IBM', 'University of Florida']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594085,Fairness & Bias,Fairer Together: Mitigating Disparate Exposure in Kemeny Rank Aggregation,"In social choice, traditional Kemeny rank aggregation combines the preferences of voters, expressed as rankings, into a single consensus ranking without consideration for how this ranking may unfairly affect marginalized groups (i.e., racial or gender). Developing fair rank aggregation methods is critical due to their societal influence in applications prioritizing job applicants, funding proposals, and scheduling medical patients. In this work, we introduce the Fair Exposure Kemeny Aggregation Problem (FairExp-kap) for combining vast and diverse voter preferences into a single ranking that is not only a suitable consensus, but ensures opportunities are not withheld from marginalized groups. In formalizing FairExp-kap, we extend the fairness of exposure notion from information retrieval to the rank aggregation context and present a complimentary metric for voter preference representation. We design algorithms for solving FairExp-kap that explicitly account for position bias, a common ranking-based concern that end-users pay more attention to higher ranked candidates. epik solves FairExp-kap exactly by incorporating non-pairwise fairness of exposure into the pairwise Kemeny optimization; while the approximate epira is a candidate swapping algorithm, that guarantees ranked candidate fairness. Utilizing comprehensive synthetic simulations and six real-world datasets, we show the efficacy of our approach illustrating that we succeed in mitigating disparate group exposure unfairness in consensus rankings, while maximally representing voter preferences.","['fair exposure rank aggregation', 'group fairness', 'rank aggregation', 'voting rules']","['Social and professional topics _ User characteristics', 'Computing methodologies']","['Kathleen Cachel', 'Elke Rundensteiner']","['Worcester Polytechnic Institute', 'Worcester Polytechnic Institute']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3594008,Fairness & Bias,Domain Adaptive Decision Trees: Implications for Accuracy and Fairness,"In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.","['Decision Trees', 'Information Gain', 'Domain Adaptation', 'Covariate Shift', 'Fairness', 'folktables']","['Computing methodologies _ Classification and regression trees', 'Computing methodologies _ Learning under covariate shift', 'Computing methodologies _ Transfer learning']","['Jose M. Alvarez', 'Kristen M. Scott', 'Bettina Berendt', 'Salvatore Ruggieri']","['Scuola Normale Superiore, University of Pisa', 'KU Leuven', 'TU Berlin, Weizenbaum Institute, Germany and KU Leuven', 'University of Pisa']","['Italy', 'Belgium', 'Belgium', 'Italy']"
https://doi.org/10.1145/3593013.3594063,Fairness & Bias,Ethical Considerations in the Early Detection of Alzheimer's Disease Using Speech and AI,"While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.","['ethics', '""Alzheimers disease""', 'speech', 'language', 'digital biomarkers', 'autonomy', 'privacy', 'welfare', 'transparency', 'fairness']","['Applied computing _ Health informatics', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Medical technologies']","['Ulla Petti', 'Rune Nyrup', 'Jeffrey M. Skopek', 'Anna Korhonen']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594014,Fairness & Bias,Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits,"This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",[],"['Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management']","['Bogdana Rakova', 'Roel Dobbe']","['Mozilla Foundation', 'Delft University of Technology']","['USA', 'Netherlands']"
https://doi.org/10.1145/3593013.3594107,Fairness & Bias,"Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity","Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions.  We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1","['Algorithmic Fairness', 'Education', 'Dropout Prediction', 'Transfer Learning', 'Intersectionality']","['Applied computing _ Law', 'social and behavioral sciences', 'Applied computing _ Education', 'Computing methodologies _ Machine learning']","['Joshua Gardner', 'Renzhe Yu', 'Quan Nguyen', 'Christopher Brooks', 'Rene Kizilcec']","['University of Washington', 'Teachers College, Columbia University', 'University of British Columbia', 'University of Michigan', 'Cornell University']","['USA', 'USA', 'Canada', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594109,Fairness & Bias,Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks,"Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.","['AI ethics', 'AI bias', 'stigma in language models', 'language models', 'representation learning', 'sentiment classification', 'prompting']",['Computing methodologies _ Natural language processing'],"['Katelyn Mei', 'Sonia Fereidooni', 'Aylin Caliskan']","['Information School, University of Washington', 'University of Washington', 'Information School, University of Washington']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594072,Fairness & Bias,Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias,"Warning: The content of this paper may be upsetting or triggering. Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d > 0.80) and sadness (d > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age] year old girl"" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.","['language-vision AI', 'generative AI', 'text-to-image generators', 'representation learning', 'AI bias', 'gender bias', 'sexualization', 'AI bias propagation', 'AI bias in applications']","['Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Learning paradigms', 'Computing methodologies _ Learning latent representations']","['Robert Wolfe', 'Yiwei Yang', 'Bill Howe', 'Aylin Caliskan']","['Information School, University of Washington', 'Information School, University of Washington', 'Information School, University of Washington', 'Information School, University of Washington']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593989,Fairness & Bias,“I Wouldn’t Say Offensive but...“: Disability-Centered Perspectives on Large Language Models,"Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.","['data annotation', 'large language models', 'algorithmic harms', 'disability representation', 'qualitative', 'artificial intelligence', 'dialog model', 'chatbot']",['Human-centered computing _ Empirical studies in accessibility'],"['Vinitha Gadiraju', 'Shaun Kane', 'Sunipa Dev', 'Alex Taylor', 'Ding Wang', 'Emily Denton', 'Robin Brewer']","['University of Colorado Boulder', 'Google Research', 'Google Research', 'City, University of London', 'Google Research', 'Google Research', 'University of Michigan']","['USA', 'USA', 'USA', 'United Kingdom', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594123,Fairness & Bias,Disparities in Text-to-Image Model Concept Possession Across Languages,"We propose the notion of conceptual possession in generative text-to-image (T2I) systems, wherein a model is considered to possess a concept if it can generate a distinctive, correct, and self-consistent population of images for a simple prompt containing that concept. We use this idea to develop a model benchmark of multilingual parity in conceptual possession across a set of almost 200 tangible nouns across 7 languages: English, Spanish, German, Chinese, Japanese, Hebrew, and Indonesian. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and that despite its simplicity our method captures the necessary conditions for the impressive “creative” generative abilities users expect from T2I models. Our benchmark will guide future work in reducing disparities across languages, improving accessibility of these technologies.","['text-to-image models', 'stable diffusion', 'dall-e', 'multilingual accessibility', 'benchmark']","['Computing methodologies _ Machine translation', 'Computing methodologies _ Language resources', 'Image representations', 'Human-centered computing _ Accessibility design and evaluation methods']","['Michael Saxon', 'William Yang Wang']","['Computer Science, University of California, Santa Barbara', 'Computer Science, University of California, Santa Barbara']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3594004,Fairness & Bias,On the Independence of Association Bias and Empirical Fairness in Language Models,"The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness—or such probes ‘into representational biases’ are said to be ‘motivated by fairness’—suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.","['Representational Bias', 'Fairness', 'Natural Language Processing']",['Computing methodologies _ Natural language processing'],"['Laura Cabello', 'Anna Katrine Jørgensen', 'Anders Søgaard']","['University of Copenhagen', 'University of Copenhagen', 'University of Copenhagen']","['Denmark', 'Denmark', 'Denmark']"
https://doi.org/10.1145/3593013.3594086,Fairness & Bias,Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).","['algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.']","['Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols']","['Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta']","['Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593982,Fairness & Bias,In the Name of Fairness: Assessing the Bias in Clinical Record De-Identification,"Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods. We further illustrate that de-identification quality is affected by polysemy in names, gender context, and clinical note characteristics. To mitigate the identified gaps, we propose a simple and method-agnostic solution by fine-tuning de-identification methods with clinical context and diverse names. Overall, it is imperative to address the bias in existing methods immediately so that downstream stakeholders can build high-quality systems to serve all demographic parties fairly.","['Fairness', 'Named Entity Recognition', 'Clinical De-identification']","['Human-centered computing _ Fairness', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Patient privacy']","['Yuxin Xiao', 'Shulammite Lim', 'Tom Joseph Pollard', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594015,Fairness & Bias,The Privacy-Bias Tradeoff: Data Minimization and Racial Disparity Assessments in U.S. Government,"An emerging concern in algorithmic fairness is the tension with privacy interests. Data minimization can restrict access to protected attributes, such as race and ethnicity, for bias assessment and mitigation. Less recognized is that for nearly 50 years, the federal government has been engaged in a large-scale experiment in data minimization, limiting (a) data sharing across federal agencies under the Privacy Act of 1974, and (b) data collection under the Paperwork Reduction Act. We document how this “privacy-bias tradeoff” has become an important battleground for fairness assessments in the U.S. government and provides rich lessons for resolving these tradeoffs. President Biden’s 2021 racial justice Executive Order 13,985 mandated that federal agencies conduct equity impact assessments (e.g., for racial disparities) of federal programs. We conduct a comprehensive assessment across high-volume claims agencies that affect many individuals, as well as all agencies filing “equity action plans,” with three findings. First, there is broad agreement in principle that equity impact assessments are important, with few parties raising privacy challenges in theory and many agencies proposing substantial efforts. Second, in practice, major agencies do not collect and may be affirmatively prohibited under the Privacy Act from linking demographic information. This has led to pathological results: until 2022, for instance, the US Dept. of Agriculture imputed race by “visual observation” when race information was not collected. Data minimization has meant that even where agencies want to acquire demographic information in principle, the legal, data infrastructure, and bureaucratic hurdles are severe. Third, we derive policy implications to address these barriers.",[],[],"['Jennifer King', 'Daniel Ho', 'Arushi Gupta', 'Victor Wu', 'Helen Webley-Brown']","['Stanford Institute for Human-Centered Artificial Intelligence, Stanford University', 'Stanford Law School, Stanford University', 'Stanford University', 'Stanford Law School, Stanford University', 'Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594000,Fairness & Bias,‘Affordances’ for Machine Learning,"The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. ‘Affordances’ are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power—and the systems power relations produce—to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.","['Affordances', 'Machine Learning', 'Design Studies', 'Mechanisms and Conditions Framework', 'AI Alignment', 'Principles-to-Practice']",['Human-centered computing _ Interaction design process and methods'],['Jenny L Davis'],"['School of Sociology, The Australian National University']",['Australia']
https://doi.org/10.1145/3593013.3594046,Fairness & Bias,"UNFair: Search Engine Manipulation, Undetectable by Amortized Inequity","Modern society increasingly relies on Information Retrieval systems to answer various information needs. Since this impacts society in many ways, there has been a great deal of work to ensure the fairness of these systems, and to prevent societal harms. There is a prevalent risk of failing to model the entire system, where nefarious actors can produce harm outside the scope of fairness metrics. We demonstrate the practical possibility of this risk through UNFair, a ranking system that achieves performance and measured fairness competitive with current state-of-the-art, while simultaneously being manipulative in setup. UNFair demonstrates how adhering to a fairness metric, Amortized Equity, can be insufficient to prevent Search Engine Manipulation. This possibility of manipulation bypassing a fairness metric discourages imposing a fairness metric ahead of time, and motivates instead a more holistic approach to fairness assessments.","['Fairness', 'Information Retrieval', 'Search Engine Manipulation Effect', 'Exposure', 'UNFair']",['Information systems _ Learning to rank'],"['Tim De Jonge', 'Djoerd Hiemstra']","['Radboud University', 'Radboud University']","['Netherlands', 'Netherlands']"
https://doi.org/10.1145/3593013.3594081,Fairness & Bias,The Devil is in the Details: Interrogating Values Embedded in the Allegheny Family Screening Tool,"The design decisions of developers and researchers in creating algorithmic tools — like constructing variables, performing feature selection, and binning model outputs — are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST’s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as “risky” by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes “making decisions based on as much information as possible,” even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.","['Algorithm', 'audit', 'policy', 'design', 'values', 'accountability']","['Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy']","['Marissa Gerchick', 'Tobi Jegede', 'Tarak Shah', 'Ana Gutierrez', 'Sophie Beiers', 'Noam Shemtov', 'Kath Xu', 'Anjana Samant', 'Aaron Horowitz']","['American Civil Liberties Union', 'American Civil Liberties Union', 'Human Rights Data Analysis Group', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union', 'American Civil Liberties Union']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594120,Fairness & Bias,Bias as Boundary Object: Unpacking The Politics Of An Austerity Algorithm Using Bias Frameworks,"Whether bias is an appropriate lens for analysis and critique remains a subject of debate among scholars. This paper contributes to this conversation by unpacking the use of bias in a critical analysis of a controversial austerity algorithm introduced by the Austrian public employment service in 2018. It was envisioned to classify the unemployed into three risk categories based on predicted prospects for re-employment. The system promised to increase efficiency and effectivity of counseling while objectifying a new austerity support measure allocation scheme. This approach was intended to cut spending for those deemed at highest risk of long term unemployment. Our in-depth analysis, based on internal documentation not available to the public, systematically traces and categorizes various problematic biases to illustrate harms to job seekers and challenge promises used to justify the adoption of the system. The classification is guided by a long-established bias framework for computer systems developed by Friedman and Nissenbaum, which provides three sensitizing basic categories. We identified in our analysis ""technical biases,"" like issues around measurement, rigidity, and coarseness of variables, ""emergent biases,"" such as disruptive events that change the labor market, and, finally, ""preexisting biases,"" like the use of variables that act as proxies for inequality.  Grounded in our case study, we argue that articulated biases can be strategically used as boundary objects to enable different actors to critically debate and challenge problematic systems without prior consensus building. We unpack benefits and risks of using bias classification frameworks to guide analysis. They have recently received increased scholarly attention and thereby may influence the identification and construction of biases. By comparing four bias frameworks and drawing on our case study, we illustrate how they are political by prioritizing certain aspects in analysis while disregarding others. Furthermore, we discuss how they vary in their granularity and how this can influence analysis. We also problematize how these frameworks tend to favor explanations for bias that center the algorithm instead of social structures. We discuss several recommendations to make bias analyses more emancipatory, arguing that biases should be seen as starting points for reflection on harmful impacts, questioning the framing imposed by the imagined “unbiased"" center that the bias is supposed to distort, and seeking out deeper explanations and histories that also center bigger social structures, power dynamics, and marginalized perspectives. Finally, we reflect on the risk that these frameworks may stabilize problematic notions of bias, for example, when they become a standard or enshrined in law.","['public employment services', 'job seeker profiling', 'algorithmic bias', 'infrastructure studies']","['Applied computing _ Computing in government', 'Human-centered computing', 'Social and professional topics _ Government technology policy', 'Computing methodologies _ Machine learning approaches']","['Gabriel Grill', 'Fabian Fischer', 'Florian Cech']","['University of Michigan', 'University of Applied Arts Vienna', 'TU Wien']","['USA', 'Austria', 'Austria']"
https://doi.org/10.1145/3593013.3594099,Fairness & Bias,The Progression of Disparities within the Criminal Justice System: Differential Enforcement and Risk Assessment Instruments,"Algorithmic risk assessment instruments (RAIs) increasingly inform decision-making in criminal justice. RAIs largely rely on arrest records as a proxy for underlying crime. Problematically, the extent to which arrests reflect overall offending can vary with the person’s characteristics. We examine how the disconnect between crime and arrest rates impacts RAIs and their evaluation. Our main contribution is a method for quantifying this bias via estimation of the amount of unobserved offenses associated with particular demographics. These unobserved offenses are then used to augment real-world arrest records to create part real, part synthetic crime records. Using this data, we estimate that four currently deployed RAIs assign 0.5–2.8 percentage points higher risk scores to Black individuals than to White individuals with a similar arrest record, but the gap grows to 4.5–11.0 percentage points when we match on the semi-synthetic crime record. We conclude by discussing the potential risks around the use of RAIs, highlighting how they may exacerbate existing inequalities if the underlying disparities of the criminal justice system are not taken into account. In light of our findings, we provide recommendations to improve the development and evaluation of such tools.",[],[],"['Miri Zilka', 'Riccardo Fogliato', 'Jiri Hron', 'Bradley Butcher', 'Carolyn Ashurst', 'Adrian Weller']","['University of Cambridge', 'Carnegie Mellon University', 'University of Cambridge', 'University of Sussex', 'The Alan Turing Institute', 'University of Cambridge, United Kingdom and The Alan Turing Institute']","['United Kingdom', 'USA', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594067,Fairness & Bias,Regulating ChatGPT and Other Large Generative AI Models,"Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",[],"['Social and professional topics_Computing / technology policy_Government / technology policy_Governmental regulations', 'Additional Keywords and Phrases: LGAIMs', 'LGAIM regulation', 'general-purpose AI systems', 'GPAIS', 'foundation models', 'large language models', 'LLMs', 'AI regulation', 'AI Act', 'direct AI regulation', 'data protection', 'GDPR', 'Digital Services Act', 'content moderation']","['Philipp Hacker', 'Andreas Engel', 'Marco Mauer']","['European New School of Digital Studies, European University Viadrina', 'Faculty of Law, Heidelberg University', 'Faculty of Law, Humboldt-University of Berlin, Germany and European New School of Digital Studies, European University Viadrina']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594078,Fairness & Bias,“I’m Fully Who I Am”: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation,"Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.","['Algorithmic Fairness', 'Natural Language Generation', 'AI Fairness Auditing', 'Queer Harms in AI']",['Computing methodologies _ Natural language generation'],"['Anaelia Ovalle', 'Palash Goyal', 'Jwala Dhamala', 'Zachary Jaggers', 'Kai-Wei Chang', 'Aram Galstyan', 'Richard Zemel', 'Rahul Gupta']","['Computer Science, University of California, Los Angeles', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon', 'Amazon Global Diversity, Equity, & Inclusion, Amazon', 'Alexa AI-NU, Amazon, USA and Department of Computer Science, University of California, Los Angeles', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594095,Fairness & Bias,Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale,"Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.",[],[],"['Federico Bianchi', 'Pratyusha Kalluri', 'Esin Durmus', 'Faisal Ladhak', 'Myra Cheng', 'Debora Nozza', 'Tatsunori Hashimoto', 'Dan Jurafsky', 'James Zou', 'Aylin Caliskan']","['Stanford University', 'Stanford University', 'Stanford University', 'Columbia University', 'Stanford University', 'Bocconi University', 'Stanford University', 'Stanford University', 'Stanford University', 'University of Washington']","['USA', 'USA', 'USA', 'USA', 'USA', 'Italy', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594116,Fairness & Bias,On The Impact of Machine Learning Randomness on Group Fairness,"Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model’s overall performance, by simply changing the data order for a single epoch.","['neural networks', 'fairness', 'randomness in training', 'evaluation']","['Computing methodologies _ Machine learning', 'General and reference _ Evaluation', 'Social and professional topics _ Computing / technology policy']","['Prakhar Ganesh', 'Hongyan Chang', 'Martin Strobel', 'Reza Shokri']","['School of Computing, National University of Singapore', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://doi.org/10.1145/3593013.3594117,Fairness & Bias,Detection and Mitigation of Algorithmic Bias via Predictive Parity,"Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP.","['algorithmic fairness', 'dependent data', 'testing for fairness', 'mitigation of bias']",[],"['Cyrus DiCiccio', 'Brian Hsu', 'Yinyin Yu', 'Preetam Nandy', 'Kinjal Basu']","['Independent', 'LinkedIn', 'LinkedIn', 'LinkedIn', 'LinkedIn']","['USA', 'USA', 'USA', 'Switzerland', 'USA']"
https://doi.org/10.1145/3593013.3594036,Fairness & Bias,Ground(Less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making,"A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on “ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the “toxicity” of online comments, or future “job performance” – predictive models target proxy labels that are readily available in existing datasets. Predictive models’ reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.","['algorithmic decision support', 'measurement', 'validity', 'causal diagrams', 'label bias', 'human-AI decision-making']","['Human-centered computing _ Human computer interaction (HCI)', 'Human-centered computing _ User studies', 'Computing methodologies _ Machine learning']","['Luke Guerdan', 'Amanda Coston', 'Zhiwei Steven Wu', 'Kenneth Holstein']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594059,Fairness & Bias,ACROCPoLis: A Descriptive Framework for Making Sense of Fairness,"Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders.",['Algorithmic fairness; socio-technical processes; social impact of AI; responsible AI'],"['Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability']","['Andrea Aler Tubella', 'Dimitri Coelho Mollo', 'Adam Dahlgren Lindström', 'Hannah Devinney', 'Virginia Dignum', 'Petter Ericson', 'Anna Jonsson', 'Timotheus Kampik', 'Tom Lenaerts', 'Julian Alfredo Mendez', 'Juan Carlos Nieves']","['Department of Computing Science, Umeå University', 'Department of Historical, Philosophical and Religious Studies, Umeå University', 'Department of Computing Science, Umeå University', 'Department of Computing Science, Umeå University', 'Department of Computing Science, Umeå University', 'Department of Computing Science, Umeå University', 'Department of Computing Science, Umeå University', 'Department of Computing Science, Umeå University, Sweden and SAP Signavio', 'Université Libre de Bruxelles, Belgium and University of California, Berkeley', 'Department of Computing Science, Umeå University', 'Department of Computing Science, Umeå University']","['Sweden', 'Sweden', 'Sweden', 'Sweden', 'Sweden', 'Sweden', 'Sweden', 'Germany', 'USA', 'Sweden', 'Sweden']"
https://doi.org/10.1145/3593013.3593983,Fairness & Bias,“How Biased Are Your Features?”: Computing Fairness Influence Functions with Global Sensitivity Analysis,"Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier’s prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm that applies variance decomposition of classifier’s prediction following local regression. Experiments demonstrate that captures FIFs of individual feature and intersectional features, provides a better approximation of bias based on FIFs, demonstrates higher correlation of FIFs with fairness interventions, and detects changes in bias due to fairness affirmative/punitive actions in the classifier.  The code is available at https://github.com/ReAILe/bias-explainer. The extended version of the paper is at https://arxiv.org/pdf/2206.00667.pdf.","['Fair Machine Learning', 'Bias', 'Explainability', 'Global Sensitivity Analysis', 'Variance Decomposition', 'Influence Functions.']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence']","['Bishwamittra Ghosh', 'Debabrota Basu', 'Kuldeep S. Meel']","['National University of Singapore', 'Equipe Scool, Univ. Lille, Inria, UMR 9189 - CRIStAL, CNRS Centrale Lille, France', 'National University of Singapore']","['Singapore', 'France', 'Singapore']"
https://doi.org/10.1145/3593013.3594101,Fairness & Bias,Counterfactual Prediction Under Outcome Measurement Error,"Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.","['algorithmic decision support', 'measurement', 'validity', 'causal inference', 'model evaluation']","['Computing methodologies _ Machine learning approaches', 'Computing methodologies _ Model verification and validation']","['Luke Guerdan', 'Amanda Coston', 'Kenneth Holstein', 'Zhiwei Steven Wu']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594042,Fairness & Bias,Gender Animus Can Still Exist Under Favorable Disparate Impact: A Cautionary Tale from Online P2P Lending,"This paper investigates gender discrimination and its underlying drivers on a prominent Chinese online peer-to-peer (P2P) lending platform. While existing studies on P2P lending focus on disparate treatment (DT), DT narrowly recognizes direct discrimination and overlooks indirect and proxy discrimination, providing an incomplete picture. In this work, we measure a broadened discrimination notion called disparate impact (DI), which encompasses any disparity in the loan’s funding rate that does not commensurate with the actual return rate. We develop a two-stage predictor substitution approach to estimate DI from observational data. Our findings reveal (i) female borrowers, given identical actual return rates, are 3.97% more likely to receive funding, (ii) at least of this DI favoring female is indirect or proxy discrimination, and (iii) DT indeed underestimates the overall female favoritism by . However, we also identify the overall female favoritism can be explained by one specific discrimination driver, rational statistical discrimination, wherein investors accurately predict the expected return rate from imperfect observations. Furthermore, female borrowers still require 2% higher expected return rate to secure funding, indicating another driver taste-based discrimination co-exists and is against female. These results altogether tell a cautionary tale: on one hand, P2P lending provides a valuable alternative credit market where the affirmative action to support female naturally emerges from the rational crowd; on the other hand, while the overall discrimination effect (both in terms of DI or DT) favors female, concerning taste-based discrimination can persist and can be obscured by other co-existing discrimination drivers, such as statistical discrimination.","['Gender Discrimination', 'Disparate Impact', 'Statistical Discrimination', 'Taste-base discrimination', 'P2P Lending']","['Human-centered computing _ Empirical studies in collaborative and social computing', 'Applied computing _ Economics']","['Xudong Shen', 'Tianhui Tan', 'Tuan Phan', 'Jussi Keppo']","['National University of Singapore', 'National University of Singapore', 'The University of Hong Kong', 'National University of Singapore']","['Singapore', 'Singapore', 'Hong Kong', 'Singapore']"
https://doi.org/10.1145/3593013.3593979,Fairness & Bias,Multi-Dimensional Discrimination in Law and Machine Learning - A Comparative Overview,"AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called “multi-dimensional discrimination” or “multi-dimensional fairness” problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.","['multi-discrimination', 'multi-fairness', 'intersectional fairness', 'sequential fairness', 'additive fairness']",[],"['Arjun Roy', 'Jan Horstmann', 'Eirini Ntoutsi']","['Institute of Computer Science, Free University of Berlin, Germany and Research Institute CODE, Bundeswehr University Munich', 'Institute for Legal Informatics, Leibniz University of Hanover', 'Research Institute CODE, Bundeswehr University Munich']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594044,Fairness & Bias,Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is Not a Decision Tree,"Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.","['EU non-discrimination law', 'algorithmic fairness', 'machine learning', 'artificial intelligence']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law']","['Hilde Weerts', 'Raphaële Xenidis', 'Fabien Tarissan', 'Henrik Palmer Olsen', 'Mykola Pechenizkiy']","['Eindhoven University of Technology', 'Sciences Po Law School', 'CNRS & ENS Paris-Saclay', 'University of Copenhagen', 'Eindhoven University of Technology']","['Netherlands', 'France', 'France', 'Denmark', 'Netherlands']"
https://doi.org/10.1145/3593013.3594121,Fairness & Bias,Legal Taxonomies of Machine Bias: Revisiting Direct Discrimination,"Previous literature on ‘fair’ machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited.","['Direct Discrimination', 'Disparate Treatment', 'Bias', 'Fairness', 'Machine Learning']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Law']","['Reuben Binns', 'Jeremias Adams-Prassl', 'Aislinn Kelly-Lyth']","['Computer Science, University of Oxford', 'Law, University of Oxford', 'Law, University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594093,Fairness & Bias,Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain,"Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.","['Co-Design', 'Document Organization', 'User-Centered Design', 'Collaborative Design']","['Human-centered computing _ Interaction paradigms', 'Human-centered computing _ User studies', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning']","['Hellina Hailu Nigatu', 'Lisa Pickoff-White', 'John Canny', 'Sarah Chasins']","['EECS, UC Berkeley', 'KQED', 'EECS, UC Berkeley', 'EECS, UC Berkeley']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594045,Fairness & Bias,On (Assessing) the Fairness of Risk Score Models,"Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such decisions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and meaningful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings – in two case studies, one in recidivism risk prediction, and one in risk of major depressive disorder (MDD) prediction.","['Algorithmic fairness', 'Risk scores', 'Ethics', 'Ranking', 'Recidivism', 'Major depressive disorder', 'Calibration']","['Computing methodologies _ Machine learning', 'Applied computing _ Health informatics', 'Applied computing _ Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy', 'Social and professional topics _ User characteristics']","['Eike Petersen', 'Melanie Ganz', 'Sune Holm', 'Aasa Feragen']","['DTU Compute, Technical University of Denmark (DTU), Denmark and Pioneer Centre for Artificial Intelligence', 'Department of Computer Science (DIKU), University of Copenhagen, Denmark and Neurobiology Research Unit (NRU), Copenhagen University Hospital (Rigshospitalet)', 'Department of Food and Resource Economics (IFRO), University of Copenhagen, Denmark and Pioneer Centre for Artificial Intelligence', 'DTU Compute, Technical University of Denmark (DTU), Denmark and Pioneer Centre for Artificial Intelligence']","['Denmark', 'Denmark', 'Denmark', 'Denmark']"
https://doi.org/10.1145/3593013.3594028,Fairness & Bias,Runtime Monitoring of Dynamic Fairness Properties,"A machine-learned system that is fair in static decision-making tasks may have biased societal impacts in the long-run. This may happen when the system interacts with humans and feedback patterns emerge, reinforcing old biases in the system and creating new biases. While existing works try to identify and mitigate long-run biases through smart system design, we introduce techniques for monitoring fairness in real time. Our goal is to build and deploy a monitor that will continuously observe a long sequence of events generated by the system in the wild, and will output, with each event, a verdict on how fair the system is at the current point in time. The advantages of monitoring are two-fold. Firstly, fairness is evaluated at run-time, which is important because unfair behaviors may not be eliminated a priori, at design-time, due to partial knowledge about the system and the environment, as well as uncertainties and dynamic changes in the system and the environment, such as the unpredictability of human behavior. Secondly, monitors are by design oblivious to how the monitored system is constructed, which makes them suitable to be used as trusted third-party fairness watchdogs. They function as computationally lightweight statistical estimators, and their correctness proofs rely on the rigorous analysis of the stochastic process that models the assumptions about the underlying dynamics of the system. We show, both in theory and experiments, how monitors can warn us (1) if a bank’s credit policy over time has created an unfair distribution of credit scores among the population, and (2) if a resource allocator’s allocation policy over time has made unfair allocations. Our experiments demonstrate that the monitors introduce very low overhead. We believe that runtime monitoring is an important and mathematically rigorous new addition to the fairness toolbox.","['algorithmic fairness', 'dynamic fairness', 'runtime monitor', 'online statistical estimator']","['Computing methodologies _ Machine learning', 'Social and professional topics _ Computing / technology policy']","['Thomas Henzinger', 'Mahyar Karimi', 'Konstantin Kueffner', 'Kaushik Mallik']","['IST Austria', 'IST Austria', 'IST Austria', 'IST Austria']","['Austria', 'Austria', 'Austria', 'Austria']"
https://doi.org/10.1145/3593013.3594068,Fairness & Bias,On the Richness of Calibration,"Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group errors, generalising previously proposed calibration scores. Complementary to such population-level scores, we explore calibration scores at the individual level and analyse their relationship to choices of grouping. We draw on these insights to introduce and axiomatise fairness deviation measures for population-level scores. We demonstrate that with appropriate choices of grouping, these novel global fairness scores can provide notions of (sub-)group or individual fairness.","['calibration', 'multicalibration', 'fairness', 'forecasting', 'evaluation']","['Computing methodologies _ Machine learning', 'Mathematics of computing _ Probability and statistics']","['Benedikt Höltgen', 'Robert C Williamson']","['University of Tübingen', 'University of Tübingen, Germany and Tübingen AI Center']","['Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594058,Fairness & Bias,Bias on Demand: A Modelling Framework That Generates Synthetic Data With Bias,"Nowadays, Machine Learning (ML) systems are widely used in various businesses and are increasingly being adopted to make decisions that can significantly impact people’s lives. However, these decision-making systems rely on data-driven learning, which poses a risk of propagating the bias embedded in the data. Despite various attempts by the algorithmic fairness community to outline different types of bias in data and algorithms, there is still a limited understanding of how these biases relate to the fairness of ML-based decision-making systems. In addition, efforts to mitigate bias and unfairness are often agnostic to the specific type(s) of bias present in the data. This paper explores the nature of fundamental types of bias, discussing their relationship to moral and technical frameworks. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced throughout the entire modelling pipeline and possibly how to mitigate it. Our primary contribution is a framework for generating synthetic datasets with different forms of biases. We use our proposed synthetic data generator to perform experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. Furthermore, we provide initial insights into mitigating specific types of bias through post-processing techniques. The implementation of the synthetic data generator and experiments can be found at https://github.com/rcrupiISP/BiasOnDemand.","['bias', 'fairness', 'synthetic data', 'moral worldviews']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Computing methodologies _ Machine learning', 'General and reference _ Metrics']","['Joachim Baumann', 'Alessandro Castelnovo', 'Riccardo Crupi', 'Nicole Inverardi', 'Daniele Regoli']","['Department of Informatics, University of Zurich, Switzerland and Zurich University of Applied Sciences', 'Data Science & Artificial Intelligence, Intesa Sanpaolo, Italy and Dept. of Informatics, Systems and Communication, University Milano Bicocca', 'Data Science & Artificial Intelligence, Intesa Sanpaolo', 'Data Science & Artificial Intelligence, Intesa Sanpaolo', 'Data Science & Artificial Intelligence, Intesa Sanpaolo']","['Switzerland', 'Italy', 'Italy', 'Italy', 'Italy']"
https://doi.org/10.1145/3593013.3593988,Fairness & Bias,The Dataset Multiplicity Problem: How Unreliable Data Impacts Predictions,"We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets’ factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label errors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predictions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted.","['Dataset multiplicity', 'procedural fairness', 'model robustness', 'data bias', 'model multiplicity']","['Computing methodologies _ Machine learning approaches', 'General and reference _ Evaluation', 'Social and professional topics _ Computing / technology policy', 'Theory of computation~Machine learning theory']","['Anna P. Meyer', 'Aws Albarghouthi', ""Loris D'Antoni""]","['Department of Computer Sciences, University of Wisconsin - Madison', 'Department of Computer Sciences, University of Wisconsin - Madison', 'Department of Computer Sciences, University of Wisconsin - Madison']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593999,Fairness & Bias,Ghosting the Machine: Judicial Resistance to a Recidivism Risk Assessment Instrument,"Recidivism risk assessment instruments are presented as an ‘evidence-based’ strategy for criminal justice reform – a way of increasing consistency in sentencing, replacing cash bail, and reducing mass incarceration. In practice, however, AI-centric reforms can simply add another layer to the sluggish, labyrinthine machinery of bureaucratic systems and are met with internal resistance. Through a community-informed interview-based study of 23 criminal judges and other criminal legal bureaucrats in Pennsylvania, I find that judges overwhelmingly ignore a recently-implemented sentence risk assessment instrument, which they disparage as “useless,” “worthless,” “boring,” “a waste of time,” “a non-thing,” and simply “not helpful.” I argue that this algorithm aversion cannot be accounted for by individuals’ distrust of the tools or automation anxieties, per the explanations given by existing scholarship. Rather, the instrument’s non-use is the result of an interplay between three organizational factors: county-level norms about pre-sentence investigation reports; alterations made to the instrument by the Pennsylvania Sentencing Commission in response to years of public and internal resistance; and problems with how information is disseminated to judges. These findings shed new light on the important role of organizational influences on professional resistance to algorithms, which helps explain why algorithm-centric reforms can fail to have their desired effect. This study also contributes to an empirically-informed argument against the use of risk assessment instruments: they are resource-intensive and have not demonstrated positive on-the-ground impacts.",['criminal justice; risk assessment instruments; algorithm aversion; human-AI interaction; community-informed'],[],['Dasha Pruss'],"['Department of History and Philosophy of Science, University of Pittsburgh']",['USA']
https://doi.org/10.1145/3593013.3594027,Fairness & Bias,Power and Resistance in the Twitter Bias Discourse,"In 2020, the saliency-based image cropping tool deployed by Twitter to generate image previews was suspected of carrying a racial bias: Twitter users complained that Black people were systematically cropped out and, thus, made invisible by the cropping tool. As a response, Twitter conducted bias analyses, concluded that the cropping tool was indeed biased, and subsequently removed it. Soon after, Twitter hosted the first ""algorithmic bias bounty challenge"", inviting the general public to detect algorithmic harm in the cropping tool.  Twitter’s image cropping algorithm is a fascinating case study for exploring the push-and-pull dynamics of power relations between, firstly, algorithmic knowledge production inherent in machine learning systems, secondly, the bias discourse as resistance, and, thirdly, ensuing corporate responses as stabilization measures towards said resistance. In order to account for this three-part narrative of the case study, this paper is structured along the examination of the following three questions: (1) How is algorithmic, and especially, data-based knowledge production entrenched in power relations? (2) In what way does the discourse around bias serve as a vehicle for resistance against said power? Why and in what way is it effective? (3) How did Twitter as a company stabilize its position within and in relation to the bias discourse?  This paper explores these questions along the following steps: Section 2 lays out the interdisciplinary theoretical perspective of the analysis, combining, firstly, a mathematical-epistemic perspective that examines the mathematics underlying both machine learning systems and bias analyses with, secondly, Foucauldian concepts that make it possible to view mathematical tools as articulations of power relations. The subsequent three sections engage with the three questions posed above: Section 3, Power, is concerned with the first question, and it focuses on the algorithmic knowledge production in relation to Twitter’s cropping tool and its mathematical-epistemic foundations. Section 4, Resistance, addresses the second question, and it examines three bias analyses of the cropping tool, as well as their epistemic limitations, and it continues by conceptualizing the bias discourse in academic scholarship and activism as resistance to power. Section 5, Stabilization, engages with the third question, discussing Twitter’s response to the bias accusations and the way in which the company was able to effectively stabilize its position – rendering the bias discourse a vehicle for counter-resistance, too. This paper will be published in the open access volume Algorithmic Regimes: Methods, Interactions, and Politics (Amsterdam University Press, forthcoming), as well as on SSRN as a preprint.","['bias', 'power', 'discourse', 'resistance', 'Twitter', 'image cropping', 'Foucault']","['Computing methodologies _ Machine learning', 'Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence']",['Paola Lopez'],"['University of Vienna, Austria and Weizenbaum Institute']",['Germany']
https://doi.org/10.1145/3593013.3594057,Fairness & Bias,Robustness Implies Fairness in Causal Algorithmic Recourse,"Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.","['explainable AI', 'algorithmic recourse', 'counterfactual explanation', 'fairness', 'robustness']","['Computing methodologies _ Causal reasoning and diagnostics', 'Information systems _ Decision support systems']","['Ahmad-Reza Ehyaei', 'Amir-Hossein Karimi', 'Bernhard Schoelkopf', 'Setareh Maghsudi']","['Department of Computer Science', 'Max Planck Institute for Intelligent Systems Tuebingen', 'Max Planck Institute for Intelligent Systems Tuebingen', 'Department of Computer Science, University of Tuebingen']","['Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594097,Fairness & Bias,Personalized Pricing with Group Fairness Constraint,"In the big data era, personalized pricing has become a popular strategy that sets different prices for the same product according to individual customers’ features. Despite its popularity among companies, this practice is controversial due to the concerns over fairness that can be potentially caused by price discrimination. In this paper, we consider the problem of single-product personalized pricing for different groups under fairness constraints. Specifically, we define group fairness constraints under different distance metrics in the personalized pricing context. We then establish a stochastic formulation that maximizes the revenue. Under the discrete price setting, we reformulate this problem as a linear program and obtain the optimal pricing policy efficiently. To bridge the gap between the discrete and continuous price setting, theoretically, we prove a general gap between the optimal revenue with continuous and discrete price set of size l. Under some mild conditions, we improve this bound to . Empirically, we demonstrate the benefits of our approach over several baseline approaches on both synthetic data and real-world data. Our results also provide managerial insights into setting a proper fairness degree as well as an appropriate size of discrete price set.","['personalized pricing', 'group fairness', 'statistical parity', 'social welfare']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Electronic commerce', 'Applied computing _ Operations research']","['Xin Chen', 'Zexing Xu', 'Zishuo Zhao', 'Yuan Zhou']","['Georgia Institute of Technology', 'University of Illinois Urbana-Champaign', 'University of Illinois Urbana-Champaign', 'Tsinghua University']","['USA', 'USA', 'USA', 'China']"
https://doi.org/10.1145/3593013.3593994,Privacy & Data Governance,Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study,"Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users’ attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users’ trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users’ preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.","['AI', 'Audit', 'Documentation', 'Label', 'Seal', 'Certification', 'Trust', 'Trustworthy', 'User study']",['Human-centered computing _ Empirical studies in HCI'],"['Nicolas Scharowski', 'Michaela Benk', 'Swen J. Kühne', 'Léane Wettstein', 'Florian Brühlmann']","['University of Basel, Center for General Psychology and Methodology', 'ETH Zurich, Mobiliar Lab for Analytics', 'Zurich University of Applied Sciences, School of Applied Psychology', 'University of Basel, Center for General Psychology and Methodology', 'University of Basel, Center for General Psychology and Methodology']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://doi.org/10.1145/3593013.3594063,Privacy & Data Governance,Ethical Considerations in the Early Detection of Alzheimer's Disease Using Speech and AI,"While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.","['ethics', '""Alzheimers disease""', 'speech', 'language', 'digital biomarkers', 'autonomy', 'privacy', 'welfare', 'transparency', 'fairness']","['Applied computing _ Health informatics', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Medical technologies']","['Ulla Petti', 'Rune Nyrup', 'Jeffrey M. Skopek', 'Anna Korhonen']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594086,Privacy & Data Governance,Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).","['algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.']","['Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols']","['Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta']","['Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594029,Privacy & Data Governance,Data Collaboratives with the Use of Decentralised Learning,"The endeavor to find appropriate data governance frameworks capable of reconciling conflicting interests in data has dramatically gained importance across disciplines and has been discussed among legal scholars, computer scientists as well as policy-makers alike. The predominant part of the current discussion is centered around the challenging task of creating a data governance framework where data is ‘as open as possible and as closed as necessary’. In this article, we elaborate on modern approaches to data governance and their limitations. It analyses how propositions evolved from property rights in data towards the creation of data access and data sharing obligations and how the corresponding debates reflect the difficulty of developing approaches that reconcile seemingly opposite objectives – such as giving individuals and businesses more control over ‘their’ data while at the same time ensuring its availability to different stakeholders. Furthermore, we propose a wider acknowledgement of data collaboratives powered by decentralised learning techniques as a possible remedy to the shortcomings of current data governance schemes. Hence, we propose a mild formalization of the set of existing technological solutions that could inform existing approaches to data governance issues. Our proposition is based on an abstractive notion of collaborative computation as well as on several principles that are essential for our definition of data collaboratives. By adopting an interdisciplinary perspective on data governance, this article highlights how innovative technological solutions can enhance control over data while at the same time ensuring its availability to other stakeholders and thereby contributing to the achievement of the policy goals of the European Strategy for Data.","['Data Governance', 'Decentralised Learning', 'Data Access', 'Data Sharing', 'European Strategy for Data']","['Collaborative and social computing theory', 'concepts and paradigms', 'Collaborative and social computing systems and tools', 'Privacy policies']","['Maciej Krzysztof Zuziak', 'Onntje Hinrichs', 'Aizhan Abdrassulova', 'Salvatore Rinzivillo']","['Knowledge Discovery and Data Mining Laboratory (KKD), National Research Council of Italy (CNR)', 'Research Group on Law, Science, Technology and Society (LSTS), Vrije Universiteit Brussel (VUB)', 'Jagiellonian University', 'Knowledge Discovery and Data Mining Laboratory (KKD), National Research Council of Italy (CNR)']","['Italy', 'Belgium', 'Poland', 'Italy']"
https://doi.org/10.1145/3593013.3594067,Privacy & Data Governance,Regulating ChatGPT and Other Large Generative AI Models,"Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",[],"['Social and professional topics_Computing / technology policy_Government / technology policy_Governmental regulations', 'Additional Keywords and Phrases: LGAIMs', 'LGAIM regulation', 'general-purpose AI systems', 'GPAIS', 'foundation models', 'large language models', 'LLMs', 'AI regulation', 'AI Act', 'direct AI regulation', 'data protection', 'GDPR', 'Digital Services Act', 'content moderation']","['Philipp Hacker', 'Andreas Engel', 'Marco Mauer']","['European New School of Digital Studies, European University Viadrina', 'Faculty of Law, Heidelberg University', 'Faculty of Law, Humboldt-University of Berlin, Germany and European New School of Digital Studies, European University Viadrina']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594103,Privacy & Data Governance,Arbitrary Decisions Are a Hidden Cost of Differentially Private Training,"Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze—both theoretically and through extensive experiments—the predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.",[],"['Computing methodologies _ Machine learning', 'Neural networks', 'Computing methodologies _ Model verification and validation', 'Security and privacy _ Privacy protections', 'Security and privacy _ Social aspects of security and privacy']","['Bogdan Kulynych', 'Hsiang Hsu', 'Carmela Troncoso', 'Flavio P. Calmon']","['SPRING Lab, EPFL', 'Harvard University', 'SPRING Lab, EPFL', 'Harvard University']","['Switzerland', 'USA', 'Switzerland', 'USA']"
https://doi.org/10.1145/3593013.3594087,Security,Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies,"AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",[],[],"['Vivian Lai', 'Chacha Chen', 'Alison Smith-Renner', 'Q. Vera Liao', 'Chenhao Tan']","['University of Colorado Boulder', 'University of Chicago', 'Dataminr Inc.', 'Microsoft Research', 'University of Chicago']","['USA', 'USA', 'USA', 'Canada', 'USA']"
https://doi.org/10.1145/3593013.3594003,Security,Simplicity Bias Leads to Amplified Performance Disparities,"Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.","['neural networks', 'simplicity bias', 'performance disparities', 'fairness']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation']","['Samuel James Bell', 'Levent Sagun']","['FAIR, Meta AI', 'FAIR, Meta AI']","['France', 'France']"
https://doi.org/10.1145/3593013.3594084,Security,A Sociotechnical Audit: Assessing Police Use of Facial Recognition,"Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.","['algorithmic audits', 'accountability', 'ethical and legal considerations', 'facial recognition technology']","['Social and professional topics _ Surveillance', 'Social and professional topics _ Technology audits', 'Security and privacy _ Human and societal aspects of security and privacy']","['Evani Radiya-Dixit', 'Gina Neff']","['Minderoo Centre for Technology and Democracy, University of Cambridge', 'Minderoo Centre for Technology and Democracy, University of Cambridge']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3593987,Security,In Her Shoes: Gendered Labelling in Crowdsourced Safety Perceptions Data from India,"In recent years, a proliferation of women’s safety mobile applications have emerged in India that crowdsource street safety perceptions to generate ‘safety maps’ used by policy makers for urban design and academics for studying mobility patterns. Men and women’s differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents’ data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.","['crowdsourced ratings', 'safety', 'gender', 'algorithmic bias', 'India']","['Human-centered computing _ HCI design and evaluation methods', 'Human-centered computing _ Empirical studies in collaborative and social computing']","['Nandana Sengupta', 'Ashwini Vaidya', 'James Evans']","['Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'University of Chicago']","['India', 'India', 'USA']"
https://doi.org/10.1145/3593013.3594096,Security,"What's Fair Is… Fair? Presenting JustEFAB, an Ethical Framework for Operationalizing Medical Ethics and Social Justice in the Integration of Clinical Machine Learning: JustEFAB","The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.","['algorithmic bias', 'fairness', 'clinical machine learning', 'ethics', 'organizational ethics', 'justice', 'accountability', 'healthcare', 'health policy', 'safe deployment']","['Social and professional topics', 'Computing/technology policy', 'Medical information policy', 'Medical technologies']","['Melissa Mccradden', 'Oluwadara Odusi', 'Shalmali Joshi', 'Ismail Akrout', 'Kagiso Ndlovu', 'Ben Glocker', 'Gabriel Maicas', 'Xiaoxuan Liu', 'Mjaye Mazwi', 'Tee Garnett', 'Lauren Oakden-Rayner', 'Myrtede Alfred', 'Irvine Sihlahla', 'Oswa Shafei', 'Anna Goldenberg']","['The Hospital for Sick Children', 'The University of Sheffield Medical School', 'Columbia University', 'The Hospital for Sick Children', 'University of Botswana', 'Imperial College London', 'Australian Institute for Machine Learning', 'University of Birmingham', 'The Hospital for Sick Children', 'The Hospital for Sick Children', 'Australian Institute for Machine Learning', 'University of Toronto', 'University of Cape Town', 'The Hospital for Sick Children', 'The Hospital for Sick Children']","['Canada', 'United Kingdom', 'USA', 'Canada', 'Botswana', 'United Kingdom', 'Australia', 'United Kingdom', 'Canada', 'Canada', 'Australia', 'Canada', 'South Africa', 'Canada', 'Canada']"
https://doi.org/10.1145/3593013.3594038,Security,Your Browsing History May Cost You: A Framework for Discovering Differential Pricing in Non-Transparent Markets,"In many online markets we “shop alone” — there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents’ attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers’ pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is $ 0.44 in the flight market and $ 0.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as $ 6.00 for flights and $ 3.00 for hotels (i.e., 15 × and 33 × the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.",[],"['General and reference _ Empirical studies', 'Information systems _ E-commerce infrastructure', 'Theory of computation _ Bayesian analysis']","['Aditya Karan', 'Naina Balepur', 'Hari Sundaram']","['Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594014,Security,Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits,"This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",[],"['Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management']","['Bogdana Rakova', 'Roel Dobbe']","['Mozilla Foundation', 'Delft University of Technology']","['USA', 'Netherlands']"
https://doi.org/10.1145/3593013.3593981,Security,The Gradient of Generative AI Release: Methods and Considerations,"As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.",[],[],['Irene Solaiman'],['Hugging Face'],['USA']
https://doi.org/10.1145/3593013.3594066,Security,Emotions and Dynamic Assemblages: A Study of Automated Social Security Using Qualitative Longitudinal Research,"In this paper we argue that qualitative longitudinal research (QLLR) is a crucial research method for studying automated decision-making (ADM) systems as complex, dynamic digital assemblages. QLLR provides invaluable insight into the lived experiences of users as data subjects of ADMs as well as into the broader digital assemblage in which these systems operate. To demonstrate the utility of this method, we draw on an ongoing, empirical study examining Universal Credit (UC), an automated social security payment used in the United Kingdom. UC is digital-by-default and uses a dynamic, means-testing payment system to determine the monthly amount of claim people are entitled to.  We first provide a brief overview of the key epistemological challenges of studying ADMs before situating our study in relation to existing qualitative analyses of ADMs and their users, as well as qualitative longitudinal research. We highlight that, thus far, QLLR has been severely under-utilized in studying ADM systems. After a brief description of our study, aims and methodology, we present our findings illustrated through empirical cases that demonstrate the potential of QLLR in this area.  Overall, we argue that QLLR provides a unique opportunity to gather information on ADMs, both over time and in real time. Capturing information real-time allows for more granular accounts and provides an opportunity for gathering in situ data on emotions and attitudes of users and data subjects. The ability to record qualitative data over time has the potential to capture dynamic trajectories, including the fluctuations and uncertainties comprising users’ lived experiences. Through the personal accounts of data subjects, QLLR also gives researchers insight into how the emotional dimensions of users’ interactions with ADMs shapes their actions responding to these systems.","['Automated Social Security', 'Digital Social Security', 'Qualitative Research', 'Longitudinal Research', 'Interviews']","['Social and professional topics', 'Social and professional topics _ Computing / technology policy']","['Morgan Currie', 'Lena Podoletz']","['University of Edinburgh', 'University of Edinburgh']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594050,Security,"To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards","The EU’s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act’s hierarchy of risks, the AI systems that are likely to incur “high-risk” to health, safety, and fundamental rights are subject to the majority of the Act’s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the “core concepts” whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act’s application.","['AI Act', 'high-risk AI', 'harmonised standards', 'taxonomy', 'semantic web']","['Social and professional topics _ Governmental regulations', 'Computing methodologies _ Knowledge representation and reasoning', 'Information systems _ Resource Description Framework (RDF)']","['Delaram Golpayegani', 'Harshvardhan J. Pandit', 'Dave Lewis']","['ADAPT Centre, Trinity College Dublin', 'ADAPT Centre, Dublin City University', 'ADAPT Centre, Trinity College Dublin']","['Ireland', 'Ireland', 'Ireland']"
https://doi.org/10.1145/3593013.3593973,Security,‘We Are Adults and Deserve Control of Our Phones’: Examining the Risks and Opportunities of a Right to Repair for Mobile Apps,"Many mobile apps are designed not just to support end-users’ needs, but also commercial aims. This can result in app designs that compromise end-user privacy, safety, and well-being. Since apps nowadays provide vital digital information and services, users often have no choice but to accept potentially harmful or manipulative app designs. What if, instead, individuals could customise their apps to make them safer and better suit their needs? This exploratory work examines this question through a multi-faceted approach; first, to understand user needs, we conducted a survey (n = 100) of changes users wanted in their apps, and of perceptions of risks in app repair. Second, to identify technical challenges, we developed a prototype that enables end-users to change their apps, and realised several modifications suggested by survey participants. Finally, we conduct a set of expert interviews (n = 8) to delve into the ethical and legal aspects of such a tool, and synthesise a framework of risks and opportunities of app repair.","['mobile apps', 'dark patterns', 'digital harms', 'right to repair', 'privacy']","['Human-centered computing _ Empirical studies in ubiquitous and mobile computing', 'Security and privacy _ Software and application security']","['Konrad Kollnig', 'Siddhartha Datta', 'Thomas Serban Von Davier', 'Max Van Kleek', 'Reuben Binns', 'Ulrik Lyngs', 'Nigel Shadbolt']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"