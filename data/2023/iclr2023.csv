link,category,title,abstract,keywords,ccs_concepts,author_names,affiliations,author_countries
https://iclr.cc/virtual/2023/poster/11509,Transparency & Explainability,Expressive Monotonic Neural Networks,"The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.","['Social Aspects of Machine Learning', 'lipschitz', 'monotonic']",[],"['Niklas Nolte', 'Ouail Kitouni', 'Mike Williams']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11585,Transparency & Explainability,Jointly Learning Visual and Auditory Speech Representations from Raw Data,"We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities' pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. At the same time, we achieve state-of-the-art results in the LRS3 low-resource setting for auditory speech recognition (as well as for VSR). Our findings point to the viability of learning powerful speech representations entirely from raw video and audio, i.e., without relying on handcrafted features. Code and models are available at https://github.com/ahaliassos/raven.","['Unsupervised and Self-supervised learning', 'lipreading', 'self-supervised learning', 'speech recognition']",[],"['Alexandros Haliassos', 'Pingchuan Ma', 'Rodrigo Mira', 'Stavros Petridis', 'Maja Pantic']","['Imperial College London', 'Facebook', 'Meta', 'Facebook', 'Facebook']",[]
https://iclr.cc/virtual/2023/poster/11477,Transparency & Explainability,Interpretable Debiasing of Vectorized Language Representations with Iterative Orthogonalization,"We propose a new mechanism to augment a word vector embedding representation that offers improved bias removal while retaining the key information—resulting in improved interpretability of the representation. Rather than removing the information associated with a concept that may induce bias, our proposed method identifies two concept subspaces and makes them orthogonal. The resulting representation has these two concepts uncorrelated. Moreover, because they are orthogonal, one can simply apply a rotation on the basis of the representation so that the resulting subspace corresponds with coordinates. This explicit encoding of concepts to coordinates works because they have been made fully orthogonal, which previous approaches do not achieve. Furthermore, we show that this can be extended to multiple subspaces. As a result, one can choose a subset of concepts to be represented transparently and explicitly, while the others are retained in the mixed but extremely expressive format of the representation.","['Deep Learning and representational learning', 'pre-trained contextualized embeddings', 'ethics', 'static embeddings', 'natural language processing', 'bias', 'debiasing', 'fairness']",[],"['Prince Osei Aboagye', 'Yan Zheng', 'Jack Shunn', 'Chin-Chia Michael Yeh', 'Junpeng Wang', 'Zhongfang Zhuang', 'Huiyuan Chen', 'Liang Wang', 'Wei Zhang', 'Jeff M. Phillips']","['University of Utah', 'VISA', 'University of Utah', 'VISA', 'VISA', 'VISA', 'VISA', 'VISA', 'VISA', 'University of Utah']",[]
https://iclr.cc/virtual/2023/poster/11080,Transparency & Explainability,HiT-MDP: Learning the SMDP option framework on MDPs with Hidden Temporal Embeddings,"The standard option framework is developed on the Semi-Markov Decision Process (SMDP) which is unstable to optimize and sample inefficient. To this end, we propose the Hidden Temporal MDP (HiT-MDP) and prove that the option-induced HiT-MDP is homomorphic equivalent to the option-induced SMDP. A novel transformer-based framework is introduced to learn options' embedding vectors (rather than conventional option tuples) on HiT-MDPs. We then derive a stable and sample efficient option discovering method under the maximum-entropy policy gradient framework. Extensive experiments on challenging Mujoco environments demonstrate HiT-MDP's efficiency and effectiveness: under widely used configurations, HiT-MDP achieves competitive, if not better, performance compared to the state-of-the-art baselines on all finite horizon and transfer learning environments. Moreover, HiT-MDP significantly outperforms all baselines on infinite horizon environments while exhibiting smaller variance, faster convergence, and better interpretability. Our work potentially sheds light on the theoretical ground of extending the option framework into a large-scale foundation model.","['Reinforcement Learning', 'reinforcement learning', 'markov decision process', 'Hiearchical Reinforcement Learning']",[],"['Chang Li', 'Dongjin Song', 'Dacheng Tao']","['University of Sydney', 'University of Connecticut', 'University of Sydney']",[]
https://iclr.cc/virtual/2023/poster/11475,Transparency & Explainability,Continuous-time identification of dynamic state-space models by deep subspace encoding,"Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.","['Deep Learning and representational learning', 'Artificial Neural Networks', 'State-space', 'Continuous-time']",[],"['Gerben I. Beintema', 'Maarten Schoukens', 'Roland Toth']","['Eindhoven University of Technology', 'Eindhoven University of Technology', 'Delft University of Technology']",[]
https://iclr.cc/virtual/2023/poster/11150,Transparency & Explainability,AE-FLOW: Autoencoders with Normalizing Flows  for  Medical Images Anomaly Detection,"Anomaly detection from medical images is an important task for clinical screening and diagnosis. In general, a large dataset of normal images are available while only few abnormal images can be collected in clinical practice. By mimicking the diagnosis process of radiologists, we attempt to tackle this problem by learning a tractable distribution of normal images and identify anomalies by differentiating the original image and the reconstructed normal image. More specifically, we propose a normalizing flow-based autoencoder for an efficient and tractable representation of normal medical images. The anomaly score consists of the likelihood originated from the normalizing  flow  and the reconstruction error of the autoencoder, which allows to identify the abnormality and provide an interpretability at both image and pixel levels. Experimental evaluation on two  medical images datasets showed that the proposed model outperformed the other approaches by a large margin, which validated the  effectiveness and robustness of the proposed method.","['Applications', 'Normalizing Flow', 'anomaly detection', 'Auto-encoder.']",[],"['Yuzhong Zhao', 'Qiaoqiao Ding', 'Xiaoqun Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University']",[]
https://iclr.cc/virtual/2023/poster/11220,Transparency & Explainability,DAVA: Disentangling Adversarial Variational Autoencoder,"The use of well-disentangled representations offers many advantages for downstream tasks, e.g. an increased sample efficiency, or better interpretability.However, the quality of disentangled interpretations is often highly dependent on the choice of dataset-specific hyperparameters, in particular the regularization strength.To address this issue, we introduce DAVA, a novel training procedure for variational auto-encoders. DAVA completely alleviates the problem of hyperparameter selection.We compare DAVA to models with optimal hyperparameters.Without any hyperparameter tuning, DAVA is competitive on a diverse range of commonly used datasets.Underlying DAVA, we discover a necessary condition for unsupervised disentanglement, which we call PIPE.We demonstrate the ability of PIPE to positively predict the performance of downstream models in abstract reasoning.We also thoroughly investigate correlations with existing supervised and unsupervised metrics. The code is available at https://github.com/besterma/dava.","['Unsupervised and Self-supervised learning', 'varational auto-encoder', 'Disentanglement learning', 'generative adversarial networks', 'curriculum learning']",[],"['Benjamin Estermann', 'Roger Wattenhofer']","['ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11341,Transparency & Explainability,Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small,"Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections.To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior ""in the wild"" in a language model.  We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.","['Social Aspects of Machine Learning', 'transparency', 'interpretability', 'transformers', 'language models', 'Mechanistic Interpretability', 'Science of ML']",[],"['Kevin Ro Wang', 'Alexandre Variengien', 'Arthur Conmy', 'Buck Shlegeris', 'Jacob Steinhardt']","['Redwood Research', 'Conjecture', 'Google DeepMind', 'Redwood Research', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2023/poster/11272,Transparency & Explainability,CUTS: Neural Causal Discovery from Irregular Time-Series Data,"Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register unstructured data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations.","['Probabilistic Methods', 'graph neural networks', 'Granger causality', 'neural networks', 'causal discovery', 'time series']",[],"['Yuxiao Cheng', 'Runzhao Yang', 'Tingxiong Xiao', 'Zongren Li', 'Jinli Suo', 'Kunlun He', 'Qionghai Dai']","['Tsinghua University, Tsinghua University', 'Department of Automation, Tsinghua University', 'Tsinghua University, Tsinghua University', 'National University of Defense Technology', 'Tsinghua University, Tsinghua University', 'Chinese PLA General hospital', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/11290,Transparency & Explainability,Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models,"Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge:  entity,  dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC-Large with 770M parameters easily outperforms large language models that are 4-39x larger. In addition, KiC also exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.","['Applications', 'Semi-parametric language model', 'text-to-text model', 'mixture-of-experts', 'natural language understanding']",[],"['Xiaoman Pan', 'Wenlin Yao', 'Hongming Zhang', 'Dian Yu', 'Dong Yu', 'Jianshu Chen']","['Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab Seattle', 'Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab']",[]
https://iclr.cc/virtual/2023/poster/11248,Transparency & Explainability,Global Explainability of GNNs via Logic Combination of Learned Concepts,"While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned.In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.","['Social Aspects of Machine Learning', 'concept learning', 'explainability', 'graph neural networks']",[],"['Steve Azzolin', 'Antonio Longa', 'Pietro Barbiero', 'Pietro Lio', 'Andrea Passerini']","['University of Trento', 'University of Trento', 'University of Cambridge', 'University of Cambridge', 'University of Trento']",[]
https://iclr.cc/virtual/2023/poster/11701,Transparency & Explainability,Characterizing the Influence of Graph Elements,"Influence function, a method from the robust statistics, measures the changes of model parameters or some functions about model parameters with respect to the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need of expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph, and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to effectively guide the adversarial attacks on GCNs.","['General Machine Learning', 'influence functions', 'Interpretable Machine Learning', 'graph neural networks']",[],"['Zizhang Chen', 'Peizhao Li', 'Hongfu Liu', 'Pengyu Hong']","['Brandeis University', 'Brandeis University', 'Brandeis University', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11654,Transparency & Explainability,A Differential Geometric View and Explainability of GNN on Evolving Graphs,"Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods.",['Social Aspects of Machine Learning'],[],"['Yazheng Liu', 'Xi Zhang', 'Sihong Xie']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'HKUST-GZ']",[]
https://iclr.cc/virtual/2023/poster/12255,Transparency & Explainability,Bias Propagation in Federated Learning,"We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.","['Algorithmic Bias', 'fairness', 'federated learning']",[],"['Hongyan Chang', 'Reza Shokri']","['National University of Singapore', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11071,Transparency & Explainability,Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees,"Current abstractive summarization models either suffer from a lack of clear interpretability or provide incomplete rationales by only highlighting parts of the source document. To this end, we propose the Summarization Program (SP), an interpretable modular framework consisting of an (ordered) list of binary trees, each encoding the step-by-step generative process of an abstractive summary sentence from the source document. A Summarization Program contains one root node per summary sentence, and a distinct tree connects each summary sentence (root node) to the document sentences (leaf nodes) from which it is derived, with the connecting nodes containing intermediate generated sentences. Edges represent different modular operations involved in summarization such as sentence fusion, compression, and paraphrasing. We first propose an efficient best-first search method over neural modules, SP-Search that identifies SPs for human summaries by directly optimizing for ROUGE scores. Next, using these programs as automatic supervision, we propose seq2seq models that generate Summarization Programs, which are then executed to obtain final summaries. We demonstrate that SP-Search effectively represents the generative process behind human summaries using modules that are typically faithful to their intended behavior. We also conduct a simulation study to show that Summarization Programs improve the interpretability of summarization models by allowing humans to better simulate model reasoning. Summarization Programs constitute a promising step toward interpretable and modular abstractive summarization, a complex task previously addressed primarily through blackbox end-to-end neural systems.",['Applications'],[],"['Swarnadeep Saha', 'Shiyue Zhang', 'Peter Hase', 'Mohit Bansal']","['Department of Computer Science, University of North Carolina, Chapel Hill', 'Bloomberg', 'University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill']",[]
https://iclr.cc/virtual/2023/poster/12170,Transparency & Explainability,Topology-aware Robust Optimization for Out-of-Distribution Generalization,"Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach, and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach.","['Deep Learning and representational learning', 'out-of-distribution generalization', 'distributionally robust optimization']",[],"['Fengchun Qiao', 'Xi Peng']","['University of Delaware', 'University of Delaware']",[]
https://iclr.cc/virtual/2023/poster/11514,Transparency & Explainability,Compositional Law Parsing with Latent Random Functions,"Human cognition has compositionality. We understand a scene by decomposing the scene into different concepts (e.g., shape and position of an object) and learning the respective laws of these concepts, which may be either natural (e.g., laws of motion) or man-made (e.g., laws of a game). The automatic parsing of these laws indicates the model's ability to understand the scene, which makes law parsing play a central role in many visual tasks. This paper proposes a deep latent variable model for Compositional LAw Parsing (CLAP), which achieves the human-like compositionality ability through an encoding-decoding architecture to represent concepts in the scene as latent variables. CLAP employs concept-specific latent random functions instantiated with Neural Processes to capture the law of concepts. Our experimental results demonstrate that CLAP outperforms the baseline methods in multiple visual tasks such as intuitive physics, abstract visual reasoning, and scene representation. The law manipulation experiments illustrate CLAP's interpretability by modifying specific latent random functions on samples. For example, CLAP learns the laws of position-changing and appearance constancy from the moving balls in a scene, making it possible to exchange laws between samples or compose existing laws into novel laws.",['Generative models'],[],"['Fan Shi', 'Bin Li', 'Xiangyang Xue']","['Fudan University', 'Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2023/poster/11337,Transparency & Explainability,ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,"Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.","['Applications', 'step-by-step reasoning', 'evaluation']",[],"['Olga Golovneva', 'Moya Peng Chen', 'Spencer Poff', 'Martin Corredor', 'Luke Zettlemoyer', 'Maryam Fazel-Zarandi', 'Asli Celikyilmaz']","['Facebook', 'Facebook', 'Facebook', 'Georgia Institute of Technology', 'University of Washington', 'FAIR - Meta', 'FAIR ']",[]
https://iclr.cc/virtual/2023/poster/11385,Transparency & Explainability,Progress measures for grokking via mechanistic interpretability,"Neural networks often exhibit emergent behavior in which qualitatively new capabilities that arise from scaling up the number of parameters, training data, or even the number of steps. One approach to understanding emergence is to find the continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. In this work, we argue that progress measures can be found via mechanistic interpretability---that is, by reverse engineering learned models into components and measuring the progress of each component over the course of training. As a case study, we study small transformers trained on a modular arithmetic tasks with emergent grokking behavior. We fully reverse engineer the algorithm learned by these networks, which uses discrete fourier transforms and trigonometric identities to convert addition to rotation about a circle. After confirming the algorithm via ablation, we then use our understanding of the algorithm to define progress measures that precede the grokking phase transition on this task. We see our result as demonstrating both that it is possible to fully reverse engineer trained networks, and that doing so can be invaluable to understanding their training dynamics.","['Social Aspects of Machine Learning', 'interpretability', 'circuits', 'grokking', 'progress measures', 'Mechanistic Interpretability']",[],"['Neel Nanda', 'Lawrence Chan', 'Tom Lieberum', 'Jess Smith', 'Jacob Steinhardt']","['Google DeepMind', 'University of California Berkeley', 'University of Amsterdam', 'University of Florida', 'University of California Berkeley']",[]
https://iclr.cc/virtual/2023/poster/11566,Transparency & Explainability,BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging,"Sleep staging is helpful in assessing sleep quality and diagnosing sleep disorders. However, how to adequately capture the temporal and spatial relations of the brain during sleep remains a challenge. In particular, existing methods cannot adaptively infer spatial-temporal relations of the brain under different sleep stages. In this paper, we propose a novel Bayesian spatial-temporal relation inference neural network, named Bayesian spatial-temporal transformer (BSTT), for sleep staging. Our model is able to adaptively infer brain spatial-temporal relations during sleep for spatial-temporal feature modeling through a well-designed Bayesian relation inference component. Meanwhile, our model also includes a spatial transformer for extracting brain spatial features and a temporal transformer for capturing temporal features. Experiments show that our BSTT outperforms state-of-the-art baselines on ISRUC and MASS datasets. In addition, the visual analysis shows that the spatial-temporal relations obtained by BSTT inference have certain interpretability for sleep staging.","['Machine Learning for Sciences', 'Sleep Staging', 'bayesian deep learning', 'Spatial-Temporal Transformer']",[],"['Yuchen Liu', 'Ziyu Jia']","['Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",[]
https://iclr.cc/virtual/2023/poster/11038,Transparency & Explainability,Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes,"Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an ""interpretable-by-design"" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.","['Social Aspects of Machine Learning', 'Prototypes', 'deep reinforcement learning', 'User Study', 'Interpretable Machine Learning']",[],"['Eoin M. Kenny', 'Mycal Tucker', 'Julie Shah']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11163,Transparency & Explainability,This Looks Like It Rather Than That: ProtoKNN For Similarity-Based Classifiers,"Among research on the interpretability of deep learning models, the 'this looks like that' framework with ProtoPNet has attracted significant attention. By combining the strong power of deep learning models with the interpretability of case-based inference, ProtoPNet can achieve high accuracy while keeping its reasoning process interpretable. Many methods based on ProtoPNet have emerged to take advantage of this benefit, but despite their practical usefulness, they run into difficulty when utilizing similarity-based classifiers, e.g., in domains where unknown class samples exist. This is because ProtoPNet and its variants adopt the training process specific to linear classifiers, which allows the prototypes to represent useful image features for class recognition. Due to this difficulty, the effectiveness of similarity-based classifiers (e.g., k-nearest neighbor (KNN)) on the 'this looks like that' framework have not been sufficiently examined. To alleviate this problem, we propose ProtoKNN, an extension of ProtoPNet that adopts KNN classifiers. Extensive experiments on multiple open datasets demonstrate that the proposed method can achieve competitive results with a state-of-the-art method.","['Deep Learning and representational learning', 'deep learning', 'Fine-grained Image Classification', 'XAI', 'Inherently Interpretable Model', 'This Looks Like That Framework']",[],"['Yuki Ukai', 'Tsubasa Hirakawa', 'Takayoshi Yamashita', 'Hironobu Fujiyoshi']","['Chubu University', 'Chubu University', 'Chubu University', 'Chubu University']",[]
https://iclr.cc/virtual/2023/poster/11003,Transparency & Explainability,ReAct: Synergizing Reasoning and Acting in Language Models,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.","['Applications', 'decision making', 'agent', 'reasoning', 'language model']",[],"['Shunyu Yao', 'Jeffrey Zhao', 'Dian Yu', 'Nan Du', 'Izhak Shafran', 'Karthik R Narasimhan', 'Yuan Cao']","['Princeton University', 'Google Brain', 'Google', 'Apple/AIML', 'Google', 'Princeton University', 'Google DeepMind']",[]
https://iclr.cc/virtual/2023/poster/11463,Transparency & Explainability,Temporal Dependencies in Feature Importance for Time Series Prediction,"Time series data introduces two key challenges for explainability methods: firstly, observations of the same feature over subsequent time steps are not independent, and secondly, the same feature can have varying importance to model predictions over time. In this paper, we propose Windowed Feature Importance in Time (WinIT), a feature removal based explainability approach to address these issues. Unlike existing feature removal explanation methods, WinIT explicitly accounts for the temporal dependence between different observations of the same feature in the construction of its importance score. Furthermore, WinIT captures the varying importance of a feature over time, by summarizing its importance over a window of past time steps. We conduct an extensive empirical study on synthetic and real-world data, compare against a wide range of leading explainability methods, and explore the impact of various evaluation strategies. Our results show that WinIT achieves significant gains over existing methods, with more consistent performance across different evaluation metrics.","['Social Aspects of Machine Learning', 'explainability', 'time series', 'recurrent']",[],"['Kin Kwan Leung', 'Jonathan Smith', 'Saba Zuberi', 'Maksims Volkovs']","['Layer 6 AI', 'Meta', 'Layer 6 AI', 'Layer6 AI']",[]
https://iclr.cc/virtual/2023/poster/11192,Transparency & Explainability,Short-Term Memory Convolutions,"The real-time processing of time series signals is a critical issue for many real-life applications. The idea of real-time processing is especially important in audio domain as the human perception of sound is sensitive to any kind of disturbance in perceived signals, especially the lag between auditory and visual modalities. The rise of deep learning (DL) models complicated the landscape of signal processing. Although they often have superior quality compared to standard DSP methods, this advantage is diminished by higher latency. In this work we propose novel method for minimization of inference time latency and memory consumption, called Short-Term Memory Convolution (STMC) and its transposed counterpart. The main advantage of STMC is the low latency comparable to long short-term memory (LSTM) networks. Furthermore, the training of STMC-based models is faster and more stable as the method is based solely on convolutional neural networks (CNNs). In this study we demonstrate an application of this solution to a U-Net model for a speech separation task and GhostNet model in acoustic scene classification (ASC) task. In case of speech separation we achieved a 5-fold reduction in inference time and a 2-fold reduction in latency without affecting the output quality. The inference time for ASC task was up to 4 times faster while preserving the original accuracy.",['Applications'],[],"['Grzegorz Stefański', 'Krzysztof Arendt', 'Paweł Daniluk', 'Bartłomiej Jasik', 'Artur Szumaczuk']","['Samsung', 'Samsung', 'Samsung', 'Samsung', 'Samsung']",[]
https://iclr.cc/virtual/2023/poster/11733,Transparency & Explainability,Visual Classification via Description from Large Language Models,"Vision-language models such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what the model ``thinks"" it is seeing to make its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.","['Applications', 'prompting', 'Multimodal', 'CLIP', 'GPT-3', 'large language models', 'zero-shot recognition', 'vision-language models']",[],"['Sachit Menon', 'Carl Vondrick']","['Columbia University', 'Columbia University']",[]
https://iclr.cc/virtual/2023/poster/12113,Transparency & Explainability,TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second,"We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass.TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior.This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures.On the $18$ datasets in the OpenML-CC18 suite that contain up to 1000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to $230\times$ speedup.This increases to a $5\,700\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML.We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.","['Deep Learning and representational learning', 'Bayesian prediction', 'Causal Reasoning', 'tabular data', 'Real-time Machine Learning', 'automl', 'Green AI']",[],"['Noah Hollmann', 'Samuel Müller', 'Katharina Eggensperger', 'Frank Hutter']","['Albert-Ludwigs-Universität Freiburg', 'University of Freiburg, Universität Freiburg', 'Eberhard-Karls-Universität Tübingen', 'University of Freiburg & Bosch']",[]
https://iclr.cc/virtual/2023/poster/12212,Transparency & Explainability,Image as Set of Points,"What is an image, and how to extract latent features? Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in a local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via a simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, only relying on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of the clustering process.  Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better performance than ConvNets or ViTs on several benchmarks.","['Deep Learning and representational learning', 'representation', 'clustering', 'Context Cluster', 'Image Processing']",[],"['Xu Ma', 'Yuqian Zhou', 'Huan Wang', 'Can Qin', 'Bin Sun', 'Chang Liu', 'Yun Fu']","['Northeastern University', 'Adobe Systems', 'Northeastern University', 'SalesForce.com', 'Topazlabs LLC.', 'Northeastern University', 'Northeastern University']",[]
https://iclr.cc/virtual/2023/poster/12010,Transparency & Explainability,Visual Imitation Learning with Patch Rewards,"Visual imitation learning enables reinforcement learning agents to learn to behave from expert visual demonstrations such as videos or image sequences, without explicit, well-defined rewards. Previous reseaches either adopt supervised learning techniques or induce simple and coarse scalar rewards from pixels, neglecting the dense information contained in the image demonstrations.In this work, we propose to measure the expertise of various local regions of image samples, or called patches, and recover multi-dimensional patch rewards accordingly. Patch reward is a more precise rewarding characterization that serves as fine-grained expertise measurement and visual explainability tool.Specifically, we present Adversarial Imitation Learning with Patch Rewards (PatchAIL), which employs a patch-based discriminator to measure the expertise of different local parts from given images and provide patch rewards.The patch-based knowledge is also used to regularize the aggregated reward and stabilize the training.We evaluate our method on the standard pixel-based benchmark DeepMind Control Suite. The experiment results have demonstrated that PatchAIL outperforms baseline methods and provides valuable interpretations for visual demonstrations.","['Reinforcement Learning', 'reinforcement learning', 'imitation learning']",[],"['Minghuan Liu', 'Tairan He', 'Weinan Zhang', 'Shuicheng YAN', 'Zhongwen Xu']","['Shanghai Jiao Tong University', 'CMU, Carnegie Mellon University', 'Shanghai Jiao Tong University', 'National University of Singapore', 'DeepMind']",[]
https://iclr.cc/virtual/2023/poster/10885,Transparency & Explainability,SIMPLE: A Gradient Estimator for k-Subset Sampling,"$k$-subset sampling is ubiquitous in machine learning, enabling regularization and interpretability through sparsity. The challenge lies in rendering $k$-subset sampling amenable to end-to-end learning. This has typically involved relaxing the reparameterized samples to allow for backpropagation, but introduces both bias and variance. In this work, we fall back to discrete $k$-subset sampling on the forward pass. This is coupled with using the gradient with respect to the exact marginals, computed efficiently, as a proxy for the true gradient. We show that our gradient estimator exhibits lower bias and variance compared to state-of-the-art estimators. Empirical results show improved performance on learning to explain and sparse models benchmarks. We provide an algorithm for computing the exact ELBO for the $k$-subset distribution, obtaining significantly lower loss compared to state-of-the-art discrete sparse VAEs. All of our algorithms are exact and efficient.",['Deep Learning and representational learning'],[],"['Kareem Ahmed', 'Zhe Zeng', 'Mathias Niepert', 'Guy Van den Broeck']","['University of California, Los Angeles', 'University of California, Los Angeles', 'Universität Stuttgart', 'University of California, Los Angeles']",[]
https://iclr.cc/virtual/2023/poster/10716,Transparency & Explainability,Modeling content creator incentives on algorithm-curated platforms,"Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices—e.g., non-negative vs. unconstrained factorization—significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.","['Theory', 'Nash equilibria', 'attention monetizing platforms', 'differentiable games', 'recommenders', 'exposure game', 'producer incentives']",[],"['Jiri Hron', 'Karl Krauth', 'Michael Jordan', 'Niki Kilbertus', 'Sarah Dean']","['Google', 'University of California Berkeley', 'University of California, Berkeley', 'Technische Universität München', 'Cornell University']",[]
https://iclr.cc/virtual/2023/poster/11637,Transparency & Explainability,Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective,"Graph Neural Networks (GNNs) have shown great promise in various graph learning tasks. However, the computational overheads of fitting GNNs to large-scale graphs grow rapidly, posing obstacles to GNNs from scaling up to real-world applications. To tackle this issue, Graph Lottery Ticket (GLT) hypothesis articulates that there always exists a sparse subnetwork/subgraph with admirable performance in GNNs with random initialization. Such a pair of core subgraph and sparse subnetwork (called graph lottery tickets) can be uncovered by iteratively applying a novel sparsification method. While GLT provides new insights for GNN compression, it requires a full pretraining process to obtain graph lottery tickets, which is not universal and friendly to real-world applications. Moreover, the graph sparsification in GLT utilizes sampling techniques, which may result in massive information loss and aggregation failure. In this paper, we explore the searching of graph lottery tickets from a complementary perspective -- transforming a random ticket into a graph lottery ticket, which allows us to more comprehensively explore the relationships between the original network/graph and their sparse counterpart. To achieve this, we propose regularization-based network pruning and hierarchical graph sparsification, leading to our Dual Graph Lottery Ticket (DGLT) framework for a joint sparsification of network and graph. Compared to GLT, our DGLT helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability. More importantly, we rigorously prove that our model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory. Extensive experimental results on various graph-related tasks validate the effectiveness of our framework.","['Deep Learning and representational learning', 'Graph information bottleneck', 'Dual Lottery Tickets Hypothesis', 'Lottery Tickets Hypothesis', 'Graph pooling']",[],"['Kun Wang', 'Yuxuan Liang', 'Pengkun Wang', 'Xu Wang', 'Pengfei Gu', 'Junfeng Fang', 'Yang Wang']","['University of Science and Technology of China', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China']",[]
https://iclr.cc/virtual/2023/poster/12080,Transparency & Explainability,Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint,"Deep learning has revolutionized human society, yet the black-box nature of deep neural networks hinders further application to reliability-demanded industries. In the attempt to unpack them, many works observe or impact internal variables to improve the comprehensibility and invertibility of the black-box models. However, existing methods rely on intuitive assumptions and lack mathematical guarantees. To bridge this gap, we introduce Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and invertibility. We perform reconstruction and backtracking on the model representations optimized by Bort and observe a clear improvement in model explainability. Based on Bort, we are able to synthesize explainable adversarial samples without additional parameters and training. Surprisingly, we find Bort constantly improves the classification accuracy of various architectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet. Code: https://github.com/zbr17/Bort.","['Optimization', 'optimizer.', 'neural network', 'explainable ai']",[],"['Borui Zhang', 'Wenzhao Zheng', 'Jie Zhou', 'Jiwen Lu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/12100,Transparency & Explainability,Multivariate Time-series Imputation with Disentangled Temporal Representations,"Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in the literature. However, these methods all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) contained in the time series. Moreover, the entangled representation usually has no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to capture cross-channel correlations and identify temporal dynamics, but they are not scalable to large-scale datasets. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to large datasets. Empirical results show that our method not only outperforms existing approaches by notable margins on three real-world datasets, but also scales well to large datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further demonstrate the robustness of our model in obtaining accurate and explainable disentangled components.","['Deep Learning and representational learning', 'multivariate time-series imputation', 'disentangled representation']",[],"['SHUAI LIU', 'Xiucheng Li', 'Gao Cong', 'Yile Chen', 'YUE JIANG']","['School of Computer Science and  Engineering, Nanyang Technological University', 'Harbin Institute of Technology', 'National Technological University', 'Nanyang Technological University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/12128,Transparency & Explainability,Visual Recognition with Deep Nearest Centroids,"We devise deep nearest centroids (DNC), a conceptually elegant yet surprisingly effective network for large-scale visual recognition, by revisiting Nearest Centroids, one of the most classic and simple classifiers. Current deep models learn the classifier in a fully parametric manner, ignoring the latent data structure and lacking simplicity and explainability. DNC instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids of training samples to describe class distributions and clearly explains the classification as the proximity of test data and the class sub-centroids in the feature space. Due to the distance-based nature, the network output dimensionality is flexible, and all the learnable parameters are only for data embedding. That means all the knowledge learnt for ImageNet classification can be completely transferred for pixel recognition learning, under the ‘pre-training and fine-tuning’ paradigm. Apart from its nested simplicity and intuitive decision-making mechanism, DNC can even possess ad-hoc explainability when the sub-centroids are selected as actual training images that humans can view and inspect. Compared with parametric counterparts, DNC performs better on image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition (ADE20K, Cityscapes), with improved transparency and fewer learnable parameters, using various network architectures (ResNet, Swin) and segmentation models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights into related fields. Our code is available at https://github.com/ChengHan111/DNC.","['Deep Learning and representational learning', 'Nearest centroids classifier', 'Cased-base reasoning', 'Explainable neural networks', 'Image segmentation', 'image classification']",[],"['Wenguan Wang', 'Cheng Han', 'Tianfei Zhou', 'Dongfang Liu']","['Zhejiang University', 'Rochester Institute of Technology', 'Swiss Federal Institute of Technology', 'Rochester Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/12223,Transparency & Explainability,A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification,"Deep neural networks based on layer-stacking architectures have historically suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic models function with clear interpretability, but how to combine them with neural networks to enhance their performance remains to be explored. In this paper, we try to marry these two systems for text classification via a structured language model. We propose a Symbolic-Neural model that can learn to explicitly predict class labels of text spans from a constituency tree without requiring any access to span-level gold labels. As the structured language model learns to predict constituency trees in a self-supervised manner, only raw texts and sentence-level labels are required as training data, which makes it essentially a general constituent-level self-interpretable classification model. Our experiments demonstrate that our approach could achieve good prediction accuracy in downstream tasks. Meanwhile, the predicted span labels are consistent with human rationales to a certain degree.","['Unsupervised and Self-supervised learning', 'Multiple instance learning', 'structured language model', 'recursive neural network', 'interpretability', 'natural language processing', 'text classification', 'unsupervised learning']",[],"['Xiang Hu', 'XinYu KONG', 'Kewei Tu']","['Alibaba Group', 'Beijing University of Posts and Telecommunications', 'ShanghaiTech University']",[]
https://iclr.cc/virtual/2023/poster/11415,Transparency & Explainability,Post-hoc Concept Bottleneck Models,"Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ""sees"" in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.","['Social Aspects of Machine Learning', 'model editing', 'concepts', 'Concept Bottleneck Models', 'interpretability']",[],"['Mert Yuksekgonul', 'Maggie Wang', 'James Zou']","['Stanford University', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2023/poster/11389,Transparency & Explainability,Provably Auditing Ordinary Least Squares in Low Dimensions,"Auditing the stability of a machine learning model to small changes in the training procedure is critical for engendering trust in practical applications. For example, a model should not be overly sensitive to removing a small fraction of its training data. However, algorithmically validating this property seems computationally challenging, even for the simplest of models: Ordinary Least Squares (OLS) linear regression. Concretely, recent work defines the stability of a regression as the minimum number of samples that need to be removed so that rerunning the analysis overturns the conclusion (Broderick et al., 2020), specifically meaning that the sign of a particular coefficient of the OLS regressor changes. But the only known approach for estimating this metric, besides the obvious exponential-time algorithm, is a greedy heuristic that may produce severe overestimates and therefore cannot certify stability. We show that stability can be efficiently certified in the low-dimensional regime: when the number of covariates is a constant but the number of samples is large, there are polynomial-time algorithms for estimating (a fractional version of) stability, with provable approximation guarantees. Applying our algorithms to the Boston Housing dataset, we exhibit regression analyses where our estimator outperforms the greedy heuristic, and can successfully certify stability even in the regime where a constant fraction of the samples are dropped.","['Theory', 'stability', 'robustness', 'Linear Regression', 'ordinary least squares']",[],"['Ankur Moitra', 'Dhruv Rohatgi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11807,Transparency & Explainability,Fooling SHAP with Stealthily Biased Sampling,"SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.","['Social Aspects of Machine Learning', 'SHAP', 'Stealthily Sampling', 'explainability', 'robustness']",[],"['Gabriel Laberge', 'Ulrich Aïvodji', 'Satoshi Hara', 'Mario Marchand', 'Foutse Khomh']","['École Polytechnique de Montréal', 'École de technologie supérieure, Université du Québec', 'Osaka University', 'Laval university', 'École Polytechnique de Montréal, Université de Montréal']",[]
https://iclr.cc/virtual/2023/poster/11086,Transparency & Explainability,Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics,"Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in `untidy' or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples.","['General Machine Learning', 'Loss trajectory', 'Metadata archaeology', 'Data auditing', 'Learning curves']",[],"['Shoaib Ahmed Siddiqui', 'Nitarshan Rajkumar', 'Tegan Maharaj', 'David Krueger', 'Sara Hooker']","['University of Cambridge', 'University of Cambridge', 'Toronto University', 'University of Cambridge', 'Cohere For AI']",[]
https://iclr.cc/virtual/2023/poster/11629,Transparency & Explainability,MultiViz: Towards Visualizing and Understanding Multimodal Models,"The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.","['Social Aspects of Machine Learning', 'visualization', 'representation learning', 'Interpretation', 'multimodal learning']",[],"['Paul Pu Liang', 'Yiwei Lyu', 'Gunjan Chhablani', 'Nihal Jain', 'Zihao Deng', 'Xingbo Wang', 'Louis-Philippe Morency', 'Ruslan Salakhutdinov']","['Carnegie Mellon University', 'University of Michigan - Ann Arbor', 'Georgia Institute of Technology', 'Amazon', 'University of Pennsylvania, University of Pennsylvania', 'Weill Cornell Medicine, Cornell University', 'Carnegie Mellon University', 'Department of Computer Science']",[]
https://iclr.cc/virtual/2023/poster/11224,Transparency & Explainability,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.","['Social Aspects of Machine Learning', 'Zero-Knowledge Proof', 'Audit', 'fairness', 'confidentiality']",[],"['Ali Shahin Shamsabadi', 'Sierra Calanda Wyllie', 'Nicholas Franzese', 'Natalie Dullerud', 'Sébastien Gambs', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software', 'University of Toronto', 'Northwestern University, Northwestern University', 'Stanford University', 'Université du Québec à Montréal', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2023/poster/11197,Transparency & Explainability,Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles,"Systems neuroscience relies on two complementary views of neural data, characterized by single neuron tuning curves and analysis of population activity. These two perspectives combine elegantly in neural latent variable models that constrain the relationship between latent variables and neural activity, modeled by simple tuning curve functions. This has recently been demonstrated using Gaussian processes, with applications to realistic and topologically relevant latent manifolds. Those and previous models, however, missed crucial shared coding properties of neural populations. We propose $\textit{feature sharing}$ across neural tuning curves which significantly improves performance and helps optimization. We also propose a solution to the $\textit{ensemble detection}$ problem, where different groups of neurons, i.e., ensembles, can be modulated by different latent manifolds. Achieved through a soft clustering of neurons during training, this allows for the separation of mixed neural populations in an unsupervised manner. These innovations lead to more interpretable models of neural population activity that train well and perform better even on mixtures of complex latent manifolds. Finally, we apply our method on a recently published grid cell dataset, and recover distinct ensembles, infer toroidal latents and predict neural tuning curves in a single integrated modeling framework.","['Neuroscience and Cognitive Science', 'Latent Variable Models', 'grid cells', 'neural activity', 'neural ensemble detection', 'neuroscience', 'tuning curves']",[],"['Martin Bjerke', 'Kristopher T Jensen', 'Claudia Battistin', 'Benjamin Adric Dunn']","['Norwegian University of Science and Technology', 'University College London, University of London', 'Norwegian Institute of Technology', 'Norwegian Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11085,Transparency & Explainability,A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation,"Latent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution.","['Neuroscience and Cognitive Science', 'dimensionality reduction', 'inter-area interactions', 'neuroscience', 'Probabilistic Methods']",[],"['Edoardo Balzani', 'Jean-Paul G Noel', 'Pedro Herrero-Vidal', 'Dora Angelaki', 'Cristina Savin']","['New York University', 'New York University', 'New York University', 'New York University', 'New York University']",[]
https://iclr.cc/virtual/2023/poster/11054,Transparency & Explainability,Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images,"Multiple instance learning (MIL) is a popular weakly-supervised learning model on the whole slide image (WSI) for AI-assisted pathology diagnosis. The recent advance in attention-based MIL allows the model to find its region-of-interest (ROI) for interpretation by learning the attention weights for image patches of WSI slides. However, we empirically find that the interpretability of some related methods is either untrustworthy as the principle of MIL is violated or unsatisfactory as the high-attention regions are not consistent with experts' annotations. In this paper, we propose Bayes-MIL to address the problem from a probabilistic perspective. The induced patch-level uncertainty is proposed as a new measure of MIL interpretability, which outperforms previous methods in matching doctors annotations. We design a slide-dependent patch regularizer (SDPR) for the attention, imposing constraints derived from the MIL assumption, on the attention distribution. SDPR explicitly constrains the model to generate correct attention values. The spatial information is further encoded by an approximate convolutional conditional random field (CRF), for better interpretability. Experimental results show Bayes-MIL outperforms the related methods in patch-level and slide-level metrics and provides much better interpretable ROI on several large-scale WSI datasets.","['Applications', 'Multiple instance learning', 'histopathology', 'medical imaging', 'bayesian neural network']",[],"['Yufei CUI', 'Ziquan Liu', 'Xiangyu Liu', 'Xue Liu', 'Tei-Wei Kuo', 'Chun Jason Xue', 'Antoni B. Chan']","[', McGill University', 'Queen Mary, University of London', 'SUN YAT-SEN UNIVERSITY', 'McGill University', 'National Taiwan University', 'Mohamed bin Zayed University of Artificial Intelligence', 'City University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/10994,Transparency & Explainability,Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning,"Latent confounding has been a long-standing obstacle for causal reasoning from observational data. One popular approach is to model the data using acyclic directed mixed graphs (ADMGs), which describe ancestral relations between variables using directed and bidirected edges. However, existing methods using ADMGs are based on either linear functional assumptions or a discrete search that is complicated to use and lacks computational tractability for large datasets. In this work, we further extend the existing body of work and develop a novel gradient-based approach to learning an ADMG with nonlinear functional relations from observational data. We first show that the presence of latent confounding is identifiable under the assumptions of bow-free ADMGs with nonlinear additive noise models. With this insight, we propose a novel neural causal model based on autoregressive flows. This not only enables us to model complex causal relationships behind the data, but also estimate their functional relationships (hence treatment effects) simultaneously. We further validate our approach via experiments on both synthetic and real-world datasets, and demonstrate the competitive performance against relevant baselines.","['Probabilistic Methods', 'causal inference', 'causality', 'variational inference', 'structural equation model', 'causal discovery', 'latent confounders']",[],"['Matthew Ashman', 'Chao Ma', 'Agrin Hilmkil', 'Joel Jennings', 'Cheng Zhang']","['University of Cambridge', 'Microsoft', 'Chalmers University', 'Google Deepmind', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/10915,Transparency & Explainability,Interpretable Geometric Deep Learning via Learnable Randomness Injection,"Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at https://github.com/Graph-COM/LRI.","['Machine Learning for Sciences', 'geometric deep learning', 'Interpretation', 'graph neural networks']",[],"['Siqi Miao', 'Yunan Luo', 'Mia Liu', 'Pan Li']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Purdue University', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/12144,Transparency & Explainability,What shapes the loss landscape of self supervised learning?,"Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.","['Deep Learning and representational learning', 'collapse', 'self-supervised learning', 'loss landscape']",[],"['Liu Ziyin', 'Ekdeep Singh Lubana', 'Masahito Ueda', 'Hidenori Tanaka']","['The University of Tokyo', 'Center for Brain Science, Harvard University', 'The University of Tokyo', 'Harvard University, Harvard University']",[]
https://iclr.cc/virtual/2023/poster/11804,Transparency & Explainability,Causality Compensated Attention for Contextual Biased Visual Recognition,"Visual attention does not always capture the essential object representation desired for robust predictions. Attention modules tend to underline not only the target object but also the common co-occurring context that the module thinks helpful in the training. The problem is rooted in the confounding effect of the context leading to incorrect causalities between objects and predictions, which is further exacerbated by visual attention. In this paper, to learn causal object features robust for contextual bias, we propose a novel attention module named Interventional Dual Attention (IDA) for visual recognition. Specifically, IDA adopts two attention layers with multiple sampling intervention, which compensates the attention against the confounder context. Note that our method is model-agnostic and thus can be implemented on various backbones. Extensive experiments show our model obtains significant improvements in classification and detection with lower computation. In particular, we achieve the state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC.","['Deep Learning and representational learning', 'object recognition', 'causal inference', 'Interventional Dual Attention', 'Confounding Context', 'attention mechanism']",[],"['Ruyang Liu', 'Jingjia Huang', 'Thomas H. Li', 'Ge Li']","['Peking University', 'ByteDance Inc.', 'AIIT, Peking University', 'Peking University Shenzhen Graduate School']",[]
https://iclr.cc/virtual/2023/poster/11158,Transparency & Explainability,A View From Somewhere: Human-Centric Face Representations,"Few datasets contain self-identified demographic information, inferring demographic information risks introducing additional biases, and collecting and storing data on sensitive attributes can carry legal risks. Besides, categorical demographic labels do not necessarily capture all the relevant dimensions of human diversity. We propose to implicitly learn a set of continuous face-varying dimensions, without ever asking an annotator to explicitly categorize a person. We uncover the dimensions by learning on A View From Somewhere (AVFS) dataset of 638,180 human judgments of face similarity. We demonstrate the utility of our learned embedding space for predicting face similarity judgments, collecting continuous face attribute values, attribute classification, and comparative dataset diversity auditing. Moreover, using a novel conditional framework, we show that an annotator's demographics influences the \emph{importance} they place on different attributes when judging similarity, underscoring the \emph{need} for diverse annotator groups to avoid biases. Data and code are available at \url{https://github.com/SonyAI/aviewfrom_somewhere}.","['Social Aspects of Machine Learning', 'annotator bias', 'computer vision', 'faces', 'diversity', 'similarity', 'cognitive', 'mental representations']",[],"['Jerone Andrews', 'Przemyslaw Joniak']","['Sony AI', 'The University of Tokyo']",[]
https://iclr.cc/virtual/2023/poster/11400,Transparency & Explainability,Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic,"Natural language inference (NLI) aims to determine the logical relationship between two sentences, such as Entailment, Contradiction, and Neutral. In recent years, deep learning models have become a prevailing approach to NLI, but they lack interpretability and explainability. In this work, we address the explainability of NLI by weakly supervised logical reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases as the semantic unit and aligns corresponding phrases in the two sentences. Then, the model predicts the NLI label for the aligned phrases, and induces the sentence label by fuzzy logic formulas. Our EPR is almost everywhere differentiable and thus the system can be trained end to end. In this way, we are able to provide explicit explanations of phrasal logical relationships in a weakly supervised manner. We further show that such reasoning results help textual explanation generation.","['Applications', 'natural language inference', 'Explainability and Interpretability', 'Neural Fuzzy Logic', 'Weakly Supervised Reasoning']",[],"['Zijun Wu', 'Zi Xuan Zhang', 'Atharva Naik', 'Zhijian Mei', 'Mauajama Firdaus', 'Lili Mou']","['University of Alberta', 'University of Alberta', 'CMU, Carnegie Mellon University', 'University of Alberta', 'University of Alberta', 'University of Alberta']",[]
https://iclr.cc/virtual/2023/poster/10889,Transparency & Explainability,Binding Language Models in Symbolic Languages,"Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at anonymized.","['Applications', 'large language model', 'code generation', 'neural symbolic', 'semantic parsing']",[],"['Zhoujun Cheng', 'Tianbao Xie', 'Peng Shi', 'Chengzu Li', 'Rahul Nadkarni', 'Yushi Hu', 'Caiming Xiong', 'Dragomir Radev', 'Mari Ostendorf', 'Luke Zettlemoyer', 'Noah A. Smith', 'Tao Yu']","['Shanghai Jiaotong University', 'the University of Hong Kong, University of Hong Kong', 'University of Waterloo', 'University of Cambridge', 'Department of Computer Science & Engineering, University of Washington', 'University of Washington', 'Salesforce Research', 'Yale University', 'University of Washington', 'University of Washington', 'University of Washington', 'The University of Hong Kong']",[]
https://iclr.cc/virtual/2023/poster/10814,Transparency & Explainability,GEASS: Neural causal feature selection for high-dimensional biological data,"Identifying nonlinear causal relationships in high-dimensional biological data is an important task. However, current neural network based causality detection approaches for such data suffer from poor interpretability and cannot scale well to the high dimensional regime. Here we present GEASS (Granger fEAture Selection of Spatiotemporal data), which identifies sparse Granger causality mechanisms of high dimensional spatiotemporal data by a single neural network. GEASS maximizes sparsity-regularized modified transfer entropy with a theoretical guarantee of recovering features with spatial/temporal Granger causal relationships. The sparsity regularization is achieved by a novel combinatorial stochastic gate layer to select sparse non-overlapping feature subsets. We demonstrate the efficacy of GEASS in several synthetic datasets and real biological data from single-cell RNA sequencing and spatial transcriptomics.","['Machine Learning for Sciences', 'feature selection', 'Granger causality', 'neural networks', 'spatial transcriptomics', 'single-cell genomics']",[],"['Mingze Dong', 'Yuval Kluger']","['Yale University', 'Yale University']",[]
https://iclr.cc/virtual/2023/poster/12201,Transparency & Explainability,ChiroDiff: Modelling chirographic data with Diffusion Models,"Generative modelling over continuous-time geometric constructs, a.k.a $chirographic\ data$ such as handwriting, sketches, drawings etc., have been accomplished through autoregressive distributions. Such strictly-ordered discrete factorization however falls short of capturing key properties of chirographic data -- it fails to build holistic understanding of the temporal concept due to one-way visibility (causality). Consequently, temporal data has been modelled as discrete token sequences of fixed sampling rate instead of capturing the true underlying concept. In this paper, we introduce a powerful model-class namely Denoising\ Diffusion\ Probabilistic\ Models or DDPMs for chirographic data that specifically addresses these flaws. Our model named ""ChiroDiff"", being non-autoregressive, learns to capture holistic concepts and therefore remains resilient to higher temporal sampling rate up to a good extent. Moreover, we show that many important downstream utilities (e.g. conditional sampling, creative mixing) can be flexibly implemented using ChiroDiff. We further show some unique use-cases like stochastic vectorization, de-noising/healing, abstraction are also possible with this model-class. We perform quantitative and qualitative evaluation of our framework on relevant datasets and found it to be better or on par with competing approaches.","['Generative models', 'generative model', 'chirographic data', 'diffusion model', 'Continuous-time']",[],"['Ayan Das', 'Yongxin Yang', 'Timothy Hospedales', 'Tao Xiang', 'Yi-Zhe Song']","['University of Surrey', 'Queen Mary University of London', 'University of Edinburgh', 'University of Surrey', 'University of Surrey']",[]
https://iclr.cc/virtual/2023/poster/12143,Transparency & Explainability,Interpretability with full complexity by constraining feature information,"Interpretability is a pressing issue for machine learning. Common approaches to interpretable machine learning constrain interactions between features of the input, sacrificing model complexity in order to render more comprehensible the effects of those features on the model's output. We approach interpretability from a new angle: constrain the information about the features without restricting the complexity of the model. We use the Distributed Information Bottleneck to optimally compress each feature so as to maximally preserve information about the output. The learned information allocation, by feature and by feature value, provides rich opportunities for interpretation, particularly in problems with many features and complex feature interactions. The central object of analysis is not a single trained model, but rather a spectrum of models serving as approximations that leverage variable amounts of information about the inputs. Information is allocated to features by their relevance to the output, thereby solving the problem of feature selection by constructing a learned continuum of feature inclusion-to-exclusion. The optimal compression of each feature---at every stage of approximation---allows fine-grained inspection of the distinctions among feature values that are most impactful for prediction. We develop a framework for extracting insight from the spectrum of approximate models and demonstrate its utility on a range of tabular datasets.",['Social Aspects of Machine Learning'],[],"['Kieran A. Murphy', 'Danielle Bassett']","['University of Pennsylvania', 'University of Pennsylvania']",[]
https://iclr.cc/virtual/2023/poster/10718,Transparency & Explainability,Concept-level Debugging of Part-Prototype Networks,"Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model’s explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task.","['Social Aspects of Machine Learning', 'debugging', 'part-prototype networks', 'concept-based models', 'self-explainable networks', 'explainability']",[],"['Stefano Teso', 'Katya Tentori', 'Fausto Giunchiglia', 'Andrea Passerini']","['University of Trento', 'University of Trento', 'University of Trento', 'University of Trento']",[]
https://iclr.cc/virtual/2023/poster/12197,Fairness & Bias,Fairness-aware Contrastive Learning with Partially Annotated Sensitive Attributes,"Learning high-quality representation is important and essential for visual recognition. Unfortunately, traditional representation learning suffers from fairness issues since the model may learn information of sensitive attributes. Recently, a series of studies have been proposed to improve fairness by explicitly decorrelating target labels and sensitive attributes. Most of these methods, however, rely on the assumption that fully annotated labels on target variable and sensitive attributes are available, which is unrealistic due to the expensive annotation cost. In this paper, we investigate a novel and practical problem of Fair Unsupervised Representation Learning with Partially annotated Sensitive labels (FURL-PS). FURL-PS has two key challenges: 1) how to make full use of the samples that are not annotated with sensitive attributes; 2) how to eliminate bias in the dataset without target labels. To address these challenges, we propose a general Fairness-aware Contrastive Learning (FairCL) framework consisting of two stages. Firstly, we generate contrastive sample pairs, which share the same visual information apart from sensitive attributes, for each instance in the original dataset. In this way, we construct a balanced and unbiased dataset. Then, we execute fair contrastive learning by closing the distance between representations of contrastive sample pairs. Besides, we also propose an unsupervised way to balance the utility and fairness of learned representations by feature reweighting. Extensive experimental results illustrate the effectiveness of our method in terms of fairness and utility, even with very limited sensitive attributes and serious data bias.","['Deep Learning and representational learning', 'Fair Representation Learning', 'semi-supervised learning', 'contrastive learning', 'data augmentation']",[],"['Fengda Zhang', 'Kun Kuang', 'Long Chen', 'Yuxuan Liu', 'Chao Wu', 'Jun Xiao']","['Nanyang Technological University', 'Zhejiang University', 'Hong Kong University of Science and Technology', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']",[]
https://iclr.cc/virtual/2023/poster/11509,Fairness & Bias,Expressive Monotonic Neural Networks,"The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.","['Social Aspects of Machine Learning', 'lipschitz', 'monotonic']",[],"['Niklas Nolte', 'Ouail Kitouni', 'Mike Williams']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/10765,Fairness & Bias,Rethinking Symbolic Regression: Morphology and Adaptability in the Context of Evolutionary Algorithms,"Symbolic Regression (SR) is the well-studied problem of finding closed-form analytical expressions that describe the relationship between variables in a measurement dataset. In this paper, we rethink SR from two perspectives: morphology and adaptability. Morphology: Current SR algorithms typically use several man-made heuristics to influence the morphology (or structure) of the expressions in the search space. These man-made heuristics may introduce unintentional bias and data leakage, especially with the relatively few equation-recovery benchmark problems available for evaluating SR approaches. To address this, we formulate a novel minimalistic approach, based on constructing a depth-aware mathematical language model trained on terminal walks of expression trees, as a replacement to these heuristics. Adaptability: Current SR algorithms tend to select expressions based on only a single fitness function (e.g., MSE on the training set). We promote the use of an adaptability framework in evolutionary SR which uses fitness functions that alternate across generations. This leads to robust expressions that perform well on the training set and are close to the true functional form. We demonstrate this by alternating fitness functions that quantify faithfulness to values (via MSE) and empirical derivatives (via a novel theoretically justified fitness metric coined MSEDI). Proof-of-concept: We combine these ideas into a minimalistic evolutionary SR algorithm that outperforms all benchmark and state of-the-art SR algorithms in problems with unknown constants added, which we claim are more reflective of SR performance for real-world applications. Our claim is then strengthened by reproducing the superior performance on real-world regression datasets from SRBench. For researchers interested in equation-recovery problems, we also propose a set of conventions that can be used to promote fairness in comparison across SR methods and to reduce unintentional bias.",['General Machine Learning'],[],"['Kei Sen Fong', 'Shelvia Wongso', 'Mehul Motani']","['National University of Singapore', 'National University of Singapore', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11047,Fairness & Bias,Agree to Disagree: Diversity through Disagreement for Better Transferability,"Gradient-based learning algorithms have an implicit \emph{simplicity bias} which in effect can limit the diversity of predictors being sampled by the learning procedure. This behavior can hinder the transferability of trained models by (i) favoring the learning of simpler but spurious features --- present in the training data but absent from the test data --- and (ii) by only leveraging a small subset of predictive features.  Such an effect is especially magnified when the test distribution does not exactly match the train distribution---referred to as the Out of Distribution (OOD) generalization problem. However, given only the training data, it is not always possible to apriori assess if a given feature is spurious or transferable. Instead, we advocate for learning an ensemble of models which capture a diverse set of predictive features. Towards this, we propose a new algorithm D-BAT (Diversity-By-disAgreement Training), which enforces agreement among the models on the training data, but disagreement on the OOD data. We show how D-BAT naturally emerges from the notion of generalized discrepancy, as well as demonstrate in multiple experiments how the proposed method can mitigate shortcut-learning, enhance uncertainty and OOD detection, as well as improve transferability.","['Deep Learning and representational learning', 'ensemble', 'OOD generalization', 'diversity']",[],"['Matteo Pagliardini', 'Martin Jaggi', 'François Fleuret', 'Sai Praneeth Karimireddy']","['Swiss Federal Institute of Technology Lausanne', 'EPFL', 'University of Geneva', 'University of California, Berkeley']",[]
https://iclr.cc/virtual/2023/poster/12264,Fairness & Bias,Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization,"Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based on a parameter-efficient tuning schema without any training data. This leads to minimal generation quality loss and improved rectification performance with acceptable computational cost. Experimental results demonstrate that compared to several strong baselines, UDDIA achieves debiasing and detoxifying simultaneously and better balances efficiency and effectiveness, taking a further step towards practical ethical NLG.","['Debias', 'detoxify', 'language generation']",[],"['Zonghan Yang', 'Xiaoyuan Yi', 'Peng Li', 'Yang Liu', 'Xing Xie']","['Department of Computer Science and Technology, Tsinghua University', 'Microsoft', 'Tsinghua University', 'Tsinghua University', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/11647,Fairness & Bias,Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization,"Many existing group fairness-aware training methods aim to achieve the group fairness by either re-weighting underrepresented groups based on certain rules or using weakly approximated surrogates for the fairness metrics in the objective as regularization terms. Although each of the learning schemes has its own strength in terms of applicability or performance, respectively, it is difficult for any method in the either category to be considered as a gold standard since their successful performances are typically limited to specific cases. To that end, we propose a principled method, dubbed as FairDRO, which unifies the two learning schemes by incorporating a well-justified group fairness metric into the training objective using a classwise distributionally robust optimization (DRO) framework. We then develop an iterative optimization algorithm that minimizes the resulting objective by automatically producing the correct re-weights for each group. Our experiments show that FairDRO is scalable and easily adaptable to diverse applications, and consistently achieves the state-of-the-art performance on several benchmark datasets in terms of the accuracy-fairness trade-off, compared to recent strong baselines.","['Social Aspects of Machine Learning', 'DRO', 'Group Fairness']",[],"['Sangwon Jung', 'Taeeon Park', 'Sanghyuk Chun', 'Taesup Moon']","['Seoul National University', 'Seoul National University', 'NAVER AI Lab', 'Seoul National University']",[]
https://iclr.cc/virtual/2023/poster/11670,Fairness & Bias,Measure the Predictive Heterogeneity,"As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as in agriculture, sociology, health care, etc. For machine learning algorithms, the ignorance of data heterogeneity will significantly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and first formalize the Predictive Heterogeneity, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with PAC bounds even in high dimensions. Additionally, we propose the Information Maximization (IM) algorithm, a bi-level optimization algorithm, to explore the predictive heterogeneity of data. Empirically, the explored predictive heterogeneity provides insights for sub-population divisions in agriculture, sociology, and object recognition, and leveraging such heterogeneity benefits the out-of-distribution generalization performance.","['Social Aspects of Machine Learning', 'predictive information', 'predictive heterogeneity', 'Data heterogeneity']",[],"['Jiashuo Liu', 'Jiayun Wu', 'Renjie Pi', 'Renzhe Xu', 'Xingxuan Zhang', 'Bo Li', 'Peng Cui']","['Tsinghua University, Tsinghua University', 'Computer Science, Tsinghua University', 'Hong Kong University of Science and Technology', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10869,Fairness & Bias,Equal Improvability: A New Fairness Notion Considering the Long-term Impact,"Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples’ feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by each rejected sample. We analyze the properties of EI and its connections with existing fairness notions. To find a classifier that satisfies the EI requirement, we propose and study three different approaches that solve EI regularized optimization problems. Through experiments on both synthetic and real datasets, we demonstrate that the proposed EI-regularized algorithms encourage us to find a fair classifier in terms of EI. Finally, we provide experimental results on dynamic scenarios which highlight the advantages of our EI metric in achieving the long-term fairness. Codes are available in anonymous GitHub repository.","['Social Aspects of Machine Learning', 'machine learning', 'Fairness and Bias in Artificial Intelligence']",[],"['Ozgur Guldogan', 'Yuchen Zeng', 'Jy-yong Sohn', 'Ramtin Pedarsani', 'Kangwook Lee']","['University of California, Santa Barbara', 'University of Wisconsin, Madison', 'Yonsei University', 'UC Santa Barbara', 'University of Wisconsin, Madison']",[]
https://iclr.cc/virtual/2023/poster/12255,Fairness & Bias,Bias Propagation in Federated Learning,"We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.","['Algorithmic Bias', 'fairness', 'federated learning']",[],"['Hongyan Chang', 'Reza Shokri']","['National University of Singapore', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/10687,Fairness & Bias,Human-Guided Fair Classification for Natural Language Processing,"Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models.","['Social Aspects of Machine Learning', 'Human Evaluation', 'Individual Fairness', 'style transfer', 'nlp', 'crowdsourcing']",[],"['Florian E. Dorner', 'Momchil Peychev', 'Nikola Konstantinov', 'Naman Goel', 'Elliott Ash', 'Martin Vechev']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'ETH Zurich', 'INSAIT, Sofia University', 'University of Oxford', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/12111,Fairness & Bias,Disparate Impact in Differential Privacy from Gradient Misalignment,"As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.","['Social Aspects of Machine Learning', 'differential privacy', 'privacy', 'fairness']",[],"['Maria S. Esipova', 'Atiyeh Ashari Ghomi', 'Yaqiao Luo', 'Jesse C. Cresswell']","['University of Waterloo', 'Department of Computer Science, University of Toronto', 'Department of Computer Science, University of Toronto', 'Layer 6 AI']",[]
https://iclr.cc/virtual/2023/poster/11800,Fairness & Bias,ManyDG: Many-domain Generalization for Healthcare Applications,"The vast amount of health data has been continuously collected for each patient, providing opportunities to support diverse healthcare predictive tasks such as seizure detection and hospitalization prediction. Existing models are mostly trained on other patients’ data and evaluated on new patients. Many of them might suffer from poor generalizability. One key reason can be overfitting due to the unique information related to patient identities and their data collection environments, referred to as patient covariates in the paper. These patient covariates usually do not contribute to predicting the targets but are often difficult to remove. As a result, they can bias the model training process and impede generalization. In healthcare applications, most existing domain generalization methods assume a small number of domains. In this paper, considering the diversity of patient covariates, we propose a new setting by treating each patient as a separate domain (leading to many domains). We develop a new domain generalization method ManyDG, that can scale to such many-domain problems. Our method identifies the patient do- main covariates by mutual reconstruction, and removes them via an orthogonal projection step. Extensive experiments show that ManyDG can boost the generalization performance on multiple real-world healthcare tasks (e.g., 3.7% Jaccard improvements on MIMIC drug recommendation) and support realistic but challenging settings such as insufficient data and continuous learning. The code is available at https://github.com/ycq091044/ManyDG.","['Machine Learning for Sciences', 'Healthcare', 'EEG', 'Patient covariate shift', 'EHR', 'domain generalization']",[],"['Chaoqi Yang', 'M Brandon Westover', 'Jimeng Sun']","['University of Illinois Urbana Champaign', 'Massachusetts General Hospital, Harvard University', 'Georgia Tech Research Corporation']",[]
https://iclr.cc/virtual/2023/poster/11545,Fairness & Bias,FaiREE: fair classification with finite-sample and distribution-free guarantee,"Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depend on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm which can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfying various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.","['Social Aspects of Machine Learning', 'classification', 'Algorithmic fairness', 'finite-sample', 'distribution-free']",[],"['Puheng Li', 'James Zou', 'Linjun Zhang']","['Stanford University', 'Stanford University', 'Rutgers University']",[]
https://iclr.cc/virtual/2023/poster/10995,Fairness & Bias,Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach,"Many machine learning problems today have multiple objective functions. They appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic multi-objective gradient correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the nonconvex setting. Simulations on multi-task supervised and reinforcement learning demonstrate the effectiveness of our method relative to the state-of-the-art methods.","['Optimization', 'machine learning', 'multi-objective optimization']",[],"['Heshan Devaka Fernando', 'Han Shen', 'Miao Liu', 'Subhajit Chaudhury', 'Keerthiram Murugesan', 'Tianyi Chen']","['Rensselaer Polytechnic Institute', 'Rensselaer Polytechnic Institute', 'International Business Machines', 'International Business Machines', 'International Business Machines', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2023/poster/11082,Fairness & Bias,Efficient Conditionally Invariant Representation Learning,"We introduce the Conditional Independence Regression CovariancE (CIRCE), a measure of conditional independence for multivariate continuous-valued variables. CIRCE applies as a regularizer in settings where we wish to learn neural features $\varphi(X)$ of data $X$ to estimate a target $Y$, while being conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are assumed to be continuous-valued but relatively low dimensional, whereas $X$ and its features may be complex and high dimensional. Relevant settings include domain-invariant learning, fairness, and causal learning. The procedure requires just a single ridge regression from $Y$ to kernelized features of $Z$, which can be done in advance. It is then only necessary to enforce independence of $\varphi(X)$ from residuals of this regression, which is possible with attractive estimation properties and consistency guarantees. By contrast, earlier measures of conditional feature dependence require multiple regressions for each step of feature learning, resulting in more severe bias and variance, and greater computational cost. When sufficiently rich features are used, we establish that CIRCE is zero if and only if $\varphi(X) \perp \!\!\! \perp Z \mid Y$. In experiments, we show superior performance to previous methods on challenging benchmarks, including learning conditionally invariant image features. Code for image data experiments is available at github.com/namratadeka/circe.","['Deep Learning and representational learning', 'kernel methods', 'conditional independence']",[],"['Roman Pogodin', 'Yazhe Li', 'Danica J. Sutherland', 'Victor Veitch', 'Arthur Gretton']","['McGill/Mila', 'University College London', 'University of British Columbia', 'University of Chicago', 'University College London']",[]
https://iclr.cc/virtual/2023/poster/11452,Fairness & Bias,MEDFAIR: Benchmarking Fairness for Medical Imaging,"A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, ten datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.","['Social Aspects of Machine Learning', 'medical imaging', 'benchmark', 'fairness', 'Bias Mitigation']",[],"['Yongshuo Zong', 'Yongxin Yang', 'Timothy Hospedales']","['University of Edinburgh', 'Queen Mary University of London', 'University of Edinburgh']",[]
https://iclr.cc/virtual/2023/poster/11957,Fairness & Bias,Robust Fair Clustering: A Novel Fairness Attack and Defense Framework,"Clustering algorithms are widely used in many societal resource allocation applications, such as loan approvals and candidate recruitment, among others, and hence, biased or unfair model outputs can adversely impact individuals that rely on these applications. To this end, many $\textit{fair}$ clustering approaches have been recently proposed to counteract this issue. Due to the potential for significant harm, it is essential to ensure that fair clustering algorithms provide consistently fair outputs even under adversarial influence. However, fair clustering algorithms have not been studied from an adversarial attack perspective. In contrast to previous research, we seek to bridge this gap and conduct a robustness analysis against fair clustering by proposing a novel $\textit{black-box fairness attack}$. Through comprehensive experiments, we find that state-of-the-art models are highly susceptible to our attack as it can reduce their fairness performance significantly. Finally, we propose Consensus Fair Clustering (CFC), the first $\textit{robust fair clustering}$ approach that transforms consensus clustering into a fair graph partitioning problem, and iteratively learns to generate fair cluster outputs. Experimentally, we observe that CFC is highly robust to the proposed attack and is thus a truly robust fair clustering alternative.","['Unsupervised and Self-supervised learning', 'consensus clustering', 'Fairness Attack', 'Data Clustering', 'Fairness Defense']",[],"['Anshuman Chhabra', 'Peizhao Li', 'Prasant Mohapatra', 'Hongfu Liu']","['University of California, Davis', 'Brandeis University', 'University of South Florida', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11297,Fairness & Bias,Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play,"Standard empirical risk minimization (ERM) training can produce deep neural network (DNN) models that are accurate on average but under-perform in under-represented population subgroups, especially when there are imbalanced group distributions in the long-tailed training data. Therefore, approaches that improve the accuracy - group robustness trade-off frontier of a DNN model (i.e. improving worst-group accuracy without sacrificing average accuracy, or vice versa) is of crucial importance.   Uncertainty-based active learning (AL) can potentially improve the frontier by preferentially sampling underrepresented subgroups to create a more balanced training dataset.  However, the quality of uncertainty estimates from modern DNNs tend to degrade in the presence of spurious correlations and dataset bias, compromising the effectiveness of AL for sampling tail groups. In this work, we propose Introspective Self-play (ISP), a simple approach to improve the uncertainty estimation of a deep neural network under dataset bias, by adding an auxiliary introspection task requiring a model to predict the bias for each data point in addition to the label. We show that ISP provably improves the bias-awareness of the model representation and the resulting uncertainty estimates. On two real-world tabular and language tasks,ISP serves as a simple “plug-in” for AL model training, consistently improving both the tail-group sampling rate and the final accuracy-fairness trade-off frontier of popular AL methods.","['Probabilistic Methods', 'active learning', 'spurious correlation', 'uncertainty quantification']",[],"['Jeremiah Zhe Liu', 'Krishnamurthy Dj Dvijotham', 'Jihyeon Lee', 'Quan Yuan', 'Balaji Lakshminarayanan', 'Deepak Ramachandran']","['Google Research', 'Google DeepMind', 'Google', 'Google', 'Google Brain', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11967,Fairness & Bias,Everybody Needs Good Neighbours: An Unsupervised Locality-based Method for Bias Mitigation,"Learning models from human behavioural data often leads to outputs that are biased with respect to user demographics, such as gender or race. This effect can be controlled by explicit mitigation methods, but this typically presupposes access to demographically-labelled training data. Such data is often not available, motivating the need for unsupervised debiasing methods. To this end, we propose a new meta-algorithm for debiasing representation learning models, which combines the notions of data locality and accuracy of model fit, such that a supervised debiasing method can optimise fairness between neighbourhoods of poorly vs. well modelled instances as identified by our method. Results over five datasets, spanning natural language processing and structured data classification tasks, show that our technique recovers proxy labels that correlate with unknown demographic data, and that our method outperforms all unsupervised baselines, while also achieving competitive performance with state-of-the-art supervised methods which are given access to demographic labels.",['Social Aspects of Machine Learning'],[],"['Xudong Han', 'Timothy Baldwin', 'Trevor Cohn']","['University of Melbourne', 'Mohamed bin Zayed University of Artificial Intelligence', 'The University of Melbourne']",[]
https://iclr.cc/virtual/2023/poster/12038,Fairness & Bias,Fair Attribute Completion on Graph with Missing Attributes,"Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most downstream tasks to improve their fairness performance. To our best knowledge, FairAC is the first method that jointly addresses the graph attribution completion and graph unfairness problems. Experimental results on benchmark datasets show that our method achieves better fairness performance with less sacrifice in accuracy, compared with the state-of-the-art methods of fair graph learning. Code is available at: https://github.com/donglgcn/FairAC.",['Deep Learning and representational learning'],[],"['Dongliang Guo', 'Zhixuan Chu', 'Sheng Li']","['University of Virginia, Charlottesville', 'Alibaba Group', 'University of Virginia, Charlottesville']",[]
https://iclr.cc/virtual/2023/poster/11158,Fairness & Bias,A View From Somewhere: Human-Centric Face Representations,"Few datasets contain self-identified demographic information, inferring demographic information risks introducing additional biases, and collecting and storing data on sensitive attributes can carry legal risks. Besides, categorical demographic labels do not necessarily capture all the relevant dimensions of human diversity. We propose to implicitly learn a set of continuous face-varying dimensions, without ever asking an annotator to explicitly categorize a person. We uncover the dimensions by learning on A View From Somewhere (AVFS) dataset of 638,180 human judgments of face similarity. We demonstrate the utility of our learned embedding space for predicting face similarity judgments, collecting continuous face attribute values, attribute classification, and comparative dataset diversity auditing. Moreover, using a novel conditional framework, we show that an annotator's demographics influences the \emph{importance} they place on different attributes when judging similarity, underscoring the \emph{need} for diverse annotator groups to avoid biases. Data and code are available at \url{https://github.com/SonyAI/aviewfrom_somewhere}.","['Social Aspects of Machine Learning', 'annotator bias', 'computer vision', 'faces', 'diversity', 'similarity', 'cognitive', 'mental representations']",[],"['Jerone Andrews', 'Przemyslaw Joniak']","['Sony AI', 'The University of Tokyo']",[]
https://iclr.cc/virtual/2023/poster/11850,Fairness & Bias,"Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased","There is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.","['Reinforcement Learning', 'multi-agent reinforcement learning', 'human-AI collaboration']",[],"['Chao Yu', 'Jiaxuan Gao', 'Weilin Liu', 'Botian Xu', 'Hao Tang', 'Jiaqi Yang', 'Yu Wang', 'Yi Wu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'University of California Berkeley', 'Tsinghua University, Tsinghua University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/11550,Fairness & Bias,Prompting GPT-3 To Be Reliable,"Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.","['Social Aspects of Machine Learning', 'prompting', 'GPT-3', 'biases', 'reliability', 'calibration', 'large language models', 'knowledge updating', 'robustness']",[],"['Chenglei Si', 'Zhe Gan', 'Zhengyuan Yang', 'Shuohang Wang', 'Jianfeng Wang', 'Jordan Lee Boyd-Graber', 'Lijuan Wang']","['Stanford University', 'Apple', 'Microsoft', 'Microsoft', 'Microsoft', 'University of Maryland, College Park', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/12184,Fairness & Bias,Stochastic Differentially Private and Fair Learning,"Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic"" refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.","['Social Aspects of Machine Learning', 'differential privacy', 'stochastic optimization', 'Algorithmic fairness', 'private fair learning']",[],"['Andrew Lowy', 'Devansh Gupta', 'Meisam Razaviyayn']","['University of Southern California', 'Indraprastha Institute of Information Technology, Delhi', 'University of Southern California']",[]
https://iclr.cc/virtual/2023/poster/11807,Fairness & Bias,Fooling SHAP with Stealthily Biased Sampling,"SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.","['Social Aspects of Machine Learning', 'SHAP', 'Stealthily Sampling', 'explainability', 'robustness']",[],"['Gabriel Laberge', 'Ulrich Aïvodji', 'Satoshi Hara', 'Mario Marchand', 'Foutse Khomh']","['École Polytechnique de Montréal', 'École de technologie supérieure, Université du Québec', 'Osaka University', 'Laval university', 'École Polytechnique de Montréal, Université de Montréal']",[]
https://iclr.cc/virtual/2023/poster/11224,Fairness & Bias,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.","['Social Aspects of Machine Learning', 'Zero-Knowledge Proof', 'Audit', 'fairness', 'confidentiality']",[],"['Ali Shahin Shamsabadi', 'Sierra Calanda Wyllie', 'Nicholas Franzese', 'Natalie Dullerud', 'Sébastien Gambs', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software', 'University of Toronto', 'Northwestern University, Northwestern University', 'Stanford University', 'Université du Québec à Montréal', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2023/poster/11351,Fairness & Bias,Fairness and Accuracy under Domain Generalization,"As machine learning (ML) algorithms are increasingly used in high-stakes applications, concerns have arisen that they may be biased against certain social groups. Although many approaches have been proposed to make ML models fair, they typically rely on the assumption that data distributions in training and deployment are identical. Unfortunately, this is commonly violated in practice and a model that is fair during training may lead to an unexpected outcome during its deployment. Although the problem of designing robust ML models under dataset shifts has been widely studied, most existing works focus only on the transfer of accuracy. In this paper, we study the transfer of both fairness and accuracy under domain generalization where the data at test time may be sampled from never-before-seen domains. We first develop theoretical bounds on the unfairness and expected loss at deployment, and then derive sufficient conditions under which fairness and accuracy can be perfectly transferred via invariant representation learning. Guided by this, we design a learning algorithm such that fair ML models learned with training data still have high fairness and accuracy when deployment environments change. Experiments on real-world data validate the proposed algorithm.","['Social Aspects of Machine Learning', 'equal opportunity', 'equalized odds', 'js-divergence', 'regularization', 'invariant representation', 'domain generalization', 'fairness', 'accuracy']",[],"['Thai-Hoang Pham', 'Xueru Zhang', 'Ping Zhang']","['Ohio State University, Columbus', 'Ohio State University', 'The Ohio State University']",[]
https://iclr.cc/virtual/2023/poster/11145,Privacy & Data Governance,On the Trade-Off between Actionable Explanations and the Right to be Forgotten,"As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the “right to be forgotten” which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms. Thus, our work raises fundamental questions about the compatibility ofthe right to an actionable explanation'' in the context of theright to be forgotten'', while also providing constructive insights on the determining factors of recourse robustness.","['Social Aspects of Machine Learning', 'transparency', 'Counterfactual Explanations', 'Algorihtmic Recourse', 'interpretability', 'explainability']",[],"['Martin Pawelczyk', 'Tobias Leemann', 'Asia Biega', 'Gjergji Kasneci']","['Harvard University', 'Technische Universität München', 'Max Planck Institute for Security and Privacy', 'Technische Universität München']",[]
https://iclr.cc/virtual/2023/poster/10982,Privacy & Data Governance,Synthetic Data Generation of Many-to-Many Datasets via Random Graph Generation,"Synthetic data generation (SDG) has become a popular approach to release private datasets.In SDG, a generative model is fitted on the private real data, and samples drawn from the model are released as the protected synthetic data.While real-world datasets usually consist of multiple tables with potential \emph{many-to-many} relationships (i.e.~\emph{many-to-many datasets}), recent research in SDG mostly focuses on modeling tables \emph{independently} or only considers generating datasets with special cases of many-to-many relationships such as \emph{one-to-many}.In this paper, we first study challenges of building faithful generative models for many-to-many datasets, identifying limitations of existing methods.We then present a novel factorization for many-to-many generative models,  which leads to a scalable generation framework by combining recent results from random graph theory and representation learning.Finally, we extend the framework to establish the notion of $(\epsilon,\delta)$-differential privacy.Through a real-world dataset, we demonstrate that our method can generate synthetic datasets while preserving information within and across tables better than its closest competitor.","['Generative models', 'differential privacy', 'random graph generation', 'synthetic data generation']",[],"['Kai Xu', 'Georgi Ganev', 'Emile Joubert', 'Rees Davison', 'Olivier Van Acker', 'Luke Robinson']","['Amazon', 'University College London, University of London', 'University of Cape Town', 'University of Oxford', 'Birkbeck College, University of London', 'Hazy']",[]
https://iclr.cc/virtual/2023/poster/11012,Privacy & Data Governance,Federated Learning from Small Datasets,"Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning.","['Deep Learning and representational learning', 'small datasets', 'Distributed', 'daisy chain', 'sparse data', 'federated learning']",[],"['Michael Kamp', 'Jonas Fischer', 'Jilles Vreeken']","['Institute for AI in Medicine IKIM', 'Harvard TH Chan School of Public Health', 'CISPA Helmholtz Center for Information Security']",[]
https://iclr.cc/virtual/2023/poster/11368,Privacy & Data Governance,Federated Nearest Neighbor Machine Translation,"To protect user privacy and meet legal regulations, federated learning (FL) is attracting significant attention. Training neural machine translation (NMT) models with traditional FL algorithm (e.g., FedAvg) typically relies on multi-round model-based interactions. However, it is impractical and inefficient for machine translation tasks due to the vast communication overheads and heavy synchronization. In this paper, we propose a novel federated nearest neighbor (FedNN) machine translation framework that, instead of multi-round model-based interactions, leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems. The whole approach equips the public NMT model trained on large-scale accessible data with a $k$-nearest-neighbor ($k$NN) classifier and integrates the external datastore constructed by private text data in all clients to form the final FL model.  A two-phase datastore encryption strategy is introduced to achieve privacy-preserving during this process.  Extensive experiments show that FedNN significantly reduces computational and communication costs compared with FedAvg, while maintaining promising performance in different FL settings.","['Applications', 'Memorization Augmentation', 'machine translation', 'federated learning']",[],"['Yichao Du', 'Zhirui Zhang', 'Bingzhe Wu', 'Lemao Liu', 'Tong Xu', 'Enhong Chen']","['University of Science and Technology of China', 'Tencent AI Lab', 'Peking University', 'Tencent AI Lab', 'University of Science and Technology of China', 'University of Science and Technology of China']",[]
https://iclr.cc/virtual/2023/poster/11519,Privacy & Data Governance,Efficient Model Updates for Approximate Unlearning of Graph-Structured Data,"With the adoption of recent laws ensuring the ``right to be forgotten'', the problem of machine unlearning has become of significant importance. This is particularly the case for graph-structured data, and learning tools specialized for such data, including graph neural networks (GNNs). This work introduces the first known approach for \emph{approximate graph unlearning} with provable theoretical guarantees. The challenges in addressing the problem are two-fold. First, there exist multiple different types of unlearning requests that need to be considered, including node feature, edge and node unlearning. Second, to establish provable performance guarantees, one needs to carefully evaluate the process of feature mixing during propagation. We focus on analyzing Simple Graph Convolutions (SGC) and their generalized PageRank (GPR) extensions, thereby laying the theoretical foundations for unlearning GNNs. Empirical evaluations of six benchmark datasets demonstrate excellent performance/complexity/privacy trade-offs of our approach compared to complete retraining and general methods that do not leverage graph information. For example, unlearning $200$ out of $1208$ training nodes of the Cora dataset only leads to a $0.1\%$ loss in test accuracy, but offers a $4$-fold speed-up compared to complete retraining with a $(\epsilon,\delta)=(1,10^{-4})$ ``privacy cost''. We also exhibit a $12\%$ increase in test accuracy for the same dataset when compared to unlearning methods that do not leverage graph information, with comparable time complexity and the same privacy guarantee.","['Social Aspects of Machine Learning', 'machine unlearning', 'graph unlearning', 'privacy']",[],"['Eli Chien', 'Chao Pan', 'Olgica Milenkovic']","['Georgia Institute of Technology', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/12111,Privacy & Data Governance,Disparate Impact in Differential Privacy from Gradient Misalignment,"As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.","['Social Aspects of Machine Learning', 'differential privacy', 'privacy', 'fairness']",[],"['Maria S. Esipova', 'Atiyeh Ashari Ghomi', 'Yaqiao Luo', 'Jesse C. Cresswell']","['University of Waterloo', 'Department of Computer Science, University of Toronto', 'Department of Computer Science, University of Toronto', 'Layer 6 AI']",[]
https://iclr.cc/virtual/2023/poster/11554,Privacy & Data Governance,Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection,"Deep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively.","['Deep Learning and representational learning', 'out-of-distribution detection', 'Heterogeneity', 'federated learning']",[],"['Shuyang Yu', 'Junyuan Hong', 'Haotao Wang', 'Zhangyang Wang', 'Jiayu Zhou']","['Michigan State University', 'University of Texas at Austin', 'University of Texas, Austin', 'University of Texas at Austin', 'Michigan State University']",[]
https://iclr.cc/virtual/2023/poster/10724,Privacy & Data Governance,Asynchronous Distributed Bilevel Optimization,"Bilevel optimization plays an essential role in many machine learning tasks, ranging from hyperparameter optimization to meta-learning. Existing studies on bilevel optimization, however, focus on either centralized or synchronous distributed setting. The centralized bilevel optimization approaches require collecting massive amount of data to a single server, which inevitably incur significant communication expenses and may give rise to data privacy risks. Synchronous distributed bilevel optimization algorithms, on the other hand, often face the straggler problem and will immediately stop working if a few workers fail to respond. As a remedy, we propose Asynchronous Distributed Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel optimization problems with both nonconvex upper-level and lower-level objective functions, and its convergence is theoretically guaranteed. Furthermore, it is revealed through theoretic analysis that the iteration complexity of ADBO to obtain the $\epsilon$-stationary point is upper bounded by $\mathcal{O}(\frac{1}{{{\epsilon ^2}}})$. Thorough empirical studies on public datasets have been conducted to elucidate the effectiveness and efficiency of the proposed ADBO.",['Optimization'],[],"['Yang Jiao', 'Tiancheng Wu', 'Dongjin Song', 'Chengtao Jian']","['Tongji University', 'Tongji University', 'University of Connecticut', 'Tongji University']",[]
https://iclr.cc/virtual/2023/poster/11725,Privacy & Data Governance,Progressive Voronoi Diagram Subdivision Enables Accurate Data-free Class-Incremental Learning,"Data-free Class-incremental Learning (CIL) is a challenging problem because rehearsing data from previous phases is strictly prohibited, causing catastrophic forgetting of Deep Neural Networks (DNNs). In this paper, we present \emph{iVoro}, a novel framework derived from computational geometry. We found Voronoi Diagram (VD), a classical model for space subdivision, is especially powerful for solving the CIL problem, because VD itself can be constructed favorably in an incremental manner -- the newly added sites (classes) will only affect the proximate classes, making the non-contiguous classes hardly forgettable. Furthermore, we bridge DNN and VD using Power Diagram Reduction, and show that the VD structure can be progressively refined along the phases using a divide-and-conquer algorithm. Moreover, our VD construction is not restricted to the deep feature space, but is also applicable to multiple intermediate feature spaces, promoting VD to be multilayer VD that efficiently captures multi-grained features from DNN. Importantly, \emph{iVoro} is also capable of handling uncertainty-aware test-time Voronoi cell assignment and has exhibited high correlations between geometric uncertainty and predictive accuracy (up to ${\sim}0.9$). Putting everything together, \emph{iVoro} achieves up to $25.26\%$, $37.09\%$, and $33.21\%$ improvements on CIFAR-100, TinyImageNet, and ImageNet-Subset, respectively, compared to the state-of-the-art non-exemplar CIL approaches. In conclusion, \emph{iVoro} enables highly accurate, privacy-preserving, and geometrically interpretable CIL that is particularly useful when cross-phase data sharing is forbidden, e.g. in medical applications.","['Deep Learning and representational learning', 'Voronoi Diagram', 'Computational Geometry']",[],"['Chunwei Ma', 'Zhanghexuan Ji', 'Ziyun Huang', 'Yan Shen', 'Mingchen Gao', 'Jinhui Xu']","['State University of New York, Buffalo', 'Johnson & Johnson MedTech LCI/WWDA', 'Pennsylvania State University, Erie', 'State University of New York, Buffalo', 'University at Buffalo, SUNY', 'State University of New York, Buffalo']",[]
https://iclr.cc/virtual/2023/poster/11920,Privacy & Data Governance,Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses,"This paper studies federated learning (FL)—especially cross-silo FL—with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person’s data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo $i$’s communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo $i$ (e.g. hospital $i$) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-silo FL) that people trust their own silo, but not the server or other silos. In this work, we provide tight (up to logarithms) upper and lower bounds for ISRL-DP FL with convex/strongly convex loss functions and homogeneous (i.i.d.) silo data. Remarkably, we show that similar bounds are attainable for smooth losses with arbitrary heterogeneous silo data distributions, via an accelerated ISRL-DP algorithm. We also provide tight upper and lower bounds for ISRL-DP federated empirical risk minimization, and use acceleration to attain the optimal bounds in fewer rounds of communication than the state-of-the-art. Finally, with a secure “shuffler” to anonymize silo messages (but without a trusted server), our algorithm attains the optimal central DP rates under more practical trust assumptions. Numerical experiments show favorable privacy-accuracy tradeoffs for our algorithm in classification and regression tasks.","['Social Aspects of Machine Learning', 'private optimization', 'stochastic convex optimization', 'cross-silo federated learning', 'differential privacy', 'distributed optimization', 'federated learning']",[],"['Andrew Lowy', 'Meisam Razaviyayn']","['University of Southern California', 'University of Southern California']",[]
https://iclr.cc/virtual/2023/poster/11949,Privacy & Data Governance,Dataless Knowledge Fusion by Merging Weights of Language Models,"Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a data-less knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.","['Applications', 'model merging', 'weight merging']",[],"['Xisen Jin', 'Xiang Ren', 'Daniel Preotiuc-Pietro', 'Pengxiang Cheng']","['University of Southern California', 'University of Southern California', 'Bloomberg', 'Bloomberg']",[]
https://iclr.cc/virtual/2023/poster/11384,Privacy & Data Governance,Distributed Differential Privacy in Multi-Armed Bandits,"We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server.  Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\epsilon,\delta$) or approximate-DP guarantee by sacrificing an additive $O\!\left(\!\frac{K\log T\sqrt{\log(1/\delta)}}{\epsilon}\!\right)\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\Theta\!\left(\!\frac{K\log T}{\epsilon}\!\right)\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures \emph{R\'{e}nyi differential privacy} -- a stronger notion than approximate DP -- under distributed trust model with a privacy cost of $O\!\left(\!\frac{K\sqrt{\log T}}{\epsilon}\!\right)\!$. Finally, as a by-product of our techniques, we also recover the best-known regret bounds for bandits under central and local models while using only \emph{discrete privacy noise}, which can avoid the privacy leakage due to floating point arithmetic of continuous noise on finite computers.","['Reinforcement Learning', 'differential privacy', 'Multi-armed Bandits']",[],"['Sayak Ray Chowdhury', 'Xingyu Zhou']","['Microsoft', 'Wayne State University']",[]
https://iclr.cc/virtual/2023/poster/11870,Privacy & Data Governance,"FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy","Federated learning (FL) is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems.  Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ with a tighter upper bound $\mathcal{O}(\frac{1}{T})$ if $K=\mathcal{O}(T)$.  Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficiency of our proposed FedSpeed, which converges significantly faster and achieves the state-of-the-art (SOTA) performance on the general FL experimental settings than several baselines including FedAvg, FedProx, FedCM, FedAdam, SCAFFOLD, FedDyn, FedADMM, etc.","['Deep Learning and representational learning', 'federated learning']",[],"['Yan Sun', 'Li Shen', 'Tiansheng Huang', 'Liang Ding', 'Dacheng Tao']","['University of Sydney', 'JD Explore Academy', 'Georgia Institute of Technology', 'JD Explore Academy, JD.com Inc.', 'University of Sydney']",[]
https://iclr.cc/virtual/2023/poster/10727,Privacy & Data Governance,Individual Privacy Accounting with Gaussian Differential Privacy,"Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual losses in a principled manner, we need a privacy accountant for adaptive compositions of mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the Rényi differential privacy by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the Rényi divergence-based fully adaptive composition results by Feldman and Zrnic (2021). We also consider measuring the individual  $(\varepsilon,\delta)$-privacy losses using the so-called privacy loss distributions. Using the Blackwell theorem, we can then use the results of Feldman and Zrnic (2021) to construct an approximative individual $(\varepsilon,\delta)$-accountant. We also show how to speed up the FFT-based individual DP accounting using the Plancherel theorem.","['Social Aspects of Machine Learning', 'differential privacy', 'gaussian differential privacy', 'privacy accounting', 'fully adaptive compositions', 'individual privacy loss']",[],"['Antti Koskela', 'Marlon Tobaben', 'Antti Honkela']","['Nokia Bell Labs', 'University of Helsinki', 'University of Helsinki']",[]
https://iclr.cc/virtual/2023/poster/12051,Privacy & Data Governance,Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors,"Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.","['Deep Learning and representational learning', 'model adaptation', 'transfer learning', 'black-box predictors']",[],"['Jianfei Yang', 'Xiangyu Peng', 'Kai Wang', 'Zheng Zhu', 'Jiashi Feng', 'Lihua Xie', 'Yang You']","['Nanyang Technological University', 'National University of Singapore', 'national university of singaore, National University of Singapore', 'PhiGent Robotics', 'ByteDance', 'Nanyang Technological University', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/12184,Privacy & Data Governance,Stochastic Differentially Private and Fair Learning,"Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic"" refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.","['Social Aspects of Machine Learning', 'differential privacy', 'stochastic optimization', 'Algorithmic fairness', 'private fair learning']",[],"['Andrew Lowy', 'Devansh Gupta', 'Meisam Razaviyayn']","['University of Southern California', 'Indraprastha Institute of Information Technology, Delhi', 'University of Southern California']",[]
https://iclr.cc/virtual/2023/poster/11362,Privacy & Data Governance,Machine Unlearning of Federated Clusters,"Federated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the ""right to be forgotten"", the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline, and prove that the client communication cost is logarithmic in the vector dimension. Additionally, to demonstrate the benefits of our unlearning mechanism over complete retraining, we provide a theoretical analysis for the unlearning performance of our approach. Simulation results show that the new FC framework exhibits superior clustering performance compared to previously reported FC baselines when the cluster sizes are highly imbalanced. Compared to completely retraining K-means++ locally and globally for each removal request, our unlearning procedure offers an average speed-up of roughly 84x across seven datasets. Our implementation for the proposed method is available at https://github.com/thupchnsky/mufc.","['Social Aspects of Machine Learning', 'machine unlearning', 'secure aggregation', 'federated clustering', 'federated learning']",[],"['Chao Pan', 'Jin Sima', 'Saurav Prakash', 'Vishal Rana', 'Olgica Milenkovic']","['University of Illinois, Urbana Champaign', 'University of Illinois Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana-Champaign', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/11224,Privacy & Data Governance,Confidential-PROFITT: Confidential PROof of FaIr Training of Trees,"Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.","['Social Aspects of Machine Learning', 'Zero-Knowledge Proof', 'Audit', 'fairness', 'confidentiality']",[],"['Ali Shahin Shamsabadi', 'Sierra Calanda Wyllie', 'Nicholas Franzese', 'Natalie Dullerud', 'Sébastien Gambs', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software', 'University of Toronto', 'Northwestern University, Northwestern University', 'Stanford University', 'Université du Québec à Montréal', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']",[]
https://iclr.cc/virtual/2023/poster/10697,Privacy & Data Governance,Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning,"Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy.  Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free.  Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it).  We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget.  With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\sqrt{d}$, where $d$ is the input dimension.  We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link, https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.","['Social Aspects of Machine Learning', 'differential privacy', 'representation learning', 'federated learning']",[],"['Zebang Shen', 'Jiayuan Ye', 'Anmin Kang', 'Hamed Hassani', 'Reza Shokri']","['Department of Computer Science, ETHZ - ETH Zurich', 'National University of Singapore', 'National University of Singapore', 'University of Pennsylvania', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/12127,Privacy & Data Governance,Easy Differentially Private Linear Regression,"Linear regression is a fundamental tool for statistical analysis. This has motivated the development of linear regression methods that also satisfy differential privacy and thus guarantee that the learned model reveals little about any one data point used to construct it. However, existing differentially private solutions assume that the end user can easily specify good data bounds and hyperparameters. Both present significant practical obstacles. In this paper, we study an algorithm which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. Given $n$ samples of $d$-dimensional data used to train $m$ models, we construct an efficient analogue using an approximate Tukey depth that runs in time $O(d^2n + dm\log(m))$. We find that this algorithm obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required.","['Social Aspects of Machine Learning', 'differential privacy', 'Linear Regression']",[],"['Kareem Amin', 'Matthew Joseph', 'Mónica Ribero', 'Sergei Vassilvitskii']","['Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/10740,Privacy & Data Governance,Regression with Label Differential Privacy,"We study the task of training regression models with the guarantee oflabeldifferential privacy (DP). Based on a global prior distribution of label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a ""randomized response on bins"", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.","['Social Aspects of Machine Learning', 'regression', 'label differential privacy']",[],"['Badih Ghazi', 'Pritish Kamath', 'Ravi Kumar', 'Ethan Jacob Leeman', 'Pasin Manurangsi', 'Avinash V Varadarajan', 'Chiyuan Zhang']","['Google', 'Google Research', 'Google', 'Google', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2023/poster/11240,Security,Learning ReLU networks to high uniform accuracy is intractable,"Statistical learning theory provides bounds on the necessary number of training samples needed to reach a prescribed accuracy in a learning problem formulated over a given target class. This accuracy is typically measured in terms of a generalization error, that is, an expected value of a given loss function. However, for several applications --- for example in a security-critical context or for problems in the computational sciences --- accuracy in this sense is not sufficient. In such cases, one would like to have guarantees for high accuracy on every input value, that is, with respect to the uniform norm. In this paper we precisely quantify the number of training samples needed for any conceivable training algorithm to guarantee a given uniform accuracy on any learning problem formulated over target classes containing (or consisting of) ReLU neural networks of a prescribed architecture. We prove that, under very general assumptions, the minimal number of training samples for this task scales exponentially both in the depth and the input dimension of the network architecture.","['Theory', 'Teacher-Student Learning', 'hardness results', 'Sample complexity', 'ReLU networks', 'learning theory']",[],"['Julius Berner', 'Philipp Grohs', 'Felix Voigtlaender']","['University of Vienna', 'University of Vienna', 'Katholische Universität Eichstätt-Ingolstadt']",[]
https://iclr.cc/virtual/2023/poster/11045,Security,Effective passive membership inference attacks in federated learning against overparameterized models,"This work considers the challenge of performing membership inference attacks in a federated learning setting ---for image classification--- where an adversary can only observe the communication between the central node and a single client (a passive white-box attack). Passive attacks are one of the hardest-to-detect attacks, since they can be performed without modifying how the behavior of the central server or its clients, and assumesno access to private data instances. The key insight of our method is empirically observing that, near parameters that generalize well in test, the gradient of large overparameterized neural network models statistically behave like high-dimensional independent isotropic random vectors.  Using this insight, we devise two attacks that are often little impacted by existing and proposed defenses. Finally, we validated the hypothesis that our attack depends on the overparametrization by showing that increasing the level of overparametrization (without changing the neural network architecture) positively correlates with our attack effectiveness.","['Deep Learning and representational learning', 'membership inference attack', 'neural networks', 'image classification', 'overparameterization', 'federated learning']",[],"['Jiacheng Li', 'Ninghui Li', 'Bruno Ribeiro']","['Purdue University', 'Purdue University', 'Purdue University']",[]
https://iclr.cc/virtual/2023/poster/11157,Security,CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning,"Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4 -- 5$\times$ lower than the theoretical bound.","['Social Aspects of Machine Learning', 'Model Auditing', 'Empirical Privacy', 'membership inference attack', 'differential privacy', 'federated learning']",[],"['Samuel Maddock', 'Alexandre Sablayrolles', 'Pierre Stock']","['University of Warwick', 'Facebook', 'Facebook']",[]
https://iclr.cc/virtual/2023/poster/11267,Security,"Inequality phenomenon in $l_{\infty}$-adversarial training, and its unrealized threats","The appearance of adversarial examples raises attention from both academia and industry. Along with the attack-defense arms race, adversarial training is the most effective against adversarial examples.However, we find inequality phenomena occur during the $l_{\infty}$-adversarial training, that few features dominate the prediction made by the adversarially trained model. We systematically evaluate such inequality phenomena by extensive experiments and find such phenomena become more obvious when performing adversarial training with increasing adversarial strength (evaluated by $\epsilon$). We hypothesize such inequality phenomena make $l_{\infty}$-adversarially trained model less reliable than the standard trained model when few ``important features"" are influenced. To validate our hypothesis, we proposed two simple attacks that either perturb or replace important features with noise or occlusion. Experiments show that $l_{\infty}$-adversarially trained model can be easily attacked when the few important features are influenced. Our work shed light on the limitation of the practicality of $l_{\infty}$-adversarial training.","['Social Aspects of Machine Learning', 'adversarial training', 'Adversarial feature represenation', 'adversarial robustness']",[],"['Ranjie Duan', 'YueFeng Chen', 'Yao Zhu', 'Xiaojun Jia', 'Rong Zhang', ""Hui Xue'""]","['Alibaba Group', 'Alibaba Group', 'Zhejiang University', ', Chinese Academy of Sciences', 'Huazhong University of Science and Technology', 'Zhejiang University, Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10910,Security,Empowering Graph Representation Learning with Test-Time Graph Transformation,"As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTrans which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTrans on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTrans performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings.","['Deep Learning and representational learning', 'out-of-distribution generalization', 'graph neural networks', 'adversarial robustness']",[],"['Wei Jin', 'Tong Zhao', 'Jiayuan Ding', 'Yozen Liu', 'Jiliang Tang', 'Neil Shah']","['Emory University', 'Snap Inc.', 'University of Southern California', 'Snap Inc.', 'Michigan State University', 'Snap Inc.']",[]
https://iclr.cc/virtual/2023/poster/11208,Security,Defending against Adversarial Audio  via Diffusion Model,"Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via off-the-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or re-training. We conduct extensive experiments on the speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. L2 or L∞-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by L2 or L∞-norm (up to +20% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by L2-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines.","['Social Aspects of Machine Learning', 'Adversarial attack and defense', 'Diffusion Models', 'speech recognition', 'AI security']",[],"['Shutong Wu', 'Jiongxiao Wang', 'Wei Ping', 'Weili Nie', 'Chaowei Xiao']","['University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'NVIDIA', 'NVIDIA', 'University of Wisconsin - Madison']",[]
https://iclr.cc/virtual/2023/poster/11253,Security,Neural-based classification rule learning for sequential data,"Discovering interpretable patterns for classification of sequential data is of key importance for a variety of fields, ranging from genomics to fraud detection or more generally interpretable decision-making.In this paper, we propose a novel differentiable fully interpretable method to discover both local and global patterns (i.e. catching a relative or absolute temporal dependency) for rule-based binary classification.It consists of a convolutional binary neural network with an interpretable neural filter and a training strategy based on dynamically-enforced sparsity.We demonstrate the validity and usefulness of the approach on synthetic datasets and on an open-source peptides dataset.Key to this end-to-end differentiable method is that the expressive patterns used in the rules are learned alongside the rules themselves.","['Social Aspects of Machine Learning', 'classification rule learning', 'binary neural network', 'interpretable AI', 'sequential data']",[],"['Marine Collery', 'Philippe Bonnard', 'François FAGES', 'Remy Kusters']","['International Business Machines', 'Ecole Nationale Supérieure en Electrotechnique, Electronique, Informatique et Hydraulique de Toulouse', 'INRIA', 'International Business Machines']",[]
https://iclr.cc/virtual/2023/poster/11256,Security,Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy,"Out-of-Distribution (OOD) detection is essential for safety-critical applications of deep neural networks. OOD detection is challenging since DNN models may produce very high logits value even for OOD samples. Hence, it is of great difficulty to discriminate OOD data by directly adopting Softmax on output logits as the confidence score. Differently, we detect the OOD sample with Hopfield energy in a store-then-compare paradigm. In more detail, penultimate layer outputs on the training set are considered as the representations of in-distribution (ID) data. Thus they can be transformed into stored patterns that serve as anchors to measure the discrepancy of unseen data for OOD detection. Starting from the energy function defined in Modern Hopfield Network for the discrepancy score calculation, we derive a simplified version SHE with theoretical analysis. In SHE, we utilize only one stored pattern to present each class, and these patterns can be obtained by simply averaging the penultimate layer outputs of training samples within this class. SHE has the advantages of hyperparameterfreeand high computational efficiency. The evaluations of nine widely-used OOD datasets show the promising performance of such a simple yet effective approach and its superiority over State-of-the-Art models. Code is available at https://github.com/zjs975584714/SHE ood detection.","['Deep Learning and representational learning', 'out-of-distribution detection', 'Hopfield Energy', 'Hyperparameter-Free']",[],"['Jinsong Zhang', 'Qiang Fu', 'Xu Chen', 'Lun Du', 'Zelin Li', 'Gang Wang', 'xiaoguang Liu', 'Shi Han', 'Dongmei Zhang']","['Research, Microsoft', 'Microsoft', 'Microsoft', 'Microsoft Research Asia', 'Research, Microsoft', 'Nankai University', 'Nankai University', 'Microsoft Research Asia', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/11701,Security,Characterizing the Influence of Graph Elements,"Influence function, a method from the robust statistics, measures the changes of model parameters or some functions about model parameters with respect to the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need of expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph, and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to effectively guide the adversarial attacks on GCNs.","['General Machine Learning', 'influence functions', 'Interpretable Machine Learning', 'graph neural networks']",[],"['Zizhang Chen', 'Peizhao Li', 'Hongfu Liu', 'Pengyu Hong']","['Brandeis University', 'Brandeis University', 'Brandeis University', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/10897,Security,TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization,"Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense. From the attack perspective, we show that TextGrad achieves remarkable improvements in both the attack success rate and the perplexity score over five state-of-the-art baselines. From the defense perspective, TextGrad-enabled adversarial training yields the most robust NLP model against a wide spectrum of NLP attacks.",['Applications'],[],"['Bairu Hou', 'Jinghan Jia', 'Yihua Zhang', 'Guanhua Zhang', 'Yang Zhang', 'Sijia Liu', 'Shiyu Chang']","['University of California, Santa Barbara', 'Michigan State University', 'Michigan State University', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'International Business Machines', 'Michigan State University', 'UC Santa Barbara']",[]
https://iclr.cc/virtual/2023/poster/11377,Security,Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication,"Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C<\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.","['Reinforcement Learning', 'reinforcement learning', 'adversarial attack', 'adversarial communication', 'certifiable robustness', 'multi-agent system']",[],"['Yanchao Sun', 'Ruijie Zheng', 'Parisa Hassanzadeh', 'Yongyuan Liang', 'Soheil Feizi', 'Sumitra Ganesh', 'Furong Huang']","['J.P. Morgan AI Research', 'Department of Computer Science, University of Maryland, College Park', 'J.P. Morgan Chase', 'SUN YAT-SEN UNIVERSITY', 'University of Maryland, College Park', 'J.P. Morgan Chase', 'Department of Computer Science, University of Maryland']",[]
https://iclr.cc/virtual/2023/poster/11924,Security,Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture of Stochastic Experts,"Equipping predicted segmentation with calibrated uncertainty is essential for safety-critical applications. In this work, we focus on capturing the data-inherent uncertainty (aka aleatoric uncertainty) in segmentation, typically when ambiguities exist in input images. Due to the high-dimensional output space and potential multiple modes in segmenting ambiguous images, it remains challenging to predict well-calibrated uncertainty for segmentation. To tackle this problem, we propose a novel mixture of stochastic experts (MoSE) model, where each expert network estimates a distinct mode of the aleatoric uncertainty and a gating network predicts the probabilities of an input image being segmented in those modes. This yields an efficient two-level uncertainty representation. To learn the model, we develop a Wasserstein-like loss that directly minimizes the distribution distance between the MoSE and ground truth annotations. The loss can easily integrate traditional segmentation quality measures and be efficiently optimized via constraint relaxation. We validate our method on the LIDC-IDRI dataset and a modified multimodal Cityscapes dataset. Results demonstrate that our method achieves the state-of-the-art or competitive performance on all metrics.","['Applications', 'Stochastic Segmentation', 'Multiple Annotations', 'Aleatoric Uncertainty', 'semantic segmentation']",[],"['Zhitong Gao', 'Yucong Chen', 'Chuyu Zhang', 'Xuming He']","['ShanghaiTech University', 'Xiaohongshu', 'ShanghaiTech University', 'ShanghaiTech University']",[]
https://iclr.cc/virtual/2023/poster/11891,Security,Towards Robustness Certification Against Universal Perturbations,"In this paper, we investigate the problem of certifying neural network robustness against universal perturbations (UPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing robustness certification methods aim to provide robustness guarantees for each sample with respect to the worst-case perturbations given a neural network. However, those sample-wise bounds will be loose when considering the UP threat model as they overlook the important constraint that the perturbation should be shared across all samples. We propose a method based on a combination of linear relaxation-based perturbation analysis and Mixed Integer Linear Programming to establish the first robust certification method for UP. In addition, we develop a theoretical framework for computing error bounds on the entire population using the certification results from a randomly sampled batch. Aside from an extensive evaluation of the proposed certification, we further show how the certification facilitates efficient comparison of robustness among different models or efficacy among different universal adversarial attack defenses and enables accurate detection of backdoor target classes.","['Social Aspects of Machine Learning', 'poisoning attack', 'adversarial attack', 'certified robustness', 'backdoor attack', 'Universal Perturbation']",[],"['Yi Zeng', 'Zhouxing Shi', 'Ming Jin', 'Feiyang Kang', 'Lingjuan Lyu', 'Cho-Jui Hsieh', 'Ruoxi Jia']","['Virginia Tech', 'University of California, Los Angeles', 'Virginia Tech', 'Virginia Polytechnic Institute and State University', 'Sony Research', 'Google', 'Virginia Tech']",[]
https://iclr.cc/virtual/2023/poster/11430,Security,Revisiting the Assumption of Latent Separability for Backdoor Defenses,"Recent studies revealed that deep learning is susceptible to backdoor poisoning attacks. An adversary can embed a hidden backdoor into a model to manipulate its predictions by only modifying a few training data, without controlling the training process. Currently, a tangible signature has been widely observed across a diverse set of backdoor poisoning attacks --- models trained on a poisoned dataset tend to learn separable latent representations for poison and clean samples. This latent separation is so pervasive that a family of backdoor defenses directly take it as a default assumption (dubbed latent separability assumption), based on which to identify poison samples via cluster analysis in the latent space. An intriguing question consequently follows: is the latent separation unavoidable for backdoor poisoning attacks? This question is central to understanding whether the assumption of latent separability provides a reliable foundation for defending against backdoor poisoning attacks. In this paper, we design adaptive backdoor poisoning attacks to present counter-examples against this assumption. Our methods include two key components: (1) a set of trigger-planted samples correctly labeled to their semantic classes (other than the target class) that can regularize backdoor learning; (2) asymmetric trigger planting strategies that help to boost attack success rate (ASR) as well as to diversify latent representations of poison samples. Extensive experiments on benchmark datasets verify the effectiveness of our adaptive attacks in bypassing existing latent separation based backdoor defenses. Moreover, our attacks still maintain a high attack success rate with negligible clean accuracy drop. Our studies call for defense designers to take caution when leveraging latent separation as an assumption in their defenses. Our codes are available at https://github.com/Unispac/Circumventing-Backdoor-Defenses.","['Social Aspects of Machine Learning', 'Backdoor Attacks']",[],"['Xiangyu Qi', 'Tinghao Xie', 'Yiming Li', 'Saeed Mahloujifar', 'Prateek Mittal']","['Princeton University', 'Princeton University', 'Zhejiang University', 'Meta', 'Princeton University']",[]
https://iclr.cc/virtual/2023/poster/11872,Security,Instance-wise Batch Label Restoration via Gradients in Federated Learning,"Gradient inversion attacks have posed a serious threat to the privacy of federated learning. The attacks search for the optimal pair of input and label best matching the shared gradients and the search space of the attacks can be reduced by pre-restoring labels. Recently, label restoration technique allows for the extraction of labels from gradients analytically, but even the state-of-the-art remains limited to identify the presence of categories (i.e., the class-wise label restoration). This work considers the more real-world settings, where there are multiple instances of each class in a training batch. An analytic method is proposed to perform instance-wise batch label restoration from only the gradient of the final layer. On the basis of the approximate recovered class-wise embeddings and post-softmax probabilities, we establish linear equations of the gradients, probabilities and labels to derive the Number of Instances (NoI) per class by the Moore-Penrose pseudoinverse algorithm. Our experimental evaluations reach over 99% Label existence Accuracy (LeAcc) and exceed 96% Label number Accuracy (LnAcc) in most cases on three image datasets and four classification models. The two metrics are used to evaluate class-wise and instance-wise label restoration accuracy, respectively. And the recovery is made feasible even with a batch size of 4096 and partially negative activations (e.g., Leaky ReLU and Swish). Furthermore, we demonstrate that our method facilitates the existing gradient inversion attacks by exploiting the recovered labels, with an increase of 6-7 in PSNR on both MNIST and CIFAR100. Our code isavailable at https://github.com/BUAA-CST/iLRG.","['Deep Learning and representational learning', 'batch label restoration', 'gradient inversion attack.', 'federated learning']",[],"['Kailang Ma', 'Yu Sun', 'Jian Cui', 'Dawei Li', 'Zhenyu Guan', 'Jianwei Liu']","['Beijing University of Aeronautics and Astronautics', 'School of Cyber Science and Technology, Beihang University', 'School of Cyber Science and Technology, Beihang University ', 'Beijing University of Aeronautics and Astronautics', 'Beijing University of Aeronautics and Astronautics', 'School of Cyber Science and Technology, Beihang University']",[]
https://iclr.cc/virtual/2023/poster/10721,Security,Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation,"Zeroth-order (ZO) optimization, in which the derivative is unavailable, has recently succeeded in many important machine learning applications. Existing algorithms rely on finite difference (FD) methods for derivative estimation and gradient descent (GD)-based approaches for optimization. However, these algorithms suffer from query inefficiency because many additional function queries are required for derivative estimation in their every GD update, which typically hinders their deployment in real-world applications where every function query is expensive. To this end, we propose a trajectory-informed derivative estimation method which only employs the optimization trajectory (i.e., the history of function queries during optimization) and hence can eliminate the need for additional function queries to estimate a derivative. Moreover, based on our derivative estimation, we propose the technique of dynamic virtual updates, which allows us to reliably perform multiple steps of GD updates without reapplying derivative estimation. Based on these two contributions, we introduce the zeroth-order optimization with trajectory-informed derivative estimation (ZoRD) algorithm for query-efficient ZO optimization. We theoretically demonstrate that our trajectory-informed derivative estimation and our ZoRD algorithm improve over existing approaches, which is then supported by our real-world experiments such as black-box adversarial attack, non-differentiable metric optimization, and derivative-free reinforcement learning.","['Optimization', 'finite difference', 'zeroth-order optimization', 'derivative estimation']",[],"['Yao Shu', 'Zhongxiang Dai', 'Weicong Sng', 'Arun Verma', 'Patrick Jaillet', 'Bryan Kian Hsiang Low']","['Tencent AI', 'Massachusetts Institute of Technology', 'National University of Singapore', 'National University of Singapore', 'Massachusetts Institute of Technology', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/11070,Security,"Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow","Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating conserved quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability.","['Optimization', 'lie group', 'gradient flow', 'flat minima', 'symmetry', 'Lie algebra', 'conserved quantity']",[],"['Bo Zhao', 'Iordan Ganev', 'Robin Walters', 'Rose Yu', 'Nima Dehmamy']","['University of California, San Diego', 'Institute for Computing and Information Sciences, Radboud University Nijmegen, Radboud University', 'Northeastern University ', 'University of California, San Diego', 'International Business Machines']",[]
https://iclr.cc/virtual/2023/poster/11491,Security,Calibrating Transformers via Sparse Gaussian Processes,"Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer’s success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.","['Probabilistic Methods', 'gaussian processes', 'transformers', 'uncertainty estimation', 'variational inference', 'bayesian neural networks']",[],"['Wenlong Chen', 'Yingzhen Li']","['Imperial College London, Imperial College London', 'Imperial College London']",[]
https://iclr.cc/virtual/2023/poster/11495,Security,The Dark Side of AutoML: Towards Architectural Backdoor Search,"This paper asks the intriguing question: is it possible to exploit neural architecture search (NAS) as a new attack vector to launch previously improbable attacks? Specifically, we present EVAS, a new attack that leverages NAS to find neural architectures with inherent backdoors and exploits such vulnerability using input-aware triggers. Compared with existing attacks, EVAS demonstrates many interesting properties: (i) it does not require polluting training data or perturbing model parameters; (ii) it is agnostic to downstream fine-tuning or even re-training from scratch; (iii) it naturally evades defenses that rely on inspecting model parameters or training data. With extensive evaluation on benchmark datasets, we show that EVAS features high evasiveness, transferability, and robustness, thereby expanding the adversary's design spectrum. We further characterize the mechanisms underlying EVAS, which are possibly explainable by architecture-level ``shortcuts'' that recognize trigger patterns. This work showcases that NAS can be exploited in a harmful way to find architectures with inherent backdoor vulnerability. The code is available at https://github.com/ain-soph/nas_backdoor.","['Deep Learning and representational learning', 'neural architecture search', 'backdoor attack and defense']",[],"['Ren Pang', 'Changjiang Li', 'Zhaohan Xi', 'Shouling Ji', 'Ting Wang']","['Pennsylvania State University', 'State University of New York at Stony Brook', 'Pennsylvania State University', 'Zhejiang University', 'State University of New York at Stony Brook']",[]
https://iclr.cc/virtual/2023/poster/12258,Security,Guiding Safe Exploration with Weakest Preconditions,"In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.","['reinforcement learning', 'safe learning', 'Safe exploration']",[],"['Greg Anderson', 'Swarat Chaudhuri', 'Isil Dillig']","['Reed College', 'University of Texas at Austin', 'University of Texas, Austin']",[]
https://iclr.cc/virtual/2023/poster/11490,Security,Perfectly Secure Steganography Using Minimum Entropy Coupling,"Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines---arithmetic coding, Meteor, and adaptive dynamic grouping---using GPT-2, WaveRNN, and Image Transformer as communication channels. We find that the minimum entropy coupling-based approach achieves superior encoding efficiency, despite its stronger security constraints. In aggregate, these results suggest that it may be natural to view information-theoretic steganography through the lens of minimum entropy coupling.","['Applications', 'Minimum Entropy Coupling', 'Information-Theoretic Steganography']",[],"['Christian Schroeder de Witt', 'Samuel Sokota', 'J Zico Kolter', 'Jakob Nicolaus Foerster', 'Martin Strohmeier']","['University of Oxford', 'Carnegie Mellon University', 'Carnegie Mellon University', 'University of Oxford, University of Oxford', 'armasuisse Science & Technology']",[]
https://iclr.cc/virtual/2023/poster/11166,Security,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.","['Social Aspects of Machine Learning', 'System 2', 'logical reasoning', 'reasoning', 'interpretability', 'language models', 'neural symbolic', 'large language models', 'Neuro-Symbolic']",[],"['Antonia Creswell', 'Murray Shanahan', 'Irina Higgins']","['DeepMind', 'Google', 'DeepMind']",[]
https://iclr.cc/virtual/2023/poster/11551,Security,Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks,"Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems.  Traditional adversarial training method requires ground-truth labels and hence cannot be directly applied to self-supervised MDE that does not have depth ground truth. Some self-supervised model hardening technique (e.g., contrastive learning) ignores the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using the depth ground truth. We improve adversarial robustness against physical-world attacks using $L_0$-norm-bounded perturbation in training. We compare our method with supervised learning-based and contrastive learning-based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation.","['Unsupervised and Self-supervised learning', 'adversarial training', 'Self-supervised Learning.', 'adversarial attack', 'Monocular Depth Estimation']",[],"['Zhiyuan Cheng', 'James Chenhao Liang', 'Guanhong Tao', 'Dongfang Liu', 'Xiangyu Zhang']","['Purdue University', 'Rochester Institute of Technology', 'Purdue University', 'Rochester Institute of Technology', ', Purdue University']",[]
https://iclr.cc/virtual/2023/poster/11554,Security,Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection,"Deep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively.","['Deep Learning and representational learning', 'out-of-distribution detection', 'Heterogeneity', 'federated learning']",[],"['Shuyang Yu', 'Junyuan Hong', 'Haotao Wang', 'Zhangyang Wang', 'Jiayu Zhou']","['Michigan State University', 'University of Texas at Austin', 'University of Texas, Austin', 'University of Texas at Austin', 'Michigan State University']",[]
https://iclr.cc/virtual/2023/poster/11498,Security,Boosting Adversarial Transferability using Dynamic Cues,"The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\eg, DINO), and Vision-language models (\eg, CLIP) to black-box \emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. As an example, an iterative attack launched from image model Deit-B with temporal prompts reduces generalization (top1 \% accuracy) of a video model by 35\% on Kinetics-400. Our approach also improves adversarial transferability to image models by 9\% on ImageNet w.r.t the current state-of-the-art approach. Our attack results indicate that the attacker does not need specialized architectures, \eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at \url{https://bit.ly/3Xd9gRQ}","['Social Aspects of Machine Learning', 'prompt learning', 'Transferability', 'adversarial attacks', 'Dynamic video modeling']",[],"['Muzammal Naseer', 'Ahmad Mahmood', 'Salman Khan', 'Fahad Khan']","['Mohamed bin Zayed University of Artificial Intelligence', 'ETHZ - ETH Zurich', 'Mohamed bin Zayed University of Artificial Intelligence', 'Inception Institute of Artificial Intelligence']",[]
https://iclr.cc/virtual/2023/poster/11112,Security,Spectral Augmentation for Self-Supervised Learning on Graphs,"Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation.","['Unsupervised and Self-supervised learning', 'graph self-supervised learning', 'graph spectral theory', 'graph augmentation']",[],"['Lu Lin', 'Jinghui Chen', 'Hongning Wang']","['Pennsylvania State University', 'Pennsylvania State University', 'Tsinghua University']",[]
https://iclr.cc/virtual/2023/poster/10995,Security,Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach,"Many machine learning problems today have multiple objective functions. They appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic multi-objective gradient correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the nonconvex setting. Simulations on multi-task supervised and reinforcement learning demonstrate the effectiveness of our method relative to the state-of-the-art methods.","['Optimization', 'machine learning', 'multi-objective optimization']",[],"['Heshan Devaka Fernando', 'Han Shen', 'Miao Liu', 'Subhajit Chaudhury', 'Keerthiram Murugesan', 'Tianyi Chen']","['Rensselaer Polytechnic Institute', 'Rensselaer Polytechnic Institute', 'International Business Machines', 'International Business Machines', 'International Business Machines', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2023/poster/12155,Security,Distilling Cognitive Backdoor Patterns within an Image,"This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the ``minimal essence"" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks.We also show that CD can potentially be applied to help detect potential biases from face datasets.Code is available at https://github.com/HanxunH/CognitiveDistillation.","['Deep Learning and representational learning', 'Backdoor defence', 'Backdoor sample detection']",[],"['Hanxun Huang', 'Xingjun Ma', 'Sarah Monazam Erfani']","['University of Melbourne', 'Fudan University', 'The University of Melbourne']",[]
https://iclr.cc/virtual/2023/poster/11683,Security,Toward Adversarial Training on Contextualized Language Representation,"Beyond the success story of adversarial training (AT) in the recent text domain on top of pre-trained language models (PLMs), our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition. This paper investigates AT from the perspective of the contextualized language representation outputted by PLM encoders. We find the current AT attacks lean to generate sub-optimal adversarial examples that can fool the decoder part but have a minor effect on the encoder. However, we find it necessary to effectively deviate the latter one to allow AT to gain. Based on the observation, we propose simple yet effective \textit{Contextualized representation-Adversarial Training} (CreAT), in which the attack is explicitly optimized to deviate the contextualized representation of the encoder. It allows a global optimization of adversarial examples that can fool the entire model. We also find CreAT gives rise to a better direction to optimize the adversarial examples, to let them less sensitive to hyperparameters. Compared to AT, CreAT produces consistent performance gains on a wider range of tasks and is proven to be more effective for language pre-training where only the encoder part is kept for downstream tasks. We achieve the new state-of-the-art performances on a series of challenging benchmarks, e.g. AdvGLUE (59.1 $ \rightarrow $ 61.1), HellaSWAG (93.0  $ \rightarrow $ 94.9), ANLI (68.1  $ \rightarrow $ 69.3).","['Applications', 'adversarial training', 'Pre-Trained Language Model']",[],"['Hongqiu Wu', 'Yongxiang Liu', 'Hanwen Shi', 'hai zhao', 'Min Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Harbin Institute of Technology, Shenzhen']",[]
https://iclr.cc/virtual/2023/poster/10773,Security,A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias,"Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.","['Deep Learning and representational learning', 'robustness', 'transfer learning', 'adaptation', 'data augmentation']",[],"['Puja Trivedi', 'Danai Koutra', 'Jayaraman J. Thiagarajan']","['University of Michigan', 'Amazon', 'Lawrence Livermore National Labs']",[]
https://iclr.cc/virtual/2023/poster/11068,Security,Few-shot Backdoor Attacks via Neural Tangent Kernels,"In a backdoor attack, an attacker injects corrupted examples into the training set. The goal of the attacker is to cause the final trained model to predict the attacker's desired target label when a predefined trigger is added to test inputs. Central to these attacks is the trade-off between the success rate of the attack and the number of corrupted training examples injected. We pose this attack as a novel bilevel optimization problem: construct strong poison examples that maximize the attack success rate of the trained model. We use neural tangent kernels to approximate the training dynamics of the model being attacked and automatically learn strong poison examples. We experiment on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt architectures on periodic and patch trigger attacks and show that NTBA-designed poisoned examples achieve, for example, an attack success rate of  90% with ten times smaller number of poison examples injected compared to the baseline. We provided an interpretation of the NTBA-designed attacks using the analysis of kernel linear regression. We further demonstrate a vulnerability in overparametrized deep neural networks, which is revealed by the shape of the neural tangent kernel.","['Deep Learning and representational learning', 'neural tangent kernel', 'kernel regression', 'robust machine learning', 'backdoor attack', 'Data Poisoning']",[],"['Jonathan Hayase', 'Sewoong Oh']","['University of Washington', 'University of Washington']",[]
https://iclr.cc/virtual/2023/poster/11711,Security,ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation,"Adversarial attack aims to generate deceptive inputs to fool a machine learning model. In deep learning, an adversarial input created for a specific neural network can also trick other neural networks. This intriguing property is known as black-box transferability of adversarial examples. To improve black-box transferability, a previously proposed method called Intermediate Level Attack (ILA) fine-tunes an adversarial example by maximizing its perturbation on an intermediate layer of the source model. Meanwhile, it has been shown that simple image transformations can also enhance attack transferability. Based on these two observations, we propose ILA-DA, which employs three novel augmentation techniques to enhance ILA. Specifically, we propose (1) an automated way to apply effective image transformations, (2) an efficient reverse adversarial update technique, and (3) an attack interpolation method to create more transferable adversarial examples. Shown by extensive experiments, ILA-DA greatly outperforms ILA and other state-of-the-art attacks by a large margin. On ImageNet, we attain an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art across nine undefended models. For defended models, ILA-DA also leads existing attacks and provides further gains when incorporated into more advanced attack methods.","['Deep Learning and representational learning', 'adversarial examples', 'data augmentation', 'Adversarial Transferability']",[],"['Chiu-Wai Yan', 'Tsz-Him Cheung', 'Dit-Yan Yeung']","['Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology']",[]
https://iclr.cc/virtual/2023/poster/11111,Security,MACTA: A Multi-agent Reinforcement Learning Approach for Cache Timing Attacks and Detection,"Security vulnerabilities in computer systems raise serious concerns as computers process an unprecedented amount of private and sensitive data today. Cache timing attacks (CTA) pose an important practical threat as they can effectively breach many protection mechanisms in today’s systems. However, the current detection techniques for cache timing attacks heavily rely on heuristics and expert knowledge, which can lead to brittleness and the inability to adapt to new attacks. To mitigate the CTA threat, we propose MACTA, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train both attackers and detectors. Following best practices, we develop a realistic simulated MARL environment, MA-AUTOCAT, which enables training and evaluation of cache-timing attackers and detectors. Our empirical results suggest that MACTA is an effective solution without any manual input from security experts. MACTA detectors can generalize to a heuristic attack not exposed in training with a 97.8% detection rate and reduce the attack bandwidth of adaptive attackers by 20% on average. In the meantime, MACTA attackers are qualitatively more effective than other attacks studied, and the average evasion rate of MACTA attackers against an unseen state-of-the-art detector can reach up to 99%. Furthermore, we found that agents equipped with a Transformer encoder can learn effective policies in situations when agents with multi-layer perceptron encoders do not in this environment, suggesting the potential of Transformer structures in CTA problems.","['Applications', 'multi-agent reinforcement learning', 'game theory', 'security']",[],"['Jiaxun Cui', 'Xiaomeng Yang', 'Mulong Luo', 'Geunbae Lee', 'Peter Stone', 'Hsien-Hsin S. Lee', 'Benjamin Lee', 'G. Edward Suh', 'Wenjie Xiong', 'Yuandong Tian']","['The University of Texas at Austin', 'Meta', 'Cornell University', 'Virginia Polytechnic Institute and State University', 'Sony AI', 'Intel', 'University of Pennsylvania, University of Pennsylvania', 'Meta AI', 'Virginia Polytechnic Institute and State University', 'Meta AI (FAIR)']",[]
https://iclr.cc/virtual/2023/poster/11720,Security,Spiking Convolutional Neural Networks for Text Classification,"Spiking neural networks (SNNs) offer a promising pathway to implement deep neural networks (DNNs) in a more energy-efficient manner since their neurons are sparsely activated and inferences are event-driven. However, there have been very few works that have demonstrated the efficacy of SNNs in language tasks partially because it is non-trivial to represent words in the forms of spikes and to deal with variable-length texts by SNNs. This work presents a ""conversion + fine-tuning'' two-step method for training SNN for text classification and proposes a simple but effective way to encode pre-trained word embeddings as spike trains. We show empirically that after further fine-tuning with surrogate gradients, the converted SNNs achieve comparable results to their DNN counterparts across multiple datasets for Both English and Chinese. We also demonstrate that such SNNs are more robust against adversarial attacks than DNNs.","['General Machine Learning', 'spiking neural networks', 'Training Method', 'text classification']",[],"['Changze Lv', 'Jianhan Xu']","['Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2023/poster/11087,Security,Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,"While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings  across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model ""IT"" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains.","['Deep Learning and representational learning', 'computer vision', 'Primate Vision', 'Inferior Temporal Cortex', 'Behavioral Alignment', 'adversarial robustness']",[],"['Joel Dapello', 'Kohitij Kar', 'Martin Schrimpf', 'Robert Baldwin Geary', 'Michael Ferguson', 'David Daniel Cox', 'James J. DiCarlo']","['Altos Labs', 'Massachusetts Institute of Technology', 'EPFL - EPF Lausanne', 'Harvard University', 'Massachusetts Institute of Technology', 'International Business Machines', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11370,Security,ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs,"Solving combinatorial optimization (CO) on graphs has been attracting increasing interests from the machine learning community whereby data-driven approaches were recently devised to go beyond traditional manually-designated algorithms. In this paper, we study the robustness of a combinatorial solver as a blackbox regardless it is classic or learning-based though the latter can often be more interesting to the ML community. Specifically, we develop a practically feasible robustness metric for general CO solvers. A no-worse optimal cost guarantee is developed as such the optimal solutions are not required to achieve for solvers, and we tackle the non-differentiable challenge in input instance disturbance by resorting to black-box adversarial attack methods. Extensive experiments are conducted on 14 unique combinations of solvers and CO problems, and we demonstrate that the performance of state-of-the-art solvers like Gurobi can degenerate by over 20% under the given time limit bound on the hard instances discovered by our robustness metric, raising concerns about the robustness of combinatorial optimization solvers.","['Optimization', 'reinforcement learning', 'robustness', 'graph neural networks', 'combinatorial optimization']",[],"['Han Lu', 'Zenan Li', 'Runzhong Wang', 'Qibing Ren', 'Xijun Li', 'Mingxuan Yuan', 'Jia Zeng', 'Xiaokang Yang', 'Junchi Yan']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Massachusetts Institute of Technology', 'Shanghai Jiaotong University', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', ""Huawei Noah's Ark Lab"", 'Shanghai Jiao Tong University, China', 'Shanghai Jiao Tong University']",[]
https://iclr.cc/virtual/2023/poster/11767,Security,Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation,"Adversarial patch attacks are an emerging security threat for real world deep learning applications. We present Demasked Smoothing, the first approach (up to our knowledge) to certify the robustness of semantic segmentation models against this threat model. Previous work on certifiably defending against patch attacks has mostly focused on image classification task and often required changes in the model architecture and additional training which is undesirable and computationally expensive. In Demasked Smoothing, any segmentation model can be applied without particular training, fine-tuning, or restriction of the architecture. Using different masking strategies, Demasked Smoothing can be applied both for certified detection and certified recovery. In extensive experiments we show that Demasked Smoothing can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset.","['Social Aspects of Machine Learning', 'adversarial patch attacks', 'certified defences', 'adversarial robustness']",[],"['Maksym Yatsura', 'Kaspar Sakmann', 'N. Grace Hua', 'Matthias Hein', 'Jan Hendrik Metzen']","['Robert Bosch GmbH, Bosch', 'Robert Bosch GmbH, Bosch', 'Robert Bosch GmbH, Bosch', 'University of Tübingen', 'Bosch Center Artificial Intelligence']",[]
https://iclr.cc/virtual/2023/poster/11886,Security,Revisiting Graph Adversarial Attack and Defense From a Data Distribution Perspective,"Recent studies have shown that structural perturbations are significantly effective in degrading the accuracy of Graph Neural Networks (GNNs) in the semi-supervised node classification (SSNC) task. However, why the gradient-based methods are so destructive is rarely explored. In this work, we discover an interesting phenomenon: the adversarial edges are not uniformly distributed on the graph. Nearly all perturbations are generated around the training nodes in poisoning attack. Combined with this phenomenon, we provide an explanation for the effectiveness of the gradient-based attack method from a data distribution perspective and revisit both poisoning attack and evasion attack in SSNC. From this new perspective, we empirically and theoretically discuss some other attack tendencies. Based on the analysis, we provide nine practical tips on both attack and defense and meanwhile leverage them to improve existing attack and defense methods. Moreover, we design a fast attack method and a self-training defense method, which outperform the state-of-the-art methods and can effectively scale to large graphs like ogbn-arxiv. We conduct extensive experiments on four benchmark datasets to verify our claims.","['Social Aspects of Machine Learning', 'Data Distribution', 'robustness', 'Graph Adversarial Attack']",[],"['Kuan Li', 'Yang Liu', 'Xiang Ao', 'Qing He']","['Hong Kong University of Science and Technology', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, CAS']",[]
https://iclr.cc/virtual/2023/poster/11836,Security,SYNC: SAFETY-AWARE NEURAL CONTROL FOR STABILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUATIONS,"Stabilization of the systems described by \textit{stochastic delay}-differential equations (SDDEs) under preset conditions is a challenging task in the control community. Here, to achieve this task, we leverage neural networks to learn control policies using the information of the controlled systems in some prescribed regions.  Specifically, two learned control policies, i.e., the neural deterministic controller (NDC) and the neural stochastic controller (NSC), work effectively in the learning procedures that rely on, respectively, the well-known LaSalle-type theorem and the newly-established theorem for guaranteeing the stochastic stability in SDDEs. We theoretically investigate the performance of the proposed controllers in terms of convergence time and energy cost.  More practically and significantly, we improve our learned control policies through considering the situation where the controlled trajectories only evolve in some specific safety set. {\color{black}  The practical validity of such control policies restricted in safety set is attributed to the theory that we further develop for safety and stability guarantees in SDDEs using the stochastic control barrier function and the spatial discretization}. We call this control as SYNC (\textbf{S}afet\textbf{Y}-aware \textbf{N}eural \textbf{C}ontrol).   The efficacy of all the articulated control policies, including the SYNC, is demonstrated systematically by using representative control problems.","['Theory', 'Stochastic delay-differential equations', 'safety guarantee', 'stochastic stabilization', 'neural networks']",[],"['Jingdong Zhang', 'Qunxi Zhu', 'Wei Yang', 'Wei Lin']","['Fudan University', 'Fudan University', 'Fudan University', 'Fudan University']",[]
https://iclr.cc/virtual/2023/poster/11957,Security,Robust Fair Clustering: A Novel Fairness Attack and Defense Framework,"Clustering algorithms are widely used in many societal resource allocation applications, such as loan approvals and candidate recruitment, among others, and hence, biased or unfair model outputs can adversely impact individuals that rely on these applications. To this end, many $\textit{fair}$ clustering approaches have been recently proposed to counteract this issue. Due to the potential for significant harm, it is essential to ensure that fair clustering algorithms provide consistently fair outputs even under adversarial influence. However, fair clustering algorithms have not been studied from an adversarial attack perspective. In contrast to previous research, we seek to bridge this gap and conduct a robustness analysis against fair clustering by proposing a novel $\textit{black-box fairness attack}$. Through comprehensive experiments, we find that state-of-the-art models are highly susceptible to our attack as it can reduce their fairness performance significantly. Finally, we propose Consensus Fair Clustering (CFC), the first $\textit{robust fair clustering}$ approach that transforms consensus clustering into a fair graph partitioning problem, and iteratively learns to generate fair cluster outputs. Experimentally, we observe that CFC is highly robust to the proposed attack and is thus a truly robust fair clustering alternative.","['Unsupervised and Self-supervised learning', 'consensus clustering', 'Fairness Attack', 'Data Clustering', 'Fairness Defense']",[],"['Anshuman Chhabra', 'Peizhao Li', 'Prasant Mohapatra', 'Hongfu Liu']","['University of California, Davis', 'Brandeis University', 'University of South Florida', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11925,Security,On the Robustness of Safe Reinforcement Learning under Observational Perturbations,"Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting.  We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward.  One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \url{https://github.com/liuzuxin/safe-rl-robustness}","['Reinforcement Learning', 'deep reinforcement learning', 'state robust reinforcement learning', 'safe reinforcement learning']",[],"['Zuxin Liu', 'Zijian Guo', 'Zhepeng Cen', 'Huan Zhang', 'Jie Tan', 'Bo Li', 'Ding Zhao']","['Carnegie Mellon University', 'Boston University, Boston University', 'CMU, Carnegie Mellon University', 'University of Illinois at Urbana-Champaign', 'Google', 'University of Illinois, Urbana Champaign', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2023/poster/11520,Security,A Unified Algebraic Perspective on Lipschitz Neural Networks,"Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition.  We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers.  Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on certified robust accuracy. Code is available at https://github.com/araujoalexandre/Lipschitz-SLL-Networks.","['Deep Learning and representational learning', 'Lipschitz neural networks', 'deep learning', 'robustness']",[],"['Alexandre Araujo', 'Aaron J Havens', 'Blaise Delattre', 'Alexandre Allauzen', 'Bin Hu']","['New York University', 'University of Illinois, Urbana Champaign', ', Université Paris-Dauphine (Paris IX)', 'Ecole supérieure de physique et chimie', 'University of Illinois, Urbana Champaign']",[]
https://iclr.cc/virtual/2023/poster/10789,Security,TextShield: Beyond Successfully Detecting Adversarial Sentences in text classification,"Adversarial attack serves as a major challenge for neural network models in NLP, which precludes the model's deployment in safety-critical applications. A recent line of work, detection-based defense, aims to distinguish adversarial sentences from benign ones. However, {the core limitation of previous detection methods is being incapable of giving correct predictions on adversarial sentences unlike defense methods from other paradigms.} To solve this issue, this paper proposes TextShield: (1) we discover a link between text attack and saliency information, and then we propose a saliency-based detector, which can effectively detect whether an input sentence is adversarial or not. (2) We design a saliency-based corrector, which converts the detected adversary sentences to benign ones. By combining the saliency-based detector and corrector, TextShield extends the detection-only paradigm to a detection-correction paradigm, thus filling the gap in the existing detection-based defense. Comprehensive experiments show that (a) TextShield consistently achieves higher or comparable performance than state-of-the-art defense methods across various attacks on different benchmarks. (b) our saliency-based detector outperforms existing detectors for detecting adversarial sentences.","['Applications', 'adversarial defense', 'adversarial attack', 'natural language processing', 'text classification']",[],"['Lingfeng Shen', 'Ze Zhang', 'Haiyun Jiang', 'Ying Chen']","['Johns Hopkins University', 'Fudan University', 'Tencent AI Lab', 'China Agricultural University']",[]
https://iclr.cc/virtual/2023/poster/11971,Security,Combating Exacerbated Heterogeneity for Robust Models in Federated Learning,"Privacy and security concerns in real-world applications have led to the development of adversarially robust federated models. However, the straightforward combination between adversarial training and federated learning in one framework can lead to the undesired robustness deterioration. We discover that the attribution behind this phenomenon is that the generated adversarial data could exacerbate the data heterogeneity among local clients, making the wrapped federated learning perform poorly. To deal with this problem, we propose a novel framework called Slack Federated Adversarial Training (SFAT), assigning the client-wise slack during aggregation to combat the intensified heterogeneity. Theoretically, we analyze the convergence of the proposed method to properly relax the objective when combining federated learning and adversarial training. Experimentally, we verify the rationality and effectiveness of SFAT on various benchmarked and real-world datasets with different adversarial training and federated optimization methods. The code is publicly available at: https://github.com/ZFancy/SFAT.",['Deep Learning and representational learning'],[],"['Jianing Zhu', 'Jiangchao Yao', 'Tongliang Liu', 'Quanming Yao', 'Jianliang Xu', 'Bo Han']","['Hong Kong Baptist University', 'Shanghai Jiaotong University', 'University of Sydney', 'Department of Electronic Engineering, Tsinghua University', 'Hong Kong Baptist University', 'HKBU']",[]
https://iclr.cc/virtual/2023/poster/11621,Security,Language Modelling with Pixels,"Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.","['Applications', 'masked autoencoder', 'representation learning', 'transformers', 'nlp', 'language model']",[],"['Phillip Rust', 'Jonas F. Lotz', 'Emanuele Bugliarello', 'Elizabeth Salesky', 'Miryam de Lhoneux', 'Desmond Elliott']","['University of Copenhagen', 'University of Copenhagen', 'Google', 'Johns Hopkins University', 'KU Leuven', '']",[]
https://iclr.cc/virtual/2023/poster/11601,Security,Neural Architecture Design and Robustness: A Dataset,"Deep learning models have proven to be successful in a wide range of machine learning tasks. Yet, they are often highly sensitive to perturbations on the input data which can lead to incorrect decisions with high confidence, hampering their deployment for practical use-cases. Thus, finding architectures that are (more) robust against perturbations has received much attention in recent years. Just like the search for well-performing architectures in terms of clean accuracy, this usually involves a tedious trial-and-error process with one additional challenge: the evaluation of a network's robustness is significantly more expensive than its evaluation for clean accuracy. Thus, the aim of this paper is to facilitate better streamlined research on architectural design choices with respect to their impact on robustness as well as, for example, the evaluation of surrogate measures for robustness. We therefore borrow one of the most commonly considered search spaces for neural architecture search for image classification, NAS-Bench-201, which contains a manageable size of 6466 non-isomorphic network designs. We evaluate all these networks on a range of common adversarial attacks and corruption types and introduce a database on neural architecture design and robustness evaluations. We further present three exemplary use cases of this dataset, in which we (i) benchmark robustness measurements based on Jacobian and Hessian matrices for their robustness predictability, (ii) perform neural architecture search on robust accuracies, and (iii) provide an initial analysis of how architectural design choices affect robustness. We find that carefully crafting the topology of a network can have substantial impact on its robustness, where networks with the same parameter count range in mean adversarial robust accuracy from 20%-41%. Code and data is available at http://robustness.vision/.","['Infrastructure', 'architecture design', 'robustness', 'dataset']",[],"['Steffen Jung', 'Jovita Lukasik', 'Margret Keuper']","['Saarland Informatics Campus, Max-Planck Institute', 'Universität Siegen', 'Universität Siegen']",[]
https://iclr.cc/virtual/2023/poster/11472,Security,Measuring Forgetting of Memorized Training Examples,"Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena.We propose a technique to measure to what extent models ``forget'' the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently.We show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image,speech, and language models empirically do forget examples over time.We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget.Our results suggest that examples seen early when training with extremely large datasets---for instance those examples used to pre-train a model---may observe privacy benefits at the expense of examples seen later.","['Social Aspects of Machine Learning', 'Memorization', 'canary extraction', 'convexity', 'membership inference', 'nondeterminism', 'forgetting']",[],"['Matthew Jagielski', 'Om Thakkar', 'Florian Tramèr', 'Daphne Ippolito', 'Katherine Lee', 'Nicholas Carlini', 'Eric Wallace', 'Shuang Song', 'Abhradeep Guha Thakurta', 'Nicolas Papernot', 'Chiyuan Zhang']","['Google', 'Google', 'ETHZ - ETH Zurich', 'School of Engineering and Applied Science, University of Pennsylvania', 'Cornell University', 'Google', 'University of California Berkeley', 'Google', 'Google', 'University of Toronto', 'Google']",[]
https://iclr.cc/virtual/2023/poster/10726,Security,LiftedCL: Lifting Contrastive Learning for Human-Centric Perception,"Human-centric perception targets for understanding human body pose, shape and segmentation. Pre-training the model on large-scale datasets and fine-tuning it on specific tasks has become a well-established paradigm in human-centric perception. Recently, self-supervised learning methods have re-investigated contrastive learning to achieve superior performance on various downstream tasks. When handling human-centric perception, there still remains untapped potential since 3D human structure information is neglected during the task-agnostic pre-training. In this paper, we propose the Lifting Contrastive Learning (LiftedCL) to obtain 3D-aware human-centric representations which absorb 3D human structure information. In particular, to induce the learning process, a set of 3D skeletons is randomly sampled by resorting to 3D human kinematic prior. With this set of generic 3D samples, 3D human structure information can be learned into 3D-aware representations through adversarial learning. Empirical results demonstrate that LiftedCL outperforms state-of-the-art self-supervised methods on four human-centric downstream tasks, including 2D and 3D human pose estimation (0.4% mAP and 1.8 mm MPJPE improvement on COCO 2D pose estimation and Human3.6M 3D pose estimation), human shape recovery and human parsing.","['Unsupervised and Self-supervised learning', 'contrastive learning', 'human-centric perception']",[],"['Ziwei Chen', 'Qiang Li', 'Xiaofeng Wang', 'Wankou Yang']","['Southeast University', 'SenseTime Research', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Southeast University']",[]
https://iclr.cc/virtual/2023/poster/10796,Security,"Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization","Graph Neural Networks (GNNs) have achieved state-of-the-art results on a variety of graph learning tasks, however, it has been demonstrated that they are vulnerable to adversarial attacks, raising serious security concerns. A lot of studies have been developed to train GNNs in a noisy environment and increase their robustness against adversarial attacks. However, existing methods have not uncovered a principled difficulty: the convoluted mixture distribution between clean and attacked data samples, which leads to sub-optimal model design and limits their frameworks’ robustness. In this work, we first begin by identifying the root cause of mixture distribution, then, for tackling it, we propose a novel method GAME - Graph Adversarial Mixture of Experts to enlarge the model capacity and enrich the representation diversity of adversarial samples, from three perspectives of model, training, and optimization. Specifically, we first propose a plug-and- play GAME layer that can be easily incorporated into any GNNs and enhance their adversarial learning capabilities. Second, we design a decoupling-based graph adversarial training in which the component of the model used to generate adversarial graphs is separated from the component used to update weights. Third, we introduce a graph diversity regularization that enables the model to learn diverse representation and further improves model performance. Extensive experiments demonstrate the effectiveness and advantages of GAME over the state-of-the-art adversarial training methods across various datasets given different attacks.","['Deep Learning and representational learning', 'mixture of experts', 'Graph adversarial learning', 'graph neural networks']",[],"['Chunhui Zhang', 'Yijun Tian', 'Mingxuan Ju', 'Zheyuan Liu', 'Yanfang Ye', 'Nitesh V Chawla', 'Chuxu Zhang']","['Dartmouth College', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'University of Notre Dame', 'Brandeis University']",[]
https://iclr.cc/virtual/2023/poster/11708,Security,Adversarial Attacks on Adversarial Bandits,"We study a security threat to adversarial multi-armed bandit, in which an attacker perturbs the loss or reward signal to control the behavior of the victim bandit player. We show that the attacker is able to mislead any no-regret adversarial bandit algorithm into selecting a suboptimal target action in every but sublinear (T−o(T )) number of rounds, while incurring only sublinear (o(T)) cumulative attack cost. This result implies critical security concern in real-world bandit-based systems, e.g., in online recommendation, an attacker might be able to hijack the recommender system and promote a desired product. Our proposed attack algorithms require knowledge of only the regret rate, thus are agnostic to the concrete bandit algorithm employed by the victim player. We also derived a theoretical lower bound on the cumulative attack cost that any victim-agnostic attack algorithm must incur. The lower bound matches the upper bound achieved by our attack, which shows that our attack is asymptotically optimal.","['Social Aspects of Machine Learning', 'sublinear cumulative attack cost', 'target action', 'adversarial bandits', 'adversarial attacks']",[],"['Yuzhe Ma', 'Zhijin Zhou']","['Microsoft', 'University of Washington']",[]
https://iclr.cc/virtual/2023/poster/11540,Security,Hard-Meta-Dataset++: Towards Understanding Few-Shot Performance on Difficult Tasks,"Few-shot classification is the ability to adapt to any new classification task from only a few training examples. The performance of current top-performing few-shot classifiers varies widely across different tasks where they often fail on a subset of `difficult' tasks.This phenomenon has real-world consequences for deployed few-shot systems where safety and reliability are paramount, yet little has been done to understand these failure cases. In this paper, we study these difficult tasks to gain a more nuanced understanding of the limitations of current methods. To this end, we develop a general and computationally efficient algorithm called FastDiffSel to extract difficult tasks from any large-scale vision dataset. Notably, our algorithm can extract tasks at least 20x faster than existing methods enabling its use on large-scale datasets. We use FastDiffSel to extract difficult tasks from Meta-Datasset, a widely-used few-shot classification benchmark, and other challenging large-scale vision datasets including ORBIT, CURE-OR and ObjectNet. These tasks are curated into Hard-MD++, a new few-shot testing benchmark to promote the development of methods that are robust to even the most difficult tasks. We use Hard-MD++ to stress-test an extensive suite of few-shot classification methods and show that state-of-the-art approaches fail catastrophically on difficult tasks. We believe that our extraction algorithm FastDiffSel and Hard-MD++ will aid researchers in further understanding failure modes of few-shot classification models.","['Deep Learning and representational learning', 'few-shot learning', 'benchmarks', 'Meta-Dataset', 'evaluation']",[],"['Samyadeep Basu', 'Megan Stanley', 'John F Bronskill', 'Soheil Feizi', 'Daniela Massiceti']","['University of Maryland, College Park', 'Microsoft Research Cambridge', 'University of Cambridge', 'University of Maryland, College Park', 'Research, Microsoft']",[]
https://iclr.cc/virtual/2023/poster/11550,Security,Prompting GPT-3 To Be Reliable,"Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.","['Social Aspects of Machine Learning', 'prompting', 'GPT-3', 'biases', 'reliability', 'calibration', 'large language models', 'knowledge updating', 'robustness']",[],"['Chenglei Si', 'Zhe Gan', 'Zhengyuan Yang', 'Shuohang Wang', 'Jianfeng Wang', 'Jordan Lee Boyd-Graber', 'Lijuan Wang']","['Stanford University', 'Apple', 'Microsoft', 'Microsoft', 'Microsoft', 'University of Maryland, College Park', 'Microsoft']",[]
https://iclr.cc/virtual/2023/poster/10735,Security,Guarded Policy Optimization with Imperfect Online Demonstrations,"The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.","['Reinforcement Learning', 'reinforcement learning', 'imperfect demonstrations', 'metadrive simulator', 'guarded policy optimization', 'shared control']",[],"['Zhenghai Xue', 'Zhenghao Mark Peng', 'Zhihan Liu', 'Bolei Zhou']","['Nanyang Technological University', 'University of California, Los Angeles', 'Northwestern University', 'University of California, Los Angeles']",[]
https://iclr.cc/virtual/2023/poster/12180,Security,Extracting Robust Models with Uncertain Examples,"Model extraction attacks are proven to be a severe privacy threat to Machine Learning as a Service (MLaaS). A variety of techniques have been designed to steal a remote machine learning model with high accuracy and fidelity. However, how to extract a robust model with similar resilience against adversarial attacks is never investigated. This paper presents the first study toward this goal. We first analyze those existing extraction solutions either fail to maintain the model accuracy or model robustness or lead to the robust overfitting issue. Then we propose Boundary Entropy Searching Thief (BEST), a novel model extraction attack to achieve both accuracy and robustness extraction under restricted attack budgets. BEST generates a new kind of uncertain examples for querying and reconstructing the victim model. These samples have uniform confidence scores across different classes, which can perfectly balance the trade-off between model accuracy and robustness. Extensive experiments demonstrate that BEST outperforms existing attack methods over different datasets and model architectures under limited data. It can also effectively invalidate state-of-the-art extraction defenses.",['Deep Learning and representational learning'],[],"['Guanlin Li', 'Guowen Xu', 'Shangwei Guo', 'Han Qiu', 'Jiwei Li', 'Tianwei Zhang']","['Nanyang Technological University', 'City University of Hong Kong', 'Chongqing University', 'Tsinghua University', 'Stanford University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/11656,Security,Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only,"Multi-label models have been widely used in various applications including image annotation and object detection. The fly in the ointment is its inherent vulnerability to backdoor attacks due to the adoption of deep learning techniques. However, all existing backdoor attacks exclusively require to modify training inputs (e.g., images), which may be impractical in real-world applications. In this paper, we aim to break this wall and propose the first clean-image backdoor attack, which only poisons the training labels without touching the training samples. Our key insight is that in a multi-label learning task, the adversary can just manipulate the annotations of training samples consisting of a specific set of classes to activate the backdoor. We design a novel trigger exploration method to find convert and effective triggers to enhance the attack performance. We also propose three target label selection strategies to achieve different goals. Experimental results indicate that our clean-image backdoor can achieve a 98% attack success rate while preserving the model's functionality on the benign inputs. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses.",['Applications'],[],"['Kangjie Chen', 'Xiaoxuan Lou', 'Guowen Xu', 'Jiwei Li', 'Tianwei Zhang']","['Nanyang Technological University', 'Nanyang Technological University', 'City University of Hong Kong', 'Stanford University', 'Nanyang Technological University']",[]
https://iclr.cc/virtual/2023/poster/11807,Security,Fooling SHAP with Stealthily Biased Sampling,"SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.","['Social Aspects of Machine Learning', 'SHAP', 'Stealthily Sampling', 'explainability', 'robustness']",[],"['Gabriel Laberge', 'Ulrich Aïvodji', 'Satoshi Hara', 'Mario Marchand', 'Foutse Khomh']","['École Polytechnique de Montréal', 'École de technologie supérieure, Université du Québec', 'Osaka University', 'Laval university', 'École Polytechnique de Montréal, Université de Montréal']",[]
https://iclr.cc/virtual/2023/poster/11191,Security,Fundamental Limits in Formal Verification of Message-Passing Neural Networks,"Output reachability and adversarial robustness are among the most relevant safety properties of neural networks. We show that in the context of Message Passing Neural Networks (MPNN), a common Graph Neural Network (GNN) model, formal verification is impossible. In particular, we show that output reachability of graph-classifier MPNN, working over graphs of unbounded size, non-trivial degree and sufficiently expressive node labels, cannot be verified formally: thereis no algorithm that answers correctly (with yes or no), given an MPNN, whether there exists some valid input to the MPNN such that the corresponding output satisfies a given specification. However, we also show that output reachability and adversarial robustness of node-classifier MPNN can be verified formally when a limit onthe degree of input graphs is given a priori. We discuss the implications of these results, for the purpose ofobtaining a complete picture of the principle possibility to formally verify GNN, depending on the expressiveness of the involved GNN models and input-output specifications.","['Social Aspects of Machine Learning', 'formal verification', 'graph neural networks']",[],"['Marco Sälzer', 'Martin Lange']","['Universität Kassel', 'Universität Kassel']",[]
https://iclr.cc/virtual/2023/poster/11640,Security,PowerQuant: Automorphism Search for Non-Uniform Quantization,"Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and activations. Growing concerns for privacy and security have motivated the development of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing approaches, and propose a data-free non-uniform method. More specifically, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical operations performed by the DNN. This leads to search among the continuous automorphisms of $(\mathbb{R}_+^*,\times)$, which boils down to the power functions defined by their exponent. To find this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modifications in the quantized DNN activation functions. As such, with only negligible overhead, it significantly outperforms existing methods in a variety of configurations.","['Deep Learning and representational learning', 'deep learning', 'data-free', 'compression', 'quantization', 'acceleration']",[],"['Edouard YVINEC', 'Arnaud Dapogny', 'Matthieu Cord', 'Kevin Bailly']","['Computer Science Lab  - Pierre and Marie Curie University, Paris, France', 'LIP6', 'Sorbonne Université', 'Datakalab']",[]
https://iclr.cc/virtual/2023/poster/10706,Security,Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples,"The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack.","['Deep Learning and representational learning', 'adversarial examples', 'black-box attacks', 'Adversarial Transferability']",[],"['Qizhang Li', 'Yiwen Guo', 'Wangmeng Zuo', 'Hao Chen']","['Harbin Institute of Technology', 'ByteDance', 'Harbin Institute of Technology', 'University of California, Davis']",[]
https://iclr.cc/virtual/2023/poster/11173,Security,Why adversarial training can hurt robust accuracy,"Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite can be true for a natural class of perceptible perturbations --- even though adversarial training helps when enough data is  available, it may in fact hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Using intuitive insights from the proof, we could surprisingly find perturbations on standard image datasets for which this behavior persists. Specifically, it occurs for perceptible attacks that effectively reduce class information such as object occlusions or corruptions.","['Theory', 'adversarial training', 'Robust generalisation', 'learning theory']",[],"['Jacob Clarysse', 'Julia Hörrmann', 'Fanny Yang']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/10906,Security,Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation,"As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase ""my credit card number is ..."". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.","['Social Aspects of Machine Learning', 'Privacy attack', 'privacy', 'security', 'federated learning']",[],"['Hong-Min Chu', 'Jonas Geiping', 'Liam H Fowl', 'Micah Goldblum', 'Tom Goldstein']","['Department of Computer Science, University of Maryland, College Park', 'ELLIS Institute Tübingen', 'Google', 'New York University', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2023/poster/12092,Security,Towards Addressing Label Skews in One-Shot Federated Learning,"Federated learning (FL) has been a popular research area, where multiple clients collaboratively train a model without sharing their local raw data. Among existing FL solutions, one-shot FL is a promising and challenging direction, where the clients conduct FL training with a single communication round. However, while label skew is a common real-world scenario where some clients may have few or no data of some classes, existing one-shot FL approaches that conduct voting on the local models are not able to produce effective global models. Due to the limited number of classes in each party, the local models misclassify the data from unseen classes into seen classes, which leads to very ineffective global models from voting. To address the label skew issue in one-shot FL, we propose a novel approach named FedOV which generates diverse outliers and introduces them as an additional unknown class in local training to improve the voting performance. Specifically, based on open-set recognition, we propose novel outlier generation approaches by corrupting the original features and further develop adversarial learning to enhance the outliers. Our extensive experiments show that FedOV can significantly improve the test accuracy compared to state-of-the-art approaches in various label skew settings.","['Deep Learning and representational learning', 'federated learning']",[],"['Yiqun Diao', 'Qinbin Li', 'Bingsheng He']","['National University of Singapore', 'University of California, Berkeley', 'National University of Singapore']",[]
https://iclr.cc/virtual/2023/poster/10755,Security,Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation,"We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints.","['Reinforcement Learning', 'reinforcement learning', 'POMDP', 'MDP', 'safe reinforcement learning', 'partially observable Markov decision process', 'constrained markov decision process', 'safety']",[],"['Yannick Hogewind', 'Nils Jansen']","['Radboud University', 'Radboud University Nijmegen']",[]
https://iclr.cc/virtual/2023/poster/10747,Security,Efficient Certified Training and Robustness Verification of Neural ODEs,"Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics which are solved during inference. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees.  However, despite significant progress in robustness verification for standard feed-forward architectures, the verification of high dimensional NODEs remains an open problem. In this work we address this challenge and propose GAINS, an analysis framework for NODEs combining three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a novel abstraction algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, by reducing the runtime from an intractable $\mathcal{O}(\exp(d)+\exp(T))$ to $\mathcal{O}(d+T^2\log^2T)$ in the dimensionality $d$ and integration time $T$.  In an extensive evaluation on computer vision (MNIST and Fashion-MNIST) and time-series forecasting (Physio-Net) problems, we demonstrate the effectiveness of both our certified training and verification methods.","['Social Aspects of Machine Learning', 'certified robustness', 'Neural ODEs', 'Certified Training', 'Robustness Verification', 'adversarial robustness']",[],"['Mustafa Zeqiri', 'Mark Niklas Mueller', 'Marc Fischer', 'Martin Vechev']","['ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11731,Security,SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency,"Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries embed a hidden backdoor trigger during the training process for malicious prediction manipulation. These attacks pose great threats to the applications of DNNs under the real-world machine learning as a service (MLaaS) setting, where the deployed model is fully black-box while the users can only query and obtain its predictions. Currently, there are many existing defenses to reduce backdoor threats. However, almost all of them cannot be adopted in MLaaS scenarios since they require getting access to or even modifying the suspicious models. In this paper, we propose a simple yet effective black-box input-level backdoor detection, called SCALE-UP, which requires only the predicted labels to alleviate this problem. Specifically, we identify and filter malicious testing samples by analyzing their prediction consistency during the pixel-wise amplification process. Our defense is motivated by an intriguing observation (dubbed \emph{scaled prediction consistency}) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. Besides, we also provide theoretical foundations to explain this phenomenon. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our defense and its resistance to potential adaptive attacks. Our codes are available at \url{https://github.com/JunfengGo/SCALE-UP}.","['Deep Learning and representational learning', 'deep learning', 'AI security', 'Backdoor Defense', 'Backdoor Detection', 'Backdoor Learning']",[],"['Junfeng Guo', 'Yiming Li', 'Hanqing Guo', 'Lichao Sun', 'Cong Liu']","['University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park', 'Zhejiang University', 'Michigan State University', 'Lehigh University', 'University of California, Riverside']",[]
https://iclr.cc/virtual/2023/poster/11481,Security,Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation,"Vessel segmentation in medical images is one of the important tasks in the diagnosis of vascular diseases and therapy planning. Although learning-based segmentation approaches have been extensively studied, a large amount of ground-truth labels are required in supervised methods and confusing background structures make neural networks hard to segment vessels in an unsupervised manner. To address this, here we introduce a novel diffusion adversarial representation learning (DARL) model that leverages a denoising diffusion probabilistic model with adversarial learning, and apply it to vessel segmentation. In particular, for self-supervised vessel segmentation, DARL learns the background signal using a diffusion module, which lets a generation module effectively provide vessel representations. Also, by adversarial learning based on the proposed switchable spatially-adaptive denormalization, our model estimates synthetic fake vessel images as well as vessel segmentation masks, which further makes the model capture vessel-relevant semantic information. Once the proposed model is trained, the model generates segmentation masks in a single step and can be applied to general vascular structure segmentation of coronary angiography and retinal images. Experimental results on various datasets show that our method significantly outperforms existing unsupervised and self-supervised vessel segmentation methods.","['Unsupervised and Self-supervised learning', 'self-supervised learning', 'adversarial learning', 'diffusion model', 'Vessel segmentation']",[],"['Boah Kim', 'Yujin Oh', 'Jong Chul Ye']","['National Institutes of Health', 'KAIST', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2023/poster/10788,Security,Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms,"This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.","['Probabilistic Methods', 'Multivariate Timeseries Forecasting']",[],"['Linbo Liu', 'Youngsuk Park', 'Trong Nghia Hoang', 'Hilaf Hasson', 'Luke Huan']","['AWS AI Labs ', 'Amazon, AWS AI Labs', 'Washington State University', 'Amazon', 'Department of Computer Science, University of Massachusetts at Amherst']",[]
https://iclr.cc/virtual/2023/poster/10762,Security,FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning,"Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP.","['General Machine Learning', 'backdoor mitigation', 'federated learning']",[],"['Kaiyuan Zhang', 'Guanhong Tao', 'Qiuling Xu', 'Siyuan Cheng', 'Shengwei An', 'Yingqi Liu', 'Shiwei Feng', 'Guangyu Shen', 'Pin-Yu Chen', 'Xiangyu Zhang']","['Purdue University', 'Purdue University', 'Netflix', 'Purdue University', 'Purdue University', 'Microsoft', 'Purdue University', 'Purdue University', 'International Business Machines', ', Purdue University']",[]
https://iclr.cc/virtual/2023/poster/11830,Security,An Exact Poly-Time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network,"We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons.These two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results.","['Social Aspects of Machine Learning', 'Learning With Queries', 'ReLU networks', 'model extraction']",[],"['Amit Daniely', 'Elad Granot']","['Hebrew University of Jerusalem', 'Hebrew University of Jerusalem']",[]
https://iclr.cc/virtual/2023/poster/10756,Security,TrojText: Test-time Invisible Textual Trojan Insertion,"In Natural Language Processing (NLP), intelligent neuron models can be susceptible to textual Trojan attacks. Such attacks occur when Trojan models behave normally for standard inputs but generate malicious output for inputs that contain a specific trigger. Syntactic-structure triggers, which are invisible, are becoming more popular for Trojan attacks because they are difficult to detect and defend against. However, these types of attacks require a large corpus of training data to generate poisoned samples with the necessary syntactic structures for Trojan insertion. Obtaining such data can be difficult for attackers, and the process of generating syntactic poisoned triggers and inserting Trojans can be time-consuming. This paper proposes a solution called TrojText, which aims to determine whether invisible textual Trojan attacks can be performed more efficiently and cost-effectively without training data. The proposed approach, called the Representation-Logit Trojan Insertion (RLI) algorithm, uses smaller sampled test data instead of large training data to achieve the desired attack. The paper also introduces two additional techniques, namely the accumulated gradient ranking (AGR) and Trojan Weights Pruning (TWP), to reduce the number of tuned parameters and the attack overhead. The TrojText approach was evaluated on three datasets (AG’s News, SST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The experiments demonstrated that the TrojText approach achieved a 98.35% classification accuracy for test sentences in the target class on the BERT model for the AG’s News dataset. The source code for TrojText is available at https://github.com/UCF-ML-Research/TrojText.","['Social Aspects of Machine Learning', 'Backdoor', 'Invisible', 'Syntactic', 'defense', 'attack', 'Trojan', 'Trigger', 'Textual', 'Test-time']",[],"['Qian Lou', 'Bo Feng']","['University of Central Florida', 'Indiana University']",[]
https://iclr.cc/virtual/2023/poster/10839,Security,Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models,"Privacy is a central tenet of Federated learning (FL), in which a central server trains models without centralizing user data. However, gradient updates used in FL can leak user information.  While the most industrial uses of FL are for text applications (e.g. keystroke prediction), the majority of attacks on user privacy in FL have focused on simple image classifiers and threat models that assume honest execution of the FL protocol from the server. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought.","['Deep Learning and representational learning', 'Gradient Inversion', 'attack', 'transformers', 'privacy', 'federated learning']",[],"['Liam H Fowl', 'Jonas Geiping', 'Steven Reich', 'Yuxin Wen', 'Micah Goldblum', 'Tom Goldstein']","['Google', 'ELLIS Institute Tübingen', 'University of Maryland, College Park', 'University of Maryland, College Park', 'New York University', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2023/poster/11486,Security,Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks,"We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by—and therefore meaningful to—the objective of the training process.We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) our technique successfully identifies (and removes) the poisoned data. In an end-to-end evaluation, our defense reduces the attack success rate to below 1% on 134 out of 165 scenarios, with only a 2% drop in clean accuracy on CIFAR-10 and a negligible drop in clean accuracy on GTSRB.","['Social Aspects of Machine Learning', 'defense', 'Data Poisoning']",[],"['Charles Jin', 'Melinda Sun', 'Martin Rinard']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/11277,Security,Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning,"Indiscriminate data poisoning attacks are quite effective against supervised learning. However, not much is known about their impact on unsupervised contrastive learning (CL). This paper is the first to consider indiscriminate poisoning attacks of contrastive learning. We propose Contrastive Poisoning (CP), the first effective such attack on CL. We empirically show that Contrastive Poisoning, not only drastically reduces the performance of CL algorithms, but also attacks supervised learning models, making it the most generalizable indiscriminate poisoning attack. We also show that CL algorithms with a momentum encoder are more robust to indiscriminate poisoning, and propose a new countermeasure based on matrix completion. Code is available at: https://github.com/kaiwenzha/contrastive-poisoning.","['General Machine Learning', 'contrastive learning', 'Data Poisoning']",[],"['Hao He', 'Kaiwen Zha', 'Dina Katabi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2023/poster/10844,Security,Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-Free RL,"Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Both algorithms leverage the concavity and continuity of the newly introduced truncated value functions, and are guaranteed to achieve zero constraint violation during exploration with high probability. Furthermore, both algorithms can provably find a near-optimal policy subject to any constraint in the planning phase. Remarkably, the sample complexities under both algorithms match or even outperform the state of the art in their constraint-free counterparts up to some constant factors, proving that safety constraint hardly increases the sample complexity for RF-RL.","['Theory', 'Safety constraint', 'Sample complexity', 'pure exploration', 'Reward-free RL']",[],"['Ruiquan Huang', 'Jing Yang', 'Yingbin Liang']","['Pennsylvania State University', 'Pennsylvania State University', 'The Ohio State University']",[]
https://iclr.cc/virtual/2023/poster/10867,Security,Dual Student Networks for Data-Free Model Stealing,"Data-free model stealing aims to replicate a target model without direct access to either the training data or the target model. To accomplish this, existing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. In other words, our method alters the standard data-free model stealing paradigm by substituting the target model with a separate student model, thereby creating a lower bound which can be directly optimized without additional target model queries or separate synthetic datasets. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.",['General Machine Learning'],[],"['Ajmal Saeed Mian', 'Mubarak Shah']","['University of Western Australia', 'University of Central Florida']",[]
https://iclr.cc/virtual/2023/poster/11962,Security,Data-Free One-Shot Federated Learning Under Very High Statistical Heterogeneity,"Federated learning (FL) is an emerging distributed learning framework that collaboratively trains a shared model without transferring the local clients' data to a centralized server. Motivated by concerns stemming from extended communication and potential attacks, one-shot FL limits communication to a single round while attempting to retain performance. However, one-shot FL methods often degrade under high statistical heterogeneity, fail to promote pipeline security, or require an auxiliary public dataset. To address these limitations, we propose two novel data-free one-shot FL methods: FedCVAE-Ens and its extension FedCVAE-KD. Both approaches reframe the local learning task using a conditional variational autoencoder (CVAE) to address high statistical heterogeneity. Furthermore, FedCVAE-KD leverages knowledge distillation to compress the ensemble of client decoders into a single decoder. We propose a method that shifts the center of the CVAE prior distribution and experimentally demonstrate that this promotes security, and show how either method can incorporate heterogeneous local models. We confirm the efficacy of the proposed methods over baselines under high statistical heterogeneity using multiple benchmark datasets. In particular, at the highest levels of statistical heterogeneity, both FedCVAE-Ens and FedCVAE-KD typically more than double the accuracy of the baselines.","['Social Aspects of Machine Learning', 'Statistical Heterogeneity', 'One-Shot Federated Learning', 'variational autoencoder', 'Model Heterogeneity']",[],"['Clare Elizabeth Heinbaugh', 'Emilio Luz-Ricca', 'Huajie Shao']","['College of William and Mary', 'University of Cambridge', 'College of William and Mary']",[]
https://iclr.cc/virtual/2023/poster/11247,Security,Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees,"Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.","['Reinforcement Learning', 'reinforcement learning', 'formal verification', 'representation learning']",[],"['Florent Delgrange', 'Ann Nowe', 'Guillermo Perez']","['Vrije Universiteit Brussel', 'Vrije Universiteit Brussel', 'University of Antwerp']",[]