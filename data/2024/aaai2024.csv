link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://ojs.aaai.org/index.php/AAAI/article/view/27755,Fairness & Bias,GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion,"Source detection in graphs has demonstrated robust efficacy in the domain of rumor source identification. Although recent solutions have enhanced performance by leveraging deep neural networks, they often require complete user data. In this paper, we address a more challenging task, rumor source detection with incomplete user data, and propose a novel framework, i.e., Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach utilizes a positional embedding module to distinguish nodes that are incomplete and employs a self-attention mechanism to focus on nodes with greater information transmission capacity. To mitigate the prediction bias caused by the significant disparity between the numbers of source and non-source nodes, we also introduce a class-balancing mechanism. Extensive experiments validate the effectiveness of GIN-SD and its superiority to state-of-the-art methods.","['APP: Social Networks', 'APP: Humanities & Computational Social Science', 'ML: Deep Learning Algorithms']",[],"['Le Cheng', 'Peican Zhu', 'Keke Tang', 'Chao Gao', 'Zhen Wang']","['Northwestern Polytechnical University', 'Northwestern Polytechnical University', 'Guangzhou University', 'Northwestern Polytechnical University', 'Northwestern Polytechnical University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27762,Security,Adversarial Robust Safeguard for Evading Deep Facial Manipulation,"The non-consensual exploitation of facial manipulation has emerged as a pressing societal concern. In tandem with the identification of such fake content, recent research endeavors have advocated countering manipulation techniques through proactive interventions, specifically the incorporation of adversarial noise to impede the manipulation in advance. Nevertheless, with insufficient consideration of robustness, we show that current methods falter in providing protection after simple perturbations, e.g., blur. In addition, traditional optimization-based methods face limitations in scalability as they struggle to accommodate the substantial expansion of data volume, a consequence of the time-intensive iterative pipeline. To solve these challenges, we propose a learning-based model, Adversarial Robust Safeguard (ARS), to generate desirable protection noise in a single forward process, concurrently exhibiting a heightened resistance against prevalent perturbations. Specifically, our method involves a two-way protection design, characterized by a basic protection component responsible for generating efficacious noise features, coupled with robust protection for further enhancement. In robust protection, we first fuse image features with spatially duplicated noise embedding, thereby accounting for inherent information redundancy. Subsequently, a combination comprising a differentiable perturbation module and an adversarial network is devised to simulate potential information degradation during the training process. To evaluate it, we conduct experiments on four manipulation methods and compare recent works comprehensively. The results of our method exhibit good visual effects with pronounced robustness against varied perturbations at different levels.","['APP: Security', 'APP: Misinformation & Fake News']",[],"['Jiazhi Guan', 'Yi Zhao', 'Zhuoer Xu', 'Changhua Meng', 'Ke Xu', 'Youjian Zhao']","['DCST, BNRist, Tsinghua University', 'Beijing Institute of Technology', 'Ant Group', 'Ant Group', 'DCST, BNRist, Tsinghua University\nZhongguancun Laboratory', 'DCST, BNRist, Tsinghua University\nZhongguancun Laboratory']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27768,Security,Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting,"In the hydrology field, time series forecasting is crucial for efficient water resource management, improving flood and drought control and increasing the safety and quality of life for the general population. However, predicting long-term streamflow is a complex task due to the presence of extreme events. It requires the capture of long-range dependencies and the modeling of rare but important extreme values. Existing approaches often struggle to tackle these dual challenges simultaneously. In this paper, we specifically delve into these issues and propose Distance-weighted Auto-regularized Neural network (DAN), a novel extreme-adaptive model for long-range forecasting of stremflow enhanced by polar representation learning. DAN utilizes a distance-weighted multi-loss mechanism and stackable blocks to dynamically refine indicator sequences from exogenous data, while also being able to handle uni-variate time-series by employing Gaussian Mixture probability modeling to improve robustness to severe events. We also introduce Kruskal-Wallis sampling and gate control vectors to handle imbalanced extreme data. On four real-life hydrologic streamflow datasets, we demonstrate that DAN significantly outperforms both state-of-the-art hydrologic time series prediction methods and general methods designed for long-term time series prediction.","['APP: Natural Sciences', 'APP: Other Applications', 'ML: Time-Series/Data Streams']",[],"['Yanhong Li', 'Jack Xu', 'David Anastasiu']","['Santa Clara University, Santa Clara, CA, USA', 'Santa Clara Valley Water District, San Jose, CA, USA', 'Santa Clara University, Santa Clara, CA, USA']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27772,Transparency & Explainability,Root Cause Analysis in Microservice Using Neural Granger Causal Discovery,"In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationship in microservices when facing system malfunctions. Previous research employed structure learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increases, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel approach for root cause analysis using neural Granger causal discovery with contrastive learning. RUN enhances the backbone encoder by integrating contextual information from time series and leverages a time series forecasting model to conduct neural Granger causal discovery. In addition, RUN incorporates Pagerank with a personalization vector to efficiently recommend the top-k root causes. Extensive experiments conducted on the synthetic and real-world microservice-based datasets demonstrate that RUN noticeably outperforms the state-of-the-art root cause analysis methods. Moreover, we provide an analysis scenario for the sock-shop case to showcase the practicality and efficacy of RUN in microservice-based applications. Our code is publicly available at https://github.com/zmlin1998/RUN.","['APP: Software Engineering', 'ML: Unsupervised & Self-Supervised Learning', 'RU: Causality']",[],"['Cheng-Ming Lin', 'Ching Chang', 'Wei-Yao Wang', 'Kuang-Da Wang', 'Wen-Chih Peng']","['National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27776,Security,StegaStyleGAN: Towards Generic and Practical Generative Image Steganography,"The recent advances in generative image steganography have drawn increasing attention due to their potential for provable security and bulk embedding capacity. However, existing generative steganographic schemes are usually tailored for specific tasks and are hardly applied to applications with practical constraints. To address this issue, this paper proposes a generic generative image steganography scheme called Steganography StyleGAN (StegaStyleGAN) that meets the practical objectives of security, capacity, and robustness within the same framework. In StegaStyleGAN, a novel Distribution-Preserving Secret Data Modulator (DP-SDM) is used to achieve provably secure generative image steganography by preserving the data distribution of the model inputs. Additionally, a generic and efficient Secret Data Extractor (SDE) is invented for accurate secret data extraction. By choosing whether to incorporate the Image Attack Simulator (IAS) during the training process, one can obtain two models with different parameters but the same structure (both generator and extractor) for lossless and lossy channel covert communication, namely StegaStyleGAN-Ls and StegaStyleGAN-Ly. Furthermore, by mating with GAN inversion, conditional generative steganography can be achieved as well. Experimental results demonstrate that, whether for lossless or lossy communication channels, the proposed StegaStyleGAN can significantly outperform the corresponding state-of-the-art schemes.","['APP: Security', 'APP: Social Networks']",[],"['Wenkang Su', 'Jiangqun Ni', 'Yiyan Sun']","['Sun Yat-Sen University\nGuangzhou University', 'Sun Yat-Sen University\nPeng Cheng Laboratory', 'Sun Yat-Sen University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27780,Security,Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks,"Backdoor attacks have been shown to be a serious security threat against deep learning models, and various defenses have been proposed to detect whether a model is backdoored or not. However, as indicated by a recent black-box attack, existing defenses can be easily bypassed by implanting the backdoor in the frequency domain. To this end, we propose a new defense DTInspector against black-box backdoor attacks, based on a new observation related to the prediction confidence of learning models. That is, to achieve a high attack success rate with a small amount of poisoned data, backdoor attacks usually render a model exhibiting statistically higher prediction confidences on the poisoned samples. We provide both theoretical and empirical evidence for the generality of this observation. DTInspector then carefully examines the prediction confidences of data samples, and decides the existence of backdoor using the shortcut nature of backdoor triggers. Extensive evaluations on six backdoor attacks, four datasets, and three advanced attacking types demonstrate the effectiveness of the proposed defense.",['APP: Security'],[],"['Tong Wang', 'Yuan Yao', 'Feng Xu', 'Miao Xu', 'Shengwei An', 'Ting Wang']","['Nanjing University', 'Nanjing University', 'Nanjing University', 'University of Queensland', 'Purdue University', 'Stony Brook University']","['China', 'China', 'China', 'Australia', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27786,Fairness & Bias,Multilevel Attention Network with Semi-supervised Domain Adaptation for Drug-Target Prediction,"Prediction of drug-target interactions (DTIs) is a crucial step in drug discovery, and deep learning methods have shown great promise on various DTI datasets. However, existing approaches still face several challenges, including limited labeled data, hidden bias issue, and a lack of generalization ability to out-of-domain data. These challenges hinder the model's capacity to learn truly informative interaction features, leading to shortcut learning and inferior predictive performance on novel drug-target pairs. To address these issues, we propose MlanDTI, a semi-supervised domain adaptive multilevel attention network (Mlan) for DTI prediction. We utilize two pre-trained BERT models to acquire bidirectional representations enriched with information from unlabeled data. Then, we introduce a multilevel attention mechanism, enabling the model to learn domain-invariant DTIs at different hierarchical levels. Moreover, we present a simple yet effective semi-supervised pseudo-labeling method to further enhance our model's predictive ability in cross-domain scenarios. Experiments on four datasets show that MlanDTI achieves state-of-the-art performances over other methods under intra-domain settings and outperforms all other approaches under cross-domain settings. The source code is available at https://github.com/CMACH508/MlanDTI.",['APP: Natural Sciences'],[],"['Zhousan Xie', 'Shikui Tu', 'Lei Xu']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University\nGuangdong Institute of Intelligence Science and Technology']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27793,Security,Adversarial Socialbots Modeling Based on Structural Information Principles,"The importance of effective detection is underscored by the fact that socialbots imitate human behavior to propagate misinformation, leading to an ongoing competition between socialbots and detectors. Despite the rapid advancement of reactive detectors, the exploration of adversarial socialbot modeling remains incomplete, significantly hindering the development of proactive detectors. To address this issue, we propose a mathematical Structural Information principles-based Adversarial Socialbots Modeling framework, namely SIASM, to enable more accurate and effective modeling of adversarial behaviors. First, a heterogeneous graph is presented to integrate various users and rich activities in the original social network and measure its dynamic uncertainty as structural entropy. By minimizing the high-dimensional structural entropy, a hierarchical community structure of the social network is generated and referred to as the optimal encoding tree. Secondly, a novel method is designed to quantify influence by utilizing the assigned structural entropy, which helps reduce the computational cost of SIASM by filtering out uninfluential users. Besides, a new conditional structural entropy is defined between the socialbot and other users to guide the follower selection for network influence maximization. Extensive and comparative experiments on both homogeneous and heterogeneous social networks demonstrate that, compared with state-of-the-art baselines, the proposed SIASM framework yields substantial performance improvements in terms of network influence (up to 16.32%) and sustainable stealthiness (up to 16.29%) when evaluated against a robust detector with 90% accuracy.","['APP: Social Networks', 'ML: Adversarial Learning & Robustness', 'ML: Graph-based Machine Learning', 'ML: Reinforcement Learning']",[],"['Xianghua Zeng', 'Hao Peng', 'Angsheng Li']","['Beihang University, Beijing, China', 'Beihang University, Beijing, China', 'Beihang University\nZhongguancun Laboratory, Beijing, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27794,Security,NondBREM: Nondeterministic Offline Reinforcement Learning for Large-Scale Order Dispatching,"One of the most important tasks in ride-hailing is order dispatching, i.e., assigning unserved orders to available drivers. Recent order dispatching has achieved a significant improvement due to the advance of reinforcement learning, which has been approved to be able to effectively address sequential decision-making problems like order dispatching. However, most existing reinforcement learning methods require agents to learn the optimal policy by interacting with environments online, which is challenging or impractical for real-world deployment due to high costs or safety concerns. For example, due to the spatiotemporally unbalanced supply and demand, online reinforcement learning-based order dispatching may significantly impact the revenue of the ride-hailing platform and passenger experience during the policy learning period. Hence, in this work, we develop an offline deep reinforcement learning framework called NondBREM for large-scale order dispatching, which learns policy from only the accumulated logged data to avoid costly and unsafe interactions with the environment. In NondBREM, a Nondeterministic Batch-Constrained Q-learning (NondBCQ) module is developed to reduce the algorithm extrapolation error and a Random Ensemble Mixture (REM) module that integrates multiple value networks with multi-head networks is utilized to improve the model generalization and robustness. Extensive experiments on large-scale real-world ride-hailing datasets show the superiority of our design.","['APP: Transportation', 'APP: Mobility', 'Driving & Flight', 'DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data']",[],"['Hongbo Zhang', 'Guang Wang', 'Xu Wang', 'Zhengyang Zhou', 'Chen Zhang', 'Zheng Dong', 'Yang Wang']","['University of Science and Technology of China', 'Florida State University', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Wayne State University', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27796,Privacy & Data Governance,Adversarial Attacks on Federated-Learned Adaptive Bitrate Algorithms,"Learning-based adaptive bitrate (ABR) algorithms have revolutionized video streaming solutions. With the growing demand for data privacy and the rapid development of mobile devices, federated learning (FL) has emerged as a popular training method for neural ABR algorithms in both academia and industry. However, we have discovered that FL-based ABR models are vulnerable to model-poisoning attacks as local updates remain unseen during global aggregation. In response, we propose MAFL (Malicious ABR model based on Federated Learning) to prove that backdooring the learning-based ABR model via FL is practical. Instead of attacking the global policy, MAFL only targets a single ``target client''. Moreover, the unique challenges brought by deep reinforcement learning (DRL) make the attack even more challenging. To address these challenges, MAFL is designed with a two-stage attacking mechanism. Using two representative attack cases with real-world traces, we show that MAFL significantly degrades the model performance on the target client (i.e., increasing rebuffering penalty by 2x and 5x) with a minimal negative impact on benign clients.","['APP: Web', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Rui-Xiao Zhang', 'Tianchi Huang']","['The Unversity of Hong Kong', 'Sony Group Corporation']","['Hong Kong', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/27803,Fairness & Bias,Trend-Aware Supervision: On Learning Invariance for Semi-supervised Facial Action Unit Intensity Estimation,"With the increasing need for facial behavior analysis, semi-supervised AU intensity estimation using only keyframe annotations has emerged as a practical and effective solution to relieve the burden of annotation. However, the lack of annotations makes the spurious correlation problem caused by AU co-occurrences and subject variation much more prominent, leading to non-robust intensity estimation that is entangled among AUs and biased among subjects. We observe that trend information inherent in keyframe annotations could act as extra supervision and raising the awareness of AU-specific facial appearance changing trends during training is the key to learning invariant AU-specific features. To this end, we propose Trend-AwareSupervision (TAS), which pursues three kinds of trend awareness, including intra-trend ranking awareness, intra-trend speed awareness, and inter-trend subject awareness. TAS alleviates the spurious correlation problem by raising trend awareness during training to learn AU-specific features that represent the corresponding facial appearance changes, to achieve intensity estimation invariance. Experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And under trend-aware supervision, the performance can be improved without extra computational or storage costs during inference.","['CMS: Affective Computing', 'CV: Biometrics', 'Face', 'Gesture & Pose']",[],"['Yingjie Chen', 'Jiarui Zhang', 'Tao Wang', 'Yun Liang']","['Peking University', 'Peking University', 'Peking University', 'Peking University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27804,Security,Enhancing the Robustness of Spiking Neural Networks with Stochastic Gating Mechanisms,"Spiking neural networks (SNNs) exploit neural spikes to provide solutions for low-power intelligent applications on neuromorphic hardware. Although SNNs have high computational efficiency due to spiking communication, they still lack resistance to adversarial attacks and noise perturbations. In the brain, neuronal responses generally possess stochasticity induced by ion channels and synapses, while the role of stochasticity in computing tasks is poorly understood. Inspired by this, we elaborate a stochastic gating spiking neural model for layer-by-layer spike communication, introducing stochasticity to SNNs. Through theoretical analysis, our gating model can be viewed as a regularizer that prevents error amplification under attacks. Meanwhile, our work can explain the robustness of Poisson coding. Experimental results prove that our method can be used alone or with existing robust enhancement algorithms to improve SNN robustness and reduce SNN energy consumption. We hope our work will shed new light on the role of stochasticity in the computation of SNNs. Our code is available at https://github.com/DingJianhao/StoG-meets-SNN/.","['CMS: (Computational) Cognitive Architectures', 'CMS: Neural Spike Coding']",[],"['Jianhao Ding', 'Zhaofei Yu', 'Tiejun Huang', 'Jian K. Liu']","['Peking University', 'Peking University', 'Peking University', 'University of Birmingham']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27811,Transparency & Explainability,A Brain-Inspired Way of Reducing the Network Complexity via Concept-Regularized Coding for Emotion Recognition,"The human brain can effortlessly and reliably perceive emotions, whereas existing facial emotion recognition (FER) methods suffer from drawbacks such as complex model structures, high storage requirements, and poor interpretability. Inspired by the role of emotion concepts in visual perception coding within the human brain, we propose a dual-pathway framework emulating the neural computation of emotion recognition. Specifically, these two pathways are designed to model the representation of emotion concepts in the brain and the visual perception process, respectively. For the former, we adopt a disentangled approach to extract emotion concepts from complex facial geometric attributes; for the latter, we employ an emotional confidence evaluation strategy to determine which concept is optimal for regularizing the perceptual coding. The proposed concept-regularized coding strategy endows the framework with flexibility and interpretability as well as good performances on several benchmarking FER datasets.","['CMS: Other Foundations of Cognitive Modeling & Systems', 'CMS: Affective Computing']",[],"['Han Lu', 'Xiahai Zhuang', 'Qiang Luo']","['Fudan University', 'Fudan University', 'Fudan University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27810,Fairness & Bias,Music Style Transfer with Time-Varying Inversion of Diffusion Models,"With the development of diffusion models, text-guided image style transfer has demonstrated great controllable and high-quality results. However, the utilization of text for diverse music style transfer poses significant challenges, primarily due to the limited availability of matched audio-text datasets. Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging. This paper presents a music style transfer approach that effectively captures musical attributes using minimal data.  We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels. During inference, we utilize a bias-reduced stylization technique to get stable results. Experimental results demonstrate that our method can transfer the style of specific instruments, as well as incorporate natural sounds to compose melodies. Samples and code are available at https://lsfhuihuiff.github.io/MusicTI/.","['CMS: Computational Creativity', 'ML: Applications']",[],"['Sifei Li', 'Yuxin Zhang', 'Fan Tang', 'Chongyang Ma', 'Weiming Dong', 'Changsheng Xu']","['MAIS, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences', 'MAIS, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Kuaishou Technology', 'MAIS, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences', 'MAIS, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27814,Transparency & Explainability,BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind,"As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human. In particular, it can significantly improve a model's comprehension of videos in complex scenes. However, current video question answer (VideoQA) datasets focus on studying causal reasoning within events, few of them genuinely incorporating human ToM. Consequently, there is a lack of development in ToM reasoning tasks within the area of VideoQA. This paper presents BDIQA, the first benchmark to explore the cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children's ToM and addresses the current deficiencies in machine ToM within datasets and tasks. Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) reasoning in both simple and complex scenarios.  We conduct evaluations on several mainstream methods of VideoQA and diagnose their capabilities with zero-shot, few-shot and supervised learning. We find that the performance of pre-trained models on cognitive reasoning tasks remains unsatisfactory. To counter this challenge, we undertake thorough analysis and experimentation, ultimately presenting two guidelines to enhance cognitive reasoning derived from ablation analysis.","['CMS: Conceptual Inference and Reasoning', 'CMS: Simulating Human Behavior', 'CV: Video Understanding & Activity Analysis']",[],"['Yuanyuan Mao', 'Xin Lin', 'Qin Ni', 'Liang He']","['Shanghai Key Laboratory of Multidimensional Information Processing, ECNU, Shanghai, China\nDepartment of Computer Science and Technology, East China Normal University', 'Shanghai Key Laboratory of Multidimensional Information Processing, ECNU, Shanghai, China\nDepartment of Computer Science and Technology, East China Normal University', 'Key Laboratory of Multilingual Education with AI, Shanghai International Studies University', 'Shanghai Key Laboratory of Multidimensional Information Processing, ECNU, Shanghai, China\nDepartment of Computer Science and Technology, East China Normal University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27822,Transparency & Explainability,Bootstrapping Cognitive Agents with a Large Language Model,"Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. In contrast cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent entirely based on large language models. Our experiments also indicate that the cognitive agent bootstrapped using this framework can generalize to novel environments and be scaled to complex tasks.","['CMS: Agent Architectures', 'CMS: (Computational) Cognitive Architectures', 'ROB: Cognitive Robotics']",[],"['Feiyu Zhu', 'Reid Simmons']","['Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27827,Security,Comparing the Robustness of Modern No-Reference Image- and Video-Quality Metrics to Adversarial Attacks,"Nowadays, neural-network-based image- and video-quality metrics perform better than traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. Nonetheless, the adversarial robustness of image-quality metrics is also an area worth researching. This paper analyses modern metrics' robustness to different adversarial attacks. We adapted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image- and video-quality metrics. Some metrics showed high resistance to adversarial attacks, which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts submissions of new metrics for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. The latest results can be found online: https://videoprocessing.ai/benchmarks/metrics-robustness.html.","['CV: Adversarial Attacks & Robustness', 'CV: Computational Photography', 'Image & Video Synthesis', 'PEAI: Safety', 'Robustness & Trustworthiness', 'ML: Adversarial Learning & Robustness']",[],"['Anastasia Antsiferova', 'Khaled Abud', 'Aleksandr Gushchin', 'Ekaterina Shumitskaya', 'Sergey Lavrushkin', 'Dmitriy Vatolin']","['MSU Institute for Artificial Intelligence\nISP RAS Research Center for Trusted Artificial Intelligence', 'Lomonosov Moscow State University', 'MSU Institute for Artificial Intelligence\nISP RAS Research Center for Trusted Artificial Intelligence\nLomonosov Moscow State University', 'ISP RAS Research Center for Trusted Artificial Intelligence\nLomonosov Moscow State University', 'MSU Institute for Artificial Intelligence\nISP RAS Research Center for Trusted Artificial Intelligence', 'MSU Institute for Artificial Intelligence\nISP RAS Research Center for Trusted Artificial Intelligence\nLomonosov Moscow State University']","['Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia']"
https://ojs.aaai.org/index.php/AAAI/article/view/27830,Fairness & Bias,Prompt-Based Distribution Alignment for Unsupervised Domain Adaptation,"Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes.  To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model.  In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment.","['CV: Multi-modal Vision', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Shuanghao Bai', 'Min Zhang', 'Wanqi Zhou', 'Siteng Huang', 'Zhirong Luan', 'Donglin Wang', 'Badong Chen']","[""Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China"", 'Westlake University Institute of Advanced Technology, Westlake Institute for Advanced Study', ""Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China\nRIKEN AIP"", 'Westlake University Institute of Advanced Technology, Westlake Institute for Advanced Study', ""School of Electrical Engineering, Xi’an University of Technology, Xi'an, China"", 'Westlake University Institute of Advanced Technology, Westlake Institute for Advanced Study', ""Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, China""]","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27835,Security,Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content Counterfactually,"Social media platforms are being increasingly used by malicious actors to share unsafe content, such as images depicting sexual activity, cyberbullying, and self-harm. Consequently, major platforms use artificial intelligence (AI) and human moderation to obfuscate such images to make them safer. Two critical needs for obfuscating unsafe images is that an accurate rationale for obfuscating image regions must be provided, and the sensitive regions should be obfuscated (e.g. blurring) for users' safety. This process involves addressing two key problems: (1) the reason for obfuscating unsafe images demands the platform to provide an accurate rationale that must be grounded in unsafe image-specific attributes, and (2) the unsafe regions in the image must be minimally obfuscated while still depicting the safe regions. In this work, we address these key issues by first performing visual reasoning by designing a visual reasoning model (VLM) conditioned on pre-trained unsafe image classifiers to provide an accurate rationale grounded in unsafe image attributes, and then proposing a counterfactual explanation algorithm that minimally identifies and obfuscates unsafe regions for safe viewing, by first utilizing an unsafe image classifier attribution matrix to guide segmentation for a more optimal subregion segmentation followed by an informed greedy search to determine the minimum number of subregions required to modify the classifier's output based on attribution score. Extensive experiments on uncurated data from social networks emphasize the efficacy of our proposed method. We make our code available at: https://github.com/SecureAIAutonomyLab/ConditionalVLM","['CV: Applications', 'APP: Social Networks']",[],"['Mazal Bethany', 'Brandon Wherry', 'Nishant Vishwamitra', 'Peyman  Najafirad']","['University of Texas at San Antonio\nSecure AI and Autonomy Lab', 'University of Texas at San Antonio\nSecure AI and Autonomy Lab', 'University of Texas at San Antonio', 'University of Texas at San Antonio\nSecure AI and Autonomy Lab']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27838,Security,Learning Generalized Segmentation for Foggy-Scenes by Bi-directional Wavelet Guidance,"Learning scene semantics that can be well generalized to foggy conditions is important for safety-crucial applications such as autonomous driving.  Existing methods need both annotated clear images and foggy images to train a curriculum domain adaptation model. Unfortunately, these methods can only generalize to the target foggy domain that has seen in the training stage, but the foggy domains vary a lot in both urban-scene styles and fog styles. In this paper, we propose to learn scene segmentation well generalized to foggy-scenes under the domain generalization setting, which does not involve any foggy images in the training stage and can generalize to any arbitrary unseen foggy scenes.  We argue that an ideal segmentation model that can be well generalized to foggy-scenes need to simultaneously enhance the content, de-correlate the urban-scene style and de-correlate the fog style.  As the content (e.g., scene semantic) rests more in low-frequency features while the style of urban-scene and fog rests more in high-frequency features, we propose a novel bi-directional wavelet guidance (BWG) mechanism to realize the above three objectives in a divide-and-conquer manner.  With the aid of Haar wavelet transformation, the low frequency component is concentrated on the content enhancement self-attention, while the high frequency component is shifted to the style and fog self-attention for de-correlation purpose. It is integrated into existing mask-level Transformer segmentation pipelines in a learnable fashion. Large-scale experiments are conducted on four foggy-scene segmentation datasets under a variety of interesting settings. The proposed method significantly outperforms existing directly-supervised, curriculum domain adaptation and domain generalization segmentation methods.  Source code is available at https://github.com/BiQiWHU/BWG.","['CV: Vision for Robotics & Autonomous Driving', 'CV: Segmentation']",[],"['Qi Bi', 'Shaodi You', 'Theo Gevers']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']","['Netherlands', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/27836,Security,DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal Contrastive Learning,"We present DanceAnyWay, a generative learning method to synthesize beat-guided dances of 3D human characters synchronized with music. Our method learns to disentangle the dance movements at the beat frames from the dance movements at all the remaining frames by operating at two hierarchical levels. At the coarser ""beat"" level, it encodes the rhythm, pitch, and melody information of the input music via dedicated feature representations only at the beat frames. It leverages them to synthesize the beat poses of the target dances using a sequence-to-sequence learning framework. At the finer ""repletion"" level, our method encodes similar rhythm, pitch, and melody information from all the frames of the input music via dedicated feature representations. It generates the full dance sequences by combining the synthesized beat and repletion poses and enforcing plausibility through an adversarial learning framework. Our training paradigm also enforces fine-grained diversity in the synthesized dances through a randomized temporal contrastive loss, which ensures different segments of the dance sequences have different movements and avoids motion freezing or collapsing to repetitive movements. We evaluate the performance of our approach through extensive experiments on the benchmark AIST++ dataset and observe improvements of about 7%-12% in motion quality metrics and 1.5%-4% in motion diversity metrics over the current baselines, respectively. We also conducted a user study to evaluate the visual quality of our synthesized dances. We noted that, on average, the samples generated by our method were about 9-48% more preferred by the participants and had a 4-27% better five-point Likert-scale score over the best available current baseline in terms of motion quality and synchronization. Our source code and project page are available at https://github.com/aneeshbhattacharya/DanceAnyWay.","['CV: Biometrics', 'Face', 'Gesture & Pose', 'APP: Other Applications']",[],"['Aneesh Bhattacharya', 'Manas Paranjape', 'Uttaran Bhattacharya', 'Aniket Bera']","['Purdue University\nIIIT Naya Raipur', 'Purdue Univesity', 'Adobe Research', 'Purdue University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27842,Transparency & Explainability,MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment,"Black-box deep learning approaches have showcased significant potential in the realm of medical image analysis. However, the stringent trustworthiness requirements intrinsic to the medical field have catalyzed research into the utilization of Explainable Artificial Intelligence (XAI), with a particular focus on concept-based methods. Existing concept-based methods predominantly apply concept annotations from a single perspective (e.g., global level), neglecting the nuanced semantic relationships between sub-regions and concepts embedded within medical images. This leads to underutilization of the valuable medical information and may cause models to fall short in harmoniously balancing interpretability and performance when employing inherently interpretable architectures such as Concept Bottlenecks. To mitigate these shortcomings, we propose a multi-modal explainable disease diagnosis framework that meticulously aligns medical images and clinical-related concepts semantically at multiple strata, encompassing the image level, token level, and concept level. Moreover, our method allows for model intervention and offers both textual and visual explanations in terms of human-interpretable concepts. Experimental results on three skin image datasets demonstrate that our method, while preserving model interpretability, attains high performance and label efficiency for concept detection and disease diagnosis. The code is available at https://github.com/Tommy-Bie/MICA.","['CV: Medical and Biological Imaging', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Yequan Bie', 'Luyang Luo', 'Hao Chen']","['Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology\nDepartment of Chemical and Biological Engineering, Hong Kong University of Science and Technology\nHKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute']","['Hong Kong', 'Hong Kong', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/27851,Fairness & Bias,Disguise without Disruption: Utility-Preserving Face De-identification,"With the rise of cameras and smart sensors, humanity generates an exponential amount of data. This valuable information, including underrepresented cases like AI in medical settings, can fuel new deep-learning tools. However, data scientists must prioritize ensuring privacy for individuals in these untapped datasets, especially for images or videos with faces, which are prime targets for identification methods. Proposed solutions to de-identify such images often compromise non-identifying facial attributes relevant to downstream tasks. In this paper, we introduce Disguise, a novel algorithm that seamlessly de-identifies facial images while ensuring the usability of the modified data. Unlike previous approaches, our solution is firmly grounded in the domains of differential privacy and ensemble-learning research. Our method involves extracting and substituting depicted identities with synthetic ones, generated using variational mechanisms to maximize obfuscation and non-invertibility. Additionally, we leverage supervision from a mixture-of-experts to disentangle and preserve other utility attributes. We extensively evaluate our method using multiple datasets, demonstrating a higher de-identification rate and superior consistency compared to prior approaches in various downstream tasks.","['CV: Bias', 'Fairness & Privacy', 'CV: Biometrics', 'Face', 'Gesture & Pose']",[],"['Zikui Cai', 'Zhongpai Gao', 'Benjamin Planche', 'Meng Zheng', 'Terrence Chen', 'M. Salman Asif', 'Ziyan Wu']","['University of California, Riverside, CA', 'United Imaging Intelligence, Burlington, MA', 'United Imaging Intelligence, Burlington, MA', 'Rensselaer Polytechnic Institute, Troy, NY', 'United Imaging Intelligence, Burlington, MA', 'University of California, Riverside, CA', 'United Imaging Intelligence, Burlington, MA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27853,Fairness & Bias,Domain-Controlled Prompt Learning,"Large pre-trained vision-language models, such as CLIP, have shown remarkable generalization capabilities across various tasks when appropriate text prompts are provided. However, adapting these models to specific domains, like remote sensing images (RSIs), medical images, etc, remains unexplored and challenging. Existing prompt learning methods often lack domain-awareness or domain-transfer mechanisms, leading to suboptimal performance due to the misinterpretation of specific images in natural image patterns.  To tackle this dilemma, we proposed a Domain-Controlled Prompt Learning for the specific domains. Specifically, the large-scale specific domain foundation model (LSDM) is first introduced to provide essential specific domain knowledge. Using lightweight neural networks, we transfer this knowledge into domain biases, which control both the visual and language branches to obtain domain-adaptive prompts in a directly incorporating manner.  Simultaneously, to overcome the existing overfitting challenge, we propose a novel noisy-adding strategy, without extra trainable parameters, to help the model escape the suboptimal solution in a global domain oscillation manner. Experimental results show our method achieves state-of-the-art performance in specific domain image recognition datasets. Our code is available at https://github.com/caoql98/DCPL.","['CV: Language and Vision', 'CV: Large Vision Models', 'CV: Multi-modal Vision']",[],"['Qinglong Cao', 'Zhengqin Xu', 'Yuntian Chen', 'Chao Ma', 'Xiaokang Yang']","['MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\nNingbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo', 'MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo', 'MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University', 'MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27854,Security,LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer,"Video recognition systems are vulnerable to adversarial examples. Recent studies show that style transfer-based and patch-based unrestricted perturbations can effectively improve attack efficiency. These attacks, however, face two main challenges: 1) Adding large stylized perturbations to all pixels reduces the naturalness of the video and such perturbations can be easily detected. 2) Patch-based video attacks are not extensible to targeted attacks due to the limited search space of reinforcement learning that has been widely used in video attacks recently. In this paper, we focus on the video black-box setting and propose a novel attack framework named LogoStyleFool by adding a stylized logo to the clean video. We separate the attack into three stages: style reference selection, reinforcement-learning-based logo style transfer, and perturbation optimization. We solve the first challenge by scaling down the perturbation range to a regional logo, while the second challenge is addressed by complementing an optimization stage after reinforcement learning. Experimental results substantiate the overall superiority of LogoStyleFool over three state-of-the-art patch-based attacks in terms of attack performance and semantic preservation. Meanwhile, LogoStyleFool still maintains its performance against two existing patch-based defense methods. We believe that our research is beneficial in increasing the attention of the security community to such subregional style transfer attacks.",['CV: Adversarial Attacks & Robustness'],[],"['Yuxin Cao', 'Ziyu Zhao', 'Xi Xiao', 'Derui Wang', 'Minhui Xue', 'Jin Lu']","['Shenzhen International Graduate School, Tsinghua University, China', 'Fan Gongxiu Honors College, Beijing University of Technology, China', 'Shenzhen International Graduate School, Tsinghua University, China', ""CSIRO's Data61, Australia"", ""CSIRO's Data61, Australia"", 'Ping An Technology (Shenzhen) Co., Ltd., China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27857,Fairness & Bias,CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection,"Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution. To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain. Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions. In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance.","['CV: 3D Computer Vision', 'CV: Vision for Robotics & Autonomous Driving', 'CV: Object Detection & Categorization', 'CV: Multi-modal Vision', 'ML: Adversarial Learning & Robustness', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Gyusam Chang', 'Wonseok Roh', 'Sujin Jang', 'Dongwook Lee', 'Daehyun Ji', 'Gyeongrok Oh', 'Jinsun Park', 'Jinkyu Kim', 'Sangpil Kim']","['Department of Artificial Intelligence, Korea University', 'Department of Artificial Intelligence, Korea University', 'Samsung Advanced Institute of Technology (SAIT)', 'Samsung Advanced Institute of Technology (SAIT)', 'Samsung Advanced Institute of Technology (SAIT)', 'Department of Artificial Intelligence, Korea University', 'School of Computer Science and Engineering, Pusan National University', 'Department of Computer Science and Engineering, Korea University', 'Department of Artificial Intelligence, Korea University']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/27864,Fairness & Bias,DDAE: Towards Deep Dynamic Vision BERT Pretraining,"Recently, masked image modeling (MIM) has demonstrated promising prospects in self-supervised representation learning. However, existing MIM frameworks recover all masked patches equivalently, ignoring that the reconstruction difficulty of different patches can vary sharply due to their diverse distance from visible patches. In this paper, we propose a novel deep dynamic supervision to enable MIM methods to dynamically reconstruct patches with different degrees of difficulty at different pretraining phases and depths of the model. Our deep dynamic supervision helps to provide more locality inductive bias for ViTs especially in deep layers, which inherently makes up for the absence of local prior for self-attention mechanism. Built upon the deep dynamic supervision, we propose Deep Dynamic AutoEncoder (DDAE), a simple yet effective MIM framework that utilizes dynamic mechanisms for pixel regression and feature self-distillation simultaneously. Extensive experiments across a variety of vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on COCO demonstrate the effectiveness of our approach.","['CV: Representation Learning for Vision', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Honghao Chen', 'Xiangwen Kong', 'Xiangyu Zhang', 'Xin Zhao', 'Kaiqi Huang']","['CRISE, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences', 'MEGVII Technology', 'MEGVII Technology', 'CRISE, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences', 'CRISE, Institute of Automation, Chinese Academy of Sciences\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences\nCAS Center for Excellence in Brain Science and Intelligence Technology']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27869,Transparency & Explainability,Null Space Matters: Range-Null Decomposition for Consistent Multi-Contrast MRI Reconstruction,"Consistency and interpretability have long been the critical issues in MRI reconstruction. While interpretability has been dramatically improved with the employment of deep unfolding networks (DUNs), current methods still suffer from inconsistencies and generate inferior anatomical structure. Especially in multi-contrast scenes, different imaging protocols often exacerbate the concerned issue. In this paper, we propose a range-null decomposition-assisted DUN architecture to ensure consistency while still providing desirable interpretability. Given the input decomposed, we argue that the inconsistency could be analytically relieved by feeding solely the null-space component into proximal mapping, while leaving the range-space counterpart fixed. More importantly, a correlation decoupling scheme is further proposed to narrow the information gap for multi-contrast fusion, which dynamically borrows isotropic features from the opponent while maintaining the modality-specific ones. Specifically, the two features are attached to different frequencies and learned individually by the newly designed isotropy encoder and anisotropy encoder. The former strives for the contrast-shared information, while the latter serves to capture the contrast-specific features. The quantitative and qualitative results show that our proposal outperforms most cutting-edge methods by a large margin. Codes will be released on https://github.com/chenjiachengzzz/RNU.","['CV: Medical and Biological Imaging', 'CV: Low Level & Physics-based Vision']",[],"['Jiacheng Chen', 'Jiawei Jiang', 'Fei Wu', 'Jianwei Zheng']","['Zhejiang University of Technology', 'Zhejiang University of Technology', 'Zhejiang University of Technology', 'Zhejiang University of Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27882,Security,TCI-Former: Thermal Conduction-Inspired Transformer for Infrared Small Target Detection,"Infrared small target detection (ISTD) is critical to national security and has been extensively applied in military areas. ISTD aims to segment small target pixels from background. Most ISTD networks focus on designing feature extraction blocks or feature fusion modules, but rarely describe the ISTD process from the feature map evolution perspective. In the ISTD process, the network attention gradually shifts towards target areas. We abstract this process as the directional movement of feature map pixels to target areas through convolution, pooling and interactions with surrounding pixels, which can be analogous to the movement of thermal particles constrained by surrounding variables and particles. In light of this analogy, we propose Thermal Conduction-Inspired Transformer (TCI-Former) based on the theoretical principles of thermal conduction. According to thermal conduction differential equation in heat dynamics, we derive the pixel movement differential equation (PMDE) in the image domain and further develop two modules: Thermal Conduction-Inspired Attention (TCIA) and Thermal Conduction Boundary Module (TCBM). TCIA incorporates finite difference method with PMDE to reach a numerical approximation so that target body features can be extracted. To further remove errors in boundary areas, TCBM is designed and supervised by boundary masks to refine target body features with fine boundary details. Experiments on IRSTD-1k and NUAA-SIRST demonstrate the superiority of our method.","['CV: Segmentation', 'CV: Object Detection & Categorization']",[],"['Tianxiang Chen', 'Zhentao Tan', 'Qi Chu', 'Yue Wu', 'Bin Liu', 'Nenghai Yu']","['School of Cyber Science and Technology, University of Science and Technology of China\nAlibaba Group\nKey Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences', 'School of Cyber Science and Technology, University of Science and Technology of China\nAlibaba Group\nKey Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences', 'School of Cyber Science and Technology, University of Science and Technology of China\nKey Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences', 'Alibaba Group', 'School of Cyber Science and Technology, University of Science and Technology of China\nKey Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences', 'School of Cyber Science and Technology, University of Science and Technology of China\nKey Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27888,Transparency & Explainability,Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning,"Knowledge-based visual reasoning remains a daunting task since it not only requires machines to interpret the concepts and relationships from visual scenes but also associate them with external world knowledge to conduct a chain of reasoning on open-world questions. Previous works, however, treat visual perception and language-based reasoning as two independent modules, failing to attend to both modules throughout all stages of reasoning. To this end, we propose Visual Chain-of-thought Prompting (VCTP) for knowledge-based reasoning, which involves the interaction between visual content and natural language in an iterative step-by-step reasoning manner. VCTP contains three stages, see, think, and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to key visual concepts from natural language questions adaptively. It then transforms key visual context into text context for prompting with a visual captioning model, and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, which is then passed through a cross-modality classifier to verify that it’s consistent with the visual context. We iterate through the think-confirm stages to ensure the verified rationale is consistent with the answer. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our VCTP enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines. Our code is available at https://github.com/UMass-Foundation-Model/VisualCoT.git","['CV: Language and Vision', 'CV: Visual Reasoning & Symbolic Representations']",[],"['Zhenfang Chen', 'Qinhong Zhou', 'Yikang Shen', 'Yining Hong', 'Zhiqing Sun', 'Dan Gutfreund', 'Chuang Gan']","['MIT-IBM Watson AI Lab', 'UMass Amherst', 'MIT-IBM Watson AI Lab', 'University of California, Los Angeles', 'Carnegie Mellon University', 'MIT-IBM Watson AI Lab', 'MIT-IBM Watson AI Lab\nUMass Amherst']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27900,Security,Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation,"Existing generative adversarial network (GAN) based conditional image generative models typically produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or style transfer. On the other hand, GAN-based diverse image generative methods require retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results. Given that many deterministic conditional image generative models have been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image generative models to generate diverse results without changing network structures or parameters? To answer this question, we re-examine the conditional image generation tasks from the perspective of adversarial attack and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation. The key idea is attacking the pre-trained deterministic generative models by adding a micro perturbation to the input condition. In this way, diverse results can be generated without any adjustment of network structures or fine-tuning of the pre-trained models. In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image. Our work opens the door to applying adversarial attack to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method.","['CV: Applications', 'CV: Language and Vision']",[],"['Tianyi Chu', 'Wei Xing', 'Jiafu Chen', 'Zhizhong Wang', 'Jiakai Sun', 'Lei Zhao', 'Haibo Chen', 'Huaizhong Lin']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Nanjing University of Science and Technology', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27902,Fairness & Bias,Decoupled Optimisation for Long-Tailed Visual Recognition,"When training on a long-tailed dataset, conventional learning algorithms tend to exhibit a bias towards classes with a larger sample size. Our investigation has revealed that this biased learning tendency originates from the model parameters, which are trained to disproportionately contribute to the classes characterised by their sample size (e.g., many, medium, and few classes). To balance the overall parameter contribution across all classes, we investigate the importance of each model parameter to the learning of different class groups, and propose a multistage parameter Decouple and Optimisation (DO) framework that decouples parameters into different groups with each group learning a specific portion of classes. To optimise the parameter learning, we apply different training objectives with a collaborative optimisation step to learn complementary information about each class group. Extensive experiments on long-tailed datasets, including CIFAR100, Places-LT, ImageNet-LT, and iNaturaList 2018, show that our framework achieves competitive performance compared to the state-of-the-art.","['CV: Object Detection & Categorization', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['Cong Cong', 'Shiyu Xuan', 'Sidong Liu', 'Shiliang Zhang', 'Maurice Pagnucco', 'Yang Song']","['School of Computer Science and Engineering, University of New South Wales, Sydney, Australia', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, China', 'Australian Institute of Health Innovation, Macquarie University, Sydney, Australia', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, China', 'School of Computer Science and Engineering, University of New South Wales, Sydney, Australia', 'School of Computer Science and Engineering, University of New South Wales, Sydney, Australia']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/27911,Fairness & Bias,Noisy Correspondence Learning with Self-Reinforcing Errors Mitigation,"Cross-modal retrieval relies on well-matched large-scale datasets that are laborious in practice. Recently, to alleviate expensive data collection, co-occurring pairs from the Internet are automatically harvested for training. However, it inevitably includes mismatched pairs, i.e., noisy correspondences, undermining supervision reliability and degrading performance. Current methods leverage deep neural networks' memorization effect to address noisy correspondences, which overconfidently focus on similarity-guided training with hard negatives and suffer from self-reinforcing errors. In light of above, we introduce a novel noisy correspondence learning framework, namely Self-Reinforcing Errors Mitigation (SREM). Specifically, by viewing sample matching as classification tasks within the batch, we generate classification logits for the given sample. Instead of a single similarity score, we refine sample filtration through energy uncertainty and estimate model's sensitivity of selected clean samples using swapped classification entropy, in view of the overall prediction distribution. Additionally, we propose cross-modal biased complementary learning to leverage negative matches overlooked in hard-negative training, further improving model optimization stability and curbing self-reinforcing errors. Extensive experiments on challenging benchmarks affirm the efficacy and efficiency of SREM.","['CV: Language and Vision', 'CV: Applications', 'CV: Image and Video Retrieval']",[],"['Zhuohang Dang', 'Minnan Luo', 'Chengyou Jia', 'Guang Dai', 'Xiaojun Chang', 'Jingdong Wang']","[""School of Computer Science and Technology, MOEKLINNS Laboratory, Xi'an Jiaotong University"", ""School of Computer Science and Technology, MOEKLINNS Laboratory, Xi'an Jiaotong University"", ""School of Computer Science and Technology, MOEKLINNS Laboratory, Xi'an Jiaotong University"", 'SGIT AI Lab\nState Grid Corporation of China', 'University of Technology Sydney\nMohamed bin Zayed University of Artificial Intelligence', 'Baidu']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27914,Fairness & Bias,A Dynamic GCN with Cross-Representation Distillation for Event-Based Learning,"Recent advances in event-based research prioritize sparsity and temporal precision. Approaches learning sparse point-based representations through graph CNNs (GCN) become more popular. Yet, these graph techniques hold lower performance than their frame-based counterpart due to two issues: (i) Biased graph structures that don't properly incorporate varied attributes (such as semantics, and spatial and temporal signals) for each vertex, resulting in inaccurate graph representations. (ii) A shortage of robust pretrained models. Here we solve the first problem by proposing a new event-based GCN (EDGCN), with a dynamic aggregation module to integrate all attributes of vertices adaptively. To address the second problem, we introduce a novel learning framework called cross-representation distillation (CRD), which leverages the dense representation of events as a cross-representation auxiliary to provide additional supervision and prior knowledge for the event graph. This frame-to-graph distillation allows us to benefit from the large-scale priors provided by CNNs while still retaining the advantages of graph-based models. Extensive experiments show our model and learning framework are effective and generalize well across multiple vision tasks.","['CV: Vision for Robotics & Autonomous Driving', 'CV: Video Understanding & Activity Analysis', 'CV: Representation Learning for Vision', 'CV: 3D Computer Vision']",[],"['Yongjian Deng', 'Hao Chen', 'Youfu Li']","['Beijing University of Technology\nEngineering Research Center of Intelligence Perception and Autonomous Control', 'Southeast University', 'City University of Hong Kong']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27918,Fairness & Bias,Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning,"We propose a generalized method for boosting the generalization ability of pre-trained vision-language models (VLMs) while fine-tuning on downstream few-shot tasks. The idea is realized by exploiting out-of-distribution (OOD) detection to predict whether a sample belongs to a base distribution or a novel distribution and then using the score generated by a dedicated competition based scoring function to fuse the zero-shot and few-shot classifier. The fused classifier is dynamic, which will bias towards the zero-shot classifier if a sample is more likely from the distribution pre-trained on, leading to improved base-to-novel generalization ability. Our method is performed only in test stage, which is applicable to boost existing methods without time-consuming re-training. Extensive experiments show that even weak distribution detectors can still improve VLMs' generalization ability. Specifically, with the help of OOD detectors, the harmonic mean of CoOp and ProGrad increase by 2.6 and 1.5 percentage points over 11 recognition datasets in the base-to-novel setting.","['CV: Language and Vision', 'CV: Multi-modal Vision', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Kun Ding', 'Haojian Zhang', 'Qiang Yu', 'Ying Wang', 'Shiming Xiang', 'Chunhong Pan']","['State Key Laboratory of Multimodal Artificial Intelligence Systems\nInstitute of Automation, Chinese Academy of Sciences', 'Engineering Laboratory for Intelligent Industrial Vision\nInstitute of Automation, Chinese Academy of Sciences', 'Research Center of Aerospace Information\nInstitute of Automation, Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems\nInstitute of Automation, Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems\nInstitute of Automation, Chinese Academy of Sciences', 'Research Center of Aerospace Information\nInstitute of Automation, Chinese Academy of Sciences']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27920,Security,Transferable Adversarial Attacks for Object Detection Using Object-Aware Significant Feature Distortion,"Transferable black-box adversarial attacks against classifiers by disturbing the intermediate-layer features have been extensively studied in recent years. However, these methods have not yet achieved satisfactory performances when directly applied to object detectors. This is largely because the features of detectors are fundamentally different from that of the classifiers. In this study, we propose a simple but effective method to improve the transferability of adversarial examples for object detectors by leveraging the properties of spatial consistency and limited equivariance of object detectors’ features. Specifically, we combine a novel loss function and deliberately designed data augmentation to distort the backbone features of object detectors by suppressing significant features corresponding to objects and amplifying the surrounding vicinal features corresponding to object boundaries. As such the target object and background area on the generated adversarial samples are more likely to be confused by other detectors. Extensive experimental results show that our proposed method achieves state-of-the-art black-box transferability for untargeted attacks on various models, including one/two-stage, CNN/Transformer-based, and anchor-free/anchor-based detectors.","['CV: Adversarial Attacks & Robustness', 'CV: Object Detection & Categorization']",[],"['Xinlong Ding', 'Jiansheng Chen', 'Hongwei Yu', 'Yu Shang', 'Yining Qin', 'Huimin Ma']","['University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'Tsinghua University', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27922,Security,Exploiting Polarized Material Cues for Robust Car Detection,"Car detection is an important task that serves as a crucial prerequisite for many automated driving functions. The large variations in lighting/weather conditions and vehicle densities of the scenes pose significant challenges to existing car detection algorithms to meet the highly accurate perception demand for safety, due to the unstable/limited color information, which impedes the extraction of meaningful/discriminative features of cars. In this work, we present a novel learning-based car detection method that leverages trichromatic linear polarization as an additional cue to disambiguate such challenging cases. A key observation is that polarization, characteristic of the light wave, can robustly describe intrinsic physical properties of the scene objects in various imaging conditions and is strongly linked to the nature of materials for cars (e.g., metal and glass) and their surrounding environment (e.g., soil and trees), thereby providing reliable and discriminative features for robust car detection in challenging scenes. To exploit polarization cues, we first construct a pixel-aligned RGB-Polarization car detection dataset, which we subsequently employ to train a novel multimodal fusion network. Our car detection network dynamically integrates RGB and polarization features in a request-and-complement manner and can explore the intrinsic material properties of cars across all learning samples. We extensively validate our method and demonstrate that it outperforms state-of-the-art detection methods. Experimental results show that polarization is a powerful cue for car detection. Our code is available at https://github.com/wind1117/AAAI24-PCDNet.","['CV: Multi-modal Vision', 'CV: Object Detection & Categorization']",[],"['Wen Dong', 'Haiyang Mei', 'Ziqi Wei', 'Ao Jin', 'Sen Qiu', 'Qiang Zhang', 'Xin Yang']","['Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology', 'Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology\nShow Lab, National University of Singapore', 'Institute of Automation, Chinese Academy of Sciences\nState Key Laboratory of Structural Analysis for Industrial Equipment, Dalian University of Technology', 'Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology', 'Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology', 'Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology', 'Key Laboratory of Social Computing and Cognitive Intelligence, Dalian University of Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27941,Fairness & Bias,"Fewer Steps, Better Performance: Efficient Cross-Modal Clip Trimming for Video Moment Retrieval Using Language","Given an untrimmed video and a sentence query, video moment retrieval using language (VMR) aims to locate a target query-relevant moment. Since the untrimmed video is overlong, almost all existing VMR methods first sparsely down-sample each untrimmed video into multiple fixed-length video clips and then conduct multi-modal interactions with the query feature and expensive clip features for reasoning, which is infeasible for long real-world videos that span hours. Since the video is downsampled into  fixed-length clips,  some query-related frames may be filtered out, which will blur the specific boundary of the target moment, take the adjacent irrelevant frames as new boundaries, easily leading to cross-modal misalignment and introducing both boundary-bias and reasoning-bias. To this end, in this paper, we propose an efficient approach, SpotVMR, to trim the query-relevant clip. Besides, our proposed SpotVMR can serve as plug-and-play module, which achieves efficiency for state-of-the-art VMR methods while maintaining good retrieval performance. Especially, we first design a novel clip search model that learns to identify promising video regions to search conditioned on the language query. Then, we introduce a  set of low-cost semantic indexing features to capture the context of objects and interactions that suggest where to search the query-relevant moment. Also, the distillation loss is utilized to  address the optimization issues arising from end-to-end joint training of the clip selector and VMR model. Extensive experiments on three challenging datasets demonstrate its effectiveness.","['CV: Language and Vision', 'NLP: Language Grounding & Multi-modal NLP']",[],"['Xiang Fang', 'Daizong Liu', 'Wanlong Fang', 'Pan Zhou', 'Zichuan Xu', 'Wenzheng Xu', 'Junyang Chen', 'Renfu Li']","['Huazhong University of Science and Technology', 'Peking University', 'Henan University\nHuazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Dalian University of Technology', 'Sichuan University', 'Shenzhen Univeristy', 'Huazhong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27944,Transparency & Explainability,Interpretable3D: An Ad-Hoc Interpretable Classifier for 3D Point Clouds,"3D decision-critical tasks urgently require research on explanations to ensure system reliability and transparency. Extensive explanatory research has been conducted on 2D images, but there is a lack in the 3D field. Furthermore, the existing explanations for 3D models are post-hoc and can be misleading, as they separate explanations from the original model. To address these issues, we propose an ad-hoc interpretable classifier for 3D point clouds (i.e., Interpretable3D). As an intuitive case-based classifier, Interpretable3D can provide reliable ad-hoc explanations without any embarrassing nuances. It allows users to understand how queries are embedded within past observations in prototype sets. Interpretable3D has two iterative training steps: 1) updating one prototype with the mean of the embeddings within the same sub-class in Prototype Estimation, and 2) penalizing or rewarding the estimated prototypes in Prototype Optimization. The mean of embeddings has a clear statistical meaning, i.e., class sub-centers. Moreover, we update prototypes with their most similar observations in the last few epochs. Finally, Interpretable3D classifies new samples according to prototypes. We evaluate the performance of Interpretable3D on four popular point cloud models: DGCNN, PointNet2, PointMLP, and PointNeXt. Our Interpretable3D demonstrates comparable or superior performance compared to softmax-based black-box models in the tasks of 3D shape classification and part segmentation. Our code is released at: github.com/FengZicai/Interpretable3D.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: 3D Computer Vision']",[],"['Tuo Feng', 'Ruijie Quan', 'Xiaohan Wang', 'Wenguan Wang', 'Yi Yang']","['University of Technology Sydney', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27943,Fairness & Bias,Debiased Novel Category Discovering and Localization,"In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.","['CV: Applications', 'CV: Object Detection & Categorization', 'CV: Representation Learning for Vision']",[],"['Juexiao Feng', 'Yuhong Yang', 'Yanchun Xie', 'Yaqian Li', 'Yandong Guo', 'Yuchen Guo', 'Yuwei He', 'Liuyu Xiang', 'Guiguang Ding']","['Tsinghua University\nBNRist\nHangzhou Zhuoxi Institute of Brain and Intelligence', 'Tsinghua University\nBNRist\nHangzhou Zhuoxi Institute of Brain and Intelligence', 'OPPO Research Institute', 'OPPO Research Institute', 'OPPO Research Institute', 'Tsinghua University\nBNRist', 'Tsinghua University\nBNRist', 'Beijing University of Posts and Telecommunications', 'Tsinghua University\nBNRist']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27947,Security,Attacking Transformers with Feature Diversity Adversarial Perturbation,"Understanding the mechanisms behind Vision Transformer (ViT), particularly its vulnerability to adversarial perturbations, is crucial for addressing challenges in its real-world applications. Existing ViT adversarial attackers rely on labels to calculate the gradient for perturbation, and exhibit low transferability to other structures and tasks. In this paper, we present a label-free white-box attack approach for ViT-based models that exhibits strong transferability to various black-box models, including most ViT variants, CNNs, and MLPs, even for models developed for other modalities. Our inspiration comes from the feature collapse phenomenon in ViTs, where the critical attention mechanism overly depends on the low-frequency component of features, causing the features in middle-to-end layers to become increasingly similar and eventually collapse. We propose the feature diversity attacker to naturally accelerate this process and achieve remarkable performance and transferability.","['CV: Adversarial Attacks & Robustness', 'CV: Object Detection & Categorization']",[],"['Chenxing Gao', 'Hang Zhou', 'Junqing Yu', 'YuTeng Ye', 'Jiale Cai', 'Junle Wang', 'Wei Yang']","['Huazhong University of Science and Technology', 'Huazhong University of Science & Technology', 'Huazhong University of Science & Technology', 'Huazhong University of Science & Technology', 'Huazhong University of Science and Technology', 'Tencent', 'Huazhong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27954,Security,A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives,"Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs containing well-designed triggers, while behaving normally on clean inputs. Prior researches have explored the invisibility of backdoor triggers to enhance attack stealthiness. However, most of them only focus on the invisibility in the spatial domain, neglecting the generation of invisible triggers in the frequency domain. This limitation renders the generated poisoned images easily detectable by recent defense methods. To address this issue, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Wavelet Transform to embed the high-frequency information of the trigger image into the clean image to ensure attack effectiveness. Then, to attain strong stealthiness, we incorporate Fourier Transform and Cosine Transform to mix the poisoned image and clean image in the frequency domain. Moreover, DUBA adopts a novel attack strategy, training the model with weak triggers and attacking with strong triggers to further enhance attack performance and stealthiness. DUBA is evaluated extensively on four datasets against popular image classifiers, showing significant superiority over state-of-the-art backdoor attacks in attack success rate and stealthiness.","['CV: Adversarial Attacks & Robustness', 'CV: Object Detection & Categorization']",[],"['Yudong Gao', 'Honglong Chen', 'Peng Sun', 'Junjian Li', 'Anqing Zhang', 'Zhibo Wang', 'Weifeng Liu']","['College of Control Science and Engineering, China University of Petroleum (East China), P.R. China', 'College of Control Science and Engineering, China University of Petroleum (East China), P.R. China', 'College of Computer Science and Electronic Engineering, Hunan University, P.R. China', 'College of Control Science and Engineering, China University of Petroleum (East China), P.R. China', 'College of Control Science and Engineering, China University of Petroleum (East China), P.R. China', 'School of Cyber Science and Technology, Zhejiang University, P.R. China', 'College of Control Science and Engineering, China University of Petroleum (East China), P.R. China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27958,Fairness & Bias,Beyond Prototypes: Semantic Anchor Regularization for Better Representation Learning,"One of the ultimate goals of representation learning is to achieve compactness within a class and well-separability between classes. Many outstanding metric-based and prototype-based methods following the Expectation-Maximization paradigm, have been proposed for this objective. However, they inevitably introduce biases into the learning process, particularly with long-tail distributed training data. In this paper, we reveal that the class prototype is not necessarily to be derived from training features and propose a novel perspective to use pre-defined class anchors serving as feature centroid to unidirectionally guide feature learning. However, the pre-defined anchors may have a large semantic distance from the pixel features, which prevents them from being directly applied. To address this issue and generate feature centroid independent from feature learning, a simple yet effective Semantic Anchor Regularization (SAR) is proposed. SAR ensures the inter-class separability of semantic anchors in the semantic space by employing a classifier-aware auxiliary cross-entropy loss during training via disentanglement learning. By pulling the learned features to these semantic anchors, several advantages can be attained: 1) the intra-class compactness and naturally inter-class separability, 2) induced bias or errors from feature learning can be avoided, and 3) robustness to the long-tailed problem. The proposed SAR can be used in a plug-and-play manner in the existing models. Extensive experiments demonstrate that the SAR performs better than previous sophisticated prototype-based methods.  The implementation is available at https://github.com/geyanqi/SAR.","['CV: Segmentation', 'CV: Representation Learning for Vision']",[],"['Yanqi Ge', 'Qiang Nie', 'Ye Huang', 'Yong Liu', 'Chengjie Wang', 'Feng Zheng', 'Wen Li', 'Lixin Duan']","['Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China', 'Tencent Youtu Lab', 'Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China', 'Tencent Youtu Lab', 'Tencent Youtu Lab\nShanghai Jiao Tong University', 'Southern University of Science and Technology', 'Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China', 'Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China\nSichuan Provincial People’s Hospital']","['China', '', 'China', '', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27961,Security,SDAC: A Multimodal Synthetic Dataset for Anomaly and Corner Case Detection in Autonomous Driving,"Nowadays, closed-set perception methods for autonomous driving perform well on datasets containing normal scenes. However, they still struggle to handle anomalies in the real world, such as unknown objects that have never been seen while training. The lack of public datasets to evaluate the model performance on anomaly and corner cases has hindered the development of reliable autonomous driving systems. Therefore, we propose a multimodal Synthetic Dataset for Anomaly and Corner case detection, called SDAC, which encompasses anomalies captured from multi-view cameras and the LiDAR sensor, providing a rich set of annotations for multiple mainstream perception tasks. SDAC is the first public dataset for autonomous driving that categorizes anomalies into object, scene, and scenario levels, allowing the evaluation under different anomalous conditions. Experiments show that closed-set models suffer significant performance drops on anomaly subsets in SDAC. Existing anomaly detection methods fail to achieve satisfactory performance, suggesting that anomaly detection remains a challenging problem. We anticipate that our SDAC dataset could foster the development of safe and reliable systems for autonomous driving.","['CV: Vision for Robotics & Autonomous Driving', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Lei Gong', 'Yu Zhang', 'Yingqing Xia', 'Yanyong Zhang', 'Jianmin Ji']","['School of Computer Science and Technology, University of Science and Technology of China, Hefei, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, China\nInstitute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, China\nInstitute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, China\nInstitute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27965,Transparency & Explainability,Knowledge-Aware Neuron Interpretation for Scene Classification,"Although neural models have achieved remarkable performance, they still encounter doubts due to the intransparency. To this end, model prediction explanation is attracting more and more attentions. However, current methods rarely incorporate external knowledge and still suffer from three limitations: (1) Neglecting concept completeness. Merely selecting concepts may not sufficient for prediction. (2) Lacking concept fusion. Failure to merge semantically-equivalent concepts. (3) Difficult in manipulating model behavior. Lack of verification for explanation on original model. To address these issues, we propose a novel knowledge-aware neuron interpretation framework to explain model predictions for image scene classification. Specifically, for concept completeness, we present core concepts of a scene based on knowledge graph, ConceptNet, to gauge the completeness of concepts. Our method, incorporating complete concepts, effectively provides better prediction explanations compared to baselines. Furthermore, for concept fusion, we introduce a knowledge graph-based method known as Concept Filtering, which produces over 23% point gain on neuron behaviors for neuron interpretation. At last, we propose Model Manipulation, which aims to study whether the core concepts based on ConceptNet could be employed to manipulate model behavior. The results show that core concepts can effectively improve the performance of original model by over 26%.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Multi-modal Vision', 'CV: Scene Analysis & Understanding', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: Language Grounding & Multi-modal NLP']",[],"['Yong Guan', 'Freddy Lécué', 'Jiaoyan Chen', 'Ru Li', 'Jeff Z. Pan']","['Tsinghua University\nShanxi University', 'Inria', 'The University of Manchester', 'Shanxi University', 'The University of Edinburgh']","['China', '', 'United Kingdom', 'China', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/27969,Fairness & Bias,Improving Panoptic Narrative Grounding by Harnessing Semantic Relationships and Visual Confirmation,"Recent advancements in single-stage Panoptic Narrative Grounding (PNG) have demonstrated significant potential. These methods predict pixel-level masks by directly matching pixels and phrases. However, they often neglect the modeling of semantic and visual relationships between phrase-level instances, limiting their ability for complex multi-modal reasoning in PNG. To tackle this issue, we propose XPNG, a “differentiation-refinement-localization” reasoning paradigm for accurately locating instances or regions. In XPNG, we introduce a Semantic Context Convolution (SCC) module to leverage semantic priors for generating distinctive features. This well-crafted module employs a combination of dynamic channel-wise convolution and pixel-wise convolution to embed semantic information and establish inter-object relationships guided by semantics. Subsequently, we propose a Visual Context Verification (VCV) module to provide visual cues, eliminating potential space biases introduced by semantics and further refining the visual features generated by the previous module. Extensive experiments on PNG benchmark datasets reveal that our approach achieves state-of-the-art performance, significantly outperforming existing methods by a considerable margin and yielding a 3.9-point improvement in overall metrics. Our codes and results are available at our project webpage: https://github.com/TianyuGoGO/XPNG.","['CV: Multi-modal Vision', 'CV: Language and Vision']",[],"['Tianyu Guo', 'Haowei Wang', 'Yiwei Ma', 'Jiayi Ji', 'Xiaoshuai Sun']","['Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China', 'Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27971,Transparency & Explainability,PICNN: A Pathway towards Interpretable Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) have exhibited great performance in discriminative feature learning for complex visual tasks. Besides discrimination power, interpretability is another important yet under-explored property for CNNs. One difficulty in the CNN interpretability is that filters and image classes are entangled. In this paper, we introduce a novel pathway to alleviate the entanglement between filters and image classes. The proposed pathway groups the filters in a late conv-layer of CNN into class-specific clusters. Clusters and classes are in a one-to-one relationship. Specifically, we use the Bernoulli sampling to generate the filter-cluster assignment matrix from a learnable filter-class correspondence matrix. To enable end-to-end optimization, we develop a novel reparameterization trick for handling the non-differentiable Bernoulli sampling. We evaluate the effectiveness of our method on ten widely used network architectures (including nine CNNs and a ViT) and five benchmark datasets. Experimental results have demonstrated that our method PICNN (the combination of standard CNNs with our proposed pathway) exhibits greater interpretability than standard CNNs while achieving higher or comparable discrimination power.","['CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Wengang Guo', 'Jiayi Yang', 'Huilin Yin', 'Qijun Chen', 'Wei Ye']","['Tongji University', 'Tongji University', 'Tongji University', 'Tongji University', 'Tongji University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27977,Fairness & Bias,Exploiting the Social-Like Prior in Transformer for Visual Reasoning,"Benefiting from instrumental global dependency modeling of self-attention (SA), transformer-based approaches have become the pivotal choices for numerous downstream visual reasoning tasks, such as visual question answering (VQA) and referring expression comprehension (REC). However, some studies have recently suggested that SA tends to suffer from rank collapse thereby inevitably leads to representation degradation as the transformer layer goes deeper. Inspired by social network theory, we attempt to make an analogy between social behavior and regional information interaction in SA, and harness two crucial notions of structural hole and degree centrality in social network to explore the possible optimization towards SA learning, which naturally deduces two plug-and-play social-like modules. Based on structural hole, the former module allows to make information interaction in SA more structured, which effectively avoids redundant information aggregation and global feature homogenization for better rank remedy, followed by latter module to comprehensively characterize and refine the representation discrimination via considering degree centrality of regions and transitivity of relations. Without bells and whistles, our model outperforms a bunch of baselines by a noticeable margin when considering our social-like prior on five benchmarks in VQA and REC tasks, and a series of explanatory results are showcased to sufficiently reveal the social-like behaviors in SA.","['CV: Language and Vision', 'CV: Visual Reasoning & Symbolic Representations']",[],"['Yudong Han', 'Yupeng Hu', 'Xuemeng Song', 'Haoyu Tang', 'Mingzhu Xu', 'Liqiang Nie']","['School of Software, Shandong University', 'School of Software, Shandong University', 'School of Computer Science and Technology, Shandong University', 'School of Software, Shandong University', 'School of Software, Shandong University', 'School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen)']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/27987,Transparency & Explainability,Optimize & Reduce: A Top-Down Approach for Image Vectorization,"Vector image representation is a popular choice when editability and flexibility in resolution are desired. However, most images are only available in raster form, making raster-to-vector image conversion (vectorization) an important task. Classical methods for vectorization are either domain-specific or yield an abundance of shapes which limits editability and interpretability. Learning-based methods, that use differentiable rendering, have revolutionized vectorization, at the cost of poor generalization to out-of-training distribution domains, and optimization-based counterparts are either slow or produce non-editable and redundant shapes. In this work, we propose Optimize & Reduce (O&R), a top-down approach to vectorization that is both fast and domain-agnostic. O&R aims to attain a compact representation of input images by iteratively optimizing Bezier curve parameters and significantly reducing the number of shapes, using a devised importance measure. We contribute a benchmark of five datasets comprising images from a broad spectrum of image complexities - from emojis to natural-like images. Through extensive experiments on hundreds of images, we demonstrate that our method is domain agnostic and outperforms existing works in both reconstruction and perceptual quality for a fixed number of shapes. Moreover, we show that our algorithm is x10 faster than the state-of-the-art optimization-based method. Our code is publicly available: https://github.com/ajevnisek/optimize-and-reduce","['CV: Applications', 'CV: Learning & Optimization for CV', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Or Hirschorn', 'Amir Jevnisek', 'Shai Avidan']","['Tel Aviv University', 'Tel Aviv University', 'Tel Aviv University']","['Israel', 'Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/28004,Fairness & Bias,Dynamic Weighted Combiner for Mixed-Modal Image Retrieval,"Mixed-Modal Image Retrieval (MMIR) as a flexible search paradigm has attracted wide attention. However, previous approaches always achieve limited performance, due to two critical factors are seriously overlooked. 1) The contribution of image and text modalities is different, but incorrectly treated equally. 2) There exist inherent labeling noises in describing users' intentions with text in web datasets from diverse real-world scenarios, giving rise to overfitting. We propose a Dynamic Weighted Combiner (DWC) to tackle the above challenges, which includes three merits. First, we propose an Editable Modality De-equalizer (EMD) by taking into account the contribution disparity between modalities, containing two modality feature editors and an adaptive weighted combiner. Second, to alleviate labeling noises and data bias, we propose a dynamic soft-similarity label generator (SSG) to implicitly improve noisy supervision. Finally, to bridge modality gaps and facilitate similarity learning, we propose a CLIP-based mutual enhancement module alternately trained by a mixed-modality contrastive loss. Extensive experiments verify that our proposed model significantly outperforms state-of-the-art methods on real-world datasets. The source code is available at https://github.com/fuxianghuang1/DWC.","['CV: Image and Video Retrieval', 'ML: Multimodal Learning', 'ML: Representation Learning']",[],"['Fuxiang Huang', 'Lei Zhang', 'Xiaowei Fu', 'Suqi Song']","['Chongqing University', 'Chongqing University', 'Chongqing University', 'Chongqing University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28015,Security,3D Visibility-Aware Generalizable Neural Radiance Fields for Interacting Hands,"Neural radiance fields (NeRFs) are promising 3D representations for scenes, objects, and humans. However, most existing methods require multi-view inputs and per-scene training, which limits their real-life applications. Moreover, current methods focus on single-subject cases, leaving scenes of interacting hands that involve severe inter-hand occlusions and challenging view variations remain unsolved. To tackle these issues, this paper proposes a generalizable visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically, given an image of interacting hands as input, our VA-NeRF first obtains a mesh-based representation of hands and extracts their corresponding geometric and textural features. Subsequently, a feature fusion module that exploits the visibility of query points and mesh vertices is introduced to adaptively merge features of both hands, enabling the recovery of features in unseen areas. Additionally, our VA-NeRF is optimized together with a novel discriminator within an adversarial learning paradigm. In contrast to conventional discriminators that predict a single real/fake label for the synthesized image, the proposed discriminator generates a pixel-wise visibility map, providing fine-grained supervision for unseen areas and encouraging the VA-NeRF to improve the visual quality of synthesized images. Experiments on the Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms conventional NeRFs significantly. Project Page: https://github.com/XuanHuang0/VANeRF.","['CV: Computational Photography', 'Image & Video Synthesis', 'CV: Representation Learning for Vision']",[],"['Xuan Huang', 'Hanhui Li', 'Zejun Yang', 'Zhisheng Wang', 'Xiaodan Liang']","['Shenzhen Campus of Sun Yat-sen University, Shenzhen, China', 'Shenzhen Campus of Sun Yat-sen University, Shenzhen, China', 'Tencent, Shenzhen, China', 'Tencent, Shenzhen, China', 'Shenzhen Campus of Sun Yat-sen University, Shenzhen, China\nDarkMatter AI Research, Guangzhou, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28019,Security,COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks,"Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehave when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with outstanding attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternating manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on standard benchmark datasets. Our code is available at https://github.com/VinAIResearch/COMBAT.",['CV: Adversarial Attacks & Robustness'],[],"['Tran Huynh', 'Dang Nguyen', 'Tung Pham', 'Anh Tran']","['VinAI Research', 'University of Maryland', 'VinAI Research', 'VinAI Research']","['Vietnam', 'Vietnam', 'United States', 'Vietnam']"
https://ojs.aaai.org/index.php/AAAI/article/view/28020,Fairness & Bias,MagiCapture: High-Resolution Multi-Concept Portrait Customization,"Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profile photos. The main challenge with this task is the absence of ground truth for the composed concepts, leading to a reduction in the quality of the final output and an identity shift of the source subject. To address these issues, we present a novel Attention Refocusing loss coupled with auxiliary priors, both of which facilitate robust learning within this weakly supervised learning setting. Our pipeline also includes additional post-processing steps to ensure the creation of highly realistic outputs. MagiCapture outperforms other baselines in both quantitative and qualitative evaluations and can also be generalized to other non-human objects.","['CV: Computational Photography', 'Image & Video Synthesis', 'CV: Applications']",[],"['Junha Hyung', 'Jaeyo Shin', 'Jaegul Choo']","['KAIST', 'Sogang University', 'KAIST']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/28026,Fairness & Bias,Revealing the Proximate Long-Tail Distribution in Compositional Zero-Shot Learning,"Compositional Zero-Shot Learning (CZSL) aims to transfer knowledge from seen state-object pairs to novel unseen pairs. In this process, visual bias caused by the diverse interrelationship of state-object combinations blurs their visual features, hindering the learning of distinguishable class prototypes. Prevailing methods concentrate on disentangling states and objects directly from visual features, disregarding potential enhancements that could arise from a data viewpoint. Experimentally, we unveil the results caused by the above problem closely approximate the long-tailed distribution. As a solution, we transform CZSL into a proximate class imbalance problem. We mathematically deduce the role of class prior within the long-tailed distribution in CZSL. Building upon this insight, we incorporate visual bias caused by compositions into the classifier's training and inference by estimating it as a proximate class prior. This enhancement encourages the classifier to acquire more discernible class prototypes for each composition, thereby achieving more balanced predictions. Experimental results demonstrate that our approach elevates the model's performance to the state-of-the-art level, without introducing additional parameters.","['CV: Language and Vision', 'ML: Classification and Regression']",[],"['Chenyi Jiang', 'Haofeng Zhang']","['Nanjing University of Science and Technology', 'Nanjing University of Science and Technology']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28036,Security,Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks,"Image stitching seamlessly integrates images captured from varying perspectives into a single wide field-of-view image. Such integration not only broadens the captured scene but also augments holistic perception in computer vision applications. Given a pair of captured images, subtle perturbations and distortions which go unnoticed by the human visual system tend to attack the correspondence matching, impairing the performance of image stitching algorithms. In light of this challenge, this paper presents the first attempt to improve the robustness of image stitching against adversarial attacks. Specifically, we introduce a stitching-oriented attack (SoA), tailored to amplify the alignment loss within overlapping regions, thereby targeting the feature matching procedure. To establish an attack resistant model, we delve into the robustness of stitching architecture and develop an adaptive adversarial training (AAT) to balance attack resistance with stitching precision. In this way, we relieve the gap between the routine adversarial training and benign models, ensuring resilience without quality compromise. Comprehensive evaluation across real-world and synthetic datasets validate the deterioration of SoA on stitching performance. Furthermore, AAT emerges as a more robust solution against adversarial perturbations, delivering superior stitching results. Code is available at: https://github.com/Jzy2017/TRIS.","['CV: Low Level & Physics-based Vision', 'CV: Adversarial Attacks & Robustness', 'CV: Computational Photography', 'Image & Video Synthesis']",[],"['Zhiying Jiang', 'Xingyuan Li', 'Jinyuan Liu', 'Xin Fan', 'Risheng Liu']","['Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28047,Transparency & Explainability,Rethinking Robustness of Model Attributions,"For machine learning models to be reliable and trustworthy, their decisions must be interpretable. As these models find increasing use in safety-critical applications, it is important that not just the model predictions but also their explanations (as feature attributions) be robust to small human-imperceptible input perturbations. Recent works have shown that many attribution methods are fragile and have proposed improvements in either these methods or the model training. We observe two main causes for fragile attributions: first, the existing metrics of robustness (e.g., top-k intersection) overpenalize even reasonable local shifts in attribution, thereby making random perturbations to appear as a strong attack, and second, the attribution can be concentrated in a small region even when there are multiple important parts in an image. To rectify this, we propose simple ways to strengthen existing metrics and attribution methods that incorporate locality of pixels in robustness metrics and diversity of pixel locations in attributions. Towards the role of model training in attributional robustness, we empirically observe that adversarially trained models have more robust attributions on smaller datasets, however, this advantage disappears in larger datasets. Code is made available at https://github.com/ksandeshk/LENS.","['CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Sandesh Kamath', 'Sankalp Mittal', 'Amit Deshpande', 'Vineeth N Balasubramanian']","['Indian Institute of Technology, Hyderabad', 'Indian Institute of Technology, Hyderabad', 'Microsoft Research, Bengaluru', 'Indian Institute of Technology, Hyderabad']","['India', 'India', '', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/28049,Security,Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN,"Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, Our idea is to mitigate the problem of over-reliance on strong filters by substituting highly activated features. To this end, we present a novel method called Catch-up Mix, which provides learning opportunities to a wide range of filters during training, focusing on filters that may lag behind. By mixing activation maps with relatively lower norms, Catch-up Mix promotes the development of more diverse representations and reduces reliance on a small subset of filters. Experimental results demonstrate the superiority of our method in various vision classification datasets, providing enhanced robustness.","['CV: Adversarial Attacks & Robustness', 'CV: Learning & Optimization for CV']",[],"['Minsoo Kang', 'Minkoo Kang', 'Suhyun Kim']","['Korea Institute of Science and Technology\nKorea University', 'Korea Institute of Science and Technology', 'Korea Institute of Science and Technology']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/28065,Transparency & Explainability,Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive Learning in VQA,"Natural language explanation in visual question answer (VQA-NLE) aims to explain the decision-making process of models by generating natural language sentences to increase users' trust in the black-box systems. Existing post-hoc methods have achieved significant progress in obtaining a plausible explanation. However, such post-hoc explanations are not always aligned with human logical inference, suffering from the issues on: 1) Deductive unsatisfiability, the generated explanations do not logically lead to the answer; 2) Factual inconsistency, the model falsifies its counterfactual explanation for answers without considering the facts in images; and 3) Semantic perturbation insensitivity, the model can not recognize the semantic changes caused by small perturbations. These problems reduce the faithfulness of explanations generated by models. To address the above issues, we propose a novel self-supervised Multi-level Contrastive Learning based natural language Explanation model (MCLE) for VQA  with semantic-level, image-level, and instance-level factual and counterfactual samples. MCLE extracts discriminative features and aligns the feature spaces from explanations with visual question and answer to generate more consistent explanations. We conduct extensive experiments, ablation analysis, and case study to demonstrate the effectiveness of our method on two VQA-NLE benchmarks.","['CV: Language and Vision', 'CV: Interpretability', 'Explainability', 'and Transparency', 'NLP: Question Answering']",[],"['Chengen Lai', 'Shengli  Song', 'Shiqi Meng', 'Jingyang Li', 'Sitong Yan', 'Guangneng Hu']","['Xidian University', 'Xidian University', 'Xidian University', 'Xidian University', 'Xidian University', 'Xidian University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28067,Transparency & Explainability,ViTree: Single-Path Neural Tree for Step-Wise Interpretable Fine-Grained Visual Categorization,"As computer vision continues to advance and finds widespread applications across various domains, the need for interpretability in deep learning models becomes paramount. Existing methods often resort to post-hoc techniques or prototypes to explain the decision-making process, which can be indirect and lack intrinsic illustration. In this research, we introduce ViTree, a novel approach for fine-grained visual categorization that combines the popular vision transformer as a feature extraction backbone with neural decision trees. By traversing the tree paths, ViTree effectively selects patches from transformer-processed features to highlight informative local regions, thereby refining representations in a step-wise manner. Unlike previous tree-based models that rely on soft distributions or ensembles of paths, ViTree selects a single tree path, offering a clearer and simpler decision-making process. This patch and path selectivity enhances model interpretability of ViTree, enabling better insights into the model's inner workings. Remarkably, extensive experimentation validates that this streamlined approach surpasses various strong competitors and achieves state-of-the-art performance while maintaining exceptional interpretability which is proved by multi-perspective methods. Code can be found at https://github.com/SJTU-DeepVisionLab/ViTree.","['CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Danning Lao', 'Qi Liu', 'Jiazi Bu', 'Junchi Yan', 'Wei Shen']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28071,Security,Modeling Stereo-Confidence out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep,"We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map.  Based on this idea, the proposed stereo-confidence method can be summarized in three folds. 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed.  2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point. 3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement.  Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods.","['CV: 3D Computer Vision', 'APP: Mobility', 'Driving & Flight', 'CV: Vision for Robotics & Autonomous Driving']",[],"['Jae Young Lee', 'Woonghyun Ka', 'Jaehyun Choi', 'Junmo Kim']","['KAIST', 'Hyundai Motor Company', 'KAIST', 'KAIST']","['South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/28074,Security,Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile,"Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG.","['CV: Computational Photography', 'Image & Video Synthesis', 'CV: Adversarial Attacks & Robustness']",[],"['Seokjun Lee', 'Seung-Won Jung', 'Hyunseok Seo']","['Korea Institute of Science and Technology\nKorea University', 'Korea University', 'Korea Institute of Science and Technology']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/28077,Transparency & Explainability,Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention,"Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Object Detection & Categorization']",[],"['Saebom Leem', 'Hyunseok Seo']","['Korea Institute of Science and Technology\nSogang University', 'Korea Institute of Science and Technology']","['South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/28078,Fairness & Bias,Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget,"Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features code not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that utilizes the implicit clustering of the Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Notably, MAE-CT does not rely on hand-crafted augmentations and frequently achieves its best performances while using only minimal augmentations (crop & flip). Further, MAE-CT is compute efficient as it requires at most 10% overhead compared to MAE re-training. Applied to large and huge Vision Transformer (ViT) models, MAE-CT excels over previous self-supervised methods trained on ImageNet in linear probing, k-NN and low-shot classification accuracy as well as in unsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a new state-of-the-art in linear probing of 82.2%. Project page: github.com/ml-jku/MAE-CT.","['CV: Representation Learning for Vision', 'CV: Large Vision Models', 'General']",[],"['Johannes Lehner', 'Benedikt Alkin', 'Andreas Fürst', 'Elisabeth Rumetshofer', 'Lukas Miklautz', 'Sepp Hochreiter']","['ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning Johannes Kepler University, Linz, Austria', 'ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning Johannes Kepler University, Linz, Austria', 'ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning Johannes Kepler University, Linz, Austria', 'ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning Johannes Kepler University, Linz, Austria', 'Faculty of Computer Science, University of Vienna, Vienna, Austria\nUniVie Doctoral School Computer Science, University of Vienna', 'ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning Johannes Kepler University, Linz, Austria\nInstitute of Advanced Research in Artificial Intelligence (IARAI)']","['Austria', 'Austria', 'Austria', 'Austria', 'Austria', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/28082,Privacy & Data Governance,Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images,"Directly predicting human epidermal growth factor receptor 2 (HER2) status from widely available hematoxylin and eosin (HE)-stained whole slide images (WSIs) can reduce technical costs and expedite treatment selection. Accurately predicting HER2 requires large collections of multi-site WSIs. Federated learning enables collaborative training of these WSIs without gigabyte-size WSIs transportation and data privacy concerns. However, federated learning encounters challenges in addressing label imbalance in multi-site WSIs from the real world. Moreover, existing WSI classification methods cannot simultaneously exploit local context information and long-range dependencies in the site-end feature representation of federated learning. To address these issues, we present a point transformer with federated learning for multi-site HER2 status prediction from HE-stained WSIs. Our approach incorporates two novel designs. We propose a dynamic label distribution strategy and an auxiliary classifier, which helps to establish a well-initialized model and mitigate label distribution variations across sites. Additionally, we propose a farthest cosine sampling based on cosine distance. It can sample the most distinctive features and capture the long-range dependencies. Extensive experiments and analysis show that our method achieves state-of-the-art performance at four sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can generalize to two unseen sites with 229 WSIs. Code is available at: https://github.com/boyden/PointTransformerFL",['CV: Medical and Biological Imaging'],[],"['Bao Li', 'Zhenyu Liu', 'Lizhi Shao', 'Bensheng Qiu', 'Hong Bu', 'Jie Tian']","['Center for Biomedical Imaging, University of Science and Technology of China, Hefei, China\nCAS Key Laboratory of Molecular Imaging, Beijing Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'CAS Key Laboratory of Molecular Imaging, Beijing Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'CAS Key Laboratory of Molecular Imaging, Beijing Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Center for Biomedical Imaging, University of Science and Technology of China, Hefei, China', 'Department of Pathology, West China Hospital, Sichuan University, Chengdu, China', 'Center for Biomedical Imaging, University of Science and Technology of China, Hefei, China\nCAS Key Laboratory of Molecular Imaging, Beijing Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of Sciences, Beijing, China\nKey Laboratory of Big Data-Based Precision Medicine, Ministry of Industry and Information Technology, School of Engineering Medicine, Beihang University, Beijing, China']","['China', '', '', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28095,Privacy & Data Governance,FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy Labels,"Federated Learning with Noisy Labels (F-LNL) aims at seeking an optimal server model via collaborative distributed learning by aggregating multiple client models trained with local noisy or clean samples. On the basis of a federated learning framework, recent advances primarily adopt label noise filtering to separate clean samples from noisy ones on each client, thereby mitigating the negative impact of label noise. However, these prior methods do not learn noise filters by exploiting knowledge across all clients, leading to sub-optimal and inferior noise filtering performance and thus damaging training stability. In this paper, we present FedDiv to tackle the challenges of F-LNL. Specifically, we propose a global noise filter called Federated Noise Filter for effectively identifying samples with noisy labels on every client, thereby raising stability during local training sessions. Without sacrificing data privacy, this is achieved by modeling the global distribution of label noise across all clients. Then, in an effort to make the global model achieve higher performance, we introduce a Predictive Consistency based Sampler to identify more credible local data for local model training, thus preventing noise memorization and further boosting the training stability. Extensive experiments on CIFAR-10, CIFAR-100, and Clothing1M demonstrate that FedDiv achieves superior performance over state-of-the-art F-LNL methods under different label noise settings for both IID and non-IID data partitions. Source code is publicly available at https://github.com/lijichang/FLNL-FedDiv.","['CV: Object Detection & Categorization', 'ML: Distributed Machine Learning & Federated Learning', 'ML: Semi-Supervised Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Jichang Li', 'Guanbin Li', 'Hui Cheng', 'Zicheng Liao', 'Yizhou Yu']","['School of Computer Science and Engineering, Research Institute of Sun Yat-sen University in Shenzhen, Sun Yat-sen University, Guangzhou, China\nDepartment of Computer Science, The University of Hong Kong, Hong Kong', 'School of Computer Science and Engineering, Research Institute of Sun Yat-sen University in Shenzhen, Sun Yat-sen University, Guangzhou, China\nGuangdong Province Key Laboratory of Information Security Technology', 'UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China', 'Department of Computer Science, The University of Hong Kong, Hong Kong\nZhejiang University', 'Department of Computer Science, The University of Hong Kong, Hong Kong']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28098,Fairness & Bias,Panoptic Scene Graph Generation with Semantics-Prototype Learning,"Panoptic Scene Graph Generation (PSG) parses objects and predicts their relationships (predicate) to connect human language and visual scenes. However, different language preferences of annotators and semantic overlaps between predicates lead to biased predicate annotations in the dataset, i.e. different predicates for the same object pairs. Biased predicate annotations make PSG models struggle in constructing a clear decision plane among predicates, which greatly hinders the real application of PSG models. To address the intrinsic bias above, we propose a novel framework named ADTrans to adaptively transfer biased predicate annotations to informative and unified ones. To promise consistency and accuracy during the transfer process, we propose to observe the invariance degree of representations in each predicate class, and learn unbiased prototypes of predicates with different intensities. Meanwhile, we continuously measure the distribution changes between each presentation and its prototype, and constantly screen potentially biased data. Finally, with the unbiased predicate-prototype representation embedding space, biased annotations are easily identified. Experiments show that ADTrans significantly improves the performance of benchmark models, achieving a new state-of-the-art performance, and shows great generalization and effectiveness on multiple datasets. Our code is released at https://github.com/lili0415/PSG-biased-annotation.",['CV: Multi-modal Vision'],[],"['Li Li', 'Wei Ji', 'Yiming Wu', 'Mengze Li', 'You Qin', 'Lina Wei', 'Roger Zimmermann']","['National University of Singapore', 'National University of Singapore', 'The University of Sydney', 'Zhejiang University', 'National University of Singapore', 'Hangzhou City University', 'National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/28103,Fairness & Bias,Long-Tailed Learning as Multi-Objective Optimization,"Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models biased towards classes with sufficient samples and performing poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from the gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate long-tailed recognition as a multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately making every update under a Pareto descent direction. Our GBG method drives classes with similar gradient directions to form a more representative gradient and provides ideal compensation to the tail classes. Moreover, we conduct extensive experiments on commonly used benchmarks in long-tailed learning and demonstrate the superiority of our method over existing SOTA methods. Our code is released at https://github.com/WickyLee1998/GBG_v1.","['CV: Bias', 'Fairness & Privacy', 'CV: Learning & Optimization for CV', 'CV: Other Foundations of Computer Vision']",[],"['Weiqi Li', 'Fan Lyu', 'Fanhua Shang', 'Liang Wan', 'Wei Feng']","['College of Intelligence and Computing, Tianjin University', 'College of Intelligence and Computing, Tianjin University\nCRIPAC, MAIS, CASIA', 'College of Intelligence and Computing, Tianjin University', 'College of Intelligence and Computing, Tianjin University', 'College of Intelligence and Computing, Tianjin University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28104,Security,Temporal-Distributed Backdoor Attack against Video Based Action Recognition,"Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are independently embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a simple yet effective backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an imperceptible, temporally distributed trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed ""collateral damage"" through extensive studies.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Xi Li', 'Songhe Wang', 'Ruiquan Huang', 'Mahanth Gowda', 'George Kesidis']","['The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsylvania State University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28108,Transparency & Explainability,Causal Representation Learning via Counterfactual Intervention,"Existing causal representation learning methods are based on the causal graph they build. However, due to the omission of bias within the causal graph, they essentially encourage models to learn biased causal effects in latent space. In this paper, we propose a novel causally disentangling framework that aims to learn unbiased causal effects. We first introduce inductive and dataset biases into traditional causal graph for the physical concepts of interest. Then, we eliminate the negative effects from these two biases by counterfactual intervention with reweighted loss function for learning unbiased causal effects. Finally, we employ the causal effects into the VAE to endow the latent representations with causality. In particular, we highlight that removing biases in this paper is regarded as a part of learning process for unbiased causal effects, which is crucial for causal disentanglement performance improvement. Through extensive experiments on real-world and synthetic datasets, we show that our method outperforms different baselines and obtains the state-of-the-art results for achieving causal representation learning.","['CV: Representation Learning for Vision', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Xiutian Li', 'Siqi Sun', 'Rui Feng']","['School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai 200433', 'School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai 200433', 'School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai 200433\nFudan Zhangjiang Institute, Shanghai, 200120\nShanghai Collaborative Innovation Center of Intelligent Visual Computing']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28110,Security,Harnessing Edge Information for Improved Robustness in Vision Transformers,"Deep Neural Networks (DNNs) have demonstrated remarkable accuracy in vision classification tasks. However, they exhibit vulnerability to additional noises known as adversarial attacks. Previous studies hypothesize that this vulnerability might stem from the fact that high-accuracy DNNs heavily rely on irrelevant and non-robust features, such as textures and the background. In this work, we reveal that edge information extracted from images can provide relevant and robust features related to shapes and the foreground. These features assist pretrained DNNs in achieving improved adversarial robustness without compromising their accuracy on clean images. A lightweight and plug-and-play EdgeNet is proposed, which can be seamlessly integrated into existing pretrained DNNs, including Vision Transformers, a recent family of state-of-the-art models for vision classification. Our EdgeNet can process edges derived from either clean nature images or noisy adversarial images, yielding robust features which can be injected into the intermediate layers of the frozen backbone DNNs. The cost of obtaining such edges using conventional edge detection algorithms (e.g., Canny edge detector) is marginal, and the cost of training the EdgeNet is equivalent to that of fine-tuning the backbone network with techniques such as Adapter.",['CV: Adversarial Attacks & Robustness'],[],"['Yanxi Li', 'Chengbin  Du', 'Chang Xu']","['University of Sydney', 'University of Sydney', 'University of Sydney']","['Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/28117,Fairness & Bias,Hypercorrelation Evolution for Video Class-Incremental Learning,"Video class-incremental learning aims to recognize new actions while restricting the catastrophic forgetting of old ones, whose representative samples can only be saved in limited memory. Semantically variable subactions are susceptible to class confusion due to data imbalance. While existing methods address the problem by estimating and distilling the spatio-temporal knowledge, we further explores that the refinement of hierarchical correlations is crucial for the alignment of spatio-temporal features. To enhance the adaptability on evolved actions, we proposes a hierarchical aggregation strategy, in which  hierarchical matching matrices are combined and jointly optimized to selectively store and retrieve relevant features from previous tasks. Meanwhile, a correlation refinement mechanism is presented to reinforce the bias on informative exemplars according to online hypercorrelation distribution. Experimental results demonstrate the effectiveness of the proposed method on three standard video class-incremental learning benchmarks, outperforming state-of-the-art methods. Code is available at: https://github.com/Lsen991031/HCE","['CV: Video Understanding & Activity Analysis', 'ML: Life-Long and Continual Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Sen Liang', 'Kai Zhu', 'Wei Zhai', 'Zhiheng Liu', 'Yang Cao']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China\nInstitute of Artificial Intelligence, Hefei Comprehensive National Science Center']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28120,Fairness & Bias,Impartial Adversarial Distillation: Addressing Biased Data-Free Knowledge Distillation via Adaptive Constrained Optimization,"Data-Free Knowledge Distillation (DFKD) enables knowledge transfer from a pretrained teacher to a light-weighted student without original training data. Existing works are limited by a strong assumption that samples used to pretrain the teacher model are balanced, which is, however, unrealistic for many real-world tasks. In this work, we investigated a pragmatic yet under-explored problem: how to perform DFKD from a teacher model pretrained from imbalanced data. We observe a seemingly counter-intuitive phenomenon, i.e., adversarial DFKD algorithms favour minority classes, while causing a disastrous impact on majority classes. We theoretically prove that a biased teacher could cause severe disparity on different groups of synthetic data in adversarial distillation, which further exacerbates the mode collapse of a generator and consequently degenerates the overall accuracy of a distilled student model. To tackle this problem, we propose a class-adaptive regularization method, aiming to encourage impartial representation learning of a generator among different classes under a constrained learning formulation. We devise a primal-dual algorithm to solve the target optimization problem. Through extensive experiments, we show that our method mitigates the biased learning of majority classes in DFKD and improves the overall performance compared with baselines. Code will be available at https://github.com/ldpbuaa/ipad.","['CV: Bias', 'Fairness & Privacy', 'CV: Applications']",[],"['Dongping Liao', 'Xitong Gao', 'Chengzhong Xu']","['State Key Lab of IoTSC, Department of Computer and Information Science, University of Macau', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'University of Macau']","['Macau', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28127,Fairness & Bias,Weakly Supervised Open-Vocabulary Object Detection,"Despite weakly supervised object detection (WSOD) being a promising step toward evading strong instance-level annotations, its capability is confined to closed-set categories within a single training dataset. In this paper, we propose a novel weakly supervised open-vocabulary object detection framework, namely WSOVOD, to extend traditional WSOD to detect novel concepts and utilize diverse datasets with only image-level annotations. To achieve this, we explore three vital strategies, including dataset-level feature adaptation, image-level salient object localization, and region-level vision-language alignment. First, we perform data-aware feature extraction to produce an input-conditional coefficient, which is leveraged into dataset attribute prototypes to identify dataset bias and help achieve cross-dataset generalization. Second, a customized location-oriented weakly supervised region proposal network is proposed to utilize high-level semantic layouts from the category-agnostic segment anything model to distinguish object boundaries. Lastly, we introduce a proposal-concept synchronized multiple-instance network, i.e., object mining and refinement with visual-semantic alignment, to discover objects matched to the text embeddings of concepts. Extensive experiments on Pascal VOC and MS COCO demonstrate that the proposed WSOVOD achieves new state-of-the-art compared with previous WSOD methods in both close-set object localization and detection tasks. Meanwhile, WSOVOD enables cross-dataset and open-vocabulary learning to achieve on-par or even better performance than well-established fully-supervised open-vocabulary object detection (FSOVOD).","['CV: Object Detection & Categorization', 'CV: Multi-modal Vision', 'CV: Language and Vision']",[],"['Jianghang Lin', 'Yunhang Shen', 'Bingquan Wang', 'Shaohui Lin', 'Ke Li', 'Liujuan Cao']","['Xiamen University', 'Tencent', 'Xiamen University', 'East China Normal University', 'Tencent', 'Xiamen University']","['China', '', 'China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28133,Security,Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping,"Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.","['CV: Adversarial Attacks & Robustness', 'CV: Multi-modal Vision']",[],"['Qinliang Lin', 'Cheng Luo', 'Zenghao Niu', 'Xilin He', 'Weicheng Xie', 'Yuanbo Hou', 'Linlin Shen', 'Siyang Song']","['Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University', 'Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University', 'Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University', 'Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University', 'Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University\nShenzhen Institute of Artificial Intelligence and Robotics for Society\nGuangdong Key Laboratory of Intelligent Information Processing', 'WAVES Research Group, Ghent University, Belgium', 'Computer Vision Institute, School of Computer Science & Software Engineering, Shenzhen University\nShenzhen Institute of Artificial Intelligence and Robotics for Society\nGuangdong Key Laboratory of Intelligent Information Processing', 'University of Leicester, UK']","['China', 'China', 'China', 'China', '', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28137,Fairness & Bias,TD²-Net: Toward Denoising and Debiasing for Video Scene Graph Generation,"Dynamic scene graph generation (SGG) focuses on detecting objects in a video and determining their pairwise relationships. Existing dynamic SGG methods usually suffer from several issues, including 1) Contextual noise, as some frames might contain occluded and blurred objects. 2) Label bias, primarily due to the high imbalance between a few positive relationship samples and numerous negative ones. Additionally, the distribution of relationships exhibits a long-tailed pattern. To address the above problems, in this paper, we introduce a network named TD2-Net that aims at denoising and debiasing for dynamic SGG. Specifically, we first propose a denoising spatio-temporal transformer module that enhances object representation with robust contextual information. This is achieved by designing a differentiable Top-K object selector that utilizes the gumbel-softmax sampling strategy to select the relevant neighborhood for each object.  Second, we introduce an asymmetrical reweighting loss to relieve the issue of label bias. This loss function integrates asymmetry focusing factors and the volume of samples to adjust the weights assigned to individual samples. Systematic experimental results demonstrate the superiority of our proposed TD2-Net over existing state-of-the-art approaches on Action Genome databases. In more detail, TD2-Net outperforms the second-best competitors by 12.7% on mean-Recall@10 for predicate classification.","['CV: Video Understanding & Activity Analysis', 'CV: Scene Analysis & Understanding']",[],"['Xin Lin', 'Chong Shi', 'Yibing Zhan', 'Zuopeng Yang', 'Yaqi Wu', 'Dacheng Tao']","['Guangzhou University', 'Guangzhou University', 'JD Explore Academy', 'Guangzhou University', 'Guangzhou University', 'The University of Sydney']","['China', 'China', '', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28134,Fairness & Bias,A Fixed-Point Approach to Unified Prompt-Based Counting,"Existing class-agnostic counting models typically rely on a single type of prompt, e.g., box annotations. This paper aims to establish a comprehensive prompt-based counting framework capable of generating density maps for concerned objects indicated by various prompt types, such as box, point, and text. To achieve this goal, we begin by converting prompts from different modalities into prompt masks without requiring training. These masks are then integrated into a class-agnostic counting methodology for predicting density maps. Furthermore, we introduce a fixed-point inference along with an associated loss function to improve counting accuracy, all without introducing new parameters. The effectiveness of this method is substantiated both theoretically and experimentally. Additionally, a contrastive training scheme is implemented to mitigate dataset bias inherent in current class-agnostic counting datasets, a strategy whose effectiveness is confirmed by our ablation study. Our model excels in prominent class-agnostic datasets and exhibits superior performance in cross-dataset adaptation tasks.","['CV: Scene Analysis & Understanding', 'CV: Language and Vision', 'CV: Applications']",[],"['Wei Lin', 'Antoni B. Chan']","['City University of Hong Kong', 'City University of Hong Kong, Hong, Kong']","['Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28140,Security,Independency Adversarial Learning for Cross-Modal Sound Separation,"The sound mixture separation is still challenging due to heavy sound overlapping and disturbance from noise. Unsupervised separation would significantly increase the difficulty. As sound overlapping always hinders accurate sound separation, we propose an Independency Adversarial Learning based Cross-Modal Sound Separation (IAL-CMS) approach, where IAL employs adversarial learning to minimize the correlation of separated sound elements, exploring high sound independence; CMS performs cross-modal sound separation, incorporating audio-visual consistent feature learning and interactive cross-attention learning to emphasize the semantic consistency among cross-modal features. Both audio-visual consistency and audio consistency are kept to guarantee accurate separation. The consistency and sound independence ensure the decomposition of overlapping mixtures into unrelated and distinguishable sound elements. The proposed approach is evaluated on MUSIC, VGGSound, and AudioSet. Extensive experiments certify that our approach outperforms existing approaches in supervised and unsupervised scenarios.","['CV: Multi-modal Vision', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Zhenkai Lin', 'Yanli Ji', 'Yang Yang']","['University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28147,Security,Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent Diffusion Model,"Adversarial attacks involve adding perturbations to the source image to cause misclassification by the target model, which demonstrates the potential of attacking face recognition models. Existing adversarial face image generation methods still can’t achieve satisfactory performance because of low transferability and high detectability. In this paper, we propose a unified framework Adv-Diffusion that can generate imperceptible adversarial identity perturbations in the latent space but not the raw pixel space, which utilizes strong inpainting capabilities of the latent diffusion model to generate realistic adversarial images. Specifically, we propose the identity-sensitive conditioned diffusion generative model to generate semantic perturbations in the surroundings. The designed adaptive strength-based adversarial perturbation algorithm can ensure both attack transferability and stealthiness. Extensive qualitative and quantitative experiments on the public FFHQ and CelebA-HQ datasets prove the proposed method achieves superior performance compared with the state-of-the-art methods without an extra generative model training process. The source code is available at https://github.com/kopper-xdu/Adv-Diffusion.","['CV: Biometrics', 'Face', 'Gesture & Pose', 'CV: Adversarial Attacks & Robustness']",[],"['Decheng Liu', 'Xijun Wang', 'Chunlei Peng', 'Nannan Wang', 'Ruimin Hu', 'Xinbo Gao']","['School of Cyber Engineering, Xidian University, Xi’an, China\nKey Laboratory of Artificial Intelligence, Ministry of Education, Shanghai, China', 'School of Artifical Intelligence, Xidian University, Xi’an, China', 'School of Cyber Engineering, Xidian University, Xi’an, China\nKey Laboratory of Artificial Intelligence, Ministry of Education, Shanghai, China', 'School of Telecommunications Engineering, Xidian University, Xi’an, China', 'Hangzhou Institute of Technology, Xidian University, Xi’an, China', 'Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28146,Security,Explicitly Perceiving and Preserving the Local Geometric Structures for 3D Point Cloud Attack,"Deep learning models for point clouds have shown to be vulnerable to adversarial attacks, which have received increasing attention in various safety-critical applications such as autonomous driving, robotics, and surveillance. Existing 3D attack methods generally employ global distance losses to implicitly constrain the point-wise perturbations for optimization. However, these simple losses are quite difficult to accurately measure and restrict the proper 3D geometry as point clouds are highly structured. Although few recent works try to exploit additional shape-aware surface knowledge to globally constrain the point position, they still fail to preserve the detailed point-to-point geometric dependency in different local regions. To this end, in this paper, we propose a novel Multi-grained Geometry-aware Attack (MGA), which explicitly captures the local topology characteristics in different 3D regions for adversarial constraint. Specifically, we first develop multi-scale spectral local filter banks adapting to different 3D object shapes to explore potential geometric structures in local regions. Considering that objects may contain complex geometries, we then extend each filter bank into multi-layer ones to gradually capture the topology contexts of the same region in a coarse-to-fine manner. Hence, the focused local geometric structures will be highlighted in the coefficients calculated by the filtering process. At last, by restricting these coefficients between benign and adversarial samples, our MGA is able to properly measure and preserve the detailed geometry contexts in the whole 3D object with trivial perturbations. Extensive experiments demonstrate that our attack can achieve superior performance on various 3D classification models, with satisfying adversarial imperceptibility and strong resistance to different defense methods.","['CV: 3D Computer Vision', 'CV: Adversarial Attacks & Robustness']",[],"['Daizong Liu', 'Wei Hu']","['Wangxuan Institute of Computer Technology, Peking University', 'Wangxuan Institute of Computer Technology, Peking University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28150,Fairness & Bias,DiDA: Disambiguated Domain Alignment for Cross-Domain Retrieval with Partial Labels,"Driven by generative AI and the Internet, there is an increasing availability of a wide variety of images, leading to the significant and popular task of cross-domain image retrieval. To reduce annotation costs and increase performance, this paper focuses on an untouched but challenging problem, i.e., cross-domain image retrieval with partial labels (PCIR). Specifically, PCIR faces great challenges due to the ambiguous supervision signal and the domain gap. To address these challenges, we propose a novel method called disambiguated domain alignment (DiDA) for cross-domain retrieval with partial labels. In detail, DiDA elaborates a novel prototype-score unitization learning mechanism (PSUL) to extract common discriminative representations by simultaneously disambiguating the partial labels and narrowing the domain gap. Additionally, DiDA proposes a prototype-based domain alignment mechanism (PBDA) to further bridge the inherent cross-domain discrepancy. Attributed to PSUL and PBDA, our DiDA effectively excavates domain-invariant discrimination for cross-domain image retrieval. We demonstrate the effectiveness of DiDA through comprehensive experiments on three benchmarks, comparing it to existing state-of-the-art methods. Code available: https://github.com/lhrrrrrr/DiDA.","['CV: Image and Video Retrieval', 'ML: Multi-instance/Multi-view Learning']",[],"['Haoran Liu', 'Ying Ma', 'Ming Yan', 'Yingke Chen', 'Dezhong Peng', 'Xu Wang']","['College of Computer Science, Sichuan University, Chengdu, China\nNational Innovation Center for UHD Video Technology, Chengdu, China', 'Faculty of Computing, Harbin Institute of Technology, Harbin, China', 'Centre for Frontier AI Research (CFAR), A*STAR, Singapore', 'Department of Computer and Information Sciences, Northumbria University, UK', 'College of Computer Science, Sichuan University, Chengdu, China\nNational Innovation Center for UHD Video Technology, Chengdu, China', 'College of Computer Science, Sichuan University, Chengdu, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28157,Fairness & Bias,Frequency Shuffling and Enhancement for Open Set Recognition,"Open-Set Recognition (OSR) aims to accurately identify known classes while effectively rejecting unknown classes to guarantee reliability. Most existing OSR methods focus on learning in the spatial domain, where subtle texture and global structure are potentially intertwined. Empirical studies have shown that DNNs trained in the original spatial domain are inclined to over-perceive subtle texture. The biased semantic perception could lead to catastrophic over-confidence when predicting both known and unknown classes. To this end, we propose an innovative approach by decomposing the spatial domain to the frequency domain to separately consider global (low-frequency) and subtle (high-frequency) information, named Frequency Shuffling and Enhancement (FreSH). To alleviate the overfitting of subtle texture, we introduce the High-Frequency Shuffling (HFS) strategy that generates diverse high-frequency information and promotes the capture of low-frequency invariance. Moreover, to enhance the perception of global structure, we propose the Low-Frequency Residual (LFR) learning procedure that constructs a composite feature space, integrating low-frequency and original spatial features. Experiments on various benchmarks demonstrate that the proposed FreSH consistently trumps the state-of-the-arts by a considerable margin.","['CV: Object Detection & Categorization', 'CV: Representation Learning for Vision']",[],"['Lijun Liu', 'Rui Wang', 'Yuan Wang', 'Lihua Jing', 'Chuan Wang']","['Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, Beijing, China', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, Beijing, China', 'Department of Electronic Engineering,Tsinghua University', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, Beijing, China', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28156,Security,DifAttack: Query-Efficient Black-Box Adversarial Attack via Disentangled Feature Space,"This work investigates efficient score-based black-box adversarial attacks with high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a Disentangled Feature space, called DifAttack, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack firstly disentangles an image's latent feature into an adversarial feature and a visual feature, where the former dominates the adversarial capability of an image, while the latter largely determines its visual appearance. We train an autoencoder for the disentanglement by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, DifAttack iteratively optimizes the adversarial feature according to the query feedback from the victim model until a successful AE is generated, while keeping the visual feature unaltered. In addition, due to the avoidance of using surrogate models' gradient information when optimizing AEs for black-box models, our proposed DifAttack inherently possesses better attack capability in the open-set scenario, where the training dataset of the victim model is unknown. Extensive experimental results demonstrate that our method achieves significant improvements in ASR and query efficiency simultaneously, especially in the targeted attack and open-set scenarios. The code is available The code is available at https://github.com/csjunjun/DifAttack.git.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Jun Liu', 'Jiantao Zhou', 'Jiandian Zeng', 'Jinyu Tian']","['State Key Laboratory of Internet of Things for Smart City,Department of Computer and Information Science,University of Macau', 'State Key Laboratory of Internet of Things for Smart City,Department of Computer and Information Science,University of Macau', 'Institute of Artificial Intelligence and Future Networks, Beijing Normal University', 'School of Computer Science and Engineering, Macau University of Science and Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28169,Fairness & Bias,Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise,"The open sourcing of large amounts of image data promotes the development of deep learning techniques. Along with this comes the privacy risk of these image datasets being exploited by unauthorized third parties to train deep learning models for commercial or illegal purposes. To avoid the abuse of data, a poisoning-based technique, ""unlearnable example"", has been proposed to significantly degrade the generalization performance of models by adding imperceptible noise to the data. To further enhance its robustness against adversarial training, existing works leverage iterative adversarial training on both the defensive noise and the surrogate model. However, it still remains unknown whether the robustness of unlearnable examples primarily comes from the effect of enhancement in the surrogate model or the defensive noise. Observing that simply removing the adversarial perturbation on the training process of the defensive noise can improve the performance of robust unlearnable examples, we identify that solely the surrogate model's robustness contributes to the performance. Furthermore, we found a negative correlation exists between the robustness of defensive noise and the protection performance, indicating defensive noise's instability issue. Motivated by this, to further boost the robust unlearnable example, we introduce Stable Error-Minimizing noise (SEM), which trains the defensive noise against random perturbation instead of the time-consuming adversarial perturbation to improve the stability of defensive noise. Through comprehensive experiments, we demonstrate that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet Subset regarding both effectiveness and efficiency.","['CV: Bias', 'Fairness & Privacy', 'PEAI: Privacy & Security', 'CV: Adversarial Attacks & Robustness', 'ML: Privacy']",[],"['Yixin Liu', 'Kaidi Xu', 'Xun Chen', 'Lichao Sun']","['Lehigh University', 'Drexel University', 'Samsung Research America', 'Lehigh University']","['United States', 'United States', '', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28183,Transparency & Explainability,Set Prediction Guided by Semantic Concepts for Diverse Video Captioning,"Diverse video captioning aims to generate a set of sentences to describe the given video in various aspects. Mainstream methods are trained with independent pairs of a video and a caption from its ground-truth set without exploiting the intra-set relationship, resulting in low diversity of generated captions. Different from them, we formulate diverse captioning into a semantic-concept-guided set prediction (SCG-SP) problem by fitting the predicted caption set to the ground-truth set, where the set-level relationship is fully captured. Specifically, our set prediction consists of two synergistic tasks, i.e., caption generation and an auxiliary task of concept combination prediction providing extra semantic supervision. Each caption in the set is attached to a concept combination indicating the primary semantic content of the caption and facilitating element alignment in set prediction. Furthermore, we apply a diversity regularization term on concepts to encourage the model to generate semantically diverse captions with various concept combinations. These two tasks share multiple semantics-specific encodings as input, which are obtained by iterative interaction between visual features and conceptual queries. The correspondence between the generated captions and specific concept combinations further guarantees the interpretability of our model. Extensive experiments on benchmark datasets show that the proposed SCG-SP achieves state-of-the-art (SOTA) performance under both relevance and diversity metrics.",['CV: Language and Vision'],[],"['Yifan Lu', 'Ziqi Zhang', 'Chunfeng Yuan', 'Peng Li', 'Yan Wang', 'Bing Li', 'Weiming Hu']","['State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'Alibaba Group\nZhejiang Linkheer Science and Technology Co., Ltd.', 'Alibaba Group\nZhejiang Linkheer Science and Technology Co., Ltd.', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences\nSchool of Information Science and Technology, ShanghaiTech University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28188,Fairness & Bias,SCP: Spherical-Coordinate-Based Learned Point Cloud Compression,"In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to fully leverage the features of circular shapes and azimuthal angle invariance. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.","['CV: 3D Computer Vision', 'CV: Applications', 'CV: Bias', 'Fairness & Privacy', 'CV: Other Foundations of Computer Vision']",[],"['Ao Luo', 'Linxin Song', 'Keisuke Nonaka', 'Kyohei Unno', 'Heming Sun', 'Masayuki Goto', 'Jiro Katto']","['KDDI Research, Inc.\nWaseda University', 'Waseda University', 'KDDI Research, Inc.', 'KDDI Research, Inc.', 'Yokohama National University', 'Waseda University', 'Waseda University']","['Japan', 'Japan', '', '', 'Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/28189,Transparency & Explainability,DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular Videos,"Reconstructing a dynamic human with loose clothing is an important but difficult task. To address this challenge, we propose a method named DLCA-Recon to create human avatars from monocular videos. The distance from loose clothing to the underlying body rapidly changes in every frame when the human freely moves and acts. Previous methods lack effective geometric initialization and constraints for guiding the optimization of deformation to explain this dramatic change, resulting in the discontinuous and incomplete reconstruction surface.To model the deformation more accurately, we propose to initialize an estimated 3D clothed human in the canonical space, as it is easier for deformation fields to learn from the clothed human than from SMPL.With both representations of explicit mesh and implicit SDF, we utilize the physical connection information between consecutive frames and propose a dynamic deformation field (DDF) to optimize deformation fields. DDF accounts for contributive forces on loose clothing to enhance the interpretability of deformations and effectively capture the free movement of loose clothing. Moreover, we propagate SMPL skinning weights to each individual and refine pose and skinning weights during the optimization to improve skinning transformation. Based on more reasonable initialization and DDF, we can simulate real-world physics more accurately. Extensive experiments on public and our own datasets validate that our method can produce superior results for humans with loose clothing compared to the SOTA methods.","['CV: 3D Computer Vision', 'CV: Learning & Optimization for CV']",[],"['Chunjie Luo', 'Fei Luo', 'Yusen Wang', 'Enxu Zhao', 'Chunxia Xiao']","['School of Computer Science, Wuhan University, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China', 'School of Computer Science, Wuhan University, Wuhan, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28196,Fairness & Bias,Modeling Continuous Motion for 3D Point Cloud Object Tracking,"The task of 3D single object tracking (SOT) with LiDAR point clouds is crucial for various applications, such as autonomous driving and robotics. However, existing approaches have primarily relied on appearance matching or motion modeling within only two successive frames, thereby overlooking the long-range continuous motion property of objects in 3D space. To address this issue, this paper presents a novel approach that views each tracklet as a continuous stream: at each timestamp, only the current frame is fed into the network to interact with multi-frame historical features stored in a memory bank, enabling efficient exploitation of sequential information. To achieve effective cross-frame message passing, a hybrid attention mechanism is designed to account for both long-range relation modeling and local geometric feature extraction. Furthermore, to enhance the utilization of multi-frame features for robust tracking, a contrastive sequence enhancement strategy is proposed, which uses ground truth tracklets to augment training sequences and promote discrimination against false positives in a contrastive manner. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art method by significant margins on multiple benchmarks.","['CV: Motion & Tracking', 'CV: 3D Computer Vision', 'CV: Vision for Robotics & Autonomous Driving']",[],"['Zhipeng Luo', 'Gongjie Zhang', 'Changqing Zhou', 'Zhonghua Wu', 'Qingyi Tao', 'Lewei Lu', 'Shijian Lu']","['S-Lab, Nanyang Technological University\nBlack Sesame Technologies', 'S-Lab, Nanyang Technological University', 'SenseTime Research', 'SenseTime Research', 'SenseTime Research', 'SenseTime Research', 'S-Lab, Nanyang Technological University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28200,Security,SlowTrack: Increasing the Latency of Camera-Based Perception in Autonomous Driving Using Adversarial Examples,"In Autonomous Driving (AD), real-time perception is a critical component responsible for detecting surrounding objects to ensure safe driving. While researchers have extensively explored the integrity of AD perception due to its safety and security implications, the aspect of availability (real-time performance) or latency has received limited attention. Existing works on latency-based attack have focused mainly on object detection, i.e., a component in camera-based AD perception, overlooking the entire camera-based AD perception, which hinders them to achieve effective system-level effects, such as vehicle crashes. In this paper, we propose SlowTrack, a novel framework for generating adversarial attacks to increase the execution time of camera-based AD perception. We propose a novel two-stage attack strategy along with the three new loss function designs. Our evaluation is conducted on four popular camera-based AD perception pipelines, and the results demonstrate that SlowTrack significantly outperforms existing latency-based attacks while maintaining comparable imperceptibility levels. Furthermore, we perform the evaluation on Baidu Apollo, an industry-grade full-stack AD system, and LGSVL, a production-grade AD simulator, with two scenarios to compare the system-level effects of SlowTrack and existing attacks. Our evaluation results show that the system-level effects can be significantly improved, i.e., the vehicle crash rate of SlowTrack is around 95% on average while existing works only have around 30%.","['CV: Adversarial Attacks & Robustness', 'CV: Vision for Robotics & Autonomous Driving', 'APP: Security']",[],"['Chen Ma', 'Ningfei Wang', 'Qi Alfred Chen', 'Chao Shen']","[""Xi'an Jiaotong University"", 'University of California, Irvine', 'University of California, Irvine', ""Xi'an Jiaotong University""]","['China', 'United States', 'United States', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28201,Transparency & Explainability,Uncertainty-Aware GAN for Single Image Super Resolution,"Generative adversarial network (GAN) has become a popular tool in the perceptual-oriented single image super-resolution (SISR) for its excellent capability to hallucinate details. However, the performance of most GAN-based SISR methods is impeded due to the limited discriminative ability of their discriminators. In specific, these discriminators only focus on the global image reconstruction quality and ignore the more fine-grained reconstruction quality for constraining the generator, as they predict the overall realness of an image instead of the pixel-level realness. Here, we first introduce the uncertainty into the GAN and propose an Uncertainty-aware GAN (UGAN) to regularize SISR solutions, where the challenging pixels with large reconstruction uncertainty and importance (e.g., texture and edge) are prioritized for optimization. The uncertainty-aware adversarial training strategy enables the discriminator to capture the pixel-level SR uncertainty, which constrains the generator to focus on image areas with high reconstruction difficulty, meanwhile, it improves the interpretability of the SR. To balance weights of multiple training losses, we introduce an uncertainty-aware loss weighting strategy to adaptively learn the optimal loss weights. Extensive experiments demonstrate the effectiveness of our approach in extracting the SR uncertainty and the superiority of the UGAN over the state-of-the-arts in terms of the reconstruction accuracy and perceptual quality.","['CV: Low Level & Physics-based Vision', 'CV: Adversarial Attacks & Robustness', 'CV: Applications', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],['Chenxi Ma'],['Fudan University'],['China']
https://ojs.aaai.org/index.php/AAAI/article/view/28199,Fairness & Bias,FedST: Federated Style Transfer Learning for Non-IID Image Segmentation,"Federated learning collaboratively trains machine learning models among different clients while keeping data privacy and has become the mainstream for breaking data silos. However, the non-independently and identically distribution (i.e., Non-IID) characteristic of different image domains among different clients reduces the benefits of federated learning and has become a bottleneck problem restricting the accuracy and generalization of federated models. In this work, we propose a novel federated image segmentation method based on style transfer, FedST, by using a denoising diffusion probabilistic model to achieve feature disentanglement and image synthesis of cross-domain image data between multiple clients. Thus it can share style features among clients while protecting structure features of image data, which effectively alleviates the influence of the Non-IID phenomenon. Experiments prove that our method achieves superior segmentation performance compared to state-of-art methods among four different Non-IID datasets in objective and subjective assessment. The code is available at https://github.com/YoferChen/FedST.","['CV: Bias', 'Fairness & Privacy', 'CV: Segmentation']",[],"['Boyuan Ma', 'Xiang Yin', 'Jing Tan', 'Yongfeng Chen', 'Haiyou Huang', 'Hao Wang', 'Weihua Xue', 'Xiaojuan Ban']","['University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'Liaoning Technical University', 'University of Science and Technology Beijing']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28217,Fairness & Bias,Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated Outlier Class Learning,"Existing out-of-distribution (OOD) methods have shown great success on balanced datasets but become ineffective in long-tailed recognition (LTR) scenarios where 1) OOD samples are often wrongly classified into head classes and/or 2) tail-class samples are treated as OOD samples. To address these issues, current studies fit a prior distribution of auxiliary/pseudo OOD data to the long-tailed in-distribution (ID) data. However, it is difficult to obtain such an accurate prior distribution given the unknowingness of real OOD samples and heavy class imbalance in LTR. A straightforward solution to avoid the requirement of this prior is to learn an outlier class to encapsulate the OOD samples. The main challenge is then to tackle the aforementioned confusion between OOD samples and head/tail-class samples when learning the outlier class. To this end, we introduce a novel calibrated outlier class learning (COCL) approach, in which 1) a debiased large margin learning method is introduced in the outlier class learning to distinguish OOD samples from both head and tail classes in the representation space and 2) an outlier-class-aware logit calibration method is defined to enhance the long-tailed classification confidence. Extensive empirical results on three popular benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that COCL substantially outperforms existing state-of-the-art OOD detection methods in LTR while being able to improve the classification accuracy on ID data.  Code is available at https://github.com/mala-lab/COCL.","['CV: Object Detection & Categorization', 'CV: Adversarial Attacks & Robustness', 'CV: Applications']",[],"['Wenjun Miao', 'Guansong Pang', 'Xiao Bai', 'Tianqi Li', 'Jin Zheng']","['School of Computer Science and Engineering, Beihang University', 'School of Computing and Information Systems, Singapore Management University', 'School of Computer Science and Engineering, Beihang University\nState Key Laboratory of Software Development Environment, Jiangxi Research Institute, Beihang University', 'School of Computer Science and Engineering, Beihang University', 'School of Computer Science and Engineering, Beihang University\nState Key Laboratory of Virtual Reality Technology and Systems, Beihang University']","['', 'Singapore', '', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/28228,Transparency & Explainability,Adversarial Attacks on the Interpretation of Neuron Activation Maximization,"Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet.","['CV: Interpretability', 'Explainability', 'and Transparency', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Geraldin Nanfack', 'Alexander Fulleringer', 'Jonathan Marty', 'Michael Eickenberg', 'Eugene Belilovsky']","['Concordia University\nMila – Quebec AI Institute', 'Concordia University\nMila – Quebec AI Institute', 'Princeton University', 'Flatiron Institute', 'Concordia University\nMila – Quebec AI Institute']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28225,Security,TETRIS: Towards Exploring the Robustness of Interactive Segmentation,"Interactive segmentation methods rely on user inputs to iteratively update the selection mask. A click specifying the object of interest is arguably the most simple and intuitive interaction type, and thereby the most common choice for interactive segmentation. However, user clicking patterns in the interactive segmentation context remain unexplored. Accordingly, interactive segmentation evaluation strategies rely more on intuition and common sense rather than empirical studies (e.g., assuming that users tend to click in the center of the area with the largest error). In this work, we conduct a real-user study to investigate real user clicking patterns. This study reveals that the intuitive assumption made in the common evaluation strategy may not hold. As a result, interactive segmentation models may show high scores in the standard benchmarks, but it does not imply that they would perform well in a real world scenario. To assess the applicability of interactive segmentation methods, we propose a novel evaluation strategy providing a more comprehensive analysis of a model's performance. To this end, we propose a methodology for finding extreme user inputs by a direct optimization in a white-box adversarial attack on the interactive segmentation model. Based on the performance with such adversarial user inputs, we assess the robustness of interactive segmentation models w.r.t click positions. Besides, we introduce a novel benchmark for measuring the robustness of interactive segmentation, and report the results of an extensive evaluation of dozens of models.","['CV: Segmentation', 'CV: Adversarial Attacks & Robustness', 'HAI: Human-Computer Interaction', 'HAI: User Experience and Usability']",[],"['Andrey Moskalenko', 'Vlad Shakhuro', 'Anna Vorontsova', 'Anton Konushin', 'Anton Antonov', 'Alexander Krapukhin', 'Denis Shepelev', 'Konstantin Soshin']","['Samsung Research', 'Samsung Research', 'Samsung Research', 'Samsung Research', 'Samsung Research', 'Samsung Research', 'Samsung Research', 'Samsung Research']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28232,Transparency & Explainability,Progressive Painterly Image Harmonization from Low-Level Styles to High-Level Styles,"Painterly image harmonization aims to harmonize a photographic foreground object on the painterly background. Different from previous auto-encoder based harmonization networks, we develop a progressive multi-stage harmonization network, which harmonizes the composite foreground from low-level styles (e.g., color, simple texture) to high-level styles (e.g., complex texture). Our network has better interpretability and harmonization performance. Moreover, we design an early-exit strategy to automatically decide the proper stage to exit, which can skip the unnecessary and even harmful late stages. Extensive experiments on the benchmark dataset demonstrate the effectiveness of our progressive harmonization network.","['CV: Computational Photography', 'Image & Video Synthesis']",[],"['Li Niu', 'Yan Hong', 'Junyan Cao', 'Liqing Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28235,Transparency & Explainability,NeSyFOLD: A Framework for Interpretable Image Classification,"Deep learning models such as CNNs have surpassed human performance in computer vision tasks such as image classi- fication. However, despite their sophistication, these models lack interpretability which can lead to biased outcomes re- flecting existing prejudices in the data. We aim to make pre- dictions made by a CNN interpretable. Hence, we present a novel framework called NeSyFOLD to create a neurosym- bolic (NeSy) model for image classification tasks. The model is a CNN with all layers following the last convolutional layer replaced by a stratified answer set program (ASP) derived from the last layer kernels. The answer set program can be viewed as a rule-set, wherein the truth value of each pred- icate depends on the activation of the corresponding kernel in the CNN. The rule-set serves as a global explanation for the model and is interpretable. We also use our NeSyFOLD framework with a CNN that is trained using a sparse kernel learning technique called Elite BackProp (EBP). This leads to a significant reduction in rule-set size without compromising accuracy or fidelity thus improving scalability of the NeSy model and interpretability of its rule-set. Evaluation is done on datasets with varied complexity and sizes. We also pro- pose a novel algorithm for labelling the predicates in the rule- set with meaningful semantic concept(s) learnt by the CNN. We evaluate the performance of our “semantic labelling algo- rithm” to quantify the efficacy of the semantic labelling for both the NeSy model and the NeSy-EBP model.","['CV: Interpretability', 'Explainability', 'and Transparency', 'KRR: Logic Programming', 'ML: Neuro-Symbolic Learning', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Parth Padalkar', 'Huaduo Wang', 'Gopal Gupta']","['The University of Texas at Dallas', 'The University of Texas at Dallas', 'The University of Texas at Dallas']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28237,Fairness & Bias,Less Is More: Label Recommendation for Weakly Supervised Point Cloud Semantic Segmentation,"Weak supervision has proven to be an effective strategy for reducing the burden of annotating semantic segmentation tasks in 3D space. However, unconstrained or heuristic weakly supervised annotation forms may lead to suboptimal label efficiency. To address this issue, we propose a novel label recommendation framework for weakly supervised point cloud semantic segmentation. Distinct from pre-training and active learning, the label recommendation framework consists of three stages: inductive bias learning, recommendations for points to be labeled, and point cloud semantic segmentation learning. In practice, we first introduce the point cloud upsampling task to induct inductive bias from structural information. During the recommendation stage, we present a cross-scene clustering strategy to generate centers of clustering as recommended points. Then we introduce a recommended point positions attention module LabelAttention to model the long-range dependency under sparse annotations. Additionally, we employ position encoding to enhance the spatial awareness of semantic features. Throughout the framework, the useful information obtained from inductive bias learning is propagated to subsequent semantic segmentation networks in the form of label positions. Experimental results demonstrate that our framework outperforms weakly supervised point cloud semantic segmentation methods and other methods for labeling efficiency on S3DIS and ScanNetV2, even at an extremely low label rate.","['CV: 3D Computer Vision', 'CV: Segmentation']",[],"['Zhiyi Pan', 'Nan Zhang', 'Wei Gao', 'Shan Liu', 'Ge Li']","['SECE, Shenzhen Graduate School, Peking University\nPeng Cheng Laboratory', 'SECE, Shenzhen Graduate School, Peking University', 'SECE, Shenzhen Graduate School, Peking University', 'Media Laboratory, Tencent', 'SECE, Shenzhen Graduate School, Peking University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28249,Fairness & Bias,Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification,"Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets’ performance over traditional adaptation methods by simply.","['CV: Language and Vision', 'CV: Large Vision Models', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Wenshuo Peng', 'Kaipeng Zhang', 'Yue Yang', 'Hao Zhang', 'Yu Qiao']","['Shanghai AI Laboratory', 'Shanghai AI Laboratory', 'Shanghai AI Laboratory\nShanghai Jiao Tong University', ""Shanghai AI Laboratory\nXi'an Jiaotong University"", 'Shanghai AI Laboraotry']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28252,Fairness & Bias,Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video,"Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias strategy (BSSARD), which dynamically generates bias-conflict samples by explicitly leveraging potentially spurious correlations between single-modality features and the temporal position of the target moments. Through adversarial training, its bias generators continuously introduce biases and generate bias-conflict samples to deceive its grounding model. Meanwhile, the grounding model continuously eliminates the introduced biases, which requires it to model multi-modality alignment information. BSSARD will cover most kinds of coupling relationships and disrupt language and visual biases simultaneously. Extensive experiments on Charades-CD and ActivityNet-CD demonstrate the promising debiasing capability of BSSARD. Source codes are available at https://github.com/qzhb/BSSARD.","['CV: Multi-modal Vision', 'CV: Language and Vision']",[],"['Zhaobo Qi', 'Yibo Yuan', 'Xiaowen Ruan', 'Shuhui Wang', 'Weigang Zhang', 'Qingming Huang']","['Harbin Institute of Technology, Weihai, China', 'Harbin Institute of Technology, Weihai, China', 'Harbin Institute of Technology, Weihai, China', 'Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China', 'Harbin Institute of Technology, Weihai, China', 'University of Chinese Academy of Sciences, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28258,Transparency & Explainability,Empowering CAM-Based Methods with Capability to Generate Fine-Grained and High-Faithfulness Explanations,"Recently, the explanation of neural network models has garnered considerable research attention. In computer vision, CAM (Class Activation Map)-based methods and LRP (Layer-wise Relevance Propagation) method are two common explanation methods. However, since most CAM-based methods can only generate global weights, they can only generate coarse-grained explanations at a deep layer. LRP and its variants, on the other hand, can generate fine-grained explanations. But the faithfulness of the explanations is too low. To address these challenges, in this paper, we propose FG-CAM (Fine-Grained CAM), which extends CAM-based methods to enable generating fine-grained and high-faithfulness explanations. FG-CAM uses the relationship between two adjacent layers of feature maps with resolution differences to gradually increase the explanation resolution, while finding the contributing pixels and filtering out the pixels that do not contribute. Our method not only solves the shortcoming of CAM-based methods without changing their characteristics, but also generates fine-grained explanations that have higher faithfulness than LRP and its variants. We also present FG-CAM with denoising, which is a variant of FG-CAM and is able to generate less noisy explanations with almost no change in explanation faithfulness. Experimental results show that the performance of FG-CAM is almost unaffected by the explanation resolution. FG-CAM outperforms existing CAM-based methods significantly in both shallow and intermediate layers, and outperforms LRP and its variants significantly in the input layer. Our code is available at https://github.com/dongmo-qcq/FG-CAM.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Learning & Optimization for CV']",[],"['Changqing Qiu', 'Fusheng Jin', 'Yining Zhang']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'Peking University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28265,Fairness & Bias,Cross-Sentence Gloss Consistency for Continuous Sign Language Recognition,"Continuous sign language recognition (CSLR) aims to recognize gloss sequences from continuous sign videos. Recent works enhance the gloss representation consistency by mining correlations between visual and contextual modules within individual sentences. However, there still remain much richer correlations among glosses across different sentences. In this paper, we present a simple yet effective Cross-Sentence Gloss Consistency (CSGC), which enforces glosses belonging to a same category to be more consistent in representation than those belonging to different categories, across all training sentences. Specifically, in CSGC, a prototype is maintained for each gloss category and benefits the gloss discrimination in a contrastive way. Thanks to the well-distinguished gloss prototype, an auxiliary similarity classifier is devised to enhance the recognition clues, thus yielding more accurate results. Extensive experiments conducted on three CSLR datasets show that our proposed CSGC significantly boosts the performance of CSLR, surpassing existing state-of-the-art works by large margins (i.e., 1.6% on PHOENIX14, 2.4% on PHOENIX14-T, and 5.7% on CSL-Daily).",['CV: Video Understanding & Activity Analysis'],[],"['Qi Rao', 'Ke Sun', 'Xiaohan Wang', 'Qi Wang', 'Bang Zhang']","['University of Technology Sydney', 'Alibaba', 'Stanford University', 'Alibaba', 'Alibaba']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28267,Fairness & Bias,Multi-Step Denoising Scheduled Sampling: Towards Alleviating Exposure Bias for Diffusion Models,"Denoising Diffusion Probabilistic Models (DDPMs) have achieved significant success in generation tasks. Nevertheless, the exposure bias issue, i.e., the natural discrepancy between the training (the output of each step is calculated individually by a given input) and inference (the output of each step is calculated based on the input iteratively obtained based on the model), harms the performance of DDPMs. To our knowledge, few works have tried to tackle this issue by modifying the training process for DDPMs, but they still perform unsatisfactorily due to 1) partially modeling the discrepancy and 2) ignoring the prediction error accumulation. To address the above issues, in this paper, we propose a multi-step denoising scheduled sampling (MDSS) strategy to alleviate the exposure bias for DDPMs. Analyzing the formulations of the training and inference of DDPMs, MDSS 1) comprehensively considers the discrepancy influence of prediction errors on the output of the model (the Gaussian noise) and the output of the step (the calculated input signal of the next step), and 2) efficiently models the prediction error accumulation by using multiple iterations of a mathematical formulation initialized from one-step prediction error obtained from the model. The experimental results, compared with previous works, demonstrate that our approach is more effective in mitigating exposure bias in DDPM, DDIM, and DPM-solver. In particular, MDSS achieves an FID score of 3.86 in 100 sample steps of DDIM on the CIFAR-10 dataset, whereas the second best obtains 4.78. The code will be available on GitHub.","['CV: Computational Photography', 'Image & Video Synthesis']",[],"['Zhiyao Ren', 'Yibing Zhan', 'Liang Ding', 'Gaoang Wang', 'Chaoyue Wang', 'Zhongyi Fan', 'Dacheng Tao']","['The University of Sydney', 'JD Explore Academy', 'JD Explore Academy', 'Zhejiang University', 'The University of Sydney', 'JD Explore Academy', 'The University of Sydney']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28274,Fairness & Bias,Polyper: Boundary Sensitive Polyp Segmentation,"We present a new boundary sensitive framework for polyp segmentation, termed Polyper.Our method is motivated by a clinical approach that seasoned medical practitioners often leverage the inherent features of interior polyp regions to tackle blurred boundaries.Inspired by this, we propose to explicitly leverages boundary regions to bolster the model's boundary discrimination capability while minimizing computational resource wastage. Our approach first extracts low-confidence boundary regions and high-confidence prediction regions from an initial segmentation map through differentiable morphological operators.Then, we design the boundary sensitive attention that concentrates on augmenting the features near the boundary regions using the high-confidence prediction region's characteristics to generate good segmentation results.Our proposed method can be seamlessly integrated with classical encoder networks, like ResNet-50, MiT-B1, and Swin Transformer.To evaludate the effectiveness of Polyper, we conduct experiments on five publicly available challenging datasets, and receive state-of-the-art performance on all of them. Code is available at https://github.com/haoshao-nku/medical_seg.git.","['CV: Medical and Biological Imaging', 'CV: Segmentation']",[],"['Hao Shao', 'Yang Zhang', 'Qibin Hou']","['VCIP, School of Computer Science, Nankai University', 'Department of Genetics and Cell Biology, College of Life Sciences, Nankai University', 'VCIP, School of Computer Science, Nankai University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28280,Fairness & Bias,CGMGM: A Cross-Gaussian Mixture Generative Model for Few-Shot Semantic Segmentation,"Few-shot semantic segmentation (FSS) aims to segment unseen objects in a query image using a few pixel-wise annotated support images, thus expanding the capabilities of semantic segmentation. The main challenge lies in extracting sufficient information from the limited support images to guide the segmentation process. Conventional methods typically address this problem by generating single or multiple prototypes from the support images and calculating their cosine similarity to the query image. However, these methods often fail to capture meaningful information for modeling the de facto joint distribution of pixel and category. Consequently, they result in incomplete segmentation of foreground objects and mis-segmentation of the complex background. To overcome this issue, we propose the Cross Gaussian Mixture Generative Model (CGMGM), a novel Gaussian Mixture Models~(GMMs)-based FSS method, which establishes the joint distribution of pixel and category in both the support and query images. Specifically, our method initially matches the feature representations of the query image with those of the support images to generate and refine an initial segmentation mask. It then employs GMMs to accurately model the joint distribution of foreground and background using the support masks and the initial segmentation mask. Subsequently, a parametric decoder utilizes the posterior probability of pixels in the query image, by applying the Bayesian theorem, to the joint distribution, to generate the final segmentation mask. Experimental results on PASCAL-5i and COCO-20i datasets demonstrate our CGMGM's effectiveness and superior performance compared to the state-of-the-art methods.","['CV: Segmentation', 'CV: Bias', 'Fairness & Privacy', 'CV: Image and Video Retrieval', 'CV: Vision for Robotics & Autonomous Driving']",[],"['Junao Shen', 'Kun Kuang', 'Jiaheng Wang', 'Xinyu Wang', 'Tian Feng', 'Wei Zhang']","['School of Software Technology, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'School of Software Technology, Zhejiang University', 'School of Software Technology, Zhejiang University', 'School of Software Technology, Zhejiang University', 'School of Software Technology, Zhejiang University\nInnovation Center of Yangtze River Delta, Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28285,Fairness & Bias,Transformer-Based No-Reference Image Quality Assessment via Supervised Contrastive Learning,"Image Quality Assessment (IQA) has long been a research hotspot in the field of image processing, especially No-Reference Image Quality Assessment (NR-IQA). Due to the powerful feature extraction ability, existing Convolution Neural Network (CNN) and Transformers based NR-IQA methods have achieved considerable progress. However, they still exhibit limited capability when facing unknown authentic distortion datasets. To further improve NR-IQA performance, in this paper, a novel supervised contrastive learning (SCL) and Transformer-based NR-IQA model SaTQA is proposed. We first train a model on a large-scale synthetic dataset by SCL (no image subjective score is required) to extract degradation features of images with various distortion types and levels. To further extract distortion information from images, we propose a backbone network incorporating the Multi-Stream Block (MSB) by combining the CNN inductive bias and Transformer long-term dependence modeling capability. Finally, we propose the Patch Attention Block (PAB) to obtain the final distorted image quality score by fusing the degradation features learned from contrastive learning with the perceptual distortion information extracted by the backbone network. Experimental results on six standard IQA datasets show that SaTQA outperforms the state-of-the-art methods for both synthetic and authentic datasets. Code is available at https://github.com/I2-Multimedia-Lab/SaTQA.","['CV: Low Level & Physics-based Vision', 'CV: Applications']",[],"['Jinsong Shi', 'Pan Gao', 'Jie Qin']","['Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28289,Fairness & Bias,DPA-P2PNet: Deformable Proposal-Aware P2PNet for Accurate Point-Based Cell Detection,"Point-based cell detection (PCD), which pursues high-performance cell sensing under low-cost data annotation, has garnered increased attention in computational pathology community. Unlike mainstream PCD methods that rely on intermediate density map representations, the Point-to-Point network (P2PNet) has recently emerged as an end-to-end solution for PCD, demonstrating impressive cell detection accuracy and efficiency. Nevertheless, P2PNet is limited to decoding from a single-level feature map due to the scale-agnostic property of point proposals, which is insufficient to leverage multi-scale information. Moreover, the spatial distribution of pre-set point proposals is biased from that of cells, leading to inaccurate cell localization. To lift these limitations, we present DPA-P2PNet in this work. The proposed method directly extracts multi-scale features for decoding according to the coordinates of point proposals on hierarchical feature maps. On this basis, we further devise deformable point proposals to mitigate the positional bias between proposals and potential cells to promote cell localization. Inspired by practical pathological diagnosis that usually combines high-level tissue structure and low-level cell morphology for accurate cell classification, we propose a multi-field-of-view (mFoV) variant of DPA-P2PNet to accommodate additional large FoV images with tissue information as model input. Finally, we execute the first self-supervised pre-training on immunohistochemistry histopathology image data and evaluate the suitability of four representative self-supervised methods on the PCD task. Experimental results on three benchmarks and a large-scale and real-world interval dataset demonstrate the superiority of our proposed models over the state-of-the-art counterparts. Codes and pre-trained weights are available at https://github.com/windygoo/DPA-P2PNet.","['CV: Medical and Biological Imaging', 'ML: Applications']",[],"['Zhongyi Shui', 'Sunyi Zheng', 'Chenglu Zhu', 'Shichuan Zhang', 'Xiaoxuan Yu', 'Honglin Li', 'Jingxiong Li', 'Pingyi Chen', 'Lin Yang']","['Zhejiang University\nWestlake University', 'Westlake University', 'Westlake University', 'Zhejiang University\nWestlake University', 'Zhejiang University\nWestlake University', 'Zhejiang University\nWestlake University', 'Zhejiang University\nWestlake University', 'Zhejiang University\nWestlake University', 'Westlake University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28307,Fairness & Bias,RL-SeqISP: Reinforcement Learning-Based Sequential Optimization for Image Signal Processing,"Hardware image signal processing (ISP), aiming at converting RAW inputs to RGB images, consists of a series of processing blocks, each with multiple parameters. Traditionally, ISP parameters are manually tuned in isolation by imaging experts according to application-specific quality and performance metrics, which is time-consuming and biased towards human perception due to complex interaction with the output image. Since the relationship between any single parameter’s variation and the output performance metric is a complex, non-linear function, optimizing such a large number of ISP parameters is challenging. To address this challenge, we propose a novel Sequential ISP parameter optimization model, called the RL-SeqISP model, which utilizes deep reinforcement learning to jointly optimize all ISP parameters for a variety of imaging applications. Concretely, inspired by the sequential tuning process of human experts, the proposed model can progressively enhance image quality by seamlessly integrating information from both the image feature space and the parameter space. Furthermore, a dynamic parameter optimization module is introduced to avoid ISP parameters getting stuck into local optima, which is able to more effectively guarantee the optimal parameters resulting from the sequential learning strategy. These merits of the RL-SeqISP model as well as its high efficiency are substantiated by comprehensive experiments on a wide range of downstream tasks, including two visual analysis tasks (instance segmentation and object detection), and image quality assessment (IQA), as compared with representative methods both quantitatively and qualitatively. In particular, even using only 10% of the training data, our model outperforms other SOTA methods by an average of 7% mAP on two visual analysis tasks.","['CV: Low Level & Physics-based Vision', 'ML: Reinforcement Learning']",[],"['Xinyu Sun', 'Zhikun Zhao', 'Lili Wei', 'Congyan Lang', 'Mingxuan Cai', 'Longfei Han', 'Juan Wang', 'Bing Li', 'Yuxuan Guo']","['Key Laboratory of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer and Information Technology, Beijing Jiaotong University\nInstitute of Automation, Chinese Academy of Sciences', 'Key Laboratory of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer and Information Technology, Beijing Jiaotong University\nInstitute of Automation, Chinese Academy of Sciences', 'Key Laboratory of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer and Information Technology, Beijing Jiaotong University', 'Key Laboratory of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer and Information Technology, Beijing Jiaotong University', 'Shanghai Jiaotong University', 'Beijing Technology and Business University', 'Institute of Automation, Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nPeopleAI Inc. Beijing, China', 'Shenzhen Heytap Technology Corp., Ltd']","['China', 'China', 'China', 'China', 'China', 'China', '', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28319,Security,Once and for All: Universal Transferable Adversarial Perturbation against Deep Hashing-Based Facial Image Retrieval,"Deep Hashing (DH)-based image retrieval has been widely applied to face-matching systems due to its accuracy and efficiency. However, this convenience comes with an increased risk of privacy leakage. DH models inherit the vulnerability to adversarial attacks, which can be used to prevent the retrieval of private images. Existing adversarial attacks against DH typically target a single image or a specific class of images, lacking universal adversarial perturbation for the entire hash dataset. In this paper, we propose the first universal transferable adversarial perturbation against DH-based facial image retrieval, a single perturbation can protect all images. Specifically, we explore the relationship between clusters learned by different DH models and define the optimization objective of universal perturbation as leaving from the overall hash center. To mitigate the challenge of single-objective optimization, we randomly obtain sub-cluster centers and further propose sub-task-based meta-learning to aid in overall optimization. We test our method with popular facial datasets and DH models, indicating impressive cross-image, -identity, -model, and -scheme universal anti-retrieval performance. Compared to state-of-the-art methods, our performance is competitive in white-box settings and exhibits significant improvements of 10%-70% in transferability in all black-box settings.","['CV: Image and Video Retrieval', 'CV: Adversarial Attacks & Robustness']",[],"['Long Tang', 'Dengpan Ye', 'Yunna Lv', 'Chuanxi Chen', 'Yunming Zhang']","['Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education , School of Cyber Science and Engineering ,Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education , School of Cyber Science and Engineering ,Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education , School of Cyber Science and Engineering ,Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education , School of Cyber Science and Engineering ,Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education , School of Cyber Science and Engineering ,Wuhan University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28318,Security,Manifold Constraints for Imperceptible Adversarial Attacks on Point Clouds,"Adversarial attacks on 3D point clouds often exhibit unsatisfactory imperceptibility, which primarily stems from the disregard for manifold-aware distortion, i.e., distortion of the underlying 2-manifold surfaces. In this paper, we develop novel manifold constraints to reduce such distortion, aiming to enhance the imperceptibility of adversarial attacks on 3D point clouds. Specifically, we construct a bijective manifold mapping between point clouds and a simple parameter shape using an invertible auto-encoder. Consequently, manifold-aware distortion during attacks can be captured within the parameter space. By enforcing manifold constraints that preserve local properties of the parameter shape, manifold-aware distortion is effectively mitigated, ultimately leading to enhanced imperceptibility. Extensive experiments demonstrate that integrating manifold constraints into conventional adversarial attack solutions yields superior imperceptibility, outperforming the state-of-the-art methods.","['CV: Adversarial Attacks & Robustness', 'CV: 3D Computer Vision', 'ML: Adversarial Learning & Robustness']",[],"['Keke Tang', 'Xu He', 'Weilong Peng', 'Jianpeng Wu', 'Yawen Shi', 'Daizong Liu', 'Pan Zhou', 'Wenping Wang', 'Zhihong Tian']","['Cyberspace Institute of Advanced Technology, Guangzhou University', 'Cyberspace Institute of Advanced Technology, Guangzhou University', 'School of Computer Science and Cyber Engineering, Guangzhou University', 'Cyberspace Institute of Advanced Technology, Guangzhou University', 'Cyberspace Institute of Advanced Technology, Guangzhou University', 'Wangxuan Institute of Computer Technology, Peking University', 'Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology', 'Department of Computer Science and Engineering, Texas A&M University', 'Cyberspace Institute of Advanced Technology, Guangzhou University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28330,Security,Taxonomy Driven Fast Adversarial Training,"Adversarial training (AT) is an effective defense method against gradient-based attacks to enhance the robustness of neural networks. Among them, single-step AT has emerged as a hotspot topic due to its simplicity and efficiency, requiring only one gradient propagation in generating adversarial examples. Nonetheless, the problem of catastrophic overfitting (CO) that causes training collapse remains poorly understood, and there exists a gap between the robust accuracy achieved through single- and multi-step AT. In this paper, we present a surprising finding that the taxonomy of adversarial examples reveals the truth of CO. Based on this conclusion, we propose taxonomy driven fast adversarial training (TDAT) which jointly optimizes learning objective, loss function, and initialization method, thereby can be regarded as a new paradigm of single-step AT. Compared with other fast AT methods, TDAT can boost the robustness of neural networks, alleviate the influence of misclassified examples, and prevent CO during the training process while requiring almost no additional computational and memory resources. Our method achieves robust accuracy improvement of 1.59%, 1.62%, 0.71%, and 1.26% on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-100 datasets, when against projected gradient descent PGD10 attack with perturbation budget 8/255. Furthermore, our proposed method also achieves state-of-the-art robust accuracy against other attacks. Code is available at https://github.com/bookman233/TDAT.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness', 'CV: Scene Analysis & Understanding']",[],"['Kun Tong', 'Chengze Jiang', 'Jie Gui', 'Yuan Cao']","['Southeast University, Nanjing, China', 'Southeast University, Nanjing, China', 'Southeast University, Nanjing, China\nEngineering Research Center of Blockchain Application, Supervision And Management (Southeast University), Ministry of Education, China\nPurple Mountain Laboratories, China', 'Ocean University of China, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28336,Transparency & Explainability,Integrated Decision Gradients: Compute Your Attributions Where the Model Makes Its Decision,"Attribution algorithms are frequently employed to explain the decisions of neural network models. Integrated Gradients (IG) is an influential attribution method due to its strong axiomatic foundation. The algorithm is based on integrating the gradients along a path from a reference image to the input image. Unfortunately, it can be observed that gradients computed from regions where the output logit changes minimally along the path provide poor explanations for the model decision, which is called the saturation effect problem. In this paper, we propose an attribution algorithm called integrated decision gradients (IDG). The algorithm focuses on integrating gradients from the region of the path where the model makes its decision, i.e., the portion of the path where the output logit rapidly transitions from zero to its final value. This is practically realized by scaling each gradient by the derivative of the output logit with respect to the path. The algorithm thereby provides a principled solution to the saturation problem. Additionally, we minimize the errors within the Riemann sum approximation of the path integral by utilizing non-uniform subdivisions determined by adaptive sampling. In the evaluation on ImageNet, it is demonstrated that IDG outperforms IG, Left-IG, Guided IG, and adversarial gradient integration both qualitatively and quantitatively using standard insertion and deletion metrics across three common models.","['CV: Interpretability', 'Explainability', 'and Transparency', 'ML: Transparent', 'Interpretable', 'Explainable ML', 'PEAI: Accountability', 'Interpretability & Explainability']",[],"['Chase Walker', 'Sumit Jha', 'Kenny Chen', 'Rickard Ewetz']","['University of Central Florida', 'Florida International University', 'Lockheed Martin', 'University of Central Florida']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28346,Transparency & Explainability,B-spine: Learning B-spline Curve Representation for Robust and Interpretable Spinal Curvature Estimation,"Spinal curvature estimation is important to the diagnosis and treatment of the scoliosis. Existing methods face several issues such as the need of expensive annotations on the vertebral landmarks and being sensitive to the image quality. It is challenging to achieve robust estimation and obtain interpretable results, especially for low-quality images which are blurry and hazy. In this paper, we propose B-Spine, a novel deep learning pipeline to learn B-spline curve representation of the spine and estimate the Cobb angles for spinal curvature estimation from low-quality X-ray images. Given a low quality input, a novel SegRefine network which employs the unpaired image-to-image translation is proposed to generate a high quality spine mask from the initial segmentation result. Next, a novel mask-based B-spline prediction model is proposed to predict the B-spline curve for the spine centerline. Finally, the Cobb angles are estimated by a hybrid approach which combines the curve slope analysis and a curve based regression model. We conduct quantitative and qualitative comparisons with the representative and SOTA learning-based methods on the public AASCE2019 dataset and our new proposed JLU-CJUH dataset which contains more challenging low-quality images. The superior performance on both datasets shows our method can achieve both robustness and interpretability for spinal curvature estimation.","['CV: Medical and Biological Imaging', 'CV: Applications']",[],"['Hao Wang', 'Qiang Song', 'Ruofeng Yin', 'Rui Ma']","['School of Artificial Intelligence, Jilin University', 'China-Japan Union Hospital, Jilin University', 'China-Japan Union Hospital, Jilin University', 'School of Artificial Intelligence, Jilin University\nEngineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28350,Transparency & Explainability,Deep Unfolded Network with Intrinsic Supervision for Pan-Sharpening,"Existing deep pan-sharpening methods lack the learning of complementary information between PAN and MS modalities in the intermediate layers, and exhibit low interpretability due to their black-box designs. To this end, an interpretable deep unfolded network with intrinsic supervision for pan-sharpening is proposed. Building upon the observation degradation process, it formulates the pan-sharpening task as a variational model minimization with spatial consistency prior and spectral projection prior. The former prior requires a joint component decomposition of PAN and MS images to extract intrinsic features. By being supervised in the intermediate layers, it can selectively provide high-frequency information for spatial enhancement. The latter prior constrains the intensity correlation between MS and PAN images derived from physical observations, so as to improve spectral fidelity. To further enhance the transparency of network design, we develop an iterative solution algorithm following the half-quadratic splitting to unfold the deep model. It rigorously adheres to the variational model, significantly enhancing the interpretability behind network design and efficiently alternating the optimization of the network. Extensive experiments demonstrate the advantages of our method compared to state-of-the-arts, showcasing its remarkable generalization capability to real-world scenes. Our code is publicly available at https://github.com/Baixuzx7/DISPNet.","['CV: Multi-modal Vision', 'CV: Computational Photography', 'Image & Video Synthesis']",[],"['Hebaixu Wang', 'Meiqi Gong', 'Xiaoguang Mei', 'Hao Zhang', 'Jiayi Ma']","['Wuhan University', 'Wuhan University', 'Wuhan University', 'Wuhan University', 'Wuhan University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28359,Privacy & Data Governance,Multi-Domain Incremental Learning for Face Presentation Attack Detection,"Previous face Presentation Attack Detection (PAD) methods aim to improve the effectiveness of cross-domain tasks. However, in real-world scenarios, the original training data of the pre-trained model is not available due to data privacy or other reasons. Under these constraints, general methods for fine-tuning single-target domain data may lose previously learned knowledge, leading to a catastrophic forgetting problem. To address these issues, we propose a multi-domain incremental learning (MDIL) method for PAD, which not only learns knowledge well from the new domain but also maintains the performance of previous domains stably. Specifically, we propose an adaptive domain-specific experts (ADE) framework based on the vision transformer to preserve the discriminability of previous domains. Furthermore,  an asymmetric classifier is designed to keep the output distribution of different classifiers consistent, thereby improving the generalization ability. Extensive experiments show that our proposed method achieves state-of-the-art performance compared to prior methods of incremental learning. Excitingly, under more stringent setting conditions, our method approximates or even outperforms the DA/DG-based methods.","['CV: Biometrics', 'Face', 'Gesture & Pose']",[],"['Keyao Wang', 'Guosheng Zhang', 'Haixiao Yue', 'Ajian Liu', 'Gang Zhang', 'Haocheng Feng', 'Junyu Han', 'Errui Ding', 'Jingdong Wang']","['Baidu Inc.', 'Baidu Inc.', 'Baidu Inc.', 'University of Chinese Academy of Sciences', 'Baidu Inc.', 'Baidu Inc.', 'Baidu Inc.', 'Baidu Inc.', 'Baidu Inc.']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28365,Security,AGS: Affordable and Generalizable Substitute Training for Transferable Adversarial Attack,"In practical black-box attack scenarios, most of the existing transfer-based attacks employ pretrained models (e.g. ResNet50) as the substitute models. Unfortunately, these substitute models are not always appropriate for transfer-based attacks. Firstly, these models are usually trained on a largescale annotated dataset, which is extremely expensive and time-consuming to construct. Secondly, the primary goal of these models is to perform a specific task, such as image classification, which is not developed for adversarial attacks. To tackle the above issues, i.e., high cost and over-fitting on taskspecific models, we propose an Affordable and Generalizable Substitute (AGS) training framework tailored for transferbased adversarial attack. Specifically, we train the substitute model from scratch by our proposed adversary-centric constrastive learning. This proposed learning mechanism introduces another sample with slight adversarial perturbations as an additional positive view of the input image, and then encourages the adversarial view and two benign views to interact comprehensively with each other. To further boost the generalizability of the substitute model, we propose adversarial invariant learning to maintain the representations of the adversarial example invariants under augmentations with various strengths. Our AGS model can be trained solely with unlabeled and out-of domain data and avoid overfitting to any task-specific models, because of its inherently self-supervised nature. Extensive experiments demonstrate that our AGS achieves comparable or superior performance compared to substitute models pretrained on the complete ImageNet training set, when executing attacks across a diverse range of target models, including ViTs, robustly trained models, object detection and segmentation models. Our source codes are available at https://github.com/lwmming/AGS.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Ruikui Wang', 'Yuanfang Guo', 'Yunhong Wang']","['State Key Laboratory of Software Development Environment, Beihang University, China\nSchool of Computer Science and Engineering, Beihang University, China', 'State Key Laboratory of Software Development Environment, Beihang University, China\nSchool of Computer Science and Engineering, Beihang University, China', 'School of Computer Science and Engineering, Beihang University, China']","['China', 'China', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/28367,Security,Towards Evidential and Class Separable Open Set Object Detection,"Detecting in open-world scenarios poses a formidable challenge for models intended for real-world deployment. The advanced closed set object detectors achieve impressive performance under the closed set setting, but often produce overconfident misprediction on unknown objects due to the lack of supervision. In this paper, we propose a novel Evidential Object Detector (EOD) to formulate the Open Set Object Detection (OSOD) problem from the perspective of Evidential Deep Learning (EDL) theory, which quantifies classification uncertainty by placing the Dirichlet Prior over the categorical distribution parameters. The task-specific customized evidential framework, equipped with meticulously designed model architecture and loss function, effectively bridges the gap between EDL theory and detection tasks. Moreover, we utilize contrastive learning as an implicit means of evidential regularization and to encourage the class separation in the latent space. Alongside, we innovatively model the background uncertainty to further improve the unknown discovery ability. Extensive experiments on benchmark datasets demonstrate the outperformance of the proposed method over existing ones.","['CV: Object Detection & Categorization', 'CV: Adversarial Attacks & Robustness']",[],"['Ruofan Wang', 'Rui-Wei Zhao', 'Xiaobo Zhang', 'Rui Feng']","['School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai', 'Academy for Engineering and Technology, Fudan University, Shanghai', 'Children’s Hospital of Fudan University, National Children’s Medical Center, Shanghai, China', 'School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai\nAcademy for Engineering and Technology, Fudan University, Shanghai\nChildren’s Hospital of Fudan University, National Children’s Medical Center, Shanghai, China\nShanghai Collaborative Innovation Center of Intelligent Visual Computing']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28370,Security,DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving,"Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset includes 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and prediction tasks. Finally, we present a baseline V2X model named V2XFormer that demonstrates superior performance for motion and accident prediction and 3D object detection compared to the single-vehicle model.","['CV: Vision for Robotics & Autonomous Driving', 'CV: 3D Computer Vision', 'CV: Adversarial Attacks & Robustness', 'CV: Motion & Tracking', 'ROB: Multimodal Perception & Sensor Fusion', 'ROB: Multi-Robot Systems']",[],"['Tianqi Wang', 'Sukmin Kim', 'Ji Wenxuan', 'Enze Xie', 'Chongjian Ge', 'Junsong Chen', 'Zhenguo Li', 'Ping Luo']","['The University of Hong Kong', 'The University of Hong Kong', 'The University of Hong Kong', ""Huawei Noah's Ark Lab"", 'The University of Hong Kong', 'Dalian University of Technology', ""Huawei Noah's Ark Lab"", 'The University of Hong Kong']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28373,Transparency & Explainability,Structural Information Guided Multimodal Pre-training for Vehicle-Centric Perception,"Understanding vehicles in images is important for various applications such as intelligent transportation and self-driving system. Existing vehicle-centric works typically pre-train models on large-scale classification datasets and then fine-tune them for specific downstream tasks. However, they neglect the specific characteristics of vehicle perception in different tasks and might thus lead to sub-optimal performance. To address this issue, we propose a novel vehicle-centric pre-training framework called VehicleMAE, which incorporates the structural information including the spatial structure from vehicle profile information and the semantic structure from informative high-level natural language descriptions for effective masked vehicle appearance reconstruction. To be specific, we explicitly extract the sketch lines of vehicles as a form of the spatial structure to guide vehicle reconstruction. The more comprehensive knowledge distilled from the CLIP big model based on the similarity between the paired/unpaired vehicle image-text sample is further taken into consideration to help achieve a better understanding of vehicles. A large-scale dataset is built to pre-train our model, termed Autobot1M, which contains about 1M vehicle images and 12693 text information. Extensive experiments on four vehicle-based downstream tasks fully validated the effectiveness of our VehicleMAE. The source code and pre-trained models will be released at https://github.com/Event-AHU/VehicleMAE.","['CV: Applications', 'CV: Language and Vision', 'CV: Large Vision Models', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Xiao Wang', 'Wentao Wu', 'Chenglong Li', 'Zhicheng Zhao', 'Zhe Chen', 'Yukai Shi', 'Jin Tang']","['Anhui University', 'Anhui University', 'Anhui University', 'Anhui University', 'La Trobe University', 'Guangdong University of Technology', 'Anhui University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28381,Fairness & Bias,Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition,"In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D3GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D3GZSL comprises two core modules: in-distribution dual space distillation (ID2SD) and out-of-distribution batch distillation (O2DBD). ID2SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O2DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and un seen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showcase that D3GZSL elevates the performance of existing generative GZSL methods, under scoring its potential to refine zero-shot learning practices. The code is available at: https://github.com/PJBQ/D3GZSL.git","['CV: Learning & Optimization for CV', 'CV: Representation Learning for Vision', 'ML: Deep Generative Models & Autoencoders']",[],"['Yijie Wang', 'Mingjian Hong', 'Luwen Huangfu', 'Sheng Huang']","['Chongqing University', 'Chongqing University', 'Fowler College of Business, San Diego State University', 'Chongqing University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28383,Fairness & Bias,SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation,"Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structure priors from ego-motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. We show that, the self-cost volume is an effective inductive bias for geometry learning, which implicitly models the single-frame scene geometry, with each slice of it indicating a relative distance map between points and objects in a latent space. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance, and showcases computational efficiency, reduced training complexity, and the ability to recover fine-grained scene details. Moreover, the self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code is available at https://github.com/hisfog/SfMNeXt-Impl.","['CV: Vision for Robotics & Autonomous Driving', 'CV: 3D Computer Vision']",[],"['Youhong Wang', 'Yunji Liang', 'Hao Xu', 'Shaohui Jiao', 'Hongkai Yu']","['Northwestern Polytechnical University\nBytedance', 'Northwestern Polytechnical University', 'Bytedance', 'Bytedance', 'Cleveland State University']","['China', 'China', '', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28390,Security,Out of Thin Air: Exploring Data-Free Adversarial Robustness Distillation,"Adversarial Robustness Distillation (ARD) is a promising task to solve the issue of limited adversarial robustness of small capacity models while optimizing the expensive computational costs of Adversarial Training (AT). Despite the good robust performance, the existing ARD methods are still impractical to deploy in natural high-security scenes due to these methods rely entirely on original or publicly available data with a similar distribution. In fact, these data are almost always private, specific, and distinctive for scenes that require high robustness. To tackle these issues, we propose a challenging but significant task called Data-Free Adversarial Robustness Distillation (DFARD), which aims to train small, easily deployable, robust models without relying on data. We demonstrate that the challenge lies in the lower upper bound of knowledge transfer information, making it crucial to mining and transferring knowledge more efficiently. Inspired by human education, we design a plug-and-play Interactive Temperature Adjustment (ITA) strategy to improve the efficiency of knowledge transfer and propose an Adaptive Generator Balance (AGB) module to retain more data information. Our method uses adaptive hyperparameters to avoid a large number of parameter tuning, which significantly outperforms the combination of existing techniques. Meanwhile, our method achieves stable and reliable performance on multiple benchmarks.","['CV: Applications', 'ML: Applications']",[],"['Yuzheng Wang', 'Zhaoyu Chen', 'Dingkang Yang', 'Pinxue Guo', 'Kaixun Jiang', 'Wenqiang Zhang', 'Lizhe Qi']","['Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28396,Transparency & Explainability,Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting,"Class-agnostic counting (CAC) aims to count objects of interest from a query image given few exemplars. This task is typically addressed by extracting the features of query image and exemplars respectively and then matching their feature similarity, leading to an extract-then-match paradigm. In this work, we show that CAC can be simplified in an extract-and-match manner, particularly using a vision transformer (ViT) where feature extraction and similarity matching are executed simultaneously within the self-attention. We reveal the rationale of such simplification from a decoupled view of the self-attention.The resulting model, termed CACViT, simplifies the CAC pipeline into a single pretrained plain ViT. Further, to compensate the loss of the scale and the order-of-magnitude information due to resizing and normalization in plain ViT, we present two effective strategies for scale and magnitude embedding. Extensive experiments on the FSC147 and the CARPK datasets show that CACViT significantly outperforms state-of-the-art CAC approaches in both effectiveness (23.60% error reduction) and generalization, which suggests CACViT provides a concise and strong baseline for CAC. Code will be available.","['CV: Applications', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Zhicheng Wang', 'Liwen Xiao', 'Zhiguo Cao', 'Hao Lu']","['Huazhong Univ. of Sci.&Tech.', 'Huazhong Univ. of Sci.&Tech.', 'Huazhong Univ. of Sci.&Tech.', 'Huazhong Univ. of Sci.&Tech.']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28403,Fairness & Bias,WeakPCSOD: Overcoming the Bias of Box Annotations for Weakly Supervised Point Cloud Salient Object Detection,"Point cloud salient object detection (PCSOD) is a newly proposed task in 3D dense segmentation. However, the acquisition of accurate 3D dense annotations comes at a high cost, severely limiting the progress of PCSOD. To address this issue, we propose the first weakly supervised PCSOD (named WeakPCSOD) model, which relies solely on cheap 3D bounding box annotations. In WeakPCSOD, we extract noise-free supervision from coarse 3D bounding boxes while mitigating shape biases inherent in box annotations. To achieve this, we introduce a novel mask-to-box (M2B) transformation and a color consistency (CC) loss. The M2B transformation, from a shape perspective, disentangles predictions from labels, enabling the extraction of noiseless supervision from labels while preserving object shapes independently of the box bias. From an appearance perspective, we further introduce the CC loss to provide dense supervision, which mitigates the non-unique predictions stemming from weak supervision and substantially reduces prediction variability. Furthermore, we employ a self-training (ST) strategy to enhance performance by utilizing high-confidence pseudo labels. Notably, the M2B transformation, CC loss, and ST strategy are seamlessly integrated into any model and incur no computational costs for inference. Extensive experiments demonstrate the effectiveness of our WeakPCSOD model, even comparable to fully supervised models utilizing dense annotations.","['CV: 3D Computer Vision', 'CV: Low Level & Physics-based Vision']",[],"['Jun Wei', 'S. Kevin Zhou', 'Shuguang Cui', 'Zhen Li']","['FNii, CUHK-Shenzhen, Shenzhen, China\nSSE, CUHK-Shenzhen, Shenzhen, China', 'School of Biomedical Engineering & Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China\nInstitute of Computing Technology, Chinese Academy of Sciences, Beijing, China', 'SSE, CUHK-Shenzhen, Shenzhen, China\nFNii, CUHK-Shenzhen, Shenzhen, China', 'SSE, CUHK-Shenzhen, Shenzhen, China\nFNii, CUHK-Shenzhen, Shenzhen, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28407,Transparency & Explainability,Factorized Diffusion Autoencoder for Unsupervised Disentangled Representation Learning,"Unsupervised disentangled representation learning aims to recover semantically meaningful factors from real-world data without supervision, which is significant for model generalization and interpretability. Current methods mainly rely on assumptions of independence or informativeness of factors, regardless of interpretability. Intuitively, visually interpretable concepts better align with human-defined factors. However, exploiting visual interpretability as inductive bias is still under-explored. Inspired by the observation that most explanatory image factors can be represented by ``content + mask'', we propose a content-mask factorization network (CMFNet) to decompose an image into different groups of content codes and masks, which are further combined as content masks to represent different visual concepts. To ensure informativeness of the representations, the CMFNet is jointly learned with a generator conditioned on the content masks for reconstructing the input image. The conditional generator employs a diffusion model to leverage its robust distribution modeling capability. Our model is called the Factorized Diffusion Autoencoder (FDAE). To enhance disentanglement of visual concepts, we propose a content decorrelation loss and a mask entropy loss to decorrelate content masks in latent space and spatial space, respectively. Experiments on Shapes3d, MPI3D and Cars3d show that our method achieves advanced performance and can generate visually interpretable concept-specific masks. Source code and supplementary materials are available at https://github.com/wuancong/FDAE.","['CV: Representation Learning for Vision', 'ML: Deep Generative Models & Autoencoders', 'ML: Representation Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Ancong Wu', 'Wei-Shi Zheng']","['Sun Yat-sen University, China', 'Sun Yat-sen University, China\nKey Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\nGuangdong Key Laboratory of Information Security Technology, China']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28406,Transparency & Explainability,Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning,"Explaining predictions of black-box neural networks is crucial when applied to decision-critical tasks. Thus, attribution maps are commonly used to identify important image regions, despite prior work showing that humans prefer explanations based on similar examples. To this end, ProtoPNet learns a set of class-representative feature vectors (prototypes) for case-based reasoning. During inference, similarities of latent features to prototypes are linearly classified to form predictions and attribution maps are provided to explain the similarity. In this work, we evaluate whether architectures for case-based reasoning fulfill established axioms required for faithful explanations using the example of ProtoPNet. We show that such architectures allow the extraction of faithful explanations. However, we prove that the attribution maps used to explain the similarities violate the axioms. We propose a new procedure to extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually, these explanations are Shapley values, calculated on the similarity scores of each prototype. They allow to faithfully answer which prototypes are present in an unseen image and quantify each pixel’s contribution to that presence, thereby complying with all axioms. The theoretical violations of ProtoPNet manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs, RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50, ResNeXt50). Our experiments show a qualitative difference between the explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the explanations with the Area Over the Perturbation Curve, on which ProtoPFaith outperforms ProtoPNet on all experiments by a factor >10^3.","['CV: Interpretability', 'Explainability', 'and Transparency', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Tom Nuno Wolf', 'Fabian Bongratz', 'Anne-Marie Rickmann', 'Sebastian Pölsterl', 'Christian Wachinger']","['Technical University of Munich\nLudwig Maximilians University Munich\nMunich Center for Machine Learning (MCML)', 'Technical University of Munich\nLudwig Maximilians University Munich\nMunich Center for Machine Learning (MCML)', 'Technical University of Munich\nLudwig Maximilians University Munich', 'Ludwig Maximilians University Munich', 'Technical University of Munich\nLudwig Maximilians University Munich\nMunich Center for Machine Learning (MCML)']","['United States', 'United States', '', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28412,Fairness & Bias,Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration,"Contrastive learning has emerged as a prevailing paradigm for high-level vision tasks, which, by introducing properly negative samples, has also been exploited for low-level vision tasks to achieve a compact optimization space to account for their ill-posed nature. However, existing methods rely on manually predefined and task-oriented negatives, which often exhibit pronounced task-specific biases. To address this challenge, our paper introduces an innovative method termed 'learning from history', which dynamically generates negative samples from the target model itself. Our approach, named Model Contrastive Learning for Image Restoration (MCLIR), rejuvenates latency models as negative models, making it compatible with diverse image restoration tasks. We propose the Self-Prior guided Negative loss (SPN) to enable it. This approach significantly enhances existing models when retrained with the proposed model contrastive paradigm. The results show significant improvements in image restoration across various tasks and architectures. For example, models retrained with SPN outperform the original FFANet and DehazeFormer by 3.41 and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly, they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over lightweight SwinIR, respectively. Code and retrained models are available at https://github.com/Aitical/MCLIR.",['CV: Low Level & Physics-based Vision'],[],"['Gang Wu', 'Junjun Jiang', 'Kui Jiang', 'Xianming Liu']","['Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28410,Fairness & Bias,G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection,"In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS.","['CV: Object Detection & Categorization', 'CV: Representation Learning for Vision']",[],"['Fan Wu', 'Jinling Gao', 'Lanqing Hong', 'Xinbing Wang', 'Chenghu Zhou', 'Nanyang Ye']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', ""Huawei Noah's Ark Lab"", 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28426,Transparency & Explainability,Segment beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation,"Augmented Reality (AR) devices, emerging as prominent mobile interaction platforms, face challenges in user safety, particularly concerning oncoming vehicles. While some solutions leverage onboard camera arrays, these cameras often have limited field-of-view (FoV) with front or downward perspectives. Addressing this, we propose a new out-of-view semantic segmentation task and Segment Beyond View (SBV), a novel audio-visual semantic segmentation method. SBV supplements the visual modality, which miss the information beyond FoV, with the auditory information using a teacher-student distillation model (Omni2Ego). The model consists of a vision teacher utilising panoramic information, an auditory teacher with 8-channel audio, and an audio-visual student that takes views with limited FoV and binaural audio as input and produce semantic segmentation for objects outside FoV. SBV outperforms existing models in comparative evaluations and shows a consistent performance across varying FoV ranges and in monaural audio settings.","['CV: Multi-modal Vision', 'CV: Segmentation', 'HAI: Human-Computer Interaction']",[],"['Renjie Wu', 'Hu Wang', 'Feras Dayoub', 'Hsiang-Ting Chen']","['The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide']","['Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/28427,Security,Towards Transferable Adversarial Attacks with Centralized Perturbation,"Adversarial transferability enables black-box attacks on unknown victim deep neural networks (DNNs), rendering attacks viable in real-world scenarios. Current transferable attacks create adversarial perturbation over the entire image, resulting in excessive noise that overfit the source model. Concentrating perturbation to dominant image regions that are model-agnostic is crucial to improving adversarial efficacy. However, limiting perturbation to local regions in the spatial domain proves inadequate in augmenting transferability. To this end, we propose a transferable adversarial attack with fine-grained perturbation optimization in the frequency domain, creating centralized perturbation. We devise a systematic pipeline to dynamically constrain perturbation optimization to dominant frequency coefficients. The constraint is optimized in parallel at each iteration, ensuring the directional alignment of perturbation optimization with model prediction. Our approach allows us to centralize perturbation towards sample-specific important frequency features, which are shared by DNNs, effectively mitigating source model overfitting. Experiments demonstrate that by dynamically centralizing perturbation on dominating frequency coefficients, crafted adversarial examples exhibit stronger transferability, and allowing them to bypass various defenses.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Shangbo Wu', 'Yu-an Tan', 'Yajie Wang', 'Ruinan Ma', 'Wencong Ma', 'Yuanzhang Li']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28430,Security,LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate,"The transferability of adversarial examples is of central importance to transfer-based black-box adversarial attacks. Previous works for generating transferable adversarial examples focus on attacking given pretrained surrogate models while the connections between surrogate models and adversarial trasferability have been overlooked. In this paper, we propose Lipschitz Regularized Surrogate (LRS) for transfer-based black-box attacks, a novel approach that transforms surrogate models towards favorable adversarial transferability. Using such transformed surrogate models, any existing transfer-based black-box attack can run without any change, yet achieving much better performance. Specifically, we impose Lipschitz regularization on the loss landscape of surrogate models to enable a smoother and more controlled optimization process for generating more transferable adversarial examples. In addition, this paper also sheds light on the connection between the inner properties of surrogate models and adversarial transferability, where three factors are identified: smaller local Lipschitz constant, smoother loss landscape, and stronger adversarial robustness. We evaluate our proposed LRS approach by attacking state-of-the-art standard deep neural networks and defense models. The results demonstrate significant improvement on the attack success rates and transferability. Our code is available at https://github.com/TrustAIoT/LRS.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Tao Wu', 'Tie Luo', 'Donald C. Wunsch II']","['Department of Computer Science, Missouri University of Science and Technology', 'Department of Computer Science, Missouri University of Science and Technology', 'Department of Electrical and Computer Engineering, Missouri University of Science and Technology']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28439,Transparency & Explainability,SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents,"Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE). Code is available at: https://github.com/ViviXiang/SocialCVAE.","['CV: Motion & Tracking', 'ROB: Motion and Path Planning', 'MAS: Agent-Based Simulation and Emergent Behavior']",[],"['Wei Xiang', 'Haoteng YIN', 'He Wang', 'Xiaogang Jin']","['Zhejiang University', 'Purdue University', 'University College London', 'Zhejiang University']","['China', 'United States', 'United States', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28444,Transparency & Explainability,Learning by Erasing: Conditional Entropy Based Transferable Out-of-Distribution Detection,"Detecting OOD inputs is crucial to deploy machine learning models to the real world safely. However, existing OOD detection methods require an in-distribution (ID) dataset to retrain the models. In this paper, we propose a Deep Generative Models (DGMs) based transferable OOD detection that does not require retraining on the new ID dataset. We first establish and substantiate two hypotheses on DGMs: DGMs exhibit a predisposition towards acquiring low-level features, in preference to semantic information; the lower bound of DGM's log-likelihoods is tied to the conditional entropy between the model input and target output. Drawing on the aforementioned hypotheses, we present an innovative image-erasing strategy, which is designed to create distinct conditional entropy distributions for each individual ID dataset. By training a DGM on a complex dataset with the proposed image-erasing strategy, the DGM could capture the discrepancy of conditional entropy distribution for varying ID datasets, without re-training. We validate the proposed method on the five datasets and show that, without retraining, our method achieves comparable performance to the state-of-the-art group-based OOD detection methods. The project codes will be open-sourced on our project website.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Adversarial Attacks & Robustness', 'CV: Low Level & Physics-based Vision', 'ML: Representation Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Meng Xing', 'Zhiyong Feng', 'Yong Su', 'Changjae Oh']","['Tianjin University; Queen Mary University of London', 'Tianjin University', 'Tianjin Normal University', 'Queen Mary University of London']","['United Kingdom', 'China', 'China', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28459,Fairness & Bias,Decoupled Contrastive Learning for Long-Tailed Recognition,"Supervised Contrastive Loss (SCL) is popular in visual representation learning.     Given an anchor image, SCL pulls two types of positive samples, i.e., its augmentation and other images from the same class together, while pushes negative images apart to optimize the learned embedding. In the scenario of long-tailed recognition, where the number of samples in each class is imbalanced, treating two types of positive samples equally leads to the biased optimization for intra-category distance. In addition, similarity relationship among negative samples, that are ignored by SCL, also presents meaningful semantic cues. To improve the performance on long-tailed recognition, this paper addresses those two issues of SCL by decoupling the training objective. Specifically, it decouples two types of positives in SCL and optimizes their relations toward different objectives to alleviate the influence of the imbalanced dataset. We further propose a patch-based self distillation to transfer knowledge from head to tail classes to relieve the under-representation of tail classes. It uses patch-based features to mine shared visual patterns among different instances and leverages a self distillation procedure to transfer such knowledge. Experiments on different long-tailed classification benchmarks demonstrate the superiority of our method. For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT dataset. Combined with the ensemble-based method, the performance can be further boosted to 59.7%, which substantially outperforms many recent works. Our code will be released.","['CV: Object Detection & Categorization', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['Shiyu Xuan', 'Shiliang Zhang']","['National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China', 'National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28460,Fairness & Bias,Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks,"Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only. However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL.  Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs. Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency. To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL. Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee. And DGP can also significantly improve communication efficiency with a theoretical analysis of its convergence and generalization. Our extensive experiments show that DGP can effectively defend against the most powerful GIAs and reduce the communication cost without sacrificing the model's utility.","['CV: Bias', 'Fairness & Privacy', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Lulu Xue', 'Shengshan Hu', 'Ruizhi Zhao', 'Leo Yu Zhang', 'Shengqing Hu', 'Lichao Sun', 'Dezhong Yao']","['Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Griffith University', 'Huazhong University of Science and Technology', 'Lehigh University', 'Huazhong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28461,Transparency & Explainability,A Convolutional Neural Network Interpretable Framework for Human Ventral Visual Pathway Representation,"Recently, convolutional neural networks (CNNs) have become the best quantitative encoding models for capturing neural activity and hierarchical structure in the ventral visual pathway. However, the weak interpretability of these black-box models hinders their ability to reveal visual representational encoding mechanisms. Here, we propose a convolutional neural network interpretable framework (CNN-IF) aimed at providing a transparent interpretable encoding model for the ventral visual pathway. First, we adapt the feature-weighted receptive field framework to train two high-performing ventral visual pathway encoding models using large-scale functional Magnetic Resonance Imaging (fMRI) in both goal-driven and data-driven approaches. We find that network layer-wise predictions align with the functional hierarchy of the ventral visual pathway. Then, we correspond feature units to voxel units in the brain and successfully quantify the alignment between voxel responses and visual concepts. Finally, we conduct Network Dissection along the ventral visual pathway including the fusiform face area (FFA), and discover variations related to the visual concept of `person'. Our results demonstrate the CNN-IF provides a new perspective for understanding encoding mechanisms in the human ventral visual pathway, and the combination of ante-hoc interpretable structure and post-hoc interpretable approaches can achieve fine-grained voxel-wise correspondence between model and brain. The source code is available at: https://github.com/BIT-YangLab/CNN-IF.","['CV: Medical and Biological Imaging', 'CV: Applications', 'CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Representation Learning for Vision']",[],"['Mufan Xue', 'Xinyu Wu', 'Jinlong Li', 'Xuesong Li', 'Guoyuan Yang']","['Advanced Research Institute of Multidisciplinary Sciences, Beijing Institute of Technology, Beijing 100081, China', 'Advanced Research Institute of Multidisciplinary Sciences, Beijing Institute of Technology, Beijing 100081, China', 'Advanced Research Institute of Multidisciplinary Sciences, Beijing Institute of Technology, Beijing 100081, China', 'School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China', 'Advanced Research Institute of Multidisciplinary Sciences, Beijing Institute of Technology, Beijing 100081, China\nSchool of Medical Technology, Beijing Institute of Technology, Beijing 100081, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28470,Security,FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks,"Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Hunmin Yang', 'Jongoh Jeong', 'Kuk-Jin Yoon']","['KAIST\nADD', 'KAIST', 'KAIST']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/28471,Fairness & Bias,Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking,"Multi-Object Tracking (MOT) aims to detect and associate all desired objects across frames. Most methods accomplish the task by explicitly or implicitly leveraging strong cues (i.e., spatial and appearance information), which exhibit powerful instance-level discrimination. However, when object occlusion and clustering occur, spatial and appearance information will become ambiguous simultaneously due to the high overlap among objects. In this paper, we demonstrate this long-standing challenge in MOT can be efficiently and effectively resolved by incorporating weak cues to compensate for strong cues. Along with velocity direction, we introduce the confidence and height state as potential weak cues. With superior performance, our method still maintains Simple, Online and Real-Time (SORT) characteristics. Also, our method shows strong generalization for diverse trackers and scenarios in a plug-and-play and training-free manner. Significant and consistent improvements are observed when applying our method to 5 different representative trackers. Further, with both strong and weak cues, our method Hybrid-SORT achieves superior performance on diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where interaction and severe occlusion frequently happen with complex motions. The code and models are available at https://github.com/ymzis69/HybridSORT.",['CV: Motion & Tracking'],[],"['Mingzhan Yang', 'Guangxin Han', 'Bin Yan', 'Wenhua Zhang', 'Jinqing Qi', 'Huchuan Lu', 'Dong Wang']","['Dalian University of Technology\nShenzhen Tvt Digital Technology Co., Ltd', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28488,Security,Mutual-Modality Adversarial Attack with Semantic Perturbation,"Adversarial attacks constitute a notable threat to machine learning systems, given their potential to induce erroneous predictions and classifications. However, within real-world contexts, the essential specifics of the deployed model are frequently treated as a black box, consequently mitigating the vulnerability to such attacks. Thus, enhancing the transferability of the adversarial samples has become a crucial area of research, which heavily relies on selecting appropriate surrogate models. To address this challenge, we propose a novel approach that generates adversarial attacks in a mutual-modality optimization scheme. Our approach is accomplished by leveraging the pre-trained CLIP model. Firstly, we conduct a visual attack on the clean image that causes semantic perturbations on the aligned embedding space with the other textual modality.  Then, we apply the corresponding defense on the textual modality by updating the prompts, which forces the re-matching on the perturbed embedding space.  Finally, to enhance the attack transferability, we utilize the iterative training strategy on the visual attack and the textual defense, where the two processes optimize from each other. We evaluate our approach on several benchmark datasets and demonstrate that our mutual-modal attack strategy can effectively produce high-transferable attacks, which are stable regardless of the target networks. Our approach outperforms state-of-the-art attack methods and can be readily deployed as a plug-and-play solution.","['CV: Adversarial Attacks & Robustness', 'CV: Applications', 'CV: Language and Vision']",[],"['Jingwen Ye', 'Ruonan Yu', 'Songhua Liu', 'Xinchao Wang']","['National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/28494,Fairness & Bias,DiffRAW: Leveraging Diffusion Model to Generate DSLR-Comparable Perceptual Quality sRGB from Smartphone RAW Images,"Deriving DSLR-quality sRGB images from smartphone RAW images has become a compelling challenge due to discernible detail disparity, color mapping instability, and spatial misalignment in RAW-sRGB data pairs. We present DiffRAW, a novel method that incorporates the diffusion model for the first time in learning RAW-to-sRGB mappings. By leveraging the diffusion model, our approach effectively learns the high-quality detail distribution of DSLR images, thereby enhancing the details of output images. Simultaneously, we use the RAW image as a diffusion condition to maintain image structure information such as contours and textures. To mitigate the interference caused by the color and spatial misalignment in training data pairs, we embed a color-position preserving condition within DiffRAW, ensuring that the output images do not exhibit color biases and pixel shift issues. To accelerate the inference process of DiffRAW, we designed the Domain Transform Diffusion Method, an efficient diffusion process with its corresponding reverse process. The Domain Transform Diffusion Method can reduce the required inference steps for diffusion model-based image restoration/enhancement algorithms while enhancing the quality of the generated images. Through evaluations on the ZRR dataset, DiffRAW consistently demonstrates state-of-the-art performance across all perceptual quality metrics (e.g., LPIPS, FID, MUSIQ), while achieving comparable results in PSNR and SSIM.","['CV: Computational Photography', 'Image & Video Synthesis', 'CV: Low Level & Physics-based Vision', 'CV: Large Vision Models', 'CV: Applications']",[],"['Mingxin Yi', 'Kai Zhang', 'Pei Liu', 'Tanli Zuo', 'Jingduo Tian']","['Tsinghua Shenzhen International Graduate School, Tsinghua University, China', 'Tsinghua Shenzhen International Graduate School, Tsinghua University, China\nResearch Institute of Tsinghua, Pearl River Delta', 'Media Technology Lab, Huawei, China', 'Media Technology Lab, Huawei, China', 'Media Technology Lab, Huawei, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28499,Security,VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models,"Visual Question Answering (VQA) is a fundamental task in computer vision and natural language process fields. Although the “pre-training & finetuning” learning paradigm significantly improves the VQA performance, the adversarial robustness of such a learning paradigm has not been explored. In this paper, we delve into a new problem: using a pre-trained multimodal source model to create adversarial image-text pairs and then transferring them to attack the target VQA models. Correspondingly, we propose a novel VQATTACK model, which can iteratively generate both im- age and text perturbations with the designed modules: the large language model (LLM)-enhanced image attack and the cross-modal joint attack module. At each iteration, the LLM-enhanced image attack module first optimizes the latent representation-based loss to generate feature-level image perturbations. Then it incorporates an LLM to further enhance the image perturbations by optimizing the designed masked answer anti-recovery loss. The cross-modal joint attack module will be triggered at a specific iteration, which updates the image and text perturbations sequentially. Notably, the text perturbation updates are based on both the learned gradients in the word embedding space and word synonym-based substitution. Experimental results on two VQA datasets with five validated models demonstrate the effectiveness of the proposed VQATTACK in the transferable attack setting, compared with state-of-the-art baselines. This work reveals a significant blind spot in the “pre-training & fine-tuning” paradigm on VQA tasks. The source code can be found in the link https://github.com/ericyinyzy/VQAttack.","['CV: Adversarial Attacks & Robustness', 'CV: Language and Vision']",[],"['Ziyi Yin', 'Muchao Ye', 'Tianrong Zhang', 'Jiaqi Wang', 'Han Liu', 'Jinghui Chen', 'Ting Wang', 'Fenglong Ma']","['The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsylvania State University', 'Dalian University of Technology', 'The Pennsylvania State University', 'Stony Brook University', 'The Pennsylvania State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28503,Security,Step Vulnerability Guided Mean Fluctuation Adversarial Attack against Conditional Diffusion Models,"The high-quality generation results of conditional diffusion models have brought about concerns regarding privacy and copyright issues. As a possible technique for preventing the abuse of diffusion models, the adversarial attack against diffusion models has attracted academic attention recently. In this work, utilizing the phenomenon that diffusion models are highly sensitive to the mean value of the input noise, we propose the Mean Fluctuation Attack (MFA) to introduce mean fluctuations by shifting the mean values of the estimated noises during the reverse process. In addition, we reveal that the vulnerability of different reverse steps against adversarial attacks actually varies significantly. By modeling the step vulnerability and using it as guidance to sample the target steps for generating adversarial examples, the effectiveness of adversarial attacks can be substantially enhanced. Extensive experiments show that our algorithm can steadily cause the mean shift of the predicted noises so as to disrupt the entire reverse generation process and degrade the generation results significantly. We also demonstrate that the step vulnerability is intrinsic to the reverse process by verifying its effectiveness in an attack method other than MFA. Code and Supplementary is available at https://github.com/yuhongwei22/MFA","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness', 'ML: Deep Generative Models & Autoencoders']",[],"['Hongwei Yu', 'Jiansheng Chen', 'Xinlong Ding', 'Yudong Zhang', 'Ting Tang', 'Huimin Ma']","['University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'Tsinghua University', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28506,Fairness & Bias,DME: Unveiling the Bias for Better Generalized Monocular Depth Estimation,"This paper aims to design monocular depth estimation models with better generalization abilities. To this end, we have conducted quantitative analysis and discovered two important insights. First, the Simulation Correlation phenomenon, commonly seen in long-tailed classification problems, also exists in monocular depth estimation, indicating that the imbalanced depth distribution in training data may be the cause of limited generalization ability. Second, the imbalanced and long-tail distribution of depth values extends beyond the dataset scale, and also manifests within each individual image, further exacerbating the challenge of monocular depth estimation. Motivated by the above findings, we propose the Distance-aware Multi-Expert (DME) depth estimation model. Unlike prior methods that handle different depth range indiscriminately, DME adopts a divide-and-conquer philosophy where each expert is responsible for depth estimation of regions within a specific depth range. As such, the depth distribution seen by each expert is more uniform and can be more easily predicted. A pixel-level routing module is further designed and learned to stitch the prediction of all experts into the final depth map. Experiments show that DME achieves state-of-the-art performance on both NYU-Depth v2 and KITTI, and also delivers favorable zero-shot generalization capability on unseen datasets.","['CV: Scene Analysis & Understanding', 'CV: Vision for Robotics & Autonomous Driving']",[],"['Songsong Yu', 'Yifan Wang', 'Yunzhi Zhuge', 'Lijun Wang', 'Huchuan Lu']","['Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28508,Transparency & Explainability,Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression,"Uncertainty quantification is critical for deploying deep neural networks (DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE) is one of the most effective means to estimate the uncertainty of the main task prediction without modifying the main task model. To be considered robust, an AuxUE must be capable of maintaining its performance and triggering higher uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to provide robust aleatoric and epistemic uncertainty. However, for vision regression tasks, current AuxUE designs are mainly adopted for aleatoric uncertainty estimates, and AuxUE robustness has not been explored. In this work, we propose a generalized AuxUE scheme for more robust uncertainty quantification on regression tasks. Concretely, to achieve a more robust aleatoric uncertainty estimation, different distribution assumptions are considered for heteroscedastic noise, and Laplace distribution is finally chosen to approximate the prediction error. For epistemic uncertainty, we propose a novel solution named Discretization-Induced Dirichlet pOsterior (DIDO), which models the Dirichlet posterior on the discretized prediction error. Extensive experiments on age estimation, monocular depth estimation, and super-resolution tasks show that our proposed method can provide robust uncertainty estimates in the face of noisy inputs and that it can be scalable to both image-level and pixel-wise tasks.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Low Level & Physics-based Vision', 'CV: Representation Learning for Vision', 'CV: Scene Analysis & Understanding']",[],"['Xuanlong Yu', 'Gianni Franchi', 'Jindong Gu', 'Emanuel Aldea']","['SATIE, Paris-Saclay University\nU2IS, ENSTA Paris, Institut Polytechnique de Paris', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris', 'University of Oxford', 'SATIE, Paris-Saclay University']","['France', 'France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/28510,Fairness & Bias,Data-Free Hard-Label Robustness Stealing Attack,"The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.","['CV: Bias', 'Fairness & Privacy', 'ML: Privacy']",[],"['Xiaojian Yuan', 'Kejiang Chen', 'Wen Huang', 'Jie Zhang', 'Weiming Zhang', 'Nenghai Yu']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Nanyang Technological University', 'University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28509,Security,Attacks on Continual Semantic Segmentation by Perturbing Incremental Samples,"As an essential computer vision task, Continual Semantic Segmentation (CSS) has received a lot of attention. However, security issues regarding this task have not been fully studied. To bridge this gap, we study the problem of attacks in CSS in this paper. We first propose a new task, namely, attacks on incremental samples in CSS, and reveal that the attacks on incremental samples corrupt the performance of CSS in both old and new classes.  Moreover, we present an adversarial sample generation method based on class shift, namely Class Shift Attack (CS-Attack), which is an offline and easy-to-implement approach for CSS. CS-Attack is able to significantly degrade the performance of models on both old and new classes without knowledge of the incremental learning approach, which undermines the original purpose of the incremental learning, i.e., learning new classes while retaining old knowledge. Experiments show that on the popular datasets Pascal VOC, ADE20k, and Cityscapes, our approach easily degrades the performance of currently popular CSS methods, which reveals the importance of security in CSS.","['CV: Adversarial Attacks & Robustness', 'CV: Segmentation', 'ML: Life-Long and Continual Learning']",[],"['Zhidong Yu', 'Wei Yang', 'Xike Xie', 'Zhenbo Shi']","['School of Computer Science and Technology, University of Science and Technology of China, Hefei 230026, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei 230026, China\nHefei National Laboratory, Hefei 230088, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei 230026, China\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou 215123, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei 230026, China\nSuzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou 215123, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28514,Fairness & Bias,SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation,"The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to inferior generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM’s pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, which directly generates prompt embeddings from class prototypes and eliminates the use of explicit prompts for improved robustness and a simpler pipeline. In addition, to address the low inter-class variance among surgical instrument categories, we propose contrastive prototype learning, further enhancing the discrimination of the class prototypes for more accurate class prompting. The results of extensive experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that SurgicalSAM achieves state-of-the-art performance while only requiring a small number of tunable parameters. The source code is available at https://github.com/wenxi-yue/SurgicalSAM.","['CV: Large Vision Models', 'CV: Medical and Biological Imaging']",[],"['Wenxi Yue', 'Jing Zhang', 'Kun Hu', 'Yong Xia', 'Jiebo Luo', 'Zhiyong Wang']","['The University of Sydney', 'The University of Sydney', 'The University of Sydney', 'Northwestern Polytechnical University', 'University of Rochester', 'The University of Sydney']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/28516,Fairness & Bias,Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature,"Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods. Our source code is available at https://github.com/wuli55555/ISSF.","['CV: Video Understanding & Activity Analysis', 'CV: Motion & Tracking', 'CV: Segmentation']",[],"['Wulian Yun', 'Mengshi Qi', 'Chuanming Wang', 'Huadong Ma']","['Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia\nBeijing University of Posts and Telecommunications', 'Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia\nBeijing University of Posts and Telecommunications', 'Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia\nBeijing University of Posts and Telecommunications', 'Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia\nBeijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28517,Fairness & Bias,Behavioral Recognition of Skeletal Data Based on Targeted Dual Fusion Strategy,"The deployment of multi-stream fusion strategy on behavioral recognition from skeletal data can extract complementary features from different information streams and improve the recognition accuracy, but suffers from high model complexity and a large number of parameters. Besides, existing multi-stream methods using a fixed adjacency matrix homogenizes the model’s discrimination process across diverse actions, causing reduction of the actual lift for the multi-stream model. Finally, attention mechanisms are commonly applied to the multi-dimensional features, including spatial, temporal and channel dimensions. But their attention scores are typically fused in a concatenated manner, leading to the ignorance of the interrelation between joints in complex actions. To alleviate these issues, the Front-Rear dual Fusion Graph Convolutional Network (FRF-GCN) is proposed to provide a lightweight model based on skeletal data. Targeted adjacency matrices are also designed for different front fusion streams, allowing the model to focus on actions of varying magnitudes. Simultaneously, the mechanism of Spatial-Temporal-Channel Parallel Attention (STC-P), which processes attention in parallel and places greater emphasis on useful information, is proposed to further improve model’s performance. FRF-GCN demonstrates significant competitiveness compared to the current state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120 and Kinetics-Skeleton 400 datasets. Our code is available at: https://github.com/sunbeam-kkt/FRF-GCN-master.","['CV: Video Understanding & Activity Analysis', 'CV: 3D Computer Vision', 'CV: Biometrics', 'Face', 'Gesture & Pose', 'CV: Motion & Tracking']",[],"['Xiao Yun', 'Chenglong Xu', 'Kevin Riou', 'Kaiwen Dong', 'Yanjing Sun', 'Song Li', 'Kevin Subrin', 'Patrick Le Callet']","['China University of Mining and Technology', 'China University of Mining and Technology', 'Nantes Université', 'China University of Mining and Technology', 'China University of Mining and Technology', 'China University of Mining and Technology', 'Nantes Université', 'Nantes Université']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28523,Fairness & Bias,Fine-Grained Knowledge Selection and Restoration for Non-exemplar Class Incremental Learning,"Non-exemplar class incremental learning aims to learn both the new and old tasks without accessing any training data from the past. This strict restriction enlarges the difficulty of alleviating catastrophic forgetting since all techniques can only be applied to current task data. Considering this challenge, we propose a novel framework of fine-grained knowledge selection and restoration. The conventional knowledge distillation-based methods place too strict constraints on the network parameters and features to prevent forgetting, which limits the training of new tasks. To loose this constraint, we proposed a novel fine-grained selective patch-level distillation to adaptively balance plasticity and stability. Some task-agnostic patches can be used to preserve the decision boundary of the old task. While some patches containing the important foreground are favorable for learning the new task.    Moreover, we employ a task-agnostic mechanism to generate more realistic prototypes of old tasks with the current task sample for reducing classifier bias for fine-grained knowledge restoration.  Extensive experiments on CIFAR100, TinyImageNet and ImageNet-Subset demonstrate the effectiveness of our method. Code is available at https://github.com/scok30/vit-cil.","['CV: Learning & Optimization for CV', 'ML: Life-Long and Continual Learning']",[],"['Jiang-Tian Zhai', 'Xialei Liu', 'Lu Yu', 'Ming-Ming Cheng']","['Nankai University', 'Nankai University', 'Tianjin University of Technology', 'Nankai University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28539,Fairness & Bias,HR-Pro: Point-Supervised Temporal Action Localization via Hierarchical Reliability Propagation,"Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HR-Pro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully-supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro.","['CV: Video Understanding & Activity Analysis', 'CV: Learning & Optimization for CV']",[],"['Huaxin Zhang', 'Xiang Wang', 'Xiaohao Xu', 'Zhiwu Qing', 'Changxin Gao', 'Nong Sang']","['Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'University of Michigan, Ann Arbor', 'Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28541,Security,Improving the Adversarial Transferability of Vision Transformers with Virtual Dense Connection,"With the great achievement of vision transformers (ViTs), transformer-based approaches have become the new paradigm for solving various computer vision tasks. However, recent research shows that similar to convolutional neural networks (CNNs), ViTs are still vulnerable to adversarial attacks. To explore the shared deficiency of models with different structures, researchers begin to analyze the cross-structure adversarial transferability, which is still under-explored. Therefore, in this work, we focus on the ViT attacks to improve the cross-structure transferability between the transformer-based and convolution-based models. Previous studies fail to thoroughly investigate the influence of the components inside the ViT models on adversarial transferability, leading to inferior performance. To overcome the drawback, we launch a motivating study by linearly down-scaling the gradients of components inside the ViT models to analyze their influence on adversarial transferability. Based on the motivating study, we find that the gradient of the skip connection most influences transferability and believe that back-propagating gradients from deeper blocks can enhance transferability. Therefore, we propose the Virtual Dense Connection method (VDC). Specifically, without changing the forward pass, we first recompose the original network  to add virtual dense connections. Then we back-propagate gradients of deeper Attention maps and Multi-layer Perceptron (MLP)  blocks via virtual dense connections when generating adversarial samples. Extensive experiments confirm the superiority of our proposed method over the state-of-the-art baselines, with an 8.2% improvement in transferability between ViT models and a 7.2% improvement in cross-structure transferability from ViTs to CNNs.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Jianping Zhang', 'Yizhan Huang', 'Zhuoer Xu', 'Weibin Wu', 'Michael R. Lyu']","['The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'Antgroup', 'Sun Yat-sen University', 'The Chinese University of Hong Kong']","['Hong Kong', 'Hong Kong', '', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28542,Security,Curvature-Invariant Adversarial Attacks for 3D Point Clouds,"Imperceptibility is one of the crucial requirements for adversarial examples. Previous adversarial attacks on 3D point cloud recognition suffer from noticeable outliers, resulting in low imperceptibility. We think that the drawbacks can be alleviated via taking the local curvature of the point cloud into consideration. Existing approaches introduce the local geometry distance into the attack objective function. However, their definition of the local geometry distance neglects different perceptibility of distortions along different directions.  In this paper, we aim to enhance the imperceptibility of adversarial attacks on 3D point cloud recognition by better preserving the local curvature of the original 3D point clouds. To this end, we propose the Curvature-Invariant Method (CIM), which directly regularizes the back-propagated gradient during the generation of adversarial point clouds based on two assumptions. Specifically, we first decompose the back-propagated gradients into the tangent plane and the normal direction. Then we directly reduce the gradient along the large curvature direction on the tangent plane and only keep the gradient along the negative normal direction. Comprehensive experimental comparisons confirm the superiority of our approach. Notably, our strategy can achieve 7.2% and 14.5% improvements in Hausdorff distance and Gaussian curvature measurements of the imperceptibility.","['CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness']",[],"['Jianping Zhang', 'Wenwei Gu', 'Yizhan Huang', 'Zhihan Jiang', 'Weibin Wu', 'Michael R. Lyu']","['The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'Sun Yat-sen University', 'The Chinese University of Hong Kong']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28548,Security,CatmullRom Splines-Based Regression for Image Forgery Localization,"IFL (Image Forgery Location) helps secure digital media forensics. However, many methods suffer from false detections (i.e., FPs) and inaccurate boundaries. In this paper, we proposed the CatmullRom Splines-based Regression Network (CSR-Net), which first rethinks the IFL task from the perspective of regression to deal with this problem. Specifically speaking, we propose an adaptive CutmullRom splines fitting scheme for coarse localization of the tampered regions. Then, for false positive cases, we first develop a novel re-scoring mechanism, which aims to filter out samples that cannot have responses on both the classification branch and the instance branch. Later on, to further restrict the boundaries, we design a learnable texture extraction module, which refines and enhances the contour representation by decoupling the horizontal and vertical forgery features to extract a more robust contour representation, thus suppressing FPs. Compared to segmentation-based methods, our method is simple but effective due to the unnecessity of post-processing. Extensive experiments show the superiority of CSR-Net to existing state-of-the-art methods, not only on standard natural image datasets but also on social media datasets.","['CV: Segmentation', 'APP: Security']",[],"['Li Zhang', 'Mingliang Xu', 'Dong Li', 'Jianming Du', 'Rujing Wang']","['Hefei Institute of Physical Science, Chinese Academy of Sciences, China\nUniversity of Science and Technology of China, China', 'University of Science and Technology of China, China', 'University of Science and Technology of China, China', 'Hefei Institute of Physical Science, Chinese Academy of Sciences, China', 'Hefei Institute of Physical Science, Chinese Academy of Sciences, China\nUniversity of Science and Technology of China, China']","['China', 'China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28561,Fairness & Bias,Exploring Base-Class Suppression with Prior Guidance for Bias-Free One-Shot Object Detection,"One-shot object detection (OSOD) aims to detect all object instances towards the given category specified by a query image. Most existing studies in OSOD endeavor to establish effective cross-image correlation with limited query information, however, ignoring the problems of the model bias towards the base classes and the generalization degradation on the novel classes. Observing this, we propose a novel algorithm, namely Base-class Suppression with Prior Guidance (BSPG) network to achieve bias-free OSOD. Specifically, the objects of base categories can be detected by a base-class predictor and eliminated by a base-class suppression module (BcS). Moreover, a prior guidance module (PG) is designed to calculate the correlation of high-level features in a non-parametric manner, producing a class-agnostic prior map with unbiased semantic information to guide the subsequent detection process. Equipped with the proposed two modules, we endow the model with a strong discriminative ability to distinguish the target objects from distractors belonging to the base classes. Extensive experiments show that our method outperforms the previous techniques by a large margin and achieves new state-of-the-art performance under various evaluation settings.",['CV: Object Detection & Categorization'],[],"['Wenwen Zhang', 'Yun Hu', 'Hangguan Shan', 'Eryun Liu']","['Zhejiang University', 'ShanghaiTech University', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28572,Fairness & Bias,TMFormer: Token Merging Transformer for Brain Tumor Segmentation with Missing Modalities,"Numerous techniques excel in brain tumor segmentation using multi-modal magnetic resonance imaging (MRI) sequences, delivering exceptional results. However, the prevalent absence of modalities in clinical scenarios hampers performance. Current approaches frequently resort to zero maps as substitutes for missing modalities, inadvertently introducing feature bias and redundant computations. To address these issues, we present the Token Merging transFormer (TMFormer) for robust brain tumor segmentation with missing modalities. TMFormer tackles these challenges by extracting and merging accessible modalities into more compact token sequences. The architecture comprises two core components: the Uni-modal Token Merging Block (UMB) and the Multi-modal Token Merging Block (MMB). The UMB enhances individual modality representation by adaptively consolidating spatially redundant tokens within and outside tumor-related regions, thereby refining token sequences for augmented representational capacity. Meanwhile, the MMB mitigates multi-modal feature fusion bias, exclusively leveraging tokens from present modalities and merging them into a unified multi-modal representation to accommodate varying modality combinations. Extensive experimental results on the BraTS 2018 and 2020 datasets demonstrate the superiority and efficacy of TMFormer compared to state-of-the-art methods when dealing with missing modalities.","['CV: Medical and Biological Imaging', 'CV: Segmentation']",[],"['Zheyu Zhang', 'Gang Yang', 'Yueyi Zhang', 'Huanjing Yue', 'Aiping Liu', 'Yunwei Ou', 'Jian Gong', 'Xiaoyan Sun']","['University of Science and Technology of China, Hefei 230026, China', 'University of Science and Technology of China, Hefei 230026, China', 'University of Science and Technology of China, Hefei 230026, China\nHefei Comprehensive National Science Center, Institute of Artificial Intelligence, Hefei 230088, China', 'Tianjin University, Tianjin 300072, China', 'University of Science and Technology of China, Hefei 230026, China', 'Beijing Tiantan Hospital, Capital Medical University, Beijing 100050, China\nHefei Comprehensive National Science Center, Institute of Artificial Intelligence, Hefei 230088, China', 'Beijing Tiantan Hospital, Capital Medical University, Beijing 100050, China\nHefei Comprehensive National Science Center, Institute of Artificial Intelligence, Hefei 230088, China', 'University of Science and Technology of China, Hefei 230026, China\nHefei Comprehensive National Science Center, Institute of Artificial Intelligence, Hefei 230088, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28571,Security,A New Benchmark and Model for Challenging Image Manipulation Detection,"The ability to detect manipulation in multimedia data is vital in digital forensics. Existing Image Manipulation Detection (IMD) methods are mainly based on detecting anomalous features arisen from image editing or double compression artifacts. All existing IMD techniques encounter challenges when it comes to detecting small tampered regions from a large image. Moreover, compression-based IMD approaches face difficulties in cases of double compression of identical quality factors. To investigate the State-of-The-Art (SoTA) IMD methods in those challenging conditions, we introduce a new Challenging Image Manipulation Detection (CIMD) benchmark dataset, which consists of two subsets, for evaluating editing-based and compression-based IMD methods, respectively. The dataset images were manually taken and tampered with high-quality annotations. In addition, we propose a new two-branch network model based on HRNet that can better detect both the image-editing and compression artifacts in those challenging conditions. Extensive experiments on the CIMD benchmark show that our model significantly outperforms SoTA IMD methods on CIMD. The dataset is available at: https://github.com/ZhenfeiZ/CIMD.",['CV: Adversarial Attacks & Robustness'],[],"['Zhenfei Zhang', 'Mingyang Li', 'Ming-Ching Chang']","['University at Albany, SUNY', 'McGill University', 'University at Albany, SUNY']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28573,Fairness & Bias,FaceRSA: RSA-Aware Facial Identity Cryptography Framework,"With the flourishing of the Internet, sharing one's photos or automated processing of faces using computer vision technology has become an everyday occurrence. While enjoying the convenience, the concern for identity privacy is also emerging. Therefore, some efforts introduced the concept of ``password'' from traditional cryptography such as RSA into the face anonymization and deanonymization task to protect the facial identity without compromising the usability of the face image. However, these methods either suffer from the poor visual quality of the synthesis results or do not possess the full cryptographic properties, resulting in compromised security. In this paper, we present the first facial identity cryptography framework with full properties analogous to RSA. Our framework leverages the powerful generative capabilities of StyleGAN to achieve megapixel-level facial identity anonymization and deanonymization. Thanks to the great semantic decoupling of StyleGAN's latent space, the identity encryption and decryption process are performed in latent space by a well-designed password mapper in the manner of editing latent code. Meanwhile, the password-related information is imperceptibly hidden in the edited latent code owing to the redundant nature of the latent space. To make our cryptographic framework possesses all the properties analogous to RSA, we propose three types of loss functions: single anonymization loss, sequential anonymization loss, and associated anonymization loss. Extensive experiments and ablation analyses demonstrate the superiority of our method in terms of the quality of synthesis results, identity-irrelevant attributes preservation, deanonymization accuracy, and completeness of properties analogous to RSA.","['CV: Applications', 'CV: Bias', 'Fairness & Privacy', 'CV: Biometrics', 'Face', 'Gesture & Pose']",[],"['Zhongyi Zhang', 'Tianyi Wei', 'Wenbo Zhou', 'Hanqing Zhao', 'Weiming Zhang', 'Nenghai Yu']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28591,Fairness & Bias,ODTrack: Online Dense Temporal Token Learning for Visual Tracking,"Online contextual reasoning and association across consecutive video frames are critical to perceive instances in visual tracking. However, most current top-performing trackers persistently lean on sparse temporal relationships between reference and search frames via an offline mode. Consequently, they can only interact independently within each image-pair and establish limited temporal correlations. To alleviate the above problem, we propose a simple, flexible and effective video-level tracking pipeline, named ODTrack, which densely associates the contextual relationships of video frames in an online token propagation manner. ODTrack receives video frames of arbitrary length to capture the spatio-temporal trajectory relationships of an instance, and compresses the discrimination features (localization information) of a target into a token sequence to achieve frame-to-frame association. This new solution brings the following benefits: 1) the purified token sequences can serve as prompts for the inference in the next video frame, whereby past information is leveraged to guide future inference; 2) the complex online update strategies are effectively avoided by the iterative propagation of token sequences, and thus we can achieve more efficient model representation and computation. ODTrack achieves a new SOTA performance on seven benchmarks, while running at real-time speed. Code and models are available at https://github.com/GXNU-ZhongLab/ODTrack.",['CV: Motion & Tracking'],[],"['Yaozong Zheng', 'Bineng Zhong', 'Qihua Liang', 'Zhiyi Mo', 'Shengping Zhang', 'Xianxian Li']","['Key Laboratory of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University\nGuangxi Key Lab of Multi-Source Information Mining and Security, Guangxi Normal University', 'Key Laboratory of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University\nGuangxi Key Lab of Multi-Source Information Mining and Security, Guangxi Normal University', 'Key Laboratory of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University\nGuangxi Key Lab of Multi-Source Information Mining and Security, Guangxi Normal University', 'Guangxi Key Laboratory of Machine Vision and Intelligent Control, Wuzhou University', 'Harbin Institute of Technology', 'Key Laboratory of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University\nGuangxi Key Lab of Multi-Source Information Mining and Security, Guangxi Normal University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28602,Fairness & Bias,Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models,"Denoising Diffusion models have exhibited remarkable capabilities in image generation. However, generating high-quality samples requires a large number of iterations. Knowledge distillation for diffusion models is an effective method to address this limitation with a shortened sampling process but causes degraded generative quality. Based on our analysis with bias-variance decomposition and experimental observations, we attribute the degradation to the spatial fitting error occurring in the training of both the teacher and student model in the distillation. Accordingly, we propose Spatial Fitting-Error Reduction Distillation model (SFERD). SFERD utilizes attention guidance from the teacher model and a designed semantic gradient predictor to reduce the student's fitting error. Empirically, our proposed model facilitates high-quality sample generation in a few function evaluations. We achieve an FID of 5.31 on CIFAR-10 and 9.39 on ImageNet 64x64 with only one step, outperforming existing diffusion methods. Our study provides a new perspective on diffusion distillation by highlighting the intrinsic denoising ability of models.","['CV: Computational Photography', 'Image & Video Synthesis', 'ML: Deep Generative Models & Autoencoders', 'ML: Representation Learning']",[],"['Shengzhe Zhou', 'Zejian Li', 'Shengyuan Zhang', 'Lefan Hou', 'Changyuan Yang', 'Guang Yang', 'Zhiyuan Yang', 'Lingyun Sun']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'ZheJiang University', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28607,Transparency & Explainability,Enhance Sketch Recognition’s Explainability via Semantic Component-Level Parsing,"Free-hand sketches are appealing for humans as a universal tool to depict the visual world. Humans can recognize varied sketches of a category easily by identifying the concurrence and layout of the intrinsic semantic components of the category, since humans draw free-hand sketches based a common consensus that which types of semantic components constitute each sketch category. For example, an airplane should at least have a fuselage and wings. Based on this analysis, a semantic component-level memory module is constructed and embedded in the proposed structured sketch recognition network in this paper. The memory keys representing semantic components of each sketch category can be self-learned and enhance the recognition network's explainability. Our proposed networks can deal with different situations of sketch recognition, i.e., with or without semantic components labels of strokes. Experiments on the SPG and SketchIME datasets demonstrate the memory module's flexibility and the recognition network's explainability. The code and data are available at https://github.com/GuangmingZhu/SketchESC.","['CV: Applications', 'CV: Interpretability', 'Explainability', 'and Transparency', 'HAI: Interaction Techniques and Devices']",[],"['Guangming Zhu', 'Siyuan Wang', 'Tianci Wu', 'Liang Zhang']","[""School of Computer Science and Technology, Xidian University, China\nKey Laboratory of Smart Human-Computer Interaction and Wearable Technology of Shaanxi Province\nXi'an Key Laboratory of Intelligent Software Engineering"", 'School of Computer Science and Technology, Xidian University, China', 'School of Computer Science and Technology, Xidian University, China', ""School of Computer Science and Technology, Xidian University, China\nKey Laboratory of Smart Human-Computer Interaction and Wearable Technology of Shaanxi Province\nXi'an Key Laboratory of Intelligent Software Engineering""]","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28605,Fairness & Bias,Intentional Evolutionary Learning for Untrimmed Videos with Long Tail Distribution,"Human intention understanding in untrimmed videos aims to watch a natural video and predict what the person’s intention is. Currently, exploration of predicting human intentions in untrimmed videos is far from enough. On the one hand, untrimmed videos with mixed actions and backgrounds have a significant long-tail distribution with concept drift characteristics. On the other hand, most methods can only perceive instantaneous intentions, but cannot determine the evolution of intentions. To solve the above challenges, we propose a loss based on Instance Confidence and Class Accuracy (ICCA), which aims to alleviate the prediction bias caused by the long-tail distribution with concept drift characteristics in video streams. In addition, we propose an intention-oriented evolutionary learning method to determine the intention evolution pattern (from what action to what action) and the time of evolution (when the action evolves). We conducted extensive experiments on two untrimmed video datasets (THUMOS14 and ActivityNET v1.3), and our method has achieved excellent results compared to SOTA methods. The code and supplementary materials are available at https://github.com/Jennifer123www/UntrimmedVideo.","['CV: Video Understanding & Activity Analysis', 'APP: Other Applications', 'CV: 3D Computer Vision', 'CV: Applications', 'DMKM: Mining of Visual', 'Multimedia & Multimodal Data', 'HAI: Applications', 'HAI: Human-Aware Planning and Behavior Prediction', 'HAI: Human-Computer Interaction', 'ML: Applications', 'ML: Deep Neural Architectures and Foundation Models']",[],"['Yuxi Zhou', 'Xiujie Wang', 'Jianhua Zhang', 'Jiajia Wang', 'Jie Yu', 'Hao Zhou', 'Yi Gao', 'Shengyong Chen']","['Tianjin University of Technology\nTsinghua University', 'Tianjin University of Technology', 'Tianjin University of Technology', 'Tianjin University of Technology', 'Tianjin University of Technology', 'Tianjin University of Technology', 'Tianjin University of Technology', 'Tianjin University of Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28613,Security,Rethinking Mesh Watermark: Towards Highly Robust and Adaptable Deep 3D Mesh Watermarking,"The goal of 3D mesh watermarking is to embed the message in 3D meshes that can withstand various attacks imperceptibly and reconstruct the message accurately from watermarked meshes. The watermarking algorithm is supposed to withstand multiple attacks, and the complexity should not grow significantly with the mesh size. Unfortunately, previous methods are less robust against attacks and lack of adaptability. In this paper, we propose a robust and adaptable deep 3D mesh watermarking Deep3DMark that leverages attention-based convolutions in watermarking tasks to embed binary messages in vertex distributions without texture assistance. Furthermore, our Deep3DMark exploits the property that simplified meshes inherit similar relations from the original ones, where the relation is the offset vector directed from one vertex to its neighbor. By doing so, our method can be trained on simplified meshes but remains effective on large size meshes (size adaptable) and unseen categories of meshes (geometry adaptable). Extensive experiments demonstrate our method remains efficient and effective even if the mesh size is 190× increased. Under mesh attacks, Deep3DMark achieves 10%∼50% higher accuracy than traditional methods, and 2× higher SNR and 8% higher accuracy than previous DNN-based methods.","['CV: Adversarial Attacks & Robustness', 'CV: Applications']",[],"['Xingyu Zhu', 'Guanhui Ye', 'Xiapu Luo', 'Xuetao Wei']","['Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen 518055, China\nDepartment of Computer Science and Engineering, Southern University of Science and Technology, China\nDepartment of Computing, Hong Kong Polytechnic University, Hong Kong', 'Department of Computer Science and Engineering, Southern University of Science and Technology, China', 'Department of Computing, Hong Kong Polytechnic University, Hong Kong', 'Department of Computer Science and Engineering, Southern University of Science and Technology, China\nResearch Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen 518055, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28611,Security,SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly,"This paper proposes SEER, a novel backdoor detection algorithm for vision-language models, addressing the gap in the literature on multi-modal backdoor detection. While backdoor detection in single-modal models has been well studied, the investigation of such defenses in multi-modal models remains limited. Existing backdoor defense mechanisms cannot be directly applied to multi-modal settings due to their increased complexity and search space explosion. In this paper, we propose to detect backdoors in vision-language models by jointly searching image triggers and malicious target texts in feature space shared by vision and language modalities. Our extensive experiments demonstrate that SEER can achieve over 92% detection rate on backdoor detection in vision-language models in various settings without accessing training data or knowledge of downstream tasks.","['CV: Language and Vision', 'CV: Adversarial Attacks & Robustness']",[],"['Liuwan Zhu', 'Rui Ning', 'Jiang Li', 'Chunsheng Xin', 'Hongyi Wu']","['University of Hawaii at Manoa', 'Old Dominion University', 'Old Dominion University', 'Old Dominion University', 'Univesity of Arizona']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28619,Security,IPRemover: A Generative Model Inversion Attack against Deep Neural Network Fingerprinting and Watermarking,"Training Deep Neural Networks (DNNs) can be expensive when data is difficult to obtain or labeling them requires significant domain expertise. Hence, it is crucial that the Intellectual Property (IP) of DNNs trained on valuable data be protected against IP infringement. DNN fingerprinting and watermarking are two lines of work in DNN IP protection. Recently proposed DNN fingerprinting techniques are able to detect IP infringement while preserving model performance by relying on the key assumption that the decision boundaries of independently trained models are intrinsically different from one another. In contrast, DNN watermarking embeds a watermark in a model and verifies IP infringement if an identical or similar watermark is extracted from a suspect model. The techniques deployed in fingerprinting and watermarking vary significantly because their underlying mechanisms are different. From an adversary's perspective, a successful IP removal attack should defeat both fingerprinting and watermarking. However, to the best of our knowledge, there is no work on such attacks in the literature yet. In this paper, we fill this gap by presenting an IP removal attack that can defeat both fingerprinting and watermarking. We consider the challenging data-free scenario whereby all data is inverted from the victim model. Under this setting, a stolen model only depends on the victim model. Experimental results demonstrate the success of our attack in defeating state-of-the-art DNN fingerprinting and watermarking techniques. This work reveals a novel attack surface that exploits generative model inversion attacks to bypass DNN IP defenses. This threat must be addressed by future defenses for reliable IP protection.","['CV: Adversarial Attacks & Robustness', 'CV: Computational Photography', 'Image & Video Synthesis', 'CV: Object Detection & Categorization']",[],"['Wei Zong', 'Yang-Wai Chow', 'Willy Susilo', 'Joonsang  Baek', 'Jongkil Kim', 'Seyit Camtepe']","['University of Wollongong', 'University of Wollongong', 'University of Wollongong', 'University of Wollongong', 'Ewha Womans University', 'CSIRO Data61']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/28641,Fairness & Bias,Composing Biases by Using CP to Decompose Minimal Functional Dependencies for Acquiring Complex Formulae,"Given a table with a minimal set of input columns that functionally determines an output column, we introduce a method that tries to gradually decompose the corresponding minimal functional dependency (mfd) to acquire a formula expressing the output column in terms of the input columns. A first key element of the method is to create sub-problems that are easier to solve than the original formula acquisition problem, either because it learns formulae with fewer inputs parameters, or as it focuses on formulae of a particular class, such as Boolean formulae; as a result, the acquired formulae can mix different learning biases such as polynomials, conditionals or Boolean expressions. A second key feature of the method is that it can be applied recursively to find formulae that combine polynomial, conditional or Boolean sub-terms in a nested manner. The method was tested on data for eight families of combinatorial objects; new conjectures were found that were previously unattainable. The method often creates conjectures that combine several formulae into one with a limited number of automatically found Boolean terms.",['CSO: Constraint Learning and Acquisition'],[],"['Ramiz Gindullin', 'Nicolas Beldiceanu', 'Jovial Cheukam-Ngouonou', 'Rémi Douence', 'Claude-Guy Quimper']","['IMT Atlantique, Nantes, France\nLS2N, Nantes, France', 'IMT Atlantique, Nantes, France\nLS2N, Nantes, France', 'IMT Atlantique, Nantes, France\nLS2N, Nantes, France\nUniversité Laval, Quebec City, Canada', 'IMT Atlantique, Nantes, France\nLS2N, Nantes, France\nINRIA, Nantes, France', 'Université Laval, Quebec City, Canada']","['France', 'France', 'France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/28655,Fairness & Bias,Learning to Learn in Interactive Constraint Acquisition,"Constraint Programming (CP) has been successfully used to model and solve complex combinatorial problems. However, modeling is often not trivial and requires expertise, which is a bottleneck to wider adoption. In Constraint Acquisition (CA), the goal is to assist the user by automatically learning the model. In (inter)active CA, this is done by interactively posting queries to the user, e.g. does this partial solution satisfy your (unspecified) constraints or not. While interactive CA methods learn the constraints, the learning is related to symbolic concept learning, as the goal is to learn an exact representation.  However, a large number of queries is required to learn the model, which is a major limitation. In this paper, we aim to alleviate this limitation by tightening the connection of CA and Machine Learning (ML), by, for the first time in interactive CA, exploiting statistical ML methods. We propose to use probabilistic classification models to guide interactive CA queries to the most promising parts. We discuss how to train classifiers to predict whether a candidate expression from the bias is a constraint of the problem or not, using both relation-based and scope-based features. We then show how the predictions can be used in all layers of interactive CA: the query generation, the scope finding, and the lowest-level constraint finding. We experimentally evaluate our proposed methods using different classifiers and show that our methods greatly outperform the state of the art, decreasing the number of queries needed to converge by up to 72%.","['CSO: Constraint Learning and Acquisition', 'CSO: Constraint Optimization', 'CSO: Constraint Programming', 'CSO: Constraint Satisfaction']",[],"['Dimosthenis Tsouros', 'Senne Berden', 'Tias Guns']","['KU Leuven', 'KU Leuven', 'KU Leuven']","['Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/28665,Fairness & Bias,RR-PU: A Synergistic Two-Stage Positive and Unlabeled Learning Framework for Robust Tax Evasion Detection,"Tax evasion, an unlawful practice in which taxpayers deliberately conceal information to avoid paying tax liabilities, poses significant challenges for tax authorities. Effective tax evasion detection is critical for assisting tax authorities in mitigating tax revenue loss. Recently, machine-learning-based methods, particularly those employing positive and unlabeled (PU) learning, have been adopted for tax evasion detection, achieving notable success. However, these methods exhibit two major practical limitations. First, their success heavily relies on the strong assumption that the label frequency (the fraction of identified taxpayers among tax evaders) is known in advance. Second, although some methods attempt to estimate label frequency using approaches like Mixture Proportion Estimation (MPE) without making any assumptions, they subsequently construct a classifier based on the error-prone label frequency obtained from the previous estimation. This two-stage approach may not be optimal, as it neglects error accumulation in classifier training resulting from the estimation bias in the first stage. To address these limitations, we propose a novel PU learning-based tax evasion detection framework called RR-PU, which can revise the bias in a two-stage synergistic manner. Specifically, RR-PU refines the label frequency initialization by leveraging a regrouping technique to fortify the MPE perspective. Subsequently, we integrate a trainable slack variable to fine-tune the initial label frequency, concurrently optimizing this variable and the classifier to eliminate latent bias in the initial stage. Experimental results on three real-world tax datasets demonstrate that RR-PU outperforms state-of-the-art methods in tax evasion detection tasks.","['DMKM: Anomaly/Outlier Detection', 'ML: Semi-Supervised Learning']",[],"['Shuzhi Cao', 'Jianfei Ruan', 'Bo Dong', 'Bin Shi', 'Qinghua Zheng']","[""School of Computer Science and Technology, Xi'an Jiaotong University, China\nShaanxi Provincial Key Laboratory of Big Data Knowledge Engineering, Xi'an Jiaotong University, China"", ""School of Computer Science and Technology, Xi'an Jiaotong University, China\nShaanxi Provincial Key Laboratory of Big Data Knowledge Engineering, Xi'an Jiaotong University, China"", ""Shaanxi Provincial Key Laboratory of Big Data Knowledge Engineering, Xi'an Jiaotong University, China\nSchool of Distance Education, Xi'an Jiaotong University, China"", ""School of Computer Science and Technology, Xi'an Jiaotong University, China\nShaanxi Provincial Key Laboratory of Big Data Knowledge Engineering, Xi'an Jiaotong University, China"", ""School of Computer Science and Technology, Xi'an Jiaotong University, China\nShaanxi Provincial Key Laboratory of Big Data Knowledge Engineering, Xi'an Jiaotong University, China""]","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28667,Fairness & Bias,Distributional Off-Policy Evaluation for Slate Recommendations,"Recommendation strategies are typically evaluated by using previously logged data, employing off-policy evaluation methods to estimate their expected performance. However, for strategies that present users with slates of multiple items, the resulting combinatorial action space renders many of these methods impractical. Prior work has developed estimators that leverage the structure in slates to estimate the expected off-policy performance, but the estimation of the entire performance distribution remains elusive. Estimating the complete distribution allows for a more comprehensive evaluation of recommendation strategies, particularly along the axes of risk and fairness that employ metrics computable from the distribution. In this paper, we propose an estimator for the complete off-policy performance distribution for slates and establish conditions under which the estimator is unbiased and consistent. This builds upon prior work on off-policy evaluation for slates and off-policy distribution estimation in reinforcement learning. We validate the efficacy of our method empirically on synthetic data as well as on a slate recommendation simulator constructed from real-world data (MovieLens-20M). Our results show a significant reduction in estimation variance and improved sample efficiency over prior work across a range of slate structures.","['DMKM: Recommender Systems', 'ML: Evaluation and Analysis', 'ML: Online Learning & Bandits', 'RU: Sequential Decision Making']",[],"['Shreyas Chaudhari', 'David Arbour', 'Georgios Theocharous', 'Nikos Vlassis']","['University of Massachusetts Amherst', 'Adobe Research', 'Adobe Research', 'Adobe Research']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28671,Transparency & Explainability,Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks,"The classic problem of node importance estimation has been conventionally studied with homogeneous network topology analysis. To deal with practical network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information. However, the major concern revolves around that their fully adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework namely SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Then based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its informativeness disparity against the reference. This establishes an interpretable node importance computation paradigm. Furthermore, SKES dives deep into the understanding that ""nodes with similar characteristics are prone to have similar importance values"" whilst guaranteeing that such informativeness disparity between any different nodes is orderly reflected by the embedding distance of their associated latent features. Extensive experiments on three widely-evaluated benchmarks demonstrate the performance superiority of SKES over several recent competing methods.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'DMKM: Applications', 'DMKM: Other Foundations of Data Mining & Knowledge Management', 'ML: Applications', 'ML: Deep Learning Algorithms']",[],"['Yankai Chen', 'Yixiang Fang', 'Qiongyan Wang', 'Xin Cao', 'Irwin King']","['The Chinese University of Hong Kong', 'The Chinese University of Hong Kong, Shenzhen', 'University of Copenhagen', 'University of New South Wales', 'The Chinese University of Hong Kong']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28676,Fairness & Bias,Discovering Sequential Patterns with Predictable Inter-event Delays,"Summarizing sequential data with serial episodes allows non-trivial insight into the data generating process. Existing methods penalize gaps in pattern occurrences equally, regardless of where in the pattern these occur. This results in a strong bias against patterns with long inter-event delays, and in addition that regularity in terms of delays is not rewarded or discovered---even though both aspects provide key insight. In this paper we tackle both these problems by explicitly modeling inter-event delay distributions. That is, we are not only interested in discovering the patterns, but also in describing how many times steps typically occur between their individual events. We formalize the problem in terms of the Minimum Description Length principle, by which we say the best set of patterns is the one that compresses the data best. The resulting optimization problem does not lend itself to exact optimization, and hence we propose Hopper to heuristically mine high quality patterns. Extensive experiments show that Hopper efficiently recovers the ground truth, discovers meaningful patterns from real-world data, and outperforms existing methods in discovering long-delay patterns.","['DMKM: Rule Mining & Pattern Mining', 'ML: Time-Series/Data Streams']",[],"['Joscha Cüppers', 'Paul Krieger', 'Jilles Vreeken']","['CISPA Helmholtz Center for Information Security', 'Saarland University', 'CISPA Helmholtz Center for Information Security']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28677,Transparency & Explainability,Unveiling Implicit Deceptive Patterns in Multi-Modal Fake News via Neuro-Symbolic Reasoning,"In the current Internet landscape, the rampant spread of fake news, particularly in the form of multi-modal content, poses a great social threat. While automatic multi-modal fake news detection methods have shown promising results, the lack of explainability remains a significant challenge. Existing approaches provide superficial explainability by displaying learned important components or views from well-trained networks, but they often fail to uncover the implicit deceptive patterns that reveal how fake news is fabricated.  To address this limitation, we begin by predefining three typical deceptive patterns, namely image manipulation, cross-modal inconsistency, and image repurposing, which shed light on the mechanisms underlying fake news fabrication. Then, we propose a novel Neuro-Symbolic Latent Model called NSLM, that not only derives accurate judgments on the veracity of news but also uncovers the implicit deceptive patterns as explanations. Specifically, the existence of each deceptive pattern is expressed as a two-valued learnable latent variable, which is acquired through amortized variational inference and weak supervision based on symbolic logic rules.  Additionally, we devise pseudo-siamese networks to capture distinct deceptive patterns effectively. Experimental results on two real-world datasets demonstrate that our NSLM achieves the best performance in fake news detection while providing insightful explanations of deceptive patterns.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'APP: Misinformation & Fake News', 'ML: Graph-based Machine Learning', 'ML: Neuro-Symbolic Learning']",[],"['Yiqi Dong', 'Dongxiao He', 'Xiaobao Wang', 'Youzhu Jin', 'Meng Ge', 'Carl Yang', 'Di Jin']","['School of New Media and Communication, Tianjin University, Tianjin, China', 'School of New Media and Communication, Tianjin University, Tianjin, China\nTianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin, China', 'Beijing-Dublin International College, Beijing University of Technology, Beijing, China', 'Saw Swee Hock School of Public Health, National University of Singapore, Singapore', 'Department of Computer Science, Emory University, Georgia, USA', 'School of New Media and Communication, Tianjin University, Tianjin, China\nTianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28689,Fairness & Bias,Intra- and Inter-group Optimal Transport for User-Oriented Fairness in Recommender Systems,"Recommender systems are typically biased toward a small group of users, leading to severe unfairness in recommendation performance, i.e., User-Oriented Fairness (UOF) issue. Existing research on UOF exhibits notable limitations in two phases of recommendation models. In the training phase, current methods fail to tackle the root cause of the UOF issue, which lies in the unfair training process between advantaged and disadvantaged users. In the evaluation phase, the current UOF metric lacks the ability to comprehensively evaluate varying cases of unfairness. In this paper, we aim to address the aforementioned limitations and ensure recommendation models treat user groups of varying activity levels equally. In the training phase, we propose a novel Intra- and Inter-GrOup Optimal Transport framework (II-GOOT) to alleviate the data sparsity problem for disadvantaged users and narrow the training gap between advantaged and disadvantaged users. In the evaluation phase, we introduce a novel metric called ?-UOF, which enables the identification and assessment of various cases of UOF. This helps prevent recommendation models from leading to unfavorable fairness outcomes, where both advantaged and disadvantaged users experience subpar recommendation performance. We conduct extensive experiments on three real-world datasets based on four backbone recommendation models to prove the effectiveness of ?-UOF and the efficiency of our proposed II-GOOT.","['DMKM: Conversational Systems for Recommendation & Retrieval', 'PEAI: Bias', 'Fairness & Equity']",[],"['Zhongxuan Han', 'Chaochao Chen', 'Xiaolin Zheng', 'Meng Li', 'Weiming Liu', 'Binhui Yao', 'Yuyuan Li', 'Jianwei Yin']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Harbin Institute of Technology (Shenzhen)', 'Zhejiang university', 'Midea', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', '', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28692,Fairness & Bias,ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis,"The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos. However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques. State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos. To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights. Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias. A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph. Experiments demonstrate that our method outperforms existing models by a significant margin. Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies. More details are available at: https://ViSTec2024.github.io/.","['DMKM: Applications', 'CV: Video Understanding & Activity Analysis', 'DMKM: Mining of Visual', 'Multimedia & Multimodal Data']",[],"['Yuchen He', 'Zeqing Yuan', 'Yihong Wu', 'Liqi Cheng', 'Dazhen Deng', 'Yingcai Wu']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28691,Security,ADA-GAD: Anomaly-Denoised Autoencoders for Graph Anomaly Detection,"Graph anomaly detection is crucial for identifying nodes that deviate from regular behavior within graphs, benefiting various domains such as fraud detection and social network. Although existing reconstruction-based methods have achieved considerable success, they may face the Anomaly Overfitting and Homophily Trap problems caused by the abnormal patterns in the graph, breaking the assumption that normal nodes are often better reconstructed than abnormal ones. Our observations indicate that models trained on graphs with fewer anomalies exhibit higher detection performance. Based on this insight, we introduce a novel two-stage framework called Anomaly-Denoised Autoencoders for Graph Anomaly Detection (ADA-GAD). In the first stage, we design a learning-free anomaly-denoised augmentation method to generate graphs with reduced anomaly levels. We pretrain graph autoencoders on these augmented graphs at multiple levels, which enables the graph autoencoders to capture normal patterns. In the next stage, the decoders are retrained for detection on the original graph, benefiting from the multi-level representations learned in the previous stage. Meanwhile, we propose the node anomaly distribution regularization to further alleviate Anomaly Overfitting. We validate the effectiveness of our approach through extensive experiments on both synthetic and real-world datasets.","['DMKM: Anomaly/Outlier Detection', 'DMKM: Graph Mining', 'Social Network Analysis & Community']",[],"['Junwei He', 'Qianqian Xu', 'Yangbangyan Jiang', 'Zitai Wang', 'Qingming Huang']","['Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences\nSchool of Computer Science and Technology, University of Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences', 'School of Computer Science and Technology, University of Chinese Academy of Sciences', 'Institute of Information Engineering, Chinese Academy of Sciences\nSchool of Cyber Security, University of Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences\nSchool of Computer Science and Technology, University of Chinese Academy of Sciences\nKey Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28701,Security,Enhancing Multi-Scale Diffusion Prediction via Sequential Hypergraphs and Adversarial Learning,"Information diffusion prediction plays a crucial role in understanding the propagation of information in social networks, encompassing both macroscopic and microscopic prediction tasks. Macroscopic prediction estimates the overall impact of information diffusion, while microscopic prediction focuses on identifying the next user to be influenced. While prior research often concentrates on one of these aspects, a few tackle both concurrently. These two tasks provide complementary insights into the diffusion process at different levels, revealing common traits and unique attributes. The exploration of leveraging common features across these tasks to enhance information prediction remains an underexplored avenue. In this paper, we propose an intuitive and effective model that addresses both macroscopic and microscopic prediction tasks. Our approach considers the interactions and dynamics among cascades at the macro level and incorporates the social homophily of users in social networks at the micro level. Additionally, we introduce adversarial training and orthogonality constraints to ensure the integrity of shared features. Experimental results on four datasets demonstrate that our model significantly outperforms state-of-the-art methods.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'APP: Social Networks']",[],"['Pengfei Jiao', 'Hongqian Chen', 'Qing Bao', 'Wang Zhang', 'Huaming Wu']","['Hangzhou Dianzi University', 'Hangzhou Dianzi University', 'Hangzhou Dianzi University', 'Tianjin University', 'Tianjin University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28708,Transparency & Explainability,Knowledge-Aware Explainable Reciprocal Recommendation,"Reciprocal recommender systems (RRS) have been widely used in online platforms such as online dating and recruitment. They can simultaneously fulfill the needs of both parties involved in the recommendation process. Due to the inherent nature of the task, interaction data is relatively sparse compared to other recommendation tasks. Existing works mainly address this issue through content-based recommendation methods. However, these methods often implicitly model textual information from a unified perspective, making it challenging to capture the distinct intentions held by each party, which further leads to limited performance and the lack of interpretability. In this paper, we propose a Knowledge-Aware Explainable Reciprocal Recommender System (KAERR), which models metapaths between two parties independently, considering their respective perspectives and requirements. Various metapaths are fused using an attention-based mechanism, where the attention weights unveil dual-perspective preferences and provide recommendation explanations for both parties. Extensive experiments on two real-world datasets from diverse scenarios demonstrate that the proposed model outperforms state-of-the-art baselines, while also delivering compelling reasons for recommendations to both parties.",['DMKM: Recommender Systems'],[],"['Kai-Huang Lai', 'Zhe-Rui Yang', 'Pei-Yuan Lai', 'Chang-Dong Wang', 'Mohsen  Guizani', 'Min Chen']","['Sun Yat-sen University', 'Sun Yat-sen University', 'South China Technology Commercialization Center', 'Sun Yat-sen University', 'Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)', 'South China University of Technology\nPazhou Lab']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28711,Transparency & Explainability,CoreRec: A Counterfactual Correlation Inference for Next Set Recommendation,"Next set recommendation aims to predict the items that are likely to be bought in the next purchase. Central to this endeavor is the task of capturing intra-set and cross-set correlations among items. However, the modeling of cross-set correlations poses challenges due to specific issues. Primarily, these correlations are often implicit, and the prevailing approach of establishing an indiscriminate link across the entire set of objects neglects factors like purchase frequency and correlations between purchased items. Such hastily formed connections across sets introduce substantial noise. Additionally, the preeminence of high-frequency items in numerous sets could potentially overshadow and distort correlation modeling with respect to low-frequency items. Thus, we devoted to mitigating misleading inter-set correlations. With a fresh perspective rooted in causality, we delve into the question of whether correlations between a particular item and items from other sets should be relied upon for item representation learning and set prediction. Technically, we introduce the Counterfactual Correlation Inference framework for next set recommendation, denoted as CoreRec. This framework establishes a counterfactual scenario in which the recommendation model impedes cross-set correlations to generate intervened predictions. By contrasting these intervened predictions with the original ones, we gauge the causal impact of inter-set neighbors on set prediction—essentially assessing whether they contribute to spurious correlations. During testing, we introduce a post-trained switch module that selects between set-aware item representations derived from either the original or the counterfactual scenarios. To validate our approach, we extensively experiment using three real-world datasets, affirming both the effectiveness of CoreRec and the cogency of our analytical approach.",['DMKM: Recommender Systems'],[],"['Kexin Li', 'Chengjiang Long', 'Shengyu Zhang', 'Xudong Tang', 'Zhichao Zhai', 'Kun Kuang', 'Jun Xiao']","['Zhejiang University', 'Meta Reality Labs', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28716,Security,A Generalized Neural Diffusion Framework on Graphs,"Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually appear monophily property, which induces the similarity based on labels among high-order neighbors without requiring the similarity among first-order neighbors. This discovery motives to design a new high-order neighbor-aware diffusion equation, and derive a new type of graph diffusion network (HiD-Net) based on the framework. With the high-order diffusion equation, HiD-Net is more robust against attacks and works on both homophily and heterophily graphs. We not only theoretically analyze the relation between HiD-Net with high-order random walk, but also provide a theoretical convergence guarantee. Extensive experimental results well demonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusion networks.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Graph-based Machine Learning']",[],"['Yibo Li', 'Xiao Wang', 'Hongrui Liu', 'Chuan Shi']","['Beijing University of Posts and Telecommunications', 'Beihang University', 'Ant Group', 'Beijing University of Posts and Telecommunications']","['China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28719,Security,Hawkes-Enhanced Spatial-Temporal Hypergraph Contrastive Learning Based on Criminal Correlations,"Crime prediction is a crucial yet challenging task within urban computing, which benefits public safety and resource optimization. Over the years, various models have been proposed, and spatial-temporal hypergraph learning models have recently shown outstanding performances. However, three correlations underlying crime are ignored, thus hindering the performance of previous models. Specifically, there are two spatial correlations and one temporal correlation, i.e., (1) co-occurrence of different types of crimes (type spatial correlation), (2) the closer to the crime center, the more dangerous it is around the neighborhood area (neighbor spatial correlation), and (3) the closer between two timestamps, the more relevant events are (hawkes temporal correlation). To this end, we propose Hawkes-enhanced Spatial-Temporal Hypergraph Contrastive Learning framework (HCL), which mines the aforementioned correlations via two specific strategies. Concretely, contrastive learning strategies are designed for two spatial correlations, and hawkes process modeling is adopted for temporal correlations. Extensive experiments demonstrate the promising capacities of HCL from four aspects, i.e., superiority, transferability, effectiveness, and sensitivity.","['DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data', 'DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Applications']",[],"['Ke Liang', 'Sihang Zhou', 'Meng Liu', 'Yue Liu', 'Wenxuan Tu', 'Yi Zhang', 'Liming Fang', 'Zhe Liu', 'Xinwang Liu']","['School of Computer, National University of Defense Technology', 'School of Intelligence Science and Technology, National University of Defense Technology', 'School of Computer, National University of Defense Technology', 'School of Computer, National University of Defense Technology', 'School of Computer, National University of Defense Technology', 'School of Computer, National University of Defense Technology', 'Nanjing University of Aeronautics and Astronautics', 'Zhejiang Lab', 'School of Computer, National University of Defense Technology']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28720,Fairness & Bias,A Comprehensive Augmentation Framework for Anomaly Detection,"Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution. This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations. Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the overfitting issue while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of object classes. We also generate a simulated dataset comprising anomalies with diverse characteristics, and experimental results demonstrate that our approach exhibits promising potential for generalizing effectively to various unseen anomalies encountered in real-world scenarios.",['DMKM: Anomaly/Outlier Detection'],[],"['Jiang Lin', 'Yaping Yan']","['Southeast University', 'Southeast University']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28724,Transparency & Explainability,Multimodal Event Causality Reasoning with Scene Graph Enhanced Interaction Network,"Multimodal event causality reasoning aims to recognize the causal relations based on the given events and accompanying image pairs, requiring the model to have a comprehensive grasp of visual and textual information. However, existing studies fail to effectively model the relations of the objects within the image and capture the object interactions across the image pair, resulting in an insufficient understanding of visual information by the model. To address these issues, we propose a Scene Graph Enhanced Interaction Network (SEIN) in this paper, which can leverage the interactions of the generated scene graph for multimodal event causality reasoning. Specifically, the proposed method adopts a graph convolutional network to model the objects and their relations derived from the scene graph structure, empowering the model to exploit the rich structural and semantic information in the image adequately. To capture the object interactions between the two images, we design an optimal transport-based alignment strategy to match the objects across the images, which could help the model recognize changes in visual information and facilitate causality reasoning. In addition, we introduce a cross-modal fusion module to combine textual and visual features for causality prediction. Experimental results indicate that the proposed SEIN outperforms state-of-the-art methods on the Vis-Causal dataset.","['DMKM: Mining of Visual', 'Multimedia & Multimodal Data', 'HAI: Applications', 'NLP: Language Grounding & Multi-modal NLP', 'RU: Applications', 'RU: Causality']",[],"['Jintao Liu', 'Kaiwen Wei', 'Chenglong Liu']","['University of Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'University of Chinese Academy of Sciences']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28726,Fairness & Bias,Online Conversion Rate Prediction via Multi-Interval Screening and Synthesizing under Delayed Feedback,"Due to the widespread adoption of the cost-per-action(CPA) display strategy that demands a real-time conversion rate prediction(CVR), delayed feedback is becoming one of the major challenges in online advertising. As the true labels of a significant quantity of samples are only available after long delays, the observed training data are usually biased, harming the performance of models. Recent studies show integrating models with varying waiting windows to observe true labels is beneficial, but the aggregation framework remains far from reaching a consensus. In this work, we propose the Multi-Interval Screening and Synthesizing model (MISS for short) for online CVR prediction. We first design a multi-interval screening model with various output heads to produce accurate and distinctive estimates. Then a light-weight synthesizing model with an assembled training pipeline is applied to thoroughly exploit the knowledge and relationship among heads, obtaining reliable predictions. Extensive experiments on two real-world advertising datasets validate the effectiveness of our model.","['DMKM: Recommender Systems', 'DMKM: Applications']",[],"['Qiming Liu', 'Xiang Ao', 'Yuyao Guo', 'Qing He']","['Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.\nUniversity of Chinese Academy of Sciences, Beijing 100049, China.', 'Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.\nUniversity of Chinese Academy of Sciences, Beijing 100049, China.\nInstitute of Intelligent Computing Technology, Suzhou.', 'Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.\nUniversity of Chinese Academy of Sciences, Beijing 100049, China.', 'Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China.\nUniversity of Chinese Academy of Sciences, Beijing 100049, China.']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28730,Security,Perturbation-Invariant Adversarial Training for Neural Ranking Models: Improving the Effectiveness-Robustness Trade-Off,"Neural ranking models (NRMs) have shown great success in information retrieval (IR). But their predictions can easily be manipulated using adversarial examples, which are crafted by adding imperceptible perturbations to legitimate documents. This vulnerability raises significant concerns about their reliability and hinders the widespread deployment of NRMs. By incorporating adversarial examples into training data, adversarial training has become the de facto defense approach to adversarial attacks against NRMs. However, this defense mechanism is subject to a trade-off between effectiveness and adversarial robustness. In this study, we establish theoretical guarantees regarding the effectiveness-robustness trade-off in NRMs. We decompose the robust ranking error into two components, i.e., a natural ranking error for effectiveness evaluation and a boundary ranking error for assessing adversarial robustness. Then, we define the perturbation invariance of a ranking model and prove it to be a differentiable upper bound on the boundary ranking error for attainable computation. Informed by our theoretical analysis, we design a novel perturbation-invariant adversarial training (PIAT) method for ranking models to achieve a better effectiveness-robustness trade-off. We design a regularized surrogate loss, in which one term encourages the effectiveness to be maximized while the regularization term encourages the output to be smooth, so as to improve adversarial robustness. Experimental results on several ranking models demonstrate the superiority of PITA compared to existing adversarial defenses.","['DMKM: Conversational Systems for Recommendation & Retrieval', 'ML: Adversarial Learning & Robustness', 'NLP: Safety and Robustness']",[],"['Yu-An Liu', 'Ruqing Zhang', 'Mingkun Zhang', 'Wei Chen', 'Maarten de Rijke', 'Jiafeng Guo', 'Xueqi Cheng']","['CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China', 'CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China', 'CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China', 'CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China', 'University of Amsterdam, Amsterdam, The Netherlands', 'CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China', 'CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28735,Transparency & Explainability,Enhancing Cognitive Diagnosis Using Un-interacted Exercises: A Collaboration-Aware Mixed Sampling Approach,"Cognitive diagnosis is a crucial task in computer-aided education, aimed at evaluating students' proficiency levels across various knowledge concepts through exercises. Current models, however, primarily rely on students' answered exercises, neglecting the complex and rich information contained in un-interacted exercises. While recent research has attempted to leverage the data within un-interacted exercises linked to interacted knowledge concepts, aiming to address the long-tail issue, these studies fail to fully explore the informative, un-interacted exercises related to broader knowledge concepts. This oversight results in diminished performance when these models are applied to comprehensive datasets. In response to this gap, we present the Collaborative-aware Mixed Exercise Sampling (CMES) framework, which can effectively exploit the information present in un-interacted exercises linked to un-interacted knowledge concepts. Specifically, we introduce a novel universal sampling module where the training samples comprise not merely raw data slices, but enhanced samples generated by combining weight-enhanced attention mixture techniques. Given the necessity of real response labels in cognitive diagnosis, we also propose a ranking-based pseudo feedback module to regulate students' responses on generated exercises. The versatility of the CMES framework bolsters existing models and improves their adaptability. Finally, we demonstrate the effectiveness and interpretability of our framework through comprehensive experiments on real-world datasets.","['DMKM: Applications', 'APP: Other Applications']",[],"['Haiping Ma', 'Changqian Wang', 'Hengshu Zhu', 'Shangshang Yang', 'Xiaoming Zhang', 'Xingyi Zhang']","['Department of Information Materials and Intelligent Sensing Laboratory of Anhui Province, Institutes of Physical Science and Information Technology, Anhui University, China', 'Department of Information Materials and Intelligent Sensing Laboratory of Anhui Province, Institutes of Physical Science and Information Technology, Anhui University, China', 'Career Science Lab, BOSS Zhipin', 'School of Artificial Intelligence, Anhui University, China', 'Department of Information Materials and Intelligent Sensing Laboratory of Anhui Province, Institutes of Physical Science and Information Technology, Anhui University, China', 'School of Computer Science and Technology, Anhui University, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28738,Transparency & Explainability,Graph Contrastive Invariant Learning from the Causal Perspective,"Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the encoder to capture the invariant information contained in causal variables, and (ii) the independence objective aims to reduce the influence of confounders on the causal variables. Experimental results demonstrate the effectiveness of our approach on node classification tasks.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Graph-based Machine Learning']",[],"['Yanhu Mo', 'Xiao Wang', 'Shaohua Fan', 'Chuan Shi']","['Beijing University of Posts and Telecommunications', 'Beihang University', 'Tsinghua Univerisity\nKey Laboratory of Big Data Artificial Intelligence in Transportation, Ministry of Education(Beijing Jiaotong University)', 'Beijing University of Posts and Telecommunications']","['China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28747,Fairness & Bias,An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention,"Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called Beyond Self-Attention for Sequential Recommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge the gap for existing Transformer-based SR models. We test our proposed approach through extensive experiments on 6 benchmark datasets. The experimental results demonstrate that our model outperforms 7 baseline methods in terms of recommendation performance. Our code is available at https://github.com/yehjin-shin/BSARec.","['DMKM: Recommender Systems', 'DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data']",[],"['Yehjin Shin', 'Jeongwhan Choi', 'Hyowon Wi', 'Noseong Park']","['Yonsei University', 'Yonsei University', 'Yonsei University', 'Yonsei University']","['Japan', 'Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/28751,Transparency & Explainability,MAPTree: Beating “Optimal” Decision Trees with Bayesian Decision Trees,"Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster than existing sampling approaches and, in contrast with those algorithms, is able to provide a certificate of optimality. The code for our experiments is available at https://github.com/ThrunGroup/maptree.","['DMKM: Rule Mining & Pattern Mining', 'ML: Bayesian Learning', 'ML: Classification and Regression', 'RU: Probabilistic Inference', 'SO: Applications', 'SO: Heuristic Search']",[],"['Colin Sullivan', 'Mo Tiwari', 'Sebastian Thrun']","['Stanford University', 'Stanford University', 'Stanford University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28757,Fairness & Bias,End-to-End Learning of LTLf Formulae by Faithful LTLf Encoding,"It is important to automatically discover the underlying tree-structured formulae from large amounts of data. In this paper, we examine learning linear temporal logic on finite traces (LTLf) formulae, which is a tree structure syntactically and characterizes temporal properties semantically. Its core challenge is to bridge the gap between the concise tree-structured syntax and the complex LTLf semantics. Besides, the learning quality is endangered by explosion of the search space and wrong search bias guided by imperfect data. We tackle these challenges by proposing an LTLf encoding method to parameterize a neural network so that the neural computation is able to simulate the inference of LTLf formulae. We first identify faithful LTLf encoding, a subclass of LTLf encoding, which has a one-to-one correspondence to LTLf formulae. Faithful encoding guarantees that the learned parameter assignment of the neural network can directly be interpreted to an LTLf formula. With such an encoding method, we then propose an end-to-end approach, TLTLf, to learn LTLf formulae through neural networks parameterized by our LTLf encoding method. Experimental results demonstrate that our approach achieves state-of-the-art performance with up to 7% improvement in accuracy, highlighting the benefits of introducing the faithful LTLf encoding.",['DMKM: Rule Mining & Pattern Mining'],[],"['Hai Wan', 'Pingjia Liang', 'Jianfeng Du', 'Weilin Luo', 'Rongzhen Ye', 'Bo Peng']","['School of Computer Science and Engineering, Sun Yat-sen University', 'School of Computer Science and Engineering, Sun Yat-sen University', 'Guangdong University of Foreign Studies\nBigmath Technology, Shenzhen', 'School of Computer Science and Engineering, Sun Yat-sen University', 'School of Computer Science and Engineering, Sun Yat-sen University', 'School of Computer Science and Engineering, Sun Yat-sen University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28759,Fairness & Bias,Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective,"With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction.  In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph.  To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy.  Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms.  To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns.  Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data).  To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism.  During this period, we frequently update trend patterns to expand new information from dynamic graphs.  Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model.  These sampled nodes cover unseen patterns and previously learned patterns.  Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework.","['DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data', 'DMKM: Data Stream Mining', 'DMKM: Graph Mining', 'Social Network Analysis & Community']",[],"['Binwu Wang', 'Pengkun Wang', 'Yudong Zhang', 'Xu Wang', 'Zhengyang Zhou', 'Lei Bai', 'Yang Wang']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Shanghai AI Laboratory', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28767,Fairness & Bias,Poincaré Differential Privacy for Hierarchy-Aware Graph Embedding,"Hierarchy is an important and commonly observed topological property in real-world graphs that indicate the relationships between supervisors and subordinates or the organizational behavior of human groups. As hierarchy is introduced as a new inductive bias into the Graph Neural Networks (GNNs) in various tasks, it implies latent topological relations for attackers to improve their inference attack performance, leading to serious privacy leakage issues. In addition, existing privacy-preserving frameworks suffer from reduced protection ability in hierarchical propagation due to the deficiency of adaptive upper-bound estimation of the hierarchical perturbation boundary. It is of great urgency to effectively leverage the hierarchical property of data while satisfying privacy guarantees. To solve the problem, we propose the Poincar\'e Differential Privacy framework, named PoinDP, to protect the hierarchy-aware graph embedding based on hyperbolic geometry. Specifically, PoinDP first learns the hierarchy weights for each entity based on the Poincar\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware Sensitivity is designed to measure the sensitivity of the hierarchical structure and adaptively allocate the privacy protection strength. Besides, Hyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian mechanism in Euclidean space to hyperbolic space to realize random perturbations that satisfy differential privacy under the hyperbolic space metric. Extensive experiment results on five real-world datasets demonstrate the proposed PoinDP’s advantages of effective privacy protection while maintaining good performance on the node classification task.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Privacy']",[],"['Yuecen Wei', 'Haonan Yuan', 'Xingcheng Fu', 'Qingyun Sun', 'Hao Peng', 'Xianxian Li', 'Chunming Hu']","['Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China\nSchool of Software, Beihang University, Beijing, China\nKey Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China', 'Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China', 'Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China\nSchool of Software, Beihang University, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28771,Fairness & Bias,Feature Distribution Matching by Optimal Transport for Effective and Robust Coreset Selection,"Training neural networks with good generalization requires large computational costs in many deep learning methods due to large-scale datasets and over-parameterized models. Despite the emergence of a number of coreset selection methods to reduce the computational costs, the problem of coreset distribution bias, i.e., the skewed distribution between the coreset and the entire dataset, has not been well studied. In this paper, we find that the closer the feature distribution of the coreset is to that of the entire dataset, the better the generalization performance of the coreset, particularly under extreme pruning. This motivates us to propose a simple yet effective method for coreset selection to alleviate the distribution bias between the coreset and the entire dataset, called feature distribution matching (FDMat). Unlike gradient-based methods, which selects samples with larger gradient values or approximates gradient values of the entire dataset, FDMat aims to select coreset that is closest to feature distribution of the entire dataset. Specifically, FDMat transfers coreset selection as an optimal transport problem from the coreset to the entire dataset in feature embedding spaces. Moreover, our method shows strong robustness due to the removal of samples far from the distribution, especially for the entire dataset containing noisy and class-imbalanced samples. Extensive experiments on multiple benchmarks show that FDMat can improve the performance of coreset selection than existing coreset methods. The code is available at https://github.com/successhaha/FDMat.","['DMKM: Data Compression', 'CV: Other Foundations of Computer Vision', 'DMKM: Anomaly/Outlier Detection', 'ML: Dimensionality Reduction/Feature Selection']",[],"['Weiwei Xiao', 'Yongyong Chen', 'Qiben Shan', 'Yaowei Wang', 'Jingyong Su']","['Harbin Institute of Technology, Shenzhen\nPengCheng Laboratory', 'Harbin Institute of Technology, Shenzhen', 'PengCheng Laboratory', 'PengCheng Laboratory', 'Harbin Institute of Technology, Shenzhen\nPengCheng Laboratory']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28769,Fairness & Bias,Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations,"Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for graph semantic mining in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including promoting out-of-distribution (OOD) applications. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that aids LLM recommender in grasping the semantics of behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By facilitating this capability, our framework enables personalized and accurate job recommendations for individual users. We evaluate the effectiveness of our approach on comprehensive real-world datasets and demonstrate its ability to improve the relevance and quality of recommended results. This research not only sheds light on the untapped potential of large language models but also provides valuable insights for developing advanced recommendation systems in the recruitment market. The findings contribute to the growing field of natural language processing and offer practical implications for enhancing job search experiences.","['DMKM: Applications', 'DMKM: Recommender Systems', 'KRR: Knowledge Engineering', 'NLP: (Large) Language Models', 'NLP: Applications']",[],"['Likang Wu', 'Zhaopeng Qiu', 'Zhi Zheng', 'Hengshu Zhu', 'Enhong Chen']","['University of Science and Technology of China\nCareer Science Lab, BOSS Zhipin', 'Career Science Lab, BOSS Zhipin', 'University of Science and Technology of China\nCareer Science Lab, BOSS Zhipin', 'Career Science Lab, BOSS Zhipin', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28773,Security,Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum,"Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides the spectrum into various mixed-frequency bands based on the correlation between spectrum energy distribution and heterophily. Then in order to make full use of the node label information, a local environmental constraint module is adaptively designed. The comprehensive experimental results on four real-world fraud detection datasets denote that SEC-GFD outperforms other competitive graph-based fraud detectors. We release our code at https://github.com/Sunxkissed/SEC-GFD.","['DMKM: Anomaly/Outlier Detection', 'DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Semi-Supervised Learning']",[],"['Fan Xu', 'Nan Wang', 'Hao Wu', 'Xuezhi Wen', 'Xibin Zhao', 'Hai Wan']","['University of Science and Technology of China', 'Beijing Jiaotong University', 'University of Science and Technology of China', 'Beijing Jiaotong University', 'Tsinghua University', 'Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28774,Transparency & Explainability,Empowering Dual-Level Graph Self-Supervised Pretraining with Motif Discovery,"While self-supervised graph pretraining techniques have shown promising results in various domains, their application still experiences challenges of limited topology learning, human knowledge dependency, and incompetent multi-level interactions. To address these issues, we propose a novel solution, Dual-level Graph self-supervised Pretraining with Motif discovery (DGPM), which introduces a unique dual-level pretraining structure that orchestrates node-level and subgraph-level pretext tasks. Unlike prior approaches, DGPM autonomously uncovers significant graph motifs through an edge pooling module, aligning learned motif similarities with graph kernel-based similarities. A cross-matching task enables sophisticated node-motif interactions and novel representation learning. Extensive experiments on 15 datasets validate DGPM's effectiveness and generalizability, outperforming state-of-the-art methods in unsupervised representation learning and transfer learning settings. The autonomously discovered motifs demonstrate the potential of DGPM to enhance robustness and interpretability.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Unsupervised & Self-Supervised Learning', 'ML: Deep Learning Algorithms', 'ML: Multi-instance/Multi-view Learning']",[],"['Pengwei Yan', 'Kaisong Song', 'Zhuoren Jiang', 'Yangyang Kang', 'Tianqianjin Lin', 'Changlong Sun', 'Xiaozhong Liu']","['Department of Information Resources Management, Zhejiang University, Hangzhou, 310058, China\nAlibaba Group, Hangzhou, 311121, China', 'Alibaba Group, Hangzhou, 311121, China\nNortheastern University, Shenyang, 110819, China', 'Department of Information Resources Management, Zhejiang University, Hangzhou, 310058, China', 'Alibaba Group, Hangzhou, 311121, China', 'Department of Information Resources Management, Zhejiang University, Hangzhou, 310058, China\nAlibaba Group, Hangzhou, 311121, China', 'Alibaba Group, Hangzhou, 311121, China', 'Computer Science Department, Worcester Polytechnic Institute, Worcester, 01609-2280, MA, USA']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28776,Fairness & Bias,FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization,"Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nonsensitive information. We also provide theoretical explanations for our rationale, concluding that F3 can be realized by emphasizing the features of each node’s heterogeneous neighbors (neighbors with different sensitive attributes). We name our method as FairSIN, and present three implementation variants from both data-centric and model-centric perspectives. Experimental results on five benchmark datasets with three different GNN backbones show that FairSIN significantly improves fairness metrics while maintaining high prediction accuracies. Codes and appendix can be found at https://github.com/BUPT-GAMMA/FariSIN.","['DMKM: Graph Mining', 'Social Network Analysis & Community']",[],"['Cheng Yang', 'Jixi Liu', 'Yunhe Yan', 'Chuan Shi']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28782,Privacy & Data Governance,RRL: Recommendation Reverse Learning,"As societies become increasingly aware of data privacy, regulations require that private information about users must be removed from both database and ML models, which is more colloquially called `the right to be forgotten`. Such privacy problems of recommendation systems, which hold large amounts of private data, are drawing increasing attention. Recent research suggests dividing the preference data into multiple shards and training submodels with these shards and forgetting users' personal preference data by retraining the submodels of marked shards. Despite the computational efficiency development compared with retraining from scratch, the overall recommendation performance deteriorates after dividing the shards because the collaborative information contained in the training data is broken. In this paper, we aim to propose a forgetting framework for recommendation models that neither separate the training data nor jeopardizes the recommendation performance, named Recommendation Reverse Learning (RRL). Given the trained recommendation model and marked preference data, we devise Reverse BPR Objective (RBPR Objective) to fine-tune the recommendation model to force it to forget the marked data. Nevertheless, as the recommendation model encode the complex collaborative information among users, we propose to utilize Fisher Information Matrix (FIM) to estimate the influence of reverse learning on other users' collaborative information and guide the updates of representations. We conduct experiments on two representative recommendation models and three public benchmark datasets to verify the efficiency of RRL. To verify the forgetting completeness, we use RRL to make the recommendation model poisoned by shilling attacks forget malicious users.","['DMKM: Conversational Systems for Recommendation & Retrieval', 'PEAI: Privacy & Security']",[],"['Xiaoyu You', 'Jianwei Xu', 'Mi Zhang', 'Zechen Gao', 'Min Yang']","['Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28785,Security,DiG-In-GNN: Discriminative Feature Guided GNN-Based Fraud Detector against Inconsistencies in Multi-Relation Fraud Graph,"Fraud detection on multi-relation graphs aims to identify fraudsters in graphs. Graph Neural Network (GNN) models leverage graph structures to pass messages from neighbors to the target nodes, thereby enriching the representations of those target nodes. However, feature and structural inconsistency in the graph, owing to fraudsters' camouflage behaviors, diminish the suspiciousness of fraud nodes which hinders the effectiveness of GNN-based models. In this work, we propose DiG-In-GNN, Discriminative Feature Guided GNN against Inconsistency, to dig into graphs for fraudsters. Specifically, we use multi-scale contrastive learning from the perspective of the neighborhood subgraph where the target node is located to generate guidance nodes to cope with the feature inconsistency. Then, guided by the guidance nodes, we conduct fine-grained neighbor selection through reinforcement learning for each neighbor node to precisely filter nodes that can enhance the message passing and therefore alleviate structural inconsistency. Finally, the two modules are integrated together to obtain discriminable representations of the nodes. Experiments on three fraud detection datasets demonstrate the superiority of the proposed method DiG-In-GNN, which obtains up to 20.73% improvement over previous state-of-the-art methods. Our code can be found at https://github.com/GraphBerry/DiG-In-GNN.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'DMKM: Anomaly/Outlier Detection']",[],"['Jinghui Zhang', 'Zhengjia Xu', 'Dingyang Lv', 'Zhan Shi', 'Dian Shen', 'Jiahui Jin', 'Fang Dong']","['Southeast University', 'Southeast University', 'Southeast University', 'Southeast University', 'Southeast University', 'Southeast University', 'Southeast University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28796,Transparency & Explainability,Explainable Origin-Destination Crowd Flow Interpolation via Variational Multi-Modal Recurrent Graph Auto-Encoder,"Origin-destination (OD) crowd flow, if more accurately inferred at a fine-grained level, has the potential to enhance the efficacy of various urban applications. While in practice for mining OD crowd flow with effect, the problem of spatially interpolating OD crowd flow occurs since the ineluctable missing values. This problem is further complicated by the inherent scarcity and noise nature of OD crowd flow data. In this paper, we propose an uncertainty-aware interpolative and explainable framework, namely UApex, for realizing reliable and trustworthy OD crowd flow interpolation. Specifically, we first design a Variational Multi-modal Recurrent Graph Auto-Encoder (VMR-GAE) for uncertainty-aware OD crowd flow interpolation. A key idea here is to formulate the problem as semi-supervised learning on directed graphs. Next, to mitigate the data scarcity, we incorporate a distribution alignment mechanism that can introduce supplementary modals into variational inference. Then, a dedicated decoder with a Poisson prior is proposed for OD crowd flow interpolation. Moreover, to make VMR-GAE more trustworthy, we develop an efficient and uncertainty-aware explainer that can provide explanations from the spatiotemporal topology perspective via the Shapley value. Extensive experiments on two real-world datasets validate that VMR-GAE outperforms the state-of-the-art baselines. Also, an exploratory empirical study shows that the proposed explainer can generate meaningful spatiotemporal explanations.","['DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data', 'DMKM: Applications', 'PEAI: Accountability', 'Interpretability & Explainability', 'RU: Applications']",[],"['Qiang Zhou', 'Xinjiang Lu', 'Jingjing Gu', 'Zhe Zheng', 'Bo Jin', 'Jingbo Zhou']","['Nanjing University of Aeronautics and Astronautics, Nanjing, China', 'Baidu Research, Beijing, China', 'Nanjing University of Aeronautics and Astronautics, Nanjing, China', 'Nanjing University of Aeronautics and Astronautics, Nanjing, China', 'Dalian University of Technology, Dalian, China', 'Baidu Research, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28798,Fairness & Bias,Analytically Tractable Models for Decision Making under Present Bias,"Time-inconsistency is a characteristic of human behavior in which people plan for long-term benefits but take actions that differ from the plan due to conflicts with short-term benefits. Such time-inconsistent behavior is believed to be caused by present bias, a tendency to overestimate immediate rewards and underestimate future rewards. It is essential in behavioral economics to investigate the relationship between present bias and time-inconsistency. In this paper, we propose a model for analyzing agent behavior with present bias in tasks to make progress toward a goal over a specific period. Unlike previous models, the state sequence of the agent can be described analytically in our model. Based on this property, we analyze three crucial problems related to agents under present bias: task abandonment, optimal goal setting, and optimal reward scheduling. Extensive analysis reveals how present bias affects the condition under which task abandonment occurs and optimal intervention strategies. Our findings are meaningful for preventing task abandonment and intervening through incentives in the real world.","['GTEP: Behavioral Game Theory', 'CSO: Constraint Optimization']",[],"['Yasunori Akagi', 'Naoki Marumo', 'Takeshi Kurashima']","['NTT Human Informatics Laboratories, NTT Corporation', 'NTT', 'NTT Corporation']","['Japan', 'Japan, 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/28803,Security,Content Filtering with Inattentive Information Consumers,"We develop a model of content filtering as a game between the filter and the content consumer, where the latter incurs information costs for examining the content. Motivating examples include censoring misinformation, spam/phish filtering, and recommender systems acting on a stream of content. When the attacker is exogenous, we show that improving the filter’s quality is weakly Pareto improving, but has no impact on equilibrium payoffs until the filter becomes sufficiently accurate. Further, if the filter does not internalize the consumer’s information costs, its lack of commitment power may render it useless and lead to inefficient outcomes. When the attacker is also strategic, improvements in filter quality may decrease equilibrium payoffs.","['GTEP: Game Theory', 'APP: Security', 'GTEP: Applications', 'GTEP: Imperfect Information']",[],"['Ian Ball', 'James Bono', 'Justin Grana', 'Nicole Immorlica', 'Brendan Lucier', 'Aleksandrs Slivkins']","['MIT', 'Microsoft', 'Edge and Node', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28807,Security,Approval-Based Committee Voting in Practice: A Case Study of (over-)Representation in the Polkadot Blockchain,"We provide the first large-scale data collection of real-world approval-based committee elections. These elections have been conducted on the Polkadot blockchain as part of their Nominated Proof-of-Stake mechanism and contain around one thousand candidates and tens of thousands of (weighted) voters each. We conduct an in-depth study of application-relevant questions, including a quantitative and qualitative analysis of the outcomes returned by different voting rules. Besides considering proportionality measures that are standard in the multiwinner voting literature, we pay particular attention to less-studied measures of overrepresentation, as these are closely related to the security of the Polkadot network. We also analyze how different design decisions such as the committee size affect the examined measures.",['GTEP: Social Choice / Voting'],[],"['Niclas Boehmer', 'Markus Brill', 'Alfonso Cevallos', 'Jonas Gehrlein', 'Luis Sánchez-Fernández', 'Ulrike Schmidt-Kraepelin']","['Harvard University', 'University of Warwick', 'Web3 Foundation', 'Web3 Foundation', 'Universidad Carlos III de Madrid', 'TU Eindhoven']","['United States', 'United Kingdom', 'Chad', 'Chad', 'Spain', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/28817,Security,The Complexity of Computing Robust Mediated Equilibria in Ordinal Games,"Usually, to apply game-theoretic methods, we must specify utilities precisely, and we run the risk that the solutions we compute are not robust to errors in this specification.  Ordinal games provide an attractive alternative: they require specifying only which outcomes are preferred to which other ones.  Unfortunately, they provide little guidance for how to play unless there are pure Nash equilibria; evaluating mixed strategies appears to fundamentally require cardinal utilities.  In this paper, we observe that we can in fact make good use of mixed strategies in ordinal games if we consider settings that allow for folk theorems.  These allow us to find equilibria that are robust, in the sense that they remain equilibria no matter which cardinal utilities are the correct ones -- as long as they are consistent with the specified ordinal preferences.  We analyze this concept and study the computational complexity of finding such equilibria in a range of settings.","['GTEP: Game Theory', 'GTEP: Equilibrium', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],['Vincent Conitzer'],['Carnegie Mellon University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/28823,Fairness & Bias,Competition among Pairwise Lottery Contests,"We investigate a two-stage competitive model involving multiple contests. In this model, each contest designer chooses two participants from a pool of candidate contestants and determines the biases. Contestants strategically distribute their efforts across various contests within their budget. We first show the existence of a pure strategy Nash equilibrium (PNE) for the contestants, and propose a fully polynomial-time approximation scheme to compute an approximate PNE. In the scenario where designers simultaneously decide the participants and biases, the subgame perfect equilibrium (SPE) may not exist. Nonetheless, when designers' decisions are made in two substages, the existence of SPE is established. In the scenario where designers can hold multiple contests, we show that the SPE always exists under mild conditions and can be computed efficiently.","['GTEP: Equilibrium', 'GTEP: Game Theory']",[],"['Xiaotie Deng', 'Hangxin Gan', 'Ningyuan Li', 'Weian Li', 'Qi Qi']","['Center on Frontiers of Computing Studies, School of Computer Science, Peking University, Beijing, China.', 'School of Mathematical Science, Nankai University, Tianjin, China.', 'Center on Frontiers of Computing Studies, School of Computer Science, Peking University, Beijing, China.', 'Center on Frontiers of Computing Studies, School of Computer Science, Peking University, Beijing, China.', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China.']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28825,Fairness & Bias,Implications of Distance over Redistricting Maps: Central and Outlier Maps,"In representative democracy, a redistricting map is chosen to partition an electorate into districts which each elects a representative. A valid redistricting map must satisfy a collection of constraints such as being compact, contiguous, and of almost-equal population. However, these constraints are loose enough to enable an enormous ensemble of valid redistricting maps. This enables a partisan legislature to gerrymander by choosing a map which unfairly favors it. In this paper, we introduce an interpretable and tractable distance measure over redistricting maps which does not use election results and study its implications over the ensemble of redistricting maps. Specifically, we define a central map which may be considered ""most typical"" and give a rigorous justification for it by showing that it mirrors the Kemeny ranking in a scenario where we have a committee voting over a collection of redistricting maps to be drawn. We include runnning time and sample complexity analysis for our algorithms, including some negative results which hold using any algorithm. We further study outlier detection based on this distance measure and show that our framework can detect some gerrymandered maps. More precisely, we show some maps that are widely considered to be gerrymandered that lie very far away from our central maps in comparison to a large ensemble of valid redistricting maps. Since our distance measure does not rely on election results, this gives a significant advantage in gerrymandering detection which is lacking in all previous methods.","['GTEP: Social Choice / Voting', 'APP: Humanities & Computational Social Science', 'DMKM: Anomaly/Outlier Detection', 'ML: Ethics', 'Bias', 'and Fairness', 'SO: Combinatorial Optimization']",[],"['Seyed A. Esmaeili', 'Darshan Chakrabarti', 'Hayley Grape', 'Brian Brubach']","['Simons Laufer Mathematical Sciences Institute and University of Chicago Data Science Institute', 'Columbia University', 'Wellesley College', 'Wellesley College']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28829,Fairness & Bias,Maxileximin Envy Allocations and Connected Goods,"Fair allocation of indivisible goods presents intriguing challenges from both a social choice perspective and an algorithmic standpoint. Due to the indivisibility of goods, it is common for one agent to envy the bundle of goods assigned to another agent and, indeed, envy-free solutions do not exist in general. In line with the classical game-theoretic concept of Nucleolus in coalitional games, we propose that a fair allocation should minimize the agents’ dissatisfaction profile in a lexicographic manner, where the dissatisfaction of an agent is defined as her maximum envy towards other agents. Therefore, we seek allocations that minimize the maximum envy. In cases where multiple solutions have an equal maximum value, we minimize the second-worst value, and so on. Additionally, as is customary in fair division problems, we also consider an efficiency requirement: among the allocations with the best agents’ dissatisfaction profile, we prioritize those that maximize the sum of agents’ utilities, known as maximum social welfare. Such allocations, referred to as maxileximin allocations, always exist. In this study, we analyze the computational properties of maxileximin allocations in the context of fair allocation problems with constraints. Specifically, we focus on the Connected Fair Division problem, where goods correspond to the nodes of a graph, and a bundle of goods is allowed if the subgraph formed by those goods is connected. We demonstrate that the problem is F∆P2 -complete, even for instances with simple graphical structures such as path and star graphs. However, we identify islands of tractability for instances with more intricate graphs, such as those having bounded treewidth, provided that the number of agents is bounded by a fixed number and utility functions use small values.","['GTEP: Fair Division', 'GTEP: Game Theory', 'GTEP: Social Choice / Voting', 'PEAI: Bias', 'Fairness & Equity']",[],"['Gianluigi Greco', 'Francesco Scarcello']","['DEMACS, University of Calabria, Italy', 'DIMES, University of Calabria, Italy']","['Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/28843,Fairness & Bias,Strategyproof Mechanisms for Group-Fair Obnoxious Facility Location Problems,"We study the group-fair obnoxious facility location problems from the mechanism design perspective where agents belong to different groups and have private location preferences on the undesirable locations of the facility. Our main goal is to design strategyproof mechanisms that elicit the true location preferences from the agents and determine a facility location that approximately optimizes several group-fair objectives. We first consider the maximum total and average group cost (group-fair) objectives. For these objectives, we propose deterministic mechanisms that achieve 3-approximation ratios and provide matching lower bounds. We then provide the characterization of 2-candidate strategyproof randomized mechanisms. Leveraging the characterization, we design randomized mechanisms with improved approximation ratios of 2 for both objectives. We also provide randomized lower bounds of 5/4 for both objectives. Moreover, we investigate intergroup and intragroup fairness (IIF) objectives, addressing fairness between groups and within each group. We present a mechanism that achieves a 4-approximation for the IIF objectives and provide tight lower bounds.","['GTEP: Social Choice / Voting', 'MAS: Mechanism Design']",[],"['Jiaqian Li', 'Minming Li', 'Hau Chan']","['City University of Hong Kong', 'City University of Hong Kong', 'University of Nebraska-Lincoln']","['Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28856,Security,Learning Coalition Structures with Games,"Coalitions naturally exist in many real-world systems involving multiple decision makers such as ridesharing, security, and online ad auctions, but the coalition structure among the agents is often unknown. We propose and study an important yet previously overseen problem -- Coalition Structure Learning (CSL), where we aim to carefully design a series of games for the agents and infer the underlying coalition structure by observing their interactions in those games. We establish a lower bound on the sample complexity  -- defined as the number of games needed to learn the structure -- of any algorithms for CSL and propose the Iterative Grouping (IG) algorithm for designing normal-form games to achieve the lower bound. We show that IG can be extended to other succinct games such as congestion games and graphical games. Moreover, we solve CSL in a more restrictive and practical setting: auctions. We show a variant of IG to solve CSL in the auction setting even if we cannot design the bidder valuations. Finally, we conduct experiments to evaluate IG in the auction setting and the results align with our theoretical analysis.","['GTEP: Coordination and Collaboration', 'GTEP: Auctions and Market-Based Systems', 'GTEP: Game Theory']",[],"['Yixuan Even Xu', 'Chun Kai Ling', 'Fei Fang']","['Tsinghua University', 'Columbia University\nCarnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28863,Transparency & Explainability,Explaining Reinforcement Learning Agents through Counterfactual Action Outcomes,"Explainable reinforcement learning (XRL) methods aim to help elucidate agent policies and decision-making processes. The majority of XRL approaches focus on local explanations, seeking to shed light on the reasons an agent acts the way it does at a specific world state. While such explanations are both useful and necessary, they typically do not portray the outcomes of the agent's selected choice of action.  In this work, we propose ``COViz'', a new local explanation method that visually compares the outcome of an agent's chosen action to a counterfactual one. In contrast to most local explanations that provide state-limited observations of the agent's motivation, our method depicts alternative trajectories the agent could have taken from the given state and their outcomes.  We evaluated the usefulness of COViz in supporting people's understanding of agents' preferences and compare it with reward decomposition, a local explanation method that describes an agent's expected utility for different actions by decomposing it into meaningful reward types. Furthermore, we examine the complementary benefits of integrating both methods. Our results show that such integration significantly improved participants' performance.","['HAI: Human-Computer Interaction', 'PEAI: Accountability', 'Interpretability & Explainability']",[],"['Yotam Amitai', 'Yael Septon', 'Ofra Amir']","['Technion - Israel Institute of Technology, Faculty of Data and Decision Science', 'Technion - Israel Institute of Technology, Faculty of Data and Decision Science', 'Technion - Israel Institute of Technology, Faculty of Data and Decision Science']","['Israel', 'Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/28868,Transparency & Explainability,Working Memory Capacity of ChatGPT: An Empirical Study,"Working memory is a critical aspect of both human intelligence and artificial intelligence, serving as a workspace for the temporary storage and manipulation of information. In this paper, we systematically assess the working memory capacity of ChatGPT, a large language model developed by OpenAI, by examining its performance in verbal and spatial n-back tasks under various conditions. Our experiments reveal that ChatGPT has a working memory capacity limit strikingly similar to that of humans. Furthermore, we investigate the impact of different instruction strategies on ChatGPT's performance and observe that the fundamental patterns of a capacity limit persist. From our empirical findings, we propose that n-back tasks may serve as tools for benchmarking the working memory capacity of large language models and hold potential for informing future efforts aimed at enhancing AI working memory.","['HAI: Other Foundations of Human Computation & AI', 'CMS: Adaptive Behavior', 'CMS: Other Foundations of Cognitive Modeling & Systems', 'CMS: Simulating Human Behavior', 'NLP: (Large) Language Models', 'NLP: Conversational AI/Dialog Systems', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Dongyu Gong', 'Xingchen Wan', 'Dingmin Wang']","['University of Oxford\nYale University', 'University of Oxford', 'University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28872,Transparency & Explainability,Decoding AI’s Nudge: A Unified Framework to Predict Human Behavior in AI-Assisted Decision Making,"With the rapid development of AI-based decision aids,  different forms of AI assistance have been increasingly integrated into the human decision making processes.  To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process.  Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult.  In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making.  By conceptualizing  AI assistance as the ``nudge'' in human decision making processes, our approach centers around modelling how different forms of AI assistance modify humans' strategy in weighing different information in making their decisions. Evaluations on behavior data collected from real human decision makers  show that the proposed framework outperforms various baselines in accurately predicting human behavior in AI-assisted decision making. Based on the proposed framework, we further provide insights into how individuals with different cognitive styles are nudged by AI assistance differently.","['HAI: Human-Computer Interaction', 'CMS: Simulating Human Behavior', 'HAI: Applications']",[],"['Zhuoyan Li', 'Zhuoran Lu', 'Ming Yin']","['Purdue University', 'Purdue University', 'Purdue University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28875,Security,Goal Alignment: Re-analyzing Value Alignment Problems Using Human-Aware AI,"While the question of misspecified objectives has gotten much attention in recent years, most works in this area primarily focus on the challenges related to the complexity of the objective specification mechanism (for example, the use of reward functions). However, the complexity of the objective specification mechanism is just one of many reasons why the user may have misspecified their objective. A foundational cause for misspecification that is being overlooked by these works is the inherent asymmetry in human expectations about the agent's behavior and the behavior generated by the agent for the specified objective. To address this, we propose a novel formulation for the objective misspecification problem that builds on the human-aware planning literature, which was originally introduced to support explanation and explicable behavioral generation. Additionally, we propose a first-of-its-kind interactive algorithm that is capable of using information generated under incorrect beliefs about the agent to determine the true underlying goal of the user.","['HAI: Human-Aware Planning and Behavior Prediction', 'HAI: Interaction Techniques and Devices', 'HAI: Learning Human Values and Preferences', 'PEAI: Safety', 'Robustness & Trustworthiness', 'PRS: Activity and Plan Recognition']",[],"['Malek Mechergui', 'Sarath Sreedharan']","['Colorado State University', 'Colorado State University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28882,Fairness & Bias,Intelligent Calibration for Bias Reduction in Sentiment Corpora Annotation Process,"This paper focuses in the inherent anchoring bias present in sequential reviews-sentiment corpora annotation processes. It proposes employing a limited subset of meticulously chosen reviews at the outset of the process, as a means of calibration, effectively mitigating the phenomenon. Through extensive experimentation we validate the phenomenon of sentiment bias in the annotation process and show that its magnitude can be influenced by pre-calibration. Furthermore, we show that the choice of the calibration set matters, hence the need for effective guidelines for choosing the reviews to be included in it. A comparison of annotators performance with the proposed calibration to annotation processes that do not use calibration or use a randomly-picked calibration set, reveals that indeed the calibration set picked is highly effective---it manages to substantially reduce the average absolute error compared to the other cases. Furthermore, the proposed selection guidelines are found to be highly robust in picking an effective calibration set also for domains different than the one based on which these rules were extracted.",['HAI: Crowd Sourcing and Human Computation'],[],"['Idan Toker', 'David Sarne', 'Jonathan Schler']","['Bar Ilan University', 'Bar-Ilan University', 'Holon Institute of Technology (HIT)']","['Israel', 'Israel', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/28889,Fairness & Bias,Scalable Motion Style Transfer with Constrained Diffusion Generation,"Current training of motion style transfer systems relies on consistency losses across style domains to preserve contents, hindering its scalable application to a large number of domains and private data. Recent image transfer works show the potential of independent training on each domain by leveraging implicit bridging between diffusion models, with the content preservation, however, limited to simple data patterns. We address this by imposing biased sampling in backward diffusion while maintaining the domain independence in the training stage. We construct the bias from the source domain keyframes and apply them as the gradient of content constraints, yielding a framework with keyframe manifold constraint gradients (KMCGs). Our validation demonstrates the success of training separate models to transfer between as many as ten dance motion styles. Comprehensive experiments find a significant improvement in preserving motion contents in comparison to baseline and ablative diffusion-based style transfer models. In addition, we perform a human study for a subjective assessment of the quality of generated dance motions. The results validate the competitiveness of KMCGs.","['HAI: Applications', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Wenjie Yin', 'Yi Yu', 'Hang Yin', 'Danica Kragic', 'Mårten Björkman']","['KTH Royal Institute of Technology', 'NII', 'University of Copenhagen', 'KTH Royal Institute of Technology', 'KTH Royal Institute of Technology']","['Sweden', 'Sweden', 'Sweden', 'Sweden', 'Sweden']"
https://ojs.aaai.org/index.php/AAAI/article/view/28891,Fairness & Bias,Beyond Mimicking Under-Represented Emotions: Deep Data Augmentation with Emotional Subspace Constraints for EEG-Based Emotion Recognition,"In recent years, using Electroencephalography (EEG) to recognize emotions has garnered considerable attention. Despite advancements, limited EEG data restricts its potential. Thus, Generative Adversarial Networks (GANs) are proposed to mimic the observed distributions and generate EEG data. However, for imbalanced datasets, GANs struggle to produce reliable augmentations for under-represented minority emotions by merely mimicking them. Thus, we introduce Emotional Subspace Constrained Generative Adversarial Networks (ESC-GAN) as an alternative to existing frameworks. We first propose the EEG editing paradigm, editing reference EEG signals from well-represented to under-represented emotional subspaces. Then, we introduce diversity-aware and boundary-aware losses to constrain the augmented subspace. Here, the diversity-aware loss encourages a diverse emotional subspace by enlarging the sample difference, while boundary-aware loss constrains the augmented subspace near the decision boundary where recognition models can be vulnerable. Experiments show ESC-GAN boosts emotion recognition performance on benchmark datasets, DEAP, AMIGOS, and SEED, while protecting against potential adversarial attacks. Finally, the proposed method opens new avenues for editing EEG signals under emotional subspace constraints, facilitating unbiased and secure EEG data augmentation.","['HAI: Brain-Sensing and Analysis', 'CMS: Affective Computing', 'ML: Deep Generative Models & Autoencoders']",[],"['Zhi Zhang', 'Shenghua Zhong', 'Yan Liu']","['Shenzhen University\nHong Kong Polytechnic University', 'Shenzhen University', 'The Hong Kong Polytechnic University']","['Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28910,Security,Complexity of Credulous and Skeptical Acceptance in Epistemic Argumentation Framework,"Dung’s Argumentation Framework (AF) has been extended in several directions. Among the numerous proposed extensions, three of them seem to be of particular interest and have correlations between them. These extensions are: constrained AF (CAF), where AF is augmented with (strong) constraints; epistemic AF (EAF), where AF is augmented with epistemic constraints; and incomplete AF (iAF), where arguments and attacks can be uncertain. While the complexity and expressiveness of CAF and iAF have been studied, that of EAF has not been explored so far. In this paper we investigate the complexity and expressivity of EAF. To this end, we first introduce the Labeled CAF (LCAF), a variation of CAF where constraints are defined over the alphabet of labeled arguments. Then, we investigate the complexity of credulous and skeptical reasoning and show that: i) EAF is more expressive than iAF (under preferred semantics), ii) although LCAF is a restriction of EAF where modal operators are not allowed, these frameworks have the same complexity, iii) the results for LCAF close a gap in the characterization of the complexity of CAF. Interestingly, even though EAF has the same complexity as LCAF, it allows modeling domain knowledge in a more natural and easy-to-understand way.",['KRR: Argumentation'],[],"['Gianvincenzo Alfano', 'Sergio Greco', 'Francesco Parisi', 'Irina Trubitsyna']","['University of Calabria', 'University of Calabria', 'University of Calabria', 'University of Calabria']","['Italy', 'Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/28914,Transparency & Explainability,Computing the Why-Provenance for Datalog Queries via SAT Solvers,"Explaining an answer to a Datalog query is an essential task towards Explainable AI, especially nowadays where Datalog plays a critical role in the development of ontology-based applications. A well-established approach for explaining a query answer is the so-called why-provenance, which essentially collects all the subsets of the input database that can be used to obtain that answer via some derivation process, typically represented as a proof tree. It is well known, however, that computing the why-provenance for Datalog queries is computationally expensive, and thus, very few attempts can be found in the literature. The goal of this work is to demonstrate how off-the-shelf SAT solvers can be exploited towards an efficient computation of the why-provenance for Datalog queries. Interestingly, our SAT-based approach allows us to build the why-provenance in an incremental fashion, that is, one explanation at a time, which is much more useful in a practical context than the one-shot computation of the whole set of explanations as done by existing approaches.","['KRR: Computational Complexity of Reasoning', 'KRR: Knowledge Representation Languages', 'KRR: Ontologies', 'KRR: Other Foundations of Knowledge Representation & Reasoning']",[],"['Marco Calautti', 'Ester Livshits', 'Andreas Pieris', 'Markus Schneider']","['University of Milan', 'University of Edinburgh', 'University of Edinburgh\nUniversity of Cyprus', 'University of Edinburgh']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28918,Security,Redefining ABA+ Semantics via Abstract Set-to-Set Attacks,"Assumption-based argumentation (ABA) is a powerful defeasible reasoning formalism which is based on the interplay of assumptions, their contraries, and inference rules. ABA with preferences (ABA+) generalizes the basic model by allowing qualitative comparison between assumptions. The integration of preferences however comes with a cost. In ABA+, the evaluation under two central and well-established semantics---grounded and complete semantics---is not guaranteed to yield an outcome. Moreover, while ABA frameworks without preferences allow for a graph-based representation in Dung-style frameworks, an according instantiation for general ABA+ frameworks has not been established so far. In this work, we tackle both issues: First, we develop a novel abstract argumentation formalism based on set-to-set attacks. We show that our so-called Hyper Argumentation Frameworks (HYPAFs) capture ABA+. Second, we propose relaxed variants of complete and grounded semantics for HYPAFs that yield an extension for all frameworks by design, while still faithfully generalizing the established semantics of Dung-style Argumentation Frameworks. We exploit the newly established correspondence between ABA+ and HYPAFs to obtain variants for grounded and complete ABA+ semantics that are guaranteed to yield an outcome. Finally, we discuss basic properties and provide a complexity analysis. Along the way, we settle the computational complexity of several ABA+ semantics.","['KRR: Argumentation', 'KRR: Computational Complexity of Reasoning', 'KRR: Nonmonotonic Reasoning']",[],"['Yannis Dimopoulos', 'Wolfgang Dvorak', 'Matthias König', 'Anna Rapberger', 'Markus Ulbricht', 'Stefan Woltran']","['University of Cyprus, Department of Computer Science', 'TU Wien, Institute of Logic and Computation', 'TU Wien, Institute of Logic and Computation', 'Imperial College London, Department of Computing', 'Leipzig University, Department of Computer Science', 'TU Wien, Institute of Logic and Computation']","['Cyprus', 'Austria', 'Austria', 'United Kingdom', 'Austria', 'Austria']"
https://ojs.aaai.org/index.php/AAAI/article/view/28919,Transparency & Explainability,Towards Epistemic-Doxastic Planning with Observation and Revision,"Epistemic planning is useful in situations where multiple agents have different knowledge and beliefs about the world, such as in robot-human interaction. One aspect that has been largely neglected in the literature is planning with observations in the presence of false beliefs. This is a particularly challenging problem because it requires belief revision. We introduce a simple specification language for reasoning about actions with knowledge and belief. We demonstrate our approach on well-known false-belief tasks such as the Sally-Anne Task and compare it to other action languages. Our logic leads to an epistemic planning formalism that is expressive enough to model second-order false-belief tasks, yet has the same computational complexity as classical planning.","['KRR: Reasoning with Beliefs', 'KRR: Action', 'Change', 'and Causality']",[],"['Thorsten Engesser', 'Andreas Herzig', 'Elise Perrotin']","['IRIT, Toulouse, France', 'IRIT, CNRS, Toulouse, France', 'CRIL, CNRS, Lens, France']","['France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/28920,Transparency & Explainability,Dynamic Tangled Derivative Logic of Metric Spaces,"Dynamical systems are abstract models of interaction between space and time. They are often used in fields such as physics and engineering to understand complex processes, but due to their general nature, they have found applications for studying computational processes, interaction in multi-agent systems, machine learning algorithms and other computer science related phenomena. In the vast majority of applications, a dynamical system consists of the action of a continuous `transition function' on a metric space. In this work, we consider decidable formal systems for reasoning about such structures. Spatial logics can be traced back to the 1940's, but our work follows a more dynamic turn that these logics have taken due to two recent developments: the study of the topological mu-calculus, and the the integration of linear temporal logic with logics based on the Cantor derivative. In this paper, we combine dynamic topological logics based on the Cantor derivative and the `next point in time' operators with an expressively complete fixed point operator to produce a combination of the topological mu-calculus with linear temporal logic. We show that the resulting logics are decidable and have a natural axiomatisation. Moreover, we prove that these logics are complete for interpretations on the Cantor space, the rational numbers, and subspaces thereof.","['KRR: Geometric', 'Spatial', 'and Temporal Reasoning', 'GTEP: Coordination and Collaboration', 'KRR: Action', 'Change', 'and Causality', 'MAS: Agent-Based Simulation and Emergent Behavior', 'MAS: Coordination and Collaboration', 'PRS: Model-Based Reasoning', 'PRS: Optimization of Spatio-temporal Systems']",[],"['David Fernández-Duque', 'Yoàv Montacute']","['University of Barcelona', 'University of Cambridge']","['Spain', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28922,Transparency & Explainability,Linear-Time Verification of Data-Aware Processes Modulo Theories via Covers and Automata,"The need to model and analyse dynamic systems operating over complex data is ubiquitous in AI and neighboring areas, in particular business process management. Analysing such data-aware systems is a notoriously difficult problem, as they are intrinsically infinite-state. Existing approaches work for specific datatypes, and/or limit themselves to the verification of safety properties. In this paper, we lift both such limitations, studying for the first time linear-time verification for so-called data-aware processes modulo theories (DMTs), from the foundational and practical point of view. The DMT model is very general, as it supports processes operating over variables that can store arbitrary types of data, ranging over infinite domains and equipped with domain-specific predicates. Specifically, we provide four contributions. First, we devise a semi-decision procedure for linear-time verification of DMTs, which works for a very large class of datatypes obeying to mild model-theoretic assumptions. The procedure relies on a unique combination of automata-theoretic and cover computation techniques to respectively deal with linear-time properties and datatypes. Second, we identify an abstract, semantic property that guarantees the existence of a faithful finite-state abstraction of the original system, and show that our method becomes a decision procedure in this case. Third, we identify concrete, checkable classes of systems that satisfy this property, generalising several results in the literature. Finally, we present an implementation and an experimental evaluation over a benchmark of real-world data-aware business processes.","['KRR: Action', 'Change', 'and Causality', 'KRR: Automated Reasoning and Theorem Proving', 'KRR: Geometric', 'Spatial', 'and Temporal Reasoning']",[],"['Alessandro Gianola', 'Marco Montali', 'Sarah Winkler']","['INESC-ID/Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal', 'Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy', 'Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy']","['Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/28929,Transparency & Explainability,On the Expressivity of Recurrent Neural Cascades,"Recurrent Neural Cascades (RNCs) are the recurrent neural networks with no cyclic dependencies among recurrent neurons. This class of recurrent networks has received a lot of attention in practice. Besides training methods for a fixed architecture such as backpropagation, the cascade architecture naturally allows for constructive learning methods, where recurrent nodes are added incrementally one at a time, often yielding smaller networks. Furthermore, acyclicity amounts to a structural prior that even for the same number of neurons yields a more favourable sample complexity compared to a fully-connected architecture. A central question is whether the advantages of the cascade architecture come at the cost of a reduced expressivity. We provide new insights into this question. We show that the regular languages captured by RNCs with sign and tanh activation with positive recurrent weights are the star-free regular languages. In order to establish our results we developed a novel framework where capabilities of RNCs are assessed by analysing which semigroups and groups a single neuron is able to implement. A notable implication of our framework is that RNCs can achieve the expressivity of all regular languages by introducing neurons that can implement groups.","['KRR: Geometric', 'Spatial', 'and Temporal Reasoning', 'KRR: Action', 'Change', 'and Causality', 'KRR: Other Foundations of Knowledge Representation & Reasoning', 'ML: Learning Theory', 'ML: Neuro-Symbolic Learning', 'ML: Other Foundations of Machine Learning', 'ML: Time-Series/Data Streams']",[],"['Nadezda Alexandrovna Knorozova', 'Alessandro Ronca']","['RelationalAI', 'University of Oxford']","['United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28933,Transparency & Explainability,Abstraction of Situation Calculus Concurrent Game Structures,"We present a general framework for abstracting agent behavior in multi-agent synchronous games in the situation calculus, which provides a first-order representation of the state and allows us to model how plays depend on the data and objects involved.  We represent such games as action theories of a special form called situation calculus synchronous game structures (SCSGSs), in which we have a single action ""tick"" whose effects depend on the combination of moves selected by the players.  In our framework, one specifies both an abstract SCSGS and a concrete SCSGS, as well as a refinement mapping that specifies how each abstract move is implemented by a Golog program defined over the concrete SCSGS.  We define notions of sound and complete abstraction with respect to a mapping over such SCSGS.  To express strategic properties on the abstract and concrete games we adopt a first-order variant of alternating-time mu-calculus mu-ATL-FO.  We show that we can exploit abstraction in verifying mu-ATL-FO properties of SCSGSs under the assumption that agents can always execute abstract moves to completion even if not fully controlling their outcomes.","['KRR: Action', 'Change', 'and Causality', 'MAS: Agent/AI Theories and Architectures']",[],"['Yves Lesperance', 'Giuseppe De Giacomo', 'Maryam Rostamigiv', 'Shakil M. Khan']","['York University, Toronto, ON, Canada', 'University Oxford, Oxford, UK', 'University of Regina, Regina, SK, Canada', 'University of Regina, Regina, SK, Canada']","['Canada', 'Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/28937,Transparency & Explainability,A General Theoretical Framework for Learning Smallest Interpretable Models,"We develop a general algorithmic framework that allows us to obtain fixed-parameter tractability for computing smallest symbolic models that represent given data. Our framework applies to all ML model types that admit a certain extension property. By showing this extension property for decision trees, decision sets, decision lists, and binary decision diagrams, we obtain that minimizing these fundamental model types is fixed-parameter tractable. Our framework even applies to ensembles, which combine individual models by majority decision.","['KRR: Computational Complexity of Reasoning', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Sebastian Ordyniak', 'Giacomo Paesani', 'Mateusz Rychlicki', 'Stefan Szeider']","['University of Leeds', 'University of Leeds', 'University of Leeds', 'TU Wien']","['United Kingdom', 'United Kingdom', 'United Kingdom', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/28936,Transparency & Explainability,Auditable Algorithms for Approximate Model Counting,"The problem of model counting, i.e., counting satisfying assignments of a Boolean formula, is a fundamental problem in computer science, with diverse applications. Given #P-hardness of the problem, many algorithms have been developed over the years to provide an approximate model count. Recently, building on the practical success of SAT-solvers used as NP oracles, the focus has shifted from theory to practical implementations of such algorithms. This has brought to focus new challenges. In this paper, we consider one such challenge – that of auditable deterministic approximate model counters wherein a counter should also generate a certificate, which allows a user (often with limited computational power) to independently audit whether the count returned by an invocation of the algorithm is indeed within the promised bounds.   We start by examining a celebrated approximate model counting algorithm due to Stockmeyer that uses polynomially many calls to a \Sigma^2_P oracle, and show that it can be audited via a \Pi^2_P formula on (n^2 log^2 n) variables, where n is the number of variables in the original formula. Since n is often large (10’s to 100’s of thousands) for typical instances, we ask if the count of variables in the certificate formula can be reduced – a critical question towards potential implementation. We show that this improvement in certification can be achieved with a tradeoff in the counting algorithm’s complexity. Specifically, we develop new deterministic approximate model counting algorithms that invoke a \Sigma^3_P oracle, but can be certified using a \Pi^2_P formula on fewer variables: our final algorithm uses just (n log n) variables.  Our study demonstrates that one can simplify certificate checking significantly if we allow the counting algorithm to access a slightly more powerful oracle. We believe this shows for the first time how the audit complexity can be traded for the complexity of approximate counting.","['KRR: Other Foundations of Knowledge Representation & Reasoning', 'CSO: Other Foundations of Constraint Satisfaction', 'KRR: Automated Reasoning and Theorem Proving', 'KRR: Computational Complexity of Reasoning', 'RU: Other Foundations of Reasoning under Uncertainty']",[],"['Kuldeep S. Meel', 'Supratik Chakraborty', 'S. Akshay']","['University of Toronto, Canada', 'Indian Institute of Technology Bombay, Mumbai, India', 'Indian Institute of Technology Bombay, Mumbai, India']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/28938,Security,Reinforcement Learning and Data-Generation for Syntax-Guided Synthesis,"Program synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis (SyGuS) this specification is a combination of a syntactic template and a logical formula, and the result is guaranteed to satisfy both. We present a reinforcement-learning guided algorithm for SyGuS which uses Monte-Carlo Tree Search (MCTS) to search the space of candidate solutions. Our algorithm learns policy and value functions which, combined with the upper confidence bound for trees, allow it to balance exploration and exploitation. A common challenge in applying machine learning approaches to syntax-guided synthesis is the scarcity of training data. To address this, we present a method for automatically generating training data for SyGuS based on anti-unification of existing first-order satisfiability problems, which we use to train our MCTS policy. We implement and evaluate this setup and demonstrate that learned policy and value improve the synthesis performance over a baseline by over 26 percentage points in the training and testing sets. Our tool outperforms state-of-the-art tool cvc5 on the training set and performs comparably in terms of the total number of problems solved on the testing set (solving 23% of the benchmarks on which cvc5 fails). We make our data set publicly available, to enable further application of machine learning methods to the SyGuS problem.","['KRR: Automated Reasoning and Theorem Proving', 'APP: Security', 'CSO: Other Foundations of Constraint Satisfaction', 'CSO: Satisfiability', 'CSO: Satisfiability Modulo Theories']",[],"['Julian Parsert', 'Elizabeth Polgreen']","['University of Oxford\nUniversity of Edinburgh', 'University of Edinburgh']","['United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28939,Transparency & Explainability,Adaptive Reactive Synthesis for LTL and LTLf Modulo Theories,"Reactive synthesis is the process of generate correct con- trollers from temporal logic specifications. Typically, synthesis is restricted to Boolean specifications in LTL. Recently, a Boolean abstraction technique allows to translate LTLT specifications that contain literals in theories into equi-realizable LTL specifications, but no full synthesis procedure exists yet. In synthesis modulo theories, the system receives valuations of environment variables (from a first-order theory T ) and outputs valuations of system variables from T . In this paper, we address how to syntheize a full controller using a combination of the static Boolean controller obtained from the Booleanized LTL specification together with on-the-fly queries to a solver that produces models of satisfiable existential T formulae. This is the first synthesis method for LTL modulo theories. Additionally, our method can produce adaptive responses which increases explainability and can improve runtime properties like performance. Our approach is applicable to both LTL modulo theories and LTLf modulo theories.","['KRR: Geometric', 'Spatial', 'and Temporal Reasoning', 'KRR: Automated Reasoning and Theorem Proving']",[],"['Andoni Rodríguez', 'César Sánchez']","['IMDEA Software Institute, Madrid, Spain\nUniversidad Politécnica de Madrid, Madrid, Spain', 'IMDEA Software Institute, Madrid, Spain']","['Spain', 'Spain']"
https://ojs.aaai.org/index.php/AAAI/article/view/28944,Security,Non-flat ABA Is an Instance of Bipolar Argumentation,"Assumption-based Argumentation (ABA) is a well-known structured argumentation formalism, whereby arguments and attacks between them are drawn from rules, defeasible assumptions and their contraries.  A common restriction imposed on ABA frameworks (ABAFs) is that they are flat, i.e. each of the defeasible assumptions can only be assumed, but not derived. While it is known that flat ABAFs can be translated into abstract argumentation frameworks (AFs) as proposed by Dung, no translation exists from general, possibly non-flat ABAFs into any kind of abstract argumentation formalism.  In this paper, we close this gap and show that bipolar AFs (BAFs) can instantiate general ABAFs. To this end we develop suitable, novel BAF semantics which borrow from the notion of deductive support. We investigate basic properties of our BAFs, including computational complexity, and prove the desired relation to ABAFs under several semantics.","['KRR: Argumentation', 'KRR: Computational Complexity of Reasoning', 'KRR: Nonmonotonic Reasoning']",[],"['Markus Ulbricht', 'Nico Potyka', 'Anna Rapberger', 'Francesca Toni']","['Leipzig University', 'Cardiff University', 'Imperial College London', 'Imperial College London']","['Austira', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28950,Fairness & Bias,No Prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation,"Ensuring fairness in Recommendation Systems (RSs) across demographic groups is critical due to the increased integration of RSs in applications such as personalized healthcare, finance, and e-commerce. Graph-based RSs play a crucial role in capturing intricate higher-order interactions among entities. However, integrating these graph models into the Federated Learning (FL) paradigm with fairness constraints poses formidable challenges as this requires access to the entire interaction graph and sensitive user information (such as gender, age, etc.) at the central server. This paper addresses the pervasive issue of inherent bias within RSs for different demographic groups without compromising the privacy of sensitive user attributes in FL environment with the graph-based model. To address the group bias, we propose F2PGNN (Fair Federated Personalized Graph Neural Network), a novel framework that leverages the power of Personalized Graph Neural Network (GNN) coupled with fairness considerations. Additionally, we use differential privacy techniques to fortify privacy protection. Experimental evaluation on three publicly available datasets showcases the efficacy of F2PGNN in mitigating group unfairness by 47% ∼ 99% compared to the state-of-the-art while preserving privacy and maintaining the utility. The results validate the significance of our framework in achieving equitable and personalized recommendations using GNN within the FL landscape. Source code is at: https://github.com/nimeshagrawal/F2PGNN-AAAI24","['ML: Distributed Machine Learning & Federated Learning', 'ML: Graph-based Machine Learning', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Privacy', 'DMKM: Recommender Systems']",[],"['Nimesh Agrawal', 'Anuj Kumar Sirohi', 'Sandeep Kumar', 'Jayadeva']","['Department of Electrical Engineering, Indian Institute of Technology, Delhi, India', 'Yardi School of Artificial Intelligence, Indian Institute of Technology, Delhi, India', 'Department of Electrical Engineering, Indian Institute of Technology, Delhi, India\nYardi School of Artificial Intelligence, Indian Institute of Technology, Delhi, India', 'Department of Electrical Engineering, Indian Institute of Technology, Delhi, India\nYardi School of Artificial Intelligence, Indian Institute of Technology, Delhi, India']","['India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/28959,Fairness & Bias,Transfer and Alignment Network for Generalized Category Discovery,"Generalized Category Discovery (GCD) is a crucial real-world task that aims to recognize both known and novel categories from an unlabeled dataset by leveraging another labeled dataset with only known categories. Despite the improved performance on known categories, current methods perform poorly on novel categories. We attribute the poor performance to two reasons: biased knowledge transfer between labeled and unlabeled data and noisy representation learning on the unlabeled data. The former leads to unreliable estimation of learning targets for novel categories and the latter hinders models from learning discriminative features. To mitigate these two issues, we propose a Transfer and Alignment Network (TAN), which incorporates two knowledge transfer mechanisms to calibrate the biased knowledge and two feature alignment mechanisms to learn discriminative features. Specifically, we model different categories with prototypes and transfer the prototypes in labeled data to correct model bias towards known categories. On the one hand, we pull instances with known categories in unlabeled data closer to these prototypes to form more compact clusters and avoid boundary overlap between known and novel categories. On the other hand, we use these prototypes to calibrate noisy prototypes estimated from unlabeled data based on category similarities, which allows for more accurate estimation of prototypes for novel categories that can be used as reliable learning targets later. After knowledge transfer, we further propose two feature alignment mechanisms to acquire both instance- and category-level knowledge from unlabeled data by aligning instance features with both augmented features and the calibrated prototypes, which can boost model performance on both known and novel categories with less noise. Experiments on three benchmark datasets show that our model outperforms SOTA methods, especially on novel categories. Theoretical analysis is provided for an in-depth understanding of our model in general. Our code and data are available at https://github.com/Lackel/TAN.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Wenbin An', 'Feng Tian', 'Wenkai Shi', 'Yan Chen', 'Yaqiang Wu', 'Qianying Wang', 'Ping Chen']","[""School of Automation Science and Engineering, Xi'an Jiaotong University\nNational Engineering Laboratory for Big Data Analytics"", ""School of Computer Science and Technology, Xi'an Jiaotong University\nNational Engineering Laboratory for Big Data Analytics"", ""School of Automation Science and Engineering, Xi'an Jiaotong University\nNational Engineering Laboratory for Big Data Analytics"", ""School of Computer Science and Technology, Xi'an Jiaotong University"", 'Lenovo Research', 'Lenovo Research', 'Department of Engineering, University of Massachusetts Boston']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28960,Fairness & Bias,Fluctuation-Based Adaptive Structured Pruning for Large Language Models,"Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive Structured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP.","['ML: Learning on the Edge & Model Compression', 'NLP: (Large) Language Models']",[],"['Yongqi An', 'Xu Zhao', 'Tao Yu', 'Ming Tang', 'Jinqiao Wang']","['Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nObjecteye Inc.', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nObjecteye Inc.\nWuhan AI Research']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28958,Security,Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift,"Diffusion models (DM) have become state-of-the-art generative models because of their capability of generating high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies.  When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image  (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on over hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.","['ML: Adversarial Learning & Robustness', 'CV: Adversarial Attacks & Robustness', 'ML: Deep Generative Models & Autoencoders', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Shengwei An', 'Sheng-Yen Chou', 'Kaiyuan Zhang', 'Qiuling Xu', 'Guanhong Tao', 'Guangyu Shen', 'Siyuan Cheng', 'Shiqing Ma', 'Pin-Yu Chen', 'Tsung-Yi Ho', 'Xiangyu Zhang']","['Purdue University', 'The Chinese University of Hong Kong', 'Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', 'Purdue University', 'University of Massachusetts Amherst', 'IBM Research', 'The Chinese University of Hong Kong', 'Purdue University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', '', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28963,Security,Generating Universal Adversarial Perturbations for Quantum Classifiers,"Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints. We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and adversarial samples.","['ML: Quantum Machine Learning', 'ML: Adversarial Learning & Robustness']",[],"['Gautham Anil', 'Vishnu Vinod', 'Apurva Narayan']","['Indian Institute of Technology Madras', 'Indian Institute of Technology Madras', 'University of Western Ontario\nUniversity of British Columbia\nUniversity of Waterloo']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/28965,Privacy & Data Governance,Task-Agnostic Privacy-Preserving Representation Learning for Federated Learning against Attribute Inference Attacks,"Federated learning (FL)  has been widely studied recently due to its property to collaboratively train data from different devices without sharing the raw  data. Nevertheless, recent studies show that an adversary can still be possible to infer private information about devices' data, e.g., sensitive attributes such as income, race, and sexual orientation. To mitigate the attribute inference attacks, various existing privacy-preserving FL methods can be adopted/adapted. However, all these existing methods have key limitations: they need to know the FL task in advance, or have intolerable computational overheads or utility losses, or do not have provable privacy guarantees.   We address these issues and design a task-agnostic privacy-preserving presentation learning method for FL (TAPPFL) against attribute inference attacks. TAPPFL is formulated via information theory. Specifically,  TAPPFL has two mutual information goals, where one goal learns task-agnostic data representations that contain the least information about the private attribute in each device's data, and the other goal ensures the learnt data representations include as much information as possible about the device data to maintain FL utility. We also derive privacy guarantees of TAPPFL against worst-case attribute inference attacks, as well as the inherent tradeoff between utility preservation and privacy protection. Extensive results on multiple datasets and applications validate the effectiveness of TAPPFL to protect data privacy, maintain the FL utility, and be efficient as well.  Experimental results also show that TAPPFL outperforms the existing defenses.","['ML: Adversarial Learning & Robustness', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Caridad Arroyo Arevalo', 'Sayedeh Leila Noorbakhsh', 'Yun Dong', 'Yuan Hong', 'Binghui Wang']","['Illinois Institute or Technology', 'Illinois institute of technology', 'Benedictine University', 'University of Connecticut', 'Illinois Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28968,Transparency & Explainability,Taming Binarized Neural Networks and Mixed-Integer Programs,"There has been a great deal of recent interest in binarized neural networks, especially because of their explainability. At the same time, automatic differentiation algorithms such as backpropagation fail for binarized neural networks, which limits their applicability.  We show that binarized neural networks admit a tame representation by reformulating the problem of training binarized neural networks as a subadditive dual of a mixed-integer program, which we show to have nice properties. This makes it possible to use the framework of Bolte et al. for implicit differentiation, which offers the possibility for practical implementation of backpropagation in the context of binarized neural networks.   This approach could also be used for a broader class of mixed-integer programs, beyond the training of binarized neural networks, as encountered in symbolic approaches to AI and beyond.","['ML: Deep Learning Theory', 'ML: Optimization']",[],"['Johannes Aspman', 'Georgios Korpas', 'Jakub Marecek']","['Czech Technical University', 'HSBC\nCzech Technical University', 'Czech Technical University']","['Czech Republic', 'Czech Republic', 'Czech Republic']"
https://ojs.aaai.org/index.php/AAAI/article/view/28971,Fairness & Bias,FairTrade: Achieving Pareto-Optimal Trade-Offs between Balanced Accuracy and Fairness in Federated Learning,"As Federated Learning (FL) gains prominence in distributed machine learning applications, achieving fairness without compromising predictive performance becomes paramount. The data being gathered from distributed clients in an FL environment often leads to class imbalance. In such scenarios, balanced accuracy rather than accuracy is the true representation of model performance. However, most state-of-the-art fair FL methods report accuracy as the measure of performance,  which can lead to misguided interpretations of the model's effectiveness to mitigate discrimination. To the best of our knowledge, this work presents the first attempt towards achieving Pareto-optimal trade-offs between balanced accuracy and fairness in a federated environment (FairTrade). By utilizing multi-objective optimization, the framework negotiates the intricate balance between model's balanced accuracy and fairness. The framework's agnostic design adeptly accommodates both statistical and causal fairness notions, ensuring its adaptability across diverse FL contexts. We provide empirical evidence of our framework's efficacy through extensive experiments on five real-world datasets and comparisons with six baselines. The empirical results underscore the potential of our framework in improving the trade-off between fairness and balanced accuracy in FL applications.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Distributed Machine Learning & Federated Learning', 'ML: Optimization', 'ML: Causal Learning']",[],"['Maryam Badar', 'Sandipan Sikdar', 'Wolfgang Nejdl', 'Marco Fisichella']","['L3S Research Center, Leibniz University Hannover, Germany', 'L3S Research Center, Leibniz University Hannover, Germany', 'L3S Research Center, Leibniz University Hannover, Germany', 'L3S Research Center, Leibniz University Hannover, Germany']","['Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/28979,Transparency & Explainability,TREE-G: Decision Trees Contesting Graph Neural Networks,"When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that in- corporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks. In these experiments, TREE-G consistently outperforms other tree-based models and often outperforms other graph-learning algorithms such as Graph Neural Networks (GNNs) and Graph Kernels, sometimes by large margins. Moreover, TREE-Gs models and their predic tions can be explained and visualized.",['ML: Graph-based Machine Learning'],[],"['Maya Bechler-Speicher', 'Amir Globerson', 'Ran Gilad-Bachrach']","['Blavatnik School of Computer Science, Tel-Aviv University', 'Blavatnik School of Computer Science, Tel-Aviv University', 'Department of Bio-Medical Engineering and Edmond J. Safra Center for Bioinformatics,Tel-Aviv University']","['Israel', 'Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/28981,Fairness & Bias,Simplicity Bias in Overparameterized Machine Learning,"A thorough theoretical understanding of the surprising generalization ability of deep networks (and other overparameterized models) is still lacking. Here we demonstrate that simplicity bias is a major phenomenon to be reckoned with in overparameterized machine learning. In addition to explaining the outcome of simplicity bias, we also study its source: following concrete rigorous examples, we argue that (i) simplicity bias can explain generalization in overparameterized learning models such as neural networks; (ii) simplicity bias and excellent generalization are optimizer-independent, as our example shows, and although the optimizer affects training, it is not the driving force behind simplicity bias; (iii) simplicity bias in pre-training models, and subsequent posteriors, is universal and stems from the subtle fact that uniformly-at-random constructed priors are not uniformly-at-random sampled ; and (iv) in neural network models, the biasing mechanism in wide (and shallow) networks is different from the biasing mechanism in deep (and narrow) networks.","['ML: Learning Theory', 'ML: Probabilistic Circuits and Graphical Models', 'ML: Deep Learning Theory']",[],['Yakir Berchenko'],"['Ben-Gurion University of the Negev, Department of Industrial Engineering and Management']",['Israel']
https://ojs.aaai.org/index.php/AAAI/article/view/28982,Transparency & Explainability,Maximizing the Success Probability of Policy Allocations in Online Systems,"The effectiveness of advertising in e-commerce largely depends on the ability of merchants to bid on and win impressions for their targeted users. The bidding procedure is highly complex due to various factors such as market competition, user behavior, and the diverse objectives of advertisers. In this paper we consider the problem at the level of user timelines instead of individual bid requests, manipulating full policies (i.e. pre-defined bidding strategies) and not bid values. In order to optimally allocate policies to users, typical multiple treatments allocation methods solve knapsack-like problems which aim at maximizing an expected value under constraints. In the specific context of online advertising, we argue that optimizing for the probability of success is a more suited objective than expected value maximization, and we introduce the SuccessProbaMax algorithm that aims at finding the policy allocation which is the most likely to outperform a fixed reference policy. Finally, we conduct comprehensive experiments both on synthetic and real-world data to evaluate its performance. The results demonstrate that our proposed algorithm outperforms conventional expected-value maximization algorithms in terms of success rate.","['ML: Optimization', 'APP: Web', 'CSO: Applications', 'CSO: Constraint Optimization', 'ML: Applications', 'ML: Calibration & Uncertainty Quantification', 'ML: Causal Learning', 'RU: Applications', 'RU: Causality', 'RU: Stochastic Optimization', 'SO: Metareasoning and Metaheuristics', 'SO: Non-convex Optimization']",[],"['Artem Betlei', 'Mariia Vladimirova', 'Mehdi Sebbar', 'Nicolas Urien', 'Thibaud Rahier', 'Benjamin Heymann']","['Criteo AI Lab, France', 'Criteo AI Lab, France', 'Criteo Ad Landscape, France', 'Criteo Ad Landscape, France', 'Criteo AI Lab, France', 'Criteo AI Lab, France']","['France', 'France', '', '', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/28986,Security,Axiomatic Aggregations of Abductive Explanations,"The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue --- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of them uniquely satisfies a set of desirable properties. We also evaluate them on multiple datasets and show that these explanations are robust to the attacks that fool SHAP and LIME.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'GTEP: Cooperative Game Theory', 'CSO: Satisfiability Modulo Theories', 'CSO: Satisfiability', 'KRR: Diagnosis and Abductive Reasoning']",[],"['Gagan Biradar', 'Yacine Izza', 'Elita Lobo', 'Vignesh Viswanathan', 'Yair Zick']","['University of Massachusetts Amherst', 'National University of Singapore', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28990,Transparency & Explainability,Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples,"Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted adversarial examples, which are generated through either well-conceived L_p-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer where to attack. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate Counterfactual ADversarial Examples to answer how to attack. The empirical results demonstrate CADE's effectiveness, as evidenced by its competitive performance across diverse attack scenarios, including white-box, transfer-based, and random intervention attacks.","['ML: Adversarial Learning & Robustness', 'ML: Causal Learning']",[],"['Ruichu Cai', 'Yuxuan Zhu', 'Jie Qiao', 'Zefeng Liang', 'Furui Liu', 'Zhifeng Hao']","['School of Computer Science, Guangdong University of Technology, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'Zhejiang Lab, Hangzhou, China', 'College of Science, Shantou University, Shantou, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28994,Fairness & Bias,Mixup-Induced Domain Extrapolation for Domain Generalization,"Domain generalization aims to learn a well-performed classifier on multiple source domains for unseen target domains under domain shift. Domain-invariant representation (DIR) is an intuitive approach and has been of great concern. In practice, since the targets are variant and agnostic, only a few sources are not sufficient to reflect the entire domain population, leading to biased DIR. Derived from PAC-Bayes framework, we provide a novel generalization bound involving the number of domains sampled from the environment (N) and the radius of the Wasserstein ball centred on the target (r), which have rarely been considered before. Herein, we can obtain two natural and significant findings: when N increases, 1) the gap between the source and target sampling environments can be gradually mitigated; 2) the target can be better approximated within the Wasserstein ball. These findings prompt us to collect adequate domains against domain shift. For seeking convenience, we design a novel yet simple Extrapolation Domain strategy induced by the Mixup scheme, namely EDM. Through a reverse Mixup scheme to generate the extrapolated domains, combined with the interpolated domains, we expand the interpolation space spanned by the sources, providing more abundant domains to increase sampling intersections to shorten r. Moreover, EDM is easy to implement and be plugged-and-played. In experiments, EDM has been plugged into several methods in both closed and open set settings, achieving up to 5.73% improvement.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Classification and Regression']",[],"['Meng Cao', 'Songcan Chen']","['College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics\nMIIT Key Laboratory of Pattern Analysis and Machine Intelligence', 'College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics\nMIIT Key Laboratory of Pattern Analysis and Machine Intelligence']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28996,Fairness & Bias,Learning to Unlearn: Instance-Wise Unlearning for Pre-trained Classifiers,"Since the recent advent of regulations for data protection (e.g., the General Data Protection Regulation), there has been increasing demand in deleting information learned from sensitive data in pre-trained models without retraining from scratch. The inherent vulnerability of neural networks towards adversarial attacks and unfairness also calls for a robust method to remove or correct information in an instance-wise fashion, while retaining the predictive performance across remaining data. To this end, we consider instance-wise unlearning, of which the goal is to delete information on a set of instances from a pre-trained model, by either misclassifying each instance away from its original prediction or relabeling the instance to a different label. We also propose two methods that reduce forgetting on the remaining data: 1) utilizing adversarial examples to overcome forgetting at the representation-level and 2) leveraging weight importance metrics to pinpoint network parameters guilty of propagating unwanted information. Both methods only require the pre-trained model and data instances to forget, allowing painless application to real-life settings where the entire training set is unavailable. Through extensive experimentation on various image classification benchmarks, we show that our approach effectively preserves knowledge of remaining data while unlearning given instances in both single-task and continual unlearning scenarios.","['ML: Classification and Regression', 'CV: Adversarial Attacks & Robustness', 'CV: Applications', 'CV: Learning & Optimization for CV', 'CV: Low Level & Physics-based Vision', 'CV: Other Foundations of Computer Vision', 'CV: Representation Learning for Vision', 'ML: Adversarial Learning & Robustness', 'ML: Other Foundations of Machine Learning', 'ML: Privacy']",[],"['Sungmin Cha', 'Sungjun Cho', 'Dasol Hwang', 'Honglak Lee', 'Taesup Moon', 'Moontae Lee']","['New York University', 'LG AI Research', 'LG AI Research', 'LG AI Research', 'ASRI / INMC / Seoul National University', 'LG AI Research\nUniversity of Illinois at Chicago']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28999,Transparency & Explainability,Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision,"Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our method across various layers discloses distinct distributed representations over the layers, which provides deeper insights into the internal mechanisms of the deep learning model.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Wonjoon Chang', 'Dahee Kwon', 'Jaesik Choi']","['KAIST', 'KAIST', 'KAIST\nINEEJI']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29001,Fairness & Bias,Offline Model-Based Optimization via Policy-Guided Gradient Search,"Offline optimization is an emerging problem in many experimental engineering domains including protein, drug or aircraft design, where online experimentation to collect evaluation data is too expensive or dangerous. To avoid that, one has to optimize an unknown function given only its offline evaluation at a fixed set of inputs. A naive solution to this problem is to learn a surrogate model of the unknown function and optimize this surrogate instead. However, such a naive optimizer is prone to erroneous overestimation of the surrogate (possibly due to over-fitting on a biased sample of function evaluation) on inputs outside the offline dataset. Prior approaches addressing this challenge have primarily focused on learning robust surrogate models. However, their search strategies are derived from the surrogate model rather than the actual offline data. To fill this important gap, we introduce a new learning-to-search perspective for offline optimization by reformulating it as an offline reinforcement learning problem. Our proposed policy-guided gradient search approach explicitly learns the best policy for a given surrogate model created from the offline data. Our empirical results on multiple benchmarks demonstrate that the learned optimization policy can be combined with existing offline surrogates to significantly improve the optimization performance.","['ML: Other Foundations of Machine Learning', 'ML: Optimization', 'SO: Learning to Search']",[],"['Yassine Chemingui', 'Aryan Deshwal', 'Trong Nghia Hoang', 'Janardhan Rao Doppa']","['Washington State University', 'Washington State University', 'Washington State University', 'Washington State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29005,Privacy & Data Governance,A Generalized Shuffle Framework for Privacy Amplification: Strengthening Privacy Guarantees and Enhancing Utility,"The shuffle model of local differential privacy is an advanced method of privacy amplification designed to enhance privacy protection with high utility.  It achieves this by randomly shuffling  sensitive data, making linking individual data points to specific individuals more challenging. However, most existing studies have focused on the shuffle model based on (ε0,0)-Locally Differentially Private (LDP) randomizers, with limited consideration for complex scenarios such as (ε0,δ0)-LDP or personalized LDP (PLDP).  This hinders a comprehensive understanding of the shuffle model's potential and limits its application in various settings. To bridge this research gap, we propose a generalized shuffle framework that can be applied to PLDP setting. This generalization allows for a broader exploration of the privacy-utility trade-off and facilitates the design of privacy-preserving analyses in diverse contexts. We prove that the shuffled PLDP process approximately preserves μ-Gaussian Differential Privacy with  μ = O(1/√n). This approach allows us to avoid the limitations and potential inaccuracies associated with inequality estimations. To strengthen the privacy guarantee, we improve the lower bound by utilizing hypothesis testing instead of relying on rough estimations like the Chernoff bound or Hoeffding's inequality. Furthermore, extensive comparative evaluations clearly show that our approach outperforms existing methods in achieving strong central privacy guarantees while preserving the utility of the global model. We have also carefully designed corresponding algorithms for average function, frequency estimation, and stochastic gradient descent.",['ML: Privacy'],[],"['E Chen', 'Yang Cao', 'Yifei Ge']","['Zhejiang Lab', 'Hokkaido University', ""Xi'an Jiaotong-Liverpool University""]","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29010,Privacy & Data Governance,On Disentanglement of Asymmetrical Knowledge Transfer for Modality-Task Agnostic Federated Learning,"There has been growing concern regarding data privacy during the development and deployment of Multimodal Foundation Models for Artificial General Intelligence (AGI), while Federated Learning (FL) allows multiple clients to collaboratively train models in a privacy-preserving manner. This paper formulates and studies Modality-task Agnostic Federated Learning (AFL) to pave the way toward privacy-preserving AGI. A unique property of AFL is the asymmetrical knowledge relationships among clients due to modality gaps, task gaps, and domain shifts between clients. This raises a challenge in learning an optimal inter-client information-sharing scheme that maximizes positive transfer and minimizes negative transfer for AFL. However, prior FL methods, mostly focusing on symmetrical knowledge transfer, tend to exhibit insufficient positive transfer and fail to fully avoid negative transfer during inter-client collaboration. To address this issue, we propose DisentAFL, which leverages a two-stage Knowledge Disentanglement and Gating mechanism to explicitly decompose the original asymmetrical inter-client information-sharing scheme into several independent symmetrical inter-client information-sharing schemes, each of which corresponds to certain semantic knowledge type learned from the local tasks. Experimental results demonstrate the superiority of our method on AFL than baselines.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Multimodal Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Jiayi Chen', 'Aidong Zhang']","['University of Virginia', 'University of Virginia']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29012,Fairness & Bias,Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models,"Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: federated models exhibit unreliability when faced with heterogeneous data, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the ""Assembled Projection Heads"" (APH) method for enhancing the reliability of federated models. By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates. Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging. We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks. Experimental results validate the efficacy of APH in model calibration and uncertainty estimation. Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30% additional computation cost for 100x inferences within large models.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Calibration & Uncertainty Quantification']",[],"['Jinqian Chen', 'Jihua Zhu', 'Qinghai Zheng', 'Zhongyu Li', 'Zhiqiang Tian']","[""School of Software Engineering, Xi'an Jiaotong University\nShaanxi Joint Key Laboratory for Artificial Intelligence, China"", ""School of Software Engineering, Xi'an Jiaotong University\nShaanxi Joint Key Laboratory for Artificial Intelligence, China"", 'College of Computer and Data Science, Fuzhou University', ""School of Software Engineering, Xi'an Jiaotong University\nShaanxi Joint Key Laboratory for Artificial Intelligence, China"", ""School of Software Engineering, Xi'an Jiaotong University\nShaanxi Joint Key Laboratory for Artificial Intelligence, China""]","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29013,Security,Discriminative Forests Improve Generative Diversity for Generative Adversarial Networks,"Improving the diversity of Artificial Intelligence Generated Content (AIGC) is one of the fundamental problems in the theory of generative models such as generative adversarial networks (GANs). Previous studies have demonstrated that the discriminator in GANs should have high capacity and robustness to achieve the diversity of generated data. However, a discriminator with high capacity tends to overfit and guide the generator toward collapsed equilibrium. In this study, we propose a novel discriminative forest GAN, named Forest-GAN, that replaces the discriminator to improve the capacity and robustness for modeling statistics in real-world data distribution. A discriminative forest is composed of multiple independent discriminators built on bootstrapped data. We prove that a discriminative forest has a generalization error bound, which is determined by the strength of individual discriminators and the correlations among them. Hence, a discriminative forest can provide very large capacity without any risk of overfitting, which subsequently improves the generative diversity. With the discriminative forest framework, we significantly improved the performance of AutoGAN with a new record FID of 19.27 from 30.71 on STL10 and improved the performance of StyleGAN2-ADA with a new record FID of 6.87 from 9.22 on LSUN-cat.","['ML: Adversarial Learning & Robustness', 'ML: Deep Generative Models & Autoencoders']",[],"['Junjie Chen', 'Jiahao Li', 'Chen Song', 'Bin Li', 'Qingcai Chen', 'Hongchang Gao', 'Wendy Hui Wang', 'Zenglin Xu', 'Xinghua Shi']","['Harbin Institute of Technology, Shenzhen', 'Harbin Institute of Technology, Shenzhen', 'Temple University', 'Temple University', 'Harbin Institute of Technology, Shenzhen', 'Temple University', 'Stevens Institute of Technology', 'Harbin Institute of Technology, Shenzhen', 'Temple University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29011,Security,Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning,"Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent reinforcement learning (MARL) can be extremely computationally expensive. Curriculum learning is an effective way to accelerate learning, but an under-explored dimension for generating a curriculum is the difficulty-to-learn of the subgames –games induced by starting from a specific state. In this work, we present a novel subgame curriculum learning framework for zero-sum games. It adopts an adaptive initial state distribution by resetting agents to some previously visited states where they can quickly learn to improve performance. Building upon this framework, we derive a subgame selection metric that approximates the squared distance to NE values and further adopt a particle-based state sampler for subgame generation. Integrating these techniques leads to our new algorithm, Subgame Automatic Curriculum Learning (SACL), which is a realization of the subgame curriculum learning framework. SACL can be combined with any MARL algorithm such as MAPPO. Experiments in the particle-world environment and Google Research Football environment show SACL produces much stronger policies than baselines. In the challenging hide-and-seek quadrant environment, SACL produces all four emergent stages and uses only half the samples of MAPPO with self-play. The project website is at https://sites.google.com/view/sacl-neurips.","['ML: Reinforcement Learning', 'MAS: Adversarial Agents', 'GTEP: Adversarial Learning']",[],"['Jiayu Chen', 'Zelai Xu', 'Yunfei Li', 'Chao Yu', 'Jiaming Song', 'Huazhong Yang', 'Fei Fang', 'Yu Wang', 'Yi Wu']","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Luma AI', 'Tsinghua University', 'Carnegie Mellon University', 'Tsinghua University', 'Tsinghua University\nShanghai Qi Zhi Institute']","['China', 'China', 'China', 'China', '', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29023,Security,Progressive Poisoned Data Isolation for Training-Time Backdoor Defense,"Deep Neural Networks (DNN) are susceptible to backdoor attacks where malicious attackers manipulate the model's predictions via data poisoning. It is hence imperative to develop a strategy for training a clean model using a potentially poisoned dataset. Previous training-time defense mechanisms typically employ an one-time isolation process, often leading to suboptimal isolation outcomes. In this study, we present a novel and efficacious defense method, termed Progressive Isolation of Poisoned Data (PIPD), that progressively isolates poisoned data to enhance the isolation accuracy and mitigate the risk of benign samples being misclassified as poisoned ones. Once the poisoned portion of the dataset has been identified, we introduce a selective training process to train a clean model. Through the implementation of these techniques, we ensure that the trained model manifests a significantly diminished attack success rate against the poisoned data. Extensive experiments on multiple benchmark datasets and DNN models, assessed against nine state-of-the-art backdoor attacks, demonstrate the superior performance of our PIPD method for backdoor defense. For instance, our PIPD achieves an average True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of 0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the performance of state-of-the-art methods. The code is available at https://github.com/RorschachChen/PIPD.git.",['ML: Privacy'],[],"['Yiming Chen', 'Haiwei Wu', 'Jiantao Zhou']","['University of Macau', 'University of Macau', 'University of Macau']","['Macau', 'Macau', 'Macau']"
https://ojs.aaai.org/index.php/AAAI/article/view/29022,Fairness & Bias,Exact Policy Recovery in Offline RL with Both Heavy-Tailed Rewards and Data Corruption,"We study offline reinforcement learning (RL) with heavy-tailed reward distribution and data corruption: (i) Moving beyond subGaussian reward distribution, we allow the rewards to have infinite variances; (ii) We allow corruptions where an attacker can arbitrarily modify a small fraction of the rewards and transitions in the dataset. We first derive a sufficient optimality condition for generalized Pessimistic Value Iteration (PEVI), which allows various estimators with proper confidence bounds and can be applied to multiple learning settings. In order to handle the data corruption and heavy-tailed reward setting, we prove that the trimmed-mean estimation achieves the minimax optimal error rate for robust mean estimation under heavy-tailed distributions. In the PEVI algorithm, we plug in the trimmed mean estimation and the confidence bound to solve the robust offline RL problem. Standard analysis reveals that data corruption induces a bias term in the suboptimality gap, which gives the false impression that any data corruption prevents optimal policy learning. By using the optimality condition for the generalized PEVI, we show that as long as the bias term is less than the ``action gap'', the policy returned by PEVI achieves the optimal value given sufficient data.","['ML: Reinforcement Learning', 'ML: Learning Theory']",[],"['Yiding Chen', 'Xuezhou Zhang', 'Qiaomin Xie', 'Xiaojin Zhu']","['University of Wisconsin-Madison', 'Boston University', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29025,Privacy & Data Governance,Fed-QSSL: A Framework for Personalized Federated Learning under Bitwidth and Data Heterogeneity,"Motivated by high resource costs of centralized machine learning schemes as well as data privacy concerns, federated learning (FL) emerged as an efficient alternative that relies on aggregating locally trained models rather than collecting clients' potentially private data. In practice, available resources and data distributions vary from one client to another, creating an inherent system heterogeneity that leads to deterioration of the performance of conventional FL algorithms. In this work, we present a federated quantization-based self-supervised learning scheme (Fed-QSSL) designed to address heterogeneity in FL systems. At clients' side, to tackle data heterogeneity we leverage distributed self-supervised learning while utilizing low-bit quantization to satisfy constraints imposed by local infrastructure and limited communication resources. At server's side, Fed-QSSL deploys de-quantization, weighted aggregation and re-quantization, ultimately creating models personalized to both data distribution as well as specific infrastructure of each client's device. We validated the proposed algorithm on real world datasets, demonstrating its efficacy, and theoretically analyzed impact of low-bit training on the convergence and robustness of the learned models.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Yiyue Chen', 'Haris Vikalo', 'Chianing Wang']","['The University of Texas at Austin', 'The University of Texas at Austin', 'Toyota InfoTech Lab USA']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29029,Fairness & Bias,Instrumental Variable Estimation for Causal Inference in Longitudinal Data with Time-Dependent Latent Confounders,"Causal inference from longitudinal observational data is a challenging problem due to the difficulty in correctly identifying the time-dependent confounders, especially in the presence of latent time-dependent confounders. Instrumental variable (IV) is a powerful tool for addressing the latent confounders issue, but the traditional IV technique cannot deal with latent time-dependent confounders in longitudinal studies. In this work, we propose a novel Time-dependent Instrumental Factor Model (TIFM) for time-varying causal effect estimation from data with latent time-dependent confounders. At each time-step, the proposed TIFM method employs the Recurrent Neural Network (RNN) architecture to infer latent IV, and then uses the inferred latent IV factor for addressing the confounding bias caused by the latent time-dependent confounders. We provide a theoretical analysis for the proposed TIFM method regarding causal effect estimation in longitudinal data. Extensive evaluation with synthetic datasets demonstrates the effectiveness of TIFM in addressing causal effect estimation over time. We further apply TIFM to a climate dataset to showcase the potential of the proposed method in tackling real-world problems.","['ML: Time-Series/Data Streams', 'ML: Causal Learning']",[],"['Debo Cheng', 'Ziqi Xu', 'Jiuyong Li', 'Lin Liu', 'Jixue Liu', 'Wentao Gao', 'Thuc Duy Le']","['University of South Australia', 'University of South Australia', 'University of South Australia', 'University of South Australia', 'University of South Australia', 'University of South Australia', 'University of South Australia']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29031,Fairness & Bias,FedGCR: Achieving Performance and Fairness for Federated Learning with Distinct Client Types via Group Customization and Reweighting,"To achieve better performance and greater fairness in Federated Learning (FL), much of the existing research has centered on individual clients, using domain adaptation techniques and redesigned aggregation schemes to counteract client data heterogeneity. However, an overlooked scenario exists where clients belong to distinctive groups, or, client types, in which groups of clients share similar characteristics such as device specifications or data patterns. Despite being common in group collaborations, this scenario has been overlooked in previous research, potentially leading to performance degradation and systemic biases against certain client types. To bridge this gap, we introduce Federated learning with Group Customization and Reweighting (FedGCR). FedGCR enhances both performance and fairness for FL with Distinct Client Types, consisting of a Federated Group Customization (FedGC) model to provide customization via a novel prompt tuning technique to mitigate the data disparity across different client-types, and a Federated Group Reweighting (FedGR) aggregation scheme to ensure uniform and unbiased performances between clients and between client types by a novel reweighting approach. Extensive experiment comparisons with prior FL methods in domain adaptation and fairness demonstrate the superiority of FedGCR in all metrics, including the overall accuracy and performance uniformity in both the group and the individual level. FedGCR achieves 82.74% accuracy and 12.26(↓) in performance uniformity on the Digit-Five dataset and 81.88% and 14.88%(↓) on DomainNet with a domain imbalance factor of 10, which significantly outperforms the state-of-the-art. Code is available at https://github.com/celinezheng/fedgcr.","['ML: Distributed Machine Learning & Federated Learning', 'CV: Bias', 'Fairness & Privacy', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Shu-Ling Cheng', 'Chin-Yuan Yeh', 'Ting-An Chen', 'Eliana Pastor', 'Ming-Syan Chen']","['Graduate Institute of Communication Engineering, National Taiwan University, Taiwan', 'Graduate Institute of Communication Engineering, National Taiwan University, Taiwan\nInstitute of Information Science, Academia Sinica, Taiwan', 'Department of Electrical Engineering, National Taiwan University, Taiwan\nInstitute of Information Science, Academia Sinica, Taiwan', 'Department of Control and Computer Engineering, Politecnico di Torino, Italy', 'Graduate Institute of Communication Engineering, National Taiwan University, Taiwan\nDepartment of Electrical Engineering, National Taiwan University, Taiwan']","['Taiwan', 'Taiwan', 'Taiwan', 'Taiwan', 'Taiwan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29033,Fairness & Bias,Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning,"Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive bias for deep learning on tabular data. Code is available at https://github.com/aigc-apps/AMFormer.","['ML: Feature Construction/Reformulation', 'ML: Deep Learning Algorithms', 'ML: Deep Neural Architectures and Foundation Models', 'ML: Dimensionality Reduction/Feature Selection']",[],"['Yi Cheng', 'Renjun Hu', 'Haochao Ying', 'Xing Shi', 'Jian Wu', 'Wei Lin']","['Zhejiang University', 'Alibaba Group', 'Zhejiang University', 'Alibaba', 'Zhejiang University', 'Alibaba Group']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29034,Transparency & Explainability,CUTS+: High-Dimensional Causal Discovery from Irregular Time-Series,"Causal discovery in time-series is a fundamental problem in the machine learning community, enabling causal reasoning and decision-making in complex scenarios. Recently, researchers successfully discover causality by combining neural networks with Granger causality, but their performances degrade largely when encountering high-dimensional data because of the highly redundant network design and huge causal graphs. Moreover, the missing entries in the observations further hamper the causal structural learning. To overcome these limitations, We propose CUTS+, which is built on the Granger-causality-based causal discovery method CUTS and raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN). Compared to previous methods on simulated, quasi-real, and real datasets, we show that CUTS+ largely improves the causal discovery performance on high-dimensional data with different types of irregular sampling.","['ML: Causal Learning', 'ML: Deep Learning Algorithms']",[],"['Yuxiao Cheng', 'Lianglong Li', 'Tingxiong Xiao', 'Zongren Li', 'Jinli Suo', 'Kunlun He', 'Qionghai Dai']","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Chinese PLA General Hospital', 'Tsinghua University', 'Chinese PLA General Hospital', 'Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29041,Security,Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration,"Research interests in the robustness of deep neural networks  against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions  is the main obstacle to OOD calibration, we propose to guide the scaling process by taking consistencies into account in terms of two different aspects - style and content - which are the key components that can well-represent data samples in multi-domain settings. Experimental results demonstrate that our proposed strategy outperforms existing works, achieving superior OOD calibration performance on various datasets. This can be accomplished by employing only the source domains without compromising accuracy, making our scheme directly applicable to various trustworthy AI systems.","['ML: Calibration & Uncertainty Quantification', 'CV: Adversarial Attacks & Robustness']",[],"['Wonjeong Choi', 'Jungwuk Park', 'Dong-Jun Han', 'Younghyun Park', 'Jaekyun Moon']","['Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science and Technology (KAIST)', 'Purdue University', 'Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science and Technology (KAIST)']","['South Korea', 'South Korea', 'United States', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29044,Security,Lyapunov-Stable Deep Equilibrium Models,"Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the Lyapunov stability of the DEQ model's fixed points, which enables the proposed model to resist minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we orthogonalize the layers after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models under well-known adversarial attacks, and experimental results demonstrate significant improvement in robustness. Furthermore, we show that the LyaDEQ model can be combined with other defense methods, such as adversarial training, to achieve even better adversarial robustness.","['ML: Deep Neural Architectures and Foundation Models', 'CV: Adversarial Attacks & Robustness', 'ML: Adversarial Learning & Robustness', 'ML: Classification and Regression']",[],"['Haoyu Chu', 'Shikui Wei', 'Ting Liu', 'Yao Zhao', 'Yuto Miyatake']","['Institute of Information Science, Beijing Jiaotong University\nGraduate School of Information Science and Technology, Osaka University\nBeijing Key Laboratory of Advanced Information Science and Network Technology', 'Institute of Information Science, Beijing Jiaotong University\nBeijing Key Laboratory of Advanced Information Science and Network Technology', 'School of Computer Science, Northwestern Polytechnical University', 'Institute of Information Science, Beijing Jiaotong University\nBeijing Key Laboratory of Advanced Information Science and Network Technology', 'Cybermedia Center, Osaka University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29047,Transparency & Explainability,Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction,"Since artificial intelligence has seen tremendous recent successes in many areas, it has sparked great interest in its potential for trustworthy and interpretable risk prediction. However, most models lack causal reasoning and struggle with class imbalance, leading to poor precision and recall. To address this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to transform original feature values into causal feature attributions for the specific risk prediction task. The causal feature attribution helps describe how much contribution the value of this feature can make to the risk prediction result. After the causal feature distillation, a deep neural network is applied to produce trustworthy prediction results with causal interpretability and high precision/recall. We evaluate the performance of our TDCFD method on several synthetic and real datasets, and the results demonstrate its superiority over the state-of-the-art methods regarding precision, recall, interpretability, and causality.","['ML: Classification and Regression', 'APP: Other Applications', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Zhixuan Chu', 'Mengxuan Hu', 'Qing Cui', 'Longfei Li', 'Sheng Li']","['Ant Group', 'University of Virginia', 'Ant Group', 'Ant Group', 'University of Virginia']","['China', 'United States', 'China', 'China', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29043,Security,Graph-Based Prediction and Planning Policy Network (GP3Net) for Scalable Self-Driving in Dynamic Environments Using Deep Reinforcement Learning,"Recent advancements in motion planning for Autonomous Vehicles (AVs) show great promise in using expert driver behaviors in non-stationary driving environments. However, learning only through expert drivers needs more generalizability to recover from domain shifts and near-failure scenarios due to the dynamic behavior of traffic participants and weather conditions. A deep Graph-based Prediction and Planning Policy Network (GP3Net) framework is proposed for non-stationary environments that encodes the interactions between traffic participants with contextual information and provides a decision for safe maneuver for AV. A spatio-temporal graph models the interactions between traffic participants for predicting the future trajectories of those participants. The predicted trajectories are utilized to generate a future occupancy map around the AV with uncertainties embedded to anticipate the evolving non-stationary driving environments. Then the contextual information and future occupancy maps are input to the policy network of the GP3Net framework and trained using Proximal Policy Optimization (PPO) algorithm. The proposed GP3Net performance is evaluated on standard CARLA benchmarking scenarios with domain shifts of traffic patterns (urban, highway, and mixed). The results show that the GP3Net outperforms previous state-of-the-art imitation learning-based planning models for different towns. Further, in unseen new weather conditions, GP3Net completes the desired route with fewer traffic infractions. Finally, the results emphasize the advantage of including the prediction module to enhance safety measures in non-stationary environments.","['ML: Applications', 'ROB: Motion and Path Planning', 'ROB: Behavior Learning & Control', 'APP: Mobility', 'Driving & Flight', 'ML: Reinforcement Learning', 'MAS: Modeling other Agents']",[],"['Jayabrata Chowdhury', 'Venkataramanan Shivaraman', 'Suresh Sundaram', 'PB Sujit']","['Indian Institute of Science, Bengaluru', 'Indian Institute of Science Education and Research, Bhopal', 'Indian Institute of Science, Bengaluru', 'Indian Institute of Science Education and Research, Bhopal']","['India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/29048,Fairness & Bias,Resource Efficient Deep Learning Hardware Watermarks with Signature Alignment,"Deep learning intellectual properties (IPs) are high-value assets that are frequently susceptible to theft. This vulnerability has led to significant interest in defending the field's intellectual properties from theft. Recently, watermarking techniques have been extended to protect deep learning hardware from privacy. These technique embed modifications that change the hardware's behavior when activated. In this work, we propose the first method for embedding watermarks in deep learning hardware that incorporates the owner's key samples into the embedding methodology. This improves our watermarks' reliability and efficiency in identifying the hardware over those generated using randomly selected key samples. Our experimental results demonstrate that by considering the target key samples when generating the hardware modifications, we can significantly increase the embedding success rate while targeting fewer functional blocks, decreasing the required hardware overhead needed to defend it.","['ML: Adversarial Learning & Robustness', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Joseph Clements', 'Yingjie Lao']","['Clemson University, Clemson, South Carolina, 29634\nApplied Research Associates, Albuquerque, New Mexico, 87110', 'Clemson University, Clemson, South Carolina, 29634\nTufts University, Medford, Massachusetts, 02155']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29052,Security,BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning,"Backdoor attacks in reinforcement learning (RL) have previously employed intense attack strategies to ensure attack success. However, these methods suffer from high attack costs and increased detectability. In this work, we propose a novel approach, BadRL, which focuses on conducting highly sparse backdoor poisoning efforts during training and testing while maintaining successful attacks. Our algorithm, BadRL, strategically chooses state observations with high attack values to inject triggers during training and testing, thereby reducing the chances of detection. In contrast to the previous methods that utilize sample-agnostic trigger patterns, BadRL dynamically generates distinct trigger patterns based on targeted state observations, thereby enhancing its effectiveness. Theoretical analysis shows that the targeted backdoor attack is always viable and remains stealthy under specific assumptions. Empirical results on various classic RL tasks illustrate that BadRL can substantially degrade the performance of a victim agent with minimal poisoning efforts (0.003% of total training steps) during training and infrequent attacks during testing. Code is available at: https://github.com/7777777cc/code.","['ML: Reinforcement Learning', 'ML: Adversarial Learning & Robustness']",[],"['Jing Cui', 'Yufei Han', 'Yuzhe Ma', 'Jianbin Jiao', 'Junge Zhang']","['University of Chinese Academy of Sciences', 'INRIA', 'Microsoft Azure AI', 'University of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences']","['United States', '', '', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29059,Transparency & Explainability,Self-Interpretable Graph Learning with Sufficient and Necessary Explanations,"Self-interpretable graph learning methods provide insights to unveil the black-box nature of GNNs by providing predictions with built-in explanations. However, current works suffer from performance degradation compared to GNNs trained without built-in explanations. We argue the main reason is that they fail to generate explanations satisfying both sufficiency and necessity, and the biased explanations further hurt GNNs' performance. In this work, we propose a novel framework for generating SUfficient aNd NecessarY explanations (SUNNY-GNN for short) that benefit GNNs' predictions. The key idea is to conduct augmentations by structurally perturbing given explanations and employ a contrastive loss to guide the learning of explanations toward sufficiency and necessity directions. SUNNY-GNN introduces two coefficients to generate hard and reliable contrastive samples. We further extend SUNNY-GNN to heterogeneous graphs. Empirical results on various GNNs and real-world graphs show that SUNNY-GNN yields accurate predictions and faithful explanations, outperforming the state-of-the-art methods by improving 3.5% prediction accuracy and 13.1% explainability fidelity on average. Our code and data are available at https://github.com/SJTU-Quant/SUNNY-GNN.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Graph-based Machine Learning']",[],"['Jiale Deng', 'Yanyan Shen']","['Department of Computer Science and Engineering Shanghai Jiao Tong University', 'Department of Computer Science and Engineering Shanghai Jiao Tong University']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29060,Fairness & Bias,Semi-supervised TEE Segmentation via Interacting with SAM Equipped with Noise-Resilient Prompting,"Semi-supervised learning (SSL) is a powerful tool to address the challenge of insufficient annotated data in medical segmentation problems. However, existing semi-supervised methods mainly rely on internal knowledge for pseudo labeling, which is biased due to the distribution mismatch between the highly imbalanced labeled and unlabeled data. Segmenting left atrial appendage (LAA) from transesophageal echocardiogram (TEE) images is a typical medical image segmentation task featured by scarcity of professional annotations and diverse data distributions, for which existing SSL models cannot achieve satisfactory performance. In this paper, we propose a novel strategy to mitigate the inherent challenge of distribution mismatch in SSL by, for the first time, incorporating a large foundation model (i.e. SAM in our implementation) into an SSL model to improve the quality of pseudo labels. We further propose a new self-reconstruction mechanism to generate both noise-resilient prompts to demonically improve SAM’s generalization capability over TEE images and self-perturbations to stabilize the training process and reduce the impact of noisy labels. We conduct extensive experiments on an in-house TEE dataset; experimental results demonstrate that our method achieves better performance than state-of-the-art SSL models.","['ML: Semi-Supervised Learning', 'CV: Medical and Biological Imaging']",[],"['Sen Deng', 'Yidan Feng', 'Haoneng Lin', 'Yiting Fan', 'Alex Pui-Wai  Lee', 'Xiaowei Hu', 'Jing Qin']","['Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University', 'Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University', 'Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University', 'Department of cardiology, Shanghai Chest Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, 200030, China', 'Division of Cardiology, Department of Medicine and Therapeutics, The Chinese University of Hong Kong', 'Shanghai Artificial Intelligence Laboratory', 'Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/29067,Security,DGA-GNN: Dynamic Grouping Aggregation GNN for Fraud Detection,"Fraud detection has increasingly become a prominent research field due to the dramatically increased incidents of fraud. The complex connections involving thousands, or even millions of nodes, present challenges for fraud detection tasks. Many researchers have developed various graph-based methods to detect fraud from these intricate graphs. However, those methods neglect two distinct characteristics of the fraud graph: the non-additivity of certain attributes and the distinguishability of grouped messages from neighbor nodes. This paper introduces the Dynamic Grouping Aggregation Graph Neural Network (DGA-GNN) for fraud detection, which addresses these two characteristics by dynamically grouping attribute value ranges and neighbor nodes. In DGA-GNN, we initially propose the decision tree binning encoding to transform non-additive node attributes into bin vectors. This approach aligns well with the GNN’s aggregation operation and avoids nonsensical feature generation. Furthermore, we devise a feedback dynamic grouping strategy to classify graph nodes into two distinct groups and then employ a hierarchical aggregation. This method extracts more discriminative features for fraud detection tasks. Extensive experiments on five datasets suggest that our proposed method achieves a 3% ~ 16% improvement over existing SOTA methods. Code is available at https://github.com/AtwoodDuan/DGA-GNN.","['ML: Graph-based Machine Learning', 'DMKM: Anomaly/Outlier Detection', 'DMKM: Graph Mining', 'Social Network Analysis & Community']",[],"['Mingjiang Duan', 'Tongya Zheng', 'Yang Gao', 'Gang Wang', 'Zunlei Feng', 'Xinyu Wang']","['Zhejiang University', 'Hangzhou City University\nZhejiang University', 'Zhejiang University', 'Bangsheng Technology Co,Ltd.\nZJU-Bangsun Joint Research Center', 'Zhejiang University\nShanghai Institute for Advanced Study of Zhejiang University', 'Zhejiang University\nZJU-Bangsun Joint Research Center']","['China', 'China', 'China', '', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29070,Transparency & Explainability,Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces,"As responsible AI gains importance in machine learning algorithms, properties like fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models (SCMs) in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use SCMs and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation (CAP) and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Adversarial Learning & Robustness']",[],"['Ahmad-Reza Ehyaei', 'Kiarash Mohammadi', 'Amir-Hossein Karimi', 'Samira Samadi', 'Golnoosh Farnadi']","['Max Planck Institute for Intelligent Systems', 'Université de Montréal, Montréal, Canada\nMila - Québec AI Institute, Montréal, Canada', 'Max Planck Institute for Intelligent Systems Germany', 'Max Planck Institute for Intelligent Systems', 'Université de Montréal, Montréal, Canada\nMila - Québec AI Institute, Montréal, Canada\nMcGill University, Montréal, Canada']","['Belgium', 'Canada', 'Germany', 'Belgium', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/29079,Fairness & Bias,Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning,"Continual learning (CL) has shown promising results and comparable performance to learning at once in a fully supervised manner. However, CL strategies typically require a large number of labeled samples, making their real-life deployment challenging. In this work, we focus on semi-supervised continual learning (SSCL), where the model progressively learns from partially labeled data with unknown categories. We provide a comprehensive analysis of SSCL and demonstrate that unreliable distributions of unlabeled data lead to unstable training and refinement of the progressing stages. This problem severely impacts the performance of SSCL. To address the limitations, we propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for semi-supervised continual learning, which leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias. Firstly, we formalize a general model of structural distillation and design a dynamic graph construction for the continual learning progress. Next, we define a structure distillation vector and design a dynamic sub-graph distillation algorithm, which enables end-to-end training and adaptability to scale up tasks. The entire proposed method is adaptable to various CL methods and supervision settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100, and ImageNet-100, with varying supervision ratios, demonstrate the effectiveness of our proposed approach in mitigating the catastrophic forgetting problem in semi-supervised continual learning scenarios. Our code is available: https://github.com/fanyan0411/DSGD.","['ML: Life-Long and Continual Learning', 'ML: Semi-Supervised Learning']",[],"['Yan Fan', 'Yu Wang', 'Pengfei Zhu', 'Qinghua Hu']","['Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University\nHaihe Laboratory of Information Technology Application Innovation', 'Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University\nHaihe Laboratory of Information Technology Application Innovation', 'Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University\nHaihe Laboratory of Information Technology Application Innovation', 'Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University\nHaihe Laboratory of Information Technology Application Innovation']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29081,Fairness & Bias,Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations,"Accurate prediction of coupon usage is crucial for promoting user consumption through targeted coupon recommendations. However, in real-world coupon recommendations, the coupon allocation process is not solely determined by the model trained with the history interaction data but is also interfered with by marketing tactics desired to fulfill specific commercial goals.This interference creates an imbalance in the interactions, which causes the data to deviate from the user's natural preferences. We refer to this deviation as the matching bias. Such biased interaction data affects the efficacy of the model, and thus it is necessary to employ debiasing techniques to prevent any negative impact. We investigate the mitigation of matching bias in coupon recommendations from a causal-effect perspective. By treating the attributes of users and coupons associated with marketing tactics as confounders, we find the confounders open the backdoor path between user-coupon matching and the conversion, which introduces spurious correlation. To remove the bad effect, we propose a novel training paradigm named Backdoor Adjustment via Group Adaptation (BAGA) for debiased coupon recommendations, which performs intervened training and inference, i.e., separately modeling each user-coupon group pair. However, modeling all possible group pairs greatly increases the computational complexity and cost. To address the efficiency challenge, we further present a simple but effective dual-tower multi-task framework and leverage the Customized Gate Control (CGC) model architecture, which separately models each user and coupon group with a separate expert module. We instantiate BAGA on five representative models: FM, DNN, NCF, MASKNET, and DEEPFM, and conduct comprehensive offline and online experiments to demonstrate the efficacy of our proposed paradigm.","['ML: Ethics', 'Bias', 'and Fairness', 'DMKM: Conversational Systems for Recommendation & Retrieval', 'ML: Deep Learning Algorithms']",[],"['Junpeng Fang', 'Gongduo Zhang', 'Qing Cui', 'Caizhi Tang', 'Lihong Gu', 'Longfei Li', 'Jinjie Gu', 'Jun Zhou']","['Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29084,Fairness & Bias,BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning,"Semi-supervised Learning (SSL) reduces the need for extensive annotations in deep learning, but the more realistic challenge of imbalanced data distribution in SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning (CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by imbalanced data distributions. Most existing methods address this issue at instance-level through reweighting or resampling, but the performance is heavily limited by their reliance on biased backbone representation. Some other methods do perform feature-level adjustments like feature blending but might introduce unfavorable noise. In this paper, we discuss the bonus of a more balanced feature distribution for the CISSL problem, and further propose a Balanced Feature-Level Contrastive Learning method (BaCon). Our method directly regularizes the distribution of instances' representations in a well-designed contrastive manner. Specifically, class-wise feature centers are computed as the positive anchors, while negative anchors are selected by a straightforward yet effective mechanism. A distribution-related temperature adjustment is leveraged to control the class-wise contrastive degrees dynamically. Our method demonstrates its effectiveness through comprehensive experiments on the CIFAR10-LT, CIFAR100-LT, STL10-LT, and SVHN-LT datasets across various settings. For example, BaCon surpasses instance-level method FixMatch-based ABC on CIFAR10-LT with a 1.21% accuracy improvement, and outperforms state-of-the-art feature-level method CoSSL on CIFAR100-LT with a 0.63% accuracy improvement. When encountering more extreme imbalance degree, BaCon also shows better robustness than other methods.",['ML: Semi-Supervised Learning'],[],"['Qianhan Feng', 'Lujing Xie', 'Shijie Fang', 'Tong Lin']","['National Key Laboratory of General Artificial Intelligence, China\nSchool of Intelligence Science and Technology, Peking University', 'Yuanpei College, Peking University', 'School of Intelligence Science and Technology, Peking University\nGoogle, Shanghai, China', 'National Key Laboratory of General Artificial Intelligence, China\nSchool of Intelligence Science and Technology, Peking University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29092,Privacy & Data Governance,Fast Machine Unlearning without Retraining through Selective Synaptic Dampening,"Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data.  While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require long-term storage of the training data. First, SSD uses the Fisher information matrix of the training and forgetting data to select parameters that are disproportionately important to the forget set. Second, SSD induces forgetting by dampening these parameters proportional to their relative importance to the forget set with respect to the wider training data. We evaluate our method against several existing unlearning methods in a range of experiments using ResNet18 and Vision Transformer. Results show that the performance of SSD is competitive with retrain-based post hoc methods, demonstrating the viability of retrain-free post hoc unlearning approaches.","['ML: Privacy', 'ML: Deep Learning Algorithms']",[],"['Jack Foster', 'Stefan Schoepf', 'Alexandra Brintrup']","['University of Cambridge\nAlan Turing Institute', 'University of Cambridge', 'University of Cambridge\nAlan Turing Institute']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/29094,Fairness & Bias,REGLO: Provable Neural Network Repair for Global Robustness Properties,"We present REGLO, a novel methodology for repairing pretrained neural networks to satisfy global robustness and individual fairness properties. A neural network is said to be globally robust with respect to a given input region if and only if all the input points in the region are locally robust. This notion of global robustness also captures the notion of individual fairness as a special case. We prove that any counterexample to a global robustness property must exhibit a corresponding large gradient. For ReLU networks, this result allows us to efficiently identify the linear regions that violate a given global robustness property. By formulating and solving a suitable robust convex optimization problem, REGLO then computes a minimal weight change that will provably repair these violating linear regions.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Adversarial Learning & Robustness', 'ML: Privacy']",[],"['Feisi Fu', 'Zhilu Wang', 'Weichao Zhou', 'Yixuan Wang', 'Jiameng Fan', 'Chao Huang', 'Qi Zhu', 'Xin Chen', 'Wenchao Li']","['Boston University', 'Northwestern University', 'Boston University', 'Northwestern University', 'Boston University', 'Univeristy of Liverpool', 'Northwestern University', 'University of New Mexico', 'Boston University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29099,Fairness & Bias,Online Sensitivity Optimization in Differentially Private Learning,"Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the 2-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former with gradient descent, with minimal repercussions on the overall privacy analysis. Our method is thoroughly assessed against alternative fixed and adaptive strategies across diverse datasets, tasks, model dimensions, and privacy levels. Our results indicate that it performs comparably or better in the evaluated scenarios, given the same privacy requirements.","['ML: Privacy', 'ML: Optimization']",[],"['Filippo Galli', 'Catuscia Palamidessi', 'Tommaso Cucinotta']","[""Scuola Normale Superiore, Pisa, Italy\nScuola Superiore Sant'Anna, Pisa, Italy"", 'INRIA, Palaiseau, France\nÉcole Polytechnique, Palaiseau, France', ""Scuola Superiore Sant'Anna, Pisa, Italy""]","['Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/29102,Fairness & Bias,Get a Head Start: On-Demand Pedagogical Policy Selection in Intelligent Tutoring,"Reinforcement learning (RL) is broadly employed in human-involved systems to enhance human outcomes. Off-policy evaluation (OPE) has been pivotal for RL in those realms since online policy learning and evaluation can be high-stake. Intelligent tutoring has raised tremendous attentions as highly challenging when applying OPE to human-involved systems, due to that students' subgroups can favor different pedagogical policies and the costly procedure that policies have to be induced fully offline and then directly deployed to the upcoming semester. In this work, we formulate on-demand pedagogical policy selection (ODPS) to tackle the challenges for OPE in intelligent tutoring. We propose a pipeline, EduPlanner, as a concrete solution for ODPS. Our pipeline results in an theoretically unbiased estimator, and enables efficient and customized policy selection by identifying subgroups over both historical data and on-arrival initial logs. We evaluate our approach on the Probability ITS that has been used in real classrooms for over eight years. Our study shows significant improvement on learning outcomes of students with EduPlanner, especially for the ones associated with low-performing subgroups.","['ML: Reinforcement Learning', 'HAI: Human-Computer Interaction', 'APP: Other Applications', 'ML: Applications']",[],"['Ge Gao', 'Xi Yang', 'Min  Chi']","['North Carolina State University', 'IBM Research', 'North Carolina State University']","['United States', '', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29109,Fairness & Bias,DeepSaDe: Learning Neural Networks That Guarantee Domain Constraint Satisfaction,"As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight updates based on a mix of gradient descent and CSP solving. Evaluation on various machine learning tasks demonstrates that our approach is flexible enough to enforce a wide variety of domain constraints and is able to guarantee them in neural networks.","['ML: Neuro-Symbolic Learning', 'CSO: Constraint Optimization', 'CSO: Constraint Satisfaction', 'CSO: Satisfiability', 'CSO: Satisfiability Modulo Theories', 'ML: Classification and Regression', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Multi-class/Multi-label Learning & Extreme Classification', 'ML: Optimization']",[],"['Kshitij Goyal', 'Sebastijan Dumancic', 'Hendrik Blockeel']","['KU Leuven, Belgium', 'Delft University of Technology, The Netherlands', 'KU Leuven, Belgium']","['Belgium', 'Belgium', 'Belgium']"
https://ojs.aaai.org/index.php/AAAI/article/view/29112,Fairness & Bias,G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks,"It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question: whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs)? Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer existing PEFTs to graph-based tasks due to the issue of feature distribution shift. To address this issue, we propose a novel structure-aware PEFT approach, named G-Adapter, which leverages graph convolution operation to introduce graph structure information (e.g., graph adjacency matrix) as an inductive bias to guide the updating process. Further, we propose Bregman proximal point optimization to alleviate feature distribution shift by preventing the model from aggressive update. Extensive experiments demonstrate that G-Adapter obtains state-of-the-art performance compared to counterparts on nine graph benchmark datasets based on diverse pre-trained GTNs, and delivers tremendous memory footprint efficiency compared to the conventional paradigm.","['ML: Graph-based Machine Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Anchun Gui', 'Jinqiang Ye', 'Han Xiao']","['Xiamen University', 'Xiamen University', 'Xiamen University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29113,Privacy & Data Governance,FedCSL: A Scalable and Accurate Approach to Federated Causal Structure Learning,"As an emerging research direction, federated causal structure learning (CSL) aims at learning causal relationships from decentralized data across multiple clients while preserving data privacy. Existing federated CSL algorithms suffer from scalability and accuracy issues, since they require computationally expensive CSL algorithms to be executed at each client. Furthermore, in real-world scenarios, the number of samples held by each client varies significantly, and existing methods still assign equal weights to the learned structural information from each client, which severely harms the learning accuracy of those methods. To address these two limitations, we propose FedCSL, a scalable and accurate method for federated CSL. Specifically, FedCSL consists of two novel strategies: (1) a federated local-to-global learning strategy that enables FedCSL to scale to high-dimensional data for tackling the scalability issue, and (2) a novel weighted aggregation strategy that does not rely on any complex encryption techniques while preserving data privacy for tackling the accuracy issue. Extensive experiments on benchmark datasets, high-dimensional synthetic datasets and a real-world dataset verify the efficacy of the proposed FedCSL method. The source code is available at https://github.com/Xianjie-Guo/FedCSL.","['ML: Causal Learning', 'ML: Distributed Machine Learning & Federated Learning', 'ML: Dimensionality Reduction/Feature Selection', 'DMKM: Scalability', 'Parallel & Distributed Systems']",[],"['Xianjie Guo', 'Kui Yu', 'Lin Liu', 'Jiuyong Li']","['Hefei University of Technology', 'Hefei University of Technology', 'University of South Australia', 'University of South Australia']","['China', 'China', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29119,Security,Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees,"Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns domain-level budgets according to domain-level importance, which is estimated by optimizing an upper error bound that we develop; with the domain-level budgets, CAL then leverages a certain instance-level query strategy to select samples to label from each domain. Our theoretical analysis shows that our method achieves a better error bound compared to current AL methods. Our empirical results demonstrate that our approach significantly outperforms the state-of-the-art AL methods on both synthetic and real-world multi-domain datasets. Code is available at https://github.com/Wang-ML-Lab/multi-domain-active-learning.","['ML: Active Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Adversarial Learning & Robustness']",[],"['Guang-Yuan Hao', 'Hengguan Huang', 'Haotian Wang', 'Jie Gao', 'Hao Wang']","['Hong Kong University of Science and Technology\nMohamed bin Zayed University of Artificial Intelligence', 'National University of Singapore', 'JD Logistics', 'Rutgers University', 'Rutgers University']","['Hong Kong', 'Singapore', '', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29124,Fairness & Bias,Fairness under Covariate Shift: Improving Fairness-Accuracy Tradeoff with Few Unlabeled Test Samples,"Covariate shift in the test data is a common practical phenomena that can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups under covariate shift is of paramount importance due to societal implications like criminal justice. We operate in the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards improving fairness under this highly challenging yet realistic scenario, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift occurs when distribution of covariates of one group shifts significantly compared to the other groups and this happens when a dominant group is over-represented. While this setting is extremely challenging for current baselines, We show that our proposed method significantly outperforms them. Our third contribution is theoretical, where we show that our weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift. Empirically and through formal sample complexity bounds, we show that this approximation to the unseen test loss does not depend on importance sampling variance which affects many other baselines.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Shreyas Havaldar', 'Jatin Chauhan', 'Karthikeyan Shanmugam', 'Jay Nandy', 'Aravindan Raghuveer']","['Google Research India', 'UCLA', 'Google Research India', 'Fujitsu Reseach India', 'Google Research India']","['India', '', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/29123,Transparency & Explainability,Selective Deep Autoencoder for Unsupervised Feature Selection,"In light of the advances in big data, high-dimensional datasets are often encountered. Incorporating them into data-driven models can enhance performance; however, this comes at the cost of high computation and the risk of overfitting, particularly due to abundant redundant features. Identifying an informative subset of the features helps in reducing the dimensionality and enhancing model interpretability. In this paper, we propose a novel framework for unsupervised feature selection, called Selective Deep Auto-Encoder (SDAE). It aims to reduce the number of features used in unlabeled datasets without compromising the quality of information obtained. It achieves this by selecting sufficient features - from the original feature set - capable of representing the entire feature space and reconstructing them. Architecturally, it leverages the use of highly nonlinear latent representations in deep Autoencoders and intrinsically learns, in an unsupervised fashion, the relevant and globally representative subset of features through a customized Selective Layer. Extensive experimental results on three high-dimensional public datasets have shown promising feature selection performance by SDAE in comparison to other existing state-of-the-art unsupervised feature selection methods.","['ML: Dimensionality Reduction/Feature Selection', 'DMKM: Data Compression', 'ML: Applications', 'ML: Deep Generative Models & Autoencoders', 'ML: Deep Neural Architectures and Foundation Models']",[],"['Wael Hassanieh', 'Abdallah Chehade']","['University of Michigan-Dearborn', 'University of Michigan-Dearborn']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29128,Fairness & Bias,IS-DARTS: Stabilizing DARTS through Precise Measurement on Candidate Importance,"Among existing Neural Architecture Search methods, DARTS is known for its efficiency and simplicity. This approach applies continuous relaxation of network representation to construct a weight-sharing supernet and enables the identification of excellent subnets in just a few GPU days. However, performance collapse in DARTS results in deteriorating architectures filled with parameter-free operations and remains a great challenge to the robustness. To resolve this problem, we reveal that the fundamental reason is the biased estimation of the candidate importance in the search space through theoretical and experimental analysis, and more precisely select operations via information-based measurements. Furthermore, we demonstrate that the excessive concern over the supernet and inefficient utilization of data in bi-level optimization also account for suboptimal results. We adopt a more realistic objective focusing on the performance of subnets and simplify it with the help of the informationbased measurements. Finally, we explain theoretically why progressively shrinking the width of the supernet is necessary and reduce the approximation error of optimal weights in DARTS. Our proposed method, named IS-DARTS, comprehensively improves DARTS and resolves the aforementioned problems. Extensive experiments on NAS-Bench-201 and DARTS-based search space demonstrate the effectiveness of IS-DARTS.","['ML: Auto ML and Hyperparameter Tuning', 'CV: Learning & Optimization for CV', 'ML: Deep Neural Architectures and Foundation Models', 'ML: Optimization']",[],"['Hongyi He', 'Longjun Liu', 'Haonan Zhang', 'Nanning Zheng']","['National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University', 'National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University', 'National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University', 'National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29132,Fairness & Bias,Exploring Channel-Aware Typical Features for Out-of-Distribution Detection,"Detecting out-of-distribution (OOD) data is essential to ensure the reliability of machine learning models when deployed in real-world scenarios. Different from most previous test-time OOD detection methods that focus on designing OOD scores, we delve into the challenges in OOD detection from the perspective of typicality and regard the feature’s high-probability region as the feature’s typical set. However, the existing typical-feature-based OOD detection method implies an assumption: the proportion of typical feature sets for each channel is fixed. According to our experimental analysis, each channel contributes differently to OOD detection. Adopting a fixed proportion for all channels results in several channels losing too many typical features or incorporating too many abnormal features, resulting in low performance. Therefore, exploring the channel-aware typical features is crucial to better-separating ID and OOD data. Driven by this insight, we propose expLoring channel-Aware tyPical featureS (LAPS). Firstly, LAPS obtains the channel-aware typical set by calibrating the channel-level typical set with the global typical set from the mean and standard deviation. Then, LAPS rectifies the features into channel-aware typical sets to obtain channel-aware typical features. Finally, LAPS leverages the channel-aware typical features to calculate the energy score for OOD detection. Theoretical and visual analyses verify that LAPS achieves a better bias-variance trade-off. Experiments verify the effectiveness and generalization of LAPS under different architectures and OOD scores.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'DMKM: Anomaly/Outlier Detection', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['Rundong He', 'Yue Yuan', 'Zhongyi Han', 'Fan Wang', 'Wan Su', 'Yilong Yin', 'Tongliang Liu', 'Yongshun Gong']","['Shandong University', 'Shandong University', 'Mohamed bin Zayed University of Artificial Intelligence', 'Shandong University', 'Shandong University', 'Shandong University', 'The University of Sydney\nMohamed bin Zayed University of Artificial Intelligence', 'Shandong University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29136,Security,Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning,"A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state.  Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ""good"" trajectories and avoids ""bad"" trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ""good"" or ""bad"". A key advantage of our approach is that we are able to work from any starting policy or set of trajectories and improve on it. In an exhaustive set of experiments, we demonstrate that our approach is able to outperform top benchmark approaches for solving Constrained RL problems, with respect to expected cost, CVaR cost, or even unknown cost constraints.","['ML: Reinforcement Learning', 'CSO: Constraint Optimization', 'ML: Imitation Learning & Inverse Reinforcement Learning', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Huy Hoang', 'Tien Mai', 'Pradeep Varakantham']","['Singapore Management University', 'Singapore Management University', 'Singapore Management University']","['Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29140,Privacy & Data Governance,Foreseeing Reconstruction Quality of Gradient Inversion: An Optimization Perspective,"Gradient inversion attacks can leak data privacy when clients share weight updates with the server in federated learning (FL). Existing studies mainly use L2 or cosine distance as the loss function for gradient matching in the attack. Our empirical investigation shows that the vulnerability ranking varies with the loss function used. Gradient norm, which is commonly used as a vulnerability proxy for gradient inversion attack, cannot explain this as it remains constant regardless of the loss function for gradient matching. In this paper, we propose a loss-aware vulnerability proxy (LAVP) for the first time. LAVP refers to either the maximum or minimum eigenvalue of the Hessian with respect to gradient matching loss at ground truth. This suggestion is based on our theoretical findings regarding the local optimization of the gradient inversion in proximity to the ground truth, which corresponds to the worst case attack scenario. We demonstrate the effectiveness of LAVP on various architectures and datasets, showing its consistent superiority over the gradient norm in capturing sample vulnerabilities. The performance of each proxy is measured in terms of Spearman's rank correlation with respect to several similarity scores. This work will contribute to enhancing FL security against any potential loss functions beyond L2 or cosine distance in the future.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Distributed Machine Learning & Federated Learning', 'ML: Privacy', 'PEAI: Privacy & Security']",[],"['Hyeong Gwon Hong', 'Yooshin Cho', 'Hanbyel Cho', 'Jaesung Ahn', 'Junmo Kim']","['KAIST', 'KAIST', 'KAIST', 'KAIST', 'KAIST']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29143,Fairness & Bias,A Sequentially Fair Mechanism for Multiple Sensitive Attributes,"In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approximate fairness, enveloping a framework accommodating the trade-off between risk and unfairness. This extension permits a targeted prioritization of fairness improvements for a specific attribute within a set of sensitive attributes, allowing for a case specific adaptation. A data-driven estimation procedure for the derived solution is developed, and comprehensive numerical experiments are conducted on both synthetic and real datasets. Our empirical findings decisively underscore the practical efficacy of our post-processing approach in fostering fair decision-making.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Classification and Regression']",[],"['Francois Hu', 'Philipp Ratz', 'Arthur Charpentier']","['Université de Montréal', 'Université du Québec à Montréal', 'Université du Québec à Montréal']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/29153,Privacy & Data Governance,eTag: Class-Incremental Learning via Embedding Distillation and Task-Oriented Generation,"Class incremental learning (CIL) aims to solve the notorious forgetting problem, which refers to the fact that once the network is updated on a new task, its performance on previously-learned tasks degenerates catastrophically. Most successful CIL methods store exemplars (samples of learned tasks) to train a feature extractor incrementally, or store prototypes (features of learned tasks) to estimate the incremental feature distribution. However, the stored exemplars would violate the data privacy concerns, while the fixed prototypes might not reasonably be consistent with the incremental feature distribution, hindering the exploration of real-world CIL applications. In this paper, we propose a data-free CIL method with embedding distillation and Task-oriented generation (eTag), which requires neither exemplar nor prototype. Embedding distillation prevents the feature extractor from forgetting by distilling the outputs from the networks' intermediate blocks. Task-oriented generation enables a lightweight generator to produce dynamic features, fitting the needs of the top incremental classifier. Experimental results confirm that the proposed eTag considerably outperforms state-of-the-art methods on several benchmark datasets.","['ML: Deep Learning Algorithms', 'ML: Life-Long and Continual Learning']",[],"['Libo Huang', 'Yan Zeng', 'Chuanguang Yang', 'Zhulin An', 'Boyu Diao', 'Yongjun Xu']","['Institute of Computing Technology, Chinese Academy of Sciences', 'School of Mathematics and Statistics, Beijing Technology and Business University', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29157,Transparency & Explainability,Factorized Explainer for Graph Neural Networks,"Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN.  In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed factorized explainer.",['ML: Graph-based Machine Learning'],[],"['Rundong Huang', 'Farhad Shirani', 'Dongsheng Luo']","['Technical University of Munich, Munich, Germany', 'Florida International University, Miami, U.S.', 'Florida International University, Miami, U.S.']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29161,Fairness & Bias,Protein 3D Graph Structure Learning for Robust Structure-Based Protein Property Prediction,"Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem, we identify a Protein 3D Graph Structure Learning Problem for Robust Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present a protein Structure embedding Alignment Optimization framework (SAO) to mitigate the problem of structure embedding bias between the predicted and experimental protein structures. Extensive experiments have shown that our framework is model-agnostic and effective in improving the property prediction of both predicted structures and experimental structures.","['ML: Applications', 'APP: Natural Sciences']",[],"['Yufei Huang', 'Siyuan Li', 'Lirong Wu', 'Jin Su', 'Haitao Lin', 'Odin Zhang', 'Zihan Liu', 'Zhangyang Gao', 'Jiangbin Zheng', 'Stan Z. Li']","['Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'Zhejiang University, Hangzhou\nAI Lab, Research Center for Industries of the Future, Westlake University', 'AI Lab, Research Center for Industries of the Future, Westlake University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29167,Fairness & Bias,Fairness without Demographics through Shared Latent Space-Based Debiasing,"Ensuring fairness in machine learning (ML) is crucial, particularly in applications that impact diverse populations. The majority of existing works heavily rely on the availability of protected features like race and gender. However, practical challenges such as privacy concerns and regulatory restrictions often prohibit the use of this data, limiting the scope of traditional fairness research. To address this, we introduce a Shared Latent Space-based Debiasing (SLSD) method that transforms data from both the target domain, which lacks protected features, and a separate source domain, which contains these features, into correlated latent representations. This allows for joint training of a cross-domain protected group estimator on the representations. We then debias the downstream ML model with an adversarial learning technique that leverages the group estimator. We also present a relaxed variant of SLSD, the R-SLSD, that occasionally accesses a small subset of protected features from the target domain during its training phase. Our extensive experiments on benchmark datasets demonstrate that our methods consistently outperform existing state-of-the-art models in standard group fairness metrics.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Classification and Regression', 'ML: Semi-Supervised Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'PEAI: Bias', 'Fairness & Equity']",[],"['Rashidul Islam', 'Huiyuan Chen', 'Yiwei Cai']","['Visa Research', 'Visa Research', 'Visa Research']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29170,Transparency & Explainability,Delivering Inflated Explanations,"In the quest for Explainable Artificial Intelligence (XAI) one of the  questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system.  Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation  is a  set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It is possible that some features may change values and still lead to the same decision. In this paper we formally define  inflated explanations  which is a set of features, and for  each feature a set of values (always including the  value of the instance being explained), such that the decision will remain unchanged, for any of the values allowed for any of the  features in the (inflated) abductive explanation. Inflated formal explanations are more informative than common abductive explanations since e.g. they allow us to see if the exact value of a feature is important, or it could be any nearby value.  Overall they allow us to better understand the role of each feature in the decision. We show that we can compute inflated explanations for not that much greater cost than abductive explanations, and that we can extend duality results for abductive explanations also to inflated explanations.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CSO: Satisfiability', 'CSO: Constraint Satisfaction', 'KRR: Diagnosis and Abductive Reasoning', 'KRR: Automated Reasoning and Theorem Proving']",[],"['Yacine Izza', 'Alexey Ignatiev', 'Peter J. Stuckey', 'Joao Marques-Silva']","['CREATE, National University of Singapore', 'Monash University', 'Monash University\nOPTIMA ARC Industrial Training and Transformation Centre', 'IRIT, CNRS']","['Australia', 'Australia', 'Australia', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29175,Transparency & Explainability,Transportable Representations for Domain Generalization,"One key assumption in machine learning literature is that the testing and training data come from the same distribution, which is often violated in practice. The anchors that allow generalizations to take place are causal, and provenient in terms of the stability and modularity of the mechanisms underlying the system of variables. Building on the theory of causal transportability, we define the notion of ``transportable representations"", and show that these representations are suitable candidates for the domain generalization task. Specifically, considering that the graphical assumptions about the underlying system are provided, the transportable representations can be characterized accordingly, and the distribution of label conditioned on the representation can be computed in terms of the source distributions. Finally, we relax the assumption of having access to the underlying graph by proving a graphical-invariance duality theorem, which delineates certain probabilistic invariances present in the source data as a sound and complete criterion for generalizable classification. Our findings provide a unifying theoretical basis for several existing approaches to the domain generalization problem.","['ML: Causal Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'RU: Causality']",[],"['Kasra Jalaldoust', 'Elias Bareinboim']","['Columbia University', 'Columbia University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29177,Transparency & Explainability,Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective,"Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective and further formalize the causality among the variables in the pre-training stage to build the corresponding structural causal model. On the basis of the understanding of the structural causal model, we propose the dimensional rationale-aware graph contrastive learning approach, which introduces a learnable dimensional rationale acquiring network and a redundancy reduction constraint. The learnable dimensional rationale acquiring network is updated by leveraging a bi-level meta-learning technique, and the redundancy reduction constraint disentangles the redundant features through a decorrelation process during learning. Empirically, compared with state-of-the-art methods, our method can yield significant performance boosts on various benchmarks with respect to discriminability and transferability. The code implementation of our method is available at https://github.com/ByronJi/DRGCL.","['ML: Unsupervised & Self-Supervised Learning', 'ML: Graph-based Machine Learning', 'ML: Causal Learning', 'ML: Representation Learning']",[],"['Qirui Ji', 'Jiangmeng Li', 'Jie Hu', 'Rui Wang', 'Changwen Zheng', 'Fanjiang Xu']","['Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences\nState Key Laboratory of Intelligent Game', 'State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences', 'Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences\nState Key Laboratory of Intelligent Game', 'Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Science & Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences']","['China', 'China', '', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29180,Transparency & Explainability,Stratified GNN Explanations through Sufficient Expansion,"Explaining the decisions made by Graph Neural Networks (GNNs) is vital for establishing trust and ensuring fairness in critical applications such as medicine and science. The prevalence of hierarchical structure in real-world graphs/networks raises an important question on GNN interpretability: ""On each level of the graph structure, which specific fraction imposes the highest influence over the prediction?"" Currently, the prevailing two categories of methods are incapable of achieving multi-level GNN explanation due to their flat or motif-centric nature. In this work, we formulate the problem of learning multi-level explanations out of GNN models and introduce a stratified explainer module, namely STFExplainer, that utilizes the concept of sufficient expansion to generate explanations on each stratum. Specifically, we learn a higher-level subgraph generator by leveraging both hierarchical structure and GNN-encoded input features. Experiment results on both synthetic and real-world datasets demonstrate the superiority of our stratified explainer on standard interpretability tasks and metrics such as fidelity and explanation recall, with an average improvement of 11% and 8% over the best alternative on each data type. The case study on material domains also confirms the value of our approach through detected multi-level graph patterns accurately reconstructing the knowledge-based ground truth.","['ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Yuwen Ji', 'Lei Shi', 'Zhimeng Liu', 'Ge Wang']","['Beihang University (BUAA)', 'Beihang University (BUAA)', 'University of Science and Technology Beijing (USTB)', 'University of Science and Technology Beijing (USTB)']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29191,Security,Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts,"We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative FL framework.  We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution  and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of  O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and show how the clients' heterogeneity influences the convergence. Numerical results validate our analysis and provide valuable insights into real-world applications.","['ML: Adversarial Learning & Robustness', 'GTEP: Other Foundations of Game Theory & Economic Paradigms', 'ML: Distributed Machine Learning & Federated Learning', 'MAS: Multiagent Learning']",[],"['Kun Jin', 'Tongxin Yin', 'Zhongzhu Chen', 'Zeyu Sun', 'Xueru Zhang', 'Yang Liu', 'Mingyan Liu']","['University of Michigan, Ann Arbor', 'University of Michigan, Ann Arbor', 'University of Michigan, Ann Arbor', 'University of Michigan, Ann Arbor', 'Ohio State University', 'UC Santa Cruz', 'University of Michigan, Ann Arbor']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29204,Transparency & Explainability,Neural Oscillators for Generalization of Physics-Informed Machine Learning,"A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators outperforms existing state-of-the-art methods on benchmark problems across various metrics. Consequently, the proposed method improves the generalization capabilities of PIML, providing accurate solutions for extrapolation and prediction beyond the training data.","['ML: Semi-Supervised Learning', 'APP: Other Applications', 'ML: Applications', 'ML: Time-Series/Data Streams']",[],"['Taniya Kapoor', 'Abhishek Chandra', 'Daniel M. Tartakovsky', 'Hongrui Wang', 'Alfredo Nunez', 'Rolf Dollevoet']","['TU Delft', 'Eindhoven University of Technology', 'Stanford University', 'TU Delft', 'TU Delft', 'TU Delft']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/29201,Transparency & Explainability,Measuring Self-Supervised Representation Quality for Downstream Classification Using Discriminative Features,"Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to$40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Representation Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Neha Kalibhat', 'Kanika Narang', 'Hamed Firooz', 'Maziar Sanjabi', 'Soheil Feizi']","['University of Maryland, College Park', 'Meta AI', 'Meta AI', 'Meta AI', 'University of Maryland, College Park']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29198,Transparency & Explainability,Towards Safe Policy Learning under Partial Identifiability: A Causal Approach,"Learning personalized treatment policies is a formative challenge in many real-world applications, including in healthcare, econometrics, artificial intelligence. However, the effectiveness of candidate policies is not always identifiable, i.e., it is not uniquely computable from the combination of the available data and assumptions about the generating mechanisms. This paper studies policy learning from data collected in various non-identifiable settings, i.e., (1) observational studies with unobserved confounding; (2) randomized experiments with partial observability; and (3) their combinations. We derive sharp, closed-formed bounds from observational and experimental data over the conditional treatment effects. Based on these novel bounds, we further characterize the problem of safe policy learning and develop an algorithm that trains a policy from data guaranteed to achieve, at least, the performance of the baseline policy currently deployed. Finally, we validate our proposed algorithm on synthetic data and a large clinical trial, demonstrating that it guarantees safe behaviors and robust performance.","['ML: Causal Learning', 'ML: Reinforcement Learning', 'RU: Causality', 'RU: Graphical Models']",[],"['Shalmali Joshi', 'Junzhe Zhang', 'Elias Bareinboim']","['Columbia University', 'Columbia University', 'Columbia University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29203,Security,Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study,"In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph neural FDE models. We establish a theoretical foundation outlining the robustness characteristics of graph neural FDE models, highlighting that they maintain more stringent output perturbation bounds in the face of input and graph topology disturbances, compared to their integer-order counterparts. Our empirical evaluations further confirm the enhanced robustness of graph neural FDE models, highlighting their potential in adversarially robust applications.","['ML: Graph-based Machine Learning', 'ML: Adversarial Learning & Robustness']",[],"['Qiyu Kang', 'Kai Zhao', 'Yang Song', 'Yihang Xie', 'Yanan Zhao', 'Sijie Wang', 'Rui She', 'Wee Peng Tay']","['Nanyang Technological University', 'Nanyang Technological University', 'C3 AI', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']","['Singapore', 'Singapore', '', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29208,Transparency & Explainability,Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor Selection,"Machine learning techniques, such as deep learning and ensemble methods, are widely used in various domains due to their ability to handle complex real-world tasks. However, their black-box nature has raised multiple concerns about the fairness, trustworthiness, and transparency of computer-assisted decision-making. This has led to the emergence of local post-hoc explainability methods, which offer explanations for individual decisions made by black-box algorithms. Among these methods, Kernel SHAP is widely used due to its model-agnostic nature and its well-founded theoretical framework. Despite these strengths, Kernel SHAP suffers from high instability: different executions of the method with the same inputs can lead to significantly different explanations, which diminishes the relevance of the explanations. The contribution of this paper is two-fold. On the one hand, we show that Kernel SHAP's instability is caused by its stochastic neighbor selection procedure, which we adapt to achieve full stability without compromising explanation fidelity. On the other hand, we show that by restricting the neighbors generation to perturbations of size 1 -- which we call the coalitions of Layer 1 -- we obtain a novel feature-attribution method that is fully stable, computationally efficient, and still meaningful.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Gwladys Kelodjou', 'Laurence Rozé', 'Véronique Masson', 'Luis Galárraga', 'Romaric Gaudel', 'Maurice Tchuente', 'Alexandre Termier']","['Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, INSA Rennes, CNRS, Inria, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Sorbonne University, IRD, University of Yaoundé I, UMI 209 UMMISCO, BP 337 Yaoundé, Cameroon', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France']","['France', 'France', 'France', 'France', 'France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/29211,Fairness & Bias,Adaptive Shortcut Debiasing for Online Continual Learning,"We propose a novel framework DropTop that suppresses the shortcut bias in online continual learning (OCL) while being adaptive to the varying degree of the shortcut bias incurred by continuously changing environment. By the observed high-attention property of the shortcut bias, highly-activated features are considered candidates for debiasing. More importantly, resolving the limitation of the online environment where prior knowledge and auxiliary data are not ready, two novel techniques---feature map fusion and adaptive intensity shifting---enable us to automatically determine the appropriate level and proportion of the candidate shortcut features to be dropped. Extensive experiments on five benchmark datasets demonstrate that, when combined with various OCL algorithms, DropTop increases the average accuracy by up to 10.4% and decreases the forgetting by up to 63.2%.","['ML: Life-Long and Continual Learning', 'ML: Time-Series/Data Streams', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Doyoung Kim', 'Dongmin Park', 'Yooju Shin', 'Jihwan Bang', 'Hwanjun Song', 'Jae-Gil Lee']","['KAIST, Daejeon, Republic of Korea', 'KAIST, Daejeon, Republic of Korea', 'KAIST, Daejeon, Republic of Korea', 'KAIST, Daejeon, Republic of Korea', 'KAIST, Daejeon, Republic of Korea', 'KAIST, Daejeon, Republic of Korea']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29214,Security,Robust Distributed Gradient Aggregation Using Projections onto Gradient Manifolds,"We study the distributed gradient aggregation problem where individual clients contribute to learning a central model by sharing parameter gradients constructed from local losses. However, errors in some gradients, caused by low-quality data or adversaries, can degrade the learning process when naively combined. Existing robust gradient aggregation approaches assume that local data represent the global data-generating distribution, which may not always apply to heterogeneous (non-i.i.d.) client data. We propose a new algorithm that can robustly aggregate gradients from potentially heterogeneous clients. Our approach leverages the manifold structure inherent in heterogeneous client gradients and evaluates gradient anomaly degrees by projecting them onto this manifold. This algorithm is implemented as a simple and efficient method that accumulates random projections within the subspace defined by the nearest neighbors within a gradient cloud. Our experiments demonstrate consistent performance improvements over state-of-the-art robust aggregation algorithms.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Adversarial Learning & Robustness']",[],['Kwang In Kim'],['POSTECH'],['South Korea']
https://ojs.aaai.org/index.php/AAAI/article/view/29216,Security,Cross-Class Feature Augmentation for Class Incremental Learning,"We propose a novel class incremental learning approach, which incorporates a feature augmentation technique motivated by adversarial attacks. We employ a classifier learned in the past to complement training examples of previous tasks. The proposed approach has an unique perspective to utilize the previous knowledge in class incremental learning since it augments features of arbitrary target classes using examples in other classes via adversarial attacks on a previously learned classifier. By allowing the Cross-Class Feature Augmentations (CCFA), each class in the old tasks conveniently populates samples in the feature space, which alleviates the collapse of the decision boundaries caused by sample deficiency for the previous tasks, especially when the number of stored exemplars is small. This idea can be easily incorporated into existing class incremental learning algorithms without any architecture modification. Extensive experiments on the standard benchmarks show that our method consistently outperforms existing class incremental learning methods by significant margins in various scenarios, especially under an environment with an extremely limited memory budget.","['ML: Life-Long and Continual Learning', 'ML: Representation Learning']",[],"['Taehoon Kim', 'Jaeyoo Park', 'Bohyung Han']","['Seoul National University', 'Seoul National University', 'Seoul National University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29218,Fairness & Bias,Relaxed Stationary Distribution Correction Estimation for Improved Offline Policy Optimization,"One of the major challenges of offline reinforcement learning (RL) is dealing with distribution shifts that stem from the mismatch between the trained policy and the data collection policy. Stationary distribution correction estimation algorithms (DICE) have addressed this issue by regularizing the policy optimization with f-divergence between the state-action visitation distributions of the data collection policy and the optimized policy. While such regularization naturally integrates to derive an objective to get optimal state-action visitation, such an implicit policy optimization framework has shown limited performance in practice. We observe that the reduced performance is attributed to the biased estimate and the properties of conjugate functions of f-divergence regularization. In this paper, we improve the regularized implicit policy optimization framework by relieving the bias and reshaping the conjugate function by relaxing the constraints. We show that the relaxation adjusts the degree of involvement of the sub-optimal samples in optimization, and we derive a new offline RL algorithm that benefits from the relaxed framework, improving from a previous implicit policy optimization algorithm by a large margin.","['ML: Reinforcement Learning', 'ML: Optimization']",[],"['Woosung Kim', 'Donghyeon Ki', 'Byung-Jun Lee']","['Korea University', 'Korea University', 'Korea University']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29223,Transparency & Explainability,Pantypes: Diverse Representatives for Self-Explainable Models,"Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions.  Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Bias', 'Fairness & Privacy', 'ML: Classification and Regression', 'ML: Clustering', 'ML: Deep Learning Algorithms', 'ML: Dimensionality Reduction/Feature Selection', 'ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Accountability', 'Interpretability & Explainability']",[],"['Rune Kjærsgaard', 'Ahcène Boubekki', 'Line Clemmensen']","['Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark', 'Machine Learning and Uncertainty, Physikalisch-Technische Bundesanstalt, Germany', 'Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark']","['Denmark', '', 'Denmark']"
https://ojs.aaai.org/index.php/AAAI/article/view/29225,Transparency & Explainability,Approximating the Shapley Value without Marginal Contributions,"The Shapley value, which is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, has recently been used intensively in explainable artificial intelligence. Its meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley value, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contribution. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparing ourselves with state-of-the-art methods.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'GTEP: Cooperative Game Theory']",[],"['Patrick Kolpaczki', 'Viktor Bengs', 'Maximilian Muschalik', 'Eyke Hüllermeier']","['Paderborn University', 'Institute of Informatics, University of Munich (LMU)\nMunich Center for Machine Learning', 'Institute of Informatics, University of Munich (LMU)\nMunich Center for Machine Learning', 'Institute of Informatics, University of Munich (LMU)\nMunich Center for Machine Learning']","['Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29230,Security,Friendly Attacks to Improve Channel Coding Reliability,"This paper introduces a novel approach called ""friendly attack"" aimed at enhancing the performance of error correction channel codes. Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance. By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint. The perturbation design is accomplished by a modified iterative fast gradient method. This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations. Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes. We demonstrate that the proposed friendly attack method can improve the reliability across different channels, modulations, codes, and decoders. This method allows us to increase the reliability of communication with a legacy receiver by simply modifying the transmitted codeword appropriately.","['ML: Applications', 'SO: Adversarial Search']",[],"['Anastasiia Kurmukova', 'Deniz Gunduz']","['Imperial College London', 'Imperial College London']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29231,Fairness & Bias,Evolving Parameterized Prompt Memory for Continual Learning,"Recent studies have demonstrated the potency of leveraging prompts in Transformers for continual learning (CL). Nevertheless, employing a discrete key-prompt bottleneck can lead to selection mismatches and inappropriate prompt associations during testing. Furthermore, this approach hinders adaptive prompting due to the lack of shareability among nearly identical instances at more granular level. To address these challenges, we introduce the Evolving Parameterized Prompt Memory (EvoPrompt), a novel method involving adaptive and continuous prompting attached to pre-trained Vision Transformer (ViT), conditioned on specific instance. We formulate a continuous prompt function as a neural bottleneck and encode the collection of prompts on network weights. We establish a paired prompt memory system consisting of a stable reference and a flexible working prompt memory. Inspired by linear mode connectivity, we progressively fuse the working prompt memory and reference prompt memory during inter-task periods, resulting in continually evolved prompt memory. This fusion involves aligning functionally equivalent prompts using optimal transport and aggregating them in parameter space with an adjustable bias based on prompt node attribution. Additionally, to enhance backward compatibility, we propose compositional classifier initialization, which leverages prior prototypes from pre-trained models to guide the initialization of new classifiers in a subspace-aware manner. Comprehensive experiments validate that our approach achieves state-of-the-art performance in both class and domain incremental learning scenarios.","['ML: Life-Long and Continual Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Muhammad Rifki  Kurniawan', 'Xiang Song', 'Zhiheng Ma', 'Yuhang He', 'Yihong Gong', 'Yang  Qi', 'Xing Wei']","[""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", 'Shenzhen Institute of Advanced Technology,Chinese Academy of Sciences', 'Xi’an Jiaotong University', ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University""]","['China', 'China', '', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29239,Security,Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs,"Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks.  Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model.  Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks. By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model. Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker. Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the naive TGNN by up to 11.2% under T-SPEAR. The code and datasets are available at https://github.com/wooner49/T-spear-shield","['ML: Graph-based Machine Learning', 'DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Adversarial Learning & Robustness']",[],"['Dongjin Lee', 'Juho Lee', 'Kijung Shin']","['School of Electrical Engineering, Daejeon, South Korea', 'Kim Jaechul Graduate School of AI, KAIST, Seoul, South Korea', 'School of Electrical Engineering, Daejeon, South Korea\nKim Jaechul Graduate School of AI, KAIST, Seoul, South Korea']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29256,Fairness & Bias,Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-Smoothness in Deep GNNs,"Despite Graph neural networks' significant performance gain over many classic techniques in various graph-related downstream tasks, their successes are restricted in shallow models due to over-smoothness and the difficulties of optimizations among many other issues. In this paper, to alleviate the over-smoothing issue, we propose a soft graph normalization method to preserve the diversities of node embeddings and prevent indiscrimination due to possible over-closeness. Combined with residual connections, we analyze the reason why the method can effectively capture the knowledge in both input graph structures and node features even with deep networks. Additionally, inspired by Curriculum Learning that learns easy examples before the hard ones, we propose a novel label-smoothing-based learning framework to enhance the optimization of deep GNNs, which iteratively smooths labels in an auxiliary graph and constructs many gradual non-smooth tasks for extracting increasingly complex knowledge and gradually discriminating nodes from coarse to fine. The method arguably reduces the risk of overfitting and generalizes better results. Finally, extensive experiments are carried out to demonstrate the effectiveness and potential of the proposed model and learning framework through comparison with twelve existing baselines including the state-of-the-art methods on twelve real-world node classification benchmarks.","['ML: Graph-based Machine Learning', 'DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Deep Learning Algorithms', 'ML: Semi-Supervised Learning']",[],"['Jin Li', 'Qirong Zhang', 'Shuling Xu', 'Xinlong Chen', 'Longkun Guo', 'Yang-Geng Fu']","['College of Computer and Data Science, Fuzhou University\nAI Thrust, Information Hub, HKUST (Guangzhou)', 'College of Computer and Data Science, Fuzhou University', 'College of Computer and Data Science, Fuzhou University', 'College of Computer and Data Science, Fuzhou University', 'College of Computer and Data Science, Fuzhou University\nShandong Fundamental Research Center for Computer Science', 'College of Computer and Data Science, Fuzhou University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29260,Fairness & Bias,Twice Class Bias Correction for Imbalanced Semi-supervised Learning,"Differing from traditional semi-supervised learning, class-imbalanced semi-supervised learning presents two distinct challenges: (1) The imbalanced distribution of training samples leads to model bias towards certain classes, and (2) the distribution of unlabeled samples is unknown and potentially distinct from that of labeled samples, which further contributes to class bias in the pseudo-labels during the training. To address these dual challenges, we introduce a novel approach called Twice Class Bias Correction (TCBC). We begin by utilizing an estimate of the class distribution from the participating training samples to correct the model, enabling it to learn the posterior probabilities of samples under a class-balanced prior. This correction serves to alleviate the inherent class bias of the model. Building upon this foundation, we further estimate the class bias of the current model parameters during the training process. We apply a secondary correction to the model's pseudo-labels for unlabeled samples, aiming to make the assignment of pseudo-labels across different classes of unlabeled samples as equitable as possible. Through extensive experimentation on CIFAR10/100-LT, STL10-LT, and the sizable long-tailed dataset SUN397, we provide conclusive evidence that our proposed TCBC method reliably enhances the performance of class-imbalanced semi-supervised learning.","['ML: Semi-Supervised Learning', 'ML: Classification and Regression']",[],"['Lan Li', 'Bowen Tao', 'Lu Han', 'De-chuan Zhan', 'Han-jia Ye']","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29262,Fairness & Bias,Feature Fusion from Head to Tail for Long-Tailed Visual Recognition,"The imbalanced distribution of long-tailed data presents a considerable challenge for deep learning models, as it causes them to prioritize the accurate classification of head classes but largely disregard tail classes. The biased decision boundary caused by inadequate semantic information in tail classes is one of the key factors contributing to their low recognition accuracy. To rectify this issue, we propose to augment tail classes by grafting the diverse semantic information from head classes, referred to as head-to-tail fusion (H2T). We replace a portion of feature maps from tail classes with those belonging to head classes. These fused features substantially enhance the diversity of tail classes. Both theoretical analysis and practical experimentation demonstrate that H2T can contribute to a more optimized solution for the decision boundary. We seamlessly integrate H2T in the classifier adjustment stage, making it a plug-and-play module. Its simplicity and ease of implementation allow for smooth integration with existing long-tailed recognition methods, facilitating a further performance boost. Extensive experiments on various long-tailed benchmarks demonstrate the effectiveness of the proposed H2T. The source code is available at https://github.com/Keke921/H2T.","['ML: Classification and Regression', 'CV: Object Detection & Categorization', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['Mengke Li', 'Zhikai HU', 'Yang Lu', 'Weichao Lan', 'Yiu-ming Cheung', 'Hui Huang']","['Guangdong Laboratory of Artificial Intelligence and Digital Economy\nShenzhen University', 'Hong Kong Baptist University', 'Xiamen University', 'Hong Kong Baptist University', 'Hong Kong Baptist University', 'Shenzhen University']","['India', 'Hong Kong', 'China', 'Hong Kong', 'Hong Kong', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29257,Transparency & Explainability,Tensorized Label Learning on Anchor Graph,"Graph-based multimedia data clustering has attracted much attention due to the impressive clustering performance for arbitrarily shaped multimedia data. However, existing graph-based clustering methods need post-processing to get labels for multimedia data with high computational complexity. Moreover, it is sub-optimal for label learning due to the fact that they exploit the complementary information embedded in data with different types pixel by pixel. To handle these problems, we present a novel label learning model with good interpretability for clustering. To be specific, our model decomposes anchor graph into the products of two matrices with orthogonal non-negative constraint to directly get soft label without any post-processing, which remarkably reduces the computational complexity. To well exploit the complementary information embedded in multimedia data, we introduce tensor Schatten p-norm regularization on the label tensor which is composed of soft labels of multimedia data. The solution can be obtained by iteratively optimizing four decoupled sub-problems, which can be solved more efficiently with good convergence. Experimental results on various datasets demonstrate the efficiency of our model.","['ML: Unsupervised & Self-Supervised Learning', 'CV: Learning & Optimization for CV', 'CV: Representation Learning for Vision', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['Jing Li', 'Quanxue Gao', 'Qianqian Wang', 'Wei Xia']","['School of Telecommunications Engineering, Xidian University', 'School of Telecommunications Engineering, Xidian University', 'School of Telecommunications Engineering, Xidian University\nKey Laboratory of Measurement and Control of Complex Systems of Engineering (Southeast University), Ministry of Education.', 'School of Telecommunications Engineering, Xidian University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29258,Security,EMGAN: Early-Mix-GAN on Extracting Server-Side Model in Split Federated Learning,"Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at ""https://github.com/zlijingtao/SFL-MEA"".","['ML: Distributed Machine Learning & Federated Learning', 'CV: Adversarial Attacks & Robustness']",[],"['Jingtao Li', 'Xing Chen', 'Li Yang', 'Adnan Siraj Rakin', 'Deliang Fan', 'Chaitali Chakrabarti']","['Sony AI', 'Arizona State University', 'University of North Carolina at Charlotte', 'Binghamton University (SUNY)', 'Johns Hopkins University', 'Arizona State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29269,Transparency & Explainability,Image Content Generation with Causal Reasoning,"The emergence of ChatGPT has once again sparked research in generative artificial intelligence (GAI). While people have been amazed by the generated results, they have also noticed the reasoning potential reflected in the generated textual content.  However, this current ability for causal reasoning is primarily limited to the domain of language generation, such as in models like GPT-3. In visual modality, there is currently no equivalent research. Considering causal reasoning in visual content generation is significant. This is because visual information contains infinite granularity. Particularly, images can provide more intuitive and specific demonstrations for certain reasoning tasks, especially when compared to coarse-grained text. Hence, we propose a new image generation task called visual question answering with image (VQAI) and establish a dataset of the same name based on the classic Tom and Jerry animated series.  Additionally, we develop a new paradigm for image generation to tackle the challenges of this task.  Finally, we perform extensive experiments and analyses, including visualizations of the generated content and discussions on the potentials and limitations. The code and data are publicly available under the license of CC BY-NC-SA 4.0 for academic and non-commercial usage at: https://github.com/IEIT-AGI/MIX-Shannon/blob/main/projects/VQAI/lgd_vqai.md.","['ML: Multimodal Learning', 'CV: Representation Learning for Vision', 'CV: Visual Reasoning & Symbolic Representations', 'KRR: Common-Sense Reasoning', 'NLP: Generation']",[],"['Xiaochuan Li', 'Baoyu Fan', 'Runze Zhang', 'Liang Jin', 'Di Wang', 'Zhenhua Guo', 'Yaqian Zhao', 'Rengang Li']","['Inspur Electronic Information Industry Co.,Ltd.\nShandong Massive Information Technology Research Institute', 'Nankai University\nInspur Electronic Information Industry Co.,Ltd.', 'Inspur Electronic Information Industry Co.,Ltd.', 'Inspur Electronic Information Industry Co.,Ltd.', 'Inspur Electronic Information Industry Co.,Ltd.', 'Inspur Electronic Information Industry Co.,Ltd.', 'Inspur Electronic Information Industry Co.,Ltd.', 'Tsinghua University\nInspur Electronic Information Industry Co.,Ltd.']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29273,Privacy & Data Governance,Towards Effective and General Graph Unlearning via Mutual Evolution,"With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose Mutual Evolution Graph Unlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7%, 2.5%, and 3.2% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch.","['ML: Graph-based Machine Learning', 'ML: Deep Learning Algorithms', 'ML: Semi-Supervised Learning', 'ML: Privacy']",[],"['Xunkai Li', 'Yulin Zhao', 'Zhengyu Wu', 'Wentao Zhang', 'Rong-Hua Li', 'Guoren Wang']","['Beijing Institute of Technology', 'Shandong University', 'Beijing Institute of Technology', 'Peking University\nNational Engineering Labratory for Big Data Analytics and Applications', 'Beijing Institute of Technology\nShenzhen Institute of Technology', 'Beijing Institute of Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29278,Transparency & Explainability,Multi-Granularity Causal Structure Learning,"Unveiling, modeling, and comprehending the causal mechanisms underpinning natural phenomena stand as fundamental endeavors across myriad scientific disciplines. Meanwhile, new knowledge emerges when discovering causal relationships from data. Existing causal learning algorithms predominantly focus on the isolated effects of variables, overlook the intricate interplay of multiple variables and their collective behavioral patterns. Furthermore, the ubiquity of high-dimensional data exacts a substantial temporal cost for causal algorithms. In this paper, we develop a novel method called MgCSL (Multi-granularity Causal Structure Learning), which first leverages sparse auto-encoder to explore coarse-graining strategies and causal abstractions from micro-variables to macro-ones. MgCSL then takes multi-granularity variables as inputs to train multilayer perceptrons and to delve the causality between variables. To enhance the efficacy on high-dimensional data, MgCSL  introduces a simplified acyclicity constraint to adeptly search the directed acyclic graph among variables. Experimental results show that MgCSL outperforms competitive baselines, and finds out explainable causal connections on fMRI datasets.","['ML: Causal Learning', 'DMKM: Graph Mining', 'Social Network Analysis & Community', 'CSO: Constraint Optimization']",[],"['Jiaxuan Liang', 'Jun Wang', 'Guoxian Yu', 'Shuyin Xia', 'Guoyin Wang']","['Shandong University', 'Shandong University', 'Shandong University', 'Chongqing Key Laboratory of Computational Intelligence\nChongqing University of Posts and Telecommunications', 'Chongqing Key Laboratory of Computational Intelligence\nChongqing University of Posts and Telecommunications']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29279,Transparency & Explainability,Inducing Clusters Deep Kernel Gaussian Process for Longitudinal Data,"We consider the problem of predictive modeling from irregularly and sparsely sampled longitudinal data with unknown, complex correlation structures and abrupt discontinuities. To address these challenges, we introduce a novel inducing clusters longitudinal deep kernel Gaussian Process (ICDKGP). ICDKGP approximates the data generating process by a zero-mean GP with a longitudinal deep kernel that models the unknown complex correlation structure in the data and a deterministic non-zero mean function to model the abrupt discontinuities. To improve the scalability and interpretability of ICDKGP, we introduce inducing clusters corresponding to centers of clusters in the training data. We formulate the training of ICDKGP as a constrained optimization problem and derive its evidence lower bound. We introduce a novel relaxation of the resulting problem which under rather mild assumptions yields a solution with error bounded relative to the original problem. We describe the results of extensive experiments demonstrating that ICDKGP substantially outperforms the state-of-the-art longitudinal methods on data with both smoothly and non-smoothly varying outcomes.","['ML: Kernel Methods', 'ML: Classification and Regression', 'ML: Representation Learning']",[],"['Junjie Liang', 'Weijieying Ren', 'Hanifi Sahar', 'Vasant Honavar']","['The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsynvalia State University', 'The Pennsylvania State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29282,Security,Value at Adversarial Risk: A Graph Defense Strategy against Cost-Aware Attacks,"Deep learning methods on graph data have achieved remarkable efficacy across a variety of real-world applications, such as social network analysis and transaction risk detection. Nevertheless, recent studies have illuminated a concerning fact: even the most expressive Graph Neural Networks (GNNs) are vulnerable to graph adversarial attacks. While several methods have been proposed to enhance the robustness of GNN models against adversarial attacks, few have focused on a simple yet realistic approach: valuing the adversarial risks and focused safeguards at the node level. This empowers defenders to allocate heightened security level to vulnerable nodes, while lower to robust nodes. With this new perspective, we propose a novel graph defense strategy RisKeeper, such that the adversarial risk can be directly kept in the input graph. We start at valuing the adversarial risk, by introducing a cost-aware projected gradient descent attack that takes into account both cost avoidance and compliance with costs budgets. Subsequently, we present a learnable approach to ascertain the ideal security level for each individual node by solving a bi-level optimization problem. Through extensive experiments on four real-world datasets, we demonstrate that our method achieves superior performance surpassing state-of-the-art methods. Our in-depth case studies provide further insights into vulnerable and robust structural patterns, serving as inspiration for practitioners to exercise heightened vigilance.","['ML: Adversarial Learning & Robustness', 'ML: Graph-based Machine Learning']",[],"['Junlong Liao', 'Wenda Fu', 'Cong Wang', 'Zhongyu Wei', 'Jiarong Xu']","['Fudan University', 'Fudan University', 'Peking University', 'Fudan University', 'Fudan University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29286,Security,Mitigating Label Noise through Data Ambiguation,"Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by ""ambiguating"" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reliable beliefs about the ground-truth, effectively helping the learner to suppress the memorization effect. In an extensive empirical evaluation, our method demonstrates favorable learning behavior on synthetic and real-world noise, confirming the effectiveness in detecting and correcting erroneous training labels.","['ML: Adversarial Learning & Robustness', 'ML: Classification and Regression', 'ML: Deep Learning Algorithms', 'RU: Uncertainty Representations']",[],"['Julian Lienen', 'Eyke Hüllermeier']","['Department of Computer Science, Paderborn University', 'Institute of Informatics, LMU Munich\nMunich Center for Machine Learning']","['United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29289,Fairness & Bias,ERL-TD: Evolutionary Reinforcement Learning Enhanced with Truncated Variance and Distillation Mutation,"Recently, an emerging research direction called Evolutionary Reinforcement Learning (ERL) has been proposed, which combines evolutionary algorithm with reinforcement learning (RL) for tackling the tasks of sequential decision making. However, the recently proposed ERL algorithms often suffer from two challenges: the inaccuracy of policy estimation caused by the overestimation bias in RL and the insufficiency of exploration caused by inefficient mutations. To alleviate these problems, we propose an Evolutionary Reinforcement Learning algorithm enhanced with Truncated variance and Distillation mutation, called ERL-TD. We utilize multiple Q-networks to evaluate state-action pairs, so that multiple networks can provide more accurate evaluations for state-action pairs, in which the variance of evaluations can be adopted to control the overestimation bias in RL. Moreover, we propose a new distillation mutation to provide a promising mutation direction, which is different from traditional mutation generating a large number of random solutions. We evaluate ERL-TD on the continuous control benchmarks from the OpenAI Gym and DeepMind Control Suite. The experiments show that ERL-TD shows excellent performance and outperforms all baseline RL algorithms on the test suites.","['ML: Reinforcement Learning', 'ML: Evolutionary Learning']",[],"['Qiuzhen Lin', 'Yangfan Chen', 'Lijia Ma', 'Wei-Neng Chen', 'Jianqiang  Li']","['Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'South China University of Technology', 'Shenzhen University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29292,Security,Towards Inductive Robustness: Distilling and Fostering Wave-Induced Resonance in Transductive GCNs against Graph Adversarial Attacks,"Graph neural networks (GNNs) have recently been shown to be vulnerable to adversarial attacks, where slight perturbations in the graph structure can lead to erroneous predictions. However, current robust models for defending against such attacks inherit the transductive limitations of graph convolutional networks (GCNs). As a result, they are constrained by fixed structures and do not naturally generalize to unseen nodes. Here, we discover that transductive GCNs inherently possess a distillable robustness, achieved through a wave-induced resonance process. Based on this, we foster this resonance to facilitate inductive and robust learning. Specifically, we first prove that the signal formed by GCN-driven message passing (MP) is equivalent to the edge-based Laplacian wave, where, within a wave system, resonance can naturally emerge between the signal and its transmitting medium. This resonance provides inherent resistance to malicious perturbations inflicted on the signal system. We then prove that merely three MP iterations within GCNs can induce signal resonance between nodes and edges, manifesting as a coupling between nodes and their distillable surrounding local subgraph. Consequently, we present Graph Resonance-fostering Network (GRN) to foster this resonance via learning node representations from their distilled resonating subgraphs. By capturing the edge-transmitted signals within this subgraph and integrating them with the node signal, GRN embeds these combined signals into the central node's representation. This node-wise embedding approach allows for generalization to unseen nodes. We validate our theoretical findings with experiments, and demonstrate that GRN generalizes robustness to unseen nodes, whilst maintaining state-of-the-art classification accuracy on perturbed graphs. Appendices can be found on arXiv version: https://arxiv.org/abs/2312.08651","['ML: Adversarial Learning & Robustness', 'ML: Graph-based Machine Learning']",[],"['Ao Liu', 'Wenshan Li', 'Tao Li', 'Beibei Li', 'Hanyuan Huang', 'Pan Zhou']","['Sichuan University', 'Chengdu University of Information Technology', 'Sichuan University', 'Sichuan University', 'Sichuan University', 'Huazhong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29295,Privacy & Data Governance,Language-Guided Transformer for Federated Multi-Label Classification,"Federated Learning (FL) is an emerging paradigm that enables multiple users to collaboratively train a robust model in a privacy-preserving manner without sharing their private data. Most existing approaches of FL only consider traditional single-label image classification, ignoring the impact when transferring the task to multi-label image classification. Nevertheless, it is still challenging for FL to deal with user heterogeneity in their local data distribution in the real-world FL scenario, and this issue becomes even more severe in multi-label image classification. Inspired by the recent success of Transformers in centralized settings, we propose a novel FL framework for multi-label classification. Since partial label correlation may be observed by local clients during training, direct aggregation of locally updated models would not produce satisfactory performances. Thus, we propose a novel FL framework of Language-Guided Transformer (FedLGT) to tackle this challenging task, which aims to exploit and transfer knowledge across different clients for learning a robust global model. Through extensive experiments on various multi-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is able to achieve satisfactory performance and outperforms standard FL techniques under multi-label FL scenarios. Code is available at https://github.com/Jack24658735/FedLGT.","['ML: Distributed Machine Learning & Federated Learning', 'CV: Language and Vision', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['I-Jieh Liu', 'Ci-Siang Lin', 'Fu-En Yang', 'Yu-Chiang Frank Wang']","['Graduate Institute of Communication Engineering, National Taiwan University', 'Graduate Institute of Communication Engineering, National Taiwan University\nNVIDIA', 'Graduate Institute of Communication Engineering, National Taiwan University\nNVIDIA', 'Graduate Institute of Communication Engineering, National Taiwan University\nNVIDIA']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29299,Fairness & Bias,TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning,"Learning universal time series representations applicable to various types of downstream tasks is challenging but valuable in real applications. Recently, researchers have attempted to leverage the success of self-supervised contrastive learning (SSCL) in Computer Vision(CV) and Natural Language Processing(NLP) to tackle time series representation. Nevertheless, due to the special temporal characteristics, relying solely on empirical guidance from other domains may be ineffective for time series and difficult to adapt to multiple downstream tasks. To this end, we review three parts involved in SSCL including 1) designing augmentation methods for positive pairs, 2) constructing (hard) negative pairs, and 3) designing SSCL loss. For 1) and 2), we find that unsuitable positive and negative pair construction may introduce inappropriate inductive biases, which neither preserve temporal properties nor provide sufficient discriminative features. For 3), just exploring segment- or instance-level semantics information is not enough for learning universal representation. To remedy the above issues, we propose a novel self-supervised framework named TimesURL. Specifically, we first introduce a frequency-temporal-based augmentation to keep the temporal property unchanged. And then, we construct double Universums as a special kind of hard negative to guide better contrastive learning. Additionally, we introduce time reconstruction as a joint optimization objective with contrastive learning to capture both segment-level and instance-level information. As a result, TimesURL can learn high-quality universal representations and achieve state-of-the-art performance in 6 different downstream tasks, including short- and long-term forecasting, imputation, classification, anomaly detection and transfer learning.",['ML: Time-Series/Data Streams'],[],"['Jiexi Liu', 'Songcan Chen']","['College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics\nMIIT Key Laboratory of Pattern Analysis and Machine Intelligence', 'College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics\nMIIT Key Laboratory of Pattern Analysis and Machine Intelligence']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29311,Security,UFDA: Universal Federated Domain Adaptation with Practical Assumptions,"Conventional Federated Domain Adaptation (FDA) approaches usually demand an abundance of assumptions, which makes them significantly less feasible for real-world situations and introduces security hazards. This paper relaxes the assumptions from previous FDAs and studies a more practical scenario named Universal Federated Domain Adaptation (UFDA). It only requires the black-box model and the label set information of each source domain, while the label sets of different source domains could be inconsistent, and the target-domain label set is totally blind. Towards a more effective solution for our newly proposed UFDA scenario, we propose a corresponding methodology called Hot-Learning with Contrastive Label Disambiguation (HCLD). It particularly tackles UFDA's domain shifts and category gaps problems by using one-hot outputs from the black-box models of various source domains. Moreover, to better distinguish the shared and unknown classes, we further present a cluster-level strategy named Mutual-Voting Decision (MVD) to extract robust consensus knowledge across peer classes from both source and target domains. Extensive experiments on three benchmark datasets demonstrate that our method achieves comparable performance for our UFDA scenario with much fewer assumptions, compared to previous methodologies with comprehensive additional assumptions.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Privacy', 'ML: Unsupervised & Self-Supervised Learning', 'PEAI: Privacy & Security']",[],"['Xinhui Liu', 'Zhenghao Chen', 'Luping Zhou', 'Dong Xu', 'Wei Xi', 'Gairui Bai', 'Yihan Zhao', 'Jizhong Zhao']","[""Xi'an Jiaotong University\nUniversity of Sydney"", 'University of Sydney', 'University of Sydney', 'The University of Hong Kong', ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University""]","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29314,Transparency & Explainability,Causality-Inspired Invariant Representation Learning for Text-Based Person Retrieval,"Text-based Person Retrieval (TPR) aims to retrieve relevant images of specific pedestrians based on the given textual query. The mainstream approaches primarily leverage pretrained deep neural networks to learn the mapping of visual and textual modalities into a common latent space for cross-modality matching. Despite their remarkable achievements, existing efforts mainly focus on learning the statistical cross-modality correlation found in training data, other than the intrinsic causal correlation. As a result, they often struggle to retrieve accurately in the face of environmental changes such as illumination, pose, and occlusion, or when encountering images with similar attributes. In this regard, we pioneer the observation of TPR from a causal view. Specifically, we assume that each image is composed of a mixture of causal factors (which are semantically consistent with text descriptions) and non-causal factors (retrieval-irrelevant, e.g., background), and only the former can lead to reliable retrieval judgments. Our goal is to extract text-critical robust visual representation (i.e., causal factors) and establish domain invariant cross-modality correlations for accurate and reliable retrieval. However, causal/non-causal factors are unobserved, so we emphasize that ideal causal factors that can simulate causal scenes should satisfy two basic principles:1） Independence: being independent of non-causal factors, and 2）Sufficiency: being causally sufficient for TPR across different environments. Building on that, we propose an Invariant Representation Learning method for TPR (IRLT), that enforces the visual representations to satisfy the two aforementioned critical properties. Extensive experiments on three datasets clearly demonstrate the advantages of IRLT over leading baselines in terms of accuracy and generalization.","['ML: Multimodal Learning', 'ML: Causal Learning']",[],"['Yu Liu', 'Guihe Qin', 'Haipeng Chen', 'Zhiyong Cheng', 'Xun Yang']","['College of Computer Science and Technology, Jilin University, China;\nKey Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China', 'College of Computer Science and Technology, Jilin University, China;\nKey Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China', 'College of Computer Science and Technology, Jilin University, China;\nKey Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China', 'Qilu University of Technology (Shandong Academy of Sciences), JiNan, China', 'University of Science and Technology of China, HeFei, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29315,Transparency & Explainability,Detection-Based Intermediate Supervision for Visual Question Answering,"Recently, neural module networks (NMNs) have yielded ongoing success in answering compositional visual questions, especially those involving multi-hop visual and logical reasoning. NMNs decompose the complex question into several sub-tasks using instance-modules from the reasoning paths of that question and then exploit intermediate supervisions to guide answer prediction, thereby improving inference interpretability. However, their performance may be hindered due to sketchy modeling of intermediate supervisions. For instance, (1) a prior assumption that each instance-module refers to only one grounded object yet overlooks other potentially associated grounded objects, impeding full cross-modal alignment learning; (2) IoU-based intermediate supervisions may introduce noise signals as the bounding box overlap issue might guide the model's focus towards irrelevant objects. To address these issues, a novel method, Detection-based Intermediate Supervision (DIS), is proposed, which adopts a generative detection framework to facilitate multiple grounding supervisions via sequence generation. As such, DIS offers more comprehensive and accurate intermediate supervisions, thereby boosting answer prediction performance. Furthermore, by considering intermediate results, DIS enhances the consistency in answering compositional questions and their sub-questions. Extensive experiments demonstrate the superiority of our proposed DIS, showcasing both improved accuracy and state-of-the-art reasoning consistency compared to prior approaches.","['ML: Multimodal Learning', 'CV: Language and Vision']",[],"['Yuhang Liu', 'Daowan Peng', 'Wei Wei', 'Yuanyuan Fu', 'Wenfeng Xie', 'Dangyang Chen']","['CCIIP Lab, School of Computer Science and Technology, Huazhong University of Science and Technology\nJoint Laboratory of HUST and Pingan Property & Casualty Research (HPL)\nByteDance Inc.', 'CCIIP Lab, School of Computer Science and Technology, Huazhong University of Science and Technology\nJoint Laboratory of HUST and Pingan Property & Casualty Research (HPL)', 'CCIIP Lab, School of Computer Science and Technology, Huazhong University of Science and Technology\nJoint Laboratory of HUST and Pingan Property & Casualty Research (HPL)', 'Ping An Property & Casualty Insurance Company of China, Ltd\nJoint Laboratory of HUST and Pingan Property & Casualty Research (HPL)', 'Ping An Property & Casualty Insurance Company of China, Ltd\nJoint Laboratory of HUST and Pingan Property & Casualty Research (HPL)', 'Ping An Property & Casualty Insurance Company of China, Ltd\nJoint Laboratory of HUST and Pingan Property & Casualty Research (HPL)']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29323,Security,On the Convergence of an Adaptive Momentum Method for Adversarial Attacks,"Adversarial examples are commonly created by solving a constrained optimization problem, typically using sign-based methods like Fast Gradient Sign Method (FGSM). These attacks can benefit from momentum with a constant parameter, such as Momentum Iterative FGSM (MI-FGSM), to enhance black-box transferability. However, the monotonic time-varying momentum parameter is required to guarantee convergence in theory, creating a theory-practice gap. Additionally, recent work shows that sign-based methods fail to converge to the optimum in several convex settings, exacerbating the issue. To address these concerns, we propose a novel method which incorporates both an innovative adaptive momentum parameter without monotonicity assumptions  and an adaptive step-size scheme that replaces the sign operation. Furthermore, we derive a regret upper bound for general convex functions. Experiments on multiple models demonstrate the efficacy of our method in generating adversarial examples with human-imperceptible noise while achieving high attack success rates, indicating its superiority over previous adversarial example generation methods.","['ML: Optimization', 'CV: Adversarial Attacks & Robustness']",[],"['Sheng Long', 'Wei Tao', 'Shuohao LI', 'Jun Lei', 'Jun Zhang']","['Laboratory for Big Data and Decision, National University of Defense Technology, Changsha 410073, China', 'Laboratory for Big Data and Decision, National University of Defense Technology, Changsha 410073, China\nStrategic Assessments and Consultation Institute, Academy of Military Science, Beijing 100091, China', 'Laboratory for Big Data and Decision, National University of Defense Technology, Changsha 410073, China', 'Laboratory for Big Data and Decision, National University of Defense Technology, Changsha 410073, China', 'Laboratory for Big Data and Decision, National University of Defense Technology, Changsha 410073, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29321,Privacy & Data Governance,Backdoor Attacks via Machine Unlearning,"As a new paradigm to erase data from a model and protect user privacy, machine unlearning has drawn significant attention. However, existing studies on machine unlearning mainly focus on its effectiveness and efficiency, neglecting the security challenges introduced by this technique. In this paper, we aim to bridge this gap and study the possibility of conducting malicious attacks leveraging machine unlearning. Specifically, we consider the backdoor attack via machine unlearning, where an attacker seeks to inject a backdoor in the unlearned model by submitting malicious unlearning requests, so that the prediction made by the unlearned model can be changed when a particular trigger presents. In our study, we propose two attack approaches. The first attack approach does not require the attacker to poison any training data of the model. The attacker can achieve the attack goal only by requesting to unlearn a small subset of his contributed training data. The second approach allows the attacker to poison a few training instances with a pre-defined trigger upfront, and then activate the attack via submitting a malicious unlearning request. Both attack approaches are proposed with the goal of maximizing the attack utility while ensuring attack stealthiness. The effectiveness of the proposed attacks is demonstrated with different machine unlearning algorithms as well as different models on different datasets.","['ML: Adversarial Learning & Robustness', 'ML: Classification and Regression', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Zihao Liu', 'Tianhao Wang', 'Mengdi Huai', 'Chenglin Miao']","['Iowa State University', 'University of Virginia', 'Iowa State University', 'Iowa State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29317,Transparency & Explainability,Diffusion Language-Shapelets for Semi-supervised Time-Series Classification,"Semi-supervised time-series classification could effectively alleviate the issue of lacking labeled data. However, existing approaches usually ignore model interpretability, making it difficult for humans to understand the principles behind the predictions of a model. Shapelets are a set of discriminative subsequences that show high interpretability in time series classification tasks. Shapelet learning-based methods have demonstrated promising classification performance. Unfortunately, without enough labeled data, the shapelets learned by existing methods are often poorly discriminative, and even dissimilar to any subsequence of the original time series. To address this issue, we propose the Diffusion Language-Shapelets model (DiffShape) for semi-supervised time series classification. In DiffShape, a self-supervised diffusion learning mechanism is designed, which uses real subsequences as a condition. This helps to increase the similarity between the learned shapelets and real subsequences by using a large amount of unlabeled data. Furthermore, we introduce a contrastive language-shapelets learning strategy that improves the discriminability of the learned shapelets by incorporating the natural language descriptions of the time series. Experiments have been conducted on the UCR time series archive, and the results reveal that the proposed DiffShape method achieves state-of-the-art performance and exhibits superior interpretability over baselines.","['ML: Time-Series/Data Streams', 'ML: Semi-Supervised Learning']",[],"['Zhen Liu', 'Wenbin Pei', 'Disen Lan', 'Qianli Ma']","['South China University of Technology', 'Dalian University of Technology', 'South China University of Technology', 'South China University of Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29329,Security,Federated Learning with Extremely Noisy Clients via Negative Distillation,"Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., >90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using noisy labels and pseudo-labels obtained by global models. The model trained on noisy labels serves as a ‘bad teacher’ in knowledge distillation, aiming to decrease the risk of providing incorrect information. Meanwhile, the model trained on pseudo-labels is involved in model aggregation if not identified as a noisy client. Consequently, through pseudo-labeling, FedNed gradually increases the trustworthiness of models trained on noisy clients, while leveraging all clients for model aggregation through negative distillation. To verify the efficacy of FedNed, we conduct extensive experiments under various settings, demonstrating that FedNed can consistently outperform baselines and achieve state-of-the-art performance.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Deep Learning Algorithms', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Yang Lu', 'Lin Chen', 'Yonggang Zhang', 'Yiliang Zhang', 'Bo Han', 'Yiu-ming Cheung', 'Hanzi Wang']","['Xiamen University', 'Xiamen University', 'Hong Kong Baptist University', 'Xiamen University', 'Hong Kong Baptist University', 'Hong Kong Baptist University', 'Xiamen University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29333,Fairness & Bias,Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision,"Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measure a model’s downstream fairness by computing the standard deviation in the probability of predicting the true occupation label across the different identity groups. Using this fairness metric, we find significant disparities between the evaluated vision-and-language models. We hope that our work demonstrates the potential value of diffusion methods for fairness evaluations.","['ML: Ethics', 'Bias', 'and Fairness', 'General']",[],"['Nicholas Lui', 'Bryan Chia', 'William Berrios', 'Candace Ross', 'Douwe Kiela']","['Stanford University', 'Stanford University', 'Contextual AI', 'Meta AI', 'Stanford University\nContextual AI']","['United States', 'United States', 'United States', '', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29334,Fairness & Bias,Three Heads Are Better than One: Complementary Experts for Long-Tailed Semi-supervised Learning,"We address the challenging problem of Long-Tailed Semi-Supervised Learning (LTSSL) where labeled data exhibit imbalanced class distribution and unlabeled data follow an unknown distribution. Unlike in balanced SSL, the generated pseudo-labels are skewed towards head classes, intensifying the training bias. Such a phenomenon is even amplified as more unlabeled data will be mislabeled as head classes when the class distribution of labeled and unlabeled datasets are mismatched. To solve this problem, we propose a novel method named ComPlementary Experts (CPE). Specifically, we train multiple experts to model various class distributions, each of them yielding high-quality pseudo-labels within one form of class distribution. Besides, we introduce Classwise Batch Normalization for CPE to avoid performance degradation caused by feature distribution mismatch between head and non-head classes. CPE achieves state-of-the-art performances on CIFAR-10-LT, CIFAR-100-LT, and STL-10-LT dataset benchmarks. For instance, on CIFAR-10-LT, CPE improves test accuracy by over >2.22% compared to baselines. Code is available at https://github.com/machengcheng2016/CPE-LTSSL.","['ML: Semi-Supervised Learning', 'ML: Classification and Regression']",[],"['Chengcheng Ma', 'Ismail Elezi', 'Jiankang Deng', 'Weiming Dong', 'Changsheng Xu']","['Institute of Automation, Chinese Academy of Sciences, Beijing, China\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', ""Huawei Noah's Ark Lab, London, UK"", ""Huawei Noah's Ark Lab, London, UK"", 'Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Institute of Automation, Chinese Academy of Sciences, Beijing, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29345,Transparency & Explainability,GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent,"Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to inaccurate trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation, to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks. The implementation is available under: https://github.com/s-marton/GradTree","['ML: Neuro-Symbolic Learning', 'ML: Classification and Regression']",[],"['Sascha Marton', 'Stefan Lüdtke', 'Christian Bartelt', 'Heiner Stuckenschmidt']","['University of Mannheim', 'University of Rostock', 'University of Mannheim', 'University of Mannheim']","['Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29346,Security,Optimal Attack and Defense for Reinforcement Learning,"To ensure the usefulness of Reinforcement Learning (RL) in real systems, it is crucial to ensure they are robust to noise and adversarial attacks. In adversarial RL, an external attacker has the power to manipulate the victim agent's interaction with the environment. We study the full class of online manipulation attacks, which include (i) state attacks, (ii) observation attacks (which are a generalization of perceived-state attacks), (iii) action attacks, and (iv) reward attacks. We show the attacker's problem of designing a stealthy attack that maximizes its own expected reward, which often corresponds to minimizing the victim's value, is captured by a Markov Decision Process (MDP) that we call a meta-MDP since it is not the true environment but a higher level environment induced by the attacked interaction. We show that the attacker can derive optimal attacks by planning in polynomial time or learning with polynomial sample complexity using standard RL techniques. We argue that the optimal defense policy for the victim can be computed as the solution to a stochastic Stackelberg game, which can be further simplified into a partially-observable turn-based stochastic game (POTBSG). Neither the attacker nor the victim would benefit from deviating from their respective optimal policies, thus such solutions are truly robust. Although the defense problem is NP-hard, we show that optimal Markovian defenses can be computed (learned) in polynomial time (sample complexity) in many scenarios.","['ML: Adversarial Learning & Robustness', 'ML: Reinforcement Learning', 'MAS: Adversarial Agents', 'MAS: Multiagent Systems under Uncertainty']",[],"['Jeremy McMahan', 'Young Wu', 'Xiaojin Zhu', 'Qiaomin Xie']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29339,Privacy & Data Governance,PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN in Federated Learning,"Federated learning (FL) has attracted growing attention since it allows for privacy-preserving collaborative training on decentralized clients without explicitly uploading sensitive data to the central server. However, recent works have revealed that it still has the risk of exposing private data to adversaries. In this paper, we conduct reconstruction attacks and enhance inference attacks on various datasets to better understand that sharing trained classification model parameters to a central server is the main problem of privacy leakage in FL. To tackle this problem, a privacy-preserving image distribution sharing scheme with GAN (PPIDSG) is proposed, which consists of a block scrambling-based encryption algorithm, an image distribution sharing method, and local classification training. Specifically, our method can capture the distribution of a target image domain which is transformed by the block encryption algorithm, and upload generator parameters to avoid classifier sharing with negligible influence on model performance. Furthermore, we apply a feature extractor to motivate model utility and train it separately from the classifier. The extensive experimental results and security analyses demonstrate the superiority of our proposed scheme compared to other state-of-the-art defense methods. The code is available at https://github.com/ytingma/PPIDSG.","['ML: Privacy', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Yuting Ma', 'Yuanzhi Yao', 'Xiaohua Xu']","['University of Science and Technology of China', 'Hefei University of Technology', 'University of Science and Technology of China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29351,Security,Input Margins Can Predict Generalization Too,"Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as 'constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general. This provides a novel insight on the relationship between generalization and classification margins, and highlights the importance of considering the data manifold for investigations of generalization in DNNs.","['ML: Representation Learning', 'ML: Adversarial Learning & Robustness', 'ML: Deep Learning Theory']",[],"['Coenraad Mouton', 'Marthinus Wilhelmus Theunissen', 'Marelie H Davel']","['Faculty of Engineering, North-West University, South Africa\nCentre for Artificial Intelligence Research, South Africa\nSouth African National Space Agency', 'Faculty of Engineering, North-West University, South Africa\nCentre for Artificial Intelligence Research, South Africa', 'Faculty of Engineering, North-West University, South Africa\nCentre for Artificial Intelligence Research, South Africa\nNational Institute for Theoretical and Computational Sciences, South Africa']","['South Africa', 'South Africa', 'South Africa']"
https://ojs.aaai.org/index.php/AAAI/article/view/29352,Transparency & Explainability,Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles,"While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'GTEP: Cooperative Game Theory', 'ML: Ensemble Methods', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Maximilian Muschalik', 'Fabian Fumagalli', 'Barbara Hammer', 'Eyke Hüllermeier']","['LMU Munich, MCML Munich, D-80539 Munich, Germany', 'Bielefeld University, CITEC, D-33619 Bielefeld, Germany', 'Bielefeld University, CITEC, D-33619 Bielefeld, Germany', 'LMU Munich, MCML Munich, D-80539 Munich, Germany']","['Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29357,Security,Secure Distributed Sparse Gaussian Process Models Using Multi-Key Homomorphic Encryption,"Distributed sparse Gaussian process (dGP) models provide an ability to achieve accurate predictive performance using data from multiple devices in a time efficient and scalable manner. The distributed computation of model, however, risks exposure of privately owned data to public manipulation. In this paper we propose a secure solution for dGP regression models using multi-key homomorphic encryption. Experimental results show that with a little sacrifice in terms of time complexity, we achieve a secure dGP model without deteriorating the predictive performance compared to traditional non-secure dGP models. We also present a practical implementation of the proposed model using several Nvidia Jetson Nano Developer Kit modules to simulate a real-world scenario. Thus, secure dGP model plugs the data security issues of dGP and provide a secure and trustworthy solution for multiple devices to use privately owned data for model computation in a distributed environment availing speed, scalability and robustness of dGP.","['ML: Privacy', 'ML: Distributed Machine Learning & Federated Learning', 'ML: Bayesian Learning', 'ML: Kernel Methods']",[],"['Adil Nawaz', 'Guopeng Chen', 'Muhammad Umair Raza', 'Zahid Iqbal', 'Jianqiang  Li', 'Victor C.M. Leung', 'Jie Chen']","['Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29365,Privacy & Data Governance,Towards Fair Graph Federated Learning via Incentive Mechanisms,"Graph federated learning (FL) has emerged as a pivotal paradigm enabling  multiple agents to collaboratively train a graph model while preserving local data privacy. Yet, current efforts overlook a key issue: agents are self-interested and would hesitant to share data without fair and satisfactory incentives. This paper is the first endeavor to address this issue by studying the incentive mechanism for graph federated learning. We identify a unique phenomenon in graph federated learning: the presence of agents posing potential harm to the federation and agents contributing with delays. This stands in contrast to previous FL incentive mechanisms that assume all agents contribute positively and in a timely manner.  In view of this, this paper presents a novel incentive mechanism  tailored for fair graph federated learning, integrating incentives derived from both model gradient and payoff. To achieve this, we first introduce an agent valuation function aimed at quantifying agent contributions through the introduction of two criteria: gradient alignment and graph diversity. Moreover, due to the high heterogeneity in graph federated learning, striking a balance between accuracy and fairness becomes particularly crucial. We introduce motif prototypes to enhance accuracy, communicated between the server and agents, enhancing global model aggregation and aiding agents in local model optimization. Extensive experiments show that our model achieves the best trade-off between accuracy and the fairness of model gradient, as well as superior payoff fairness.","['ML: Graph-based Machine Learning', 'ML: Privacy']",[],"['Chenglu Pan', 'Jiarong Xu', 'Yue Yu', 'Ziqi Yang', 'Qingbiao Wu', 'Chunping Wang', 'Lei Chen', 'Yang Yang']","['Zhejiang University\nFudan University', 'Fudan University', 'Fudan University', 'Zhejiang University\nZJU-Hangzhou Global Scientific and Technological Innovation Center', 'Zhejiang University', 'FinVolution Group', 'FinVolution Group', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', '', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29368,Fairness & Bias,FedLF: Layer-Wise Fair Federated Learning,"Fairness has become an important concern in Federated Learning (FL). An unfair model that performs well for some clients while performing poorly for others can reduce the willingness of clients to participate. In this work, we identify a direct cause of unfairness in FL - the use of an unfair direction to update the global model, which favors some clients while conflicting with other clients’ gradients at the model and layer levels. To address these issues, we propose a layer-wise fair Federated Learning algorithm (FedLF). Firstly, we formulate a multi-objective optimization problem with an effective fair-driven objective for FL. A layer-wise fair direction is then calculated to mitigate the model and layer-level gradient conflicts and reduce the improvement bias. We further provide the theoretical analysis on how FedLF can improve fairness and guarantee convergence. Extensive experiments on different learning tasks and models demonstrate that FedLF outperforms the SOTA FL algorithms in terms of accuracy and fairness. The source code is available at https://github.com/zibinpan/FedLF.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Applications', 'PEAI: Bias', 'Fairness & Equity']",[],"['Zibin Pan', 'Chi Li', 'Fangchen Yu', 'Shuyi Wang', 'Haijin Wang', 'Xiaoying Tang', 'Junhua Zhao']","['The Chinese University of Hong Kong, Shenzhen\nThe Shenzhen Institute of Artificial Intelligence and Robotics for Society', 'The Chinese University of Hong Kong, Shenzhen\nShenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen', 'The Chinese University of Hong Kong, Shenzhen\nThe Shenzhen Institute of Artificial Intelligence and Robotics for Society', 'The Chinese University of Hong Kong, Shenzhen\nThe Shenzhen Institute of Artificial Intelligence and Robotics for Society', 'The Chinese University of Hong Kong, Shenzhen\nThe Shenzhen Institute of Artificial Intelligence and Robotics for Society\nThe Guangdong Provincial Key Laboratory of Future Networks of Intelligence', 'The Chinese University of Hong Kong, Shenzhen\nThe Shenzhen Institute of Artificial Intelligence and Robotics for Society']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/29372,Transparency & Explainability,CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems,"We present CrystalBox, a novel, model-agnostic, posthoc explainability framework for Deep Reinforcement Learning (DRL) controllers in the large family of input-driven environments which includes computer systems. We combine the natural decomposability of reward functions in input-driven environments with the explanatory power of decomposed returns. We propose an efficient algorithm to generate future-based explanations across both discrete and continuous control environments. Using applications such as adaptive bitrate streaming and congestion control, we demonstrate CrystalBox's capability to generate high-fidelity explanations. We further illustrate its higher utility across three practical use cases: contrastive explanations, network observability, and guided reward design, as opposed to prior explainability techniques that identify salient features.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Reinforcement Learning', 'APP: Web', 'APP: Other Applications']",[],"['Sagar Patel', 'Sangeetha Abdu Jyothi', 'Nina Narodytska']","['University of California, Irvine', 'University of California, Irvine\nVMware Research', 'VMware Research']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29375,Fairness & Bias,Variational Hybrid-Attention Framework for Multi-Label Few-Shot Aspect Category Detection,"Multi-label few-shot aspect category detection (FS-ACD) is a challenging sentiment analysis task, which aims to learn a multi-label learning paradigm with limited training data. The difficulty of this task is how to use limited data to generalize effective discriminative representations for different categories. Nowadays, all advanced FS-ACD works utilize the prototypical network to learn label prototypes to represent different aspects. However, such point-based estimation methods are inherently noise-susceptible and bias-vulnerable. To this end, this paper proposes a novel Variational Hybrid-Attention Framework (VHAF) for the FS-ACD task. Specifically, to alleviate the data noise, we adopt a hybrid-attention mechanism to generate more discriminative aspect-specific embeddings. Then, based on these embeddings, we introduce the variational distribution inference to obtain the aspect-specific distribution as a more robust aspect representation, which can eliminate the scarce data bias for better inference. Moreover, we further leverage an adaptive threshold estimation to help VHAF better identify multiple relevant aspects. Extensive experiments on three datasets demonstrate the effectiveness of our VHAF over other state-of-the-art methods. Code is available at https://github.com/chengzju/VHAF.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Multi-class/Multi-label Learning & Extreme Classification']",[],"['Cheng Peng', 'Ke Chen', 'Lidan Shou', 'Gang Chen']","['The State Key Laboratory of Blockchain and Data Security, Zhejiang University', 'The State Key Laboratory of Blockchain and Data Security, Zhejiang University', 'The State Key Laboratory of Blockchain and Data Security, Zhejiang University', 'The State Key Laboratory of Blockchain and Data Security, Zhejiang University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29378,Transparency & Explainability,Learning Performance Maximizing Ensembles with Explainability Guarantees,"In this paper we propose a method for the optimal allocation of observations between an intrinsically explainable glass box model and a black box model. An optimal allocation being defined as one which, for any given explainability level (i.e. the proportion of observations for which the explainable model is the prediction function), maximizes the performance of the ensemble on the underlying task, and maximizes performance of the explainable model on the observations allocated to it, subject to the maximal ensemble performance condition. The proposed method is shown to produce such explainability optimal allocations on a benchmark suite of tabular datasets across a variety of explainable and black box model types. These learned allocations are found to consistently maintain ensemble performance at very high explainability levels (explaining 74% of observations on average), and in some cases even outperform both the component explainable and black box models while improving explainability.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Ensemble Methods', 'ML: Learning Preferences or Rankings', 'ML: Deep Learning Algorithms', 'ML: Classification and Regression', 'CSO: Satisfiability']",[],"['Vincent Pisztora', 'Jia Li']","['Pennsylvania State University', 'Pennsylvania State University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29379,Fairness & Bias,Reconciling Predictive and Statistical Parity: A Causal Approach,"Since the rise of fair machine learning as a critical field of inquiry, many different notions on how to quantify and measure discrimination have been proposed in the literature. Some of these notions, however, were shown to be mutually incompatible. Such findings make it appear that numerous different kinds of fairness exist, thereby making a consensus on the appropriate measure of fairness harder to reach, hindering the applications of these tools in practice. In this paper, we investigate one of these key impossibility results that relates the notions of statistical and predictive parity. Specifically, we derive a new causal decomposition formula for the fairness measures associated with predictive parity, and obtain a novel insight into how this criterion is related to statistical parity through the legal doctrines of disparate treatment, disparate impact, and the notion of business necessity. Our results show that through a more careful causal analysis, the notions of statistical and predictive parity are not really mutually exclusive, but complementary and spanning a spectrum of fairness notions through the concept of business necessity. Finally, we demonstrate the importance of our findings on a real-world example.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Causal Learning']",[],"['Drago Plecko', 'Elias Bareinboim']","['Columbia University', 'Columbia University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29382,Transparency & Explainability,Towards Modeling Uncertainties of Self-Explaining Neural Networks via Conformal Prediction,"Despite the recent progress in deep neural networks (DNNs), it remains challenging to explain the predictions made by DNNs. Existing explanation methods for DNNs mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations. The fact that post-hoc methods can fail to reveal the actual original reasoning process of DNNs raises the need to build DNNs with built-in interpretability. Motivated by this, many self-explaining neural networks have been proposed to generate not only accurate predictions but also clear and intuitive insights into why a particular decision was made. However, existing self-explaining networks are limited in providing distribution-free uncertainty quantification for the two simultaneously generated prediction outcomes (i.e., a sample's final prediction and its corresponding explanations for interpreting that prediction). Importantly, they also fail to establish a connection between the confidence values assigned to the generated explanations in the interpretation layer and those allocated to the final predictions in the ultimate prediction layer. To tackle the aforementioned challenges, in this paper, we design a novel uncertainty modeling framework for self-explaining networks, which not only demonstrates strong distribution-free uncertainty modeling performance for the generated explanations in the interpretation layer but also excels in producing efficient and effective prediction sets for the final predictions based on the informative high-level basis explanations. We perform the theoretical analysis for the proposed framework. Extensive experimental evaluation demonstrates the effectiveness of the proposed uncertainty framework.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'PEAI: Accountability', 'Interpretability & Explainability']",[],"['Wei Qian', 'Chenxu Zhao', 'Yangyi Li', 'Fenglong Ma', 'Chao Zhang', 'Mengdi Huai']","['Iowa State University', 'Iowa State University', 'Iowa State University', 'Pennsylvania State University', 'Georgia Institute of Technology', 'Iowa Sate University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29388,Security,Integer Is Enough: When Vertical Federated Learning Meets Rounding,"Vertical Federated Learning (VFL) is a solution increasingly used by companies with the same user group but differing features, enabling them to collaboratively train a machine learning model.  VFL ensures that clients exchange intermediate results extracted by their local models, without sharing raw data.  However, in practice, VFL encounters several challenges, such as computational and communication overhead, privacy leakage risk, and adversarial attack.  Our study reveals that the usage of floating-point (FP) numbers is a common factor causing these issues, as they can be redundant and contain too much information.  To address this, we propose a new architecture called rounding layer, which converts intermediate results to integers.  Our theoretical analysis and empirical results demonstrate the benefits of the rounding layer in reducing computation and memory overhead, providing privacy protection, preserving model performance, and mitigating adversarial attacks.  We hope this paper inspires further research into novel architectures to address practical issues in VFL.",['ML: Distributed Machine Learning & Federated Learning'],[],"['Pengyu Qiu', 'Yuwen Pu', 'Yongchao Liu', 'Wenyan Liu', 'Yun Yue', 'Xiaowei Zhu', 'Lichun Li', 'Jinbao Li', 'Shouling Ji']","['Zhejiang University\nAnt Group', 'Zhejiang university', 'Ant Group', 'Ant Group\nZhejiang University', 'Ant Group', 'Ant Group', 'Ant Group', 'Qilu University of Technology', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29385,Security,Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective,"Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, in reality, model updates can easily become mixed and scattered throughout due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge themselves through picking up from the candidates. We compare Snowball with state-of-the-art defenses to backdoor attacks in FL on five real-world datasets, demonstrating its superior resistance to backdoor attacks and slight impact on the accuracy of the global model.",['ML: Distributed Machine Learning & Federated Learning'],[],"['Zhen Qin', 'Feiyi Chen', 'Chen Zhi', 'Xueqiang Yan', 'Shuiguang Deng']","['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'School of Software Technology, Zhejiang University, Ningbo, China', 'Huawei Technologies Co. Ltd., Shanghai, China', 'College of Computer Science and Technology, Zhejiang University, Hangzhou, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29397,Transparency & Explainability,Using Stratified Sampling to Improve LIME Image Explanations,"We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator.  Experiments show the efficacy of the proposed approach.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'RU: Stochastic Optimization', 'SO: Sampling/Simulation-based Search']",[],"['Muhammad Rashid', 'Elvio G. Amparore', 'Enrico Ferrari', 'Damiano Verda']","['University of Torino, Computer Science Department, C.so Svizzera 185, 10149 Torino, Italy', 'University of Torino, Computer Science Department, C.so Svizzera 185, 10149 Torino, Italy', 'Rulex Innovation Labs, Via Felice Romani 9, 16122 Genova, Italy', 'Rulex Innovation Labs, Via Felice Romani 9, 16122 Genova, Italy']","['Japan', 'Japan', 'Colombia', 'Colombia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29394,Fairness & Bias,Fair Participation via Sequential Policies,"Leading approaches to algorithmic fairness and policy-induced distribution shift are often misaligned with long-term objectives in sequential settings. We aim to correct these shortcomings by ensuring that both the objective and fairness constraints account for policy-induced distribution shift. First, we motivate this problem using an example in which individuals subject to algorithmic predictions modulate their willingness to participate with the policy maker. Fairness in this example is measured by the variance of group participation rates. Next, we develop a method for solving the resulting constrained, non-linear optimization problem and prove that this method converges to a fair, locally optimal policy given first-order information. Finally, we experimentally validate our claims in a semi-synthetic setting.","['ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Bias', 'Fairness & Equity']",[],"['Reilly Raab', 'Ross Boczar', 'Maryam Fazel', 'Yang Liu']","['University of California, Santa Cruz', 'University of Washington', 'University of Washington', 'University of California, Santa Cruz']","['United States', 'United States', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29398,Fairness & Bias,NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation,"Causal effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each of these methods addresses a specific aspect of causal effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network (NN) architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Causal Effect Estimator (NESTER), a generalized method for causal effect estimation. NESTER integrates the ideas used in existing methods based on multi-head NNs for causal effect estimation into one framework. We design a Domain Specific Language (DSL) tailored for causal effect estimation based on causal inductive biases used in literature. We conduct a theoretical analysis to investigate NESTER's efficacy in estimating causal effects. Our comprehensive empirical results show that NESTER performs better than state-of-the-art methods on benchmark datasets.","['ML: Causal Learning', 'ML: Deep Learning Algorithms', 'ML: Neuro-Symbolic Learning']",[],"['Abbavaram Gowtham Reddy', 'Vineeth N Balasubramanian']","['Indian Institute of Technology, Hyderabad', 'Indian Institute of Technology, Hyderabad']","['India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/29403,Transparency & Explainability,Limitations of Face Image Generation,"Text-to-image diffusion models have achieved widespread popularity due to their unprecedented image generation capability. In particular, their ability to synthesize and modify human faces has spurred research into using generated face images in both training data augmentation and model performance assessments. In this paper, we study the efficacy and shortcomings of generative models in the context of face generation. Utilizing a combination of qualitative and quantitative measures, including embedding-based metrics and user studies, we present a framework to audit the characteristics of generated faces conditioned on a set of social attributes. We applied our framework on faces generated through state-of-the-art text-to-image diffusion models. We identify several limitations of face image generation that include faithfulness to the text prompt, demographic disparities, and distributional shifts. Furthermore, we present an analytical model that provides insights into how training data selection contributes to the performance of generative models. Our survey data and analytics code can be found online at https://github.com/wi-pi/Limitations_of_Face_Generation","['ML: Ethics', 'Bias', 'and Fairness', 'CV: Bias', 'Fairness & Privacy', 'ML: Deep Generative Models & Autoencoders']",[],"['Harrison Rosenberg', 'Shimaa Ahmed', 'Guruprasad Ramesh', 'Kassem Fawaz', 'Ramya Korlakai Vinayak']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29402,Privacy & Data Governance,Protect Your Score: Contact-Tracing with Differential Privacy Guarantees,"The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score with epsilon=1 differential privacy, we achieve a two to ten-fold reduction in the infection rate of the virus. To the best of our knowledge, this presents the first contact tracing algorithm with differential privacy guarantees when revealing risk scores for COVID19.","['ML: Distributed Machine Learning & Federated Learning', 'APP: Security', 'MAS: Multiagent Systems under Uncertainty', 'ML: Graph-based Machine Learning']",[],"['Rob Romijnders', 'Christos Louizos', 'Yuki M. Asano', 'Max Welling']","['University of Amsterdam', 'Qualcomm AI research', 'University of Amsterdam', 'University of Amsterdam']","['Netherlands', '', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/29413,Transparency & Explainability,Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems,"Cognitive diagnosis assessment is a fundamental and crucial task for student learning. It models the student-exercise interaction, and discovers the students' proficiency levels on each knowledge attribute. In real-world intelligent education systems, generalization and interpretability of cognitive diagnosis methods are of equal importance. However, most existing methods can hardly make the best of both worlds due to the complicated student-exercise interaction. To this end, this paper proposes a symbolic cognitive diagnosis~(SCD) framework to simultaneously enhance generalization and interpretability. The SCD framework incorporates the symbolic tree to explicably represent the complicated student-exercise interaction function, and utilizes gradient-based optimization methods to effectively learn the student and exercise parameters. Meanwhile, the accompanying challenge is that we need to tunnel the discrete symbolic representation and continuous parameter optimization. To address this challenge, we propose to hybridly optimize the representation and parameters in an alternating manner. To fulfill SCD, it alternately learns the symbolic tree by derivative-free genetic programming and learns the student and exercise parameters via gradient-based Adam. The extensive experimental results on various real-world datasets show the superiority of SCD on both generalization and interpretability. The ablation study verifies the efficacy of each ingredient in SCD, and the case study explicitly showcases how the interpretable ability of SCD works.","['ML: Applications', 'ML: Optimization']",[],"['Junhao Shen', 'Hong Qian', 'Wei Zhang', 'Aimin Zhou']","['East China Normal University', 'East China Normal University', 'East China Normal University', 'East China Normal University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29414,Transparency & Explainability,BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence,"Measuring the coherence of text is a vital aspect of evaluating the quality of written content. Recent advancements in neural coherence modeling have demonstrated their efficacy in capturing entity coreference and discourse relations, thereby enhancing coherence evaluation. However, many existing methods heavily depend on static embeddings or focus narrowly on nearby context, constraining their capacity to measure the overarching coherence of long texts. In this paper, we posit that coherent texts inherently manifest a sequential and cohesive interplay among sentences, effectively conveying the central theme, purpose, or standpoint. To explore this abstract relationship, we introduce the ""BB Score,"" a novel reference-free metric grounded in Brownian bridge theory for assessing text coherence. Our findings showcase that when synergized with a simple additional classification component, this metric attains a performance level comparable to state-of-the-art techniques on standard artificial discrimination tasks. We also establish in downstream tasks that this metric effectively differentiates between human-written documents and text generated by large language models within specific domains. Furthermore, we illustrate the efficacy of this approach in detecting written styles attributed to various large language models, underscoring its potential for generalizability. In summary, we present a novel Brownian bridge coherence metric capable of measuring both local and global text coherence, while circumventing the need for end-to-end model training. This flexibility allows for its application in various downstream tasks.","['ML: Evaluation and Analysis', 'NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Zhecheng Sheng', 'Tianhao Zhang', 'Chen Jiang', 'Dongyeop Kang']","['University of Minnesota, Minneapolis, MN', 'University of Minnesota, Minneapolis, MN', 'University of Minnesota, Minneapolis, MN', 'University of Minnesota, Minneapolis, MN']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29418,Security,A Closer Look at Curriculum Adversarial Training: From an Online Perspective,"Curriculum adversarial training empirically finds that gradually increasing the hardness of adversarial examples can further improve the adversarial robustness of the trained model compared to conventional adversarial training. However, theoretical understanding of this strategy remains limited. In an attempt to bridge this gap, we analyze the adversarial training process from an online perspective. Specifically, we treat adversarial examples in different iterations as samples from different adversarial distributions. We then introduce the time series prediction framework and deduce novel generalization error bounds. Our theoretical results not only demonstrate the effectiveness of the conventional adversarial training algorithm but also explain why curriculum adversarial training methods can further improve adversarial generalization. We conduct comprehensive experiments to support our theory.","['ML: Learning Theory', 'ML: Adversarial Learning & Robustness']",[],"['Lianghe Shi', 'Weiwei Liu']","['Wuhan University', 'Wuhan University']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29420,Security,Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation,"Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher models. In this paper, we propose the teacher-agnostic data-free knowledge distillation (TA-DFKD) method, with the goal of more robust and stable performance regardless of teacher models. Our basic idea is to assign the teacher model a lenient expert role for evaluating samples, rather than a strict supervisor that enforces its class-prior on the generator. Specifically, we design a sample selection approach that takes only clean samples verified by the teacher model without imposing restrictions on the power of generating diverse samples. Through extensive experiments, we show that our method successfully achieves both robustness and training stability across various teacher models, while outperforming the existing DFKD methods.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Adversarial Learning & Robustness', 'ML: Deep Generative Models & Autoencoders', 'ML: Deep Learning Algorithms']",[],"['Hyunjune Shin', 'Dong-Wan Choi']","['Inha University', 'Inha University']","['South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29427,Fairness & Bias,Non-exemplar Domain Incremental Object Detection via Learning Domain Bias,"Domain incremental object detection (DIOD) aims to gradually learn a unified object detection model from a dataset stream composed of different domains, achieving good performance in all encountered domains. The most critical obstacle to this goal is the catastrophic forgetting problem, where the performance of the model improves rapidly in new domains but deteriorates sharply in old ones after a few sessions. To address this problem, we propose a non-exemplar DIOD method named learning domain bias (LDB), which learns domain bias independently at each new session, avoiding saving examples from old domains. Concretely, a base model is first obtained through training during session 1. Then, LDB freezes the weights of the base model and trains individual domain bias for each new incoming domain, adapting the base model to the distribution of new domains. At test time, since the domain ID is unknown, we propose a domain selector based on nearest mean classifier (NMC), which selects the most appropriate domain bias for a test image. Extensive experimental evaluations on two series of datasets demonstrate the effectiveness of the proposed LDB method in achieving high accuracy on new and old domain datasets. The code is available at https://github.com/SONGX1997/LDB.","['ML: Life-Long and Continual Learning', 'CV: Object Detection & Categorization']",[],"['Xiang Song', 'Yuhang He', 'Songlin Dong', 'Yihong Gong']","[""School of Software Engineering, Xi'an Jiaotong University"", ""College of Artificial Intelligence, Xi'an Jiaotong University"", ""College of Artificial Intelligence, Xi'an Jiaotong University"", ""College of Artificial Intelligence, Xi'an Jiaotong University""]","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29430,Fairness & Bias,Multi-Dimensional Fair Federated Learning,"Federated learning (FL) has emerged as a promising collaborative and secure paradigm for training a model from decentralized data without compromising privacy. Group fairness and client fairness are two dimensions of fairness that are important for FL. Standard FL can result in disproportionate disadvantages for certain clients, and it still faces the challenge of treating different groups equitably in a population. The problem of privately training fair FL models without compromising the generalization capability of disadvantaged clients remains open. In this paper, we propose a method, called mFairFL, to address this problem and achieve group fairness and client fairness simultaneously. mFairFL leverages differential multipliers to construct an optimization objective for empirical risk minimization with fairness constraints. Before aggregating locally trained models, it first detects conflicts among their gradients, and then iteratively curates the direction and magnitude of gradients to mitigate these conflicts. Theoretical analysis proves mFairFL facilitates the fairness in model development. The experimental evaluations based on three benchmark datasets show significant advantages of mFairFL compared to seven state-of-the-art baselines.","['ML: Ethics', 'Bias', 'and Fairness', 'CSO: Constraint Optimization', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Cong Su', 'Guoxian Yu', 'Jun Wang', 'Hui Li', 'Qingzhong Li', 'Han Yu']","['Shandong University', 'Shandong University', 'Shandong University', 'Shandong University', 'Shandong University', 'Nanyang Technological University (NTU)']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29435,Fairness & Bias,Towards Real-World Test-Time Adaptation: Tri-net Self-Training with Balanced Normalization,"Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training (ST) in learning from unlabeled data and adapt ST for test-time adaptation. However, ST alone is prone to over adaption which is responsible for the poor performance under continual domain shift. Hence, we propose to improve self-training under continual domain shift by regularizing model updates with an anchored loss. The final TTA model, termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm layers. We evaluate TRIBE on four datasets representing real-world TTA settings. TRIBE consistently achieves the state-of-the-art performance across multiple evaluation protocols.  The code is available at https://github.com/Gorilla-Lab-SCUT/TRIBE.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Life-Long and Continual Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Yongyi Su', 'Xun Xu', 'Kui Jia']","['South China University of Technology', 'Institute for Infocomm Research, A*STAR\nSouth China University of Technology', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29442,Transparency & Explainability,ACAMDA: Improving Data Efficiency in Reinforcement Learning through Guided Counterfactual Data Augmentation,"Data augmentation plays a crucial role in improving the data efficiency of reinforcement learning (RL). However, the generation of high-quality augmented data remains a significant challenge. To overcome this, we introduce ACAMDA (Adversarial Causal Modeling for Data Augmentation), a novel framework that integrates two causality-based tasks: causal structure recovery and counterfactual estimation. The unique aspect of ACAMDA lies in its ability to recover temporal causal relationships from limited non-expert datasets. The identification of the sequential cause-and-effect allows the creation of realistic yet unobserved scenarios. We utilize this characteristic to generate guided counterfactual datasets, which, in turn, substantially reduces the need for extensive data collection. By simulating various state-action pairs under hypothetical actions, ACAMDA enriches the training dataset for diverse and heterogeneous conditions. Our experimental evaluation shows that ACAMDA outperforms existing methods, particularly when applied to novel and unseen domains.","['ML: Reinforcement Learning', 'RU: Causality']",[],"['Yuewen Sun', 'Erli Wang', 'Biwei Huang', 'Chaochao Lu', 'Lu Feng', 'Changyin Sun', 'Kun Zhang']","['Mohamed bin Zayed University of Artificial Intelligence\nCarnegie Mellon University', 'NEC Labs, China', 'University of California San Diego', 'Shanghai AI Laboratory', 'NEC Labs, China', 'Anhui University', 'Mohamed bin Zayed University of Artificial Intelligence\nCarnegie Mellon University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29441,Fairness & Bias,Dual Self-Paced Cross-Modal Hashing,"Cross-modal hashing~(CMH) is an efficient technique to retrieve relevant data across different modalities, such as images, texts, and videos, which has attracted more and more attention due to its low storage cost and fast query speed. Although existing CMH methods achieve remarkable processes, almost all of them treat all samples of varying difficulty levels without discrimination, thus leaving them vulnerable to noise or outliers. Based on this observation, we reveal and study dual difficulty levels implied in cross-modal hashing learning, \ie instance-level and feature-level difficulty. To address this problem, we propose a novel Dual Self-Paced Cross-Modal Hashing (DSCMH) that mimics human cognitive learning to learn hashing from ``easy'' to ``hard'' in both instance and feature levels, thereby embracing robustness against noise/outliers. Specifically, our DSCMH assigns weights to each instance and feature to measure their difficulty or reliability, and then uses these weights to automatically filter out the noisy and irrelevant data points in the original space. By gradually increasing the weights during training, our method can focus on more instances and features from ``easy'' to ``hard'' in training, thus mitigating the adverse effects of noise or outliers. Extensive experiments are conducted on three widely-used benchmark datasets to demonstrate the effectiveness and robustness of the proposed DSCMH over 12 state-of-the-art CMH methods.","['ML: Multimodal Learning', 'ML: Multi-instance/Multi-view Learning', 'SO: Learning to Search']",[],"['Yuan Sun', 'Jian Dai', 'Zhenwen Ren', 'Yingke Chen', 'Dezhong Peng', 'Peng Hu']","['Sichuan University\nNational Innovation Center for UHD Video Technology', 'Tsinghua University', 'Southwest University of Science and Technology', 'Northumbria University', 'Sichuan University\nNational Innovation Center for UHD Video Technology', 'Sichuan University']","['United States', 'China', 'China', 'United Kingdom', 'United States', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29446,Privacy & Data Governance,FedCompetitors: Harmonious Collaboration in Federated Learning with Competing Participants,"Federated learning (FL) provides a privacy-preserving approach for collaborative training of machine learning models. Given the potential data heterogeneity, it is crucial to select appropriate collaborators for each FL participant (FL-PT) based on data complementarity. Recent studies have addressed this challenge. Similarly, it is imperative to consider the inter-individual relationships among FL-PTs where some FL-PTs engage in competition. Although FL literature has acknowledged the significance of this scenario, practical methods for establishing FL ecosystems remain largely unexplored. In this paper, we extend a principle from the balance theory, namely “the friend of my enemy is my enemy”, to ensure the absence of conflicting interests within an FL ecosystem. The extended principle and the resulting problem are formulated via graph theory and integer linear programming. A polynomial-time algorithm is proposed to determine the collaborators of each FL-PT. The solution guarantees high scalability, allowing even competing FL-PTs to smoothly join the ecosystem without conflict of interest. The proposed framework jointly considers competition and data heterogeneity. Extensive experiments on real-world and synthetic data demonstrate its efficacy compared to five alternative approaches, and its ability to establish efficient collaboration networks among FL-PTs.","['ML: Distributed Machine Learning & Federated Learning', 'MAS: Coordination and Collaboration', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Shanli Tan', 'Hao Cheng', 'Xiaohu Wu', 'Han Yu', 'Tiantian He', 'Yew Soon Ong', 'Chongjun Wang', 'Xiaofeng Tao']","['Beijing University of Posts and Telecommunications', 'Nanjing University', 'Beijing University of Posts and Telecommunications', 'Nanyang Technological University (NTU)', 'Agency for Science, Technology and Research (A*STAR)', 'Nanyang Technological University, Singapore\nA*STAR', 'Nanjing University', 'Beijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China', '', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29451,Fairness & Bias,DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction),"The Adam optimizer is a popular choice in contemporary deep learning due to its strong empirical performance. However we observe that in privacy sensitive scenarios, the traditional use of Differential Privacy (DP) with the Adam optimizer leads to sub-optimal performance on several tasks. We find that this performance degradation is due to a DP bias in Adam's second moment estimator, introduced by the addition of independent noise in the gradient computation to enforce DP guarantees. This DP bias leads to a different scaling for low variance parameter updates, that is inconsistent with the behavior of non-private Adam, and Adam's sign descent interpretation. We propose the DP-AdamBC optimization algorithm, which corrects for the bias in the second moment estimation and retrieves the expected behaviour of Adam. Empirically, DP-AdamBC significantly improves the optimization performance of DP-Adam by up to 3.5% in final accuracy in image, text, and graph node classification tasks.",['ML: Privacy'],[],"['Qiaoyue Tang', 'Frederick Shpilevskiy', 'Mathias Lécuyer']","['University of British Columbia', 'University of British Columbia', 'University of British Columbia']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/29454,Fairness & Bias,z-SignFedAvg: A Unified Stochastic Sign-Based Compression for Federated Learning,"Federated Learning (FL) is a promising privacy-preserving distributed learning paradigm but suffers from high communi- cation cost when training large-scale machine learning models. Sign-based methods, such as SignSGD, have been proposed as a biased gradient compression technique for reducing the communication cost. However, sign-based algorithms could diverge under heterogeneous data, which thus motivated the de- velopment of advanced techniques, such as the error-feedback method and stochastic sign-based compression, to fix this issue. Nevertheless, these methods still suffer from slower convergence rates, and none of them allows multiple local SGD updates like FedAvg. In this paper, we propose a novel noisy perturbation scheme with a general symmetric noise distribution for sign-based compression, which not only al- lows one to flexibly control the bias-variance tradeoff for the compressed gradient, but also provides a unified viewpoint to existing stochastic sign-based methods. More importantly, the proposed scheme enables the development of the very first sign-based FedAvg algorithm (z-SignFedAvg) to accelerate the convergence. Theoretically, we show that z-SignFedAvg achieves a faster convergence rate than existing sign-based methods and, under the uniformly distributed noise, can enjoy the same convergence rate as its uncompressed counterpart. Extensive experiments are conducted to demonstrate that the z-SignFedAvg can achieve competitive empirical performance on real datasets and outperforms existing schemes.","['ML: Distributed Machine Learning & Federated Learning', 'SO: Non-convex Optimization']",[],"['Zhiwei Tang', 'Yanmeng Wang', 'Tsung-Hui Chang']","['School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China\nShenzhen Research Institute of Big Data, Shenzhen, China', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China\nShenzhen Research Institute of Big Data, Shenzhen, China', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China\nShenzhen Research Institute of Big Data, Shenzhen, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29458,Transparency & Explainability,An Information-Flow Perspective on Algorithmic Fairness,"This work presents insights gained by investigating the relationship between algorithmic fairness and the concept of secure information flow. The problem of enforcing secure information flow is well-studied in the context of information security: If secret information may ""flow"" through an algorithm or program in such a way that it can influence the program’s output, then that is considered insecure information flow as attackers could potentially observe (parts of) the secret.  There is a strong correspondence between secure information flow and algorithmic fairness: if protected attributes such as race, gender, or age are treated as secret program inputs, then secure information flow means that these ""secret"" attributes cannot influence the result of a program. While most research in algorithmic fairness evaluation concentrates on studying the impact of algorithms (often treating the algorithm as a black-box), the concepts derived from information flow can be used both for the analysis of disparate treatment as well as disparate impact w.r.t. a structural causal model.  In this paper, we examine the relationship between quantitative as well as qualitative information-flow properties and fairness. Moreover, based on this duality, we derive a new quantitative notion of fairness called fairness spread, which can be easily analyzed using quantitative information flow and which strongly relates to counterfactual fairness. We demonstrate that off-the-shelf tools for information-flow properties can be used in order to formally analyze a program's algorithmic fairness properties, including the new notion of fairness spread as well as established notions such as demographic parity.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Information Theory', 'PEAI: Bias', 'Fairness & Equity', 'RU: Causality', 'RU: Graphical Models']",[],"['Samuel Teuber', 'Bernhard Beckert']","['Karlsruhe Institute of Technology', 'Karlsruhe Institute of Technology']","['Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29463,Fairness & Bias,Weisfeiler and Lehman Go Paths: Learning Topological Features via Path Complexes,"Graph Neural Networks (GNNs), despite achieving remarkable performance across different tasks, are theoretically bounded by the 1-Weisfeiler-Lehman test, resulting in limitations in terms of graph expressivity. Even though prior works on topological higher-order GNNs overcome that boundary, these models often depend on assumptions about sub-structures of graphs. Specifically, topological GNNs leverage the prevalence of cliques, cycles, and rings to enhance the message-passing procedure. Our study presents a novel perspective by focusing on simple paths within graphs during the topological message-passing process, thus liberating the model from restrictive inductive biases. We prove that by lifting graphs to path complexes, our model can generalize the existing works on topology while inheriting several theoretical results on simplicial complexes and regular cell complexes. Without making prior assumptions about graph sub-structures, our method outperforms earlier works in other topological domains and achieves state-of-the-art results on various benchmarks.",['ML: Graph-based Machine Learning'],[],"['Quang Truong', 'Peter Chin']","['Dartmouth College', 'Dartmouth College']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29466,Fairness & Bias,Causal Strategic Learning with Competitive Selection,"We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it.  Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement.  Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes.  Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance.  To that end, we provide an analytical form of the optimal selection rule and a mechanism to retrieve the causal parameters from observational data, under certain assumptions on agents' behaviour.  Secondly, when there are multiple decision makers, the interference between selection rules introduces another source of biases in estimating the underlying causal parameters.  To address this problem, we provide a cooperative protocol which all decision makers must collectively adopt to recover the true causal parameters.  Lastly, we complement our theoretical results with simulation studies. Our results highlight not only the importance of causal modeling as a strategy to mitigate the effect of gaming, as suggested by previous work, but also the need of a benevolent regulator to enable it.","['ML: Causal Learning', 'GTEP: Game Theory']",[],"['Kiet Q. H. Vo', 'Muneeb Aadil', 'Siu Lun Chau', 'Krikamol Muandet']","['CISPA Helmholtz Center for Information Security, Saarbrücken, Germany\nSaarland University, Saarbrücken, Germany', 'CISPA Helmholtz Center for Information Security, Saarbrücken, Germany\nSaarland University, Saarbrücken, Germany', 'CISPA Helmholtz Center for Information Security, Saarbrücken, Germany', 'CISPA Helmholtz Center for Information Security, Saarbrücken, Germany']","['Germany', 'Germany', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29462,Privacy & Data Governance,DeRDaVa: Deletion-Robust Data Valuation for Machine Learning,"Data valuation is concerned with determining a fair valuation of data from data sources to compensate them or to identify training examples that are the most or least useful for predictions. With the rising interest in personal data ownership and data protection regulations, model owners will likely have to fulfil more data deletion requests. This raises issues that have not been addressed by existing works: Are the data valuation scores still fair with deletions? Must the scores be expensively recomputed? The answer is no. To avoid recomputations, we propose using our data valuation framework DeRDaVa upfront for valuing each data source's contribution to preserving robust model performance after anticipated data deletions. DeRDaVa can be efficiently approximated and will assign higher values to data that are more useful or less likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to risk-averse/seeking model owners who are concerned with the worst/best-cases model utility. We also empirically demonstrate the practicality of our solutions.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'GTEP: Cooperative Game Theory']",[],"['Xiao Tian', 'Rachael Hwee Ling Sim', 'Jue  Fan', 'Bryan Kian Hsiang Low']","['National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29467,Privacy & Data Governance,Data Disparity and Temporal Unavailability Aware Asynchronous Federated Learning for Predictive Maintenance on Transportation Fleets,"Predictive maintenance has emerged as a critical application in modern transportation, leveraging sensor data to forecast potential damages proactively using machine learning. However, privacy concerns limit data sharing, making Federated learning an appealing approach to preserve data privacy. Nevertheless, challenges arise due to disparities in data distribution and temporal unavailability caused by individual usage patterns in transportation. In this paper, we present a novel asynchronous federated learning approach to address system heterogeneity and facilitate machine learning for predictive maintenance on transportation fleets. The approach introduces a novel data disparity aware aggregation scheme and a federated early stopping method for training. To validate the effectiveness of our approach, we evaluate it on two independent real-world datasets from the transportation domain: 1) oil dilution prediction of car combustion engines and 2) remaining lifetime prediction of plane turbofan engines. Our experiments show that we reliably outperform five state-of-the-art baselines, including federated and classical machine learning models. Moreover, we show that our approach generalises to various prediction model architectures.","['ML: Distributed Machine Learning & Federated Learning', 'APP: Mobility', 'Driving & Flight']",[],"['Leonie von Wahl', 'Niklas Heidenreich', 'Prasenjit Mitra', 'Michael Nolting', 'Nicolas Tempelmeier']","['Volkswagen Group', 'Volkswagen Group', 'Penn State University, State College', 'Volkswagen Group', 'Volkswagen Group']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29468,Privacy & Data Governance,Federated Graph Learning under Domain Shift with Generalizable Prototypes,"Federated Graph Learning is a privacy-preserving collaborative approach for training a shared model on graph-structured data in the distributed environment. However, in real-world scenarios, the client graph data usually originate from diverse domains, this unavoidably hinders the generalization performance of the final global model. To address this challenge, we start the first attempt to investigate this scenario by learning a well-generalizable model. In order to improve the performance of the global model from different perspectives, we propose a novel framework called Federated Graph Learning with Generalizable Prototypes (FGGP). It decouples the global model into two levels and bridges them via prototypes.  These prototypes, which are semantic centers derived from the feature extractor, can provide valuable classification information. At the classification model level, we innovatively eschew the traditional classifiers, then instead leverage clustered prototypes to capture fruitful domain information and enhance the discriminative capability of the classes, improving the performance of multi-domain predictions. Furthermore, at the feature extractor level, we go beyond traditional approaches by implicitly injecting distinct global knowledge and employing contrastive learning to obtain more powerful prototypes while enhancing the feature extractor generalization ability. Experimental results on various datasets are presented to validate the effectiveness of the proposed method.","['ML: Distributed Machine Learning & Federated Learning', 'DMKM: Graph Mining', 'Social Network Analysis & Community']",[],"['Guancheng Wan', 'Wenke Huang', 'Mang Ye']","['National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence Hubei Key Laboratory of Multimedia and Network Communication Engineering School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China', 'National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence Hubei Key Laboratory of Multimedia and Network Communication Engineering School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China', 'National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence Hubei Key Laboratory of Multimedia and Network Communication Engineering School of Computer Science, Hubei Luojia Laboratory, Wuhan University, Wuhan, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29476,Privacy & Data Governance,Practical Privacy-Preserving MLaaS: When Compressive Sensing Meets Generative Networks,"The Machine-Learning-as-a-Service (MLaaS) framework allows one to grab low-hanging fruit of machine learning techniques and data science, without either much expertise for this sophisticated sphere or provision of specific infrastructures. However, the requirement of revealing all training data to the service provider raises new concerns in terms of privacy leakage, storage consumption, efficiency, bandwidth, etc. In this paper, we propose a lightweight privacy-preserving MLaaS framework by combining Compressive Sensing (CS) and Generative Networks. It’s constructed on the favorable facts observed in recent works that general inference tasks could be fulfilled with generative networks and classifier trained on compressed measurements, since the generator could model the data distribution and capture discriminative information which are useful for classification. To improve the performance of the MLaaS framework, the supervised generative models of the server are trained and optimized with prior knowledge provided by the client. In order to prevent the service provider from recovering the original data as well as identifying the queried results, a noise-addition mechanism is designed and adopted into the compressed data domain. Empirical results confirmed its performance superiority in accuracy and resource consumption against the state-of-the-art privacy preserving MLaaS frameworks.","['ML: Classification and Regression', 'PEAI: Privacy & Security']",[],"['Jia Wang', 'Wuqiang Su', 'Zushu Huang', 'Jie Chen', 'Chengwen Luo', 'Jianqiang  Li']","['Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29475,Transparency & Explainability,V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models,"Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively. Supplementary materials such as audio samples are provided at our demo website: https://v2a-mapper.github.io/.","['ML: Multimodal Learning', 'ML: Deep Neural Architectures and Foundation Models', 'ML: Applications']",[],"['Heng Wang', 'Jianbo Ma', 'Santiago Pascual', 'Richard Cartwright', 'Weidong Cai']","['University of Sydney', 'Dolby Laboratories', 'Dolby Laboratories', 'Dolby Laboratories', 'University of Sydney']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29482,Fairness & Bias,MetaCARD: Meta-Reinforcement Learning with Task Uncertainty Feedback via Decoupled Context-Aware Reward and Dynamics Components,"Meta-Reinforcement Learning (Meta-RL) aims to reveal shared characteristics in dynamics and reward functions across diverse training tasks. This objective is achieved by meta-learning a policy that is conditioned on task representations with encoded trajectory data or context, thus allowing rapid adaptation to new tasks from a known task distribution. However, since the trajectory data generated by the policy may be biased, the task inference module tends to form spurious correlations between trajectory data and specific tasks, thereby leading to poor adaptation to new tasks. To address this issue, we propose the Meta-RL with task unCertAinty feedback through decoupled context-aware Reward and Dynamics components (MetaCARD). MetaCARD distinctly decouples the dynamics and rewards when inferring tasks and integrates task uncertainty feedback from policy evaluation into the task inference module. This design effectively reduces uncertainty in tasks with changes in dynamics or/and reward functions, thereby enabling accurate task identification and adaptation. The experiment results on both Meta-World and classical MuJoCo benchmarks show that MetaCARD significantly outperforms prevailing Meta-RL baselines, demonstrating its remarkable adaptation ability in sophisticated environments that involve changes in both reward functions and dynamics.","['ML: Reinforcement Learning', 'ML: Representation Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Min Wang', 'Xin Li', 'Leiji Zhang', 'Mingzhong Wang']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'The University of the Sunshine Coast']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29493,Transparency & Explainability,Probability-Polarized Optimal Transport for Unsupervised Domain Adaptation,"Optimal transport (OT) is an important methodology to measure distribution discrepancy, which has achieved promising performance in artificial intelligence applications, e.g., unsupervised domain adaptation. However, from the view of transportation, there are still limitations: 1) the local discriminative structures for downstream tasks, e.g., cluster structure for classification, cannot be explicitly admitted by the learned OT plan; 2) the entropy regularization induces a dense OT plan with increasing uncertainty. To tackle these issues, we propose a novel Probability-Polarized OT (PPOT) framework, which can characterize the structure of OT plan explicitly. Specifically, the probability polarization mechanism is proposed to guide the optimization direction of OT plan, which generates a clear margin between similar and dissimilar transport pairs and reduces the uncertainty. Further, a dynamic mechanism for margin is developed by incorporating task-related information into the polarization, which directly captures the intra/inter class correspondence for knowledge transportation. A mathematical understanding for PPOT is provided from the view of gradient, which ensures interpretability. Extensive experiments on several datasets validate the effectiveness and empirical efficiency of PPOT.","['ML: Classification and Regression', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Yan Wang', 'Chuan-Xian Ren', 'Yi-Ming Zhai', 'You-Wei Luo', 'Hong Yan']","['School of Mathematics, Sun Yat-Sen University, China', 'School of Mathematics, Sun Yat-Sen University, China', 'School of Mathematics, Sun Yat-Sen University, China', 'School of Mathematics, Sun Yat-Sen University, China', 'Department of Electrical Engineering, City University of Hong Kong, Hong Kong']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29491,Privacy & Data Governance,IGAMT: Privacy-Preserving Electronic Health Record Synthesization with Heterogeneity and Irregularity,"Integrating electronic health records (EHR) into machine learning-driven clinical research and hospital applications is important, as it harnesses extensive and high-quality patient data to enhance outcome predictions and treatment personalization. Nonetheless, due to privacy and security concerns, the secondary purpose of EHR data is consistently governed and regulated, primarily for research intentions, thereby constraining researchers' access to EHR data. Generating synthetic EHR data with deep learning methods is a viable and promising approach to mitigate privacy concerns, offering not only a supplementary resource for downstream applications but also sidestepping the confidentiality risks associated with real patient data. While prior efforts have concentrated on EHR data synthesis, significant challenges persist in the domain of generating synthetic EHR data: balancing the heterogeneity of real EHR including temporal and non-temporal features, addressing the missing values and irregular measures, and ensuring the privacy of the real data used for model training. Existing works in this domain only focused on solving one or two aforementioned challenges. In this work, we propose IGAMT, an innovative framework to generate privacy-preserved synthetic EHR data that not only maintain high quality with heterogeneous features, missing values, and irregular measures but also balances the privacy-utility trade-off. Extensive experiments prove that IGAMT significantly outperforms baseline architectures in terms of visual resemblance and comparable performance in downstream applications. Ablation case studies also prove the effectiveness of the techniques applied in IGAMT.","['ML: Deep Generative Models & Autoencoders', 'ML: Privacy', 'ML: Time-Series/Data Streams']",[],"['Wenjie Wang', 'Pengfei Tang', 'Jian Lou', 'Yuanming Shao', 'Lance Waller', 'Yi-an Ko', 'Li Xiong']","['ShanghaiTech University', 'Emory University', 'ZJU-Hangzhou Global Scientific and Technological Innovation Center', 'ShanghaiTech University', 'Emory University', 'Emory Unviversity', 'Emory University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29492,Fairness & Bias,Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning,"Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training (D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi-heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to achieve domain independence. Despite its extraordinary simplicity and efficiency, D-Train performs remarkably well in extensive evaluations of various datasets from standard benchmarks to applications of satellite imagery and recommender systems.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Ximei Wang', 'Junwei Pan', 'Xingzhuo Guo', 'Dapeng Liu', 'Jie Jiang']","['Tencent Inc.', 'Tencent Inc.', 'Tsinghua University', 'Tencent Inc.', 'Tencent Inc.']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29496,Transparency & Explainability,Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement,"As attitude and motion sensing components, inertial sensors are widely used in various portable devices, covering consumer electronics, sports health, aerospace, etc. But the severe intrinsic errors of inertial sensors heavily restrain their function implementation, especially the advanced functionality, including motion trajectory recovery and motion semantic recognition, which attracts considerable attention. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanism (CRM), which enables the network to extract and represent category features without increasing trainable parameters. Furthermore, CRM transforms the common fully connected network into category representations, which provide closer supervision to the feature extractor than the far and trivial one-hot classification labels. We call this process of imposing interpretability on a network and using it to supervise the feature extractor the feature supervision mechanism, and its effectiveness is demonstrated experimentally and theoretically in this paper. The enhanced inertial signal can perform impracticable tasks with regard to the original signal, such as trajectory reconstruction. Both quantitative and visual results show that WDSNet outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised method, achieves the state-of-the-art performance of all the compared fully-supervised methods.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'APP: Internet of Things', 'Sensor Networks & Smart Cities', 'ML: Feature Construction/Reformulation', 'ML: Deep Learning Algorithms', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Yifeng Wang', 'Yi Zhao']","['Harbin Institute of Technology, Shenzhen', 'Harbin Institute of Technology, Shenzhen']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29497,Security,Lost Domain Generalization Is a Natural Consequence of Lack of Training Domains,"We show a hardness result for the number of training domains required to achieve a small population error in the test domain. Although many domain generalization algorithms have been developed under various domain-invariance assumptions, there is significant evidence to indicate that out-of-distribution (o.o.d.) test accuracy of state-of-the-art o.o.d. algorithms is on par with empirical risk minimization and random guess on the domain generalization benchmarks such as DomainBed. In this work, we analyze its cause and attribute the lost domain generalization to the lack of training domains. We show that, in a minimax lower bound fashion, any learning algorithm that outputs a classifier with an ε excess error to the Bayes optimal classifier requires at least poly(1/ε) number of training domains, even though the number of training data sampled from each training domain is large. Experiments on the DomainBed benchmark demonstrate that o.o.d. test accuracy is monotonically increasing as the number of training domains increases. Our result sheds light on the intrinsic hardness of domain generalization and suggests benchmarking o.o.d. algorithms by the datasets with a sufficient number of training domains.","['ML: Adversarial Learning & Robustness', 'General', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Learning Theory']",[],"['Yimu Wang', 'Yihan Wu', 'Hongyang Zhang']","['University of Waterloo', 'University of Maryland, College Park', 'University of Waterloo']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/29504,Security,DRF: Improving Certified Robustness via Distributional Robustness Framework,"Randomized smoothing (RS) has provided state-of-the-art (SOTA) certified robustness against adversarial perturbations for large neural networks. Among studies in this field, methods based on adversarial training (AT) achieve remarkably robust performance by applying adversarial examples to construct the smoothed classifier. These AT-based RS methods typically seek a pointwise adversary that generates the worst-case adversarial examples by perturbing each input independently. However, there are unexplored benefits to considering such adversarial robustness across the entire data distribution. To this end, we provide a novel framework called DRF, which connects AT-based RS methods with distributional robustness (DR), and show that these methods are special cases of their counterparts in our framework. Due to the advantages conferred by DR, our framework can control the trade-off between the clean accuracy and certified robustness of smoothed classifiers to a significant extent. Our experiments demonstrate that DRF can substantially improve the certified robustness of AT-based RS.","['ML: Adversarial Learning & Robustness', 'CV: Adversarial Attacks & Robustness', 'APP: Security']",[],"['Zekai Wang', 'Zhengyu Zhou', 'Weiwei Liu']","['School of Computer Science, Wuhan University', 'School of Computer Science, Wuhan University', 'School of Computer Science, Wuhan University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29503,Security,Consistency-GAN: Training GANs with Consistency Model,"For generative learning tasks, there are three crucial criteria for generating samples from the models: quality,  coverage/diversity, and sampling speed. Among the existing generative models, Generative adversarial networks (GANs) and diffusion models demonstrate outstanding quality performance while suffering from notable limitations. GANs can generate high-quality results and enable fast sampling, their drawbacks, however, lie in the limited diversity of the generated samples. On the other hand, diffusion models excel at generating high-quality results with a commendable diversity. Yet, its iterative generation process necessitates hundreds to thousands of sampling steps, leading to slow speeds that are impractical for real-time scenarios. To address the aforementioned problem, this paper proposes a novel Consistency-GAN model. In particular, to aid in the training of the GAN, we introduce instance noise, which employs consistency models using only a few steps compared to the conventional diffusion process. Our evaluations on various datasets indicate that our approach significantly accelerates sampling speeds compared to traditional diffusion models, while preserving sample quality and diversity. Furthermore, our approach also has better model coverage than traditional adversarial training methods.","['ML: Adversarial Learning & Robustness', 'ML: Deep Generative Models & Autoencoders']",[],"['Yunpeng Wang', 'Meng Pang', 'Shengbo Chen', 'Hong Rao']","['Nanchang University', 'Nanchang University', 'Henan University', 'Nanchang University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29506,Security,Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits,"Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB algorithms.","['ML: Online Learning & Bandits', 'ML: Adversarial Learning & Robustness']",[],"['Zhiwei Wang', 'Huazheng Wang', 'Hongning Wang']","['Tsinghua University', 'Oregon State University', 'Tsinghua University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29515,Fairness & Bias,"Reproduce, Replicate, Reevaluate. The Long but Safe Way to Extend Machine Learning Methods","Reproducibility is a desirable property of scientific research. On the one hand, it increases confidence in results. On the other hand, reproducible results can be extended on a solid basis. In rapidly developing fields such as machine learning, the latter is particularly important to ensure the reliability of research. In this paper, we present a systematic approach to reproducing (using the available implementation), replicating (using an alternative implementation) and reevaluating (using different datasets) state-of-the-art experiments. This approach enables the early detection and correction of deficiencies and thus the development of more robust and transparent machine learning methods. We detail the independent reproduction, replication, and reevaluation of the initially published experiments with a method that we want to extend. For each step, we identify issues and draw lessons learned. We further discuss solutions that have proven effective in overcoming the encountered problems. This work can serve as a guide for further reproducibility studies and generally improve reproducibility in machine learning.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Applications', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Graph-based Machine Learning', 'ML: Neuro-Symbolic Learning', 'ML: Representation Learning']",[],"['Luisa Werner', 'Nabil Layaïda', 'Pierre Genevès', 'Jérôme Euzenat', 'Damien Graux']","['Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG F-38000 Grenoble, France', 'Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG F-38000 Grenoble, France', 'Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG F-38000 Grenoble, France', 'Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG F-38000 Grenoble, France', 'ADAPT SFI Research Centre, Trinity College Dublin, Ireland.']","['France', 'France', 'France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/29520,Fairness & Bias,OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning,"Model-based offline reinforcement learning (RL) algorithms have emerged as a promising paradigm for offline RL. These algorithms usually learn a dynamics model from a static dataset of transitions, use the model to generate synthetic trajectories, and perform conservative policy optimization within these trajectories.  However, our observations indicate that policy optimization methods used in these model-based offline RL algorithms are not effective at exploring the learned model and induce biased exploration, which ultimately impairs the performance of the algorithm. To address this issue, we propose Offline Conservative ExplorAtioN (OCEAN),  a novel rollout approach to model-based offline RL. In our method, we incorporate additional exploration techniques and introduce three conservative constraints based on uncertainty estimation to mitigate the potential impact of significant dynamic errors resulting from exploratory transitions.  Our work is a plug-in method and can be combined with classical model-based RL algorithms, such as MOPO, COMBO, and RAMBO. Experiment results of our method on the D4RL MuJoCo benchmark show that OCEAN significantly improves the performance of existing algorithms.",['ML: Reinforcement Learning'],[],"['Fan Wu', 'Rui Zhang', 'Qi Yi', 'Yunkai Gao', 'Jiaming Guo', 'Shaohui Peng', 'Siming Lan', 'Husheng Han', 'Yansong Pan', 'Kaizhao Yuan', 'Pengwei Jin', 'Ruizhi Chen', 'Yunji Chen', 'Ling Li']","['Intelligent Software Research Center, Institute of Software, CAS, Beijing, China\nUniversity of Chinese Academy of Sciences, UCAS, Beijing, China', 'SKL of Processors, Institute of Computing Technology, CAS, Beijing, China', 'University of Science and Technology of China, USTC, Hefei, China', 'University of Science and Technology of China, USTC, Hefei, China', 'SKL of Processors, Institute of Computing Technology, CAS, Beijing, China', 'Intelligent Software Research Center, Institute of Software, CAS, Beijing, China', 'University of Science and Technology of China, USTC, Hefei, China', 'SKL of Processors, Institute of Computing Technology, CAS, Beijing, China\nUniversity of Chinese Academy of Sciences, UCAS, Beijing, China', 'University of Chinese Academy of Sciences, UCAS, Beijing, China', 'University of Chinese Academy of Sciences, UCAS, Beijing, China', 'University of Chinese Academy of Sciences, UCAS, Beijing, China\nSKL of Processors, Institute of Computing Technology, CAS, Beijing, China', 'Intelligent Software Research Center, Institute of Software, CAS, Beijing, China', 'SKL of Processors, Institute of Computing Technology, CAS, Beijing, China\nUniversity of Chinese Academy of Sciences, UCAS, Beijing, China', 'Intelligent Software Research Center, Institute of Software, CAS, Beijing, China\nUniversity of Chinese Academy of Sciences, UCAS, Beijing, China']","['China', '', 'China', 'China', '', '', 'China', 'China', 'China', 'China', 'China', '', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29522,Transparency & Explainability,SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies,"With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receiving a favorable decision. Prior work on sequential algorithmic recourse---which recommends a series of changes---focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safe Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures ""Value at Risk"" and ""Conditional Value at Risk"" from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different risk-aversion levels using risk measures and recourse desiderata (sparsity and proximity).","['ML: Transparent', 'Interpretable', 'Explainable ML', 'PEAI: Accountability', 'Interpretability & Explainability']",[],"['Haochen Wu', 'Shubham Sharma', 'Sunandita Patra', 'Sriram Gopalakrishnan']","['University of Michigan', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29525,Privacy & Data Governance,FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise,"Federated learning (FL) has emerged as a promising paradigm for training segmentation models on decentralized medical data, owing to its privacy-preserving property. However, existing research overlooks the prevalent annotation noise encountered in real-world medical datasets, which limits the performance ceilings of FL. In this paper, we, for the first time, identify and tackle this problem. For problem formulation, we propose a contour evolution for modeling non-independent and identically distributed (Non-IID) noise across pixels within each client and then extend it to the case of multi-source data to form a heterogeneous noise model (i.e., Non-IID annotation noise across clients). For robust learning from annotations with such two-level Non-IID noise, we emphasize the importance of data quality in model aggregation, allowing high-quality clients to have a greater impact on FL. To achieve this, we propose Federated learning with Annotation quAlity-aware AggregatIon, named FedA3I, by introducing a quality factor based on client-wise noise estimation. Specifically, noise estimation at each client is accomplished through the Gaussian mixture model and then incorporated into model aggregation in a layer-wise manner to up-weight high-quality clients. Extensive experiments on two real-world medical image segmentation datasets demonstrate the superior performance of FedA3I against the state-of-the-art approaches in dealing with cross-client annotation noise. The code is available at https://github.com/wnn2000/FedAAAI.","['ML: Distributed Machine Learning & Federated Learning', 'CV: Medical and Biological Imaging', 'CV: Segmentation', 'ML: Calibration & Uncertainty Quantification']",[],"['Nannan Wu', 'Zhaobin Sun', 'Zengqiang Yan', 'Li Yu']","['School of Electronic Information and Communications, Huazhong University of Science and Technology', 'School of Electronic Information and Communications, Huazhong University of Science and Technology', 'School of Electronic Information and Communications, Huazhong University of Science and Technology', 'School of Electronic Information and Communications, Huazhong University of Science and Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29529,Security,Data Poisoning to Fake a Nash Equilibria for Markov Games,"We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium for a two-player zero-sum Markov game. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL algorithms.","['ML: Adversarial Learning & Robustness', 'GTEP: Adversarial Learning']",[],"['Young Wu', 'Jeremy McMahan', 'Xiaojin Zhu', 'Qiaomin Xie']","['University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'University of Wisconsin - Madison']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29545,Fairness & Bias,FairWASP: Fast and Optimal Fair Wasserstein Pre-processing,"Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Our work is based on reformulating the pre-processing task as a large-scale mixed-integer program (MIP), for which we propose a highly efficient algorithm based on the cutting plane method. Experiments demonstrate that our proposed optimization algorithm significantly outperforms state-of-the-art commercial solvers in solving both the MIP and its linear program relaxation. Further experiments highlight the competitive performance of FairWASP in reducing disparities while preserving accuracy in downstream classification settings.","['ML: Optimization', 'CSO: Constraint Optimization', 'ML: Classification and Regression', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Zikai Xiong', 'Niccolò Dalmasso', 'Alan Mishler', 'Vamsi K. Potluru', 'Tucker Balch', 'Manuela Veloso']","['Massachusetts Institute of Technology', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research', 'J.P. Morgan AI Research']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29556,Security,E2E-AT: A Unified Framework for Tackling Uncertainty in Task-Aware End-to-End Learning,"Successful machine learning involves a complete pipeline of data, model, and downstream applications. Instead of treating them separately, there has been a prominent increase of attention within the constrained optimization (CO) and machine learning (ML) communities towards combining prediction and optimization models. The so-called end-to-end (E2E) learning captures the task-based objective for which they will be used for decision making. Although a large variety of E2E algorithms have been presented, it has not been fully investigated how to systematically address uncertainties involved in such models. Most of the existing work considers the uncertainties of ML in the input space and improves robustness through adversarial training. We extend this idea to E2E learning and prove that there is a robustness certification procedure by solving augmented integer programming. Furthermore, we show that neglecting the uncertainty of COs during training causes a new trigger for generalization errors. To include all these components, we propose a unified framework that covers the uncertainties emerging in both the input feature space of the ML models and the COs. The framework is described as a robust optimization problem and is practically solved via end-to-end adversarial training (E2E-AT). Finally, the performance of E2E-AT is evaluated by a real-world end-to-end power system operation problem, including load forecasting and sequential scheduling tasks.","['ML: Adversarial Learning & Robustness', 'CSO: Applications', 'CSO: Constraint Optimization', 'ML: Optimization']",[],"['Wangkun Xu', 'Jianhong Wang', 'Fei Teng']","['Department of EEE, Imperial College London, UK', 'Center for AI Fundamentals, University of Manchester, UK', 'Department of EEE, Imperial College London, UK']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/29554,Privacy & Data Governance,LSTKC: Long Short-Term Knowledge Consolidation for Lifelong Person Re-identification,"Lifelong person re-identification (LReID) aims to train a unified model from diverse data sources step by step. The severe domain gaps between different training steps result in catastrophic forgetting in LReID, and existing methods mainly rely on data replay and knowledge distillation techniques to handle this issue. However, the former solution needs to store historical exemplars which inevitably impedes data privacy. The existing knowledge distillation-based models usually retain all the knowledge of the learned old models without any selections, which will inevitably include erroneous and detrimental knowledge that severely impacts the learning performance of the new model. To address these issues, we propose an exemplar-free LReID method named LongShort Term Knowledge Consolidation (LSTKC) that contains a Rectification-based Short-Term Knowledge Transfer module (R-STKT) and an Estimation-based Long-Term Knowledge Consolidation module (E-LTKC). For each learning iteration within one training step, R-STKT aims to filter and rectify the erroneous knowledge contained in the old model and transfer the rectified knowledge to facilitate the short-term learning of the new model. Meanwhile, once one training step is finished, E-LTKC proposes to further consolidate the learned long-term knowledge via adaptively fusing the parameters of models from different steps. Consequently, experimental results show that our LSTKC exceeds the state-of-the-art methods by 6.3%/9.4% and 7.9%/4.5%, 6.4%/8.0% and 9.0%/5.5% average mAP/R@1 on seen and unseen domains under two different training orders of the challenging LReID benchmark respectively.","['ML: Life-Long and Continual Learning', 'CV: Image and Video Retrieval']",[],"['Kunlun Xu', 'Xu Zou', 'Jiahuan Zhou']","['Peking University', 'Huazhong University of Science and Technology', 'Peking University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29564,Fairness & Bias,Exploiting Geometry for Treatment Effect Estimation via Optimal Transport,"Estimating treatment effects from observational data suffers from the issue of confounding bias, which is induced by the imbalanced confounder distributions between the treated and control groups. As an effective approach, re-weighting learns a group of sample weights to balance the confounder distributions. Existing methods of re-weighting highly rely on a propensity score model or moment alignment. However, for complex real-world data, it is difficult to obtain an accurate propensity score prediction. Although moment alignment is free of learning a propensity score model, accurate estimation for high-order moments is computationally difficult and still remains an open challenge, and first and second-order moments are insufficient to align the distributions and easy to be misled by outliers. In this paper, we exploit geometry to capture the intrinsic structure involved in data for balancing the confounder distributions, so that confounding bias can be reduced even with outliers. To achieve this, we construct a connection between treatment effect estimation and optimal transport, a powerful tool to capture geometric information. After that, we propose an optimal transport model to learn sample weights by extracting geometry from confounders, in which geometric information between groups and within groups is leveraged for better confounder balancing. A projected mirror descent algorithm is employed to solve the derived optimization problem. Experimental studies on both synthetic and real-world datasets demonstrate the effectiveness of our proposed method.",['ML: Causal Learning'],[],"['Yuguang Yan', 'Zeqin Yang', 'Weilin Chen', 'Ruichu Cai', 'Zhifeng Hao', 'Michael Kwok-Po Ng']","['School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China', 'College of Science, Shantou University, Shantou, China', 'Department of Mathematics, Hong Kong Baptist University, Hong Kong, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29565,Fairness & Bias,Wasserstein Differential Privacy,"Differential privacy (DP) has achieved remarkable results in the field of privacy-preserving machine learning. However, existing DP frameworks do not satisfy all the conditions for becoming metrics, which prevents them from deriving better basic private properties and leads to exaggerated values on privacy budgets. We propose Wasserstein differential privacy (WDP), an alternative DP framework to measure the risk of privacy leakage, which satisfies the properties of symmetry and triangle inequality. We show and prove that WDP has 13 excellent properties, which can be theoretical supports for the better performance of WDP than other DP frameworks.  In addition, we derive a general privacy accounting method called Wasserstein accountant, which enables WDP to be applied in stochastic gradient descent (SGD) scenarios containing subsampling. Experiments on basic mechanisms, compositions and deep learning show that the privacy budgets obtained by Wasserstein accountant are relatively stable and less influenced by order. Moreover, the overestimation on privacy budgets can be effectively alleviated. The code is available at https://github.com/Hifipsysta/WDP.","['ML: Privacy', 'ML: Deep Learning Theory', 'PEAI: Privacy & Security', 'CV: Bias', 'Fairness & Privacy']",[],"['Chengyi Yang', 'Jiayin Qi', 'Aimin Zhou']","['East China Normal University', 'Guangzhou University', 'East China Normal University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29566,Transparency & Explainability,Federated Causality Learning with Explainable Adaptive Optimization,"Discovering the causality from observational data is a crucial task in various scientific domains. With increasing awareness of privacy, data are not allowed to be exposed, and it is very hard to learn causal graphs from dispersed data, since these data may have different distributions. In this paper, we propose a federated causal discovery strategy (FedCausal) to learn the unified global causal graph from decentralized heterogeneous data. We design a global optimization formula to naturally aggregate the causal graphs from client data and constrain the acyclicity of the global graph without exposing local data. Unlike other federated causal learning algorithms, FedCausal unifies the local and global optimizations into a complete directed acyclic graph (DAG) learning process with a flexible optimization objective. We prove that this optimization objective has a high interpretability and can adaptively handle homogeneous and heterogeneous data. Experimental results on synthetic and real datasets show that FedCausal can effectively deal with non-independently and identically distributed (non-iid) data and has a superior performance.","['ML: Causal Learning', 'CSO: Constraint Optimization', 'DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Dezhi Yang', 'Xintong He', 'Jun Wang', 'Guoxian Yu', 'Carlotta Domeniconi', 'Jinglin Zhang']","['Shandong University', 'National University of Singapore', 'Shandong University', 'Shandong University', 'George Mason University', 'Shandong University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29568,Fairness & Bias,Exploring One-Shot Semi-supervised Federated Learning with Pre-trained Diffusion Models,"Recently, semi-supervised federated learning (semi-FL) has been proposed to handle the commonly seen real-world scenarios with labeled data on the server and unlabeled data on the clients. However, existing methods face several challenges such as communication costs, data heterogeneity, and training pressure on client devices. To address these challenges, we introduce the powerful diffusion models (DM) into semi-FL and propose FedDISC, a Federated Diffusion-Inspired Semi-supervised Co-training method. Specifically, we first extract prototypes of the labeled server data and use these prototypes to predict pseudo-labels of the client data. For each category, we compute the cluster centroids and domain-specific representations to signify the semantic and stylistic information of their distributions. After adding noise, these representations are sent back to the server, which uses the pre-trained DM to generate synthetic datasets complying with the client distributions and train a global model on it. With the assistance of vast knowledge within DM, the synthetic datasets have comparable quality and diversity to the client images, subsequently enabling the training of global models that achieve performance equivalent to or even surpassing the ceiling of supervised centralized training. FedDISC works within one communication round, does not require any local training, and involves very minimal information uploading, greatly enhancing its practicality. Extensive experiments on three large-scale datasets demonstrate that FedDISC effectively addresses the semi-FL problem on non-IID clients and outperforms the compared SOTA methods. Sufficient visualization experiments also illustrate that the synthetic dataset generated by FedDISC exhibits comparable diversity and quality to the original client dataset, with a neglectable possibility of leaking privacy-sensitive information of the clients.","['ML: Distributed Machine Learning & Federated Learning', 'CV: Bias', 'Fairness & Privacy', 'CV: Large Vision Models']",[],"['Mingzhao Yang', 'Shangchao Su', 'Bin Li', 'Xiangyang Xue']","['Fudan University', 'Fudan University', 'Fudan University', 'Fudan University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29574,Security,Adversarial Purification with the Manifold Hypothesis,"In this work, we formulate a novel framework for adversarial robustness using the manifold hypothesis. This framework provides sufficient conditions for defending against adversarial examples. We develop an adversarial purification method with this framework. Our method combines manifold learning with variational inference to provide adversarial robustness without the need for expensive adversarial training. Experimentally, our approach can provide adversarial robustness even if attackers are aware of the existence of the defense. In addition, our method can also serve as a test-time defense mechanism for variational autoencoders.","['ML: Adversarial Learning & Robustness', 'ML: Deep Generative Models & Autoencoders']",[],"['Zhaoyuan Yang', 'Zhiwei Xu', 'Jing Zhang', 'Richard Hartley', 'Peter Tu']","['GE Research', 'Australian National University', 'Australian National University', 'Australian National University', 'GE Research']","['', 'Australia', 'Australia', 'Australia', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29575,Fairness & Bias,Dynamic Knowledge Injection for AIXI Agents,"Prior approximations of AIXI, a Bayesian optimality notion for general reinforcement learning, can only approximate AIXI's Bayesian environment model using an a-priori defined set of models. This is a fundamental source of epistemic uncertainty for the agent in settings where the existence of systematic bias in the predefined model class cannot be resolved by simply collecting more data from the environment. We address this issue in the context of Human-AI teaming by considering a setup where additional knowledge for the agent in the form of new candidate models arrives from a human operator in an online fashion. We introduce a new agent called DynamicHedgeAIXI that maintains an exact Bayesian mixture over dynamically changing sets of models via a time-adaptive prior constructed from a variant of the Hedge algorithm. The DynamicHedgeAIXI agent is the richest direct approximation of AIXI known to date and comes with good performance guarantees. Experimental results on epidemic control on contact networks validates the agent's practical utility.","['ML: Reinforcement Learning', 'ML: Bayesian Learning', 'ML: Online Learning & Bandits', 'HAI: Human-in-the-loop Machine Learning']",[],"['Samuel Yang-Zhao', 'Kee Siong Ng', 'Marcus Hutter']","['Australian National University', 'Australian National University', 'Google DeepMind\nAustralian National University']","['Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29586,Fairness & Bias,Effective Causal Discovery under Identifiable Heteroscedastic Noise Model,"Capturing the underlying structural causal relations represented by Directed Acyclic Graphs (DAGs) has been a fundamental task in various AI disciplines. Causal DAG learning via the continuous optimization framework has recently achieved promising performance in terms of accuracy and efficiency. However, most methods make strong assumptions of homoscedastic noise, i.e., exogenous noises have equal variances across variables, observations, or even both. The noises in real data usually violate both assumptions due to the biases introduced by different data collection processes. To address the heteroscedastic noise issue, we introduce relaxed implementable sufficient conditions and prove the identifiability of a general class of SEM subject to those conditions. Based on the identifiable general SEM, we propose a novel formulation for DAG learning which accounts for the noise variance variation across variables and observations. We then propose an effective two-phase iterative DAG learning algorithm to address the increasing optimization difficulties and learn a causal DAG from data with heteroscedastic variables noise under varying variance. We show significant empirical gains of the proposed approaches over state-of-the-art methods on both synthetic data and real data.","['ML: Causal Learning', 'ML: Structured Learning']",[],"['Naiyu Yin', 'Tian Gao', 'Yue Yu', 'Qiang Ji']","['Rensselaer Polytechnic Institute', 'IBM Research', 'Lehigh University', 'Renselaer Polytechnic Institute']","['United States', '', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29584,Security,Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers,"Byzantine machine learning has garnered considerable attention in light of the unpredictable faults that can occur in large-scale distributed learning systems. The key to secure resilience against Byzantine machines in distributed learning is resilient aggregation mechanisms. Although abundant resilient aggregation rules have been proposed, they are designed in ad-hoc manners, imposing extra barriers on comparing, analyzing, and improving the rules across performance criteria. This paper studies near-optimal aggregation rules using clustering in the presence of outliers. Our outlier-robust clustering approach utilizes geometric properties of the update vectors provided by workers. Our analysis show that constant approximations to the 1-center and 1-mean clustering problems with outliers provide near-optimal resilient aggregators for metric-based criteria, which have been proven to be crucial in the homogeneous and heterogeneous cases respectively. In addition, we discuss two contradicting types of attacks under which no single aggregation rule is guaranteed to improve upon the naive average. Based on the discussion, we propose a two-phase resilient aggregation framework. We run experiments for image classification using a non-convex loss function. The proposed algorithms outperform previously known aggregation rules by a large margin with both homogeneous and heterogeneous data distributions among non-faulty workers. Code and appendix are available at https://github.com/jerry907/AAAI24-RASHB.","['ML: Adversarial Learning & Robustness', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Yuhao Yi', 'Ronghui You', 'Hong  Liu', 'Changxin Liu', 'Yuan Wang', 'Jiancheng Lv']","['Sichuan University', 'Nankai University', 'Sichuan University', 'KTH Royal Institute of Technology', 'Hunan University', 'Sichuan University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29591,Security,Chronic Poisoning: Backdoor Attack against Split Learning,"Split learning is a computing resource-friendly distributed learning framework that protects client training data by splitting the model between the client and server. Previous work has proved that split learning faces a severe risk of privacy leakage, as a malicious server can recover the client's private data by hijacking the training process. In this paper, we first explore the vulnerability of split learning to server-side backdoor attacks, where our goal is to compromise the model's integrity. Since the server-side attacker cannot access the training data and client model in split learning, the traditional poisoning-based backdoor attack methods are no longer applicable. Therefore, constructing backdoor attacks in split learning poses significant challenges. Our strategy involves the attacker establishing a shadow model on the server side that can encode backdoor samples and guiding the client model to learn from this model during the training process, thereby enabling the client to acquire the same capability. Based on these insights, we propose a three-stage backdoor attack framework named SFI. Our attack framework minimizes assumptions about the attacker's background knowledge and ensures that the attack process remains imperceptible to the client. We implement SFI on various benchmark datasets, and extensive experimental results demonstrate its effectiveness and generality. For example, success rates of our attack on MNIST, Fashion, and CIFAR10 datasets all exceed 90%, with limited impact on the main task.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Fangchao Yu', 'Bo Zeng', 'Kai Zhao', 'Zhi Pang', 'Lina Wang']","['Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University', 'Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29593,Security,Barely Supervised Learning for Graph-Based Fraud Detection,"In recent years, graph-based fraud detection methods have garnered increasing attention for their superior ability to tackle the issue of camouflage in fraudulent scenarios. However, these methods often rely on a substantial proportion of samples as the training set, disregarding the reality of scarce annotated samples in real-life scenarios. As a theoretical framework within semi-supervised learning, the principle of consistency regularization posits that unlabeled samples should be classified into the same category as their own perturbations. Inspired by this principle, this study incorporates unlabeled samples as an auxiliary during model training, designing a novel barely supervised learning method to address the challenge of limited annotated samples in fraud detection. Specifically, to tackle the issue of camouflage in fraudulent scenarios, we employ disentangled representation learning based on edge information for a small subset of annotated nodes. This approach partitions node features into three distinct components representing different connected edges, providing a foundation for the subsequent augmentation of unlabeled samples. For the unlabeled nodes used in auxiliary training, we apply both strong and weak augmentation and design regularization losses to enhance the detection performance of the model in the context of extremely limited labeled samples. Across five publicly available datasets, the proposed model showcases its superior detection capability over baseline models.","['ML: Semi-Supervised Learning', 'ML: Graph-based Machine Learning', 'DMKM: Graph Mining', 'Social Network Analysis & Community']",[],"['Hang Yu', 'Zhengyang Liu', 'Xiangfeng Luo']","['Shanghai University', 'Shanghai University', 'Shanghai University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29600,Security,PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion,"The generalization of neural networks is a central challenge in machine learning, especially concerning the performance under distributions that differ from training ones. Current methods, mainly based on the data-driven paradigm such as data augmentation, adversarial training, and noise injection, may encounter limited generalization due to model non-smoothness. In this paper, we propose to investigate generalization from a Partial Differential Equation (PDE) perspective, aiming to enhance it directly through the underlying function of neural networks, rather than focusing on adjusting input data. Specifically, we first establish the connection between neural network generalization and the smoothness of the solution to a specific PDE, namely transport equation. Building upon this, we propose a general framework that introduces adaptive distributional diffusion into transport equation to enhance the smoothness of its solution, thereby improving generalization. In the context of neural networks, we put this theoretical framework into practice as PDE+ (PDE with Adaptive Distributional Diffusion) which diffuses each sample into a distribution covering semantically similar inputs. This enables better coverage of potentially unobserved distributions in training, thus improving generalization beyond merely data-driven methods. The effectiveness of PDE+ is validated through extensive experimental settings, demonstrating its superior performance compared to state-of-the-art methods. Our code is available at https://github.com/yuanyige/pde-add.","['ML: Adversarial Learning & Robustness', 'CV: Adversarial Attacks & Robustness', 'ML: Deep Learning Algorithms', 'ML: Deep Learning Theory']",[],"['Yige Yuan', 'Bingbing Xu', 'Bo Lin', 'Liang Hou', 'Fei Sun', 'Huawei Shen', 'Xueqi Cheng']","['CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences', 'Department of Mathematics, National University of Singapore', 'CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences', 'CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'CAS Key Laboratory of AI Safety & Security, Institute of Computing Technology, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences']","['China', 'China', 'Singapore', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29609,Fairness & Bias,Learning Cluster-Wise Anchors for Multi-View Clustering,"Due to its effectiveness and efficiency, anchor based multi-view clustering (MVC) has recently attracted much attention. Most existing approaches try to adaptively learn anchors to construct an anchor graph for clustering. However, they generally focus on improving the diversity among anchors by using orthogonal constraint and ignore the underlying semantic relations, which may make the anchors not representative and discriminative enough. To address this problem, we propose an adaptive Cluster-wise Anchor learning based MVC method, CAMVC for short. We first make an anchor cluster assumption that supposes the prior cluster structure of target anchors by pre-defining a consensus cluster indicator matrix. Based on the prior knowledge, an explicit cluster structure of latent anchors is enforced by learning diverse cluster centroids, which can explore both inter-cluster diversity and intra-cluster consistency of anchors, and improve the subspace representation discrimination. Extensive results demonstrate the effectiveness and superiority of our proposed method compared with some state-of-the-art MVC approaches.","['ML: Multi-instance/Multi-view Learning', 'ML: Clustering', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Chao Zhang', 'Xiuyi Jia', 'Zechao Li', 'Chunlin Chen', 'Huaxiong Li']","['Nanjing University', 'Nanjing University of Science and Technology', 'Nanjing University of Science and Technology', 'Nanjing University', 'Nanjing University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29610,Transparency & Explainability,Targeted Activation Penalties Help CNNs Ignore Spurious Signals,"Neural networks (NNs) can learn to rely on spurious signals in the training data, leading to poor generalisation. Recent methods tackle this problem by training NNs with additional ground-truth annotations of such signals. These methods may, however, let spurious signals re-emerge in deep convolutional NNs (CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the same problem by penalising activations to control the re-emergence of spurious signals in deep CNNs, while also lowering training times and memory usage. In addition, ground-truth annotations can be expensive to obtain. We show that TAP still works well with annotations generated by pre-trained models as effective substitutes of ground-truth annotations. We demonstrate the power of TAP against two state-of-the-art baselines on the MNIST benchmark and on two clinical image datasets, using four different CNN architectures.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Bias', 'Fairness & Privacy', 'CV: Interpretability', 'Explainability', 'and Transparency', 'HAI: Human-in-the-loop Machine Learning', 'ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Dekai Zhang', 'Matt Williams', 'Francesca Toni']","['Department of Computing, Imperial College London', 'Department of Radiotherapy, Charing Cross Hospital\nInstitute of Global Health Innovation, Imperial College London', 'Department of Computing, Imperial College London']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29611,Fairness & Bias,Robust Test-Time Adaptation for Zero-Shot Prompt Tuning,"CLIP has demonstrated remarkable generalization across diverse downstream tasks. By aligning images and texts in a shared feature space, they enable zero-shot classification via hand-crafted prompts. However, recent studies have shown that hand-crafted prompts may be unsuitable in practical applications. Specifically, choosing an appropriate prompt for a given task requires accurate data and knowledge, which may not be obtainable in practical situations. An inappropriate prompt can result in poor performance. Moreover, if there is no training data, tuning prompts arbitrarily through unlabeled test data may lead to serious performance degradation when giving hand-crafted prompts. Our study reveals that the aforementioned problems are mainly due to the biases in testing data (Data Bias) and pre-trained CLIP model (Model Bias). The Data Bias makes it challenging to choose an appropriate prompt, while Model Bias renders some predictions inaccurate and biased, which leads to error accumulation. To address these biases, we propose robust test-time Adaptation for zeroshot Prompt tuning (ADAPROMPT). Specifically, we ensemble multiple prompts to avoid the worst-case results and dynamically tune prompts to adapt to Data Bias during testing. Furthermore, we adopt a confidence-aware buffer to store balanced and confident unlabeled test data to tune prompts in order to overcome Model Bias. Our extensive experiments on several benchmarks demonstrate that ADAPROMPT alleviates model bias, adapts to data bias and mostly outperforms the state-of-the-art methods at a small time cost. Moreover, our experimental results reveal that ADAPROMPT hardly encounters any performance degradation on these datasets.","['ML: Time-Series/Data Streams', 'ML: Unsupervised & Self-Supervised Learning', 'ML: Semi-Supervised Learning']",[],"['Ding-Chu Zhang', 'Zhi Zhou', 'Yu-Feng Li']","['Nanjing University', 'Nanjing University', 'Nanjing University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29620,Fairness & Bias,United We Stand: Accelerating Privacy-Preserving Neural Inference by Conjunctive Optimization with Interleaved Nexus,"Privacy-preserving Machine Learning as a Service (MLaaS) enables the powerful cloud server to run its well-trained neural model upon the input from resource-limited client, with both of server's model parameters and client's input data protected. While computation efficiency is critical for the practical implementation of privacy-preserving MLaaS and it is inspiring to witness recent advances towards efficiency improvement, there still exists a significant performance gap to real-world applications. In general, state-of-the-art frameworks perform function-wise efficiency optimization based on specific cryptographic primitives. Although it is logical, such independent optimization for each function makes noticeable amount of expensive operations unremovable and misses the opportunity to further accelerate the performance by jointly considering privacy-preserving computation among adjacent functions. As such, we propose COIN: Conjunctive Optimization with Interleaved Nexus, which remodels mainstream computation for each function to conjunctive counterpart for composite function, with a series of united optimization strategies. Specifically, COIN jointly computes a pair of consecutive nonlinear-linear functions in the neural model by reconstructing the intermediates throughout the whole procedure, which not only eliminates the most expensive crypto operations without invoking extra encryption enabler, but also makes the online crypto complexity independent of filter size. Experimentally, COIN demonstrates 11.2x to 29.6x speedup over various function dimensions from modern networks, and 6.4x to 12x speedup on the total computation time when applied in networks with model input from small-scale CIFAR10 to large-scale ImageNet.","['ML: Privacy', 'CV: Bias', 'Fairness & Privacy']",[],"['Qiao Zhang', 'Tao Xiang', 'Chunsheng Xin', 'Hongyi Wu']","['Chongqing University', 'Chongqing University', 'Old Dominion University', 'The University of Arizona']","['China', 'China', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29621,Privacy & Data Governance,A Learnable Discrete-Prior Fusion Autoencoder with Contrastive Learning for Tabular Data Synthesis,"The actual collection of tabular data for sharing involves confidentiality and privacy constraints, leaving the potential risks of machine learning for interventional data analysis unsafely averted. Synthetic data has emerged recently as a privacy-protecting solution to address this challenge. However, existing approaches regard discrete and continuous modal features as separate entities, thus falling short in properly capturing their inherent correlations. In this paper, we propose a novel contrastive learning guided Gaussian Transformer autoencoder, termed GTCoder, to synthesize photo-realistic multimodal tabular data for scientific research. Our approach introduces a transformer-based fusion module that seamlessly integrates multimodal features, permitting for mining more informative latent representations. The attention within the fusion module directs the integrated output features to focus on critical components that facilitate the task of generating latent embeddings. Moreover, we formulate a contrastive learning strategy to implicitly constrain the embeddings from discrete features in the latent feature space by encouraging the similar discrete feature distributions closer while pushing the dissimilar further away, in order to better enhance the representation of the latent embedding. Experimental results indicate that GTCoder is effective to generate photo-realistic synthetic data, with interactive interpretation of latent embedding, and performs favorably against some baselines on most real-world and simulated datasets.","['ML: Deep Learning Algorithms', 'APP: Security', 'ML: Applications', 'ML: Deep Neural Architectures and Foundation Models', 'ML: Privacy']",[],"['Rongchao Zhang', 'Yiwei Lou', 'Dexuan Xu', 'Yongzhi Cao', 'Hanpin Wang', 'Yu Huang']","['Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'School of Software & Microelectronics, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China\nNational Engineering Research Center for Software Engineering, Peking University, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29624,Privacy & Data Governance,Reviewing the Forgotten Classes for Domain Adaptation of Black-Box Predictors,"For addressing the data privacy and portability issues of domain adaptation, Domain Adaptation of Black-box Predictors (DABP) aims to adapt a black-box source model to an unlabeled target domain without accessing both the source-domain data and details of the source model. Although existing DABP approaches based on knowledge distillation (KD) have achieved promising results, we experimentally find that these methods all have the minority class forgetting issue, which refers that the trained model completely forgets some minority classes. To address this issue, we propose a method called Reviewing the Forgotten Classes (RFC), which including two main modules. Firstly, we propose a simple but effective component called selection training (ST). ST selects classes that the model tends to forget according to the learning status of the model and obtains clean samples of the selected classes with the small-loss criterion for enhanced training. ST is orthogonal to previous methods and can effectively alleviate their minority class forgetting issue. Secondly, we find that neighborhood clustering (NC) can help the model learn more balanced than KD so that further alleviate the minority class forgetting issue. However, NC is based on the fact that target features from the source model already form some semantic structure, while DABP is unable to obtain the source model. Thus, we use KD and ST to warm up the target model to form a certain semantic structure. Overall, our method inherits the merits of both ST and NC, and achieves state of the art on three DABP benchmarks.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Shaojie Zhang', 'Chun Shen', 'Shuai Lü', 'Zeyu Zhang']","['Jilin University', 'Jilin University', 'Jilin University', 'Jilin University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29626,Security,Learning with Noisy Labels Using Hyperspherical Margin Weighting,"Datasets often include noisy labels, but learning from them is difficult. Since mislabeled examples usually have larger loss values in training, the small-loss trick is regarded as a standard metric to identify the clean example from the training set for better performance. Nonetheless, this proposal ignores that some clean but hard-to-learn examples also generate large losses. They could be misidentified by this criterion. In this paper, we propose a new metric called the Integrated Area Margin (IAM), which is superior to the traditional small-loss trick, particularly in recognizing the clean but hard-to-learn examples. According to the IAM, we further offer the Hyperspherical Margin Weighting (HMW) approach. It is a new sample weighting strategy that restructures the importance of each example. It should be highlighted that our approach is universal and can strengthen various methods in this field. Experiments on both benchmark and real-world datasets indicate that our HMW outperforms many state-of-the-art approaches in learning with noisy label tasks. Codes are available at https://github.com/Zhangshuojackpot/HMW.","['ML: Multi-class/Multi-label Learning & Extreme Classification', 'ML: Adversarial Learning & Robustness']",[],"['Shuo Zhang', 'Yuwen Li', 'Zhongyu Wang', 'Jianqing Li', 'Chengyu Liu']","['Southeast University', 'Southeast University', 'Southeast University', 'Southeast University', 'Southeast University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29627,Fairness & Bias,One Step Closer to Unbiased Aleatoric Uncertainty Estimation,"Neural networks are powerful tools in various applications, and quantifying their uncertainty is crucial for reliable decision-making. In the deep learning field, the uncertainties are usually categorized into aleatoric (data) and epistemic (model) uncertainty. In this paper, we point out that the existing popular variance attenuation method highly overestimates aleatoric uncertainty. To address this issue, we proposed a new estimation method by actively de-noising the observed data. By conducting a broad range of experiments, we demonstrate that our proposed approach provides a much closer approximation to the actual data uncertainty than the standard method.",['ML: Calibration & Uncertainty Quantification'],[],"['Wang Zhang', 'Ziwen Martin Ma', 'Subhro Das', 'Tsui-Wei Lily Weng', 'Alexandre Megretski', 'Luca Daniel', 'Lam M. Nguyen']","['Massachusetts Institute of Technology', 'Harvard University', 'MIT-IBM Watson AI Lab, IBM Research', 'University of California San Diego', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'IBM Research, Thomas J. Watson Research Center']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29628,Transparency & Explainability,Gaussian Process Neural Additive Models,"Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) poses a major step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that utilize a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensions. It suffers no loss in performance compared with deeper NAM approaches because GPs are well-suited to learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular datasets, showing that it achieves comparable performance in both classification and regression tasks with a massive reduction in the number of parameters.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Classification and Regression', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Wei Zhang', 'Brian Barr', 'John Paisley']","['Columbia University', 'Capital One', 'Columbia University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29629,Security,From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks,"Despite the tremendous success of deep neural networks (DNNs) across various fields, their susceptibility to potential backdoor attacks seriously threatens their application security, particularly in safety-critical or security-sensitive ones. Given this growing threat, there is a pressing need for research into purging backdoors from DNNs. However, prior efforts on erasing backdoor triggers not only failed to withstand increasingly powerful attacks but also resulted in reduced model performance. In this paper, we propose From Toxic to Trustworthy (FTT), an innovative approach to eliminate backdoor triggers while simultaneously enhancing model accuracy. Following the stringent and practical assumption of limited availability of clean data, we introduce a self-attention distillation (SAD) method to remove the backdoor by aligning the shallow and deep parts of the network. Furthermore, we first devise a semi-supervised learning (SSL) method that leverages ubiquitous and available poisoned data to further purify backdoors and improve accuracy. Extensive experiments on various attacks and models have shown that our FTT can reduce the attack success rate from 97% to 1% and improve the accuracy of 4% on average, demonstrating its effectiveness in mitigating backdoor attacks and improving model performance. Compared to state-of-the-art (SOTA) methods, our FTT can reduce the attack success rate by 2 times and improve the accuracy by 5%, shedding light on backdoor cleansing.","['ML: Adversarial Learning & Robustness', 'CV: Adversarial Attacks & Robustness']",[],"['Xianda Zhang', 'Baolin Zheng', 'Jianbao Hu', 'Chengyang Li', 'Xiaoying Bai']","['Department of Computer Science and Technology, Peking University\nAdvanced Institute of Big Data', 'Alibaba Group', 'University of Glasgow', 'Department of Computer Science and Technology, Peking University', 'Advanced Institute of Big Data']","['', 'China', 'United Kingdom', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29633,Fairness & Bias,A Perspective of Q-value Estimation on Offline-to-Online Reinforcement Learning,"Offline-to-online Reinforcement Learning (O2O RL) aims to improve the performance of offline pretrained policy using only a few online samples. Built on offline RL algorithms, most O2O methods focus on the balance between RL objective and pessimism, or the utilization of offline and online samples. In this paper, from a novel perspective, we systematically study the challenges that remain in O2O RL and identify that the reason behind the slow improvement of the performance and the instability of online finetuning lies in the inaccurate Q-value estimation inherited from offline pretraining. Specifically, we demonstrate that the estimation bias and the inaccurate rank of Q-value cause a misleading signal for the policy update, making the standard offline RL algorithms, such as CQL and TD3-BC, ineffective in the online finetuning. Based on this observation, we address the problem of Q-value estimation by two techniques: (1) perturbed value update and (2) increased frequency of Q-value updates. The first technique smooths out biased Q-value estimation with sharp peaks, preventing early-stage policy exploitation of sub-optimal actions. The second one alleviates the estimation bias inherited from offline pretraining by accelerating learning. Extensive experiments on the MuJoco and Adroit environments demonstrate that the proposed method, named SO2, significantly alleviates Q-value estimation issues, and consistently improves the performance against the state-of-the-art methods by up to 83.1%.",['ML: Reinforcement Learning'],[],"['Yinmin Zhang', 'Jie Liu', 'Chuming Li', 'Yazhe Niu', 'Yaodong Yang', 'Yu Liu', 'Wanli Ouyang']","['University of Sydney\nShanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong\nShanghai Artificial Intelligence Laboratory', 'University of Sydney\nShanghai Artificial Intelligence Laboratory', 'Shanghai Artificial Intelligence Laboratory', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Artificial Intelligence Laboratory']","['Australia', 'Hong Kong', 'Australia', 'China', 'United States', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29631,Fairness & Bias,Class-Attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective,"Modern classification problems exhibit heterogeneities across individual classes: Each class may have unique attributes, such as sample size, label quality, or predictability (easy vs difficult), and variable importance at test-time. Without care, these heterogeneities impede the learning process, most notably, when optimizing fairness objectives. Confirming this, under a gaussian mixture setting, we show that the optimal SVM classifier for balanced accuracy needs to be adaptive to the class attributes. This motivates us to propose CAP: An effective and general method that generates a class-specific learning strategy (e.g.~hyperparameter) based on the attributes of that class. This way, optimization process better adapts to heterogeneities. CAP leads to substantial improvements over the naive approach of assigning separate hyperparameters to each class. We instantiate CAP for loss function design and post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show that CAP is competitive with prior art and its flexibility unlocks clear benefits for fairness objectives beyond balanced accuracy. Finally, we evaluate CAP on problems with label noise as well as weighted test objectives to showcase how CAP can jointly adapt to different heterogeneities.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Auto ML and Hyperparameter Tuning']",[],"['Xuechen Zhang', 'Mingchen Li', 'Jiasi Chen', 'Christos Thrampoulidis', 'Samet Oymak']","['University of California Riverside', 'University of Michigan, Ann Arbor', 'University of Michigan, Ann Arbor', 'University of British Columbia', 'University of Michigan, Ann Arbor']","['United States', '', '', 'Canada', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29634,Fairness & Bias,Mitigating Label Bias in Machine Learning: Fairness through Confident Learning,"Discrimination can occur when the underlying unbiased labels are overwritten by an agent with potential bias, resulting in biased datasets that unfairly harm specific groups and cause classifiers to inherit these biases. In this paper, we demonstrate that despite only having access to the biased labels, it is possible to eliminate bias by filtering the fairest instances within the framework of confident learning. In the context of confident learning, low self-confidence usually indicates potential label errors; however, this is not always the case. Instances, particularly those from underrepresented groups, might exhibit low confidence scores for reasons other than labeling errors. To address this limitation, our approach employs truncation of the confidence score and extends the confidence interval of the probabilistic threshold. Additionally, we incorporate with co-teaching paradigm for providing a more robust and reliable selection of fair instances and effectively mitigating the adverse effects of biased labels. Through extensive experimentation and evaluation of various datasets, we demonstrate the efficacy of our approach in promoting fairness and reducing the impact of label bias in machine learning models.","['ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Bias', 'Fairness & Equity']",[],"['Yixuan Zhang', 'Boyu Li', 'Zenan Ling', 'Feng Zhou']","['Hangzhou Dianzi University', 'University of Technology Sydney', 'Huazhong University of Science and Technology', 'Renmin University of China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29640,Fairness & Bias,Dynamic Reactive Spiking Graph Neural Network,"Spiking Graph Neural Networks are emerging tools for analyzing graph data along with low energy consumption and certain biological fidelity. Existing methods directly integrate same-reactive spiking neurons into graph neural networks for processing propagated graphs. However, such same-reactive neurons are not biological-functionality enough compared to the brain's dynamic-reactive ones, limiting the model's expression. Meanwhile, insufficient long-range neighbor information can be excavated with the few-step propagated graph, restricting discrimination of graph spiking embeddings. Inspired by the dynamic cognition in the brain, we propose a Dynamic Reactive Spiking Graph Neural Network that can enhance model's expressive ability in higher biological fidelity. Specifically, we design dynamic reactive spiking neurons to process spiking graph inputs, which have unique optimizable thresholds to spontaneously explore dynamic reactive states between neurons. Moreover, discriminative graph positional spikes are learned and integrated adaptively into spiking outputs through our neurons, thereby exploring long-range neighbors more thoroughly. Finally, with the dynamic reactive mechanism and learnable positional integration, we can obtain a powerful and highly bio-fidelity model with low energy consumption. Experiments on various domain-related datasets can demonstrate the effectiveness of our model. Our code is available at https://github.com/hzhao98/DRSGNN.","['ML: Graph-based Machine Learning', 'ML: Semi-Supervised Learning']",[],"['Han Zhao', 'Xu Yang', 'Cheng Deng', 'Junchi Yan']","['Xidian University', 'Xidian University', 'Xidian University', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29642,Fairness & Bias,Robust Visual Recognition with Class-Imbalanced Open-World Noisy Data,"Learning from open-world noisy data, where both closed-set and open-set noise co-exist in the dataset, is a realistic but underexplored setting. Only recently, several efforts have been initialized to tackle this problem. However, these works assume the classes are balanced when dealing with open-world noisy data. This assumption often violates the nature of real-world large-scale datasets, where the label distributions are generally long-tailed, i.e. class-imbalanced. In this paper, we study the problem of robust visual recognition with class-imbalanced open-world noisy data. We propose a probabilistic graphical model-based approach: iMRF to achieve label noise correction that is robust to class imbalance via an efficient iterative inference of a Markov Random Field (MRF) in each training mini-batch. Furthermore, we design an agreement-based thresholding strategy to adaptively collect clean samples from all classes that includes corrected closed-set noisy samples while rejecting open-set noisy samples. We also introduce a noise-aware balanced cross-entropy loss to explicitly eliminate the bias caused by class-imbalanced data. Extensive experiments on several benchmark datasets including synthetic and real-world noisy datasets demonstrate the superior performance robustness of our method over existing methods. Our code is available at https://github.com/Na-Z/LIOND.","['ML: Classification and Regression', 'CV: Object Detection & Categorization', 'CV: Adversarial Attacks & Robustness']",[],"['Na Zhao', 'Gim Hee Lee']","['Singapore University of Technology and Design', 'National University of Singapore']","['Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29644,Security,Robust Nonparametric Regression under Poisoning Attack,"This paper studies robust nonparametric regression, in which an adversarial attacker can modify the values of up to q samples from a training dataset of size N. Our initial solution is an M-estimator based on Huber loss minimization. Compared with simple kernel regression, i.e. the Nadaraya-Watson estimator, this method can significantly weaken the impact of malicious samples on the regression performance. We provide the convergence rate as well as the corresponding minimax lower bound. The result shows that, with proper bandwidth selection, supremum error is minimax optimal. The L2 error is optimal with relatively small q, but is suboptimal with larger q. The reason is that this estimator is vulnerable if there are many attacked samples concentrating in a small region. To address this issue, we propose a correction method by projecting the initial estimate to the space of Lipschitz functions. The final estimate is nearly minimax optimal for arbitrary q, up to a logarithmic factor.","['ML: Learning Theory', 'ML: Information Theory']",[],"['Puning Zhao', 'Zhiguo Wan']","['Zhejiang Lab, Hangzhou, Zhejiang, China', 'Zhejiang Lab, Hangzhou, Zhejiang, China']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29648,Transparency & Explainability,A Twist for Graph Classification: Optimizing Causal Information Flow in Graph Neural Networks,"Graph neural networks (GNNs) have achieved state-of-the-art results on many graph representation learning tasks by exploiting statistical correlations. However, numerous observations have shown that such correlations may not reflect the true causal mechanisms underlying the data and thus may hamper the ability of the model to generalize beyond the observed distribution. To address this problem, we propose an Information-based Causal Learning (ICL) framework that combines information theory and causality to analyze and improve graph representation learning to transform information relevance to causal dependence. Specifically, we first introduce a multi-objective mutual information optimization objective derived from information-theoretic analysis and causal learning principles to simultaneously extract invariant and interpretable causal information and reduce reliance on non-causal information in correlations. To optimize this multi-objective objective, we enable a causal disentanglement layer that effectively decouples the causal and non-causal information in the graph representations. Moreover, due to the intractability of mutual information estimation, we derive variational bounds that enable us to transform the above objective into a tractable loss function. To balance the multiple information objectives and avoid optimization conflicts, we leverage multi-objective gradient descent to achieve a stable and efficient transformation from informational correlation to causal dependency. Our approach provides important insights into modulating the information flow in GNNs to enhance their reliability and generalization. Extensive experiments demonstrate that our approach significantly improves the robustness and interpretability of GNNs across different distribution shifts. Visual analysis demonstrates how our method converts informative dependencies in representations into causal dependencies.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Deep Learning Algorithms', 'ML: Deep Learning Theory', 'ML: Graph-based Machine Learning', 'ML: Information Theory', 'ML: Representation Learning', 'ML: Semi-Supervised Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Zhe Zhao', 'Pengkun Wang', 'Haibin Wen', 'Yudong Zhang', 'Zhengyang Zhou', 'Yang Wang']","['University of Science and Technology of China\nCity University of Hong Kong', 'University of Science and Technology of China', 'Shaoguan University', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29659,Security,Federated Label-Noise Learning with Local Diversity Product Regularization,"Training data in federated learning (FL) frameworks can have label noise, since they must be stored and annotated on clients' devices.  If trained over such corrupted data, the models learn the wrong knowledge of label noise, which highly degrades their performance.   Although several FL schemes are designed to combat label noise, they suffer performance degradation when the clients' devices only have limited local training samples. To this end, a new scheme called federated label-noise learning (FedLNL) is developed in this paper. The key problem of FedLNL is how to estimate a noise transition matrix (NTM) accurately in the case of limited local training samples. If a gradient-based update method is used to update the local NTM on each client's device, it can generate too large gradients for the local NTM, causing a high estimation error of the local NTM. To tackle this issue, an alternating update method for the local NTM and the local classifier is designed in FedLNL, where the local NTM is updated by a Bayesian inference-based update method. Such an alternating update method makes the loss function of existing NTM-based schemes not applicable to FedLNL. To enable federated optimization of FedLNL, a new regularizer on the parameters of the classifier called local diversity product regularizer is designed for the loss function of FedLNL.  The results show that FedLNL improves the test accuracy of a trained model by up to 25.98%, compared with the state-of-the-art FL schemes that tackle label-noise issues.","['ML: Distributed Machine Learning & Federated Learning', 'ML: Adversarial Learning & Robustness']",[],"['Xiaochen Zhou', 'Xudong Wang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29657,Security,On the Robustness of Neural-Enhanced Video Streaming against Adversarial Attacks,"The explosive growth of video traffic on today's Internet promotes the rise of Neural-enhanced Video Streaming (NeVS), which effectively improves the rate-distortion trade-off by employing a cheap neural super-resolution model for quality enhancement on the receiver side. Missing by existing work, we reveal that the NeVS pipeline may suffer from a practical threat, where the crucial codec component (i.e., encoder for compression and decoder for restoration) can trigger adversarial attacks in a man-in-the-middle manner to significantly destroy video recovery performance and finally incurs the malfunction of downstream video perception tasks. In this paper, we are the first attempt to inspect the vulnerability of NeVS and discover a novel adversarial attack, called codec hijacking, where the injected invisible perturbation conspires with the malicious encoding matrix by reorganizing the spatial-temporal bit allocation within the bitstream size budget. Such a zero-day vulnerability makes our attack hard to defend because there is no visual distortion on the recovered videos until the attack happens. More seriously, this attack can be extended to diverse enhancement models, thus exposing a wide range of video perception tasks under threat. Evaluation based on state-of-the-art video codec benchmark illustrates that our attack significantly degrades the recovery performance of NeVS over previous attack methods. The damaged video quality finally leads to obvious malfunction of downstream tasks with over 75% success rate. We hope to arouse public attention on codec hijacking and its defence.","['General', 'ML: Applications', 'ML: Deep Learning Algorithms', 'ML: Evaluation and Analysis', 'ML: Feature Construction/Reformulation', 'ML: Adversarial Learning & Robustness']",[],"['Qihua Zhou', 'Jingcai Guo', 'Song Guo', 'Ruibin Li', 'Jie Zhang', 'Bingjie Wang', 'Zhenda Xu']","['The Hong Kong Polytechnic University, Hong Kong', 'The Hong Kong Polytechnic University, Hong Kong\nThe Hong Kong Polytechnic University Shenzhen Research Institute, Shenzhen, China', 'The Hong Kong University of Science and Technology, Hong Kong', 'The Hong Kong Polytechnic University, Hong Kong', 'The Hong Kong Polytechnic University, Hong Kong', 'The Hong Kong Polytechnic University, Hong Kong', 'Hong Kong Polytechnic University, Hong Kong']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/29662,Transparency & Explainability,Towards the Disappearing Truth: Fine-Grained Joint Causal Influences Learning with Hidden Variable-Driven Causal Hypergraphs in Time Series,"Causal discovery under Granger causality framework has yielded widespread concerns in time series analysis task. Nevertheless, most previous methods are unaware of the underlying causality disappearing problem, that is, certain weak causalities are less focusable and may be lost during the modeling process, thus leading to biased causal conclusions. Therefore, we propose to introduce joint causal influences (i.e., causal influences from the union of multiple variables) as additional causal indication information to help identify weak causalities. Further, to break the limitation of existing methods that implicitly and coarsely model joint causal influences, we propose a novel hidden variable-driven causal hypergraph neural network to meticulously explore the locality and diversity of joint causal influences, and realize its explicit and fine-grained modeling. Specifically, we introduce hidden variables to construct a causal hypergraph for explicitly characterizing various fine-grained joint causal influences. Then, we customize a dual causal information transfer mechanism (encompassing a multi-level causal path and an information aggregation path) to realize the free diffusion and meticulous aggregation of joint causal influences and facilitate its adaptive learning. Finally, we design a multi-view collaborative optimization constraint to guarantee the characterization diversity of causal hypergraph and capture remarkable forecasting relationships (i.e., causalities). Experiments are conducted to demonstrate the superiority of the proposed model.","['ML: Causal Learning', 'ML: Time-Series/Data Streams']",[],"['Kun Zhu', 'Chunhui Zhao']","['College of Control Science and Engineering, Zhejiang University, Hangzhou, China', 'College of Control Science and Engineering, Zhejiang University, Hangzhou, China']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29661,Transparency & Explainability,Adaptive Meta-Learning Probabilistic Inference Framework for Long Sequence Prediction,"Long sequence prediction has broad and significant application value in fields such as finance, wind power, and weather. However, the complex long-term dependencies of long sequence data and the potential domain shift problems limit the effectiveness of traditional models in practical scenarios. To this end, we propose an Adaptive Meta-Learning Probabilistic Inference Framework (AMPIF) based on sequence decomposition, which can effectively enhance the long sequence prediction ability of various basic models. Specifically, first, we decouple complex sequences into seasonal and trend components through a frequency domain decomposition module. Then, we design an adaptive meta-learning task construction strategy, which divides the seasonal and trend components into different tasks through a clustering-matching approach. Finally, we design a dual-stream amortized network (ST-DAN) to capture shared information between seasonal-trend tasks and use the support set to generate task-specific parameters for rapid generalization learning on the query set. We conducted extensive experiments on six datasets, including wind power and finance scenarios, and the results show that our method significantly outperforms baseline methods in prediction accuracy, interpretability, and algorithm stability and can effectively enhance the long sequence prediction capabilities of base models. The source code is publicly available at https://github.com/Zhu-JP/AMPIF.","['ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data', 'ML: Representation Learning']",[],"['Jianping Zhu', 'Xin Guo', 'Yang Chen', 'Yao Yang', 'Wenbo Li', 'Bo Jin', 'Fei Wu']","['Dalian University of Technology', 'Dalian University of Technology', 'ZHEJIANG UNIVERSITY', 'Zhejiang Lab', 'Zhejiang laboratory', 'Dalian University of Technology', 'Zhejiang University, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29663,Fairness & Bias,Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation,"Estimating the individuals' potential response to varying treatment doses is crucial for decision-making in areas such as precision medicine and management science. Most recent studies predict counterfactual outcomes by learning a covariate representation that is independent of the treatment variable. However, such independence constraints neglect much of the covariate information that is useful for counterfactual prediction, especially when the treatment variables are continuous. To tackle the above issue, in this paper, we first theoretically demonstrate the importance of the balancing and prognostic representations for unbiased estimation of the heterogeneous dose-response curves, that is, the learned representations are constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses. Based on this, we propose a novel Contrastive balancing Representation learning Network using a partial distance measure, called CRNet, for estimating the heterogeneous dose-response curves without losing the continuity of treatments. Extensive experiments are conducted on synthetic and real-world datasets demonstrating that our proposal significantly outperforms previous methods.","['ML: Causal Learning', 'ML: Representation Learning']",[],"['Minqin Zhu', 'Anpeng Wu', 'Haoxuan Li', 'Ruoxuan Xiong', 'Bo Li', 'Xiaoqing Yang', 'Xuan Qin', 'Peng Zhen', 'Jiecheng Guo', 'Fei Wu', 'Kun Kuang']","['Department of Computer Science and Technology, Zhejiang University', 'Department of Computer Science and Technology, Zhejiang University\nMohamed bin Zayed University of Artificial Intelligence', 'Center for Data Science, Peking University', 'Department of Quantitative Theory and Methods, Emory University', 'School of Economics and Management, Tsinghua University', 'Didi Chuxing', 'Didi Chuxing', 'Didi Chuxing', 'Didi Chuxing', 'Department of Computer Science and Technology, Zhejiang University', 'Department of Computer Science and Technology, Zhejiang University']","['China', '', 'China', 'China', 'China', '', '', '', '', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29669,Transparency & Explainability,MFABA: A More Faithful and Accelerated Boundary-Based Attribution Method for Deep Neural Networks,"To better understand the output of deep neural networks (DNN), attribution based methods have been an important approach for model interpretability, which assign a score for each input dimension to indicate its importance towards the model outcome. Notably, the attribution methods use the ax- ioms of sensitivity and implementation invariance to ensure the validity and reliability of attribution results. Yet, the ex- isting attribution methods present challenges for effective in- terpretation and efficient computation. In this work, we in- troduce MFABA, an attribution algorithm that adheres to ax- ioms, as a novel method for interpreting DNN. Addition- ally, we provide the theoretical proof and in-depth analy- sis for MFABA algorithm, and conduct a large scale exper- iment. The results demonstrate its superiority by achieving over 101.5142 times faster speed than the state-of-the-art at- tribution algorithms. The effectiveness of MFABA is thor- oughly evaluated through the statistical analysis in compar- ison to other methods, and the full implementation package is open-source at: https://github.com/LMBTough/MFABA.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Interpretability', 'Explainability', 'and Transparency', 'ML: Adversarial Learning & Robustness']",[],"['Zhiyu Zhu', 'Huaming Chen', 'Jiayu Zhang', 'Xinyi Wang', 'Zhibo Jin', 'Minhui Xue', 'Dongxiao Zhu', 'Kim-Kwang Raymond Choo']","['The University of Sydney', 'The University of Sydney', 'SuZhouYierqi', 'University of Malaya', 'The University of Sydney', ""CSIRO's Data61"", 'Wayne State University', 'University of Texas at San Antonio']","['Australia', 'Australia', 'China', 'Malawi', 'Australia', 'India', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29667,Security,Detection and Defense of Unlearnable Examples,"Privacy preserving has become increasingly critical with the emergence of social media. Unlearnable examples have been proposed to avoid leaking personal information on the Internet by degrading the generalization abilities of deep learning models. However, our study reveals that unlearnable examples are easily detectable. We provide theoretical results on linear separability of certain unlearnable poisoned dataset and simple network-based detection methods that can identify all existing unlearnable examples, as demonstrated by extensive experiments. Detectability of unlearnable examples with simple networks motivates us to design a novel defense method. We propose using stronger data augmentations coupled with adversarial noises generated by simple networks, to degrade the detectability and thus provide effective defense against unlearnable examples with a lower cost. Adversarial training with large budgets is a widely-used defense method on unlearnable examples. We establish quantitative criteria between the poison and adversarial budgets, which determine the existence of robust unlearnable examples or the failure of the adversarial defense.","['ML: Adversarial Learning & Robustness', 'CV: Adversarial Attacks & Robustness']",[],"['Yifan Zhu', 'Lijia Yu', 'Xiao-Shan Gao']","['Academy of Mathematics and Systems Science, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Software, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29682,Security,Learning Efficient and Robust Multi-Agent Communication via Graph Information Bottleneck,"Efficient communication learning among agents has been shown crucial for cooperative multi-agent reinforcement learning (MARL), as it can promote the action coordination of agents and ultimately improve performance. Graph neural network (GNN) provide a general paradigm for communication learning, which consider agents and communication channels as nodes and edges in a graph, with the action selection corresponding to node labeling. Under such paradigm, an agent aggregates information from neighbor agents, which can reduce uncertainty in local decision-making and induce implicit action coordination. However, this communication paradigm is vulnerable to adversarial attacks and noise, and how to learn robust and efficient communication under perturbations has largely not been studied. To this end, this paper introduces a novel Multi-Agent communication mechanism via Graph Information bottleneck (MAGI), which can optimally balance the robustness and expressiveness of the message representation learned by agents. This communication mechanism is aim at learning the minimal sufficient message representation for an agent by maximizing the mutual information (MI) between the message representation and the selected action, and simultaneously constraining the MI between the message representation and the agent feature. Empirical results demonstrate that MAGI is more robust and efficient than state-of-the-art GNN-based MARL methods.","['MAS: Agent Communication', 'ML: Reinforcement Learning', 'MAS: Multiagent Learning']",[],"['Shifei Ding', 'Wei Du', 'Ling Ding', 'Lili Guo', 'Jian Zhang']","['School of Computer Science and Technology, China University of Mining and Technology\nMine Digitization Engineering Research Center of Ministry of Education of the People’s Republic of China', 'School of Computer Science and Technology, China University of Mining and Technology', 'College of Intelligence and Computing, Tianjin University', 'School of Computer Science and Technology, China University of Mining and Technology\nMine Digitization Engineering Research Center of Ministry of Education of the People’s Republic of China', 'School of Computer Science and Technology, China University of Mining and Technology\nMine Digitization Engineering Research Center of Ministry of Education of the People’s Republic of China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29680,Transparency & Explainability,RGMComm: Return Gap Minimization via Discrete Communications in Multi-Agent Reinforcement Learning,"Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in partially observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents, leading to the generation of continuous messages with high communication overhead and poor interpretability. Prior attempts at discrete communication methods generate one-hot vectors trained as part of agents' actions and use the Gumbel softmax operation for calculating message gradients, which are all heuristic designs that do not provide any quantitative guarantees on the expected return.  This paper establishes an upper bound on the return gap between an ideal policy with full observability and an optimal partially observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. To minimize the return gap, we propose the Return-Gap-Minimization Communication (RGMComm) algorithm, which is a surprisingly simple design of discrete message generation functions and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss function, which incorporates cosine-distance as the clustering metric. Evaluations show that RGMComm significantly outperforms state-of-the-art multi-agent communication baselines and can achieve nearly optimal returns with few-bit messages that are naturally interpretable.","['MAS: Multiagent Learning', 'ML: Transparent', 'Interpretable', 'Explainable ML', 'MAS: Agent Communication']",[],"['Jingdi Chen', 'Tian Lan', 'Carlee Joe-Wong']","['The George Washington University', 'The George Washington University', 'Carnegie Mellon University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29692,Security,Stability of Multi-Agent Learning in Competitive Networks: Delaying the Onset of Chaos,"The behaviour of multi agent learning in competitive network games is often studied within the context of zero sum games, in which convergence guarantees may be obtained. However, outside of this class the behaviour of learning is known to display complex behaviours and convergence cannot be always guaranteed. Nonetheless, in order to develop a complete picture of the behaviour of multi agent learning in competitive settings, the zero sum assumption must be lifted. Motivated by this we study the Q Learning dynamics, a popular model of exploration and exploitation in multi agent learning, in competitive network games. We determine how the degree of competition, exploration rate and network connectivity impact the convergence of Q Learning. To study generic competitive games, we parameterise network games in terms of correlations between agent payoffs and study the average behaviour of the Q Learning dynamics across all games drawn from a choice of this parameter. This statistical approach establishes choices of parameters for which Q Learning dynamics converge to a stable fixed point. Differently to previous works, we find that the stability of Q Learning is explicitly dependent only on the network connectivity rather than the total number of agents. Our experiments validate these findings and show that, under certain network structures, the total number of agents can be increased without increasing the likelihood of unstable or chaotic behaviours.","['MAS: Multiagent Learning', 'GTEP: Adversarial Learning', 'ML: Reinforcement Learning']",[],"['Aamal Hussain', 'Francesco Belardinelli']","['Imperial College London', 'Imperial College London']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29691,Security,Learning in Online Principal-Agent Interactions: The Power of Menus,"We study a ubiquitous learning challenge in online principal-agent problems during which the principal learns the agent's private information from the agent's revealed preferences in historical interactions. This paradigm includes important special cases such as pricing and contract design, which have been widely studied in recent literature. However, existing work considers the case where the principal can only choose a single strategy at every round to interact with the agent and then observe the agent's revealed preference through their actions. In this paper, we extend this line of study to allow the principal to offer a menu of strategies to the agent and learn additionally from observing the agent's selection from the menu. We provide a thorough investigation of several online principal-agent problem settings and characterize their sample complexities, accompanied by the corresponding algorithms we have developed. We instantiate this paradigm to several important design problems — including Stackelberg (security) games, contract design, and information design. Finally, we also explore the connection between our findings and existing results about online learning in Stackelberg games, and we offer a solution that can overcome a key hard instance of previous work.","['MAS: Multiagent Learning', 'GTEP: Imperfect Information', 'MAS: Modeling other Agents']",[],"['Minbiao Han', 'Michael Albert', 'Haifeng Xu']","['Department of Computer Science, The University of Chicago', 'Darden Business School, University of Virginia', 'Department of Computer Science, The University of Chicago']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29693,Fairness & Bias,Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing,"Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.","['MAS: Coordination and Collaboration', 'ML: Reinforcement Learning']",[],"['Haobin Jiang', 'Ziluo Ding', 'Zongqing Lu']","['Peking University', 'Peking University\nBeijing Academy of Artificial Intelligence', 'Peking University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29709,Fairness & Bias,Leveraging Partial Symmetry for Multi-Agent Reinforcement Learning,"Incorporating symmetry as an inductive bias into multi-agent reinforcement learning (MARL) has led to improvements in generalization, data efficiency, and physical consistency. While prior research has succeeded in using perfect symmetry prior, the realm of partial symmetry in the multi-agent domain remains unexplored. To fill in this gap, we introduce the partially symmetric Markov game, a new subclass of the Markov game. We then theoretically show that the performance error introduced by utilizing symmetry in MARL is bounded, implying that the symmetry prior can still be useful in MARL even in partial symmetry situations. Motivated by this insight, we propose the Partial Symmetry Exploitation (PSE) framework that is able to adaptively incorporate symmetry prior in MARL under different symmetry-breaking conditions. Specifically, by adaptively adjusting the exploitation of symmetry, our framework is able to achieve superior sample efficiency and overall performance of MARL algorithms. Extensive experiments are conducted to demonstrate the superior performance of the proposed framework over baselines. Finally, we implement the proposed framework in real-world multi-robot testbed to show its superiority.","['MAS: Multiagent Learning', 'ML: Reinforcement Learning', 'ROB: Multi-Robot Systems', 'ML: Representation Learning']",[],"['Xin Yu', 'Rongye Shi', 'Pu Feng', 'Yongkai Tian', 'Simin Li', 'Shuhao Liao', 'Wenjun Wu']","['Beihang University', 'Beihang University', 'Beihang University', 'Beihang University', 'Beihang University', 'Beihang University', 'Beihang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29708,Security,Robust Communicative Multi-Agent Reinforcement Learning with Active Defense,"Communication in multi-agent reinforcement learning (MARL) has been proven to effectively promote cooperation among agents recently. Since communication in real-world scenarios is vulnerable to noises and adversarial attacks, it is crucial to develop robust communicative MARL technique. However, existing research in this domain has predominantly focused on passive defense strategies, where agents receive all messages equally, making it hard to balance performance and robustness. We propose an active defense strategy, where agents automatically reduce the impact of potentially harmful messages on the final decision. There are two challenges to implement this strategy, that are defining unreliable messages and adjusting the unreliable messages' impact on the final decision properly. To address them, we design an Active Defense Multi-Agent Communication framework (ADMAC), which estimates the reliability of received messages and adjusts their impact on the final decision accordingly with the help of a decomposable decision structure. The superiority of ADMAC over existing methods is validated by experiments in three communication-critical tasks under four types of attacks.","['MAS: Agent Communication', 'MAS: Adversarial Agents']",[],"['Lebin Yu', 'Yunbo Qiu', 'Quanming Yao', 'Yuan Shen', 'Xudong Zhang', 'Jian Wang']","['Tsinghua university', 'Tsinghua university', 'Tsinghua university', 'Tsinghua university', 'Tsinghua university', 'Tsinghua university']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29710,Transparency & Explainability,ProAgent: Building Proactive Cooperative Agents with Large Language Models,"Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents' capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates' subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method. For more information about our project, please visit https://pku-proagent.github.io.","['MAS: Coordination and Collaboration', 'MAS: Modeling other Agents', 'MAS: Multiagent Planning', 'NLP: (Large) Language Models']",[],"['Ceyao Zhang', 'Kaijie Yang', 'Siyi Hu', 'Zihao Wang', 'Guanghe Li', 'Yihang Sun', 'Cheng Zhang', 'Zhaowei Zhang', 'Anji Liu', 'Song-Chun Zhu', 'Xiaojun Chang', 'Junge Zhang', 'Feng Yin', 'Yitao Liang', 'Yaodong Yang']","['SSE, The Chinese University of Hong Kong, Shenzhen\nInstitute for Artificial Intelligence, Peking University', 'Institute of Automation, Chinese Academy of Sciences', 'ReLER, AAII, University of Technology Sydney', 'Institute for Artificial Intelligence, Peking University\nNational Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)', 'Institute for Artificial Intelligence, Peking University', 'Institute for Artificial Intelligence, Peking University', 'Institute for Artificial Intelligence, Peking University', 'Institute for Artificial Intelligence, Peking University\nNational Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)', 'Institute for Artificial Intelligence, Peking University', 'National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)', 'ReLER, AAII, University of Technology Sydney', 'Institute of Automation, Chinese Academy of Sciences', 'SSE, The Chinese University of Hong Kong, Shenzhen', 'Institute for Artificial Intelligence, Peking University', 'Institute for Artificial Intelligence, Peking University']","['United States', '', 'United States', '', 'United States', 'United States', 'United States', '', 'United States', '', 'United States', '', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29717,Fairness & Bias,WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia,"Wikipedia can be edited by anyone and thus contains various quality sentences. Therefore, Wikipedia includes some poor-quality edits, which are often marked up by other editors. While editors' reviews enhance the credibility of Wikipedia, it is hard to check all edited text. Assisting in this process is very important, but a large and comprehensive dataset for studying it does not currently exist. Here, we propose WikiSQE, the first large-scale dataset for sentence quality estimation in Wikipedia. Each sentence is extracted from the entire revision history of English Wikipedia, and the target quality labels were carefully investigated and selected. WikiSQE has about 3.4 M sentences with 153 quality labels. In the experiment with automatic classification using competitive machine learning models, sentences that had problems with citation, syntax/semantics, or propositions were found to be more difficult to detect. In addition, by performing human annotation, we found that the model we developed performed better than the crowdsourced workers. WikiSQE is expected to be a valuable resource for other tasks in NLP.","['NLP: Sentence-level Semantics', 'Textual Inference', 'etc.', 'PEAI: Bias', 'Fairness & Equity', 'ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Kenichiro Ando', 'Satoshi Sekine', 'Mamoru Komachi']","['RIKEN AIP', 'RIKEN AIP', 'Hitotsubashi University']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29719,Transparency & Explainability,All Should Be Equal in the Eyes of LMs: Counterfactually Aware Fair Text Generation,"Fairness in Language Models (LMs) remains a long-standing challenge, given the inherent biases in training data that can be perpetuated by models and affect the downstream tasks. Recent methods employ expensive retraining or attempt debiasing during inference by constraining model outputs to contrast from a reference set of biased templates/exemplars. Regardless, they don’t address the primary goal of fairness to maintain equitability across different demographic groups. In this work, we posit that inferencing LMs to generate unbiased output for one demographic under a context ensues from being aware of outputs for other demographics under the same context. To this end, we propose Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically compares the model’s understanding of diverse demographics to generate more equitable sentences. We conduct an extensive empirical evaluation using base LMs of varying sizes and across three diverse datasets and found that CAFIE outperforms strong baselines. CAFIE produces fairer text and strikes the best balance between fairness and language modeling capability.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'ML: Ethics', 'Bias', 'and Fairness', 'NLP: (Large) Language Models', 'NLP: Safety and Robustness']",[],"['Pragyan Banerjee', 'Abhinav Java', 'Surgan Jandial', 'Simra Shahid', 'Shaz Furniturewala', 'Balaji Krishnamurthy', 'Sumit Bhatia']","['Indian Institute of Technology Guwahati', 'MDSR Labs, Adobe', 'MDSR Labs, Adobe', 'MDSR Labs, Adobe', 'Birla Institute of Technology and Science, Pilani', 'MDSR Labs, Adobe', 'MDSR Labs, Adobe']","['Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29721,Transparency & Explainability,When Do Program-of-Thought Works for Reasoning?,"In the realm of embodied artificial intelligence, the reasoning capabilities of Large Language Models (LLMs) play a pivotal role. Although there are effective methods like program-of-thought  prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score CIRS, which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning  and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: (Large) Language Models']",[],"['Zhen Bi', 'Ningyu Zhang', 'Yinuo Jiang', 'Shumin Deng', 'Guozhou Zheng', 'Huajun Chen']","['Zhejiang University\nZhejiang University - Ant Group Joint Laboratory of Knowledge Graph', 'Zhejiang University\nZhejiang University - Ant Group Joint Laboratory of Knowledge Graph', 'Zhejiang University\nZhejiang University - Ant Group Joint Laboratory of Knowledge Graph', 'NUS-NCS Joint Lab, National University of Singapore', 'Zhejiang University\nZhejiang University - Ant Group Joint Laboratory of Knowledge Graph\nDonghai Laboratory', 'Zhejiang University\nZhejiang University - Ant Group Joint Laboratory of Knowledge Graph\nDonghai Laboratory']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29723,Transparency & Explainability,MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models,"The emergence of various medical large language models (LLMs) in the medical domain has highlighted the need for unified evaluation standards, as manual evaluation of LLMs proves to be time-consuming and labor-intensive. To address this issue, we introduce MedBench, a comprehensive benchmark for the Chinese medical domain, comprising 40,041 questions sourced from authentic examination exercises and medical reports of diverse branches of medicine. In particular, this benchmark is composed of four key components: the Chinese Medical Licensing Examination, the Resident Standardization Training Examination, the Doctor In-Charge Qualification Examination, and real-world clinic cases encompassing examinations, diagnoses, and treatments. MedBench replicates the educational progression and clinical practice experiences of doctors in Mainland China, thereby establish- ing itself as a credible benchmark for assessing the mastery of knowledge and reasoning abilities in medical language learning models. We perform extensive experiments and conduct an in-depth analysis from diverse perspectives, which culminate in the following findings: (1) Chinese medical LLMs underperform on this benchmark, highlighting the need for significant advances in clinical knowledge and diagnostic precision. (2) Several general-domain LLMs surprisingly possess considerable medical knowledge. These findings elucidate both the capabilities and limitations of LLMs within the context of MedBench, with the ultimate goal of aiding the medical research community.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Yan Cai', 'Linlin Wang', 'Ye Wang', 'Gerard de Melo', 'Ya Zhang', 'Yanfeng Wang', 'Liang He']","['East China Normal University', 'East China Normal University\nShanghai Artificial Intelligence Laboratory', 'East China Normal University', 'Hasso Plattner Institute\nUniversity of Potsdam', 'Shanghai Jiao Tong University\nShanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University\nShanghai Artificial Intelligence Laboratory', 'East China Normal University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29729,Transparency & Explainability,CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature Removal Problem,"The minimal feature removal problem in the post-hoc explanation area aims to identify the minimal feature set (MFS). Prior studies using the greedy algorithm to calculate the minimal feature set lack the exploration of feature interactions under a monotonic assumption which cannot be satisfied in general scenarios. In order to address the above limitations,  we  propose a Cooperative Integrated Dynamic Refining method (CIDR) to efficiently discover  minimal feature sets. Specifically, we design Cooperative Integrated Gradients (CIG) to detect interactions between features. By incorporating CIG and  characteristics of the minimal feature set, we transform the minimal feature removal problem into a knapsack problem. Additionally, we  devise an auxiliary Minimal Feature Refinement algorithm to determine the  minimal feature set from numerous candidate sets. To the best of our knowledge, our work is the first to address the minimal feature removal problem in the field of natural language processing. Extensive experiments demonstrate that CIDR is capable of tracing representative minimal feature sets with improved interpretability across various models and datasets.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: Other']",[],"['Qian Chen', 'Taolin Zhang', 'Dongyang Li', 'Xiaofeng He']","['School of Computer Science and Technology, East China Normal University, Shanghai, China', 'Alibaba Group', 'School of Computer Science and Technology, East China Normal University, Shanghai, China', 'School of Computer Science and Technology, East China Normal University, Shanghai, China\nNPPA Key Laboratory of Publishing Integration Development, ECNUP, Shanghai, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29728,Transparency & Explainability,Benchmarking Large Language Models in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Jiawei Chen', 'Hongyu Lin', 'Xianpei Han', 'Le Sun']","['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China\nUniversity of Chinese Academy of Sciences, Beijing, China', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China\nState Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China\nState Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29735,Transparency & Explainability,Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons,"Pre-trained language models (PLMs) contain vast amounts of factual knowledge, but how the knowledge is stored in the parameters remains unclear. This paper delves into the complex task of understanding how factual knowledge is stored in multilingual PLMs, and introduces the Architecture-adapted Multilingual Integrated Gradients method, which successfully localizes knowledge neurons more precisely compared to current methods, and is more universal across various architectures and languages. Moreover, we conduct an in-depth exploration on knowledge neurons, leading to the following two important discoveries: (1) The discovery of Language-Independent Knowledge Neurons, which store factual knowledge in a form that transcends language. We design cross-lingual knowledge editing experiments, demonstrating that the PLMs can accomplish this task based on language-independent neurons; (2) The discovery of Degenerate Knowledge Neurons, a novel type of neuron showing that different knowledge neurons can store the same fact. Its property of functional overlap endows the PLMs with a robust mastery of factual knowledge. We design fact-checking experiments, proving that the degenerate knowledge neurons can help the PLMs to detect wrong facts. Experiments corroborate these findings, shedding light on the mechanisms of factual knowledge storage in multilingual PLMs, and contribute valuable insights to the field. The code is available at https://github.com/heng840/AMIG.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: (Large) Language Models']",[],"['Yuheng Chen', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao']","['Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29734,Transparency & Explainability,Benchmarking Large Language Models on Controllable Generation under Diversified Instructions,"While large language models (LLMs) have exhibited impressive instruction-following capabilities, it is still unclear whether and to what extent they can respond to explicit constraints that might be entailed in various instructions. As a significant aspect of LLM alignment, it is thus important to formulate such a specialized set of instructions as well as investigate the resulting behavior of LLMs. To address this vacancy, we propose a new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs' responses to instructions with various constraints. We construct a large collection of constraints-attributed instructions as a test suite focused on both generalization and coverage. Specifically, we advocate an instruction diversification process to synthesize diverse forms of constraint expression and also deliberate the candidate task taxonomy with even finer-grained sub-categories. Finally, we automate the entire evaluation process to facilitate further developments. Different from existing studies on controllable text generation, CoDI-Eval extends the scope to the prevalent instruction-following paradigm for the first time. We provide extensive evaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval, revealing their limitations in following instructions with specific constraints and there is still a significant gap between open-source and commercial closed-source LLMs. We believe this benchmark will facilitate research into improving the controllability of LLMs' responses to instructions. Our data and code are available at https://github.com/Xt-cyh/CoDI-Eval.","['NLP: (Large) Language Models', 'NLP: Generation', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Yihan Chen', 'Benfeng Xu', 'Quan Wang', 'Yi Liu', 'Zhendong Mao']","['University of Science and Technology of China', 'University of Science and Technology of China', 'MOE Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications', 'State Key Laboratory of Communication Content Cognition, People’s Daily Online, Beijing, China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29736,Transparency & Explainability,Talk Funny! A Large-Scale Humor Response Dataset with Chain-of-Humor Interpretation,"Humor is a crucial part of human communication. Understanding humor and generating humorous responses in dialogue can provide natural and empathic human-computer interactions. However, most existing pre-trained language models (PLMs) perform  unsatisfactorily in humor generation. On one hand, the serious shortage of humor corpus and datasets pose challenges for constructing models that can understand and generate humorous expressions. On the other hand, humor generation relies on rich knowledge and commonsense, which is often tacit and unspoken. In this paper, we construct the largest Chinese Explainable Humor Response Dataset to date with chain-of-humor and humor mind map annotations, which can be used to comprehensively evaluate as well as improve the humorous response ability of PLMs. We further design humor-related auxiliary tasks to further enhance PLMs' humorous response performance. Extensive evaluations demonstrate that our proposed dataset and auxiliary tasks effectively help PLMs to generate humorous responses, laying the groundwork for future humor research.","['NLP: (Large) Language Models', 'NLP: Conversational AI/Dialog Systems', 'NLP: Generation', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Yuyan Chen', 'Yichen Yuan', 'Panjun Liu', 'Dayiheng Liu', 'Qinghao Guan', 'Mengfei Guo', 'Haiming Peng', 'Bang Liu', 'Zhixu Li', 'Yanghua Xiao']","['Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Beijing Institute of Technology', 'Alibaba DAMO Academy', 'University of Zurich', 'Beijing Jiaotong University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'RALI \\& Mila, Université de Montréal', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\nFudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China']","['China', '', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29741,Transparency & Explainability,How to Protect Copyright Data in Optimization of Large Language Models?,"Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we observe that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'ML: Optimization']",[],"['Timothy Chu', 'Zhao Song', 'Chiwun Yang']","['Google', 'Adobe Research', 'Sun Yat-sen University']","['United States', 'United States', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29742,Transparency & Explainability,Unsupervised Layer-Wise Score Aggregation for Textual OOD Detection,"Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on anomaly scores (\textit{e.g.}, Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results can be achieved, provided that an oracle selects the best layer. We propose a data-driven, unsupervised method to leverage this observation to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a more significant number of classes (up to 150), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust and consistent results comparable to using the best layer according to an oracle while removing manual feature selection altogether.","['NLP: Safety and Robustness', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Text Classification']",[],"['Maxime Darrin', 'Guillaume Staerman', 'Eduardo Dadalto Camara Gomes', 'Jackie C. K. Cheung', 'Pablo Piantanida', 'Pierre Colombo']","['International Laboratory on Learning Systems\nMILA - Quebec AI Institute\nMcGill University\nUniversité Paris-Saclay', 'Université Paris-Saclay\nCNRS\nINRIA, CEA, Paris', 'Université Paris-Saclay\nLaboratoire signaux et systèmes\nCNRS\nCentraleSupelec', 'McGill University\nMILA - Quebec AI Institute\nCanada CIFAR AI Chair, Mila', 'International Laboratory on Learning Systems\nMILA - Quebec AI Institute\nUniversité Paris-Saclay\nCNRS', 'Université Paris-Saclay\nCentraleSupelec\nEqual, Paris\nMICS']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29743,Transparency & Explainability,Spanning the Spectrum of Hatred Detection: A Persian Multi-Label Hate Speech Dataset with Annotator Rationales,"With the alarming rise of hate speech in online communities, the demand for effective NLP models to identify instances of offensive language has reached a critical point. However, the development of such models heavily relies on the availability of annotated datasets, which are scarce, particularly for less-studied languages. To bridge this gap for the Persian language, we present a novel dataset specifically tailored to multi-label hate speech detection.  Our dataset, called Phate, consists of an extensive collection of over seven thousand manually-annotated Persian tweets, offering a rich resource for training and evaluating hate speech detection models on this language. Notably, each annotation in our dataset specifies the targeted group of hate speech and includes a span of the tweet which elucidates the rationale behind the assigned label. The incorporation of these information expands the potential applications of our dataset, facilitating the detection of targeted online harm or allowing the benchmark to serve research on interpretability of hate speech detection models. The dataset, annotation guideline, and all associated codes are accessible at https://github.com/Zahra-D/Phate.","['NLP: Other', 'NLP: Applications']",[],"['Zahra  Delbari', 'Nafise Sadat Moosavi', 'Mohammad Taher Pilehvar']","['Tehran Institute for Advanced Studies', 'Department of Computer Science, University of Sheffield', 'Cardiff University']","['', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/29756,Security,Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy,"To mitigate potential risks associated with language models (LMs), recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. In this paper, we show that watermarking algorithms designed for LMs cannot be seamlessly applied to conditional text generation (CTG) tasks without a notable decline in downstream task performance. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation with the input context. Compared to the baseline watermarks, our proposed watermark yields significant improvements in both automatic and human evaluations across various text generation models, including BART and Flan-T5, for CTG tasks such as summarization and data-to-text generation. Meanwhile, it maintains detection ability with higher z-scores but lower AUC scores, suggesting the presence of a detection paradox that poses additional challenges for watermarking CTG.","['NLP: Generation', 'NLP: Safety and Robustness']",[],"['Yu Fu', 'Deyi Xiong', 'Yue Dong']","['University of California, Riverside', 'Tianjin University', 'University of California, Riverside']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29764,Transparency & Explainability,A General Search-Based Framework for Generating Textual Counterfactual Explanations,"One of the prominent methods for explaining the decision of a machine-learning classifier is by a counterfactual example. Most current algorithms for generating such examples in the textual domain are based on generative language models.  Generative models, however, are trained to minimize a specific loss function in order to fulfill certain requirements for the generated texts.  Any change in the requirements may necessitate costly retraining, thus potentially limiting their applicability. In this paper, we present a general search-based framework for generating counterfactual explanations in the textual domain.   Our framework is model-agnostic, domain-agnostic, anytime, and does not require retraining in order to adapt to changes in the user requirements.  We model the task as a search problem in a space where the initial state is the classified text, and the goal state is a text in a given target class.  Our framework includes domain-independent modification operators, but can also exploit domain-specific knowledge through specialized operators. The search algorithm attempts to find a text from the target class with minimal user-specified distance from the original classified object.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'ML: Transparent', 'Interpretable', 'Explainable ML', 'PEAI: Accountability', 'Interpretability & Explainability', 'SO: Heuristic Search']",[],"['Daniel Gilo', 'Shaul Markovitch']","['Department of Computer Science Technion - Israel Institute of Technology', 'Department of Computer Science Technion - Israel Institute of Technology']","['Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/29768,Transparency & Explainability,DINGO: Towards Diverse and Fine-Grained Instruction-Following Evaluation,"Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilities on instruction-following remains a challenge due to complexity and diversity of real-world user instructions. While existing evaluation methods focus on general skills, they suffer from two main shortcomings, i.e., lack of fine-grained task-level evaluation and reliance on singular instruction expression. To address these problems, this paper introduces DINGO, a fine-grained and diverse instruction-following evaluation dataset that has two main advantages: (1) DINGO is based on a manual annotated, fine-grained and multi-level category tree with 130 nodes derived from real-world user requests; (2) DINGO includes diverse instructions, generated by both GPT-4 and human experts. Through extensive experiments, we demonstrate that DINGO can not only provide more challenging and comprehensive evaluation for LLMs, but also provide task-level fine-grained directions to further improve LLMs.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'ML: Evaluation and Analysis', 'NLP: (Large) Language Models', 'NLP: Applications']",[],"['Zihui Gu', 'Xingwu Sun', 'Fengzong Lian', 'Zhanhui Kang', 'Chengzhong Xu', 'Ju Fan']","['Renmin University of China\nTencent Inc.', 'Tencent Inc.\nUniversity of Macau', 'Tencent Inc.', 'Tencent Inc.', 'University of Macau', 'Renmin University of China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29771,Security,Detecting and Preventing Hallucinations in Large Vision Language Models,"Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.","['NLP: Language Grounding & Multi-modal NLP', 'CV: Language and Vision', 'CV: Visual Reasoning & Symbolic Representations', 'NLP: Safety and Robustness']",[],"['Anisha Gunjal', 'Jihan Yin', 'Erhan Bas']","['Scale AI', 'Scale AI', 'Scale AI']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29777,Transparency & Explainability,Can Large Language Models Understand Real-World Complex Instructions?,"Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs’ ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Qianyu He', 'Jie Zeng', 'Wenhao Huang', 'Lina Chen', 'Jin Xiao', 'Qianxi He', 'Xunzhe Zhou', 'Jiaqing Liang', 'Yanghua Xiao']","['Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'School of Data Science, Fudan University', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\nFudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29783,Transparency & Explainability,Learning Robust Rationales for Model Explainability: A Guidance-Based Approach,"Selective rationalization can be regarded as a straightforward self-explaining approach for enhancing model explainability in natural language processing tasks. It aims to provide explanations that are more accessible and understandable to non-technical users by first selecting subsets of input texts as rationales and then predicting based on chosen subsets. However, existing methods that follow this select-then-predict framework may suffer from the rationalization degeneration problem, resulting in sub-optimal or unsatisfactory rationales that do not align with human judgments. This problem may further lead to rationalization failure, resulting in meaningless rationales that ultimately undermine people's trust in the rationalization model. To address these challenges, we propose a Guidance-based Rationalization method (G-RAT) that effectively improves robustness against failure situations and the quality of rationales by using a guidance module to regularize selections and distributions. Experimental results on two synthetic settings prove that our method is robust to the rationalization degeneration and failure problems, while the results on two real datasets show its effectiveness in providing rationales in line with human judgments. The source code is available at https://github.com/shuaibo919/g-rat.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Shuaibo Hu', 'Kui Yu']","['School of Computer and Information, Hefei University of Technology', 'School of Computer and Information, Hefei University of Technology']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29792,Transparency & Explainability,Revisiting Document-Level Relation Extraction with Context-Guided Link Prediction,"Document-level relation extraction (DocRE) poses the challenge of identifying relationships between entities within a document. Existing approaches rely on logical reasoning or contextual cues from entities. This paper reframes document-level RE as link prediction over a Knowledge Graph (KG) with distinct benefits: 1) Our approach amalgamates entity context and document-derived logical reasoning, enhancing link prediction quality. 2) Predicted links between entities offer interpretability, elucidating employed reasoning. We evaluate our approach on benchmark datasets - DocRED, ReDocRED, and DWIE. The results indicate that our proposed method outperforms the state-of-the-art models and suggests that incorporating context-based Knowledge Graph link prediction techniques can enhance the performance of document-level relation extraction models.","['NLP: Information Extraction', 'ML: Graph-based Machine Learning']",[],"['Monika Jain', 'Raghava Mutharaju', 'Ramakanth Kavuluru', 'Kuldeep Singh']","['Indraprastha Institute of Information Technology, Delhi, India', 'Indraprastha Institute of Information Technology, Delhi, India', 'University of Kentucky, Lexington, Kentucky, United States', 'Cerence GmbH and Zerotha Research, Germany']","['India', 'India', 'United States', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29795,Transparency & Explainability,Debiasing Multimodal Sarcasm Detection with Contrastive Learning,"Despite commendable achievements made by existing work, prevailing multimodal sarcasm detection studies rely more on textual content over visual information. It unavoidably induces spurious correlations between textual words and labels, thereby significantly hindering the models' generalization capability. To address this problem, we define the task of out-of-distribution (OOD) multimodal sarcasm detection, which aims to evaluate models' generalizability when the word distribution is different in training and testing settings. Moreover, we propose a novel debiasing multimodal sarcasm detection framework with contrastive learning, which aims to mitigate the harmful effect of biased textual factors for robust OOD generalization. In particular, we first design counterfactual data augmentation to construct the positive samples with dissimilar word biases and negative samples with similar word biases. Subsequently, we devise an adapted debiasing contrastive learning mechanism to empower the model to learn robust task-relevant features and alleviate the adverse effect of biased words. Extensive experiments show the superiority of the proposed framework.","['NLP: Safety and Robustness', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Language Grounding & Multi-modal NLP']",[],"['Mengzhao Jia', 'Can Xie', 'Liqiang Jing']","['Shandong University', 'Shandong University', 'University of Texas at Dallas']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29802,Transparency & Explainability,Large Language Models Are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales,"Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ""reasoning-aware"" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area.","['NLP: (Large) Language Models', 'NLP: Applications', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Taeyoon Kwon', 'Kai Tzu-iunn Ong', 'Dongjin Kang', 'Seungjun Moon', 'Jeong Ryong Lee', 'Dosik Hwang', 'Beomseok Sohn', 'Yongsik Sim', 'Dongha Lee', 'Jinyoung Yeo']","['Yonsei University', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Yonsei university', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Yonsei University']","['Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29808,Transparency & Explainability,Task Contamination: Language Models May Not Be Few-Shot Anymore,"Large language models (LLMs) offer impressive performance in various zero-shot and few-shot tasks. However, their success in zero-shot or few-shot settings may be affected by task contamination, a potential limitation that has not been thoroughly examined. This paper investigates how zero-shot and few-shot performance of LLMs has changed chronologically over datasets released over time, and over  LLMs released over time. Utilizing GPT-3 series models and several other recent open-sourced LLMs, and controlling for dataset difficulty, we find that datasets released prior to the LLM training data creation date perform surprisingly better than datasets released post the LLM training data creation date. This strongly indicates that, for many LLMs, there exists task contamination on zero-shot and few-shot evaluation for datasets prior to the LLMs' training data creation date. Additionally, we utilize training data inspection, training data extraction, and a membership inference attack, which reveal further evidence of task contamination. Importantly, we find that for tasks with no possibility of task contamination, LLMs rarely demonstrate statistically significant improvements over simple majority baselines, in both zero and few-shot settings.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Changmao Li', 'Jeffrey Flanigan']","['University of California, Santa Cruz', 'University of California, Santa Cruz']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29818,Transparency & Explainability,PMET: Precise Model Editing in a Transformer,"Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we introduce PMET, which simultaneously optimizes Transformer Component (TC, namely MHSA and FFN) hidden states, while only using the optimized TC hidden states of FFN to precisely update FFN weights. Our experiments demonstrate that PMET exhibits state-of-the-art performance on both the \textsc{counterfact} and zsRE datasets. Our ablation experiments substantiate the effectiveness of our enhancements, further reinforcing the finding that the MHSA encodes certain general knowledge extraction patterns and indicating its storage of a small amount of factual knowledge. Our code is available at \url{https://github.com/xpq-tech/PMET}.","['NLP: (Large) Language Models', 'KRR: Action', 'Change', 'and Causality', 'KRR: Other Foundations of Knowledge Representation & Reasoning', 'NLP: Applications', 'NLP: Other']",[],"['Xiaopeng Li', 'Shasha Li', 'Shezheng Song', 'Jing Yang', 'Jun Ma', 'Jie Yu']","['National University of Defense Technology', 'National University of Defense Technology', 'National University of Defense Technology', 'National University of Defense Technology', 'National University of Defense Technology', 'National University of Defense Technology']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29811,Transparency & Explainability,Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark,"Artificial intelligence (AI) has made remarkable progress across various domains, with large language models like ChatGPT gaining substantial attention for their human-like text-generation capabilities. Despite these achievements, improving spatial reasoning remains a significant challenge for these models. Benchmarks like StepGame evaluate AI spatial reasoning, where ChatGPT has shown unsatisfactory performance. However, the presence of template errors in the benchmark has an impact on the evaluation results. Thus there is potential for ChatGPT to perform better if these template errors are addressed, leading to more accurate assessments of its spatial reasoning capabilities. In this study, we refine the StepGame benchmark, providing a more accurate dataset for model evaluation. We analyze GPT’s spatial reasoning performance on the rectified benchmark, identifying proficiency in mapping natural language text to spatial relations but limitations in multi-hop reasoning. We provide a flawless solution to the benchmark by combining template-to-relation mapping with logic-based reasoning. This combination demonstrates proficiency in performing qualitative reasoning on StepGame without encountering any errors. We then address the limitations of GPT models in spatial reasoning. To improve spatial reasoning, we deploy Chain-of-Thought and Tree-of-thoughts prompting strategies, offering insights into GPT’s cognitive process. Our investigation not only sheds light on model deficiencies but also proposes enhancements, contributing to the advancement of AI with more robust spatial reasoning capabilities.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'DMKM: Mining of Spatial', 'Temporal or Spatio-Temporal Data', 'NLP: (Large) Language Models', 'NLP: Other', 'PRS: Model-Based Reasoning', 'PRS: Optimization of Spatio-temporal Systems']",[],"['Fangjun Li', 'David C. Hogg', 'Anthony G. Cohn']","['University of Leeds', 'University of Leeds', 'University of Leeds and the Alan Turing Institute']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/29819,Transparency & Explainability,Dialogues Are Not Just Text: Modeling Cognition for Dialogue Coherence Evaluation,"The generation of logically coherent dialogues by humans relies on underlying cognitive abilities. Based on this, we redefine the dialogue coherence evaluation process, combining cognitive judgment with the basic text to achieve a more human-like evaluation. We propose a novel dialogue evaluation framework based on Dialogue Cognition Graph (DCGEval) to implement the fusion by in-depth interaction between cognition modeling and text modeling. The proposed Abstract Meaning Representation (AMR) based graph structure called DCG aims to uniformly model four dialogue cognitive abilities. Specifically, core-semantic cognition is modeled by converting the utterance into an AMR graph, which can extract essential semantic information without redundancy. The temporal and role cognition are modeled by establishing logical relationships among the different AMR graphs. Finally, the commonsense knowledge from ConceptNet is fused to express commonsense cognition. Experiments demonstrate the necessity of modeling human cognition for dialogue evaluation, and our DCGEval presents stronger correlations with human judgments compared to other state-of-the-art evaluation metrics.","['NLP: Conversational AI/Dialog Systems', 'CMS: Other Foundations of Cognitive Modeling & Systems', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Xue Li', 'Jia Su', 'Yang Yang', 'Zipeng Gao', 'Xinyu Duan', 'Yi Guan']","['Harbin Institute of Technology', 'Huawei Cloud', 'Harbin Institute of Technology', 'University of Science and Technology of China', 'Huawei Cloud', 'Harbin Institute of Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29822,Transparency & Explainability,LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction,"Data contamination in evaluation is getting increasingly prevalent with the emergence of language models pre-trained on super large, automatically crawled corpora. This problem leads to significant challenges in the accurate assessment of model capabilities and generalisations. In this paper, we propose LatestEval, an automatic method that leverages the most recent texts to create uncontaminated reading comprehension evaluations. LatestEval avoids data contamination by only using texts published within a recent time window, ensuring no overlap with the training corpora of pre-trained language models. We develop the LatestEval automated pipeline to 1) gather the latest texts; 2) identify key information, and 3) construct questions targeting the information while removing the existing answers from the context. This encourages models to infer the answers themselves based on the remaining context, rather than just copy-paste. Our experiments demonstrate that language models exhibit negligible memorisation behaviours on LatestEval as opposed to previous benchmarks, suggesting a significantly reduced risk of data contamination and leading to a more robust evaluation. Data and code are publicly available at: https://github.com/liyucheng09/LatestEval.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: (Large) Language Models']",[],"['Yucheng Li', 'Frank Guerin', 'Chenghua Lin']","['University of Surrey', 'University of Surrey', 'University of Manchester']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/29824,Transparency & Explainability,Machine-Created Universal Language for Cross-Lingual Transfer,"There are two primary approaches to addressing cross-lingual transfer: multilingual pre-training, which implicitly aligns the hidden representations of various languages, and translate-test, which explicitly translates different languages into an intermediate language, such as English. Translate-test offers better interpretability compared to multilingual pre-training. However, it has lower performance than multilingual pre-training and struggles with word-level tasks due to translation altering word order. As a result, we propose a new Machine-created Universal Language (MUL) as an alternative intermediate language. MUL comprises a set of discrete symbols forming a universal vocabulary and a natural language to MUL translator for converting multiple natural languages to MUL. MUL unifies shared concepts from various languages into a single universal word, enhancing cross-language transfer. Additionally, MUL retains language-specific words and word order, allowing the model to be easily applied to word-level tasks. Our experiments demonstrate that translating into MUL yields improved performance compared to multilingual pre-training, and our analysis indicates that MUL possesses strong interpretability. The code is at: https://github.com/microsoft/Unicoder/tree/master/MCUL.","['NLP: Machine Translation', 'Multilinguality', 'Cross-Lingual NLP']",[],"['Yaobo Liang', 'Quanzhi Zhu', 'Junhe Zhao', 'Nan Duan']","['Microsoft Research Asia', 'Microsoft Research Asia', 'Microsoft Research Asia', 'Microsoft Research']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29834,Transparency & Explainability,Robust Evaluation Measures for Evaluating Social Biases in Masked Language Models,"Many evaluation measures are used to evaluate social biases in masked language models (MLMs). However, we find that these previously proposed evaluation measures are lacking robustness in scenarios with limited datasets. This is because these measures are obtained by comparing the pseudo-log-likelihood (PLL) scores of the stereotypical and anti-stereotypical samples using an indicator function. The disadvantage is the limited mining of the PLL score sets without capturing its distributional information. In this paper, we represent a PLL score set as a Gaussian distribution and use Kullback-Leibler (KL) divergence and Jensen–Shannon (JS) divergence to construct evaluation measures for the distributions of stereotypical and anti-stereotypical PLL scores. Experimental results on the publicly available datasets StereoSet (SS) and CrowS-Pairs (CP) show that our proposed measures are significantly more robust and interpretable than those proposed previously.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Safety and Robustness', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],['Yang Liu'],['Tianjin University'],['China']
https://ojs.aaai.org/index.php/AAAI/article/view/29840,Transparency & Explainability,Mastering Context-to-Label Representation Transformation for Event Causality Identification with Diffusion Models,"To understand event structures of documents, event causality identification (ECI) emerges as a crucial task, aiming to discern causal relationships among event mentions. The latest approach for ECI has introduced advanced deep learning models where transformer-based encoding models, complemented by enriching components, are typically leveraged to learn effective event context representations for causality prediction. As such, an important step for ECI models is to transform the event context representations into causal label representations to perform logits score computation for training and inference purposes. Within this framework, event context representations might encapsulate numerous complicated and noisy structures due to the potential long context between the input events while causal label representations are intended to capture pure information about the causal relations to facilitate score estimation. Nonetheless, a notable drawback of existing ECI models stems from their reliance on simple feed-forward networks to handle the complex context-to-label representation transformation process, which might require drastic changes in the representations to hinder the learning process. To overcome this issue, our work introduces a novel method for ECI where, instead abrupt transformations, event context representations are gradually updated to achieve effective label representations. This process will be done incrementally to allow filtering of irrelevant structures at varying levels of granularity for causal relations. To realize this, we present a diffusion model to learn gradual representation transition processes between context and causal labels. It operates through a forward pass for causal label representation noising and a reverse pass for reconstructing label representations from random noise.  Our experiments on different datasets across multiple languages demonstrate the advantages of the diffusion model with state-of-the-art performance for ECI.",['NLP: Information Extraction'],[],"['Hieu Man', 'Franck Dernoncourt', 'Thien Huu Nguyen']","['University of Oregon', 'Adobe Research', 'University of Oregon\nVinAI Research']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29842,Transparency & Explainability,Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution,"Modern language modeling tasks are often underspecified: for a given token prediction, many words may satisfy the user’s intent of producing natural language at inference time, however only one word will minimize the task’s loss function at training time. We introduce a simple causal mechanism to describe the role underspecification plays in the generation of spurious correlations. Despite its simplicity, our causal model directly informs the development of two lightweight black-box evaluation methods, that we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid in the detection of inference-time task underspecification by exploiting 2) previously unreported gender vs. time and gender vs. location spurious correlations on LLMs with a range of A) sizes: from BERT-base to GPT-3.5, B) pre-training objectives: from masked & autoregressive language modeling to a mixture of these objectives, and C) training stages: from pre-training only to reinforcement learning from human feedback (RLHF). Code and open-source demos available at https://github.com/2dot71mily/uspec.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'RU: Causality', 'NLP: (Large) Language Models']",[],['Emily McMilin'],['Independent Researcher'],['']
https://ojs.aaai.org/index.php/AAAI/article/view/29845,Transparency & Explainability,Accelerating the Global Aggregation of Local Explanations,"Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a naive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session.    We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-k words with the highest global impact according to different aggregation functions. Some of our techniques are lossless and some are lossy. We show that for a very mild loss of quality, we are able to accelerate the computation by up to 30 times, reducing the computation from hours to minutes. We also devise and study a probabilistic model that accounts for noise in the Anchor algorithm and diminishes the bias toward words that are frequent yet low in impact.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Alon Mor', 'Yonatan Belinkov', 'Benny Kimelfeld']","['Technion, Haifa, Israel', 'Technion, Haifa, Israel', 'Technion, Haifa, Israel']","['Israel', 'Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/29853,Transparency & Explainability,A Joint Framework with Heterogeneous-Relation-Aware Graph and Multi-Channel Label Enhancing Strategy for Event Causality Extraction,"Event Causality Extraction (ECE) aims to extract the cause-effect event pairs with their structured event information from plain texts. As far as we know, the existing ECE methods mainly focus on the correlation between arguments, without explicitly modeling the causal relationship between events, and usually design two independent frameworks to extract cause events and effect events, respectively, which cannot effectively capture the dependency between the subtasks. Therefore, we propose a joint multi-label extraction framework for ECE to alleviate the above limitations. In particular, 1) we design a heterogeneous-relation-aware graph module to learn the potential relationships between events and arguments, in which we construct the heterogeneous graph by taking the predefined event types and all the words in the sentence as nodes, and modeling three relationships of ""event-event"", ""event-argument"" and ""argument-argument"" as edges. 2) We also design a multi-channel label enhancing module to better learn the distributed representation of each label in the multi-label extraction framework, and further enhance the interaction between the subtasks by considering the preliminary results of cause-effect type identification and event argument extraction. The experimental results on the benchmark dataset ECE-CCKS show that our approach outperforms previous state-of-the-art methods, and that our model also performs well on the complex samples with multiple cause-effect event pairs.","['NLP: Information Extraction', 'NLP: Discourse', 'Pragmatics & Argument Mining']",[],"['Ruili Pu', 'Yang Li', 'Jun Zhao', 'Suge Wang', 'Deyu Li', 'Jian Liao', 'Jianxing Zheng']","['School of Computer and Information Technology, Shanxi University, Taiyuan, China', 'School of Finance, Shanxi University of Finance and Economics, Taiyuan, China', 'National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China', 'School of Computer and Information Technology, Shanxi University, Taiyuan, China\nKey Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China', 'School of Computer and Information Technology, Shanxi University, Taiyuan, China\nKey Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Shanxi University, Taiyuan, China', 'School of Computer and Information Technology, Shanxi University, Taiyuan, China', 'School of Computer and Information Technology, Shanxi University, Taiyuan, China']","['China', 'China', '', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29856,Transparency & Explainability,Using Artificial Populations to Study Psychological Phenomena in Neural Models,"The recent proliferation of research into transformer based natural language processing has led to a number of studies which attempt to detect the presence of human-like cognitive behavior in the models. We contend that, as is true of human psychology, the investigation of cognitive behavior in language models must be conducted in an appropriate population of an appropriate size for the results to be meaningful. We leverage work in uncertainty estimation in a novel approach to efficiently construct experimental populations. The resultant tool, PopulationLM, has been made open source. We provide theoretical grounding in the uncertainty estimation literature and motivation from current cognitive work regarding language models. We discuss the methodological lessons from other scientific communities and attempt to demonstrate their application to two artificial population studies. Through population based experimentation we find that language models exhibit behavior consistent with typicality effects among categories highly represented in training. However, we find that language models don't tend to exhibit structural priming effects. Generally, our results show that single models tend to over estimate the presence of cognitive behaviors in neural models.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Jesse Roberts', 'Kyle Moore', 'Drew Wilenzick', 'Douglas Fisher']","['Vanderbilt University', 'Vanderbilt University', 'Cornell University', 'Vanderbilt University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29859,Transparency & Explainability,OntoFact: Unveiling Fantastic Fact-Skeleton of LLMs via Ontology-Driven Reinforcement Learning,"Large language models (LLMs) have demonstrated impressive proficiency in information retrieval, while they are prone to generating incorrect responses that conflict with reality, a phenomenon known as intrinsic hallucination. The critical challenge lies in the unclear and unreliable fact distribution within LLMs trained on vast amounts of data. The prevalent approach frames the factual detection task as a question-answering paradigm, where the LLMs are asked about factual knowledge and examined for correctness. However, existing studies primarily focused on deriving test cases only from several specific domains, such as movies and sports, limiting the comprehensive observation of missing knowledge and the analysis of unexpected hallucinations. To address this issue, we propose OntoFact, an adaptive framework for detecting unknown facts of LLMs, devoted to mining the ontology-level skeleton of the missing knowledge. Specifically, we argue that LLMs could expose the ontology-based similarity among missing facts and introduce five representative knowledge graphs (KGs) as benchmarks. We further devise a sophisticated ontology-driven reinforcement learning (ORL) mechanism to produce error-prone test cases with specific entities and relations automatically. The ORL mechanism rewards the KGs for navigating toward a feasible direction for unveiling factual errors. Moreover, empirical efforts demonstrate that dominant LLMs are biased towards answering Yes rather than No, regardless of whether this knowledge is included. To mitigate the overconfidence of LLMs, we leverage a hallucination-free detection (HFD) strategy to tackle unfair comparisons between baselines, thereby boosting the result robustness. Experimental results on 5 datasets, using 32 representative LLMs, reveal a general lack of fact in current LLMs. Notably, ChatGPT exhibits fact error rates of 51.6% on DBpedia and 64.7% on YAGO, respectively. Additionally, the ORL mechanism demonstrates promising error prediction scores, with F1 scores ranging from 70% to 90% across most LLMs. Compared to the exhaustive testing, ORL achieves an average recall of 80% while reducing evaluation time by 35.29% to 63.12%.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Ziyu Shang', 'Wenjun Ke', 'Nana Xiu', 'Peng Wang', 'Jiajun Liu', 'Yanhui Li', 'Zhizhao Luo', 'Ke Ji']","['School of Computer Science and Engineering, Southeast University', 'School of Computer Science and Engineering, Southeast University\nKey Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'School of Cyber Science and Engineering, Southeast University', 'School of Computer Science and Engineering, Southeast University\nKey Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'School of Computer Science and Engineering, Southeast University', 'State Key Laboratory for Novel Software Technology, Nanjing University', 'Beijing Institute of Computer Technology and Application', 'School of Computer Science and Engineering, Southeast University']","['China', '', 'China', '', 'China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29857,Transparency & Explainability,Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling,"Human evaluation is viewed as a reliable evaluation method for NLG which is expensive and time-consuming. To save labor and costs, researchers usually perform human evaluation on a small subset of data sampled from the whole dataset in practice. However, different selection subsets will lead to different rankings of the systems. To give a more correct inter-system ranking and make the gold standard human evaluation more reliable, we propose a Constrained Active Sampling Framework (CASF) for reliable human judgment. CASF operates through a Learner, a Systematic Sampler and a Constrained Controller to select representative samples for getting a more correct inter-system ranking. Experiment results on 137 real NLG evaluation setups with 44 human evaluation metrics across 16 datasets and 5 NLG tasks demonstrate CASF receives 93.18\% top-ranked system recognition accuracy and ranks first or ranks second on 90.91\% of the human metrics with 0.83 overall inter-system ranking Kendall correlation. Code and data are publicly available online.","['NLP: Generation', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Jie Ruan', 'Xiao Pu', 'Mingqi Gao', 'Xiaojun Wan', 'Yuesheng Zhu']","['Peking University', 'Peking University', 'Peking University', 'Peking University', 'Peking University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29861,Transparency & Explainability,CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models,"As an indispensable ingredient of intelligence, commonsense reasoning is crucial for large language models (LLMs) in real-world scenarios. In this paper, we propose CORECODE, a dataset that contains abundant commonsense knowledge manually annotated on dyadic dialogues, to evaluate the commonsense reasoning and commonsense conflict detection capabilities of Chinese LLMs. We categorize commonsense knowledge in everyday conversations into three dimensions: entity, event, and social interaction. For easy and consistent annotation, we standardize the form of commonsense knowledge annotation in open-domain dialogues as ""domain: slot = value"". A total of 9 domains and 37 slots are defined to capture diverse commonsense knowledge. With these pre-defined domains and slots, we collect 76,787 commonsense knowledge annotations from 19,700 dialogues through crowdsourcing. To evaluate and enhance the commonsense reasoning capability for LLMs on the curated dataset, we establish a series of dialogue-level reasoning and detection tasks, including commonsense knowledge filling, commonsense knowledge generation, commonsense conflict phrase detection, domain identification, slot identification, and event causal inference. A wide variety of existing open-source Chinese LLMs are evaluated with these tasks on our dataset. Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting. We release the data and codes of CORECODE at https://github.com/danshi777/CORECODE to promote commonsense reasoning evaluation and study of LLMs in the context of daily conversations.","['NLP: Conversational AI/Dialog Systems', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Dan Shi', 'Chaobin You', 'Jiantao Huang', 'Taihao Li', 'Deyi Xiong']","['Tianjin University', 'Tianjin University', 'Zhejiang Lab', 'Zhejiang Lab', 'Tianjin University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29871,Security,Collaborative Synthesis of Patient Records through Multi-Visit Health State Inference,"Electronic health records (EHRs) have become the foundation of machine learning applications in healthcare, while the utility of real patient records is often limited by privacy and security concerns. Synthetic EHR generation provides an additional perspective to compensate for this limitation. Most existing methods synthesize new records based on real EHR data, without consideration of different types of events in EHR data, which cannot control the event combinations in line with medical common sense. In this paper, we propose MSIC,  a Multi-visit health Status Inference model for Collaborative EHR synthesis to address these limitations. First, we formulate the synthetic EHR generation process as a probabilistic graphical model and tightly connect different types of events by modeling the latent health states. Then, we derive a health state inference method tailored for the multi-visit scenario to effectively utilize previous records to synthesize current and future records. Furthermore, we propose to generate medical reports to add textual descriptions for each medical event,  providing broader applications for synthesized EHR data. For generating different paragraphs in each visit, we incorporate a multi-generator deliberation framework to collaborate the message passing of multiple generators and employ a two-phase decoding strategy to generate high-quality reports. Our extensive experiments on the widely used benchmarks, MIMIC-III and MIMIC-IV, demonstrate that MSIC advances state-of-the-art results on the quality of synthetic data while maintaining low privacy risks.","['NLP: Applications', 'ML: Applications', 'NLP: Generation']",[],"['Hongda Sun', 'Hongzhan Lin', 'Rui Yan']","['Gaoling School of Artificial Intelligence, Renmin University of China', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Gaoling School of Artificial Intelligence, Renmin University of China\nEngineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education']","['China', 'China', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29873,Transparency & Explainability,UMIE: Unified Multimodal Information Extraction with Instruction Tuning,"Multimodal information extraction (MIE) gains significant attention as the popularity of multimedia content increases. However, current MIE methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across MIE tasks. To address these issues, we propose UMIE, a unified multimodal information extractor to unify three MIE tasks as a generation problem using instruction tuning, being able to effectively extract both textual and visual mentions. Extensive experiments show that our single UMIE outperforms various state-of-the-art (SoTA) methods across six MIE datasets on three tasks. Furthermore, in-depth analysis demonstrates UMIE's strong generalization in the zero-shot setting, robustness to instruction variants, and interpretability. Our research serves as an initial step towards a unified MIE model and initiates the exploration into both instruction tuning and large language models within the MIE domain. Our code, data, and model are available at https://github.com/ZUCC-AI/UMIE.","['NLP: Information Extraction', 'NLP: Language Grounding & Multi-modal NLP']",[],"['Lin  Sun', 'Kai Zhang', 'Qingyuan Li', 'Renze Lou']","['Hangzhou City University', 'Ohio State University', 'Zhejiang University', 'Pennsylvania State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29872,Transparency & Explainability,SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research,"Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a ""dynamic"" subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Liangtai Sun', 'Yang Han', 'Zihan Zhao', 'Da Ma', 'Zhennan Shen', 'Baocai Chen', 'Lu Chen', 'Kai Yu']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29885,Security,Mitigating the Impact of False Negative in Dense Retrieval with Contrastive Confidence Regularization,"In open-domain Question Answering (QA), dense text retrieval is crucial for finding relevant passages to generate answers. Typically, contrastive learning is used to train a retrieval model, which maps passages and queries to the same semantic space, making similar ones closer and dissimilar ones further apart. However, training such a system is challenging due to the false negative problem, where relevant passages may be missed during data annotation. Hard negative sampling, commonly used to improve contrastive learning, can introduce more noise in training. This is because hard negatives are those close to a given query, and thus more likely to be false negatives. To address this, we propose a novel contrastive confidence regularizer for Noise Contrastive Estimation (NCE) loss, a commonly used contrastive loss. Our analysis shows that the regularizer helps make the dense retrieval model more robust against false negatives with a theoretical guarantee. Additionally, we propose a model-agnostic method to filter out noisy negative passages in the dataset, improving any downstream dense retrieval models. Through experiments on three datasets, we demonstrate that our method achieves better retrieval performance in comparison to existing state-of-the-art dense retrieval systems.","['NLP: Question Answering', 'DMKM: Conversational Systems for Recommendation & Retrieval', 'ML: Unsupervised & Self-Supervised Learning', 'General', 'NLP: Safety and Robustness', 'NLP: Learning & Optimization for NLP', 'NLP: (Large) Language Models']",[],"['Shiqi Wang', 'Yeqin Zhang', 'Cam-Tu Nguyen']","['National Key Laboratory for Novel Software Technology, Nanjing University\nSchool of Artificial Intelligence, Nanjing University', 'National Key Laboratory for Novel Software Technology, Nanjing University\nSchool of Artificial Intelligence, Nanjing University', 'National Key Laboratory for Novel Software Technology, Nanjing University\nSchool of Artificial Intelligence, Nanjing University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29887,Transparency & Explainability,LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs,"Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.","['NLP: Applications', 'CMS: Conceptual Inference and Reasoning', 'DMKM: Recommender Systems', 'NLP: (Large) Language Models']",[],"['Yan Wang', 'Zhixuan Chu', 'Xin Ouyang', 'Simeng Wang', 'Hongyan Hao', 'Yue Shen', 'Jinjie Gu', 'Siqiao Xue', 'James Zhang', 'Qing Cui', 'Longfei Li', 'Jun Zhou', 'Sheng Li']","['Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'Ant Group', 'University of Virginia']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29888,Fairness & Bias,A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling,"The goal of document-level relation extraction (RE) is to identify relations between entities that span multiple sentences. Recently, incomplete labeling in document-level RE has received increasing attention, and some studies have used methods such as positive-unlabeled learning to tackle this issue, but there is still a lot of room for improvement. Motivated by this, we propose a positive-augmentation and positive-mixup positive-unlabeled metric learning framework (P3M). Specifically, we formulate document-level RE as a metric learning problem. We aim to pull the distance closer between entity pair embedding and their corresponding relation embedding, while pushing it farther away from the none-class relation embedding. Additionally, we adapt the positive-unlabeled learning to this loss objective. In order to improve the generalizability of the model, we use dropout to augment positive samples and propose a positive-none-class mixup method. Extensive experiments show that P3M improves the F1 score by approximately 4-10 points in document-level RE with incomplete labeling, and achieves state-of-the-art results in fully labeled scenarios. Furthermore, P3M has also demonstrated robustness to prior estimation bias in incomplete labeled scenarios.","['NLP: Information Extraction', 'ML: Semi-Supervised Learning']",[],"['Ye Wang', 'Huazheng Pan', 'Tao Zhang', 'Wen Wu', 'Wenxin Hu']","['East China Normal University', 'East China Normal University', 'Tsinghua University', 'East China Normal University', 'East China Normal University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29890,Transparency & Explainability,STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering,"Recently we have witnessed the rapid development of video question answering models. However, most models can only handle simple videos in terms of temporal reasoning, and their performance tends to drop when answering temporal-reasoning questions on long and informative videos.  To tackle this problem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable Intermediate Results for video question answering. STAIR is a neural module network, which contains a program generator to decompose a given question into a hierarchical combination of several sub-tasks, and a set of lightweight neural modules to complete each of these sub-tasks. Though neural module networks are already widely studied on image-text tasks, applying them to videos is a non-trivial task, as reasoning on videos requires different abilities. In this paper, we define a set of basic video-text sub-tasks for video question answering and design a set of lightweight modules to complete them. Different from most prior works, modules of STAIR return intermediate outputs specific to their intentions instead of always returning attention maps, which makes it easier to interpret and collaborate with pre-trained models. We also introduce intermediate supervision to make these intermediate outputs more accurate. We conduct extensive experiments on several video question answering datasets under various settings to show STAIR's performance, explainability, compatibility with pre-trained models, and applicability when program annotations are not available.  Code: https://github.com/yellow-binary-tree/STAIR","['NLP: Question Answering', 'CV: Video Understanding & Activity Analysis']",[],"['Yueqian Wang', 'Yuxuan Wang', 'Kai  Chen', 'Dongyan Zhao']","['Wangxuan Institute of Computer Technology, Peking University', 'Beijing Institute for General Artificial Intelligence\nNational Key Laboratory of General Artificial Intelligence', 'School of Economics, Peking University', 'Wangxuan Institute of Computer Technology, Peking University\nNational Key Laboratory of General Artificial Intelligence']","['United States', '', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29895,Transparency & Explainability,"On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling","Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstrate that our method surpasses state-of-the-art baselines, effectively improving the affinity, rationality, and diversity of hierarchical topic modeling with better performance on downstream tasks.","['NLP: Text Classification', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: Applications']",[],"['Xiaobao Wu', 'Fengjun Pan', 'Thong Nguyen', 'Yichao Feng', 'Chaoqun Liu', 'Cong-Duy Nguyen', 'Anh Tuan Luu']","['Nanyang Technological University, Singapore', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore', 'Nanyang Technological University, Singapore', 'Nanyang Technological University, Singapore\nDAMO Academy, Alibaba Group, Singapore', 'Nanyang Technological University, Singapore', 'Nanyang Technological University, Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29897,Transparency & Explainability,De-biased Attention Supervision for Text Classification with Causality,"In text classification models, while the unsupervised attention mechanism can enhance performance, it often produces attention distributions that are puzzling to humans, such as assigning high weight to seemingly insignificant conjunctions. Recently, numerous studies have explored Attention Supervision (AS) to guide the model toward more interpretable attention distributions. However, such AS can impact classification performance, especially in specialized domains. In this paper, we address this issue from a causality perspective. Firstly, we leverage the causal graph to reveal two biases in the AS: 1) Bias caused by the label distribution of the dataset. 2) Bias caused by the words' different occurrence ranges that some words can occur across labels while others only occur in a particular label. We then propose a novel De-biased Attention Supervision (DAS) method to eliminate these biases with causal techniques. Specifically, we adopt backdoor adjustment on the label-caused bias and reduce the word-caused bias by subtracting the direct causal effect of the word. Through extensive experiments on two professional text classification datasets (e.g., medicine and law), we demonstrate that our method achieves improved classification accuracy along with more coherent attention distributions.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: Text Classification']",[],"['Yiquan Wu', 'Yifei Liu', 'Ziyu Zhao', 'Weiming Lu', 'Yating Zhang', 'Changlong Sun', 'Fei Wu', 'Kun Kuang']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Alibaba Group', 'Alibaba Group', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29901,Transparency & Explainability,ALISON: Fast and Effective Stylometric Authorship Obfuscation,"Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features,  allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: (Large) Language Models', 'NLP: Applications', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'APP: Security']",[],"['Eric Xing', 'Saranya Venkatraman', 'Thai Le', 'Dongwon Lee']","['Washington University in St. Louis', 'The Pennsylvania State University', 'University of Mississippi', 'The Pennsylvania State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29904,Fairness & Bias,Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification,"Few-shot named entity recognition (NER) aims to recognize novel named entities in low-resource domains utilizing existing knowledge. However, the present few-shot NER models assume that the labeled data are all clean without noise or outliers, and there are few works focusing on the robustness of the cross-domain transfer learning ability to textual adversarial attacks in Few-shot NER. In this work, we comprehensively explore and assess the robustness of few-shot NER models under textual adversarial attack scenario, and found the vulnerability of existing few-shot NER models. Furthermore, we propose a robust two-stage few-shot NER method with Boundary Discrimination and Correlation Purification (BDCP). Specifically, in the span detection stage, the entity boundary discriminative module is introduced to provide a highly distinguishing boundary representation space to detect entity spans. In the entity typing stage, the correlations between entities and contexts are purified by minimizing the interference information and facilitating correlation generalization to alleviate the perturbations caused by textual adversarial attacks. In addition, we construct adversarial examples for few-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive evaluations on those two groups of few-shot NER datasets containing adversarial examples demonstrate the robustness and superiority of the proposed method.","['NLP: Information Extraction', 'NLP: Safety and Robustness']",[],"['Xiaojun Xue', 'Chunxia Zhang', 'Tianxiang Xu', 'Zhendong Niu']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29905,Transparency & Explainability,Tackling Vision Language Tasks through Learning Inner Monologues,"Visual language tasks such as Visual Question Answering (VQA) or Visual Entailment (VE) require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability.  To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating Inner Monologue, a cognitive process in which an individual engages in silent verbal communication with themselves. More specifically, we enable LLMs and VLMs to interact through natural language conversation (i.e., Inner Monologue) and propose to use a two-stage training process to learn how to do Inner Monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and achieves competitive performance with less training data when compared with state-of-the-art models while concurrently keeping the interpretability. The results suggest that by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, broadening its potential applications across various AI challenges beyond vision and language tasks.","['NLP: Language Grounding & Multi-modal NLP', 'CV: Language and Vision', 'ML: Multimodal Learning']",[],"['Diji Yang', 'Kezhen Chen', 'Jinmeng Rao', 'Xiaoyuan Guo', 'Yawen Zhang', 'Jie Yang', 'Yi Zhang']","['University of California, Santa Cruz', 'Mineral.ai', 'Mineral.ai', 'Mineral.ai', 'Mineral.ai', 'Mineral.ai', 'University of California, Santa Cruz']","['', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/29907,Security,Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-World Multi-Turn Dialogue,"Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot align responses with experts' intentions. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from continuous pre-training, SFT, to Reinforcement Learning from Human Feedback (RLHF). Additionally, we construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We also define a refined annotation rule and evaluation criteria given the unique characteristics of the biomedical domain. Extensive experimental results show that Zhongjing outperforms baselines in various capacities and matches the performance of ChatGPT in some abilities, despite the 100x parameters. Ablation studies also demonstrate the contributions of each component: pre-training enhances medical knowledge, and RLHF further improves instruction-following ability and safety. Our code, datasets, and models are available at https://github.com/SupritYoung/Zhongjing.","['NLP: (Large) Language Models', 'NLP: Applications', 'NLP: Conversational AI/Dialog Systems']",[],"['Songhua Yang', 'Hanjie Zhao', 'Senbin Zhu', 'Guangyu Zhou', 'Hongfei Xu', 'Yuxiang Jia', 'Hongying Zan']","['Zhengzhou University', 'Zhengzhou University', 'Zhengzhou University', 'Zhengzhou University', 'Zhengzhou University', 'Zhengzhou University', 'Zhengzhou University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29909,Transparency & Explainability,Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following,"In this paper, we present our finding that prepending a Task-Agnostic Prefix Prompt (TAPP) to the input improves the instruction-following ability of various Large Language Models (LLMs) during inference. TAPP is different from canonical prompts for LLMs in that it is a fixed prompt prepended to the beginning of every input regardless of the target task for zero-shot generalization. We observe that both base LLMs (i.e. not fine-tuned to follow instructions) and instruction-tuned models benefit from TAPP, resulting in 34.58% and 12.26% improvement on average, respectively. This implies that the instruction-following ability of LLMs can be improved during inference time with a fixed prompt constructed with simple heuristics. We hypothesize that TAPP assists language models to better estimate the output distribution by focusing more on the instruction of the target task during inference. In other words, such ability does not seem to be sufficiently activated in not only base LLMs but also many instruction-fine-tuned LLMs.","['NLP: (Large) Language Models', 'NLP: Applications', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Seonghyeon Ye', 'Hyeonbin Hwang', 'Sohee Yang', 'Hyeongu Yun', 'Yireun Kim', 'Minjoon Seo']","['KAIST', 'KAIST', 'UCL, KAIST', 'LG AI Research', 'LG AI Research', 'KAIST']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29914,Transparency & Explainability,CK12: A Rounded K12 Knowledge Graph Based Benchmark for Chinese Holistic Cognition Evaluation,"New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present a meticulously designed evaluation benchmark that leverages the knowledge graph. This evaluation comprises 584 level-1 knowledge points and 1,989 level-2 knowledge points, thereby encompassing a comprehensive spectrum of the K12 education domain knowledge. The primary objective is to comprehensively assess the high-level comprehension aptitude and reasoning capabilities of LLMs operating within the Chinese context. Our evaluation incorporates five distinct question types with 39,452 questions. We test the current mainstream LLMs by three distinct modes. Firstly, four prompt evaluation modes were employed to assess the fundamental capacity. Additionally, for choice questions, a result-oriented evaluation approach was designed through data augmentation to assess the model's proficiency in advanced knowledge and reasoning. Moreover, a subset with reasoning process is derived, and the process-oriented testing method is used to test the model's interpretability and higher-order reasoning capacity. We further show models' capability in our knowledge points, and anticipate the evaluation can assist in the assessment of the strengths and deficiencies of LLMs on knowledge points, thus fostering their development within the Chinese context. Our Dataset will be publicly available in https://github.com/tal-tech/chinese-k12-evaluation.",['NLP: (Large) Language Models'],[],"['Weihao You', 'Pengcheng Wang', 'Changlong Li', 'Zhilong Ji', 'Jinfeng Bai']","['Tomorrow Advancing Life', 'Tomorrow Advancing Life', 'Tomorrow Advancing Life', 'Tomorrow Advancing Life', 'Tomorrow Advancing Life']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29912,Transparency & Explainability,History Matters: Temporal Knowledge Editing in Large Language Model,"The imperative task of revising or updating the knowledge stored within large language models arises from two distinct sources: intrinsic errors inherent in the model which should be corrected and outdated knowledge due to external shifts in the real world which should be updated. Prevailing efforts in model editing conflate these two distinct categories of edits arising from distinct reasons and directly modify the original knowledge in models into new knowledge. However, we argue that preserving the model's original knowledge remains pertinent. Specifically, if a model's knowledge becomes outdated due to evolving worldly dynamics, it should retain recollection of the historical knowledge while integrating the newfound knowledge. In this work, we introduce the task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe (Assessment of TempOral Knowledge Editing) to evaluate current model editing methods. We find that while existing model editing methods are effective at making models remember new knowledge, the edited model catastrophically forgets historical knowledge. To address this gap, we propose a simple and general framework termed Multi-Editing with Time Objective (METO) for enhancing existing editing models, which edits both historical and new knowledge concurrently and optimizes the model's prediction for the time of each fact. Our assessments demonstrate that while AToKe is still difficult, METO maintains the effectiveness of learning new knowledge and meanwhile substantially improves the performance of edited models on utilizing historical knowledge.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'KRR: Applications', 'General']",[],"['Xunjian Yin', 'Jin Jiang', 'Liming Yang', 'Xiaojun Wan']","['Peking University', 'Peking University', 'Tsinghua University', 'Peking University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29916,Security,MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA,"Large language models (LLMs) have shown great success in various Natural Language Processing (NLP) tasks, whist they still need updates after deployment to fix errors or keep pace with the changing knowledge in the world. Researchers formulate such problem as Model Editing and have developed various editors focusing on different axes of editing properties. However, current editors can hardly support all properties and rely on heavy computational resources. In this paper, we propose a plug-in Model Editing method based on neuron-indexed dynamic LoRA (MELO), which alters the behavior of language models by dynamically activating certain LoRA blocks according to the index built in an inner vector database. Our method satisfies various editing properties with high efficiency and can be easily integrated into multiple LLM backbones. Experimental results show that our proposed MELO achieves state-of-the-art editing performance on three sequential editing tasks (document classification, question answering and hallucination correction), while requires the least trainable parameters and computational cost.","['NLP: Learning & Optimization for NLP', 'NLP: (Large) Language Models', 'NLP: Safety and Robustness']",[],"['Lang Yu', 'Qin Chen', 'Jie Zhou', 'Liang He']","['School of Computer Science and Technology, East China Normal University\nShanghai Institute of AI for Education, East China Normal University', 'School of Computer Science and Technology, East China Normal University\nShanghai Institute of AI for Education, East China Normal University', 'School of Computer Science and Technology, East China Normal University\nShanghai Institute of AI for Education, East China Normal University', 'School of Computer Science and Technology, East China Normal University\nShanghai Institute of AI for Education, East China Normal University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29918,Transparency & Explainability,TaskLAMA: Probing the Complex Task Understanding of Language Models,"Structured  Complex  Task  Decomposition  (SCTD)  is  the problem of breaking down a complex real-world task (such as planning a wedding) into a directed acyclic graph over individual steps that contribute to achieving the task, with edges specifying temporal dependencies between steps. SCTD is an important component of assistive planning tools, and a challenge  for  commonsense  reasoning  systems.  We  probe  how accurately SCTD can be done with the knowledge extracted from  pre-trained  Large  Language  Models  (LLMs).  We  introduce a new high-quality human-annotated dataset for this problem and novel metrics to fairly assess  performance of LLMs against several baselines. Our experiments reveal that LLMs  are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15% to 280% over the best baseline. We also propose a number of approaches to further improve their performance, with a relative improvement of 7% to 37%. However, we find that LLMs still struggle to predict pairwise temporal dependencies, which reveals a gap in their understanding of complex tasks.","['NLP: (Large) Language Models', 'APP: Other Applications', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'PRS: Temporal Planning']",[],"['Quan Yuan', 'Mehran Kazemi', 'Xin Xu', 'Isaac Noble', 'Vaiva Imbrasaite', 'Deepak Ramachandran']","['Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29925,Fairness & Bias,Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment,"Multi-hop fact verification aims to detect the veracity of the given claim by integrating and reasoning over multiple pieces of evidence. Conventional multi-hop fact verification models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets. Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or counterfactual reasoning. However, existing causal inference-based debiasing methods, which mainly formulate fact verification as a single-hop reasoning task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence. To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop fact verification from a causal perspective with front-door adjustment. Specifically, in the structural causal model, the reasoning path between the treatment (the input claim-evidence graph) and the outcome (the veracity label) is introduced as the mediator to block the confounder. With the front-door adjustment, the causal effect between the treatment and the outcome is decomposed into the causal effect between the treatment and the mediator, which is estimated by applying the idea of random walk, and the causal effect between the mediator and the outcome, which is estimated with normalized weighted geometric mean approximation. To investigate the effectiveness of the proposed method, an adversarial multi-hop fact verification dataset and a symmetric multi-hop fact verification dataset are proposed with the help of the large language model. Experimental results show that Causal Walk outperforms some previous debiasing methods on both existing datasets and the newly constructed datasets. Code and data will be released at https://github.com/zcccccz/CausalWalk.","['NLP: Applications', 'ML: Causal Learning']",[],"['Congzhi Zhang', 'Linhai Zhang', 'Deyu Zhou']","['School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China', 'School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China', 'School of Computer Science and Engineering, Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29921,Transparency & Explainability,InterpretARA: Enhancing Hybrid Automatic Readability Assessment with Linguistic Feature Interpreter and Contrastive Learning,"The hybrid automatic readability assessment (ARA) models that combine deep and linguistic features have recently received rising attention due to their impressive performance. However, the utilization of linguistic features is not fully realized, as ARA models frequently concentrate excessively on numerical values of these features, neglecting valuable structural information embedded within them. This leads to limited contribution of linguistic features in these hybrid ARA models, and in some cases, it may even result in counterproductive outcomes. In this paper, we propose a novel hybrid ARA model named InterpretARA through introducing a linguistic interpreter to better comprehend the structural information contained in linguistic features, and leveraging the contrastive learning that enables the model to understand relative difficulty relationships among texts and thus enhances deep representations. Both document-level and segment-level deep representations are extracted and used for the readability assessment. A series of experiments are conducted over four English corpora and one Chinese corpus to demonstrate the effectiveness of the proposed model. Experimental results show that InterpretARA outperforms state-of-the-art models in most corpora, and the introduced linguistic interpreter can provide more useful information than existing ways for ARA.","['NLP: Applications', 'ML: Classification and Regression', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: Machine Translation', 'Multilinguality', 'Cross-Lingual NLP', 'NLP: Sentiment Analysis', 'Stylistic Analysis', 'and Argument Mining', 'NLP: Text Classification']",[],"['Jinshan Zeng', 'Xianchao Tong', 'Xianglong Yu', 'Wenyan Xiao', 'Qing Huang']","['Jiangxi Normal University', 'Jiangxi Normal University', 'Jiangxi Normal University', 'Jiangxi University of Science and Technology', 'Jiangxi Normal University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29927,Fairness & Bias,Quantum Interference Model for Semantic Biases of Glosses in Word Sense Disambiguation,"Word Sense Disambiguation (WSD) aims to determine the meaning of the target word according to the given context. Currently, a single representation enhanced by glosses from different dictionaries or languages is used to characterize each word sense. By analyzing the similarity between glosses of the same word sense, we find semantic biases among them, revealing that the glosses have their own descriptive perspectives. Therefore, the traditional approach of integrating all glosses by a single representation results in failing to present the unique semantics revealed by the individual glosses. In this paper, a quantum superposition state is employed to formalize the representations of multiple glosses of the same word sense to reveal their distributions. Furthermore, the quantum interference model is leveraged to calculate the probability that the target word belongs to this superposition state. The advantage is that the interference term can be regarded as a confidence level to guide word sense recognition. Finally, experiments are performed under standard WSD evaluation framework and the latest cross-lingual datasets, and the results verify the effectiveness of our model.",['NLP: Lexical Semantics and Morphology'],[],"['Junwei Zhang', 'Ruifang He', 'Fengyu Guo', 'Chang Liu']","['Center for Artificial Intelligence and Intelligent Medicine, Hangzhou Institute of Medicine, Chinese Academy of Sciences, Zhejiang Province, China.', 'Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin, China.', 'College of Computer and Information Engineering, Tianjin Normal University, Tianjin, China.', 'CSSC Systems Engineering Research Institute, Beijing, China.']","['', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29933,Security,Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains,"Accurately typing entity mentions from text segments is a fundamental task for various natural language processing applications. Many previous approaches rely on massive human-annotated data to perform entity typing. Nevertheless, collecting such data in highly specialized science and engineering domains (e.g., software engineering and security) can be time-consuming and costly, without mentioning the domain gaps between training and inference data if the model needs to be applied to confidential datasets. In this paper, we study the task of seed-guided fine-grained entity typing in science and engineering domains, which takes the name and a few seed entities for each entity type as the only supervision and aims to classify new entity mentions into both seen and unseen types (i.e., those without seed entities). To solve this problem, we propose SEType which first enriches the weak supervision by finding more entities for each seen type from an unlabeled corpus using the contextualized representations of pre-trained language models. It then matches the enriched entities to unlabeled text to get pseudo-labeled samples and trains a textual entailment model that can make inferences for both seen and unseen types. Extensive experiments on two datasets covering four domains demonstrate the effectiveness of SEType in comparison with various baselines. Code and data are available at: https://github.com/yuzhimanhua/SEType.","['NLP: Information Extraction', 'APP: Security', 'APP: Software Engineering']",[],"['Yu Zhang', 'Yunyi Zhang', 'Yanzhen Shen', 'Yu Deng', 'Lucian Popa', 'Larisa  Shwartz', 'ChengXiang  Zhai', 'Jiawei Han']","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'IBM Thomas J. Watson Research Center', 'IBM Almaden Research Center', 'IBM Thomas J. Watson Research Center', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29934,Transparency & Explainability,LLMEval: A Preliminary Study on How to Evaluate Large Language Models,"Recently, the evaluation of Large Language Models has emerged as a popular area of research.  The three crucial questions for LLM evaluation are ``what, where, and how to evaluate''. However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with. As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion. In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems.  We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs.  A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results. We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future. The dataset and the results are publicly available at  https://github.com/llmeval. The version with the appendix are publicly available at https://arxiv.org/abs/2312.07398.","['NLP: (Large) Language Models', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Safety and Robustness']",[],"['Yue Zhang', 'Ming Zhang', 'Haipeng Yuan', 'Shichun Liu', 'Yongyao Shi', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']","['School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'Shanghai Advanced Institute of Finance, Shanghai Jiaotong University, Shanghai, China', 'Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29938,Fairness & Bias,Graph Reasoning Transformers for Knowledge-Aware Question Answering,"Augmenting Language Models (LMs) with structured knowledge graphs (KGs) aims to leverage structured world knowledge to enhance the capability of LMs to complete knowledge-intensive tasks. However, existing methods are unable to effectively utilize the structured knowledge in a KG due to their inability to capture the rich relational semantics of knowledge triplets. Moreover, the modality gap between natural language text and KGs has become a challenging obstacle when aligning and fusing cross-modal information. To address these challenges, we propose a novel knowledge-augmented question answering (QA) model, namely, Graph Reasoning Transformers (GRT). Different from conventional node-level methods, the GRT serves knowledge triplets as atomic knowledge and utilize a triplet-level graph encoder to capture triplet-level graph features. Furthermore, to alleviate the negative effect of the modality gap on joint reasoning, we propose a representation alignment  pretraining to align the cross-modal representations and introduce a cross-modal information fusion module with attention bias to enable fine-grained information fusion. Extensive experiments conducted on three knowledge-intensive QA benchmarks show that the GRT outperforms the state-of-the-art KG-augmented QA systems, demonstrating the effectiveness and adaptation of our proposed model.","['NLP: Question Answering', 'NLP: Applications']",[],"['Ruilin Zhao', 'Feng Zhao', 'Liang Hu', 'Guandong Xu']","['Natural Language Processing and Knowledge Graph Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China\nData Science and Machine Intelligence Lab, University of Technology Sydney, Sydney, Australia', 'Natural Language Processing and Knowledge Graph Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China', 'College of Electronic and Information Engineering, Tongji University, Shanghai, China', 'Data Science and Machine Intelligence Lab, University of Technology Sydney, Sydney, Australia']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29939,Fairness & Bias,MultiSum: A Multi-Facet Approach for Extractive Social Summarization Utilizing Semantic and Sociological Relationships,"Social summarization aims to provide summaries for a large number of social texts (called posts)  about a single topic. To extract a summary, both  the representation of post and summary selection method are crucial. Previous methods introduce social relation to enhance post embedding to mitigate the sparse representation due to its brief and informal expression. However, they ignore that there are multiple relations between posts. Besides, existing graph-based centrality calculation approaches tend to select posts from one aspect. This leads to facet bias especially when there are multiple viewpoints. In this paper, we propose a model named MultiSum to improve social summarization. Specifically, 1) We use graph convolutional networks to fuse text content with social and semantic relations to improve post representation; 2) The similarity between the summary and all aspects is incorporated into the centrality score during the selection phase, encouraging the model to pay attention to different facets. Experimental results on English and Chinese corpora support the effectiveness of this model. Furthermore, external evaluations by human experts and large language models demonstrate the validity of MultiSum in facet coverage and redundancy reduction.","['NLP: Applications', 'NLP: Summarization']",[],"['Tanglong Zhao', 'Ruifang He', 'Jing Xu', 'Bo Wang']","['College of Intelligence and Computing, Tianjin University, Tianjin, China\nTianjin Key Laboratory of Cognitive Computing and Application, Tianjin, China', 'College of Intelligence and Computing, Tianjin University, Tianjin, China\nTianjin Key Laboratory of Cognitive Computing and Application, Tianjin, China', 'College of Intelligence and Computing, Tianjin University, Tianjin, China\nTianjin Key Laboratory of Cognitive Computing and Application, Tianjin, China', 'College of Intelligence and Computing, Tianjin University, Tianjin, China\nTianjin Key Laboratory of Cognitive Computing and Application, Tianjin, China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29941,Transparency & Explainability,SENCR: A Span Enhanced Two-Stage Network with Counterfactual Rethinking for Chinese NER,"Recently, lots of works that incorporate external lexicon information into character-level Chinese named entity recognition(NER) to overcome the lackness of natural delimiters of words, have achieved many advanced performance. However, obtaining and maintaining high-quality lexicons is costly, especially in special domains. In addition, the entity boundary bias caused by high mention coverage in some boundary characters poses a significant challenge to the generalization of NER models but receives little attention in the existing literature. To address these issues, we propose SENCR, a Span Enhanced Two-stage Network with Counterfactual Rethinking for Chinese NER, that contains a boundary detector for boundary supervision, a convolution-based type classifier for better span representation and a counterfactual rethinking(CR) strategy for debiased boundary detection in inference. The proposed boundary detector and type classifier are jointly trained with the same contextual encoder and then the trained boundary detector is debiased by our proposed CR strategy without modifying any model parameters in the inference stage. Extensive experiments on four Chinese NER datasets show the effectiveness of our proposed approach.","['NLP: Information Extraction', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy']",[],"['Hang Zheng', 'Qingsong Li', 'Shen Chen', 'Yuxuan Liang', 'Li Liu']","['School of Big Data and Software Engineering, Chongqing University, China', 'School of Big Data and Software Engineering, Chongqing University, China', 'School of Big Data and Software Engineering, Chongqing University, China', 'The Hong Kong University of Science and Technology (Guangzhou), China', 'School of Big Data and Software Engineering, Chongqing University, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29948,Transparency & Explainability,Quantifying and Analyzing Entity-Level Memorization in Large Language Models,"Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. The results demonstrate that LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: (Large) Language Models']",[],"['Zhenhong Zhou', 'Jiuyang Xiang', 'Chaomeng Chen', 'Sen Su']","['Beijing University of Posts and Telecommunications', 'University of Michigan', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29949,Security,MathAttack: Attacking Large Language Models towards Math Solving Ability,"With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the robustness of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of robustness in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. The code and dataset is available at: https://github.com/zhouzihao501/MathAttack.","['NLP: (Large) Language Models', 'NLP: Applications']",[],"['Zihao Zhou', 'Qiufeng Wang', 'Mingyu Jin', 'Jie Yao', 'Jianan Ye', 'Wei Liu', 'Wei Wang', 'Xiaowei Huang', 'Kaizhu Huang']","[""Xi'an Jiaotong-Liverpool University"", ""Xi'an Jiaotong-Liverpool University"", 'Northwestern University', ""Xi'an Jiaotong-Liverpool University"", ""Xi'an Jiaotong-Liverpool University"", 'ShanghaiTech University', ""Xi'an Jiaotong-Liverpool University"", 'University of Liverpool', 'Duke Kunshan University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29950,Security,LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack,"Natural language processing models are vulnerable to adversarial examples. Previous textual adversarial attacks adopt model internal information (gradients or  confidence scores) to generate adversarial examples. However, this information is unavailable in the real world. Therefore, we focus on a more realistic and challenging setting, named hard-label attack, in which the attacker can only query the model and obtain a discrete prediction label. Existing hard-label attack algorithms tend to initialize adversarial examples by random substitution and then utilize complex heuristic algorithms to optimize the adversarial perturbation. These methods require a lot of  model queries and the attack success rate is restricted by adversary initialization. In this paper,  we propose a novel hard-label attack algorithm named LimeAttack, which leverages a local explainable method to approximate word importance ranking, and then adopts beam search to find the optimal solution. Extensive experiments show that LimeAttack achieves the better attacking performance compared with existing hard-label attack under the same query budget. In addition, we evaluate the effectiveness of LimeAttack on large language models and some defense methods, and results indicate that adversarial examples remain a significant threat to  large language models. The adversarial examples crafted by LimeAttack  are highly transferable and  effectively improve model robustness in adversarial training.","['NLP: Text Classification', 'NLP: Safety and Robustness']",[],"['Hai Zhu', 'Qingyang Zhao', 'Weiwei Shang', 'Yuren Wu', 'Kai Liu']","['University of Science and Technology of China\nPing An Technology', 'Xidian University', 'University of Science and Technology of China', 'Ping An Technology', 'Lazada']","['China', 'China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29953,Transparency & Explainability,Towards Explainable Joint Models via Information Theory for Multiple Intent Detection and Slot Filling,"Recent joint models for multi-intent detection and slot filling have obtained promising results through modeling the unidirectional or bidirectional guidance between intent and slot. However, existing works design joint models heuristically and lack some theoretical exploration, including (1) theoretical measurement of the joint-interaction quality; (2) explainability of design and optimization methods of joint models, which may limit the performance and efficiency of designs. In this paper, we mathematically define the cross-task information gain (CIG) to measure the quality of joint processes from an information-theoretic perspective and discover an implicit optimization of CIG in previous models. Based on this, we propose a novel multi-stage iterative framework with theoretical effectiveness, explainability, and convergence, which can explicitly optimize information for cross-task interactions. Further, we devise an information-based joint model (InfoJoint) that conforms to this theoretical framework to gradually reduce the cross-task propagation of erroneous semantics through CIG iterative maximization. Extensive experiment results on two public datasets show that InfoJoint outperforms the state-of-the-art models by a large margin.","['NLP: Conversational AI/Dialog Systems', 'NLP: Text Classification']",[],"['Xianwei Zhuang', 'Xuxin Cheng', 'Yuexian Zou']","['School of ECE, Peking University, China', 'School of ECE, Peking University, China', 'School of ECE, Peking University, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29964,Privacy & Data Governance,MERGE: Fast Private Text Generation,"The drastic increase in language models' parameters has led to a new trend of deploying models in cloud servers, raising growing concerns about private inference for Transformer-based models. Existing two-party privacy-preserving techniques, however, only take into account natural language understanding (NLU) scenarios. Private inference in natural language generation (NLG), crucial for applications like translation and code completion, remains underexplored. In addition, previous privacy-preserving techniques suffer from convergence issues during model training and exhibit poor inference speed when used with NLG models due to the neglect of time-consuming operations in auto-regressive generations. To address these issues, we propose a fast private text generation framework for Transformer-based language models, namely MERGE. MERGE reuses the output hidden state as the word embedding to bypass the embedding computation and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Extensive experiments show that MERGE achieves a 26.5x speedup to the vanilla encrypted model under the sequence length 512, and reduces 80% communication cost, with an up to 10x speedup to state-of-the-art approximated models.","['PEAI: Privacy & Security', 'NLP: (Large) Language Models', 'NLP: Generation']",[],"['Zi Liang', 'Pinghui Wang', 'Ruofei Zhang', 'Nuo Xu', 'Shuo Zhang', 'Lifeng Xing', 'Haitao Bai', 'Ziyang Zhou']","[""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University""]","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29962,Security,Reward Penalties on Augmented States for Solving Richly Constrained RL Effectively,"Constrained Reinforcement Learning employs trajectory-based cost constraints (such as expected cost, Value at Risk, or Conditional VaR cost) to compute safe policies. The challenge lies in handling these constraints effectively while optimizing expected reward. Existing methods convert such trajectory-based constraints into local cost constraints, but they rely on cost estimates, leading to either aggressive or conservative solutions with regards to cost. We propose an unconstrained formulation that employs reward penalties over states augmented with costs to compute safe policies. Unlike standard primal-dual methods, our approach penalizes only infeasible trajectories through state augmentation. This ensures that increasing the penalty parameter always guarantees a feasible policy, a feature lacking in primal-dual methods. Our approach exhibits strong empirical performance and theoretical properties, offering a fresh paradigm for solving complex Constrained RL problems, including rich constraints like expected cost, Value at Risk, and Conditional Value at Risk. Our experimental results demonstrate superior performance compared to leading approaches across various constraint types on multiple benchmark problems.","['PEAI: Safety', 'Robustness & Trustworthiness', 'CSO: Constraint Optimization', 'ML: Reinforcement Learning', 'PRS: Planning with Markov Models (MDPs', 'POMDPs)', 'RU: Stochastic Optimization']",[],"['Hao Jiang', 'Tien Mai', 'Pradeep Varakantham', 'Huy Hoang']","['Singapore Management University', 'Singapore Management University', 'Singapore Management University', 'Singapore Management University']","['Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29955,Fairness & Bias,Quality-Diversity Generative Sampling for Learning with Synthetic Data,"Generative models can serve as surrogates for some real data sources by creating synthetic training datasets, but in doing so they may transfer biases to downstream tasks. We focus on protecting quality and diversity when generating synthetic training datasets. We propose quality-diversity generative sampling (QDGS), a framework for sampling data uniformly across a user-defined measure space, despite the data coming from a biased generator. QDGS is a model-agnostic framework that uses prompt guidance to optimize a quality objective across measures of diversity for synthetically generated data, without fine-tuning the generative model. Using balanced synthetic datasets generated by QDGS, we first debias classifiers trained on color-biased shape datasets as a proof-of-concept. By applying QDGS to facial data synthesis, we prompt for desired semantic concepts, such as skin tone and age, to create an intersectional dataset with a combined blend of visual features. Leveraging this balanced data for training classifiers improves fairness while maintaining accuracy on facial recognition benchmarks. Code available at: https://github.com/Cylumn/qd-generative-sampling.","['PEAI: Bias', 'Fairness & Equity', 'CV: Bias', 'Fairness & Privacy']",[],"['Allen Chang', 'Matthew C. Fontaine', 'Serena Booth', 'Maja J. Matarić', 'Stefanos Nikolaidis']","['University of Southern California', 'University of Southern California', 'Massachusetts Institute of Technology', 'University of Southern California', 'University of Southern California']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29960,Security,How to Overcome Curse-of-Dimensionality for Out-of-Distribution Detection?,"Machine learning models deployed in the wild can be challenged by out-of-distribution (OOD) data from unknown classes. Recent advances in OOD detection rely on distance measures to distinguish samples that are relatively far away from the in-distribution (ID) data. Despite the promise, distance-based methods can suffer from the curse-of-dimensionality problem, which limits the efficacy in high dimensional feature space. To combat this problem, we propose a novel framework, Subspace Nearest Neighbor (SNN), for OOD detection. In training, our method regularizes the model and its feature representation by leveraging the most relevant subset of dimensions (i.e. subspace). The subspace learning yields highly distinguishable distance measures between ID and OOD data. We provide comprehensive experiments and ablations to validate the efficacy of SNN. Compared to the current best distance-based method, SNN reduces the average FPR95 by 15.96% on the CIFAR-100 benchmark.","['PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Soumya Suvra Ghosal', 'Yiyou Sun', 'Yixuan Li']","['University of Wisconsin, Madison', 'University of Wisconsin, Madison', 'University of Wisconsin, Madison']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29957,Security,Conditional Backdoor Attack via JPEG Compression,"Deep neural network (DNN) models have been proven vulnerable to backdoor attacks. One trend of backdoor attacks is developing more invisible and dynamic triggers to make attacks stealthier. However, these invisible and dynamic triggers can be inadvertently mitigated by some widely used passive denoising operations, such as image compression, making the efforts under this trend questionable. Another trend is to exploit the full potential of backdoor attacks by proposing new triggering paradigms, such as hibernated or opportunistic backdoors. In line with these trends, our work investigates the first conditional backdoor attack, where the backdoor is activated by a specific condition rather than pre-defined triggers. Specifically, we take the JPEG compression as our condition and jointly optimize the compression operator and the target model's loss function, which can force the target model to accurately learn the JPEG compression behavior as the triggering condition. In this case, besides the conditional triggering feature, our attack is also stealthy and robust to denoising operations. Extensive experiments on the MNIST, GTSRB and CelebA verify our attack's effectiveness, stealthiness and resistance to existing backdoor defenses and denoising operations. As a new triggering paradigm, the conditional backdoor attack brings a new angle for assessing the vulnerability of DNN models, and conditioned over JPEG compression magnifies its threat due to the universal usage of JPEG.","['PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Qiuyu Duan', 'Zhongyun Hua', 'Qing Liao', 'Yushu Zhang', 'Leo Yu Zhang']","['Harbin Institute of Technology, Shenzhen', 'Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies', 'Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies', 'Nanjing University of Aeronautics and Astronautics', 'Griffith University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29958,Privacy & Data Governance,Complementary Knowledge Distillation for Robust and Privacy-Preserving Model Serving in Vertical Federated Learning,"Vertical Federated Learning (VFL) enables an active party with labeled data to enhance model performance (utility) by collaborating with multiple passive parties that possess auxiliary features corresponding to the same sample identifiers (IDs). Model serving in VFL is vital for real-world, delay-sensitive applications, and it faces two major challenges: 1) robustness against arbitrarily-aligned data and stragglers; and 2) privacy protection, ensuring minimal label leakage to passive parties. Existing methods fail to transfer knowledge among parties to improve robustness in a privacy-preserving way. In this paper, we introduce a privacy-preserving knowledge transfer framework, Complementary Knowledge Distillation (CKD), designed to enhance the robustness and privacy of multi-party VFL systems. Specifically, we formulate a Complementary Label Coding (CLC) objective to encode only complementary label information of the active party's local model for passive parties to learn. Then, CKD selectively transfers the CLC-encoded complementary knowledge 1) from the passive parties to the active party, and 2) among the passive parties themselves. Experimental results on four real-world datasets demonstrate that CKD outperforms existing approaches in terms of robustness against arbitrarily-aligned data, while also minimizing label privacy leakage.","['PEAI: Safety', 'Robustness & Trustworthiness', 'ML: Privacy', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'PEAI: Privacy & Security']",[],"['Dashan Gao', 'Sheng Wan', 'Lixin Fan', 'Xin Yao', 'Qiang Yang']","['Southern University of Science and Technology, Shenzhen, China\nHong Kong University of Science and Technology, Hong Kong SAR, China', 'Southern University of Science and Technology, Shenzhen, China\nHong Kong University of Science and Technology, Hong Kong SAR, China', 'WeBank AI Lab, Shenzhen, China', 'Southern University of Science and Technology, Shenzhen, China', 'Hong Kong University of Science and Technology, Hong Kong SAR, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29961,Security,Exploiting Discrepancy in Feature Statistic for Out-of-Distribution Detection,"Recent studies on out-of-distribution (OOD) detection focus on designing models or scoring functions that can effectively distinguish between unseen OOD data and in-distribution (ID) data. In this paper, we propose a simple yet novel ap- proach to OOD detection by leveraging the phenomenon that the average of feature vector elements from convolutional neural network (CNN) is typically larger for ID data than for OOD data. Specifically, the average of feature vector elements is used as part of the scoring function to further separate OOD data from ID data. We also provide mathematical analysis to explain this phenomenon. Experimental evaluations demonstrate that, when combined with a strong baseline, our method can achieve state-of-the-art performance on several OOD detection benchmarks. Furthermore, our method can be easily integrated into various CNN architectures and requires less computation. Source code address: https://github.com/SYSU-MIA-GROUP/statistical_discrepancy_ood.","['PEAI: Safety', 'Robustness & Trustworthiness', 'CV: Object Detection & Categorization', 'CV: Other Foundations of Computer Vision']",[],"['Xiaoyuan Guan', 'Jiankang Chen', 'Shenshen Bu', 'Yuren Zhou', 'Wei-Shi Zheng', 'Ruixuan Wang']","['School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\nKey Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\nKey Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\nKey Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\nKey Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29965,Fairness & Bias,Does Few-Shot Learning Suffer from Backdoor Attacks?,"The field of few-shot learning (FSL) has shown promising results in scenarios where training data is limited, but its vulnerability to backdoor attacks remains largely unexplored. We first explore this topic by first evaluating the performance of the existing backdoor attack methods on few-shot learning scenarios. Unlike in standard supervised learning, existing backdoor attack methods failed to perform an effective attack in FSL due to two main issues. Firstly, the model tends to overfit to either benign features or trigger features, causing a tough trade-off between attack success rate and benign accuracy. Secondly, due to the small number of training samples, the dirty label or visible trigger in the support set can be easily detected by victims, which reduces the stealthiness of attacks. It seemed that FSL could survive from backdoor attacks.  However, in this paper, we propose the Few-shot Learning Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor attacks. Specifically, we first generate a trigger to maximize the gap between poisoned and benign features. It enables the model to learn both benign and trigger features, which solves the problem of overfitting. To make it more stealthy, we hide the trigger by optimizing two types of imperceptible perturbation, namely attractive and repulsive perturbation, instead of attaching the trigger directly. Once we obtain the perturbations, we can poison all samples in the benign support set into a hidden poisoned support set and fine-tune the model on it. Our method demonstrates a high Attack Success Rate (ASR) in FSL tasks with different few-shot learning paradigms while preserving clean accuracy and maintaining stealthiness. This study reveals that few-shot learning still suffers from backdoor attacks, and its security should be given attention.","['PEAI: Privacy & Security', 'CV: Bias', 'Fairness & Privacy', 'ML: Privacy']",[],"['Xinwei Liu', 'Xiaojun Jia', 'Jindong Gu', 'Yuan Xun', 'Siyuan Liang', 'Xiaochun Cao']","['SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences\nSchool of Cyber Security, University of Chinese Academy of Sciences', 'Nanyang Technological University, Singapore', 'University of Oxford, UK', 'SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences\nSchool of Cyber Security, University of Chinese Academy of Sciences', 'School of Computing, National University of Singapore, Singapore', 'School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen, China']","['China', 'Singapore', 'United Kingdom', 'China', 'Singapore', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29969,Fairness & Bias,Towards Fairness in Online Service with K Servers and Its Application on Fair Food Delivery,"The k-SERVER problem is one of the most prominent problems in online algorithms with several variants and extensions. However, simplifying assumptions like instantaneous server movements and zero service time has hitherto limited its applicability to real-world problems. In this paper, we introduce a realistic generalization of k-SERVER without such assumptions – the k-FOOD problem, where requests with source-destination locations and an associated pickup time window arrive in an online fashion, and each has to be served by exactly one of the available k servers. The k-FOOD problem offers the versatility to model a variety of real-world use cases such as food delivery, ride sharing, and quick commerce. Moreover, motivated by the need for fairness in online platforms, we introduce the FAIR k-FOOD problem with the max-min objective. We establish that both k-FOOD and FAIR k-FOOD problems are strongly NP-hard and develop an optimal offline algorithm that arises naturally from a time-expanded flow network. Subsequently, we propose an online algorithm DOC4FOOD involving virtual movements of servers to the nearest request location. Experiments on a real-world food-delivery dataset, alongside synthetic datasets, establish the efficacy of the proposed algorithm against state-of-the-art fair food delivery algorithms.","['PEAI: Bias', 'Fairness & Equity', 'APP: Other Applications', 'PEAI: AI & Jobs/Labor']",[],"['Daman Deep Singh', 'Amit Kumar', 'Abhijnan Chakraborty']","['Indian Institute of Technology Delhi, India', 'Indian Institute of Technology Delhi, India', 'Indian Institute of Technology Delhi, India']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/29968,Transparency & Explainability,Responsibility in Extensive Form Games,"Two different forms of responsibility, counterfactual and seeing-to-it, have been extensively discussed in philosophy and AI in the context of a single agent or multiple agents acting simultaneously. Although the generalisation of counterfactual responsibility to a setting where multiple agents act in some order is relatively straightforward, the same cannot be said about seeing-to-it responsibility. Two versions of seeing-to-it modality applicable to such settings have been proposed in the literature. Neither of them perfectly captures the intuition of responsibility. The paper proposes a definition of seeing-to-it responsibility for such settings that amalgamate the two modalities.  The paper shows that the newly proposed notion of responsibility and counterfactual responsibility are not definable through each other and studies the responsibility gap for these two forms of responsibility. It shows that although these two forms of responsibility are not enough to ascribe responsibility in each possible situation, this gap does not exist if higher-order responsibility is taken into account.","['PEAI: Philosophical Foundations of AI', 'KRR: Action', 'Change', 'and Causality', 'PEAI: AI & Epistemology', 'PEAI: Accountability', 'Interpretability & Explainability']",[],['Qi SHI'],['University of Southampton'],['United Kingdom']
https://ojs.aaai.org/index.php/AAAI/article/view/29967,Privacy & Data Governance,Towards the Robustness of Differentially Private Federated Learning,"Robustness and privacy protection are two important factors of trustworthy federated learning (FL). Existing FL works usually secure data privacy by perturbing local model gradients via the differential privacy (DP) technique, or defend against poisoning attacks by filtering the local gradients in the outlier of the gradient distribution before aggregation. However, these two issues are often addressed independently in existing works, and how to secure federated learning in both privacy and robustness still needs further exploration. In this paper, we unveil that although DP noisy perturbation can improve the learning robustness, DP-FL frameworks are not inherently robust and are vulnerable to a carefully-designed attack method. Furthermore, we reveal that it is challenging for existing robust FL methods to defend against attacks on DP-FL. This can be attributed to the fact that the local gradients of DP-FL are perturbed by random noise, and the selected central gradients inevitably incorporate a higher proportion of poisoned gradients compared to conventional FL. To address this problem, we further propose a new defense method for DP-FL (named Robust-DPFL), which can effectively distinguish poisoned and clean local gradients in DP-FL and robustly update the global model. Experiments on three benchmark datasets demonstrate that baseline methods cannot ensure task accuracy, data privacy, and robustness simultaneously, while Robust-DPFL can effectively enhance the privacy protection and robustness of federated learning meanwhile maintain the task performance.","['PEAI: Privacy & Security', 'ML: Privacy', 'ML: Adversarial Learning & Robustness']",[],"['Tao Qi', 'Huili Wang', 'Yongfeng Huang']","['Tsinghua University', 'Tsinghua University', 'Tsinghua University\nZhongguancun Laboratory']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29966,Security,Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation,"Model extraction attacks (MEAs) enable an attacker to replicate the functionality of a victim deep neural network (DNN) model by only querying its API service remotely, posing a severe threat to the security and integrity of pay-per-query DNN-based services. Although the majority of current research on MEAs has primarily concentrated on neural classifiers, there is a growing prevalence of image-to-image translation (I2IT) tasks in our everyday activities. However, techniques developed for MEA of DNN classifiers cannot be directly transferred to the case of I2IT, rendering the vulnerability of I2IT models to MEA attacks often underestimated. This paper unveils the threat of MEA in I2IT tasks from a new perspective. Diverging from the traditional approach of bridging the distribution gap between attacker queries and victim training samples, we opt to mitigate the effect caused by the different distributions, known as the domain shift. This is achieved by introducing a new regularization term that penalizes high-frequency noise, and seeking a flatter minimum to avoid overfitting to the shifted distribution. Extensive experiments on different image translation tasks, including image super-resolution and style transfer, are performed on different backbone victim models, and the new design consistently outperforms the baseline by a large margin across all metrics. A few real-life I2IT APIs are also verified to be extremely vulnerable to our attack, emphasizing the need for enhanced defenses and potentially revised API publishing policies.","['PEAI: Safety', 'Robustness & Trustworthiness', 'PEAI: Privacy & Security']",[],"['Di Mi', 'Yanjun Zhang', 'Leo Yu Zhang', 'Shengshan Hu', 'Qi Zhong', 'Haizhuan Yuan', 'Shirui Pan']","['Xiangtan University', 'University of Technology Sydney', 'Griffith University', 'Huazhong University of Science and Technology', 'City University of Macau', 'Xiangtan University', 'Griffith University']","['China', 'Australia', 'Australia', 'China', 'United States', 'China', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29972,Fairness & Bias,"U-trustworthy Models. Reliability, Competence, and Confidence in Decision-Making","With growing concerns regarding bias and discrimination in predictive models, the AI community has increasingly focused on assessing AI system trustworthiness. Conventionally, trustworthy AI literature relies on the probabilistic framework and calibration as prerequisites for trustworthiness. In this work, we depart from this viewpoint by proposing a novel trust framework inspired by the philosophy literature on trust. We present a precise mathematical definition of trustworthiness, termed U-trustworthiness, specifically tailored for a subset of tasks aimed at maximizing a utility function. We argue that a model’s U-trustworthiness is contingent upon its ability to maximize Bayes utility within this task subset. Our first set of results challenges the probabilistic framework by demonstrating its potential to favor less trustworthy models and introduce the risk of misleading trustworthiness assessments. Within the context of U-trustworthiness, we prove that properly-ranked models are inherently U-trustworthy. Furthermore, we advocate for the adoption of the AUC metric as the preferred measure of trustworthiness. By offering both theoretical guarantees and experimental validation, AUC enables robust evaluation of trustworthiness, thereby enhancing model selection and hyperparameter tuning to yield more trustworthy outcomes.","['PEAI: Safety', 'Robustness & Trustworthiness', 'ML: Evaluation and Analysis', 'ML: Other Foundations of Machine Learning']",[],"['Ritwik Vashistha', 'Arya Farahi']","['University of Texas at Austin', 'University of Texas at Austin']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29974,Fairness & Bias,SAME: Sample Reconstruction against Model Extraction Attacks,"While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.","['PEAI: Privacy & Security', 'CV: Adversarial Attacks & Robustness', 'CV: Bias', 'Fairness & Privacy', 'ML: Privacy', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Yi Xie', 'Jie Zhang', 'Shiqian Zhao', 'Tianwei Zhang', 'Xiaofeng Chen']","['Xidian University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Xidian University']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29975,Privacy & Data Governance,High-Fidelity Gradient Inversion in Distributed Learning,"Distributed learning frameworks aim to train global models by sharing gradients among clients while preserving the data privacy of each individual client. However, extensive research has demonstrated that these learning frameworks do not absolutely ensure the privacy, as training data can be reconstructed from shared gradients. Nevertheless, the existing privacy-breaking attack methods have certain limitations. Some are applicable only to small models, while others can only recover images in small batch size and low resolutions, or with low fidelity. Furthermore, when there are some data with the same label in a training batch, existing attack methods usually perform poorly. In this work, we successfully address the limitations of existing attacks by two steps. Firstly, we model the coefficient of variation (CV) of features and design an evolutionary algorithm based on the minimum CV to accurately reconstruct the labels of all training data. After that, we propose a stepwise gradient inversion attack, which dynamically adapts the objective function, thereby effectively and rationally promoting the convergence of attack results towards an optimal solution. With these two steps, our method is able to recover high resolution images (224*224 pixel, from ImageNet and Web) with high fidelity in distributed learning scenarios involving complex models and larger batch size. Experiment results demonstrate the superiority of our approach, reveal the potential vulnerabilities of the distributed learning paradigm, and emphasize the necessity of developing more secure mechanisms. Source code is available at https://github.com/MiLab-HITSZ/2023YeHFGradInv.","['PEAI: Privacy & Security', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Zipeng Ye', 'Wenjian Luo', 'Qi Zhou', 'Yubo Tang']","['School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies', 'School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies\nPeng Cheng Laboratory', 'School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies', 'School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29976,Security,Robustness Verification of Deep Reinforcement Learning Based Control Systems Using Reward Martingales,"Deep Reinforcement Learning (DRL) has gained prominence as an effective approach for control systems. However, its practical deployment is impeded by state perturbations that can severely impact system performance. Addressing this critical challenge requires robustness verification about system performance, which involves tackling two quantitative questions: (i) how to establish guaranteed bounds for expected cumulative rewards, and (ii) how to determine tail bounds for cumulative rewards. In this work, we present the first approach for robustness verification of DRL-based control systems by introducing  reward martingales, which offer a rigorous mathematical foundation to characterize the impact of state perturbations on system performance in terms of cumulative rewards. Our verified results provide provably quantitative certificates for the two questions. We then show that reward martingales can be implemented and trained via neural networks, against different types of control policies. Experimental results demonstrate that our certified bounds tightly enclose simulation outcomes on various DRL-based control systems, indicating the effectiveness and generality of the proposed approach.","['PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Dapeng Zhi', 'Peixin Wang', 'Cheng Chen', 'Min Zhang']","['Shanghai Key Laboratory for Trustworthy Computing, East China Normal University', 'University of Oxford', 'Shanghai Key Laboratory for Trustworthy Computing, East China Normal University', 'Shanghai Key Laboratory for Trustworthy Computing, East China Normal University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29978,Transparency & Explainability,Batch Normalization Is Blind to the First and Second Derivatives of the Loss,"We prove that when we do the Taylor series expansion of the loss function, the BN operation will block the influence of the first-order term and most influence of the second-order term of the loss. We also find that such a problem is caused by the standardization phase of the BN operation. We believe that proving the blocking of certain loss terms provides an analytic perspective for potential detects of a deep model with BN operations, although the blocking problem is not fully equivalent to significant damages in all tasks on benchmark datasets. Experiments show that the BN operation significantly affects feature representations in specific tasks.","['PEAI: Accountability', 'Interpretability & Explainability', 'ML: Deep Learning Theory']",[],"['Zhanpeng Zhou', 'Wen Shen', 'Huixin Chen', 'Ling Tang', 'Yuefeng Chen', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Alibaba Group', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29977,Fairness & Bias,Regulating AI: Applying Insights from Behavioural Economics and Psychology to the Application of Article 5 of the EU AI Act,"Article 5 of the European Union’s Artificial Intelligence Act is intended to regulate AI use to prevent potentially harmful consequences. Nevertheless, applying this legislation practically is likely to be challenging because of ambiguously used terminologies and because it fails to specify which manipulation techniques may be invoked by AI, potentially leading to significant harm. This paper aims to bridge this gap by defining key terms and demonstrating how AI may invoke these techniques, drawing from insights in psychology and behavioural economics. First, this paper provides definitions of the terms “subliminal techniques”, “manipulative techniques” and “deceptive techniques”. Secondly, we identified from the literature in cognitive psychology and behavioural economics three subliminal and five manipulative techniques and exemplify how AI might implement these techniques to manipulate users in real-world case scenarios. These illustrations may serve as a practical guide for stakeholders to detect cases of AI manipulation and consequently devise preventive measures. Article 5 has also been criticised for offering inadequate protection. We critically assess the protection offered by Article 5, proposing specific revisions to paragraph 1, points (a) and (b) of Article 5 to increase its protective effectiveness.","['PEAI: AI & Law', 'Justice', 'Regulation & Governance', 'PEAI: Societal Impact of AI']",[],"['Huixin Zhong', ""Eamonn O'Neill"", 'Janina A. Hoffmann']","['Centre for Doctoral Training in Accountable, Responsible and Transparent AI, University of Bath', 'Centre for Doctoral Training in Accountable, Responsible and Transparent AI, University of Bath', 'Centre for Doctoral Training in Accountable, Responsible and Transparent AI, University of Bath']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30007,Fairness & Bias,Equity-Transformer: Solving NP-Hard Min-Max Routing Problems as Sequential Generation with Equity Context,"Min-max routing problems aim to minimize the maximum tour length among multiple agents as they collaboratively visit all cities, i.e., the completion time. These problems include impactful real-world applications but are known as NP-hard. Existing methods are facing challenges, particularly in large-scale problems that require the coordination of numerous agents to cover thousands of cities. This paper proposes Equity-Transformer to solve large-scale min-max routing problems. First, we model min-max routing problems into sequential planning, reducing the complexity and enabling the use of a powerful Transformer architecture. Second, we propose key inductive biases that ensure equitable workload distribution among agents. The effectiveness of Equity-Transformer is demonstrated through its superior performance in two representative min-max routing tasks: the min-max multi-agent traveling salesman problem (min-max mTSP) and the min-max multi-agent pick-up and delivery problem (min-max mPDP). Notably, our method achieves significant reductions of runtime, approximately 335 times, and cost values of about 53% compared to a competitive heuristic (LKH3) in the case of 100 vehicles with 1,000 cities of mTSP. We provide reproducible source code: https://github.com/kaist-silab/equity-transformer.","['PRS: Routing', 'SO: Combinatorial Optimization']",[],"['Jiwoo Son', 'Minsu Kim', 'Sanghyeok Choi', 'Hyeonah Kim', 'Jinkyoo Park']","['Korea Advanced Institute of Science and Technology (KAIST)\nOmelet', 'Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science and Technology (KAIST)\nOmelet']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/30011,Transparency & Explainability,s-ID: Causal Effect Identification in a Sub-population,"Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup, which is distinguished from the whole population through the influence of systematic biases in the sampling process. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.","['RU: Causality', 'KRR: Action', 'Change', 'and Causality', 'ML: Causal Learning', 'RU: Probabilistic Inference']",[],"['Amir Mohammad Abouei', 'Ehsan Mokhtarian', 'Negar Kiyavash']","['EPFL', 'EPFL', 'EPFL']","['Switzerland', 'Switzerland', 'Switzerland']"
https://ojs.aaai.org/index.php/AAAI/article/view/30013,Transparency & Explainability,Backward Responsibility in Transition Systems Using General Power Indices,"To improve reliability and the understanding of AI systems, there is increasing interest in the use of formal methods, e.g. model checking. Model checking tools produce a counterexample when a model does not satisfy a property. Understanding these counterexamples is critical for efficient debugging, as it allows the developer to focus  on the parts of the program that caused the issue.  To this end, we present a new technique that ascribes a responsibility value to each state in a transition system that does not satisfy a given safety property. The value is higher if the non-deterministic choices in a state have more power to change the outcome, given the behaviour observed in the counterexample. For this, we employ a concept from cooperative game theory – namely general power indices, such as the Shapley value – to compute the responsibility of the states.  We present an optimistic and pessimistic version of responsibility that differ in how they treat the states that do not lie on the counterexample. We give a characterisation of optimistic responsibility that leads to an efficient algorithm for it and show computational hardness of the pessimistic version. We also present a tool to compute responsibility and show how a stochastic algorithm can be used to approximate responsibility in larger models. These methods can be deployed in the design phase, at runtime and at inspection time to gain insights on causal relations within the behavior of AI systems.","['RU: Causality', 'GTEP: Cooperative Game Theory']",[],"['Christel Baier', 'Roxane van den Bossche', 'Sascha Klüppelholz', 'Johannes Lehmann', 'Jakob Piribauer']","['TU Dresden, Germany\nCentre for Tactile Internet with Human-in-the-Loop (CeTI)', 'Université Paris-Saclay, ENS Paris-Saclay, France', 'TU Dresden, Germany', 'TU Dresden, Germany\nCentre for Tactile Internet with Human-in-the-Loop (CeTI)', 'TU Dresden, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30017,Transparency & Explainability,Identification of Causal Structure with Latent Variables Based on Higher Order Cumulants,"Causal discovery with latent variables is a crucial but challenging task. Despite the emergence of numerous methods aimed at addressing this challenge, they are not fully identified to the structure that two observed variables are influenced by one latent variable and there might be a directed edge in between. Interestingly, we notice that this structure can be identified through the utilization of higher-order cumulants. By leveraging the higher-order cumulants of non-Gaussian data, we provide an analytical solution for estimating the causal coefficients or their ratios. With the estimated (ratios of) causal coefficients, we propose a novel approach to identify the existence of a causal edge between two observed variables subject to latent variable influence. In case when such a causal edge exits, we introduce an asymmetry criterion to determine the causal direction. The experimental results demonstrate the effectiveness of our proposed method.","['RU: Causality', 'ML: Causal Learning']",[],"['Wei Chen', 'Zhiyi Huang', 'Ruichu Cai', 'Zhifeng Hao', 'Kun Zhang']","['School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China\nCollege of Science, Shantou University, Shantou, China', 'Department of Philosophy, Carnegie Mellon University, Pittsburgh, PA, United States\nMohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30019,Security,Probabilistic Offline Policy Ranking with Approximate Bayesian Computation,"In practice, it is essential to compare and rank candidate policies offline before real-world deployment for safety and reliability. Prior work seeks to solve this offline policy ranking (OPR) problem through value-based methods, such as Off-policy evaluation (OPE). However, they fail to analyze special case performance (e.g., worst or best cases), due to the lack of holistic characterization of policies’ performance. It is even more difficult to estimate precise policy values when the reward is not fully accessible under sparse settings. In this paper, we present Probabilistic Offline Policy Ranking (POPR), a framework to address OPR problems by leveraging expert data to characterize the probability of a candidate policy behaving like experts, and approximating its entire performance posterior distribution to help with ranking. POPR does not rely on value estimation, and the derived performance posterior can be used to distinguish candidates in worst-, best-, and average-cases. To estimate the posterior, we propose POPR-EABC, an Energy-based Approximate Bayesian Computation (ABC) method conducting likelihood-free inference. POPR-EABC reduces the heuristic nature of ABC by a smooth energy function, and improves the sampling efficiency by a pseudo-likelihood. We empirically demonstrate that POPR-EABC is adequate for evaluating policies in both discrete and continuous action spaces across various experiment environments, and facilitates probabilistic comparisons of candidate policies before deployment.","['RU: Probabilistic Inference', 'ML: Bayesian Learning', 'ML: Reinforcement Learning', 'RU: Uncertainty Representations']",[],"['Longchao Da', 'Porter Jenkins', 'Trevor Schwantes', 'Jeffrey Dotson', 'Hua Wei']","['Arizona State University', 'Brigham Young University', 'Brigham Young University', 'Brigham Young University', 'Arizona State University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30021,Transparency & Explainability,Identifiability of Direct Effects from Summary Causal Graphs,"Dynamic structural causal models (SCMs) are a powerful framework for reasoning in dynamic systems about direct effects which measure how a change in one variable affects another variable while holding all other variables constant. The causal relations in a dynamic structural causal model can be qualitatively represented with an acyclic full-time causal graph. Assuming linearity and no hidden confounding and given the full-time causal graph, the direct causal effect is always identifiable. However, in many application such a graph is not available for various reasons but nevertheless experts have access to the summary causal graph of the full-time causal graph which represents causal relations between time series while omitting temporal information and allowing cycles. This paper presents a complete identifiability result which characterizes all cases for which the direct effect is graphically identifiable from a summary causal graph and gives two sound finite adjustment sets that can be used to estimate the direct effect whenever it is identifiable.","['RU: Causality', 'KRR: Action', 'Change', 'and Causality']",[],"['Simon Ferreira', 'Charles K. Assaad']","['EasyVista\nENS de Lyon', 'EasyVista']","['France', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30023,Transparency & Explainability,Identification for Tree-Shaped Structural Causal Models in Polynomial Time,"Linear structural causal models (SCMs) are used to express and analyze the relationships between random variables. Direct causal effects are represented as directed edges and confounding factors as bidirected edges. Identifying the causal parameters from correlations between the nodes is an open problem in artificial intelligence. In this paper, we study SCMs whose directed component forms a tree. Van der Zander et al. give a PSPACE-algorithm for the identification problem in this case, which is a significant improvement over the general Gröbner basis approach, which has doubly-exponential time complexity in the number of structural parameters. However, they do not show that their algorithm is complete. In this work, we present a randomized polynomial-time algorithm, which solves the identification problem for tree-shaped SCMs. For every structural parameter, our algorithms decides whether it is generically identifiable, generically 2-identifiable, or generically unidentifiable. (No other cases can occur.) In the first two cases, it provides one or two  fractional affine square root terms of polynomials (FASTPs) for the corresponding parameter, respectively. In particular, our algorithm is not only polynomial time, but also complete for for tree-shaped SCMs.","['RU: Causality', 'RU: Graphical Models']",[],"['Aaryan Gupta', 'Markus Bläser']","['Indian Institute of Technology Bombay', 'Saarland University']","['India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30027,Transparency & Explainability,Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach,"This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm’s reward distribution. A major obstacle in this setting is the existence of compound biases from the observational data. Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase. In this work, we formulate this problem from a causal perspective. First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply. Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data. The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy. We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret.","['RU: Causality', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Online Learning & Bandits']",[],"['Wen Huang', 'Xintao Wu']","['University of Arkansas', 'University of Arkansas']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30025,Transparency & Explainability,Uncertainty Quantification in Heterogeneous Treatment Effect Estimation with Gaussian-Process-Based Partially Linear Model,"Estimating heterogeneous treatment effects across individuals has attracted growing attention as a statistical tool for performing critical decision-making. We propose a Bayesian inference framework that quantifies the uncertainty in treatment effect estimation to support decision-making in a relatively small sample size setting. Our proposed model places Gaussian process priors on the nonparametric components of a semiparametric model called a partially linear model. This model formulation has three advantages. First, we can analytically compute the posterior distribution of a treatment effect without relying on the computationally demanding posterior approximation. Second, we can guarantee that the posterior distribution concentrates around the true one as the sample size goes to infinity. Third, we can incorporate prior knowledge about a treatment effect into the prior distribution, improving the estimation efficiency. Our experimental results show that even in the small sample size setting, our method can accurately estimate the heterogeneous treatment effects and effectively quantify its estimation uncertainty.","['RU: Causality', 'ML: Bayesian Learning', 'ML: Causal Learning']",[],"['Shunsuke Horii', 'Yoichi Chikahara']","['Waseda University', 'NTT']","['Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30030,Transparency & Explainability,Probabilities of Causation with Nonbinary Treatment and Effect,"Probabilities of causation are proven to be critical in modern decision-making. This paper deals with the problem of estimating the probabilities of causation when treatment and effect are not binary. Pearl defined the binary probabilities of causation, such as the probability of necessity and sufficiency (PNS), the probability of sufficiency (PS), and the probability of necessity (PN). Tian and Pearl then derived sharp bounds for these probabilities of causation using experimental and observational data. In this paper, we define and provide theoretical bounds for all types of probabilities of causation with multivalued treatments and effects. We further discuss examples where our bounds guide practical decisions and use simulation studies to evaluate how informative the bounds are for various data combinations.","['RU: Causality', 'KRR: Action', 'Change', 'and Causality']",[],"['Ang Li', 'Judea Pearl']","['Florida State University', 'University of California, Los Angeles']","['United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30028,Fairness & Bias,Effectiveness of Constant Stepsize in Markovian LSA and Statistical Inference,"In this paper, we study the effectiveness of using a constant stepsize in statistical inference via linear stochastic approximation (LSA) algorithms with Markovian data. After establishing a Central Limit Theorem (CLT), we outline an inference procedure that uses averaged LSA iterates to construct confidence intervals (CIs). Our procedure leverages the fast mixing property of constant-stepsize LSA for better covariance estimation and employs Richardson-Romberg (RR) extrapolation to reduce the bias induced by constant stepsize and Markovian data. We develop theoretical results for guiding stepsize selection in RR extrapolation, and identify several important settings where the bias provably vanishes even without extrapolation. We conduct extensive numerical experiments and compare against classical inference approaches. Our results show that using a constant stepsize enjoys easy hyperparameter tuning, fast convergence, and consistently better CI coverage, especially when data is limited.","['RU: Stochastic Optimization', 'ML: Reinforcement Learning', 'RU: Probabilistic Inference']",[],"['Dongyan (Lucy) Huo', 'Yudong Chen', 'Qiaomin Xie']","['Cornell University', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30031,Transparency & Explainability,Unit Selection with Nonbinary Treatment and Effect,"The unit selection problem aims to identify a set of individuals who are most likely to exhibit a desired mode of behavior or to evaluate the percentage of such individuals in a given population, for example, selecting individuals who would respond one way if encouraged and a different way if not encouraged. Using a combination of experimental and observational data, Li and Pearl solved the binary unit selection problem (binary treatment and effect) by deriving tight bounds on the ""benefit function,"" which is the payoff/cost associated with selecting an individual with given characteristics. This paper extends the benefit function to the general form such that the treatment and effect are not restricted to binary. We then propose an algorithm to test the identifiability of the nonbinary benefit function and an algorithm to compute the bounds of the nonbinary benefit function using experimental and observational data.","['RU: Causality', 'KRR: Action', 'Change', 'and Causality']",[],"['Ang Li', 'Judea Pearl']","['Florida State University', 'University of California, Los Angeles']","['United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30033,Transparency & Explainability,TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences,"Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a unified topological neural Poisson auto-regressive model with two processes. In the generation process, we employ a variant of the neural Poisson process to model the event sequences, considering influences from both the topological network and the Granger causal structure. In the inference process, we formulate an amortized inference algorithm to infer the latent Granger causal structure. We encapsulate these two processes within a unified likelihood function, providing an end-to-end framework for this task. Experiments on simulated and real-world data demonstrate the effectiveness of our approach.","['RU: Causality', 'ML: Causal Learning', 'ML: Time-Series/Data Streams']",[],"['Yuequn Liu', 'Ruichu Cai', 'Wei Chen', 'Jie Qiao', 'Yuguang Yan', 'Zijian Li', 'Keli Zhang', 'Zhifeng Hao']","['School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China\nMachine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates', 'Huawei Noah’s Ark Lab, Huawei, Paris, France', 'College of Science, Shantou University, Shantou, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30035,Transparency & Explainability,Root Cause Explanation of Outliers under Noisy Mechanisms,"Identifying root causes of anomalies in causal processes is vital across disciplines. Once identified, one can isolate the root causes and implement necessary measures to restore the normal operation. Causal processes are often modelled as graphs with entities being nodes and their paths/interconnections as edge. Existing work only consider the contribution of nodes in the generative process, thus can not attribute the outlier score to the edges of the mechanism if the anomaly occurs in the connections. In this paper, we consider both individual edge and node of each mechanism when identifying the root causes. We introduce a noisy functional causal model to account for this purpose. Then, we employ Bayesian learning and inference methods to infer the noises of the nodes and edges. We then represent the functional form of a target outlier leaf as a function of the node and edge noises. Finally, we propose an efficient gradient-based attribution method to compute the anomaly attribution scores which scales linearly with the number of nodes and edges. Experiments on simulated datasets and two real-world scenario datasets show better anomaly attribution performance of the proposed method compared to the baselines. Our method scales to larger graphs with more nodes and edges.","['RU: Causality', 'RU: Graphical Models', 'RU: Probabilistic Inference']",[],"['Phuoc Nguyen', 'Truyen Tran', 'Sunil Gupta', 'Thin Nguyen', 'Svetha Venkatesh']","['Deakin University', 'Deakin University', 'Deakin University, Australia', 'Deakin University', 'Deakin University']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/30036,Transparency & Explainability,Identification of Causal Structure in the Presence of Missing Data with Additive Noise Model,"Missing data are an unavoidable complication frequently encountered in many causal discovery tasks.  When a missing process depends on the missing values themselves (known as self-masking missingness), the recovery of the joint distribution becomes unattainable, and detecting the presence of such self-masking missingness remains a perplexing challenge. Consequently, due to the inability to reconstruct the original distribution and to discern the underlying missingness mechanism, simply applying existing causal discovery methods would lead to wrong conclusions. In this work, we found that the recent advances additive noise model has the potential for learning causal structure under the existence of the self-masking missingness. With this observation, we aim to investigate the identification problem of learning causal structure from missing data under an additive noise model with different missingness mechanisms, where the `no self-masking missingness' assumption can be eliminated appropriately.  Specifically, we first elegantly extend the scope of identifiability of causal skeleton to the case with weak self-masking missingness (i.e., no other variable could be the cause of self-masking indicators except itself). We further provide the sufficient and necessary identification conditions of the causal direction under additive noise model and show that the causal structure can be identified up to an IN-equivalent pattern. We finally propose a practical algorithm based on the above theoretical results on learning the causal skeleton and causal direction. Extensive experiments on synthetic and real data demonstrate the efficiency and effectiveness of the proposed algorithms.",['RU: Causality'],[],"['Jie Qiao', 'Zhengming Chen', 'Jianhua Yu', 'Ruichu Cai', 'Zhifeng Hao']","['School of Computer Science, Guangdong University of Technology, Guangzhou 510006, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou 510006, China\nMachine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE', 'School of Computer Science, Guangdong University of Technology, Guangzhou 510006, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou 510006, China\nPeng Cheng Laboratory, Shenzhen 518066, China', 'College of Science, Shantou University, Shantou 515063, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30037,Transparency & Explainability,Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis,"Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e.,  X->Y and Y->X are distributed equivalent. Fortunately, in this work, we found that the causal order from X to its child Y is identifiable if X is a root vertex and has at least two directed paths to Y, or the ancestor of X with the most directed path to X has a directed path to Y without passing X. Specifically, we propose a Poisson Branching Structure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using high-order cumulants. Theoretical results establish the connection between the path and cumulant and demonstrate that the path information can be obtained from the cumulant. With the path information, causal order is identifiable under some graphical conditions. A practical algorithm for learning causal structure under PB-SCM is proposed and the experiments demonstrate and verify the effectiveness of the proposed method.",['RU: Causality'],[],"['Jie Qiao', 'Yu Xiang', 'Zhengming Chen', 'Ruichu Cai', 'Zhifeng Hao']","['School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China', 'College of Science, Shantou University, Shantou, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30038,Transparency & Explainability,A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the Same Skeleton,"Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its non-descendants conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular, that all DAGs in the same MEC must have the same skeleton (underlying undirected graph) and v-structures (induced subgraph of the form a->b<-c).  These combinatorial characterizations also suggest several natural algorithmic questions. One of these is: given an undirected graph G as input, how many distinct Markov equivalence classes have the skeleton G? Much work has been devoted in the last few years to this and other closely related problems. However, to the best of our knowledge, a polynomial-time algorithm for the problem remains unknown.  In this paper, we make progress towards this goal by giving a fixed parameter tractable algorithm for the above problem, with the parameters being the treewidth and the maximum degree of the input graph G. The main technical ingredient in our work is a construction we refer to as shadow, which lets us create a local description of long-range constraints imposed by the combinatorial characterizations of MECs.","['RU: Graphical Models', 'ML: Other Foundations of Machine Learning', 'RU: Causality']",[],['Vidya Sagar Sharma'],"['Tata Institute of Fundamental Research, Mumbai']",['India']
https://ojs.aaai.org/index.php/AAAI/article/view/30044,Transparency & Explainability,Neural Causal Abstractions,"The ability of humans to understand the world in terms of cause and effect relationships, as well as their ability to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem under the theory of causal abstractions, but it is an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true model is not known, and limited data is available in most practical settings. In this paper, we focus on a family of causal abstractions constructed by clustering variables and their domains, redefining abstractions to be amenable to individual causal distributions. We show that such abstractions can be learned in practice using Neural Causal Models, allowing us to utilize the deep learning toolkit to solve causal tasks (identification, estimation, sampling) at different levels of abstraction granularity. Finally, we show how representation learning can be used to learn abstractions, which we apply in our experiments to scale causal inferences to high dimensional settings such as with image data.","['RU: Causality', 'ML: Causal Learning', 'ML: Deep Generative Models & Autoencoders', 'ML: Representation Learning']",[],"['Kevin Xia', 'Elias Bareinboim']","['Columbia University', 'Columbia University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30047,Fairness & Bias,Deep Copula-Based Survival Analysis for Dependent Censoring with Identifiability Guarantees,"Censoring is the central problem in survival analysis where either the time-to-event (for instance, death), or the time-to censoring (such as loss of follow-up) is observed for each sample. The majority of existing machine learning-based survival analysis methods assume that survival is conditionally independent of censoring given a set of covariates; an assumption that cannot be verified since only marginal distributions is available from the data. The existence of dependent censoring, along with the inherent bias in current estimators has been demonstrated in a variety of applications, accentuating the need for a more nuanced approach. However, existing methods that adjust for dependent censoring require practitioners to specify the ground truth copula. This requirement poses a significant challenge for practical applications, as model misspecification can lead to substantial bias. In this work, we propose a flexible deep learning-based survival analysis method that simultaneously accommodate for dependent censoring and eliminates the requirement for specifying the ground truth copula. We theoretically prove the identifiability of our model under a broad family of copulas and survival distributions. Experiments results from a wide range of datasets demonstrate that our approach successfully discerns the underlying dependency structure and significantly reduces survival estimation bias when compared to existing methods.","['RU: Probabilistic Inference', 'ML: Classification and Regression', 'ML: Applications', 'APP: Humanities & Computational Social Science']",[],"['Weijia Zhang', 'Chun Kai Ling', 'Xuanhui Zhang']","['The University of Newcastle', 'Carnegie Mellon University', 'Nanjing University']","['Australia', 'United States', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30043,Transparency & Explainability,Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs,"Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment – a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. In 2022, Jeong, Tian, and Bareinboim presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an O(n³(n+m)) run time, where n denotes the number of variables and m the number of edges of the causal graph. In our work, we give the first linear-time, i.e., O(n+m), algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result implies an O(n(n+m)) delay enumeration algorithm of all front-door adjustment sets, again improving previous work by a factor of n³. Moreover, we provide the first linear-time algorithm for finding a minimal front-door adjustment set. We offer implementations of our algorithms in multiple programming languages to facilitate practical usage and empirically validate their feasibility, even for large graphs.","['RU: Causality', 'RU: Graphical Models']",[],"['Marcel Wienöbst', 'Benito van der Zander', 'Maciej Liśkiewicz']","['University of Lübeck', 'University of Lübeck', 'University of Lübeck']","['Ireland', 'Ireland', 'Ireland']"
https://ojs.aaai.org/index.php/AAAI/article/view/30059,Security,Limited Query Graph Connectivity Test,"We propose a combinatorial optimisation model called Limited Query Graph Connectivity Test. We consider a graph whose edges have two possible states (On/Off). The edges' states are hidden initially. We could query an edge to reveal its state. Given a source s and a destination t, we aim to test s−t connectivity by identifying either a path (consisting of only On edges) or a cut (consisting of only Off edges). We are limited to B queries, after which we stop regardless of whether graph connectivity is established. We aim to design a query policy that minimizes the expected number of queries.  Our model is mainly motivated by a cyber security use case where we need to establish whether attack paths exist in a given network, between a source (i.e., a compromised user node) and a destination (i.e., a high-privilege admin node).  Edge query is resolved by manual effort from the IT admin, which is the motivation behind query minimization.  Our model is highly related to Stochastic Boolean Function Evaluation (SBFE).  There are two existing exact algorithms for SBFE that are prohibitively expensive. We propose a signifcantly more scalable exact algorithm. While previous exact algorithms only scale for trivial graphs (i.e., past works experimented on at most 20 edges), we empirically demonstrate that our algorithm is scalable for a wide range of much larger practical graphs (i.e., graphs representing Windows domain networks with tens of thousands of edges).  We also propose three heuristics. Our best-performing heuristic is via limiting the planning horizon of the exact algorithm. The other two are via reinforcement learning (RL) and Monte Carlo tree search (MCTS). We also derive an algorithm for computing the performance lower bound. Experimentally, we show that all our heuristics are near optimal.  The heuristic building on the exact algorithm outperforms all other heuristics, surpassing RL, MCTS and eight existing heuristics ported from SBFE and related literature.","['SO: Combinatorial Optimization', 'APP: Security', 'HAI: Planning and Decision Support for Human-Machine Teams', 'PRS: Planning under Uncertainty', 'SO: Applications']",[],"['Mingyu Guo', 'Jialiang Li', 'Aneta Neumann', 'Frank Neumann', 'Hung Nguyen']","['The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/30068,Transparency & Explainability,"Paths, Proofs, and Perfection: Developing a Human-Interpretable Proof System for Constrained Shortest Paths","People want to rely on optimization algorithms for complex decisions but verifying the optimality of the solutions can then become a valid concern, particularly for critical decisions taken by non-experts in optimization. One example is the shortest-path problem on a network, occurring in many contexts from transportation to logistics to telecommunications. While the standard shortest-path problem is both solvable in polynomial time and certifiable by duality, introducing side constraints makes solving and certifying the solutions much harder. We propose a proof system for constrained shortest-path problems, which gives a set of logical rules to derive new facts about feasible solutions. The key trait of the proposed proof system is that it specifically includes high-level graph concepts within its reasoning steps (such as connectivity or path structure), in contrast to, e.g., using linear combinations of model constraints. Thus, using our proof system, we can provide a step-by-step, human-auditable explanation showing that the path given by an external solver cannot be improved. Additionally, to maximize the advantages of this setup, we propose a proof search procedure that specifically aims to find small proofs of this form using a procedure similar to A* search. We evaluate our proof system on constrained shortest path instances generated from real-world road networks and experimentally show that we may indeed derive more interpretable proofs compared to an integer programming approach, in some cases leading to much smaller proofs.","['SO: Combinatorial Optimization', 'CSO: Search']",[],"['Konstantin Sidorov', 'Gonçalo Homem de Almeida Correia', 'Mathijs de Weerdt', 'Emir Demirović']","['Delft University of Technology', 'Delft University of Technology', 'Delft University of Technology', 'Delft University of Technology']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/30086,Transparency & Explainability,Rethinking the Development of Large Language Models from the Causal Perspective: A Legal Text Prediction Case Study,"While large language models (LLMs) exhibit impressive performance on a wide range of NLP tasks, most of them fail to learn the causality from correlation, which disables them from learning rationales for predicting. Rethinking the whole developing process of LLMs is of great urgency as they are adopted in various critical tasks that need rationales, including legal text prediction (e.g., legal judgment prediction). In this paper, we first explain the underlying theoretical mechanism of their failure and argue that both the data imbalance and the omission of causality in model design and selection render the current training-testing paradigm failed to select the unique causality-based model from correlation-based models. Second, we take the legal text prediction task as the testbed and reconstruct the developing process of LLMs by simultaneously infusing causality into model architectures and organizing causality-based adversarial attacks for evaluation. Specifically, we base our reconstruction on our theoretical analysis and propose a causality-aware self-attention mechanism (CASAM), which prevents LLMs from entangling causal and non-causal information by restricting the interaction between causal and non-causal words. Meanwhile, we propose eight kinds of legal-specific attacks to form causality-based model selection. Our extensive experimental results demonstrate that our proposed CASAM achieves state-of-the-art (SOTA) performances and the strongest robustness on three commonly used legal text prediction benchmarks. We make our code publicly available at https://github.com/Carrot-Red/Rethink-LLM-development.",['General'],[],"['Haotian Chen', 'Lingwei Zhang', 'Yiran Liu', 'Yang Yu']","['Fudan University', 'Johns Hopkins University', 'Tsinghua University', 'Tsinghua University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30082,Security,On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods,"Most existing evaluations of explainable machine learning (ML) methods rely on simplifying assumptions or proxies that do not reflect real-world use cases; the handful of more robust evaluations on real-world settings have shortcomings in their design, generally leading to overestimation of methods' real-world utility.  In this work, we seek to address this by conducting a study that evaluates post-hoc explainable ML methods in a setting consistent with the application context and provide a template for future evaluation studies. We modify and improve a prior study on e-commerce fraud detection by relaxing the original work's simplifying assumptions that departed from the deployment context. Our study finds no evidence for the utility of the tested explainable ML methods in the context, which is a drastically different conclusion from the earlier work. This highlights how seemingly trivial experimental design choices can yield misleading conclusions about method utility. In addition, our work carries lessons about the necessity of not only evaluating explainable ML methods using tasks, data, users, and metrics grounded in the intended application context but also developing methods tailored to specific applications, moving beyond general-purpose explainable ML methods.",['General'],[],"['Kasun Amarasinghe', 'Kit T. Rodolfa', 'Sérgio Jesus', 'Valerie Chen', 'Vladimir Balayan', 'Pedro Saleiro', 'Pedro Bizarro', 'Ameet Talwalkar', 'Rayid Ghani']","['Carnegie Mellon University, Pittsburgh, PA', 'Stanford University, Palo Alto, CA', 'Feedzai, Lisboa, Portugal', 'Carnegie Mellon University, Pittsburgh, PA', 'Feedzai, Lisboa, Portugal', 'Feedzai, Lisboa, Portugal', 'Feedzai, Lisboa, Portugal', 'Carnegie Mellon University, Pittsburgh, PA', 'Carnegie Mellon University, Pittsburgh, PA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30084,Security,Robust Uncertainty Quantification Using Conformalised Monte Carlo Prediction,"Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model’s confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over comparable UQ methods, like MC dropout, RAPS and CQR, both in classification and regression benchmarks. MC-CP can be easily added to existing models, making its deployment simple. The MC-CP code and replication package is available at https://github.com/team-daniel/MC-CP.",['General'],[],"['Daniel Bethell', 'Simos Gerasimou', 'Radu Calinescu']","['University of York', 'University of York', 'University of York']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30080,Fairness & Bias,ImageCaptioner2: Image Captioner for Image Captioning Bias Amplification Assessment,"Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed ImageCaptioner2, for image captioning. Instead of measuring the absolute bias in the model or the data, ImageCaptioner2pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, ImageCaptioner2incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using language classifiers. Finally, we apply our ImageCaptioner2metric across 11 different image captioning architectures on three different datasets, i.e., MS-COCO caption dataset, Artemis V1, and Artemis V2, and on three different protected attributes, i.e., gender, race, and emotions. Consequently, we verify the effectiveness of our ImageCaptioner2metric by proposing Anonymous-Bench, which is a novel human evaluation paradigm for bias metrics. Our metric shows significant superiority over the recent bias metric; LIC, in terms of human alignment, where the correlation scores are 80% and 54% for our metric and LIC, respectively. The code and more details are available at https://eslambakr.github.io/imagecaptioner2.github.io/.",['General'],[],"['Eslam Abdelrahman', 'Pengzhan Sun', 'Li Erran Li', 'Mohamed Elhoseiny']","['King Abdullah University of Science and Technology (KAUST)', 'National University of Singapore', 'AWS AI, Amazon', 'King Abdullah University of Science and Technology (KAUST)']","['Saudi Arabia', 'Singapore', 'Japan', 'Saudi Arabia']"
https://ojs.aaai.org/index.php/AAAI/article/view/30081,Transparency & Explainability,A Framework for Data-Driven Explainability in Mathematical Optimization,"Advancements in mathematical programming have made it possible to efficiently tackle large-scale real-world problems that were deemed intractable just a few decades ago. However, provably optimal solutions may not be accepted due to the perception of optimization software as a black box. Although well understood by scientists, this lacks easy accessibility for practitioners. Hence, we advocate for introducing the explainability of a solution as another evaluation criterion, next to its objective value, which enables us to find trade-off solutions between these two criteria. Explainability is attained by comparing against (not necessarily optimal) solutions that were implemented in similar situations in the past. Thus, solutions are preferred that exhibit similar features. Although we prove that already in simple cases the explainable model is NP-hard, we characterize relevant polynomially solvable cases such as the explainable shortest path problem. Our numerical experiments on both artificial as well as real-world road networks show the resulting Pareto front. It turns out that the cost of enforcing explainability can be very small.",['General'],[],"['Kevin-Martin Aigner', 'Marc Goerigk', 'Michael Hartisch', 'Frauke Liers', 'Arthur Miehlich']","['Friedrich-Alexander Universität Erlangen-Nürnberg', 'University of Passau', 'University of Siegen', 'Friedrich-Alexander Universität Erlangen-Nürnberg', 'Friedrich-Alexander Universität Erlangen-Nürnberg']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30088,Security,Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming,"Despite remarkable achievements in artificial intelligence, the deployability of learning-enabled systems in high-stakes real-world environments still faces persistent challenges. For example, in safety-critical domains like autonomous driving, robotic manipulation, and healthcare, it is crucial not only to achieve high performance but also to comply with given constraints. Furthermore, adaptability becomes paramount in non-stationary domains, where environmental parameters are subject to change. While safety and adaptability are recognized as key qualities for the new generation of AI, current approaches have not demonstrated effective adaptable performance in constrained settings. Hence, this paper breaks new ground by studying the unique challenges of ensuring safety in nonstationary environments by solving constrained problems through the lens of the meta-learning approach (learning to learn). While unconstrained meta-learning already encounters complexities in end to end differentiation of the loss due to the bi-level nature, its constrained counterpart introduces an additional layer of difficulty, since the constraints imposed on task-level updates complicate the differentiation process. To address the issue, we first employ successive convex-constrained policy updates across multiple tasks with differentiable convex programming, which allows meta-learning in constrained scenarios by enabling end-to-end differentiation. This approach empowers the agent to rapidly adapt to new tasks under nonstationarity while ensuring compliance with safety constraints. We also provide a theoretical analysis demonstrating guaranteed monotonic improvement of our approach, justifying our algorithmic designs. Extensive simulations across diverse environments provide empirical validation with significant improvement over established benchmarks.",['General'],[],"['Minjae Cho', 'Chuangchuang Sun']","['Mississippi State University', 'Mississippi State University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30091,Security,Find the Lady: Permutation and Re-synchronization of Deep Neural Networks,"Deep neural networks are characterized by multiple symmetrical, equi-loss solutions that are redundant. Thus, the order of neurons in a layer and feature maps can be given arbitrary permutations, without affecting (or minimally affecting) their output. If we shuffle these neurons, or if we apply to them some perturbations (like fine-tuning) can we put them back in the original order i.e. re-synchronize? Is there a possible corruption threat? Answering these questions is important for applications like neural network white-box watermarking for ownership tracking and integrity verification. We advance a method to re-synchronize the order of permuted neurons. Our method is also effective if neurons are further altered by parameter pruning, quantization, and fine-tuning, showing robustness to integrity attacks. Additionally, we provide theoretical and practical evidence for the usual means to corrupt the integrity of the model, resulting in a solution to counter it. We test our approach on popular computer vision datasets and models, and we illustrate the threat and our countermeasure on a popular white-box watermarking method.",['General'],[],"['Carl De Sousa Trias', 'Mihai Petru Mitrea', 'Attilio Fiandrotti', 'Marco Cagnazzo', 'Sumanta Chaudhuri', 'Enzo Tartaglione']","['Télécom SudParis, Institut Polytechnique de Paris, France', 'Télécom SudParis, Institut Polytechnique de Paris, France', 'University of Turin, Italy', 'University of Padua, Italy', 'LTCI, Télécom Paris, Institut Polytechnique de Paris, France', 'LTCI, Télécom Paris, Institut Polytechnique de Paris, France']","['France', 'France', 'France', 'France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/30092,Security,Stability Analysis of Switched Linear Systems with Neural Lyapunov Functions,"Neural-based, data-driven analysis and control of dynamical systems have been recently investigated and have shown great promise, e.g. for safety verification or stability analysis. Indeed, not only do neural networks allow for an entirely model-free, data-driven approach, but also for handling arbitrary complex functions via their power of representation (as opposed to, e.g. algebraic optimization techniques that are restricted to polynomial functions). Whilst classical Lyapunov techniques allow to provide a formal and robust guarantee of stability of a switched dynamical system, very little is yet known about correctness guarantees for Neural Lyapunov functions, nor about their performance (amount of data needed for a certain accuracy).  We formally introduce Neural Lyapunov functions for the stability analysis of switched linear systems: we benchmark them on this paradigmatic problem, which is notoriously difficult (and in general Turing-undecidable), but which admits existing recently-developed technologies and theoretical results. Inspired by switched systems theory, we provide theoretical guarantees on the representative power of neural networks, leveraging recent results from the ML community. We additionally experimentally display how Neural Lyapunov functions compete with state-of-the-art results and techniques, while admitting a wide range of improvement, both in theory and in practice. This study intends to improve our understanding of the opportunities and current limitations of neural-based data-driven analysis and control of complex dynamical systems.",['General'],[],"['Virginie Debauche', 'Alec Edwards', 'Raphaël M. Jungers', 'Alessandro Abate']","['ICTEAM Institute, UCLouvain, Louvain-la-Neuve, Belgium', 'Department of Computer Science, University of Oxford, United Kingdom', 'ICTEAM Institute, UCLouvain, Louvain-la-Neuve, Belgium', 'Department of Computer Science, University of Oxford, United Kingdom']","['Belgium', 'United Kingdom', 'Belgium', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30093,Security,Robustness Verification of Multi-Class Tree Ensembles,"Tree ensembles are one of the most widely used model classes.  However, these models are susceptible to adversarial examples, which are slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to verify the robustness of tree ensembles to such attacks. However, existing verification algorithms for tree ensembles are only able to analyze binary classifiers and hence address multiclass problems by reducing them to binary ones using a one-versus-other strategy. In this paper, we show that naively applying this strategy can yield incorrect results in certain situations. We address this shortcoming by proposing a novel approximate heuristic approach to verification for multiclass tree ensembles. Our approach is based on a novel generalization of the verification task, which we show emits other relevant verification queries.",['General'],[],"['Laurens Devos', 'Lorenzo Cascioli', 'Jesse Davis']","['KU Leuven', 'KU Leuven', 'KU Leuven']","['Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30090,Security,TTTS: Tree Test Time Simulation for Enhancing Decision Tree Robustness against Adversarial Examples,"Decision trees are widely used for addressing learning tasks involving tabular data. Yet, they are susceptible to adversarial attacks. In this paper, we present Tree Test Time Simulation (TTTS), a novel inference-time methodology that incorporates Monte Carlo simulations into decision trees to enhance their robustness. TTTS introduces a probabilistic modification to the decision path, without altering the underlying tree structure. Our comprehensive empirical analysis of 50 datasets yields promising results. Without the presence of any attacks, TTTS has successfully improved model performance from an AUC of 0.714 to 0.773. Under the challenging conditions of white-box attacks, TTTS demonstrated its robustness by boosting performance from an AUC of 0.337 to 0.680. Even when subjected to black-box attacks, TTTS maintains high accuracy and enhances the model's performance from an AUC of 0.628 to 0.719. Compared to defenses such as Feature Squeezing, TTTS proves to be much more effective. We also found that TTTS exhibits similar robustness in decision forest settings across different attacks.",['General'],[],"['Seffi Cohen', 'Ofir Arbili', 'Yisroel Mirsky', 'Lior Rokach']","['Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev']","['Israel', 'Israel', 'Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/30094,Security,P2BPO: Permeable Penalty Barrier-Based Policy Optimization for Safe RL,"Safe Reinforcement Learning (SRL) algorithms aim to learn a policy that maximizes the reward while satisfying the safety constraints. One of the challenges in SRL is that it is often difficult to balance the two objectives of reward maximization and safety constraint satisfaction. Existing algorithms utilize constraint optimization techniques like penalty-based, barrier penalty-based, and Lagrangian-based dual or primal policy optimizations methods. However, they suffer from training oscillations and approximation errors, which impact the overall learning objectives.  This paper proposes the Permeable Penalty Barrier-based Policy Optimization (P2BPO) algorithm that addresses this issue by allowing a small fraction of penalty beyond the penalty barrier, and a parameter is used to control this permeability. In addition, an adaptive penalty parameter is used instead of a constant one, which is initialized with a low value and increased gradually as the agent violates the safety constraints. We have also provided a theoretical proof of the proposed method's performance guarantee bound, which ensures that P2BPO can learn a policy satisfying the safety constraints with high probability while achieving a higher expected reward. Furthermore, we compare P2BPO with other SRL algorithms on various SRL tasks and demonstrate that it achieves better rewards while adhering to the constraints.",['General'],[],"['Sumanta Dey', 'Pallab Dasgupta', 'Soumyajit Dey']","['Indian Institute of Technology Kharagpur', 'Synopsys', 'Indian Institute of Technology Kharagpur']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30095,Transparency & Explainability,Trade-Offs in Fine-Tuned Diffusion Models between Accuracy and Interpretability,"Recent advancements in diffusion models have significantly impacted the trajectory of generative machine learning re-search, with many adopting the strategy of fine-tuning pre-trained models using domain-specific text-to-image datasets. Notably, this method has been readily employed for medical applications, such as X-ray image synthesis, leveraging the plethora of associated radiology reports. Yet, a prevailing concern is the lack of assurance on whether these models genuinely comprehend their generated content. With the evolution of text conditional image generation, these models have grown potent enough to facilitate object localization scrutiny. Our research underscores this advancement in the critical realm of medical imaging, emphasizing the crucial role of interpretability. We further unravel a consequential trade-off between image fidelity – as gauged by conventional metrics – and model interpretability in generative diffusion models. Specifically, the adoption of learnable text encoders when fine-tuning results in diminished interpretability. Our in-depth exploration uncovers the underlying factors responsible for this divergence. Consequently, we present a set of design principles for the development of truly interpretable generative models. Code is available at https://github.com/MischaD/chest-distillation.",['General'],[],"['Mischa Dombrowski', 'Hadrien Reynaud', 'Johanna P. Müller', 'Matthew Baugh', 'Bernhard Kainz']","['Friedrich-Alexander-Universität Erlangen-Nürnberg, DE', 'Imperial College London, UK', 'Friedrich-Alexander-Universität Erlangen-Nürnberg, DE', 'Imperial College London, UK', 'Friedrich-Alexander-Universität Erlangen-Nürnberg, DE\nImperial College London, UK']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30096,Fairness & Bias,From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space,"Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method for model correction on the concept level that explicitly reduces model sensitivity towards biases via gradient penalization. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures. Code and Appendix are available on https://github.com/frederikpahde/rrclarc.",['General'],[],"['Maximilian Dreyer', 'Frederik Pahde', 'Christopher J. Anders', 'Wojciech Samek', 'Sebastian Lapuschkin']","['Fraunhofer Heinrich Hertz Institute', 'Fraunhofer Heinrich Hertz Institute', 'Technical University of Berlin', 'Fraunhofer Heinrich Hertz Institute\nTechnical University of Berlin\nBIFOLD – Berlin Institute for the Foundations of Learning and Data', 'Fraunhofer Heinrich Hertz Institute']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30099,Security,Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain,"3D point cloud has been wildly used in security crucial domains, such as self-driving and 3D face recognition. Backdoor attack is a serious threat that usually destroy Deep Neural Networks (DNN) in the training stage. Though a few 3D backdoor attacks are designed to achieve guaranteed attack efficiency, their deformation will alarm human inspection. To obtain invisible backdoored point cloud, this paper proposes a novel 3D backdoor attack, named IBAPC, which generates backdoor trigger in the graph spectral domain. The effectiveness is grounded by the advantage of graph spectral signal that it can induce both global structure and local points to be responsible for the caused deformation in spatial domain. In detail, a new backdoor implanting function is proposed whose aim is to transform point cloud to graph spectral signal for conducting backdoor trigger. Then, we design a backdoor training procedure which updates the parameter of backdoor implanting function and victim 3D DNN alternately. Finally, the backdoored 3D DNN and its associated backdoor implanting function is obtained by finishing the backdoor training procedure. Experiment results suggest that IBAPC achieves SOTA attack stealthiness from three aspects including objective distance measurement, subjective human evaluation, graph spectral signal residual. At the same time, it obtains competitive attack efficiency. The code is available at https://github.com/f-lk/IBAPC.",['General'],[],"['Linkun Fan', 'Fazhi He', 'Tongzhen Si', 'Wei Tang', 'Bing Li']","['School of Computer Science, Wuhan University', 'School of Computer Science, Wuhan University', 'School of Information Science and Engineering, University of Ninan', 'School of Computer Science, Wuhan University', 'School of Computer Science, Wuhan University\nHubei Luojia Laboratory']","['China', 'China', '', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30102,Security,Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation,"Ensuring the safety of Reinforcement Learning (RL) is crucial for its deployment in real-world applications. Nevertheless, managing the trade-off between reward and safety during exploration presents a significant challenge. Improving reward performance through policy adjustments may adversely affect safety performance. In this study, we aim to address this conflicting relation by leveraging the theory of gradient manipulation. Initially, we analyze the conflict between reward and safety gradients. Subsequently, we tackle the balance between reward and safety optimization by proposing a soft switching policy optimization method, for which we provide convergence analysis. Based on our theoretical examination, we provide a safe RL framework to overcome the aforementioned challenge, and we develop a Safety-MuJoCo Benchmark to assess the performance of safe RL algorithms. Finally, we evaluate the effectiveness of our method on the Safety-MuJoCo Benchmark and a popular safe benchmark, Omnisafe. Experimental results demonstrate that our algorithms outperform several state-of-the-art baselines in terms of balancing reward and safety optimization.",['General'],[],"['Shangding Gu', 'Bilgehan Sel', 'Yuhao Ding', 'Lu Wang', 'Qingwei Lin', 'Ming Jin', 'Alois Knoll']","['Technical University of Munich', 'Virginia Tech', 'University of California - Berkeley', 'MIcrosoft', 'Microsoft Research', 'Virginia Tech', 'Technical University of Munich']","['', 'United States', 'United States', '', '', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30103,Transparency & Explainability,π-Light: Programmatic Interpretable Reinforcement Learning for Resource-Limited Traffic Signal Control,"The recent advancements in Deep Reinforcement Learning (DRL) have significantly enhanced the performance of adaptive Traffic Signal Control (TSC). However, DRL policies are typically represented by neural networks, which are over-parameterized black-box models. As a result, the learned policies often lack interpretability, and cannot be deployed directly in the real-world edge hardware due to resource constraints. In addition, the DRL methods often exhibit limited generalization performance, struggling to generalize the learned policy to other geographical regions. These factors limit the practical application of learning-based approaches. To address these issues, we suggest the use of an inherently interpretable program for representing the control policy. We present a new approach, Programmatic Interpretable reinforcement learning for traffic signal control (π-light), designed to autonomously discover non-differentiable programs. Specifically, we define a Domain Specific Language (DSL) and transformation rules for constructing programs, and utilize Monte Carlo Tree Search (MCTS) to find the optimal program in a discrete space. Extensive experiments demonstrate that our method consistently outperforms baseline approaches. Moreover, π-Light exhibits superior generalization capabilities compared to DRL, enabling training and evaluation across intersections from different cities. Finally, we analyze how the learned program policies can directly deploy on edge devices with extremely limited resources.",['General'],[],"['Yin Gu', 'Kai Zhang', 'Qi Liu', 'Weibo Gao', 'Longfei Li', 'Jun Zhou']","['Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science & School of Computer Science and Technology, University of Science and Technology of China\nState Key Laboratory of Cognitive Intelligence', 'Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science & School of Computer Science and Technology, University of Science and Technology of China\nState Key Laboratory of Cognitive Intelligence', 'Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science & School of Computer Science and Technology, University of Science and Technology of China\nState Key Laboratory of Cognitive Intelligence', 'Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science & School of Computer Science and Technology, University of Science and Technology of China\nState Key Laboratory of Cognitive Intelligence', 'Ant Financial Services Group', 'Ant Financial Services Group']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30104,Transparency & Explainability,Generative Model for Decision Trees,"Decision trees are among the most popular supervised models due to their interpretability and knowledge representation resembling human reasoning. Commonly-used decision tree induction algorithms are based on greedy top-down strategies. Although these approaches are known to be an efficient heuristic, the resulting trees are only locally optimal and tend to have overly complex structures. On the other hand, optimal decision tree algorithms attempt to create an entire decision tree at once to achieve global optimality. We place our proposal between these approaches by designing a generative model for decision trees. Our method first learns a latent decision tree space through a variational architecture using pre-trained decision tree models. Then, it adopts a genetic procedure to explore such latent space to find a compact decision tree with good predictive performance. We compare our proposal against classical tree induction methods, optimal approaches, and ensemble models. The results show that our proposal can generate accurate and shallow, i.e., interpretable, decision trees.",['General'],[],"['Riccardo Guidotti', 'Anna Monreale', 'Mattia Setzu', 'Giulia Volpi']","['University of Pisa, Pisa, Italy\nISTI-CNR, Pisa, Italy', 'University of Pisa, Pisa, Italy', 'University of Pisa, Pisa, Italy', 'University of Pisa, Pisa, Italy']","['Italy', 'Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/30106,Security,Provable Robustness against a Union of L_0 Adversarial Attacks,"Sparse or L0 adversarial attacks arbitrarily perturb an unknown subset of the features. L0 robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art L0 certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of L0 evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art L0 defenses, FPA is up to 3,000x faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.",['General'],[],"['Zayd Hammoudeh', 'Daniel Lowd']","['University of Oregon\nQualtrics, Inc.', 'University of Oregon']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30109,Transparency & Explainability,On the Concept Trustworthiness in Concept Bottleneck Models,"Concept Bottleneck Models (CBMs), which break down the reasoning process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck. However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a black box, giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues). The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement. To conduct a comprehensive analysis on this issue, in this study we establish a benchmark to assess the trustworthiness of concepts in CBMs. A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions. Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions. Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM. The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts. Our code is available at https://github.com/hqhQAQ/ProtoCBM.",['General'],[],"['Qihan Huang', 'Jie Song', 'Jingwen Hu', 'Haofei Zhang', 'Yong Wang', 'Mingli Song']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'State Grid Shandong Electric Power Company', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30110,Security,Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models,"Although recent personalization methods have democratized high-resolution image synthesis by enabling swift concept acquisition with minimal examples and lightweight computation, they also present an exploitable avenue for highly accessible backdoor attacks. This paper investigates a critical and unexplored aspect of text-to-image (T2I) diffusion models - their potential vulnerability to backdoor attacks via personalization. By studying the prompt processing of popular personalization methods (epitomized by Textual Inversion and DreamBooth), we have devised dedicated personalization-based backdoor attacks according to the different ways of dealing with unseen tokens and divide them into two families: nouveau-token and legacy-token backdoor attacks. In comparison to conventional backdoor attacks involving the fine-tuning of the entire text-to-image diffusion model, our proposed personalization-based backdoor attack method can facilitate more tailored, efficient, and few-shot attacks. Through comprehensive empirical study, we endorse the utilization of the nouveau-token backdoor attack due to its impressive effectiveness, stealthiness, and integrity, markedly outperforming the legacy-token backdoor attack.",['General'],[],"['Yihao Huang', 'Felix Juefei-Xu', 'Qing Guo', 'Jie Zhang', 'Yutong Wu', 'Ming Hu', 'Tianlin Li', 'Geguang Pu', 'Yang Liu']","['Nanyang Technological University, Singapore', 'New York University, USA', 'CFAR and IHPC, Agency for Science, Technology and Research (A*STAR), Singapore', 'Nanyang Technological University, Singapore', 'Nanyang Technological University, Singapore', 'Nanyang Technological University, Singapore', 'Nanyang Technological University, Singapore', 'East China Normal University, China', 'Nanyang Technological University, Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/30112,Fairness & Bias,Learning Fair Policies for Multi-Stage Selection Problems from Observational Data,"We consider the problem of learning fair policies for multi-stage selection problems from observational data. This problem arises in several high-stakes domains such as company hiring, loan approval, or bail decisions where outcomes (e.g., career success, loan repayment, recidivism) are only observed for those selected. We propose a multi-stage framework that can be augmented with various fairness constraints, such as demographic parity or equal opportunity. This problem is a highly intractable infinite chance-constrained program involving the unknown joint distribution of covariates and outcomes.  Motivated by the potential impact of selection decisions on people’s lives and livelihoods, we propose to focus on interpretable linear selection rules. Leveraging tools from causal inference and sample average approximation, we obtain an asymptotically consistent solution to this selection problem by solving a mixed binary conic optimization problem, which can be solved using standard off-the-shelf solvers. We conduct extensive computational experiments on a variety of datasets adapted from the UCI repository on which we show that our proposed approaches can achieve an 11.6% improvement in precision and a 38% reduction in the measure of unfairness compared to the existing selection policy.",['General'],[],"['Zhuangzhuang Jia', 'Grani A. Hanasusanto', 'Phebe Vayanos', 'Weijun Xie']","['University of Illinois Urbana-Champaign', 'University of Illinois Urbana-Champaign', 'University of Southern California', 'Georgia Institute of Technology']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30114,Fairness & Bias,Analysis of Differentially Private Synthetic Data: A Measurement Error Approach,"Differentially private (DP) synthetic datasets have been receiving significant attention from academia, industry, and government. However, little is known about how to perform statistical inference using DP synthetic datasets. Naive approaches that do not take into account the induced uncertainty due to the DP mechanism will result in biased estimators and invalid inferences. In this paper, we present a class of maximum likelihood estimator (MLE)-based easy-to-implement bias-corrected DP estimators with valid asymptotic confidence intervals (CI) for parameters in regression settings, by establishing the connection between additive DP mechanisms and measurement error models. Our simulation shows that our estimator has comparable performance to the widely used sufficient statistic perturbation (SSP) algorithm in some scenarios but with the advantage of releasing a synthetic dataset and obtaining statistically valid asymptotic CIs, which can achieve better coverage when compared to the naive CIs obtained by ignoring the DP mechanism.",['General'],[],"['Yangdi Jiang', 'Yi Liu', 'Xiaodong Yan', 'Anne-Sophie Charest', 'Linglong Kong', 'Bei Jiang']","['Department of Mathematical and Statistical Sciences, University of Alberta', 'Department of Mathematical and Statistical Sciences, University of Alberta', 'Zhongtai Securities Institute for Financial Studies, Shandong University', 'Department of Mathematics and Statistics, Laval University', 'Department of Mathematical and Statistical Sciences, University of Alberta', 'Department of Mathematical and Statistical Sciences, University of Alberta']","['', '', 'China', 'United States', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30113,Security,NeRFail: Neural Radiance Fields-Based Multiview Adversarial Attack,"Adversarial attacks, i.e., generating adversarial perturbations with a small magnitude to deceive deep neural networks, are important for investigating and improving model trustworthiness. Traditionally, the topic was scoped within 2D images without considering 3D multiview information. Benefiting from Neural Radiance Fields (NeRF), one can easily reconstruct a 3D scene with a Multi-Layer Perceptron (MLP) from given 2D views and synthesize photo-realistic renderings of novel vantages. This opens up a door to discussing the possibility of undertaking to attack multiview NeRF network with downstream tasks from different rendering angles, which we denote Neural Radiance Fiels-based multiview adversarial Attack (NeRFail). The goal is, given one scene and a subset of views, to deceive the recognition results of agnostic view angles as well as given views. To do so, we propose a transformation mapping from pixels to 3D points such that our attack generates multiview adversarial perturbations by attacking a subset of images with different views, intending to prevent the downstream classifier from correctly predicting images rendered by NeRF from other views. Experiments show that our multiview adversarial perturbations successfully obfuscate the downstream classifier at both known and unknown views. Notably, when retraining another NeRF on the perturbed training data, we show that the perturbation can be inherited and reproduced. The code can be found at https://github.com/jiang-wenxiang/NeRFail.",['General'],[],"['Wenxiang Jiang', 'Hanwei Zhang', 'Xi Wang', 'Zhongwen Guo', 'Hao Wang']","['Ocean University of China', 'Institute of Intelligent Software, Guangzhou\nSaarland University', 'LIX, Ecole Polytechnique, CNRS, Institut Polytechnique de Paris', 'Ocean University of China', 'Norwegian University of Science and Technology,\nSchool of Cyber Engineering, Xidian University, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30111,Security,Stronger and Transferable Node Injection Attacks,"Despite the increasing popularity of graph neural networks (GNNs), the security risks associated with their deployment have not been well explored. Existing works follow the standard adversarial attacks to maximize cross-entropy loss within an L-infinity norm bound. We analyze the robustness of GNNs against node injection attacks (NIAs) in black-box settings by allowing new nodes to be injected and attacked. In this work, we propose to design stronger and transferable NIAs. First, we propose margin aware attack (MAA) that uses a maximum margin loss to generate NIAs. We then propose a novel margin and direction aware attack (MDA) that diversifies the initial directions of MAA attack by minimizing the cosine similarity of the injected nodes with respect to their respective random initialization in addition to the maximization of max-margin loss. This makes the NIAs stronger. We further observe that using L2 norm of gradients in the attack step leads to an enhanced diversity amongst the node features, thereby further enhancing the strength of the attack. We incorporate transferability in NIAs by perturbing the surrogate model before generating the attack. An analysis of eigen spectrum density of the hessian of the loss emphasizes that perturbing the weights of the surrogate model improves the transferability. Our experimental results demonstrate that the proposed resilient node injection attack (R-NIA) consistently outperform PGD by margins about 7-15% on both large and small graph datasets. R-NIA is significantly stronger and transferable than existing NIAs on graph robustness benchmarks.",['General'],[],"['Samyak Jain', 'Tanima Dutta']","['Indian Institute of Technology (BHU) Varanasi', 'Indian Institute of Technology (BHU) Varanasi']","['India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30117,Security,DeepBern-Nets: Taming the Complexity of Certifying Neural Networks Using Bernstein Polynomial Activations and Precise Bound Propagation,"Formal certification of Neural Networks (NNs) is crucial for ensuring their safety, fairness, and robustness. Unfortunately, on the one hand, sound and complete certification algorithms of ReLU-based NNs do not scale to large-scale NNs. On the other hand, incomplete certification algorithms are easier to compute, but they result in loose bounds that deteriorate with the depth of NN, which diminishes their effectiveness. In this paper, we ask the following question; can we replace the ReLU activation function with one that opens the door to incomplete certification algorithms that are easy to compute but can produce tight bounds on the NN's outputs? We introduce DeepBern-Nets, a class of NNs with activation functions based on Bernstein polynomials instead of the commonly used ReLU activation. Bernstein polynomials are smooth and differentiable functions with desirable properties such as the so-called range enclosure and subdivision properties. We design a novel Interval Bound Propagation (IBP) algorithm, called Bern-IBP, to efficiently compute tight bounds on DeepBern-Nets outputs. Our approach leverages the properties of Bernstein polynomials to improve the tractability of neural network certification tasks while maintaining the accuracy of the trained networks. We conduct experiments in adversarial robustness and reachability analysis settings to assess the effectiveness of the approach. Our proposed framework achieves high certified accuracy for adversarially-trained NNs, which is often a challenging task for certifiers of ReLU-based NNs. This work establishes Bernstein polynomial activation as a promising alternative for improving NN certification tasks across various NNs applications.",['General'],[],"['Haitham Khedr', 'Yasser Shoukry']","['University of California, Irvine', 'University of California, Irvine']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30115,Fairness & Bias,Chasing Fairness in Graphs: A GNN Architecture Perspective,"There has been significant progress in improving the performance of graph neural networks (GNNs) through enhancements in graph data, model architecture design, and training strategies. For fairness in graphs, recent studies achieve fair representations and predictions through either graph data pre-processing (e.g., node feature masking, and topology rewiring) or fair training strategies (e.g., regularization, adversarial debiasing, and fair contrastive learning). How to achieve fairness in graphs from the model architecture perspective is less explored. More importantly, GNNs exhibit worse fairness performance compared to multilayer perception since their model architecture (i.e., neighbor aggregation) amplifies biases. To this end, we aim to achieve fairness via a new GNN architecture. We propose Fair Message Passing (FMP) designed within a unified optimization framework for GNNs. Notably, FMP explicitly renders sensitive attribute usage in forward propagation for node classification task using cross-entropy loss without data pre-processing. In FMP, the aggregation is first adopted to utilize neighbors' information and then the bias mitigation step explicitly pushes demographic group node presentation centers together. In this way, FMP scheme can aggregate useful information from neighbors and mitigate bias to achieve better fairness and prediction tradeoff performance.  Experiments on node classification tasks demonstrate that the proposed FMP outperforms several baselines in terms of fairness and accuracy on three real-world datasets. The code is available at https://github.com/zhimengj0326/FMP.",['General'],[],"['Zhimeng Jiang', 'Xiaotian Han', 'Chao Fan', 'Zirui Liu', 'Na Zou', 'Ali Mostafavi', 'Xia Hu']","['Texas A&M University', 'Texas A&M University', 'Clemson University', 'Rice University', 'University of Houston', 'Texas A&M University', 'Rice University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30118,Privacy & Data Governance,Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer Level Attack and Knowledge Distillation,"Recently, serious concerns have been raised about the privacy issues related to training datasets in machine learning algorithms when including personal data. Various regulations in different countries, including the GDPR grant individuals to have personal data erased, known as ‘the right to be forgotten’ or ‘the right to erasure’. However, there has been less research on effectively and practically deleting the requested personal data from the training set while not jeopardizing the overall machine learning performance. In this work, we propose a fast and novel machine unlearning paradigm at the layer level called layer attack unlearning, which is highly accurate and fast compared to existing machine unlearning algorithms. We introduce the Partial-PGD algorithm to locate the samples to forget efficiently. In addition, we only use the last layer of the model inspired by the Forward-Forward algorithm for unlearning process. Lastly, we use Knowledge Distillation (KD) to reliably learn the decision boundaries from the teacher using soft label information to improve accuracy performance. We conducted extensive experiments with SOTA machine unlearning models and demonstrated the effectiveness of our approach for accuracy and end-to-end unlearning performance.",['General'],[],"['Hyunjune Kim', 'Sangyong Lee', 'Simon S. Woo']","['Sungkyunkwan University', 'Sungkyunkwan University', 'Sungkyunkwan University']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/30120,Security,OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples,"Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors lack robustness against attacks: they degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, a malicious user might attempt to deliberately evade the detectors based on detection results, but this has not been assumed in previous studies. In this paper, we propose OUTFOX, a framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output. In this framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are harder to detect, while the detector uses the adversarially generated essays as examples for in-context learning to learn to detect essays from a strong attacker. Experiments in the domain of student essays show that the proposed detector improves the detection performance on the attacker-generated texts by up to +41.3 points F1-score. Furthermore, the proposed detector shows a state-of-the-art detection performance: up to 96.9 points F1-score, beating existing detectors on non-attacked texts. Finally, the proposed attacker drastically degrades the performance of detectors by up to -57.0 points F1-score, massively outperforming the baseline paraphrasing method for evading detection.",['General'],[],"['Ryuto Koike', 'Masahiro Kaneko', 'Naoaki Okazaki']","['Tokyo Institute of Technology', 'MBZUAI\nTokyo Institute of Technology', 'Tokyo Institute of Technology']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30121,Security,Accelerating Adversarially Robust Model Selection for Deep Neural Networks via Racing,"Recent research has introduced several approaches to formally verify the robustness of neural network models against perturbations in their inputs, such as the ones that occur in adversarial attacks. At the same time, this particular verification task is known to be computationally challenging. More specifically, assessing the robustness of a neural network against input perturbations can easily take several hours of compute time per input vector, even when using state-of-the-art verification approaches. In light of this, it becomes challenging to select from a given set of neural network models the one that is best in terms of robust accuracy, i.e., the fraction of instances for which the model is known to be robust against adversarial perturbations, especially when given limited computing resources. To tackle this problem, we propose a racing method specifically adapted to the domain of robustness verification. This racing method utilises Delta-values, which can be seen as an efficiently computable proxy for the distance of a given input to a neural network model to the decision boundary. We present statistical evidence indicating significant differences in the empirical cumulative distribution between robust and non-robust inputs as a function of Delta-values. Using this information, we show that it is possible to reliably expose vulnerabilities in the model with relatively few input iterations. Overall, when applied to selecting the most robust network from sets of 31 MNIST and 27 CIFAR-10 networks, our proposed method achieves speedups of a factor of 108 and 42, respectively, in terms of cumulative running time compared to standard local robustness verification on the complete testing sets.",['General'],[],"['Matthias König', 'Holger H. Hoos', 'Jan N. van Rijn']","['Leiden Institute of Advanced Computer Science, Leiden University', 'Leiden Institute of Advanced Computer Science, Leiden University\nChair for AI Methodology, RWTH Aachen University', 'Leiden Institute of Advanced Computer Science, Leiden University']","['Netherlands', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/30127,Security,Promoting Counterfactual Robustness through Diversity,"Counterfactual explanations shed light on the decisions of black-box models by explaining how an input can be altered to obtain a favourable decision from the model (e.g., when a loan application has been rejected). However, as noted recently, counterfactual explainers may lack robustness in the sense that a minor change in the input can cause a major change in the explanation. This can cause confusion on the user side and open the door for adversarial attacks. In this paper, we study some sources of non-robustness.  While there are fundamental reasons for why an explainer that returns a single counterfactual cannot be robust in all instances, we show that some interesting robustness guarantees can be given by reporting  multiple rather than a single counterfactual. Unfortunately, the number of counterfactuals that need to be reported for the theoretical guarantees to hold can be prohibitively large. We therefore propose an approximation algorithm that uses a diversity criterion to select a feasible number of most relevant explanations and study its robustness empirically. Our experiments indicate that our method improves the state-of-the-art in generating robust explanations, while maintaining other desirable properties and providing competitive computational performance.",['General'],[],"['Francesco Leofante', 'Nico Potyka']","['Department of Computing, Imperial College London, UK', 'School of Computer Science and Informatics, Cardiff University, UK']","['United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30128,Security,Revisiting the Information Capacity of Neural Network Watermarks: Upper Bound Estimation and Beyond,"To trace the copyright of deep neural networks, an owner can embed its identity information into its model as a watermark. The capacity of the watermark quantify the maximal volume of information that can be verified from the watermarked model. Current studies on capacity focus on the ownership verification accuracy under ordinary removal attacks and fail to capture the relationship between robustness and fidelity. This paper studies the capacity of deep neural network watermarks from an information theoretical perspective. We propose a new definition of deep neural network watermark capacity analogous to channel capacity, analyze its properties, and design an algorithm that yields a tight estimation of its upper bound under adversarial overwriting. We also propose a universal non-invasive method to secure the transmission of the identity message beyond capacity by multiple rounds of ownership verification.  Our observations provide evidence for neural network owners and defenders that are curious about the tradeoff between the integrity of their ownership and the performance degradation of their products.",['General'],[],"['Fangqi Li', 'Haodong Zhao', 'Wei Du', 'Shilin Wang']","['School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University', 'School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University', 'School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University', 'School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30129,Security,PointCVaR: Risk-Optimized Outlier Removal for Robust 3D Point Cloud Classification,"With the growth of 3D sensing technology, the deep learning system for 3D point clouds has become increasingly important, especially in applications such as autonomous vehicles where safety is a primary concern. However, there are growing concerns about the reliability of these systems when they encounter noisy point clouds, either occurring naturally or introduced with malicious intent. This paper highlights the challenges of point cloud classification posed by various forms of noise, from simple background noise to malicious adversarial/backdoor attacks that can intentionally skew model predictions. While there's an urgent need for optimized point cloud denoising, current point outlier removal approaches, an essential step for denoising, rely heavily on handcrafted strategies and are not adapted for higher-level tasks, such as classification. To address this issue, we introduce an innovative point outlier cleansing method that harnesses the power of downstream classification models. Using gradient-based attribution analysis, we define a novel concept: point risk. Drawing inspiration from tail risk minimization in finance, we recast the outlier removal process as an optimization problem, named PointCVaR. Extensive experiments show that our proposed technique not only robustly filters diverse point cloud outliers but also consistently and significantly enhances existing robust methods for point cloud classification. A notable feature of our approach is its effectiveness in defending against the latest threat of backdoor attacks in point clouds.",['General'],[],"['Xinke Li', 'Junchi Lu', 'Henghui Ding', 'Changsheng Sun', 'Joey Tianyi Zhou', 'Yeow Meng Chee']","['National University of Singapore', 'Nanyang Technological University', 'Nanyang Technological University', 'National University of Singapore', 'Institute of High Performance Computing (IHPC), A*STAR\nCentre for Frontier AI Research (CFAR), A*STAR', 'National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore', '', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/30130,Security,Game-Theoretic Unlearnable Example Generator,"Unlearnable example attacks are data poisoning attacks aiming to degrade the clean test accuracy of deep learning by adding imperceptible perturbations to the training samples, which can be formulated as a bi-level optimization problem. However, directly solving this optimization problem is intractable for deep neural networks. In this paper, we investigate unlearnable example attacks from a game-theoretic perspective, by formulating the attack as a nonzero sum Stackelberg game. First, the existence of game equilibria is proved under the normal setting and the adversarial training setting.  It is shown that the game equilibrium gives the most powerful poison attack in that the victim has the lowest test accuracy among all networks within the same hypothesis space when certain loss functions are used. Second, we propose a novel attack method, called the Game Unlearnable Example (GUE), which has three main gradients.  (1) The poisons are obtained by directly solving the equilibrium of the Stackelberg game with a first-order algorithm.  (2) We employ an autoencoder-like generative network model as the poison attacker. (3) A novel payoff function is introduced to evaluate the performance of the poison. Comprehensive experiments demonstrate that GUE can effectively poison the model in various scenarios.  Furthermore, the GUE still works by using a relatively small percentage of the training data to train the generator, and the poison generator can generalize to unseen data well. Our implementation code can be found at https://github.com/hong-xian/gue.",['General'],[],"['Shuang Liu', 'Yihan Wang', 'Xiao-Shan Gao']","['Academy of Mathematics and Systems Science, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences\nUniversity of Chinese Academy of Sciences']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30131,Security,Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning,"Backdoors on federated learning will be diluted by subsequent benign updates. This is reflected in the significant reduction of attack success rate as iterations increase, ultimately failing. We use a new metric to quantify the degree of this weakened backdoor effect, called attack persistence. Given that research to improve this performance has not been widely noted, we propose a Full Combination Backdoor Attack (FCBA) method. It aggregates more combined trigger information for a more complete backdoor pattern in the global model. Trained backdoored global model is more resilient to benign updates, leading to a higher attack success rate on the test set. We test on three datasets and evaluate with two models across various settings. FCBA's persistence outperforms SOTA federated learning backdoor attacks. On GTSRB, post-attack 120 rounds, our attack success rate rose over 50% from baseline. The core code of our method is available at https://github.com/PhD-TaoLiu/FCBA.",['General'],[],"['Tao Liu', 'Yuhang Zhang', 'Zhu Feng', 'Zhiqin Yang', 'Chen Xu', 'Dapeng Man', 'Wu Yang']","['College of Computer Science and Technology, Harbin Engineering University, China', 'College of Computer Science and Technology, Harbin Engineering University, China', 'College of Computer Science and Technology, Harbin Engineering University, China', 'Southampton Ocean Engineering Joint Institute, Harbin Engineering University, China', 'College of Computer Science and Technology, Harbin Engineering University, China', 'College of Computer Science and Technology, Harbin Engineering University, China', 'College of Computer Science and Technology, Harbin Engineering University, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30132,Security,Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning,"Safety in goal directed Reinforcement Learning (RL) settings has typically been handled through constraints over trajectories and have demonstrated good performance in primarily short horizon tasks. In this paper, we are specifically interested in the problem of solving temporally extended decision making problems such as robots cleaning different areas in a house while avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge to move to a charging dock; in the presence of complex safety constraints.  Our key contribution is a  (safety) Constrained Search with Hierarchical Reinforcement Learning (CoSHRL) mechanism that combines an upper level constrained search agent (which computes a reward maximizing policy from a given start to a far away goal state while satisfying cost constraints) with a low-level goal conditioned RL agent (which estimates cost and reward values to move between nearby states). A major advantage of CoSHRL is that it can handle constraints on the cost value distribution (e.g., on Conditional Value at Risk, CVaR) and can adjust to flexible constraint thresholds without retraining. We perform extensive experiments with different types of safety constraints to demonstrate the utility of our approach over leading approaches in constrained and hierarchical RL.",['General'],[],"['Yuxiao Lu', 'Arunesh Sinha', 'Pradeep Varakantham']","['Singpore Management University', 'Rutgers University', 'Singapore Management University']","['Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/30134,Security,Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees,"Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called ε-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight —with provable probabilistic guarantees— lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.",['General'],[],"['Luca Marzari', 'Davide Corsi', 'Enrico Marchesini', 'Alessandro Farinelli', 'Ferdinando Cicalese']","['University of Verona', 'University of Verona', 'Massachusetts Institute of Technology', 'University of Verona', 'University of Verona']","['Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/30135,Security,Divide-and-Aggregate Learning for Evaluating Performance on Unlabeled Data,"Artificial Intelligence (AI) models have become an integral part of modern society, significantly improving human lives. However, ensuring the reliability and safety of these models is of paramount importance. One critical aspect is the continuous monitoring and verification of model performance to prevent any potential risks. Real-time online evaluation of AI models is necessary to maintain their effectiveness and mitigate any harm caused by performance degradation. The traditional approach to model evaluation involves supervised methods that rely on manual labeling to compare results with model predictions. Unfortunately, this method is not suitable for online model monitoring due to its inherent lag and high cost. While there have been attempts to explore free-label model evaluation, these approaches often consider only the global features of the entire dataset. Additionally, they can only perform model evaluation based on a single dimension of model confidence or features. In this paper, we propose a novel approach called Divide-and-Aggregate Learning (DAL) for unsupervised model evaluation. Our method addresses the limitations of previous approaches by dividing the output of the model into buckets, capturing local information of the distribution. We then aggregate this local information to obtain global information and further represent the relationship between the distribution and model performance. Importantly, our method can simultaneously handle the confidence distribution and feature distribution of the model output. Extensive experiments have been conducted to demonstrate the effectiveness of our DAL model. The results show that our approach outperforms previous methods on four widely used datasets. We will make our source code publicly available.",['General'],[],"['Shuyu Miao', 'Jian Liu', 'Lin Zheng', 'Hong Jin']","['Ant Group', 'Ant Group', 'Ant Group', 'Ant Group']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30138,Security,Feature Unlearning for Pre-trained GANs and VAEs,"We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST, CelebA, and FFHQ datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is more robust under the presence of malicious parties.",['General'],[],"['Saemi Moon', 'Seunghyuk Cho', 'Dongwoo Kim']","['POSTECH, CSE', 'POSTECH, GSAI', 'POSTECH, CSE\nPOSTECH, GSAI']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/30137,Security,Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration for Control Policy Synthesis,"This paper addresses the problem of maintaining safety during training in Reinforcement Learning (RL), such that the safety constraint violations are bounded at any point during learning. As enforcing safety during training might severely limit the agent’s exploration, we propose here a new architecture that handles the trade-off between efficient progress and safety during exploration. As the exploration progresses, we update via Bayesian inference Dirichlet-Categorical models of the transition probabilities of the Markov decision process that describes the environment dynamics. We then propose a way to approximate moments of belief about the risk associated to the action selection policy. We demonstrate that this approach can be easily interleaved with RL and we present experimental results to showcase the performance of the overall architecture.",['General'],[],"['Rohan Mitta', 'Hosein Hasanbeig', 'Jun Wang', 'Daniel Kroening', 'Yiannis Kantaros', 'Alessandro Abate']","['University of Oxford, Oxford, UK', 'Microsoft Research', 'Washington University in St. Louis', 'Amazon', 'Washington University in St. Louis', 'University of Oxford, Oxford, UK']","['United Kingdom', '', 'United States', 'Japan', 'United States', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30136,Security,SentinelLMs: Encrypted Input Adaptation and Fine-Tuning of Language Models for Private and Secure Inference,"This paper addresses the privacy and security concerns associated with deep neural language models, which serve as crucial components in various modern AI-based applications. These models are often used after being pre-trained and fine-tuned for specific tasks, with deployment on servers accessed through the internet. However, this introduces two fundamental risks: (a) the transmission of user inputs to the server via the network gives rise to interception vulnerabilities, and (b) privacy concerns emerge as organizations that deploy such models store user data with restricted context. To address this, we propose a novel method to adapt and fine-tune transformer-based language models on passkey-encrypted user-specific text. The original pre-trained language model first undergoes a quick adaptation (without any further pre-training) with a series of irreversible transformations applied to the tokenizer and token embeddings. This enables the model to perform inference on encrypted inputs while preventing reverse engineering of text from model parameters and intermediate outputs. After adaptation, models are fine-tuned on encrypted versions of existing training datasets. Experimental evaluation employing adapted versions of renowned models (e.g., BERT, RoBERTa) across established benchmark English and multilingual datasets for text classification and sequence labeling shows that encrypted models achieve performance parity with their original counterparts. This serves to safeguard performance, privacy, and security cohesively.",['General'],[],"['Abhijit Mishra', 'Mingda Li', 'Soham Deo']","['University of Texas at Austin', 'University of Texas at Austin', 'University of Texas at Austin']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30139,Security,Reward Certification for Policy Smoothed Reinforcement Learning,"Reinforcement Learning (RL) has achieved remarkable success in safety-critical areas, but it can be weakened by adversarial attacks. Recent studies have introduced ``smoothed policies"" to enhance its robustness. Yet, it is still challenging to establish a provable guarantee to certify the bound of its total reward. Prior methods relied primarily on computing bounds using Lipschitz continuity or calculating the probability of cumulative reward being above specific thresholds. However, these techniques are only suited for continuous perturbations on the RL agent's observations and are restricted to perturbations bounded by the l2-norm. To address these limitations, this paper proposes a general  black-box certification method, called ReCePS, which is capable of directly certifying the cumulative reward of the smoothed policy under various lp-norm bounded perturbations. Furthermore, we extend our methodology to certify perturbations on action spaces. Our approach leverages f-divergence to measure the distinction between the original distribution and the perturbed distribution, subsequently determining the certification bound by solving a convex optimisation problem. We provide a comprehensive theoretical analysis and run experiments in multiple environments. Our results show that our method not only improves the tightness of certified lower bound of the mean cumulative reward but also demonstrates better efficiency than state-of-the-art methods.",['General'],[],"['Ronghui Mu', 'Leandro Soriano Marcolino', 'Yanghao Zhang', 'Tianle Zhang', 'Xiaowei Huang', 'Wenjie Ruan']","['University of Liverpool', 'Lancaster University', 'University of Liverpool', 'University of Liverpool', 'University of Liverpool', 'University of Liverpool']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30141,Security,Neural Closure Certificates,"Notions of transition invariants and closure certificates have seen recent use in the formal verification of controlled dynamical systems against \omega-regular properties. Unfortunately, existing approaches face limitations in two directions. First, they require a closed-form mathematical expression representing the model of the system. Such an expression may be difficult to find, too complex to be of any use, or unavailable due to security or privacy constraints. Second, finding such invariants typically rely on optimization techniques such as sum-of-squares (SOS) or satisfiability modulo theory (SMT) solvers. This restricts the classes of systems that need to be formally verified. To address these drawbacks, we introduce a notion of neural closure certificates. We present a data-driven algorithm that trains a neural network to represent a closure certificate. Our approach is formally correct under some mild assumptions, i.e., one is able to formally show that the unknown system  satisfies the \omega-regular property of interest if a neural closure certificate can be computed. Finally, we demonstrate the efficacy of our approach with relevant case studies.",['General'],[],"['Alireza Nadali', 'Vishnu Murali', 'Ashutosh Trivedi', 'Majid Zamani']","['University of Colorado Boulder', 'University of Colorado Boulder', 'University of Colorado Boulder', 'University of Colorado Boulder']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30142,Transparency & Explainability,SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models,"Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, we start with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two open source generative language models and we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We demonstrate that the deliberate design of the templates in our benchmark (e.g., adding biasing text to the prompt or using different verbs that change the answer that indicates bias) impacts the model tendencies to generate socially biased output. Additionally, through manual evaluation, we discover problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning.  Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.",['General'],[],"['Manish Nagireddy', 'Lamogha Chiazor', 'Moninder Singh', 'Ioana Baldini']","['IBM Research', 'IBM Research', 'IBM Research', 'IBM Research']","['', '', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30145,Transparency & Explainability,Q-SENN: Quantized Self-Explaining Neural Networks,"Explanations in Computer Vision are often desired, but most Deep Neural Networks can only provide saliency maps with questionable faithfulness. Self-Explaining Neural Networks (SENN) extract interpretable concepts with fidelity, diversity, and grounding to combine them linearly for decision-making. While they can explain what was recognized, initial realizations lack accuracy and general applicability. We propose the Quantized-Self-Explaining Neural Network “Q-SENN”. Q-SENN satisfies or exceeds the desiderata of SENN while being applicable to more complex datasets and maintaining most or all of the accuracy of an uninterpretable baseline model, outperforming previous work in all considered metrics. Q-SENN describes the relationship between every class and feature as either positive, negative or neutral instead of an arbitrary number of possible relations, enforcing more binary human-friendly features. Since every class is assigned just 5 interpretable features on average, Q-SENN shows convincing local and global interpretability. Additionally, we propose a feature alignment method, capable of aligning learned features with human language-based concepts without additional supervision. Thus, what is learned can be more easily verbalized. The code is published: https://github.com/ThomasNorr/Q-SENN",['General'],[],"['Thomas Norrenbrock', 'Marco Rudolph', 'Bodo Rosenhahn']","['Leibniz University Hannover\nInstitute for Information Processing (tnt)\nL3S', 'Leibniz University Hannover\nInstitute for Information Processing (tnt)\nL3S', 'Leibniz University Hannover\nInstitute for Information Processing (tnt)\nL3S']","['Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30146,Security,Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is crucial to safety-critical machine learning applications and has been extensively studied. While recent studies have predominantly focused on classifier-based methods, research on deep generative model (DGM)-based methods have lagged relatively. This disparity may be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods to unknown OOD inputs than to their known training data. This paper focuses on explaining the underlying mechanism of this phenomenon. We propose a hypothesis that less complex images concentrate in high-density regions in the latent space, resulting in a higher likelihood assignment in the Normalizing Flow (NF). We experimentally demonstrate its validity for five NF architectures, concluding that their likelihood is untrustworthy. Additionally, we show that this problem can be alleviated by treating image complexity as an independent variable. Finally, we provide evidence of the potential applicability of our hypothesis in another DGM, PixelCNN++.",['General'],[],"['Genki Osada', 'Tsubasa Takahashi', 'Takashi Nishide']","['LINE Corporation', 'LINE Corporation', 'University of Tsukuba']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30150,Security,Visual Adversarial Examples Jailbreak Aligned Large Language Models,"Warning: this paper contains data, prompts, and model outputs that are offensive in nature.  Recently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",['General'],[],"['Xiangyu Qi', 'Kaixuan Huang', 'Ashwinee Panda', 'Peter Henderson', 'Mengdi Wang', 'Prateek Mittal']","['Princeton University', 'Princeton University', 'Princeton University', 'Princeton University', 'Princeton University', 'Princeton University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30151,Transparency & Explainability,Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance,"While modern explanation methods have been shown to be inconsistent and contradictory, the explainability of black-box models nevertheless remains desirable. When the role of explanations extends from understanding models to aiding decision making, the semantics of explanations is not always fully understood – to what extent do explanations ``explain” a decision and to what extent do they merely advocate for a decision? Can we help humans gain insights from explanations accompanying correct predictions and not over-rely on incorrect predictions advocated for by explanations? With this perspective in mind, we introduce the notion of dissenting explanations: conflicting predictions with accompanying explanations. We first explore the advantage of dissenting explanations in the setting of model multiplicity, where multiple models with similar performance may have different predictions. Through a human study on the task of identifying deceptive reviews, we demonstrate that dissenting explanations reduce overreliance on model predictions, without reducing overall accuracy. Motivated by the utility of dissenting explanations we present both global and local methods for their generation.",['General'],[],"['Omer Reingold', 'Judy Hanwen Shen', 'Aditi Talati']","['Stanford University', 'Stanford University', 'Stanford University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30152,Transparency & Explainability,I-CEE: Tailoring Explanations of Image Classification Models to User Expertise,"Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate ""one-size-fits-all'' explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different users. We posit that by tailoring the example set to user expertise, I-CEE can better facilitate users' understanding and simulatability of the model. To evaluate our approach, we conduct detailed experiments in both simulation and with human participants (N = 100) on multiple datasets. Experiments with simulated users show that I-CEE improves users' ability to accurately predict the model's decisions (simulatability) compared to baselines, providing promising preliminary results. Experiments with human participants demonstrate that our method significantly improves user simulatability accuracy, highlighting the importance of human-centered XAI.",['General'],[],"['Yao Rong', 'Peizhu Qian', 'Vaibhav Unhelkar', 'Enkelejda Kasneci']","['Technical University of Munich', 'Rice University', 'Rice University', 'Technical University of Munich']","['', 'United States', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30153,Privacy & Data Governance,A Simple and Practical Method for Reducing the Disparate Impact of Differential Privacy,"Differentially private (DP) mechanisms have been deployed in a variety of high-impact social settings (perhaps most notably by the U.S. Census). Since all DP mechanisms involve adding noise to results of statistical queries, they are expected to impact our ability to accurately analyze and learn from data, in effect trading off privacy with utility. Alarmingly, the impact of DP on utility can vary significantly among different sub-populations. A simple way to reduce this disparity is with stratification. First compute an independent private estimate for each group in the data set (which may be the intersection of several protected classes), then, to compute estimates of global statistics, appropriately recombine these group estimates. Our main observation is that naive stratification often yields high-accuracy estimates of population-level statistics, without the need for additional privacy budget. We support this observation theoretically and empirically. Our theoretical results center on the private mean estimation problem, while our empirical results center on extensive experiments on private data synthesis to demonstrate the effectiveness of stratification on a variety of private mechanisms. Overall, we argue that this straightforward approach provides a strong baseline against which future work on reducing utility disparities of DP mechanisms should be compared.",['General'],[],"['Lucas Rosenblatt', 'Julia Stoyanovich', 'Christopher Musco']","['New York University', 'New York University', 'New York University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30154,Transparency & Explainability,Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations,"Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.",['General'],[],"['Mikołaj Sacha', 'Bartosz Jura', 'Dawid Rymarczyk', 'Łukasz Struski', 'Jacek Tabor', 'Bartosz Zieliński']","['Faculty of Mathematics and Computer Science, Jagiellonian University\nDoctoral School of Exact and Natural Sciences, Jagiellonian University', 'Łukasiewicz Research Network – Poznań Institute of Technology\nFaculty of Management and Social Communication, Jagiellonian University', 'Faculty of Mathematics and Computer Science, Jagiellonian University\nDoctoral School of Exact and Natural Sciences, Jagiellonian University\nArdigen SA', 'Faculty of Mathematics and Computer Science, Jagiellonian University', 'Faculty of Mathematics and Computer Science, Jagiellonian University', 'Faculty of Mathematics and Computer Science, Jagiellonian University\nIDEAS NCBR']","['Poland', 'Poland', 'Poland', 'Poland', 'Poland', 'Poland']"
https://ojs.aaai.org/index.php/AAAI/article/view/30156,Fairness & Bias,Towards Fairer Centroids in K-means Clustering,"There has been much recent interest in developing fair clustering algorithms that seek to do justice to the representation of groups defined along sensitive attributes such as race and sex. Within the centroid clustering paradigm, these algorithms are seen to generate clusterings where different groups are disadvantaged within different clusters with respect to their representativity, i.e., distance to centroid. In view of this deficiency, we propose a novel notion of cluster-level centroid fairness that targets the representativity unfairness borne by groups within each cluster, along with a metric to quantify the same. Towards operationalising this notion, we draw on ideas from political philosophy aligned with consideration for the worst-off group to develop Fair-Centroid; a new clustering method that focusses on enhancing the representativity of the worst-off group within each cluster. Our method uses an iterative optimisation paradigm wherein an initial cluster assignment is refined by reassigning objects to clusters such that the worst-off group in each cluster is benefitted. We compare our notion with a related fairness notion and show through extensive empirical evaluations on real-world datasets that our method significantly enhances cluster-level centroid fairness at low impact on cluster coherence.",['General'],[],"['Stanley Simoes', 'Deepak P', 'Muiris MacCarthaigh']","[""Queen's University Belfast"", ""Queen's University Belfast"", ""Queen's University Belfast""]","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30158,Privacy & Data Governance,Bidirectional Contrastive Split Learning for Visual Question Answering,"Visual Question Answering (VQA) based on multi-modal data facilitates real-life applications such as home robots and medical diagnoses. One significant challenge is to devise a robust decentralized learning framework for various client models where centralized data collection is refrained due to confidentiality concerns. This work aims to tackle privacy-preserving VQA by decoupling a multi-modal model into representation modules and a contrastive module, leveraging inter-module gradients sharing and inter-client weight sharing. To this end, we propose Bidirectional Contrastive Split Learning (BiCSL) to train a global multi-modal model on the entire data distribution of decentralized clients. We employ the contrastive loss that enables a more efficient self-supervised learning of decentralized modules. Comprehensive experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models, demonstrating the effectiveness of the proposed method. Furthermore, we inspect BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently, BiCSL shows significantly enhanced resilience when exposed to the multi-modal adversarial attack compared to the centralized learning method, which provides a promising approach to decentralized multi-modal learning.",['General'],[],"['Yuwei Sun', 'Hideya Ochiai']","['The University of Tokyo\nRIKEN AIP', 'The University of Tokyo']","['Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30160,Transparency & Explainability,Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention,"Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However, the enigmatic ``black-box'' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications. While past approaches, such as attention visualization, pivotal subnetwork extraction, and concept-based analyses, offer some insight, they often focus on either local or global explanations within a single dimension, occasionally falling short in providing comprehensive clarity. In response, we propose a novel methodology anchored in sparsity-guided techniques, aiming to provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively integrates sparsity to elucidate three intertwined layers of interpretation: input, subnetwork, and concept levels. In addition, the newly introduced dimension of interpretable inference-time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real-world datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors, setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.",['General'],[],"['Zhen Tan', 'Tianlong Chen', 'Zhenyu Zhang', 'Huan Liu']","['Arizona State University', 'University of North Carolina at Chapel Hill', 'University of Texas at Austin', 'Arizona State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30161,Fairness & Bias,Toward More Generalized Malicious URL Detection Models,"This paper reveals a data bias issue that can profoundly hinder the performance of machine learning models in malicious URL detection. We describe how such bias can be diagnosed using interpretable machine learning techniques and further argue that such biases naturally exist in the real world security data for training a classification model. To counteract these challenges, we propose a debiased training strategy that can be applied to most deep-learning based models to alleviate the negative effects of the biased features. The solution is based on the technique of adversarial training to train deep neural networks learning invariant embedding from biased data. Through extensive experimentation, we substantiate that our innovative strategy fosters superior generalization capabilities across both CNN-based and RNN-based detection models. The findings presented in this work not only expose a latent issue in the field but also provide an actionable remedy, marking a significant step forward in the pursuit of more reliable and robust malicious URL detection.",['General'],[],"['Yun-Da Tsai', 'Cayon Liow', 'Yin Sheng Siang', 'Shou-De Lin']","['National Taiwan University', 'National Taiwan University', 'National Taiwan University', 'National Taiwan University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30163,Security,Pure-Past Action Masking,"We present Pure-Past Action Masking (PPAM), a lightweight approach to action masking for safe reinforcement learning. In PPAM, actions are disallowed (“masked”) according to specifications expressed in Pure-Past Linear Temporal Logic (PPLTL). PPAM can enforce non-Markovian constraints, i.e., constraints based on the history of the system, rather than just the current state of the (possibly hidden) MDP. The features used in the safety constraint need not be the same as those used by the learning agent, allowing a clear separation of concerns between the safety constraints and reward specifications of the (learning) agent. We prove formally that an agent trained with PPAM can learn any optimal policy that satisfies the safety constraints, and that they are as expressive as shields, another approach to enforce non-Markovian constraints in RL. Finally, we provide empirical results showing how PPAM can guarantee constraint satisfaction in practice.",['General'],[],"['Giovanni Varricchione', 'Natasha Alechina', 'Mehdi Dastani', 'Giuseppe De Giacomo', 'Brian Logan', 'Giuseppe Perelli']","['Utrecht University', 'Open University\nUtrecht University', 'Utrecht University', 'University of Oxford', 'University of Aberdeen\nUtrecht University', 'Sapienza University of Rome']","['Netherlands', 'United Kingdom', 'Netherlands', 'United Kingdom', 'United Kingdom', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30164,Security,Long-Term Safe Reinforcement Learning with Binary Feedback,"Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binary-feedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conservatively takes only a safe action at every time step while inferring its effect on future safety under proper assumptions. Our theoretical results show that LoBiSaRL guarantees the long-term safety constraint, with high probability. Finally, our empirical results demonstrate that our algorithm is safer than existing methods without significantly compromising performance in terms of reward.",['General'],[],"['Akifumi Wachi', 'Wataru Hashimoto', 'Kazumune Hashimoto']","['LINE Corporation', 'Osaka University', 'Osaka University']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30165,Fairness & Bias,Identifying Reasons for Bias: An Argumentation-Based Approach,"As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identification of bias.",['General'],[],"['Madeleine Waller', 'Odinaldo Rodrigues', 'Oana Cocarascu']","[""King's College London"", ""King's College London"", ""King's College London""]","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30167,Fairness & Bias,Moderate Message Passing Improves Calibration: A Universal Way to Mitigate Confidence Bias in Graph Neural Networks,"Confidence calibration in Graph Neural Networks (GNNs) aims to align a model's predicted confidence with its actual accuracy. Recent studies have indicated that GNNs exhibit an under-confidence bias, which contrasts the over-confidence bias commonly observed in deep neural networks. However, our deeper investigation into this topic reveals that not all GNNs exhibit this behavior. Upon closer examination of message passing in GNNs, we found a clear link between message aggregation and confidence levels. Specifically, GNNs with extensive message aggregation, often seen in deep architectures or when leveraging large amounts of labeled data, tend to exhibit overconfidence. This overconfidence can be attributed to factors like over-learning and over-smoothing. Conversely, GNNs with fewer layers, known for their balanced message passing and superior node representation, may exhibit under-confidence. To counter these confidence biases, we introduce the Adaptive Unified Label Smoothing (AU-LS) technique. Our experiments show that AU-LS outperforms existing methods, addressing both over and under-confidence in various GNN scenarios.",['General'],[],"['Min Wang', 'Hao Yang', 'Jincai Huang', 'Qing Cheng']","['College of Systems Engineering, National University of Defense Technology', 'College of Systems Engineering, National University of Defense Technology', 'College of Systems Engineering, National University of Defense Technology', 'College of Systems Engineering, National University of Defense Technology']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30168,Transparency & Explainability,Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks,"A plethora of fair graph neural networks (GNNs) have been proposed to promote algorithmic fairness for high-stake real-life contexts. Meanwhile, explainability is generally proposed to help machine learning practitioners debug models by providing human-understandable explanations. However, seldom work on explainability is made to generate explanations for fairness diagnosis in GNNs. From the explainability perspective, this paper explores the problem of what subgraph patterns cause the biased behavior of GNNs, and what actions could practitioners take to rectify the bias? By answering the two questions, this paper aims to produce compact, diagnostic, and actionable explanations that are responsible for discriminatory behavior. Specifically, we formulate the problem of generating diagnostic and actionable explanations as a multi-objective combinatorial optimization problem. To solve the problem, a dedicated multi-objective evolutionary algorithm is presented to ensure GNNs' explainability and fairness in one go. In particular, an influenced nodes-based gradient approximation is developed to boost the computation efficiency of the evolutionary algorithm. We provide a theoretical analysis to illustrate the effectiveness of the proposed framework. Extensive experiments have been conducted to demonstrate the superiority of the proposed method in terms of classification performance, fairness, and interpretability.",['General'],[],"['Zhenzhong Wang', 'Qingyuan Zeng', 'Wanyu Lin', 'Min Jiang', 'Kay Chen Tan']","['The Hong Kong Polytechnic University', 'Xiamen University', 'The Hong Kong Polytechnic University', 'Xiamen University', 'The Hong Kong Polytechnic University']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/30171,Security,Concealing Sensitive Samples against Gradient Leakage in Federated Learning,"Federated Learning (FL) is a distributed learning paradigm that enhances users' privacy by eliminating the need for clients to share raw, private data with the server. Despite the success, recent studies expose the vulnerability of FL to model inversion attacks, where adversaries reconstruct users’ private data via eavesdropping on the shared gradient information. We hypothesize that a key factor in the success of such attacks is the low entanglement among gradients per data within the batch during stochastic optimization. This creates a vulnerability that an adversary can exploit to reconstruct the sensitive data. Building upon this insight, we present a simple, yet effective defense strategy that obfuscates the gradients of the sensitive data with concealed samples. To achieve this, we propose synthesizing concealed samples to mimic the sensitive data at the gradient level while ensuring their visual dissimilarity from the actual sensitive data. Compared to the previous art, our empirical evaluations suggest that the proposed technique provides the strongest protection while simultaneously maintaining the FL performance. Code is located at https://github.com/JingWu321/DCS-2.",['General'],[],"['Jing Wu', 'Munawar Hayat', 'Mingyi  Zhou', 'Mehrtash Harandi']","['Monash University', 'Monash University', 'Monash University', 'Monash University']","['Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/30169,Security,Physics-Informed Representation and Learning: Control and Risk Quantification,"Optimal and safety-critical control are fundamental problems for stochastic systems, and are widely considered in real-world scenarios such as robotic manipulation and autonomous driving. In this paper, we consider the problem of efficiently finding optimal and safe control for high-dimensional systems. Specifically, we propose to use dimensionality reduction techniques from a comparison theorem for stochastic differential equations together with a generalizable physics-informed neural network to estimate the optimal value function and the safety probability of the system. The proposed framework results in substantial sample efficiency improvement compared to existing methods. We further develop an autoencoder-like neural network to automatically identify the low-dimensional features in the system to enhance the ease of design for system integration. We also provide experiments and quantitative analysis to validate the efficacy of the proposed method.  Source code is available at https://github.com/jacobwang925/path-integral-PINN.",['General'],[],"['Zhuoyuan Wang', 'Reece Keller', 'Xiyu Deng', 'Kenta Hoshino', 'Takashi Tanaka', 'Yorie Nakahira']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Kyoto University', 'University of Texas at Austin', 'Carnegie Mellon University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30176,Security,Representation-Based Robustness in Goal-Conditioned Reinforcement Learning,"While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention, its algorithmic robustness against adversarial perturbations remains unexplored. The attacks and robust representation training methods that are designed for traditional RL become less effective when applied to GCRL. To address this challenge, we first propose the Semi-Contrastive Representation attack, a novel approach inspired by the adversarial contrastive attack. Unlike existing attacks in RL, it only necessitates information from the policy function and can be seamlessly implemented during deployment. Then, to mitigate the vulnerability of existing GCRL algorithms, we introduce Adversarial Representation Tactics, which combines Semi-Contrastive Adversarial Augmentation with Sensitivity-Aware Regularizer to improve the adversarial robustness of the underlying RL agent against various types of perturbations. Extensive experiments validate the superior performance of our attack and defence methods across multiple state-of-the-art GCRL algorithms. Our code is available at https://github.com/TrustAI/ReRoGCRL.",['General'],[],"['Xiangyu Yin', 'Sihao Wu', 'Jiaxu Liu', 'Meng Fang', 'Xingyu Zhao', 'Xiaowei Huang', 'Wenjie Ruan']","['University of Liverpool', 'University of Liverpool', 'University of Liverpool', 'University of Liverpool', 'University of Warwick', 'University of Liverpool', 'University of Liverpool']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30177,Security,Enhancing Off-Policy Constrained Reinforcement Learning through Adaptive Ensemble C Estimation,"In the domain of real-world agents, the application of Reinforcement Learning (RL) remains challenging due to the necessity for safety constraints. Previously, Constrained Reinforcement Learning (CRL) has predominantly focused on on-policy algorithms. Although these algorithms exhibit a degree of efficacy, their interactivity efficiency in real-world settings is sub-optimal, highlighting the demand for more efficient off-policy methods. However, off-policy CRL algorithms grapple with challenges in precise estimation of the C-function, particularly due to the fluctuations in the constrained Lagrange multiplier. Addressing this gap, our study focuses on the nuances of C-value estimation in off-policy CRL and introduces the Adaptive Ensemble C-learning (AEC) approach to reduce these inaccuracies. Building on state-of-the-art off-policy algorithms, we propose AEC-based CRL algorithms designed for enhanced task optimization. Extensive experiments on nine constrained robotics tasks reveal the superior interaction efficiency and performance of our algorithms in comparison to preceding methods.",['General'],[],"['Hengrui Zhang', 'Youfang Lin', 'Shuo Shen', 'Sheng Han', 'Kai Lv']","['School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\nBeijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China', 'School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\nBeijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China', 'Cooperation Product Department, Interactive Entertainment Group, Tencent', 'School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\nBeijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China', 'School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\nBeijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30181,Security,A Huber Loss Minimization Approach to Byzantine Robust Federated Learning,"Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on epsilon, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of epsilon. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.",['General'],[],"['Puning Zhao', 'Fei Yu', 'Zhiguo Wan']","['Zhejiang Lab', 'Zhejiang Lab', 'Zhejiang Lab']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30182,Privacy & Data Governance,Responsible Bandit Learning via Privacy-Protected Mean-Volatility Utility,"For ensuring the safety of users by protecting the privacy,  the traditional privacy-preserving bandit algorithm aiming to maximize the mean reward has been widely studied in scenarios such as online ride-hailing, advertising recommendations, and personalized healthcare. However, classical bandit learning is irresponsible in such practical applications as they fail to account for risks in online decision-making and ignore external system information. This paper firstly proposes  privacy protected mean-volatility utility as the objective of bandit learning and proves its responsibility, because it aims at achieving the maximum probability of utility by considering the risk.  Theoretically, our proposed responsible bandit learning is expected to achieve the fastest convergence rate among current bandit algorithms  and generates more statistical power than classical normality-based test. Finally, simulation studies provide supporting evidence for the theoretical results and demonstrate stronger performance when using stricter privacy budgets.",['General'],[],"['Shanshan Zhao', 'Wenhai Cui', 'Bei Jiang', 'Linglong Kong', 'Xiaodong Yan']","['Mathematics Discipline, Shandong University', 'Mathematics Discipline, Shandong University', 'Mathematics Discipline, University of Alberta', 'Mathematics Discipline, University of Alberta', 'Mathematics Discipline, Shandong University\nShandong National Center for Applied Mathematics']","['China', 'China', 'Canada', 'Canada', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30180,Security,GaLileo: General Linear Relaxation Framework for Tightening Robustness Certification of Transformers,"Transformers based on attention mechanisms exhibit vulnerability to adversarial examples, posing a substantial threat to the security of their applications. Aiming to solve this problem, the concept of robustness certification is introduced to formally ascertain the presence of any adversarial example within a specified region surrounding a given sample. However, prior works have neglected the dependencies among inputs of softmax (the most complex function in attention mechanisms) during linear relaxations. This oversight has consequently led to imprecise certification results. In this work, we introduce GaLileo, a general linear relaxation framework designed to certify the robustness of Transformers. GaLileo effectively surmounts the trade-off between precision and efficiency in robustness certification through our innovative n-dimensional relaxation approach. Notably, our relaxation technique represents a pioneering effort as the first linear relaxation for n-dimensional functions such as softmax. Our novel approach successfully transcends the challenges posed by the curse of dimensionality inherent in linear relaxations, thereby enhancing linear bounds by incorporating input dependencies. Our evaluations encompassed a thorough analysis utilizing the SST and Yelp datasets along with diverse Transformers of different depths and widths. The experimental results demonstrate that, as compared to the baseline method CROWN-BaF, GaLileo achieves up to 3.24 times larger certified radii while requiring similar running times. Additionally, GaLileo successfully attains certification for Transformers' robustness against multi-word lp perturbations, marking a notable accomplishment in this field.",['General'],[],"['Yunruo Zhang', 'Lujia Shen', 'Shanqing Guo', 'Shouling Ji']","['Shandong University', 'Zhejiang University', 'Shandong University', 'Zhejiang University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30179,Transparency & Explainability,LR-XFL: Logical Reasoning-Based Explainable Federated Learning,"Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients’ local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively. The explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model’s robustness to errors. It has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.",['General'],[],"['Yanci Zhang', 'Han Yu']","['Nanyang Technological University', 'Nanyang Technological University']","['Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/30183,Security,UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation,"Recent advances in backdoor attacks, like leveraging complex triggers or stealthy implanting techniques, have   introduced new challenges in backdoor scanning, limiting the usability of Deep Neural Networks (DNNs) in various scenarios. In this paper, we propose Unlearning-based Model Ablation (UMA), a novel approach to facilitate backdoor scanning and defend against advanced backdoor attacks. UMA filters out backdoor-irrelevant features by ablating the inherent features of the target class within the model and subsequently reveals the backdoor through dynamic trigger optimization. We evaluate our method on 1700 models (700 benign and 1000 trojaned) with 6 model structures, 7 different backdoor attacks and 4 datasets. Our results demonstrate that the proposed methodology effectively detect these advanced backdoors. Specifically, our method can achieve 91% AUC-ROC and 86.6% detection accuracy on average, which outperforms the baselines, including Neural Cleanse, ABS, K-Arm and MNTD.",['General'],[],"['Yue Zhao', 'Congyi Li', 'Kai Chen']","['Institute of Information Engineering, Chinese Academy of Sciences, China;', 'Institute of Information Engineering, Chinese Academy of Sciences, China;\nSchool of Cyber Security, University of Chinese Academy of Science, China', 'Institute of Information Engineering, Chinese Academy of Sciences, China;\nSchool of Cyber Security, University of Chinese Academy of Science, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30186,Security,DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models,"Dataset sanitization is a widely adopted proactive defense against poisoning-based backdoor attacks, aimed at filtering out and removing poisoned samples from training datasets. However, existing methods have shown limited efficacy in countering the ever-evolving trigger functions, and often leading to considerable degradation of benign accuracy. In this paper, we propose DataElixir, a novel sanitization approach tailored to purify poisoned datasets. We leverage diffusion models to eliminate trigger features and restore benign features, thereby turning the poisoned samples into benign ones. Specifically, with multiple iterations of the forward and reverse process, we extract intermediary images and their predicted labels for each sample in the original dataset. Then, we identify anomalous samples in terms of the presence of label transition of the intermediary images, detect the target label by quantifying distribution discrepancy, select their purified images considering pixel and feature distance, and determine their ground-truth labels by training a benign model. Experiments conducted on 9 popular attacks demonstrates that DataElixir effectively mitigates various complex attacks while exerting minimal impact on benign accuracy, surpassing the performance of baseline defense methods.",['General'],[],"['Jiachen Zhou', 'Peizhuo Lv', 'Yibing Lan', 'Guozhu Meng', 'Kai Chen', 'Hualong Ma']","['Institute of Information Engineering, Chinese Academy of Sciences, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, China', 'Institute of Information Engineering, Chinese Academy of Sciences, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, China', 'Institute of Information Engineering, Chinese Academy of Sciences, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, China', 'Institute of Information Engineering, Chinese Academy of Sciences, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, China', 'Institute of Information Engineering, Chinese Academy of Sciences, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, China', 'Institute of Information Engineering, Chinese Academy of Sciences, China\nSchool of Cyber Security, University of Chinese Academy of Sciences, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30184,Security,AdvST: Revisiting Data Augmentations for Single Domain Generalization,"Single domain generalization (SDG) aims to train a robust model against unknown target domain shifts using data from a single source domain. Data augmentation has been proven an effective approach to SDG. However, the utility of standard augmentations, such as translate, or invert, has not been fully exploited in SDG; practically, these augmentations are used as a part of a data preprocessing procedure. Although it is intuitive to use many such augmentations to boost the robustness of a model to out-of-distribution domain shifts, we lack a principled approach to harvest the benefit brought from multiple these augmentations. Here,  we conceptualize standard data augmentations with learnable parameters as semantics transformations that can manipulate certain semantics of a sample, such as the geometry or color of an image. Then, we propose Adversarial learning with Semantics Transformations (AdvST) that augments the source domain data with semantics transformations and  learns a robust model with the augmented data. We theoretically show that AdvST essentially optimizes a distributionally robust optimization objective defined on a set of semantics distributions induced by the parameters of semantics transformations. We demonstrate that AdvST can produce samples that expand the coverage on target domain data. Compared with the state-of-the-art methods, AdvST, despite being a simple method, is surprisingly competitive and achieves the best average SDG performance on the Digits, PACS, and DomainNet datasets. Our code is available at https://github.com/gtzheng/AdvST.",['General'],[],"['Guangtao Zheng', 'Mengdi Huai', 'Aidong Zhang']","['University of Virginia', 'Iowa State University', 'University of Virginia']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30187,Security,Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against Query-Based Attacks,"Although promising, existing defenses against query-based attacks share a common limitation: they offer increased robustness against attacks at the price of a considerable accuracy drop on clean samples. In this work, we show how to efficiently establish, at test-time, a solid tradeoff between robustness and accuracy when mitigating query-based attacks. Given that these attacks necessarily explore low-confidence regions, our insight is that activating dedicated defenses, such as random noise defense and random image transformations, only for low-confidence inputs is sufficient to prevent them. Our approach is independent of training and supported by theory. We verify the effectiveness of our approach for various existing defenses by conducting extensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm that our proposal can indeed enhance these defenses by providing better tradeoffs between robustness and accuracy when compared to state-of-the-art approaches while being completely training-free.",['General'],[],"['Pascal Zimmer', 'Sébastien Andreina', 'Giorgia Azzurra Marson', 'Ghassan Karame']","['Ruhr-Universität Bochum, Germany', 'NEC Labs Europe, Germany', 'NEC Labs Europe, Germany', 'Ruhr-Universität Bochum, Germany']","['Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30188,Security,Coevolutionary Algorithm for Building Robust Decision Trees under Minimax Regret,"In recent years, there has been growing interest in developing robust machine learning (ML) models that can withstand adversarial attacks, including one of the most widely adopted, efficient, and interpretable ML algorithms—decision trees (DTs). This paper proposes a novel coevolutionary algorithm (CoEvoRDT) designed to create robust DTs capable of handling noisy high-dimensional data in adversarial contexts. Motivated by the limitations of traditional DT algorithms, we leverage adaptive coevolution to allow DTs to evolve and learn from interactions with perturbed input data. CoEvoRDT alternately evolves competing populations of DTs and perturbed features, enabling construction of DTs with desired properties. CoEvoRDT is easily adaptable to various target metrics, allowing the use of tailored robustness criteria such as minimax regret. Furthermore, CoEvoRDT has potential to improve the results of other state-of-the-art methods by incorporating their outcomes (DTs they produce) into the initial population and optimize them in the process of coevolution. Inspired by the game theory, CoEvoRDT utilizes mixed Nash equilibrium to enhance convergence. The method is tested on 20 popular datasets and shows superior performance compared to 4 state-of-the-art algorithms. It outperformed all competing methods on 13 datasets with adversarial accuracy metrics, and on all 20 considered datasets with minimax regret. Strong experimental results and flexibility in choosing the error measure make CoEvoRDT a promising approach for constructing robust DTs in real-world applications.",['General'],[],"['Adam Żychowski', 'Andrew Perrault', 'Jacek Mańdziuk']","['Warsaw University of Technology', 'The Ohio State University', 'Warsaw University of Technology\nAGH University of Krakow']","['Poland', 'Poland', 'Poland']"
https://ojs.aaai.org/index.php/AAAI/article/view/30197,Fairness & Bias,Referee-Meta-Learning for Fast Adaptation of Locational Fairness,"When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model. Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level.",['General'],[],"['Weiye Chen', 'Yiqun Xie', 'Xiaowei Jia', 'Erhu He', 'Han Bao', 'Bang An', 'Xun Zhou']","['University of Maryland', 'University of Maryland', 'University of Pittsburgh', 'University of Pittsburgh', 'University of Iowa', 'University of Iowa', 'University of Iowa']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30194,Security,Early Detection of Extreme Storm Tide Events Using Multimodal Data Processing,"Sea-level rise is a well-known consequence of climate change. Several studies have estimated the social and economic impact of the increase in extreme flooding. An efficient way to mitigate its consequences is the development of a flood alert and prediction system, based on high-resolution numerical models and robust sensing networks. However, current models use various simplifying assumptions that compromise accuracy to ensure solvability within a reasonable timeframe, hindering more regular and cost-effective forecasts for various locations along the shoreline. To address these issues, this work proposes a hybrid model for multimodal data processing that combines physics-based numerical simulations, data obtained from a network of sensors, and satellite images to provide refined wave and sea-surface height forecasts, with real results obtained in a critical location within the Port of Santos (the largest port in Latin America). Our approach exhibits faster convergence than data-driven models while achieving more accurate predictions. Moreover, the model handles irregularly sampled time series and missing data without the need for complex preprocessing mechanisms or data imputation while keeping low computational costs through a combination of time encoding, recurrent and graph neural networks. Enabling raw sensor data to be easily combined with existing physics-based models opens up new possibilities for accurate extreme storm tide events forecast systems that enhance community safety and aid policymakers in their decision-making processes.",['General'],[],"['Marcel Barros', 'Andressa Pinto', 'Andres Monroy', 'Felipe Moreno', 'Jefferson Coelho', 'Aldomar Pietro Silva', 'Caio Fabricio Deberaldini Netto', 'José Roberto Leite', 'Marlon Mathias', 'Eduardo Tannuri', 'Artur Jordao', 'Edson Gomi', 'Fabio Cozman', 'Marcelo Dottori', 'Anna Helena Reali Costa']","['Universidade de São Paulo', 'Universidade de São Paulo', 'Massachusetts Institute of Technology', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo', 'Universidade de São Paulo']","['Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil']"
https://ojs.aaai.org/index.php/AAAI/article/view/30199,Security,Auto311: A Confidence-Guided Automated System for Non-emergency Calls,"Emergency and non-emergency response systems are essential services provided by local governments and critical to protecting lives, the environment, and property. The effective handling of (non-)emergency calls is critical for public safety and well-being. By reducing the burden through non-emergency callers, residents in critical need of assistance through 911 will receive a fast and effective response. Collaborating with the Department of Emergency Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call recordings and developed Auto311, the first automated system to handle 311 non-emergency calls, which (1) effectively and dynamically predicts ongoing non-emergency incident types to generate tailored case reports during the call; (2) itemizes essential information from dialogue contexts to complete the generated reports; and (3) strategically structures system-caller dialogues with optimized confidence. We used real-world data to evaluate the system's effectiveness and deployability. The experimental results indicate that the system effectively predicts incident type with an average F-1 score of 92.54%. Moreover, the system successfully itemizes critical information from relevant contexts to complete reports, evincing a 0.93 average consistency score compared to the ground truth. Additionally, emulations demonstrate that the system effectively decreases conversation turns as the utterance size gets more extensive and categorizes the ongoing call with 94.49% mean accuracy.",['General'],[],"['Zirong Chen', 'Xutong Sun', 'Yuanhe Li', 'Meiyi Ma']","['Vanderbilt University, Nashville, TN', 'Vanderbilt University, Nashville, TN', 'Vanderbilt University, Nashville, TN', 'Vanderbilt University, Nashville, TN']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30200,Privacy & Data Governance,Blind-Touch: Homomorphic Encryption-Based Distributed Neural Network Inference for Privacy-Preserving Fingerprint Authentication,"Fingerprint authentication is a popular security mechanism for smartphones and laptops. However, its adoption in web and cloud environments has been limited due to privacy concerns over storing and processing biometric data on servers. This paper introduces Blind-Touch, a novel machine learning-based fingerprint authentication system leveraging homomorphic encryption to address these privacy concerns. Homomorphic encryption allows computations on encrypted data without decrypting. Thus, Blind-Touch can keep fingerprint data encrypted on the server while performing machine learning operations. Blind-Touch combines three strategies to efficiently utilize homomorphic encryption in machine learning: (1) It optimizes the feature vector for a distributed architecture, processing the first fully connected layer (FC-16) in plaintext on the client side and the subsequent layer (FC-1) post-encryption on the server, thereby minimizing encrypted computations; (2) It employs a homomorphic encryption-compatible data compression technique capable of handling 8,192 authentication results concurrently; and (3) It utilizes a clustered server architecture to simultaneously process authentication results, thereby enhancing scalability with increasing user numbers. Blind-Touch achieves high accuracy on two benchmark fingerprint datasets, with a 93.6% F1- score for the PolyU dataset and a 98.2% F1-score for the SOKOTO dataset. Moreover, Blind-Touch can match a fingerprint among 5,000 in about 0.65 seconds. With its privacy-focused design, high accuracy, and efficiency, Blind-Touch is a promising alternative to conventional fingerprint authentication for web and cloud applications.",['General'],[],"['Hyunmin Choi', 'Simon S. Woo', 'Hyoungshick Kim']","['NAVER Cloud, South Korea\nDepartment of Computer Science and Engineering, Sungkyunkwan University, South Korea', 'Department of Artificial Intelligence, Sungkyunkwan University, South Korea\nDepartment of Computer Science and Engineering, Sungkyunkwan University, South Korea', 'Department of Computer Science and Engineering, Sungkyunkwan University, South Korea']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/30202,Fairness & Bias,Fair Sampling in Diffusion Models through Switching Mechanism,"Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness.  To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.",['General'],[],"['Yujin Choi', 'Jinseong Park', 'Hoki Kim', 'Jaewook Lee', 'Saerom Park']","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University', 'Ulsan National Institute of Science and Technology']","['United States', 'United States', 'United States', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30203,Fairness & Bias,Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification,"Variance in predictions across different trained models is a significant, under-explored source of error in fair binary classification. In practice, the variance on some data examples is so large that decisions can be effectively arbitrary. To investigate this problem, we take an experimental approach and make four overarching contributions. We: 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair binary classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our experiments reveal shocking insights about the reliability of conclusions on benchmark datasets. Most fair binary classification benchmarks are close-to-fair when taking into account the amount of arbitrariness present in predictions -- before we even try to apply any fairness interventions. This finding calls into question the practical utility of common algorithmic fairness methods, and in turn suggests that we should reconsider how we choose to measure fairness in binary classification.",['General'],[],"['A. Feder Cooper', 'Katherine Lee', 'Madiha Zahrah Choksi', 'Solon Barocas', 'Christopher De Sa', 'James Grimmelmann', 'Jon Kleinberg', 'Siddhartha Sen', 'Baobao Zhang']","['The Center for Generative AI, Law, and Policy Research\nCornell University', 'The Center for Generative AI, Law, and Policy Research\nCornell University', 'Cornell University', 'Cornell University\nMicrosoft Research', 'Cornell University', 'The Center for Generative AI, Law, and Policy Research\nCornell University', 'Cornell University', 'Microsoft Research', 'Syracuse University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', '', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30204,Privacy & Data Governance,Finding ε and δ of Traditional Disclosure Control Systems,"This paper analyzes the privacy of traditional Statistical Disclosure Control (SDC) systems under a differential privacy interpretation. SDCs, such as cell suppression and swapping, promise to safeguard the confidentiality of data and are routinely adopted in data analyses with profound societal and economic impacts. Through a formal analysis and empirical evaluation of demographic data from real households in the U.S., the paper shows that widely adopted SDC systems not only induce vastly larger privacy losses than classical differential privacy mechanisms, but, they may also come at a cost of larger accuracy and fairness.",['General'],[],"['Saswat Das', 'Keyu Zhu', 'Christine Task', 'Pascal Van Hentenryck', 'Ferdinando Fioretto']","['University of Virginia', 'Georgia Tech', 'Knexus Research', 'Georgia Institute of Technology', 'University of Virginia']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30211,Transparency & Explainability,Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency,"Predictive analytics has been widely used in various domains, including education, to inform decision-making and improve outcomes. However, many predictive models are proprietary and inaccessible for evaluation or modification by researchers and practitioners, limiting their accountability and ethical design. Moreover, predictive models are often opaque and incomprehensible to the officials who use them, reducing their trust and utility. Furthermore, predictive models may introduce or exacerbate bias and inequity, as they have done in many sectors of society. Therefore, there is a need for transparent, interpretable, and fair predictive models that can be easily adopted and adapted by different stakeholders. In this paper, we propose a fair predictive model based on multivariate adaptive regression splines (MARS) that incorporates fairness measures in the learning process. MARS is a non-parametric regression model that performs feature selection, handles non-linear relationships, generates interpretable decision rules, and derives optimal splitting criteria on the variables. Specifically, we integrate fairness into the knot optimization algorithm and provide theoretical and empirical evidence of how it results in a fair knot placement. We apply our fairMARS model to real-world data and demonstrate its effectiveness in terms of accuracy and equity. Our paper contributes to the advancement of responsible and ethical predictive analytics for social good.",['General'],[],"['Parian Haghighat', 'Denisa Gándara', 'Lulu Kang', 'Hadis Anahideh']","['University of Illinois at Chicago', 'The University of Texas at Austin', 'Department of Mathematics and Statistics, University of Massachusetts Amherst', 'University of Illinois Chicago']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30212,Fairness & Bias,Fair Graph Learning Using Constraint-Aware Priority Adjustment and Graph Masking in River Networks,"Accurate prediction of water quality and quantity is crucial for sustainable development and human well-being. However, existing data-driven methods often suffer from spatial biases in model performance due to heterogeneous data, limited observations, and noisy sensor data. To overcome these challenges, we propose Fair-Graph, a novel graph-based recurrent neural network that leverages interrelated knowledge from multiple rivers to predict water flow and temperature within large-scale stream networks. Additionally, we introduce node-specific graph masks for information aggregation and adaptation to enhance prediction over heterogeneous river segments. To reduce performance disparities across river segments, we introduce a centralized coordination strategy that adjusts training priorities for segments. We evaluate the prediction of water temperature within the Delaware River Basin, and the prediction of streamflow using simulated data from U.S. National Water Model in the Houston River network. The results showcase improvements in predictive performance and highlight the proposed model's ability to maintain spatial fairness over different river segments.",['General'],[],"['Erhu He', 'Yiqun Xie', 'Alexander Sun', 'Jacob Zwart', 'Jie Yang', 'Zhenong Jin', 'Yang Wang', 'Hassan Karimi', 'Xiaowei Jia']","['University of Pittsburgh', 'The University of Maryland', 'The University of Texas at Austin', 'U.S. geological survey', 'University of Minnesota', 'University of Minnesota', 'University of Pittsburgh', 'University of Pittsburgh', 'University of Pittsburgh']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30215,Fairness & Bias,Long-Term Fair Decision Making through Deep Generative Models,"This paper studies long-term fair machine learning which aims to mitigate group disparity over the long term in sequential decision-making systems. To define long-term fairness, we leverage the temporal causal graph and use the 1-Wasserstein distance between the interventional distributions of different demographic groups at a sufficiently large time step as the quantitative metric. Then, we propose a three-phase learning framework where the decision model is trained on high-fidelity data generated by a deep generative model. We formulate the optimization problem as a performative risk minimization and adopt the repeated gradient descent algorithm for learning. The empirical evaluation shows the efficacy of the proposed method using both synthetic and semi-synthetic datasets.",['General'],[],"['Yaowei Hu', 'Yongkai Wu', 'Lu Zhang']","['University of Arkansas', 'Clemson University', 'University of Arkansas']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30220,Fairness & Bias,Adversarial Fairness Network,"Fairness is becoming a rising concern in machine learning. Recent research has discovered that state-of-the-art models are amplifying social bias by making biased prediction towards some population groups (characterized by sensitive features like race or gender). Such unfair prediction among groups renders trust issues and ethical concerns in machine learning, especially for sensitive fields such as employment, criminal justice, and trust score assessment. In this paper, we introduce a new framework to improve machine learning fairness. The goal of our model is to minimize the influence of sensitive feature from the perspectives of both data input and predictive model. To achieve this goal, we reformulate the data input by eliminating the sensitive information and strengthen model fairness by minimizing the marginal contribution of the sensitive feature. We propose to learn the sensitive-irrelevant input via sampling among features and design an adversarial network to minimize the dependence between the reformulated input and the sensitive information. Empirical results validate that our model achieves comparable or better results than related state-of-the-art methods w.r.t. both fairness metrics and prediction performance.",['General'],[],"['Taeuk Jang', 'Xiaoqian Wang', 'Heng Huang']","['Purdue University', 'Purdue University', 'University of Maryland at College Park']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30228,Transparency & Explainability,Depression Detection via Capsule Networks with Contrastive Learning,"Depression detection is a challenging and crucial task in psychological illness diagnosis. Utilizing online user posts to predict whether a user suffers from depression seems an effective and promising direction. However, existing methods suffer from either poor interpretability brought by the black-box models or underwhelming performance caused by the completely separate two-stage model structure. To alleviate these limitations, we propose a novel capsule network integrated with contrastive learning for depression detection (DeCapsNet). The highlights of DeCapsNet can be summarized as follows. First, it extracts symptom capsules from user posts by leveraging meticulously designed symptom descriptions, and then distills them into class-indicative depression capsules. The overall workflow is in an explicit hierarchical reasoning manner and can be well interpreted by the Patient Health Questionnaire-9 (PHQ9), which is one of the most widely adopted questionnaires for depression diagnosis. Second, it integrates with contrastive learning, which can facilitate the embeddings from the same class to be pulled closer, while simultaneously pushing the embeddings from different classes apart. In addition, by adopting the end-to-end training strategy, it does not necessitate additional data annotation, and mitigates the potential adverse effects from the upstream task to the downstream task. Extensive experiments on three widely-used datasets show that in both within-dataset and cross-dataset scenarios our proposed method outperforms other strong baselines significantly.",['General'],[],"['Han Liu', 'Changya Li', 'Xiaotong Zhang', 'Feng Zhang', 'Wei Wang', 'Fenglong Ma', 'Hongyang Chen', 'Hong Yu', 'Xianchao Zhang']","['Dalian University of Technology', 'Dalian University of Technology', 'Dalian University of Technology', 'Peking University', 'Shenzhen MSU-BIT University', 'The Pennsylvania State University', 'Zhejiang Lab', 'Dalian University of Technology', 'Dalian University of Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30235,Fairness & Bias,DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling,"Many applications use computer vision to detect and count objects in massive image collections. However, automated methods may fail to deliver accurate counts, especially when the task is very difficult or requires a fast response time. For example, during disaster response, aid organizations aim to quickly count damaged buildings in satellite images to plan relief missions, but pre-trained building and damage detectors often perform poorly due to domain shifts. In such cases, there is a need for human-in-the-loop approaches to accurately count with minimal human effort. We propose DISCount -- a detector-based importance sampling framework for counting in large image collections. DISCount uses an imperfect detector and human screening to estimate low-variance unbiased counts. We propose techniques for counting over multiple spatial or temporal regions using a small amount of screening and estimate confidence intervals.  This enables end-users to stop screening when estimates are sufficiently accurate, which is often the goal in real-world applications.  We demonstrate our method with two applications: counting birds in radar imagery to understand responses to climate change, and counting damaged buildings in satellite imagery for damage assessment in regions struck by a natural disaster. On the technical side we develop variance reduction techniques based on control variates and prove the (conditional) unbiasedness of the estimators.  DISCount leads to a 9-12x reduction in the labeling costs to obtain the same error rates compared to naive screening for tasks we consider, and surpasses alternative covariate-based screening approaches.",['General'],[],"['Gustavo Perez', 'Subhransu Maji', 'Daniel Sheldon']","['University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30237,Fairness & Bias,IndicCONAN: A Multilingual Dataset for Combating Hate Speech in Indian Context,"Hate speech (HS) is a growing concern in many parts of the world, including India, where it has led to numerous instances of violence and discrimination. The development of effective counter-narratives (CNs) is a critical step in combating hate speech, but there is a lack of research in this area, especially in non-English languages. In this paper, we introduce a new dataset, IndicCONAN, of counter-narratives against hate speech in Hindi and Indian English. We propose a scalable human-in-the-loop approach for generating counter-narratives by an auto-regressive language model through machine generation - human correction cycle, where the model uses augmented data from previous cycles to generate new training samples. These newly generated samples are then reviewed and edited by annotators, leading to further model refnement. The dataset consists of over 2,500 exam- ˜ ples of counter-narratives each in both English and Hindi corresponding to various hate speeches in the Indian context. We also present a framework for generating CNs conditioned on specifc CN type with a mean perplexity of 3.85 for English and 3.70 for Hindi, a mean toxicity score of 0.04 for English and 0.06 for Hindi, and a mean diversity of 0.08 for English and 0.14 for Hindi. Our dataset and framework provide valuable resources for researchers and practitioners working to combat hate speech in the Indian context.",['General'],[],"['Nihar Ranja Sahoo', 'Gyana Prakash Beria', 'Pushpak Bhattacharyya']","['Indian Institute of Technology, Bombay', 'Indian Institute of Technology, Bombay', 'Indian Institute of Technology, Bombay']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30236,Fairness & Bias,Discretionary Trees: Understanding Street-Level Bureaucracy via Machine Learning,"Street-level bureaucrats interact directly with people on behalf of government agencies to perform a wide range of functions, including, for example, administering social services and policing. A key feature of street-level bureaucracy is that the civil servants, while tasked with implementing agency policy, are also granted significant discretion in how they choose to apply that policy in individual cases. Using that discretion could be beneficial, as it allows for exceptions to policies based on human interactions and evaluations, but it could also allow biases and inequities to seep into important domains of societal resource allocation. In this paper, we use machine learning techniques to understand street-level bureaucrats' behavior. We leverage a rich dataset that combines demographic and other information on households with information on which homelessness interventions they were assigned during a period when assignments were not formulaic. We find that  caseworker decisions in this time are highly predictable overall, and some, but not all of this predictivity can be captured by simple decision rules. We theorize that the decisions not captured by the simple decision rules can be considered applications of caseworker discretion. These discretionary decisions are far from random in both the characteristics of such households and in terms of the outcomes of the decisions. Caseworkers typically only apply discretion to households that would be considered less vulnerable. When they do apply discretion to assign households to more intensive interventions, the marginal benefits to those households are significantly higher than would be expected if the households were chosen at random; there is no similar reduction in marginal benefit to households that are discretionarily allocated less intensive interventions, suggesting that caseworkers are using their knowledge and experience to improve outcomes for households experiencing homelessness.",['General'],[],"['Gaurab Pokharel', 'Sanmay Das', 'Patrick Fowler']","['George Mason University', 'George Mason University', 'Washington University in St. Louis']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30254,Fairness & Bias,"Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability","Automatic Essay Scoring (AES) is a well-established educational pursuit that employs machine learning to evaluate student-authored essays. While much effort has been made in this area, current research primarily focuses on either (i) boosting the predictive accuracy of an AES model for a specific prompt (i.e., developing prompt-specific models), which often heavily relies on the use of the labeled data from the same target prompt; or (ii) assessing the applicability of AES models developed on non-target prompts to the intended target prompt (i.e., developing the AES models in a cross-prompt setting). Given the inherent bias in machine learning and its potential impact on marginalized groups, it is imperative to investigate whether such bias exists in current AES methods and, if identified, how it intervenes with an AES model's accuracy and generalizability. Thus, our study aimed to uncover the intricate relationship between an AES model's accuracy, fairness, and generalizability, contributing practical insights for developing effective AES models in real-world education. To this end, we meticulously selected nine prominent AES methods and evaluated their performance using seven distinct metrics on an open-sourced dataset, which contains over 25,000 essays and various demographic information about students such as gender, English language learner status, and economic status. Through extensive evaluations, we demonstrated that: (1) prompt-specific models tend to outperform their cross-prompt counterparts in terms of predictive accuracy; (2) prompt-specific models frequently exhibit a greater bias towards students of different economic statuses compared to cross-prompt models; (3) in the pursuit of generalizability, traditional machine learning models (e.g., SVM) coupled with carefully engineered features hold greater potential for achieving both high accuracy and fairness than complex neural network models.",['General'],[],"['Kaixun Yang', 'Mladen Raković', 'Yuyang Li', 'Quanlong Guan', 'Dragan Gašević', 'Guangliang Chen']","['Monash University', 'Monash University', 'Monash University', 'Jinan University', 'Monash University', 'Monash University']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/30256,Fairness & Bias,Fairness-Aware Structured Pruning in Transformers,"The increasing size of large language models (LLMs) has introduced challenges in their training and inference. Removing model components is perceived as a solution to tackle the large model sizes, however, existing pruning methods solely focus on performance, without considering an essential aspect for the responsible use of LLMs: model fairness. It is crucial to address the fairness of LLMs towards diverse groups, such as women, Black people, LGBTQ+, Jewish communities, among others, as they are being deployed and available to a wide audience. In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities. Our approach is practical in terms of time and resources, as it does not require fine-tuning the final pruned, and fairer, model. Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance. WARNING: This work uses language that is offensive in nature.",['General'],[],"['Abdelrahman Zayed', 'Gonçalo Mordido', 'Samira Shabanian', 'Ioana Baldini', 'Sarath Chandar']","['Mila\nPolytechnique Montreal', 'Mila\nPolytechnique Montreal', 'Independent Researcher', 'IBM Research', 'Mila\nPolytechnique Montreal\nCanada CIFAR AI Chair']","['Canada', 'Canada', 'Canada', '', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/30259,Security,Pre-trained Online Contrastive Learning for Insurance Fraud Detection,"Medical insurance fraud has always been a crucial challenge in the field of healthcare industry. Existing fraud detection models mostly focus on offline learning scenes. However, fraud patterns are constantly evolving, making it difficult for models trained on past data to detect newly emerging fraud patterns, posing a severe challenge in medical fraud detection. Moreover, current incremental learning models are mostly designed to address catastrophic forgetting, but often exhibit suboptimal performance in fraud detection. To address this challenge, this paper proposes an innovative online learning method for medical insurance fraud detection, named POCL. This method combines contrastive learning pre-training with online updating strategies. In the pre-training stage, we leverage contrastive learning pre-training to learn on historical data, enabling deep feature learning and obtaining rich risk representations. In the online learning stage, we adopt a Temporal Memory Aware Synapses online updating strategy, allowing the model to perform incremental learning and optimization based on continuously emerging new data. This ensures timely adaptation to fraud patterns and reduces forgetting of past knowledge. Our model undergoes extensive experiments and evaluations on real-world insurance fraud datasets. The results demonstrate our model has significant advantages in accuracy compared to the state-of-the-art baseline methods, while also exhibiting lower running time and space consumption. Our sources are released at https://github.com/finint/POCL.",['General'],[],"['Rui Zhang', 'Dawei Cheng', 'Jie Yang', 'Yi Ouyang', 'Xian Wu', 'Yefeng Zheng', 'Changjun Jiang']","['Tongji University\nShanghai Artificial Intelligence Laboratory', 'Tongji University\nKey Laboratory of Artificial Intelligence, Ministry of Education\nShanghai Artificial Intelligence Laboratory', 'Tongji University', 'Tencent YouTu Lab', 'Tencent YouTu Lab', 'Tencent YouTu Lab', 'Tongji University\nShanghai Artificial Intelligence Laboratory']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30264,Security,AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation Using Intelligent Sensing System,"The application of artificial intelligence technology has greatly enhanced and fortified the safety of energy pipelines, particularly in safeguarding against external threats. The predominant methods involve the integration of intelligent sensors to detect external vibration, enabling the identification of event types and locations, thereby replacing manual detection methods. However, practical implementation has exposed a limitation in current methods - their constrained ability to accurately discern the spatial dimensions of external signals, which complicates the authentication of threat events. Our research endeavors to overcome the above issues by harnessing deep learning techniques to achieve a more fine-grained recognition and localization process. This refinement is crucial in effectively identifying genuine threats to pipelines, thus enhancing the safety of energy transportation. This paper proposes a radial threat estimation method for energy pipelines based on distributed optical fiber sensing technology. Specifically, we introduce a continuous multi-view and multi-domain feature fusion methodology to extract comprehensive signal features and construct a threat estimation and recognition network. The utilization of collected acoustic signal data is optimized, and the underlying principle is elucidated. Moreover, we incorporate the concept of transfer learning through a pre-trained model, enhancing both recognition accuracy and training efficiency. Empirical evidence gathered from real-world scenarios underscores the efficacy of our method, notably in its substantial reduction of false alarms and remarkable gains in recognition accuracy. More generally, our method exhibits versatility and can be extrapolated to a broader spectrum of recognition tasks and scenarios.",['General'],[],"['Chengyuan Zhu', 'Yiyuan Yang', 'Kaixiang Yang', 'Haifeng Zhang', 'Qinmin Yang', 'C. L. Philip Chen']","['Zhejiang University, Hangzhou, China', 'University of Oxford, Oxfordshire, United Kingdom', 'South China University of Technology, Guangzhou, China', 'Research Institute of Tsinghua University, Pearl River Delta, Guangzhou, China', 'Zhejiang University, Hangzhou, China', 'South China University of Technology, Guangzhou, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30263,Fairness & Bias,Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation,"Online dating platforms have gained widespread popularity as a means for individuals to seek potential romantic relationships. While recommender systems have been designed to improve the user experience in dating platforms by providing personalized recommendations, increasing concerns about fairness have encouraged the development of fairness-aware recommender systems from various perspectives (e.g., gender and race). However, sexual orientation, which plays a significant role in finding a satisfying relationship, is under-investigated. To fill this crucial gap, we propose a novel metric, Opposite Gender Interaction Ratio (OGIR), as a way to investigate potential unfairness for users with varying preferences towards the opposite gender. We empirically analyze a real online dating dataset and observe existing recommender algorithms could suffer from group unfairness according to OGIR. We further investigate the potential causes for such gaps in recommendation quality, which lead to the challenges of group quantity imbalance and group calibration imbalance. Ultimately, we propose a fair recommender system based on re-weighting and re-ranking strategies to respectively mitigate these associated imbalance challenges. Experimental results demonstrate both strategies improve fairness while their combination achieves the best performance towards maintaining model utility while improving fairness.",['General'],[],"['Yuying Zhao', 'Yu Wang', 'Yi Zhang', 'Pamela Wisniewski', 'Charu Aggarwal', 'Tyler Derr']","['Vanderbilt University', 'Vanderbilt University', 'Vanderbilt University', 'Vanderbilt University', 'IBM T. J. Watson Research Center', 'Vanderbilt University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30275,Transparency & Explainability,Adventures of Trustworthy Vision-Language Models: A Survey,"Recently, transformers have become incredibly popular in computer vision and vision-language tasks. This notable rise in their usage can be primarily attributed to the capabilities offered by attention mechanisms and the outstanding ability of transformers to adapt and apply themselves to a variety of tasks and domains. Their versatility and state-of-the-art performance have established them as indispensable tools for a wide array of applications. However, in the constantly changing landscape of machine learning, the assurance of the trustworthiness of transformers holds utmost importance. This paper conducts a thorough examination of vision-language transformers, employing three fundamental principles of responsible AI: Bias, Robustness, and Interpretability. The primary objective of this paper is to delve into the intricacies and complexities associated with the practical use of transformers, with the overarching goal of advancing our comprehension of how to enhance their reliability and accountability.","['Vision-Language Models', 'Interpretability', 'Bias', 'Robustness']",[],"['Mayank Vatsa', 'Anubhooti Jain', 'Richa Singh']","['IIT Jodhpur, India', 'IIT Jodhpur, India', 'IIT Jodhpur, India']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30278,Fairness & Bias,Demystifying Algorithmic Fairness in an Uncertain World,"Significant progress in the field of fair machine learning (ML) has been made to counteract algorithmic discrimination against marginalized groups. However, fairness remains an active research area that is far from settled. One key bottleneck is the implicit assumption that environments, where ML is developed and deployed, are certain and reliable. In a world that is characterized by volatility, uncertainty, complexity, and ambiguity, whether what has been developed in algorithmic fairness can still serve its purpose is far from obvious. In this talk, I will first discuss how to improve algorithmic fairness under two kinds of predictive uncertainties, i.e., aleatoric uncertainty (i.e., randomness and ambiguity in the data) and epistemic uncertainty (i.e., a lack of data or knowledge), respectively. The former regards historical bias reflected in the data and the latter corresponds to the bias perpetuated or amplified during model training due to lack of data or knowledge. In particular, the first work studies pushing the fairness-utility trade-off through aleatoric uncertainty, and the second work investigates fair few-shot learning. The last work introduces coverage-based fairness that ensures different groups enjoy identical treatment and receive equal coverage.","['Algorithmic Fairness', 'Uncertainty', 'Responsible AI']",[],['Lu Cheng'],['University of Illinois Chicago'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30281,Security,Towards Robust Visual Understanding: from Recognition to Reasoning,"Models that learn from data are widely and rapidly being deployed today for real-world use, but they suffer from unforeseen failures due to distribution shift, adversarial attacks, noise and corruption, and data scarcity.  But many failures also occur because many modern AI tasks require reasoning beyond pattern matching -- and such reasoning abilities are difficult to formulate as data-based input-output function fitting.  The reliability problem has become increasingly important under the new paradigm of semantic ``multimodal'' learning.  My research provides avenues to develop robust and reliable computer vision systems, particularly by leveraging the interactions between vision and language. In this AAAI New Faculty highlights talk, I will cover three thematic areas of my research, ranging from robustness in computer vision, open-domain reliability in visual reasoning, and challenges and opportunities in evaluation of generative models. Readers are encouraged to refer to my website (www.tejasgokhale.com) for more details and updates from my lab's activities towards the goal of robust visual understanding.","['Reliability', 'Robustness And Generalization', 'Computer Vision', 'Semantic Vision', 'Vision And Language', 'Multimodal Learning']",[],['Tejas Gokhale'],"['University of Maryland, Baltimore County']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30284,Privacy & Data Governance,Collaborative Learning across Heterogeneous Systems with Pre-Trained Models,"The increasingly decentralized and private nature of data in our digital society has  motivated the development of personalized, collaborative intelligent systems that enable knowledge aggregation across multiple data owners while accommodating for their data privacy and system constraints. However, collaborative learning has only been investigated in simple and limited settings: isolated task scenarios where learning begins from scratch and does not build on prior expertise; learned model is represented in task-specific forms which are not generalizable to unseen, emerging scenarios; and more often, a universal model representation is assumed across collaborators, ignoring their local compute constraints or input representations. This restricts its practicality in continual learning scenarios with limited task data, which demand continuous adaptation and knowledge transfer across different information silos, tasks, and learning models, as well as the utilization of prior solution expertises. To overcome these limitations, my research has been focused on developing effective and scalable resource-aware collaborative learning frameworks across heterogeneous systems.","['Collaborative Learning', 'Generative Model', 'Probabilistic Machine Learning', 'Distributed Learning']",[],['Trong Nghia Hoang'],['Washington State University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30286,Privacy & Data Governance,Fostering Trustworthiness in Machine Learning Algorithms,"Recent years have seen a surge in research that develops and applies machine learning algorithms to create intelligent learning systems. However, traditional machine learning algorithms have primarily focused on optimizing accuracy and efficiency, and they often fail to consider how to foster trustworthiness in their design. As a result, machine learning models usually face a trust crisis in real-world applications. Driven by these urgent concerns about trustworthiness, in this talk, I will introduce my research efforts towards the goal of making machine learning trustworthy. Specifically, I will delve into the following key research topics: security vulnerabilities and robustness, model explanations, and privacy-preserving mechanisms.","['Artificial Intelligence', 'Machine Learning', 'Trustworthiness']",[],['Mengdi Huai'],['Iowa State University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30295,Security,Harmonious Mobility for Robots that Work with and around People,"The integration of advances from machine learning and computer vision with the classical autonomy stack has brought successful robot deployments in fulfilment, manufacturing, and transportation. However, unstructured and dynamic environments such as pedestrian spaces and streets, workplaces, and homes pose additional challenges such as modeling human behavior, understanding user perceptions, and ensuring human safety and comfort. My work addresses such challenges to enable robots to fluently work with and around people to increase productivity and assist users.","['Robotics', 'Human-robot Interaction', 'Robot Navigation', 'Multiagent Systems', 'Motion Planning And Control', 'Human Modeling', 'User Studies', 'Robot Systems']",[],['Christoforos Mavrogiannis'],['University of Michigan'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30292,Transparency & Explainability,When Causal Inference Meets Graph Machine Learning,"Graphs (i.e., networks) are ubiquitous in daily life, as they can effectively model a plethora of real-world systems with connected units, such as social networks and biological networks. Recent years have witnessed rapid development in graph-based machine learning (GML) in various high-impact domains. Currently, the mainstream GML methods are based on statistical learning, e.g., utilizing the statistical correlations between node features, graph structure, and labels for node classification. However, statistical learning has been widely criticized for only capturing the superficial relations between variables in the data system, and consequently, rendering the lack of trustworthiness in real-world applications. Therefore, it is crucial to understand the causality in the data system and the learning process. Causal inference is the discipline that investigates the causality inside a system, for example, to identify and estimate the causal effect of a certain treatment (e.g., wearing a face mask) on an important outcome (e.g., COVID-19 infection). Involving the concepts and philosophy of causal inference in ML methods is often considered significant for human-level intelligence and can serve as the foundation of artificial intelligence (AI). However, most traditional causal inference studies rely on strong assumptions, and focus on independent and identically distributed (i.i.d.) data, while causal inference on graphs is faced with many barriers. Therefore, we aim to bridge the gap between causal inference and GML.","['Trustworthy AI', 'Causal Inference', 'Graph Learning']",[],['Jing Ma'],['Case Western Reserve University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30296,Transparency & Explainability,Recent Advancements in Inverse Reinforcement Learning,"Inverse reinforcement learning (IRL) has seen significant advancements in recent years. This class of approaches aims to efficiently learn the underlying reward function that rationalizes the behavior exhibited by expert agents, often represented by humans. In contrast to mere behavioral cloning, the reconstruction of a reward function yields appealing implications, as it allows for more effective interpretability of the expert’s decisions and provides a transferable specification of the expert’s objectives for application in even different environments. Unlike the well-understood field of reinforcement learning (RL) from a theoretical perspective, IRL still grapples with limited understanding, significantly constraining its applicability. A fundamental challenge in IRL is the inherent ambiguity in selecting a reward function, given the existence of multiple candidate functions, all explaining the expert’s behavior.  In this talk, I will survey three of my papers that have made notable contributions to the IRL field: “Provably Efficient Learning of Transferable Rewards”, “Towards Theoretical Understanding of Inverse Reinforcement Learning”, and “Inverse Reinforcement Learning with Sub-optimal Experts"".  The central innovation introduced by the first paper is a novel formulation of the IRL problem that overcomes the issue of ambiguity. IRL is reframed as the problem of learning the feasible reward set, which is the set of all rewards that can explain the expert’s behavior. This approach postpones the selection of the reward function, thereby circumventing the ambiguity issues. Furthermore, the feasible reward set exhibits convenient geometric properties that enable the development of efficient algorithms for its computation.   Building on this novel formulation of IRL, the second paper addresses the problem of efficiently learning the feasible reward set when the environment and the expert’s policy are not known in advance. It introduces a novel way to assess the dissimilarity between feasible reward sets based on the Hausdorff distance and presents a new PAC (probabilistic approximately correct) framework. The most significant contribution of this paper is the introduction of the first sample complexity lower bound, which highlights the challenges inherent in the IRL problem. Deriving this lower bound necessitated the development of novel technical tools. The paper also demonstrates that when a generative model of the environment is available, a uniform sampling strategy achieves a sample complexity that matches the lower bound, up to logarithmic factors.  Finally, in the third paper, the IRL problem in the presence of sub-optimal experts is investigated. Specifically, the paper assumes the availability of multiple sub-optimal experts, in addition to the expert agent, which provides additional demonstrations, associated with a known quantification of the maximum amount of sub-optimality. The paper shows that this richer information mitigates the ambiguity problem, significantly reducing the size of the feasible reward set while retaining its favorable geometric properties. Furthermore, the paper explores the associated statistical problem and derives novel lower bounds for sample complexity, along with almost matching algorithms. These selected papers represent notable advancements in IRL, contributing to the establishment of a solid theoretical foundation for IRL and extending the framework to accommodate scenarios with sub-optimal experts.","['Artificial Intelligence', 'Machine Learning', 'Reinforcement Learning', 'Inverse Reinforcement Learning']",[],['Alberto Maria Metelli'],['Politecnico di Milano'],['Italy']
https://ojs.aaai.org/index.php/AAAI/article/view/30300,Transparency & Explainability,Towards Human-like Learning from Relational Structured Data,"Relational structured data is a way of representing knowledge using nodes and edges, while also capturing the meaning of that knowledge in a structured form that can be used for machine learning. Compared with vision and natural language data, relational structured data represents and manipulates structured knowledge, which can be beneficial for tasks that involve reasoning or inference. On the other hand, vision and NLP deal more with unstructured data (like images and text), and they often require different types of models and algorithms to extract useful information or features from the data. Human-like Learning develops methods that can harness relational structures and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. With Human-like Learning, the learning algorithm is efficient and can adapt to new or unseen situations, which is crucial in real-world applications where environments may change unpredictably. Moreover, the models are easier for humans to understand and interpret, which is important for transparency and trust in AI systems. In this talk, we present our recent attempts towards human-like learning from relational structured data.","['Graph Neural Networks', 'Automated Machine Learning', 'Neural Architecture Search', 'Few-shot Learning', 'Knowledge Graph', 'Biomedical Network']",[],['Quanming Yao'],['Tsinghua University'],['China']
https://ojs.aaai.org/index.php/AAAI/article/view/30301,Fairness & Bias,Fairness with Censorship: Bridging the Gap between Fairness Research and Real-World Deployment,"Recent works in artificial intelligence fairness attempt to mitigate discrimination by proposing constrained optimization programs that achieve parity for some fairness statistics. Most assume the availability of class label which is impractical in many real-world applications such as precision medicine, actuarial analysis and recidivism prediction. To this end, this talk revisits fairness and reveals idiosyncrasies of existing fairness literature assuming the availability of class label that limits their real-world utility. The primary artifacts are formulating fairness with censorship to account for scenarios where the class label is not guaranteed, and a suite of corresponding new fairness notions, algorithms, and theoretical constructs to bridge the gap between the design of a ``fair'' model in the lab and its deployment in the real-world.","['Fairness', 'Censorship', 'Real-world Deployment']",[],['Wenbin Zhang'],['Florida International University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30298,Transparency & Explainability,Towards Trustworthy Deep Learning,"Deep neural networks (DNNs) have achieved unprecedented success across many scientific and engineering fields in the last decades. Despite its empirical success, unfortunately, recent studies have shown that there are various failure modes and blindspots in DNN models which may result in unexpected serious failures and potential harms, e.g. the existence of adversarial examples and small perturbations. This is not acceptable especially for safety critical and high stakes applications in the real-world, including healthcare, self-driving cars, aircraft control systems, hiring and malware detection protocols. Moreover, it has been challenging to understand why and when DNNs will fail due to their complicated structures and black-box behaviors. Lacking interpretability is one critical issue that may seriously hinder the deployment of DNNs in high-stake applications, which need interpretability to trust the prediction, to understand potential failures, and to be able to mitigate harms and eliminate biases in the model.   To make DNNs trustworthy and reliable for deployment, it is necessary and urgent to develop methods and tools that can (i) quantify and improve their robustness against adversarial and natural perturbations, and (ii) understand their underlying behaviors and further correct errors to prevent injuries and damages. These are the important first steps to enable Trustworthy AI and Trustworthy Machine Learning. In this talk, I will survey a series of research efforts in my lab contributed to tackling the grand challenges in (i) and (ii). In the first part of my talk, I will overview our research effort in Robust Machine Learning since 2017, where we have proposed the first attack-agnostic robustness evaluation metric, the first efficient robustness certification algorithms for various types of perturbations, and efficient robust learning algorithms across supervised learning to deep reinforcement learning.    In the second part of my talk, I will survey a series of exciting results in my lab on accelerating interpretable machine learning and explainable AI. Specifically, I will show how we could bring interpretability into deep learning by leveraging recent advances in multi-modal models. I'll present recent works in our group on automatically dissecting neural networks with open vocabulary concepts, designing interpretable neural networks without concept labels, and briefly overview our recent efforts on demystifying black-box DNN training process, automated neuron explanations for Large Language Models and the first robustness evaluation of a family of neuron-level interpretation techniques.","['Deep Learning', 'Trustworthy Machine Learning', 'Robust Machine Learning', 'Interpretable Machine Learning']",[],['Tsui-Wei (Lily) Weng'],['UCSD'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30302,Fairness & Bias,Fair and Optimal Prediction via Post-Processing,In this talk I will discuss our recent work on characterizing the inherent tradeoff between fairness and accuracy in both classification and regression problems. I will also present a post-processing algorithm that derives optimal fair predictors from Bayes score functions.,"['Trustworthy Machine Learning', 'Algorithmic Fairness', 'Domain Generalization', 'Robustness']",[],['Han Zhao'],['University of Illinois Urbana-Champaign'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30304,Security,"Combating Insider Threat in the Open-World Environments: Identification, Monitoring, and Data Augmentation","Recent years have witnessed a dramatic increase in a class of security threats known as ""insider threats"". These threats occur when individuals with authorized access to an organization's network engage in harmful activities, potentially leading to the disclosure of vital information or adversely affecting the organization's systems (e.g., financial loss, system crashes, and national security challenges).  Distinct from other types of terror attacks, combating insider threats exhibits several unique challenges, including (1) rarity, (2) non-separability, (3) label scarcity, (4) dynamics, and (5) heterogeneity, making themselves extremely difficult to identify and mitigate. We target the challenging problem of combating insider threats in open-world environments by leveraging a variety of data sources (e.g., internal system logs, employee networks, human trafficking, and smuggling networks). To effectively combat these intricate threats, we introduce an interactive learning mechanism that is composed of three mutually beneficial learning modules: insider identification, insider monitoring, and data augmentation. Each module plays a crucial role in enhancing our ability to detect and mitigate insider threats, thereby contributing to a more secure and resilient organizational environment.","['Open-World Machine Learning', 'Rare Category Analysis', 'Insider Threat Detection']",[],['Dawei Zhou'],"['Virginia Tech, VA, USA']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30594,Security,Temporal Logic Explanations for Dynamic Decision Systems Using Anchors and Monte Carlo Tree Search (Abstract Reprint),"For many automated perception and decision tasks, state-of-the-art performance may be obtained by algorithms that are too complex for their behavior to be completely understandable or predictable by human users, e.g., because they employ large machine learning models. To integrate these algorithms into safety-critical decision and control systems, it is particularly important to develop methods that can promote trust into their decisions and help explore their failure modes. In this article, we combine the anchors methodology with Monte Carlo Tree Search to provide local model-agnostic explanations for the behaviors of a given black-box model making decisions by processing time-varying input signals. Our approach searches for descriptive explanations for these decisions in the form of properties of the input signals, expressed in Signal Temporal Logic, which are highly likely to reproduce the observed behavior. To illustrate the methodology, we apply it in simulations to the analysis of a hybrid (continuous-discrete) control system and a collision avoidance system for unmanned aircraft (ACAS Xu) implemented by a neural network.",['Journal Track'],[],"['Tzu-Yi Chiu', 'Jerome Le Ny', 'Jean-Pierre David']","['Electrical Engineering Department, Ecole Polytechnique Montreal', 'Electrical Engineering Department, Ecole Polytechnique Montreal', 'Electrical Engineering Department, Ecole Polytechnique Montreal']","['France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/30596,Transparency & Explainability,Counterfactual Explanations for Misclassified Images: How Human and Machine Explanations Differ (Abstract Reprint),"Counterfactual explanations have emerged as a popular solution for the eXplainable AI (XAI) problem of elucidating the predictions of black-box deep-learning systems because people easily understand them, they apply across different problem domains and seem to be legally compliant. Although over 100 counterfactual methods exist in the XAI literature, each claiming to generate plausible explanations akin to those preferred by people, few of these methods have actually been tested on users (∼7%). Even fewer studies adopt a user-centered perspective; for instance, asking people for their counterfactual explanations to determine their perspective on a “good explanation”. This gap in the literature is addressed here using a novel methodology that (i) gathers human-generated counterfactual explanations for misclassified images, in two user studies and, then, (ii) compares these human-generated explanations to computationally-generated explanations for the same misclassifications. Results indicate that humans do not “minimally edit” images when generating counterfactual explanations. Instead, they make larger, “meaningful” edits that better approximate prototypes in the counterfactual class. An analysis based on “explanation goals” is proposed to account for this divergence between human and machine explanations. The implications of these proposals for future work are discussed.",['Journal Track'],[],"['Eoin Delaney', 'Arjun Pakrashi', 'Derek Greene', 'Mark T. Keane']","['School of Computer Science, University College Dublin, Belfield, Dublin, Ireland\nInsight Centre for Data Analytics, Belfield, Dublin, Ireland\nVistaMilk SFI Research Centre, Belfield, Dublin, Ireland', 'School of Computer Science, University College Dublin, Belfield, Dublin, Ireland\nVistaMilk SFI Research Centre, Belfield, Dublin, Ireland', 'School of Computer Science, University College Dublin, Belfield, Dublin, Ireland\nInsight Centre for Data Analytics, Belfield, Dublin, Ireland\nVistaMilk SFI Research Centre, Belfield, Dublin, Ireland', 'School of Computer Science, University College Dublin, Belfield, Dublin, Ireland\nInsight Centre for Data Analytics, Belfield, Dublin, Ireland\nVistaMilk SFI Research Centre, Belfield, Dublin, Ireland']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30597,Transparency & Explainability,Reasoning about Causality in Games (Abstract Reprint),"Causal reasoning and game-theoretic reasoning are fundamental topics in artificial intelligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl's causal hierarchy to the game-theoretic domain, or as extending Koller and Milch's multi-agent influence diagrams to the causal domain. We then consider three key questions: i) How can the (causal) dependencies in games – either between variables, or between strategies – be modelled in a uniform, principled manner?  ii) How may causal queries be computed in causal games, and what assumptions does this require?  iii) How do causal games compare to existing formalisms?  To address question i), we introduce mechanised games, which encode dependencies between agents' decision rules and the distributions governing the game. In response to question ii), we present definitions of predictions, interventions, and counterfactuals, and discuss the assumptions required for each. Regarding question iii), we describe correspondences between causal games and other formalisms, and explain how causal games can be used to answer queries that other causal or game-theoretic models do not support. Finally, we highlight possible applications of causal games, aided by an extensive open-source Python library.",['Journal Track'],[],"['Lewis Hammond', 'James Fox', 'Tom Everitt', 'Ryan Carey', 'Alessandro Abate', 'Michael Wooldridge']","['University of Oxford, United Kingdom', 'University of Oxford, United Kingdom', 'DeepMind, United Kingdom', 'University of Oxford, United Kingdom', 'University of Oxford, United Kingdom', 'University of Oxford, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30599,Security,Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees (Abstract Reprint),"Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary material.",['Journal Track'],[],"['Kai-Chieh Hsu', 'Allen Z. Ren', 'Duy P. Nguyen', 'Anirudha Majumdar', 'Jaime F. Fisac']","['Department of Electrical and Computer Engineering, Princeton University, United States', 'Department of Mechanical and Aerospace Engineering, Princeton University, United States', 'Department of Electrical and Computer Engineering, Princeton University, United States', 'Department of Mechanical and Aerospace Engineering, Princeton University, United States', 'Department of Electrical and Computer Engineering, Princeton University, United States']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30601,Security,Discovering Agents (Abstract Reprint),"Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial – often the causal model is just assumed by the modeller without much justification – and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents – roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering the presence of agents from empirical data, given a set of variables and under certain assumptions. We also provide algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.",['Journal Track'],[],"['Zachary Kenton', 'Ramana Kumar', 'Sebastian Farquhar', 'Jonathan Richens', 'Matt MacDermott', 'Tom Everitt']","['DeepMind, United Kingdom of Great Britain and Northern Ireland', 'DeepMind, United Kingdom of Great Britain and Northern Ireland', 'DeepMind, United Kingdom of Great Britain and Northern Ireland', 'DeepMind, United Kingdom of Great Britain and Northern Ireland', 'Imperial College London, United Kingdom of Great Britain and Northern Ireland', 'DeepMind, United Kingdom of Great Britain and Northern Ireland']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/30611,Security,Accurate Parameter Estimation for Safety-Critical Systems with Unmodeled Dynamics (Abstract Reprint),"Analysis and synthesis of safety-critical autonomous systems are carried out using models which are often dynamic. Two central features of these dynamic systems are parameters and unmodeled dynamics. Much of feedback control design is parametric in nature and as such, accurate and fast estimation of the parameters in the modeled part of the dynamic system is a crucial property for designing risk-aware autonomous systems. This paper addresses the use of a spectral lines-based approach for estimating parameters of the dynamic model of an autonomous system. Existing literature has treated all unmodeled components of the dynamic system as sub-Gaussian noise and proposed parameter estimation using Gaussian noise-based exogenous signals. In contrast, we allow the unmodeled part to have deterministic unmodeled dynamics, which are almost always present in physical systems, in addition to sub-Gaussian noise. In addition, we propose a deterministic construction of the exogenous signal in order to carry out parameter estimation. We introduce a new tool kit which employs the theory of spectral lines, retains the stochastic setting, and leads to non-asymptotic bounds on the parameter estimation error. Unlike the existing stochastic approach, these bounds are tunable through an optimal choice of the spectrum of the exogenous signal leading to accurate parameter estimation. We also show that this estimation is robust to unmodeled dynamics, a property that is not assured by the existing approach. Finally, we show that under ideal conditions with no deterministic unmodeled dynamics, the proposed approach can ensure a Õ(√t) Regret, matching existing literature. Experiments are provided to support all theoretical derivations, which show that the spectral lines-based approach outperforms the Gaussian noise-based method when unmodeled dynamics are present, in terms of both parameter estimation error and Regret obtained using the parameter estimates with a Linear Quadratic Regulator in feedback.",['Journal Track'],[],"['Arnab Sarker', 'Peter Fisher', 'Joseph Gaudio', 'Anuradha Annaswamy']","['Massachusetts Institute of Technology, United States of America', 'Massachusetts Institute of Technology, United States of America', 'Massachusetts Institute of Technology, United States of America', 'Massachusetts Institute of Technology, United States of America']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30614,Fairness & Bias,Post-trained Convolution Networks for Single Image Super-resolution (Abstract Reprint),"A new method is proposed to increase the accuracy of the state-of-the-art single image super-resolution (SISR) using novel training procedure. The proposed method, named post-trained convolutional neural network (CNN), is carried out stochastic dual simplex algorithm (SDSA) in the last reconstruction layer. The method utilizes contextual information to update the last reconstruction layer of CNN. The extracted contextual information is projected to the last reconstructed layer by optimized weights and the bias is managed through SDSA. Post-trained CNN is applied to the very deep super-resolution (VDSR) method to show its performance. The quantitative and visual results demonstrate that the proposed post-trained VDSR (PTVDSR) exhibits excellent and competitive performance when compared with the VDSR and other super-resolution methods.",['Journal Track'],[],['Seid Miad Zandavi'],"['School of Computer Science, The University of Sydney, Sydney, Australia\nSchool of Biotechnology and Biomolecular Sciences, The University of New South Wales (UNSW), Sydney, Australia\nDepartment of Pediatrics, Harvard Medical School, Boston, MA, USA']",['Australia']
https://ojs.aaai.org/index.php/AAAI/article/view/30309,Privacy & Data Governance,General Commerce Intelligence: Glocally Federated NLP-Based Engine for Privacy-Preserving and Sustainable Personalized Services of Multi-Merchants,"One of the most crucial capabilities in the commercial sector is a personalized prediction of a customer's next purchase. We present a novel method of creating a commerce intelligence engine that caters to multiple merchants intended for the UB Platform, managed by e-payment company Harex InfoTech. To cultivate this intelligence, we utilized payment receipt data and created a Natural Language Processing (NLP)-based commerce model using a Transformer to accommodate multinational and merchant trade. Our model, called General Commerce Intelligence (GCI), provides a range of services for merchants, including product recommendations, product brainstorming, product bundling, event promotions, collaborative marketing, target marketing, and demand fore-casting etc. To bolster user privacy and foster sustainable business collaboration, especially among micro-, small-, and medium-sized enterprises (MSMEs), the GCI model was trained through federated learning, especially with glocalization. This study delves into the structure, development, and assessment of GCI, showcasing its transformative capacity to implement User Centric AI and re-shape the global commerce landscape to benefit MSMEs.","['Business & E-commerce', 'Federated Learning', 'Natural Language', 'Recommendation Systems', 'Track: Deployed Applications']",[],"['Kyoung Jun Lee', 'Baek Jeong', 'Suhyeon Kim', 'Dam Kim', 'Dongju Park']","['Kyung Hee University\nHarex InfoTech', 'Kyung Hee University', 'Kyung Hee University', 'Harex InfoTech', 'Harex InfoTech']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30317,Privacy & Data Governance,HiFi-Gas: Hierarchical Federated Learning Incentive Mechanism Enhanced Gas Usage Estimation,"Gas usage estimation plays a critical role in various aspects of the power generation and delivery business, including budgeting, resource planning, and environmental preservation. Federated Learning (FL) has demonstrated its potential in enhancing the accuracy and reliability of gas usage estimation by enabling distributedly owned data to be leveraged, while ensuring privacy and confidentiality. However, to effectively motivate stakeholders to contribute their high-quality local data and computational resources for this purpose, incentive mechanism design is key. In this paper, we report our experience designing and deploying the Hierarchical FL Incentive mechanism for Gas usage estimation (HiFi-Gas) system. It is designed to cater to the unique structure of gas companies and their affiliated heating stations. HiFi-Gas provides effective incentivization in a hierarchical federated learning framework that consists of a horizontal federated learning (HFL) component for effective collaboration among gas companies and multiple vertical federated learning (VFL) components for the gas company and its affiliated heating stations. To motivate active participation and ensure fairness among gas companies and heating stations, we incorporate a multi-dimensional contribution-aware reward distribution function that considers both data quality and model contributions. Since its deployment in the ENN Group in December 2022, HiFi-Gas has successfully provided incentives for gas companies and heating stations to actively participate in FL training, resulting in more than 12% higher average gas usage estimation accuracy and substantial gas procurement cost savings. This implementation marks the first successful deployment of a hierarchical FL incentive approach in the energy industry.","['Federated Learning', 'Track: Deployed Applications']",[],"['Hao Sun', 'Xiaoli Tang', 'Chengyi Yang', 'Zhenpeng Yu', 'Xiuli Wang', 'Qijie Ding', 'Zengxiang Li', 'Han Yu']","['ENN Group', 'Nanyang Technological University', 'ENN Group', 'ENN Group', 'ENN Group', 'ENN Group', 'ENN Group', 'Nanyang Technological University (NTU)']","['', 'Singapore', '', '', '', '', '', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/30316,Security,IBCA: An Intelligent Platform for Social Insurance Benefit Qualification Status Assessment,"Social insurance benefits qualification assessment is an important task to ensure that retirees enjoy their benefits according to the regulations. It also plays a key role in curbing social security frauds. In this paper, we report the deployment of the Intelligent Benefit Certification and Analysis (IBCA) platform, an AI-empowered platform for verifying the status of retirees to ensure proper dispursement of funds in Shandong province, China. Based on an improved Gated Recurrent Unit (GRU) neural network, IBCA aggregates missing value interpolation, temporal information, and global and local feature extraction to perform accurate retiree survival rate prediction. Based on the predicted results, a reliability assessment mechanism based on Variational Auto-Encoder (VAE) and Monte-Carlo Dropout (MC Dropout) is executed to perform reliability assessment. Deployed since November 2019, the IBCA platform has been adopted by 12 cities across the Shandong province, handling over 50 terabytes of data. It has empowered human resources and social services, civil affairs, and health care institutions to collaboratively provide high-quality public services. Under the IBCA platform, the efficiency of resources utilization as well as the accuracy of benefit qualification assessment have been significantly improved. It has helped Dareway Software Co. Ltd earn over RMB 50 million of revenue.","['Health', 'Medical & Medicine', 'Track: Deployed Applications', 'Data Mining', 'Deep Learning and Neural Networks']",[],"['Yuliang Shi', 'Lin Cheng', 'Cheng Jiang', 'Hui Zhang', 'Guifeng Li', 'Xiaoli Tang', 'Han Yu', 'Zhiqi Shen', 'Cyril Leung']","['School of Software, Shandong University (SDU), Jinan, China\nDareway Software Co. Ltd, Jinan, China\nJoint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University (SDU), Jinan, China', 'School of Software, Shandong University (SDU), Jinan, China', 'Dareway Software Co. Ltd, Jinan, China', 'School of Software, Shandong University (SDU), Jinan, China\nDareway Software Co. Ltd, Jinan, China', 'Dareway Software Co. Ltd, Jinan, China', 'Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University (SDU), Jinan, China\nSchool of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore', 'Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University (SDU), Jinan, China\nSchool of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore', 'Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University (SDU), Jinan, China\nSchool of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore', 'Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Shandong University (SDU), Jinan, China\nDepartment of Electrical and Computer Engineering, The University of British Columbia (UBC), Vancouver, BC, Canada']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30321,Security,Improving Autonomous Separation Assurance through Distributed Reinforcement Learning with Attention Networks,"Advanced Air Mobility (AAM) introduces a new, efficient mode of transportation with the use of vehicle autonomy and electrified aircraft to provide increasingly autonomous transportation between previously underserved markets. Safe and efficient navigation of low altitude aircraft through highly dense environments requires the integration of a multitude of complex observations, such as surveillance, knowledge of vehicle dynamics, and weather. The processing and reasoning on these observations pose challenges due to the various sources of uncertainty in the information while ensuring cooperation with a variable number of aircraft in the airspace. These challenges coupled with the requirement to make safety-critical decisions in real-time rule out the use of conventional separation assurance techniques. We present a decentralized reinforcement learning framework to provide autonomous self-separation capabilities within AAM corridors with the use of speed and vertical maneuvers. The problem is formulated as a Markov Decision Process and solved by developing a novel extension to the sample-efficient, off-policy soft actor-critic (SAC) algorithm. We introduce the use of attention networks for variable-length observation processing and a distributed computing architecture to achieve high training sample throughput as compared to existing approaches. A comprehensive numerical study shows that the proposed framework can ensure safe and efficient separation of aircraft in high density, dynamic environments with various sources of uncertainty.","['Distributed AI', 'Track: Emerging Applications', 'Transportation', 'Deep Learning and Neural Networks']",[],"['Marc W. Brittain', 'Luis E. Alvarez', 'Kara Breeden']","['MIT Lincoln Laboratory', 'MIT Lincoln Laboratory', 'MIT Lincoln Laboratory']","['Serbia', 'Serbia', 'Serbia']"
https://ojs.aaai.org/index.php/AAAI/article/view/30322,Privacy & Data Governance,Neural Bookmarks: Information Retrieval with Deep Learning and EEG Data,"In neural memory decoding, a concept being mentally recalled is identified using brain data. Recently, the feasibility of neural memory decoding with EEG data has been demonstrated. Here we propose a new application – neural information retrieval – that uses neural memory decoding to allow a document to be retrieved merely by thinking about it. In this paper we describe neural memory decoding, define the application of neural information retrieval, present experimental results related to the practicality of the application, and discuss issues of deployment and data privacy.","['Deep Learning and Neural Networks', 'Bioinformatics', 'Information and Knowledge Access', 'Multidisciplinary Topics and Applications', 'Track: Emerging Applications']",[],"['Glenn Bruns', 'Michael Haidar']","['California State University, Monterey Bay', 'California State University, Monterey Bay']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30326,Fairness & Bias,Pharmacokinetics-Informed Neural Network for Predicting Opioid Administration Moments with Wearable Sensors,"Long-term and high-dose prescription opioid use places individuals at risk for opioid misuse, opioid use disorder (OUD), and overdose. Existing methods for monitoring opioid use and detecting misuse rely on self-reports, which are prone to reporting bias, and toxicology testing, which may be infeasible in outpatient settings. Although wearable technologies for monitoring day-to-day health metrics have gained significant traction in recent years due to their ease of use, flexibility, and advancements in sensor technology, their application within the opioid use space remains underexplored. In the current work, we demonstrate that oral opioid administrations can be detected using physiological signals collected from a wrist sensor. More importantly, we show that models informed by opioid pharmacokinetics increase reliability in predicting the timing of opioid administrations. Forty-two individuals who were prescribed opioids as a part of their medical treatment in-hospital and after discharge were enrolled. Participants wore a wrist sensor throughout the study, while opioid administrations were tracked using electronic medical records and self-reports. We collected 1,983 hours of sensor data containing 187 opioid administrations from the inpatient setting and 927 hours of sensor data containing 40 opioid administrations from the outpatient setting. We demonstrate that a self-supervised pre-trained model, capable of learning the canonical time series of plasma concentration of the drug derived from opioid pharmacokinetics, can reliably detect opioid administration in both settings. Our work suggests the potential of pharmacokinetic-informed, data-driven models to objectively detect opioid use in daily life.","['Machine Learning', 'Pattern recognition', 'Health', 'Medical & Medicine']",[],"['Bhanu Teja Gullapalli', 'Stephanie Carreiro', 'Brittany P Chapman', 'Eric L Garland', 'Tauhidur Rahman']","['University of California San Diego', 'UMass Chan Medical School', 'UMass Chan Medical School', 'University of Utah', 'University of California San Diego']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30327,Security,VeriCompress: A Tool to Streamline the Synthesis of Verified Robust Compressed Neural Networks from Scratch,"AI's widespread integration has led to neural networks (NN) deployment on edge and similar limited-resource platforms for safety-critical scenarios. Yet, NN's fragility raises concerns about reliable inference. Moreover, constrained platforms demand compact networks. This study introduces VeriCompress, a tool that automates the search and training of compressed models with robustness guarantees. These models are well-suited for safety-critical applications and adhere to predefined architecture and size limitations, making them deployable on resource-restricted platforms. The method trains models 2-3 times faster than the state-of-the-art approaches, surpassing them by average accuracy and robustness gains of 15.1 and 9.8 percentage points, respectively. When deployed on a resource-restricted generic platform, these models require 5-8 times less memory and 2-4 times less inference time than models used in verified robustness literature. Our comprehensive evaluation across various model architectures and datasets, including MNIST, CIFAR, SVHN, and a relevant pedestrian detection dataset, showcases VeriCompress's capacity to identify compressed verified robust models with reduced computation overhead compared to current standards. This underscores its potential as a valuable tool for end users, such as developers of safety-critical applications on edge or Internet of Things platforms, empowering them to create suitable models for safety-critical, resource-constrained platforms in their respective domains.","['Formal analysis and verification', 'Track: Emerging Applications']",[],"['Sawinder Kaur', 'Yi Xiao', 'Asif Salekin']","['Syracuse University', 'Syracuse University', 'Syracuse University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30330,Fairness & Bias,Combining Machine Learning and Queueing Theory for Data-Driven Incarceration-Diversion Program Management,"Incarceration-diversion programs have proven effective in reducing recidivism. Accurate prediction of the number of individuals with different characteristics in the program and their program outcomes based on given eligibility criteria is crucial for successful implementation, because this prediction serves as the foundation for determining the appropriate program size and the consequent staffing requirements. However, this task poses challenges due to the complexities arising from varied outcomes and lengths-of-stay for the diverse individuals in incarceration-diversion programs. In collaboration with an Illinois government agency, we develop a framework to address these issues. Our framework combines ML and queueing model simulation, providing accurate predictions for the program census and interpretable insights into program dynamics and the impact of different decisions in counterfactual scenarios. Additionally, we deploy a user-friendly web app beta-version that allows program managers to visualize census data by counties and race groups. We showcase two decision support use cases: Changing program admission criteria and launching similar programs in new counties.","['Government', 'Machine Learning', 'Analytics and Data Science', 'Criminal Justice', 'Track: Emerging Applications']",[],"['Bingxuan Li', 'Antonio Castellanos', 'Pengyi Shi', 'Amy Ward']","['Purdue University', 'University of Chicago', 'Purdue University', 'University of Chicago']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30346,Transparency & Explainability,AI Evaluation Authorities: A Case Study Mapping Model Audits to Persistent Standards,"Intelligent system audits are labor-intensive assurance activities that are typically performed once and discarded along with the opportunity to programmatically test all similar products for the market. This study illustrates how several incidents (i.e., harms) involving Named Entity Recognition (NER) can be prevented by scaling up a previously-performed audit of NER systems. The audit instrument's diagnostic capacity is maintained through a security model that protects the underlying data (i.e., addresses Goodhart's Law). An open-source evaluation infrastructure is released along with an example derived from a real-world audit that reports aggregated findings without exposing the underlying data.",['Track: AI Incidents and Best Practices (paper)'],[],"['Arihant Chadda', 'Sean McGregor', 'Jesse Hostetler', 'Andrea Brennen']","['IQT Labs', 'UL Digital Safety Research Institute', 'UL Digital Safety Research Institute', 'IQT Labs']","['', 'India', 'India', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30349,Security,Merging AI Incidents Research with Political Misinformation Research: Introducing the Political Deepfakes Incidents Database,"This article presents the Political Deepfakes Incidents Database (PDID), a collection of politically-salient deepfakes, encompassing synthetically-created videos, images, and less-sophisticated `cheapfakes.' The project is driven by the rise of generative AI in politics, ongoing policy efforts to address harms, and the need to connect AI incidents and political communication research. The database contains political deepfake content, metadata, and researcher-coded descriptors drawn from political science, public policy, communication, and misinformation studies. It aims to help reveal the prevalence, trends, and impact of political deepfakes, such as those featuring major political figures or events. The PDID can benefit policymakers, researchers, journalists, fact-checkers, and the public by providing insights into deepfake usage, aiding in regulation, enabling in-depth analyses, supporting fact-checking and trust-building efforts, and raising awareness of political deepfakes. It is suitable for research and application on media effects, political discourse, AI ethics, technology governance, media literacy, and countermeasures.","['Databases', 'Generative AI', 'Track: AI Incidents and Best Practices (paper)']",[],"['Christina P. Walker', 'Daniel S. Schiff', 'Kaylyn Jackson Schiff']","['Purdue University', 'Purdue University', 'Purdue University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30348,Transparency & Explainability,AI Risk Profiles: A Standards Proposal for Pre-deployment AI Risk Disclosures,"As AI systems’ sophistication and proliferation have increased, awareness of the risks has grown proportionally. The AI industry is increasingly emphasizing the need for transparency, with proposals ranging from standardizing use of technical disclosures, like model cards, to regulatory licensing regimes. Since the AI value chain is complicated, with actors bringing varied expertise, perspectives, and values, it is crucial that consumers of transparency disclosures be able to understand the risks of the AI system in question. In this paper we propose a risk profiling standard which can guide downstream decision-making, including triaging further risk assessment, informing procurement and deployment, and directing regulatory frameworks. The standard is built on our proposed taxonomy of AI risks, which distills the wide variety of risks proposed in the literature into a high-level categorization. We outline the myriad data sources needed to construct informative Risk Profiles and propose a template and methodology for collating risk information into a standard, yet flexible, structure. We apply this methodology to a number of prominent AI systems using publicly available information. To conclude, we discuss design decisions for the profiles and future work.","['Assurance', 'Generative AI', 'Track: AI Incidents and Best Practices (paper)', 'Education and Training']",[],"['Eli Sherman', 'Ian Eisenberg']","['Credo AI', 'Credo AI']","['', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30347,Security,When Your AI Becomes a Target: AI Security Incidents and Best Practices,"In contrast to vast academic efforts to study AI security, few real-world reports of AI security incidents exist. Released incidents prevent a thorough investigation of the attackers' motives, as crucial information about the company and AI application is missing. As a consequence, it often remains unknown how to avoid incidents.     We tackle this gap and combine previous reports with freshly collected incidents to a small database of 32 AI security incidents. We analyze the attackers' target and goal, influencing factors, causes, and mitigations. Many incidents stem from non-compliance with best practices in security and privacy-enhancing technologies.  In the case of direct AI attacks, access control may provide some mitigation, but there is little scientific work on best practices. Our paper is thus a call for action to address these gaps.","['Multidisciplinary Topics and Applications', 'Human-Computer Interaction', 'Machine Learning', 'Track: AI Incidents and Best Practices (paper)']",[],"['Kathrin Grosse', 'Lukas Bieringer', 'Tarek R. Besold', 'Battista Biggio', 'Alexandre Alahi']","['EPFL, Switzerland', 'QuantPi, Germany', 'TU Eindhoven, The Netherlands', 'University of Cagliari, Italy', 'EPFL, Switzerland']","['Switzerland', '', 'Netherlands', 'Italy', 'Switzerland']"
https://ojs.aaai.org/index.php/AAAI/article/view/30356,Fairness & Bias,Practical Sentiment Analysis for Education: The Power of Student Crowdsourcing,"Sentiment analysis provides a promising tool to automatically assess the emotions voiced in written student feedback such as periodically collected unit-of-study reflections. The commonly used dictionary-based approaches are limited to major languages and fail to capture contextual differences.  Pretrained large language models have been shown to be biased and online versions raise privacy concerns. Hence, we resort to traditional supervised machine learning (ML) approaches which are designed to overcome these issues by learning from domain-specific labeled data. However, these labels are hard to come by -- in our case manually annotating student feedback is prone to bias and time-consuming, especially in high-enrollment courses. In this work, we investigate the use of student crowdsourced labels for supervised sentiment analysis for education. Specifically, we compare crowdsourced and student self-reported labels with human expert annotations and use them in various ML approaches to evaluate the performance on predicting emotions of written student feedback collected from large computer science classes. We find that the random forest model trained with student-crowdsourced labels tremendously improves the identification of reflections with negative sentiment.  In addition to our quantitative study, we describe our crowdsourcing experiment which was intentionally designed to be an educational activity in an introduction to data science course.","['Crowdsourcing', 'Sentiment Analysis', 'Computer Science Education', 'Student Feedback']",[],"['Robert Kasumba', 'Marion Neumman']","['Washington University in St. Louis', 'Washington University in St. Louis']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30362,Security,CyberQ: Generating Questions and Answers for Cybersecurity Education Using Knowledge Graph-Augmented LLMs,"Building a skilled cybersecurity workforce is paramount to building a safer digital world. However, the diverse skill set, constantly emerging vulnerabilities, and deployment of new cyber threats make learning cybersecurity challenging. Traditional education methods struggle to cope with cybersecurity's rapidly evolving landscape and keep students engaged and motivated. Different studies on students' behaviors show that an interactive mode of education by engaging through a question-answering system or dialoguing is one of the most effective learning methodologies. There is a strong need to create advanced AI-enabled education tools to promote interactive learning in cybersecurity. Unfortunately, there are no publicly available standard question-answer datasets to build such systems for students and novice learners to learn cybersecurity concepts, tools, and techniques. The education course material and online question banks are unstructured and need to be validated and updated by domain experts, which is tedious when done manually. In this paper, we propose CyberGen, a novel unification of large language models (LLMs) and knowledge graphs (KG) to generate the questions and answers for cybersecurity automatically. Augmenting the structured knowledge from knowledge graphs in prompts improves factual reasoning and reduces hallucinations in LLMs. We used the knowledge triples from cybersecurity knowledge graphs (AISecKG) to design prompts for ChatGPT and generate questions and answers using different prompting techniques. Our question-answer dataset, CyberQ, contains around 4k pairs of questions and answers. The domain expert manually evaluated the random samples for consistency and correctness. We train the generative model using the CyberQ dataset for question answering task.","['AI In Education', 'Question-Answering Systems', 'QA Dataset', 'Large Language Models (LLM)', 'Knowledge Graphs (KG)', 'Cybersecurity Education']",[],"['Garima Agrawal', 'Kuntal Pal', 'Yuli Deng', 'Huan Liu', 'Ying-Chih Chen']","['Arizona State University', 'Arizona State University', 'Arizona State University', 'Arizona State University', 'Arizona State University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30373,Fairness & Bias,A Picture Is Worth a Thousand Words: Co-designing Text-to-Image Generation Learning Materials for K-12 with Educators,"Text-to-image generation (TTIG) technologies are Artificial Intelligence (AI) algorithms that use natural language algorithms in combination with visual generative algorithms. TTIG tools have gained popularity in recent months, garnering interest from non-AI experts, including educators and K-12 students. While they have exciting creative potential when used by K-12 learners and educators for creative learning, they are also accompanied by serious ethical implications, such as data privacy, spreading misinformation, and algorithmic bias. Given the potential learning applications, social implications, and ethical concerns, we designed 6-hour learning materials to teach K-12 teachers from diverse subject expertise about the technical implementation, classroom applications, and ethical implications of TTIG algorithms. We piloted the learning materials titled “Demystify text-to-image generative tools for K-12 educators"" with 30 teachers across two workshops with the goal of preparing them to teach about and use TTIG tools in their classrooms. We found that teachers demonstrated a technical, applied and ethical understanding of TTIG algorithms and successfully designed prototypes of teaching materials for their classrooms.","['Generative AI', 'Text-to-image Generation', 'Creative ML', 'Teacher Education']",[],"['Safinah Ali', 'Prerna Ravi', 'Katherine Moore', 'Hal Abelson', 'Cynthia Breazeal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30377,Security,"AI, Ethics, and Education: The Pioneering Path of Sidekick Academy","Generative artificial intelligence (AI) is swiftly cementing its role as an indispensable tool for students transitioning from K-12 to higher education and professional spheres. Yet, harnessing its full potential requires more than mere familiarity. Students must be equipped with the skills to engage with AI both productively and ethically. Left unchecked, AI usage can pose risks, especially if students lack proper guidance or understanding of their actions. Moreover, effective interaction with AI necessitates skills in prompt engineering to yield desired outcomes. Sidekick Academy is a digital online platform where students can safely experiment with and learn about AI. This article delves into the genesis of Sidekick Academy, offering a glimpse into its lessons on how to use AI and complex debate on ethical use. It also sheds light on the academy's ""sandbox"" - a secure space for students to explore AI without jeopardizing their safety or privacy.","['Ethics', 'Teaching AI', 'Generative Artificial Intelligence', 'K-12 Pedagogy']",[],"['Elizabeth Radday', 'Matt Mervis']","['EdAdvance', 'EdAdvance']","['', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30374,Privacy & Data Governance,Constructing Dreams Using Generative AI,"Generative AI tools introduce new and accessible forms of media creation for youth. They also raise ethical concerns about the generation of fake media, data protection, privacy and ownership of AI-generated art. Since generative AI is already being used in products used by youth, it is critical that they understand how these tools work and how they can be used or misused. In this work, we facilitated students’ generative AI learning through expression of their imagined future identities. We designed a learning workshop - Dreaming with AI - where students learned about the inner workings of generative AI tools, used text-to-image generation algorithms to create their imaged future dreams, reflected on the potential benefits and harms of generative AI tools and voiced their opinions about policies for the use of these tools in classrooms. In this paper, we present the learning activities and experiences of 34 high school students who engaged in our workshops. Students reached creative learning objectives by using prompt engineering to create their future dreams, gained technical knowledge by learning the abilities, limitations, text-visual mappings and applications of generative AI, and identified most potential societal benefits and harms of generative AI.","['High School', 'Generative AI', 'Creative Learning', 'Futuring']",[],"['Safinah Ali', 'Prerna Ravi', 'Randi Williams', 'Daniella DiPaola', 'Cynthia Breazeal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30379,Fairness & Bias,Dr. R.O. Bott Will See You Now: Exploring AI for Wellbeing with Middle School Students,"Artificial Intelligence (AI) is permeating almost every area of society, reshaping how many people, including youth, navigate the world. Despite the increased presence of AI, most people lack a baseline knowledge of how AI works. Moreover, social barriers often hinder equal access to AI courses, perpetuating disparities in participation in the field. To address this, it is crucial to design AI curricula that are effective, inclusive, and relevant, especially to learners from backgrounds that are historically excluded from working in tech. In this paper, we present AI for Wellbeing, a curriculum where students explore conversational AI and the ethical considerations around using it to promote wellbeing. We specifically designed content, educator materials, and educational technologies to meet the interests and needs of students and educators from diverse backgrounds. We piloted AI for Wellbeing in a 5-day virtual workshop with middle school teachers and students. Then, using a mixed-methods approach, we analyzed students' work and teachers' feedback. Our results suggest that the curriculum content and design effectively engaged students, enabling them to implement meaningful AI projects for wellbeing. We hope that the design of this curriculum and insights from our evaluation will inspire future efforts to create culturally relevant K-12 AI curricula.","['Artificial Intelligence Education', 'Ethics', 'Middle School Computing Education', 'Social Robots', 'Machine Learning', 'Chatbots', 'Design Justice']",[],"['Randi Williams', 'Sharifa Alghowinem', 'Cynthia Breazeal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30390,Transparency & Explainability,Semi-factual Explanations in AI,"Most of the recent works on post-hoc example-based eXplainable AI (XAI) methods revolves around employing counterfactual explanations to provide justification of the predictions made by AI systems. Counterfactuals show what changes to the input-features change the output decision. However, a lesser-known, special-case of the counterfacual is the semi-factual, which provide explanations about what changes to the input-features do not change the output decision.  Semi-factuals are potentially as useful as counterfactuals but have received little attention in the XAI literature.  My doctoral research aims to establish a comprehensive framework for the use of semi-factuals in XAI by developing novel methods for their computation, supported by user tests.","['Semi-factual Explanation', 'Counterfactual Explanation', 'Interpretable AI', 'AI Fairness', 'Trustworthy AI']",[],['Saugat Aryal'],['University College Dublin'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30387,Transparency & Explainability,Discovering Heterogeneous Causal Effects in Relational Data,"Causal inference in relational data should account for the non-IID nature of the data and the interference phenomenon, which occurs when a unit's outcome is influenced by the treatments or outcomes of others. Existing solutions to causal inference under interference consider either homogeneous influence from peers or specific heterogeneous influence contexts (e.g., local neighborhood structure). This thesis investigates causal reasoning in relational data and the automated discovery of heterogeneous causal effects under arbitrary heterogeneous peer influence contexts and effect modification.","['Causal Inference', 'Network Effects', 'Network Interference', 'Heterogeneous Treatment Effects', 'Causal Discovery']",[],['Shishir Adhikari'],['University of Illinois Chicago'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30391,Transparency & Explainability,Domain Engineering to Represent Human Behavior Using Multi-Agent Planning and Inductive Methodologies,"This research combines multi agent planning, the psycholinguistics of question asking, procedural grounded theory, and hierarchical task networks to represent domains for automated planning.","['CMS: Social Cognition And Interaction', 'HAI: Human-Aware Planning And Behavior Prediction', 'KRR: Action Change And Causality', 'NLP: Discourse Pragmatics And Argument Mining', 'PRS: Activity And Plan Recognition']",[],['Salena Torres Ashton'],"['School of Information, University of Arizona']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30393,Fairness & Bias,"Identifying, Mitigating, and Anticipating Bias in Algorithmic Decisions","Today's machine learning (ML) applications predominantly adhere to a standard paradigm: the decision maker designs the algorithm by optimizing a model for some objective function. While this has proven to be a powerful approach in many domains, it comes with inherent side effects: the power over the algorithmic outcomes lies solely in the hands of the algorithm designer, and alternative objectives, such as fairness, are often disregarded. This is particularly problematic if the algorithm is used to make consequential decisions that affect peoples lives. My research focuses on developing principled methods to characterize and address the mismatch between these different objectives.","['Algorithmic Fairness', 'Responsible AI', 'Data Science For Social Good', 'Feedback Loops', 'Ethical Automated Decision Making']",[],['Joachim Baumann'],['University of Zurich'],['Switzerland']
https://ojs.aaai.org/index.php/AAAI/article/view/30396,Transparency & Explainability,Temporal Dependencies and Spatio-Temporal Patterns of Time Series Models,"The widespread use of Artificial Intelligence (AI) has highlighted the importance of understanding AI model behavior. This understanding is crucial for practical decision-making, assessing model reliability, and ensuring trustworthiness. Interpreting time series forecasting models faces unique challenges compared to image and text data. These challenges arise from the temporal dependencies between time steps and the evolving importance of input features over time. My thesis focuses on addressing these challenges by aiming for more precise explanations of feature interactions, uncovering spatiotemporal patterns, and demonstrating the practical applicability of these interpretability techniques using real-world datasets and state-of-the-art deep learning models.","['Deep Learning', 'Interpretation', 'Time Series', 'Spatio-temporal', 'Explainability']",[],['Md. Khairul Islam'],['University of Virginia'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30395,Transparency & Explainability,Towards Trustworthy Autonomous Systems via Conversations and Explanations,"Autonomous systems fulfil an increasingly important role in our societies, however, AI-powered systems have seen less success over the years, as they are expected to tackle a range of social, legal, or technological challenges and modern neural network-based AI systems cannot yet provide guarantees to many of these challenges. Particularly important is that these systems are black box decision makers, eroding human oversight, contestation, and agency. To address this particular concern, my thesis focuses on integrating social explainable AI with cognitive methods and natural language processing to shed light on the internal processes of autonomous systems in a way accessible to lay users. I propose a causal explanation generation model for decision-making called CEMA based on counterfactual simulations in multi-agent systems. I also plan to integrate CEMA with a broader natural language processing pipeline to support targeted and personalised explanations that address people's cognitive biases. I hope that my research will have a positive impact on the public acceptance of autonomous agents by building towards more trustworthy AI.","['Trustworthy Autonomous Systems', 'Social Explainable AI', 'Multi-agent Systems', 'Human-centric AI', 'AI Regulation']",[],['Balint Gyevnar'],['University of Edinburgh'],['United Kingdom']
https://ojs.aaai.org/index.php/AAAI/article/view/30399,Transparency & Explainability,Making AI Policies Transparent to Humans through Demonstrations,"Demonstrations are a powerful way of increasing the transparency of AI policies to humans. Though we can approximately model human learning from demonstrations as inverse reinforcement learning, we note that human learning can differ from algorithmic learning in key ways, e.g. humans are computationally limited and may sometimes struggle to understand all of the nuances of a demonstration. Unlike related work that provide demonstrations to humans that simply maximize information gain, I leverage concepts from the human education literature, such as the zone of proximal development and scaffolding, to show demonstrations that balance informativeness and difficulty of understanding to maximize human learning.","['Explainable AI', 'Policy Summarization', 'Transparency', 'Human-agent Interaction']",[],['Michael S. Lee'],['Carnegie Mellon University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30401,Transparency & Explainability,Thesis Summary: Operationalizing User-Inclusive Transparency in Artificial Intelligence Systems,"Artificial intelligence system architects can increase user trust by designing systems that are inherently transparent. We propose the idea of representing an AI system as an amalgamation of the AI Model (algorithms), data (input and output, including outcomes), and the user interface with visual interpretations (e.g. graphs, Venn diagrams). By designing human controls and feedback mechanisms for AI systems that allow users to exert control over them we can integrate transparency into existing user interfaces. Our plan is to design prototypes of transparent user interfaces for AI systems using well-known usability principles. By conducting surveys we will study their impact to see if these principles help the user to work with the AI system with confidence and if the user perceives the system to be adequately transparent.","['Transparency', 'Usability Principles', 'Explanations', 'Algorithmic Bias', 'Uncertainity', 'Algorithmic Fairness', 'Data', 'Accuracy', 'Trust']",[],['Deepa Muralidhar'],['Georgia State University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30402,Transparency & Explainability,Learning Generalizable and Composable Abstractions for Transfer in Reinforcement Learning,"Reinforcement Learning (RL) in complex environments presents many challenges: agents require learning concise representations of both environments and behaviors for efficient reasoning and generalizing experiences to new, unseen situations. However, RL approaches can be sample-inefficient and difficult to scale, especially in long-horizon sparse reward settings. To address these issues, the goal of my doctoral research is to develop methods that automatically construct semantically meaningful state and temporal abstractions for efficient transfer and generalization. In my work, I develop hierarchical approaches for learning transferable, generalizable knowledge in the form of symbolically represented options, as well as for integrating search techniques with RL to solve new problems by efficiently composing the learned options. Empirical results show that the resulting approaches effectively learn and transfer knowledge, achieving superior sample efficiency compared to SOTA methods while also enhancing interpretability.","['Hierarchical Planning And Reinforcement Learning', 'Transfer And Generalization', 'Learning Abstractions', 'Option Discovery', 'Representation Learning']",[],['Rashmeet Kaur Nayyar'],['Arizona State University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30410,Fairness & Bias,The Generalization and Robustness of Transformer-Based Language Models on Commonsense Reasoning,"The advent of powerful transformer-based discriminative language models and, more recently, generative GPT-family models, has led to notable advancements in natural language processing (NLP), particularly in commonsense reasoning tasks. One such task is commonsense reasoning, where performance is usually evaluated through multiple-choice question-answering benchmarks. Till date, many such benchmarks have been proposed and `leaderboards' tracking state-of-the-art performance on those benchmarks suggest that transformer-based models are approaching human-like performance. However, due to documented problems such as hallucination and bias, the research focus is shifting from merely quantifying accuracy on the task to an in-depth, context-sensitive probing of LLMs' generalization and robustness. To gain deeper insight into diagnosing these models' performance in commonsense reasoning scenarios, this thesis addresses three main studies: the generalization ability of transformer-based language models on commonsense reasoning, the trend in confidence distribution of these language models confronted with ambiguous inference tasks, and a proposed risk-centric evaluation framework for both discriminative and generative language models.","['Commonsense Reasoning', 'Large Language Models', 'Robustness', 'Generalizability', 'Natural Language Processing']",[],['Ke Shen'],['USC Information Sciences Institute'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30412,Transparency & Explainability,Autonomous Policy Explanations for Effective Human-Machine Teaming,"Policy explanation, a process for describing the behavior of an autonomous system, plays a crucial role in effectively conveying an agent's decision-making rationale to human collaborators and is essential for safe real-world deployments. It becomes even more critical in effective human-robot teaming, where good communication allows teams to adapt and improvise successfully during uncertain situations by enabling value alignment within the teams. This thesis proposal focuses on improving human-machine teaming by developing novel human-centered explainable AI (xAI) techniques that empower autonomous agents to communicate their capabilities and limitations via multiple modalities, teach and influence human teammates' behavior as decision-support systems, and effectively build and manage trust in HRI systems.","['Explainable AI', 'Human-robot Interaction', 'Reinforcement Learning', 'Human-centric AI', 'Augmented Reality']",[],['Aaquib Tabrez'],['University of Colorado Boulder'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30415,Transparency & Explainability,Neuro-Symbolic Integration for Reasoning and Learning on Knowledge Graphs,"The goal of this thesis is to address knowledge graph completion tasks using neuro-symbolic methods. Neuro-symbolic methods allow the joint utilization of symbolic information defined as meta-rules in ontologies and knowledge graph embedding methods that represent entities and relations of the graph in a low-dimensional vector space. This approach has the potential to improve the resolution of knowledge graph completion tasks in terms of reliability, interpretability, data-efficiency and robustness.","['Neuro-symbolic Integration', 'Knowledge Graph Embedding', 'Knowledge Graphs', 'Graph Learning']",[],['Luisa Werner'],"['Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG F-38000 Grenoble, France']",['France']
https://ojs.aaai.org/index.php/AAAI/article/view/30413,Transparency & Explainability,To Know the Causes of Things: Text Mining for Causal Relations,"Causality expresses the relation between two arguments, one of which represents the cause and the other the effect (or consequence). Causal text mining refers to the extraction and usage of causal information from text. Given an input sequence, we are interested to know if and where causal information occurs. My research is focused on the end-to-end challenges of causal text mining. This involves extracting, representing, and applying causal knowledge from unstructured text. The corresponding research questions are: (1) How to extract causal information from unstructured text effectively? (2) How to represent extracted causal relationships in a graph that is interpretable and useful for some application? (3) How can we capitalize on extracted causal knowledge for downstream tasks? What tasks or fields will benefit from such knowledge? In this paper, I outline past and on-going works, and highlight future research challenges.","['Causal Text Mining', 'Natural Language Processing', 'Reasoning', 'Knowledge Mining', 'Knowledge Graphs']",[],['Fiona Anting Tan'],"['Institute of Data Science, National University of Singapore']",['Singapore']
https://ojs.aaai.org/index.php/AAAI/article/view/30422,Privacy & Data Governance,The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers (Student Abstract),"To enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption.  In particular, we believe that the ReLU and addition-based attention mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the costly multiplication of encrypted variables.","['AI Architectures', 'Computational Sustainability', 'Knowledge Representation', 'Machine Learning']",[],['Rickard Brännvall'],"['Computer Science Department, RISE Research Institutes of Sweden\nMachine Learning Group, Luleå University of Technology, Sweden']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30424,Security,Data-Driven Discovery of Design Specifications (Student Abstract),"Ensuring a machine learning model’s trustworthiness is crucial to prevent potential harm. One way to foster trust is through the formal verification of the model’s adherence to essential design requirements. However, this approach relies on well-defined, application-domain-centric criteria with which to test the model, and such specifications may be cumbersome to collect in practice. We propose a data-driven approach for creating specifications to evaluate a trained model effectively. Implementing this framework allows us to prove that the model will exhibit safe behavior while minimizing the false-positive prediction rate. This strategy enhances predictive accuracy and safety, providing deeper insight into the model’s strengths and weaknesses, and promotes trust through a systematic approach.","['Design Specifications', 'Formal Verification', 'Trustworthy', 'Data-driven']",[],"['Angela Chen', 'Nicholas Gisolfi', 'Artur Dubrawski']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30425,Transparency & Explainability,Interpreting Temporal Knowledge Graph Reasoning (Student Abstract),"Temporal knowledge graph reasoning is an essential task that holds immense value in diverse real-world applications. Existing studies mainly focus on leveraging structural and sequential dependencies, excelling in tasks like entity and link prediction. However, they confront a notable interpretability gap in their predictions, a pivotal facet for comprehending model behavior. In this study, we propose an innovative method, LSGAT, which not only exhibits remarkable precision in entity predictions but also enhances interpretability by identifying pivotal historical events influencing event predictions. LSGAT enables concise explanations for prediction outcomes, offering valuable insights into the otherwise enigmatic ""black box"" reasoning process. Through an exploration of the implications of the most influential events, it facilitates a deeper understanding of the underlying mechanisms governing predictions.","['Data Mining', 'Knowledge Discovery', 'Knowledge Representation', 'Applications Of AI']",[],"['Bin Chen', 'Kai Yang', 'Wenxin Tai', 'Zhangtao Cheng', 'Leyuan Liu', 'Ting Zhong', 'Fan Zhou']","['University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China\nKashi Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China\nKashi Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China\nKashi Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China\nKashi Institute of Electronics and Information Industry']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30428,Transparency & Explainability,Dual Mapping of 2D StyleGAN for 3D-Aware Image Generation and Manipulation (Student Abstract),"3D-aware GANs successfully solve the problem of 3D-consistency generation and furthermore provide a 3D shape of the generated object. However, the application of the volume renderer disturbs the disentanglement of the latent space, which makes it difficult to manipulate 3D-aware GANs and lowers the image quality of style-based generators. In this work, we devise a dual-mapping framework to make the generated images of pretrained 2D StyleGAN consistent in 3D space. We utilize a tri-plane representation to estimate the 3D shape of the generated object and two mapping networks to bridge the latent space of StyleGAN and the 3D tri-plane space. Our method does not alter the parameters of the pretrained generator, which means the interpretability of latent space is preserved for various image manipulations. Experiments show that our method lifts the 3D awareness of pretrained 2D StyleGAN to 3D-aware GANs and outperforms the 3D-aware GANs in controllability and image quality.","['3D-aware GAN', 'Pretrained GAN', 'Image Manipulation']",[],"['Zhuo Chen', 'Haimei Zhao', 'Chaoyue Wang', 'Bo Yuan', 'Xiu Li']","['Shenzhen International Graduate School, Tsinghua University', 'University of Sydney', 'University of Sydney', 'University of Queensland', 'Shenzhen International Graduate School, Tsinghua University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30442,Security,Spatial-Temporal Augmentation for Crime Prediction (Student Abstract),"Crime prediction stands as a pivotal concern within the realm of urban management due to its potential threats to public safety. While prior research has predominantly focused on unraveling the intricate dependencies among urban regions and temporal dynamics, the challenges posed by the scarcity and uncertainty of historical crime data have not been thoroughly investigated. This study introduces an innovative spatial-temporal augmented learning framework for crime prediction, namely STAug. In STAug, we devise a CrimeMix to improve the ability of generalization. Furthermore, we harness a spatial-temporal aggregation to capture and incorporate multiple correlations covering the temporal, spatial, and crime-type aspects. Experiments on two real-world datasets underscore the superiority of STAug over several baselines.","['Data Mining', 'Applications Of AI', 'AI And The Web']",[],"['Hongzhu Fu', 'Fan Zhou', 'Qing Guo', 'Qiang Gao']","['University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'School of Computer Science and Technology, Beijing Institute of Technology', 'Southwestern University of Finance and Economics\nKash Institute of Electronics and Information Industry']","['China', 'China', 'China', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30443,Fairness & Bias,Evaluating the Efficacy of Prompting Techniques for Debiasing Language Model Outputs (Student Abstract),"Achieving fairness in Large Language Models (LLMs) continues to pose a persistent challenge, as these models are prone to inheriting biases from their training data, which can subsequently impact their performance in various applications. There is a need to systematically explore whether structured prompting techniques can offer opportunities for debiased text generation by LLMs. In this work, we designed an evaluative framework to test the efficacy of different prompting techniques for debiasing text along different dimensions. We aim to devise a general structured prompting approach to achieve fairness that generalizes well to different texts and LLMs.","['LLM', 'Debiasing', 'Fairness', 'Zeroshot', 'Text Generation']",[],"['Shaz Furniturewala', 'Surgan Jandial', 'Abhinav Java', 'Simra Shahid', 'Pragyan Banerjee', 'Balaji Krishnamurthy', 'Sumit Bhatia', 'Kokil Jaidka']","['Birla Institute of Technology and Science Pilani, Pilani', 'MDSR Labs Adobe', 'MDSR Labs, Adobe', 'MDSR Labs Adobe', 'Birla Institute of Technology and Science Pilani, Pilani', 'MDSR Labs Adobe', 'MDSR Labs Adobe', 'National University of Singapore']","['India', 'India', 'India', 'India', 'India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30448,Security,BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract),"Image segmentation is foundational to computer vision applications, and the Segment Anything Model (SAM) has become a leading base model for these tasks. However, SAM falters in specialized downstream challenges, leading to various customized SAM models. We introduce BadSAM, a backdoor attack tailored for SAM, revealing that customized models can harbor malicious behaviors. Using the CAMO dataset, we confirm BadSAM's efficacy and identify SAM vulnerabilities. This study paves the way for the development of more secure and customizable vision foundation models.","['Backdoor Attacks', 'Foundation Model', 'Segment Anything Model']",[],"['Zihan Guan', 'Mengxuan Hu', 'Zhongliang Zhou', 'Jielu Zhang', 'Sheng Li', 'Ninghao Liu']","['The University of Virginia', 'The University of Virginia', 'The University of Georgia', 'The University of Georgia', 'The University of Virginia', 'The University of Georgia']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30455,Security,BertRLFuzzer: A BERT and Reinforcement Learning Based Fuzzer (Student Abstract),"We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).","['Machine Learning', 'Reinforcement Learning', 'Applications Of AI', 'AI And The Web', 'Fuzzing', 'BERT Models', 'Transformers', 'Security Vulnerabilities']",[],"['Piyush Jha', 'Joseph Scott', 'Jaya Sriram Ganeshna', 'Mudit Singh', 'Vijay Ganesh']","['Georgia Institute of Technology', 'University of Waterloo', 'University of Waterloo', 'University of Waterloo', 'Georgia Institute of Technology']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/30458,Transparency & Explainability,Evaluating the Effectiveness of Explainable Artificial Intelligence Approaches (Student Abstract),"Explainable Artificial Intelligence (XAI), a promising future technology in the field of healthcare, has attracted significant interest. Despite ongoing efforts in the development of XAI approaches, there has been inadequate evaluation of explanation effectiveness and no standardized framework for the evaluation has been established. This study aims to examine the relationship between subjective interpretability and perceived plausibility for various XAI explanations and to determine the factors affecting users' acceptance of the XAI explanation.","['Explainable Artificial Intelligence', 'AI For Healthcare', 'Focus Group Interview']",[],"['Jinsun Jung', 'Hyeoneui Kim']","['College of Nursing, Seoul National University\nCenter for Human-Caring Nurse Leaders for the Future by Brain Korea 21 (BK 21) Four Project', 'College of Nursing, Seoul National University\nCenter for Human-Caring Nurse Leaders for the Future by Brain Korea 21 (BK 21) Four Project\nThe Research Institute of Nursing Science']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30454,Transparency & Explainability,Explainable Earnings Call Representation Learning (Student Abstract),"Earnings call transcripts hold valuable insights that are vital for investors and analysts when making informed decisions. However, extracting these insights from lengthy and complex transcripts can be a challenging task. The traditional manual examination is not only time-consuming but also prone to errors and biases. Deep learning-based representation learning methods have emerged as promising and automated approaches to tackle this problem. Nevertheless, they may encounter significant challenges, such as the unreliability of the representation encoding process and certain domain-specific requirements in the context of finance. To address these issues, we propose a novel transcript representation learning model. Our model leverages the structural information of transcripts to effectively extract key insights, while endowing model with explainability via variational information bottleneck. Extensive experiments on two downstream financial tasks demonstrate the effectiveness of our approach.","['Representation Learning', 'Contrastive Learning', 'Earnings Call Transcript', 'Information Bottleneck']",[],"['Yanlong Huang', 'Yue Lei', 'Wenxin Tai', 'Zhangtao Cheng', 'Ting Zhong', 'Kunpeng Zhang']","['University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'University of Maryland, College Park']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30457,Security,Power Grid Anomaly Detection via Hybrid LSTM-GIN Model (Student Abstract),"Cyberattacks on power grids pose significant risks to national security. Power grid attacks typically lead to abnormal readings in power output, frequency, current, and voltage. Due to the interconnected structure of power grids, abnormalities can spread throughout the system and cause widespread power outages if not detected and dealt with promptly. Our research proposes a novel anomaly detection system for power grids that prevents overfitting. We created a network graph to represent the structure of the power grid, where nodes represent power grid components like generators and edges represent connections between nodes such as overhead power lines. We combine the capabilities of Long Short-Term Memory (LSTM) models with a Graph Isomorphism Network (GIN) in a hybrid model to pinpoint anomalies in the grid. We train our model on each category of nodes that serves a similar structural purpose to prevent overfitting of the model. We then assign each node in the graph a unique signature using a GIN. Our model achieved a 99.92% accuracy rate, which is significantly higher than a version of our model without structural encoding, which had an accuracy level of 97.30%. Our model allows us to capture structural and temporal components of power grids and develop an attack detection system with high accuracy without overfitting.","['Machine Learning', 'Anomaly Detection', 'Long Short-Term Memory', 'Graph Isomorphic Networks', 'Power Grids', 'Data-driven Security']",[],"['Amelia Jobe', 'Richard Ky', 'Sandra Luo', 'Akshay Dhamsania', 'Sumit Purohit', 'Edoardo Serra']","['Boise State University', 'San Jose State University', 'University of Texas at Dallas', 'Texas A&M University', 'Pacific Northwest National Laboratory', 'Boise State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30467,Security,Meta-Crafting: Improved Detection of Out-of-Distributed Texts via Crafting Metadata Space (Student Abstract),"Detecting out-of-distribution (OOD) samples is crucial for robust NLP models. Recent works observe two OOD types: background shifts (style change) and semantic shifts (content change), but existing detection methods vary in effectiveness for each type. To this end, we propose Meta-Crafting, a unified OOD detection method by constructing a new discriminative feature space utilizing 7 model-driven metadata chosen empirically that well detects both types of shifts. Our experimental results demonstrate state-of-the-art robustness to both shifts and significantly improved detection on stress datasets.","['Out-of-distribution Detection', 'Natural Langauge Processing', 'Adversarial Attacks']",[],"['Ryan Koo', 'Yekyung Kim', 'Dongyeop Kang', 'Jaehyung Kim']","['University of Minnesota', 'Hyundai Motor Group', 'University of Minnesota', 'Korea Advanced Institute of Science & Technology']","['United States', 'United States', 'United States', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30470,Transparency & Explainability,Automated Assessment of Fidelity and Interpretability: An Evaluation Framework for Large Language Models’ Explanations (Student Abstract),"As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability: fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations. We apply our framework to evaluate GPT-3.5 and the impact of prompts on the quality of its explanations. In conclusion, our framework streamlines the evaluation of explanations from LLMs, promoting the development of safer models.","['Large Language Models', 'Explainable AI', 'Explainability', 'Fidelity', 'Faithfulness', 'Interpretability']",[],"['Mu-Tien Kuo', 'Chih-Chung Hsueh', 'Richard Tzong-Han Tsai']","['Chingshin Academy\nResearch Center for Humanities and Social Sciences, Academia Sinica', 'Chingshin Academy\nResearch Center for Humanities and Social Sciences, Academia Sinica', 'Dept. of Computer Science and Engineering, National Central University, Taiwan\nResearch Center for Humanities and Social Sciences, Academia Sinica']","['Taiwan', 'Taiwan', 'Taiwan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30468,Security,Attacking CNNs in Histopathology with SNAP: Sporadic and Naturalistic Adversarial Patches (Student Abstract),"Convolutional neural networks (CNNs) are being increasingly adopted in medical imaging. However, in the race for developing accurate models, their robustness is often overlooked. This elicits a significant concern given the safety-critical nature of the healthcare system. Here, we highlight the vulnerability of CNNs against a sporadic and naturalistic adversarial patch attack (SNAP). We train SNAP to mislead the ResNet50 model predicting metastasis in histopathological scans of lymph node sections, lowering the accuracy by 27%. This work emphasizes the need for defense strategies before deploying CNNs in critical healthcare settings.","['Adversarial Attacks', 'Histopathology', 'Convolutional Neural Networks']",[],"['Daya Kumar', 'Abhijith Sharma', 'Apurva Narayan']","['Western University, London, ON, Canada', 'University of British Columbia, Kelowna, BC, Canada', 'Western University, London, ON, Canada\nUniversity of British Columbia, Kelowna, BC, Canada']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/30476,Transparency & Explainability,Fair Representation Learning with Maximum Mean Discrepancy Distance Constraint (Student Abstract),"Unsupervised learning methods such as principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoding are regularly used in dimensionality reduction within the statistical learning scene. However, despite a pivot toward fairness and explainability in machine learning over the past few years, there have been few rigorous attempts toward a generalized framework of fair and explainable representation learning. Our paper explores the possibility of such a framework that leverages maximum mean discrepancy to remove information derived from a protected class from generated representations. For the optimization, we introduce a binary search component to optimize the Lagrangian coefficients. We present rigorous mathematical analysis and experimental results of our framework applied to t-SNE.","['Fair Representation Learning', 'Fairness', 'MMD', 'Maximum Mean Discrepancy', 'PCA', 'T-SNE']",[],"['Alexandru Lopotenco', 'Ian Tong Pan', 'Jack Zhang', 'Guan Xiong Qiao']","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30481,Security,Towards Robustness to Natural Variations and Distribution Shift (Student Abstract),"This research focuses on improving the robustness of machine learning systems to natural variations and distribution shifts. A design trade space is presented, and various methods are compared, including adversarial training, data augmentation techniques, and novel approaches inspired by model-based robust optimization formulations.","['Robustness', 'Distribution Shift', 'Natural Variation', 'Deep Learning', 'AI Safety']",[],"['Josué Martínez-Martínez', 'Olivia Brown', 'Rajmonda Caceres']","['University of Connecticut\nMIT Lincoln Laboratory', 'MIT Lincoln Laboratory', 'MIT Lincoln Laboratory']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30487,Security,MaxEnt Loss: Calibrating Graph Neural Networks under Out-of-Distribution Shift (Student Abstract),"We present a new, simple and effective loss function for calibrating graph neural networks (GNNs). Miscalibration is the problem whereby a model's probabilities does not reflect it's correctness, making it difficult and possibly dangerous for real-world deployment. We compare our method against other baselines on a novel ID and OOD graph form of the Celeb-A faces dataset. Our findings show that our method improves calibration for GNNs, which are not immune to miscalibration in-distribution (ID) and out-of-distribution (OOD). Our code is available for review at https://github.com/dexterdley/CS6208/tree/main/Project.","['Graph Neural Networks', 'Calibration', 'Uncertainty Estimation', 'Machine Learning Safety', 'Out-of-Distribution']",[],['Dexter Neo'],['National University of Singapore'],['Singapore']
https://ojs.aaai.org/index.php/AAAI/article/view/30503,Fairness & Bias,FAIR-FER: A Latent Alignment Approach for Mitigating Bias in Facial Expression Recognition (Student Abstract),"Facial Expression Recognition (FER) is an extensively explored research problem in the domain of computer vision and artificial intelligence. FER, a supervised learning problem, requires significant training data representative of multiple socio-cultural demographic attributes. However, most of the FER dataset consists of images annotated by humans, which propagates individual and demographic biases. This work attempts to mitigate this bias using representation learning based on latent spaces, thereby increasing a deep learning model's fairness and overall accuracy.","['Computer Vision', 'Machine Learning', 'Applications Of AI']",[],"['Syed Sameen Ahmad Rizvi', 'Aryan Seth', 'Pratik Narang']","['Birla Institute of Technology & Science, Pilani', 'Birla Institute of Technology & Science, Pilani', 'Birla Institute of Technology & Science, Pilani']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30506,Privacy & Data Governance,Instance-Wise Laplace Mechanism via Deep Reinforcement Learning (Student Abstract),"Recent research has shown a growing interest in per-instance differential privacy (pDP), highlighting the fact that each data instance within a dataset may incur distinct levels of privacy loss. However, conventional additive noise mechanisms apply identical noise to all query outputs, thereby deteriorating data statistics. In this study, we propose an instance-wise Laplace mechanism, which adds non-identical Laplace noises to the query output for each data instance. A challenge arises from the complex interaction of additive noise, where the noise introduced to individual instances impacts the pDP of other instances, adding complexity and resilience to straightforward solutions. To tackle this problem, we introduce an instance-wise Laplace mechanism algorithm via deep reinforcement learning and validate its ability to better preserve data statistics on a real dataset, compared to the original Laplace mechanism.","['Constraint Satisfaction', 'Data Mining', 'Machine Learning', 'Reinforcement Learning', 'Applications Of AI']",[],"['Sehyun Ryu', 'Hosung Joo', 'Jonggyu Jang', 'Hyun Jong Yang']","['POSTECH', 'POSTECH', 'POSTECH', 'POSTECH']","['South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/30507,Privacy & Data Governance,Frequency Oracle for Sensitive Data Monitoring (Student Abstract),"As data privacy issues grow, finding the best privacy preservation algorithm for each situation is increasingly essential. This research has focused on understanding the frequency oracles (FO) privacy preservation algorithms. FO conduct the frequency estimation of any value in the domain. The aim is to explore how each can be best used and recommend which one to use with which data type. We experimented with different data scenarios and federated learning settings. Results showed clear guidance on when to use a specific algorithm.","['Federated Learning', 'Local Differential Privacy', 'Frequency Oracles']",[],"['Richard Sances', 'Olivera Kotevska', 'Paul Laiu']","['Virginia Polytechnic Institute and State University', 'Oak Ridge National Laboratory', 'Oak Ridge National Laboratory']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30509,Security,Rider Posture-Based Continuous Authentication with Few-Shot Learning for Mobility Scooters (Student Abstract),"Current practice of mobility scooter user authentication using physical keys and traditional password-based one-time security mechanisms cannot meet the needs of many mobility scooter riders, especially senior citizens having issues in recalling memory. Now seamless authentication approaches are needed to provide ongoing protection for mobility scooters against takeovers and unauthorized access. Existing continuous authentication techniques do not work well in a mobility scooter setting due to issues such as user comfort, deployment cost and enrollment time, among others. In that direction, our contributions in this research effort are two-fold: (i) we propose a novel system that incorporates advances in few-shot learning, hierarchical processing, and contextual embedding to establish continuous authentication for mobility scooter riders using only posture data. This security system, trained on data collected from real mobility scooter riders, demonstrates quick enrollment and easy deployability, while successfully serving as an unobtrusive first layer of security. (ii) we provide to the research community the largest publicly available repository of mobility scooter riders' body key-points data to enable further research in this direction.","['Applications Of AI', 'Continuous Authentication', 'Pose Estimation', 'Few Shot Learning']",[],"['Devan Shah', 'Ruoqi Huang', 'Tingting Chen', 'Murtuza Jadliwala']","['Princeton University', 'California State Polytechnic University, Pomona', 'California State Polytechnic University, Pomona', 'University of Texas, San Antonio']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30512,Fairness & Bias,Diverse Yet Biased: Towards Mitigating Biases in Generative AI (Student Abstract),"Generative Artificial Intelligence (AI) has garnered significant attention for its remarkable ability to generate text, images, and other forms of content. However, an inherent and increasingly concerning issue within generative AI systems is bias. These AI models often exhibit an Anglo-centric bias and tend to overlook the importance of diversity. This can be attributed to their training on extensive datasets sourced from the internet, which inevitably inherit the biases present in those data sources. Employing these datasets leads to AI-generated content that mirrors and perpetuates existing biases, encompassing various aspects such as gender, ethnic and cultural stereotypes. Addressing bias in generative AI is a complex challenge that necessitates substantial efforts. In order to tackle this issue, we propose a methodology for constructing moderately sized datasets with a social inclination. These datasets can be employed to rectify existing imbalances in datasets or to train models to generate socially inclusive material. Additionally, we present preliminary findings derived from training our model on these socially inclined datasets.","['Bias Mitigation', 'Generative AI', 'Computer Vision', 'Diversity And Inclusion', 'Fairness']",[],['Akshit Singh'],"['Indian Institute of Technology, Jodhpur, India']",['India']
https://ojs.aaai.org/index.php/AAAI/article/view/30513,Privacy & Data Governance,Confidence Is All You Need for MI Attacks (Student Abstract),"In this evolving era of machine learning security, membership inference attacks have emerged as a potent threat to the confidentiality of sensitive data. In this attack, adversaries aim to determine whether a particular point was used during the training of a target model. This paper proposes a new method to gauge a data point’s membership in a model’s training set. Instead of correlating loss with membership, as is traditionally done, we have leveraged the fact that training examples generally exhibit higher confidence values when classified into their actual class. During training, the model is essentially being ’fit’ to the training data and might face particular difficulties in generalization to unseen data. This asymmetry leads to the model achieving higher confidence on the training data as it exploits the specific patterns and noise present in the training data. Our proposed approach leverages the confidence values generated by the machine-learning model. These confidence values provide a probabilistic measure of the model’s certainty in its predictions and can further be used to infer the membership of a given data point. Additionally, we also introduce another variant of our method that allows us to carry out this attack without knowing the ground truth(true class) of a given data point, thus offering an edge over existing label-dependent attack methods.","['Confidence Value', 'Likelihood Ratio Attack (LiRA)', 'Membership Inference Attacks', 'Model Privacy', 'Adversarial Attacks']",[],"['Abhishek Sinha', 'Himanshi Tibrewal', 'Mansi Gupta', 'Nikhar Waghela', 'Shivank Garg']","['Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667']","['India', 'India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30517,Transparency & Explainability,Evaluation of Large Language Models on Code Obfuscation (Student Abstract),"Obfuscation intends to decrease interpretability of code and identification of code behavior. Large Language Models(LLMs) have been proposed for code synthesis and code analysis. This paper attempts to understand how well LLMs can analyse code and identify code behavior. Specifically, this paper systematically evaluates several LLMs’ capabilities to detect obfuscated code and identify behavior across a variety of obfuscation techniques with varying levels of complexity. LLMs proved to be better at detecting obfuscations that changed identifiers, even to misleading ones, compared to obfuscations involving code insertions (unused variables, as well as variables that replace constants with expressions that evaluate to those constants). Hardest to detect were obfuscations that layered multiple simple transformations. For these, only 20-40% of the LLMs’ responses were correct. Adding misleading documentation was also successful in misleading LLMs. We provide all our code to replicate results at https://github.com/SwindleA/LLMCodeObfuscation. Overall, our results suggest a gap in LLMs’ ability to understand code.","['Large Language Models', 'Applications of AI', 'GPT']",[],"['Adrian Swindle', 'Derrick McNealy', 'Giri Krishnan', 'Ramyaa Ramyaa']","['Saint Louis University', 'University of Southern Mississippi', 'University of California, San Diego', 'New Mexico Institute of Mining and Technology']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30522,Transparency & Explainability,Opening the Black Box: Unraveling the Classroom Dialogue Analysis (Student Abstract),"This paper explores proposing interpreting methods from explainable artificial intelligence to address the interpretability issues in deep learning-based models for classroom dialogue. Specifically, we developed a Bert-based model to automatically detect student talk moves within classroom dialogues, utilizing the TalkMoves dataset. Subsequently, we proposed three generic interpreting methods, namely saliency, input*gradient, and integrated gradient, to explain the predictions of classroom dialogue models by computing input relevance (i.e., contribution). The experimental results show that the three interpreting methods can effectively unravel the classroom dialogue analysis, thereby potentially fostering teachers' trust.","['Explainable Artificial Intelligence', 'Artificial Intelligence In Education', 'Classroom Dialogue Analysis', 'Talk Move']",[],['Deliang Wang'],['The University of Hong Kong'],['Hong Kong']
https://ojs.aaai.org/index.php/AAAI/article/view/30532,Fairness & Bias,Biases Mitigation and Expressiveness Preservation in Language Models: A Comprehensive Pipeline (Student Abstract),"Pre-trained language models (PLMs) have greatly transformed various downstream tasks, yet frequently display social biases from training data, raising fairness concerns. Recent efforts to debias PLMs come with limitations: they either fine-tune the entire parameters in PLMs, which is time-consuming and disregards the expressiveness of PLMs, or ignore the reintroducing biases from downstream tasks when applying debiased models to them. Hence, we propose a two-stage pipeline to mitigate biases from both internal and downstream contexts while preserving expressiveness in language models. Specifically, for the debiasing procedure, we resort to continuous prefix-tuning, not fully fine-tuning the PLM, in which we design a debiasing term for optimization and an alignment term to keep words’ relative distances and ensure the model's expressiveness. For downstream tasks, we perform causal intervention across different demographic groups for invariant predictions. Results on three GLUE tasks show our method alleviates biases from internal and downstream contexts, while keeping PLM expressiveness intact.","['Natural Language Processing', 'Social Bias', 'Bias Mitigation']",[],"['Liu Yu', 'Ludie Guo', 'Ping Kuang', 'Fan Zhou']","['University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30533,Transparency & Explainability,Leverage the Explainability of Transformer Models to Improve the DNA 5-Methylcytosine Identification (Student Abstract),"DNA methylation is an epigenetic mechanism for regulating gene expression, and it plays an important role in many biological processes. While methylation sites can be identified using laboratory techniques, much work is being done on developing computational approaches using machine learning. Here, we present a deep-learning algorithm for determining the 5-methylcytosine status of a DNA sequence. We propose an ensemble framework that treats the self-attention score as an explicit feature that is added to the encoder layer generated by fine-tuned language models. We evaluate the performance of the model under different data distribution scenarios.","['Natural Language Processing', 'Transfer Learning', 'DNA Methylation', 'Transformer-based Language Model']",[],"['Wenhuan Zeng', 'Daniel H. Huson']","['Tübingen University', 'University of Tuebingen']","['Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30537,Transparency & Explainability,Automated Natural Language Explanation of Deep Visual Neurons with Large Models (Student Abstract),"Interpreting deep neural networks through examining neurons offers distinct advantages when it comes to exploring the inner workings of Deep Neural Networks. Previous research has indicated that specific neurons within deep vision networks possess semantic meaning and play pivotal roles in model performance. Nonetheless, the current methods for generating neuron semantics heavily rely on human intervention, which hampers their scalability and applicability. To address this limitation, this paper proposes a novel post-hoc framework for generating semantic explanations of neurons with large foundation models, without requiring human intervention or prior knowledge. Experiments are conducted with both qualitative and quantitative analysis to verify the effectiveness of our proposed approach.","['Explainable AI', 'Large Language Models', 'Applications Of AI', 'Deep Learning', 'Interpretation']",[],"['Chenxu Zhao', 'Wei Qian', 'Yucheng Shi', 'Mengdi Huai', 'Ninghao Liu']","['Iowa State University', 'Iowa State University', 'University of Georgia', 'Iowa State University', 'University of Georgia']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30536,Transparency & Explainability,MRMLREC: A Two-Stage Approach for Addressing Data Sparsity in MOOC Video Recommendation (Student Abstract),"With the abundance of learning resources available on massive open online courses (MOOCs) platforms, the issue of interactive data sparsity has emerged as a significant challenge.This paper introduces MRMLREC, an efficient MOOC video recommendation which consists of two main stages: multi-relational representation and multi-level recommendation, aiming to solve the problem of data sparsity. In the multi-relational representation stage, MRMLREC adopts a tripartite approach, constructing relational graphs based on temporal sequences, courses-videos relation, and knowledge concepts-video relation. These graphs are processed by a Graph Convolution Network (GCN) and two variant Graph Attention Networks (GAT) to derive representations. A variant of the Long Short-Term Memory Network (LSTM) then integrates these multi-dimensional data to enhance the overall representation. The multi-level recommendation stage introduces three prediction tasks at varying levels—courses, knowledge concepts, and videos—to mitigate data sparsity and improve the interpretability of video recommendations. Beam search (BS) is employed to identify top-β items at each level, refining the subsequent level's search space and enhancing recommendation efficiency. Additionally, an optional layer offers both personalization and diversification modes, ensuring variety in recommended videos and maintaining learner engagement. Comprehensive experiments demonstrate the effectiveness of MRMLREC on two real-world instances from Xuetang X.","['MOOC Video Recommendation', 'Data Sparsity', 'Beam Search']",[],"['Ye Zhang', 'Yanqi Gao', 'Yupeng Zhou', 'Jianan Wang', 'Minghao Yin']","['Northeast Normal University', 'Northeast Normal University', 'Northeast Normal University', 'Northeast Normal University', 'Northeast Normal University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30540,Security,LLM-Powered Synthetic Environments for Self-Driving Scenarios,"This paper outlines a proposal exploring the potential use of Large Language Models (LLMs), particularly GPT-4, in crafting realistic synthetic environments for self-driving scenarios. The envisioned approach involves dynamic scene generation within game engines, leveraging LLMs to introduce challenging elements for autonomous vehicles. The proposed evaluation process outlines assessments such as realistic testing, safety metrics, and user interaction, aiming to set the stage for potential improvements in self-driving system performance. The paper aims to contribute to the AI field by discussing how LLMs could be utilized to create valuable testing grounds for autonomous vehicles, potentially fostering the development of more robust self-driving technology. The envisioned impact is the eventual enhancement of road safety and the possible acceleration of the adoption of autonomous vehicles, paving the way for a future with safer and more efficient transportation.","['Synthetic Environments', 'Large Language Models (LLMs)', 'Self-Driving Scenarios']",[],['Oluwanifemi Adebayo Moses Adekanye'],['Bowen University'],['Nigeria']
https://ojs.aaai.org/index.php/AAAI/article/view/30542,Security,Evaluating AI Red Teaming’s Readiness to Address Environmental Harms: A Thematic Analysis of LLM Discourse,"This research explores the discourse surrounding red teaming and aims to identify any themes in the online discussion of potential environmental harms stemming from Large Language Models (LLMs). Focusing on the AI Red Teaming event at DEFCON 31, this study employs reflexive thematic analysis on diverse social networking site sources to extract insights into public discussion of LLM red teaming and its environmental implications. The findings intend to inform future research, highlighting the need for responsible AI development that addresses environmental concerns.","['Red Teaming', 'Thematic Analysis', 'Large Language Models (LLM)', 'Responsible AI', 'Sustainability In AI']",[],['Amy Au'],['University of British Columbia'],['Canada']
https://ojs.aaai.org/index.php/AAAI/article/view/30543,Fairness & Bias,Enhancing Healthcare Predictions with Deep Learning Models,"This study leverages Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to enhance diagnostics and predictions in healthcare. By training on extensive healthcare datasets, this project aims to improve early disease detection and health risk assessments. Evaluation emphasizes accuracy, reliability, and ethical considerations, including bias mitigation. This research promises to bridge AI advancements and clinical applications, offering significant improvements in diagnostic capabilities and healthcare accessibility.","['Healthcare', 'UMBC', 'Computer Science']",[],['Adam Baji'],"['University of Maryland, Baltimore County']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30544,Security,Securing Billion Bluetooth Devices Leveraging Learning-Based Techniques,"As the most popular low-power communication protocol, cybersecurity research on Bluetooth Low Energy (BLE) has garnered significant attention. Due to BLE’s inherent security limitations and firmware vulnerabilities, spoofing attacks can easily compromise BLE devices and tamper with privacy data. In this paper, we proposed BLEGuard, a hybrid detection mechanism combined cyber-physical features with learning-based techniques. We established a physical network testbed to conduct attack simulations and capture advertising packets. Four different network features were utilized to implement detection and classification algorithms. Preliminary results have verified the feasibility of our proposed methods.","['Bluetooth Low Energy', 'Security And Privacy', 'Deep Learning', 'Reconstruction', 'Classification']",[],['Hanlin Cai'],"['National University of Ireland Maynooth, Maynooth, Co. Kildare, Ireland\nMaynooth International Engineering College, Fuzhou University, Fujian, China']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30550,Transparency & Explainability,A Novel Approach for Longitudinal Modeling of Aging Health and Predicting Mortality Rates,"Aging is a complex stochastic process that affects healthy functioning through various pathways. In contrast to the more commonly used cross-sectional methods, our research focuses on longitudinal modeling of aging, a less explored but crucial area. We have developed a Stochastic Differential Equation (SDE) model, at the forefront of aging research, designed to accurately forecast the health trajectories and survival rates of individuals. This model adeptly delineates the connections between different health indicators and provides clear, interpretable results. Our approach utilizes the SDE framework to encapsulate the inherent uncertainty in the aging process. Moreover, it incorporates a Recurrent Neural Network (RNN) to integrate past health data into future health projections. We plan to train and test our model using a comprehensive dataset tailored for aging studies. This model is not only computationally cost-effective but also highly relevant in assessing health risks in older populations, particularly for those at high risk. It can serve as an essential tool in anticipating and preparing for challenges like infectious disease outbreaks. Overall, our research aims to improve health equity and global health security significantly, offering substantial benefits to public health and deepening our understanding of the aging process.","['Aging', 'Math Modeling', 'Stochastic Differential Equation', 'Interpretability', 'Health And Mortality']",[],['Hannah Guan'],['Harvard College'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30553,Fairness & Bias,Transforming Healthcare: A Comprehensive Approach to Mitigating Bias and Fostering Empathy through AI-Driven Augmented Reality,"The integration of Artificial Intelligence (AI) into Augmented Reality (AR) for medical applications is propelled by the aim to address evident healthcare disparities. Certain communities have encountered disparities in medical diagnoses, exemplified by Black individuals exhibiting a 2.4 times higher likelihood of schizophrenia diagnosis compared to their white counterparts (Faber et al., 2023). These disparities often arise from structured interview assessments overlooking cultural nuances, resulting in increased misdiagnosis rates. This study leverages AI and AR to develop unbiased diagnostic tools and enhance empathy in healthcare professionals' training. Uniquely prioritizing the reduction of biased language and the fostering of empathy through AI-driven Natural Language Processing (NLP) and AI-driven virtual patients, the research aims to enhance diagnostic accuracy while promoting cultural sensitivity among healthcare professionals. Aligned with broader goals of achieving equitable healthcare and reducing disparities, the evaluation involves pre- and post-training assessments to measure language improvements and empathy enhancements. Successful implementation could lead to a more equitable healthcare landscape, fostering trust in AI-driven systems and ensuring fairer medical care for diverse communities.","['Artificial Intelligence To Mitigate Bias In Diagnosis', 'Bias In Self-supervised Machine Learning', 'Applications of AI', 'AI systems for digital health']",[],['Erica Okeh'],['Howard University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30560,Transparency & Explainability,SemLa: A Visual Analysis System for Fine-Grained Text Classification,"Fine-grained text classification requires models to distinguish between many fine-grained classes that are hard to tell apart. However, despite the increased risk of models relying on confounding features and predictions being especially difficult to interpret in this context, existing work on the interpretability of fine-grained text classification is severely limited. Therefore, we introduce our visual analysis system, SemLa, which incorporates novel visualization techniques that are tailored to this challenge. Our evaluation based on case studies and expert feedback shows that SemLa can be a powerful tool for identifying model weaknesses, making decisions about data annotation, and understanding the root cause of errors.","['Artificial Intelligence', 'Natural language processing and speech recognition']",[],"['Munkhtulga Battogtokh', 'Cosmin Davidescu', 'Michael Luck', 'Rita Borgo']","[""King's College London"", 'ContactEngine', ""King's College London"", ""King's College London""]","['United States', '', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30559,Transparency & Explainability,"Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models","This work undertakes studies to evaluate Interpretability Methods for Time Series Deep Learning. Sensitivity analysis assesses how input changes affect the output, constituting a key component of interpretation. Among the post-hoc interpretation methods such as back-propagation, perturbation, and approximation, my work will investigate perturbation-based sensitivity Analysis methods on modern Transformer models to benchmark their performances. Specifically, my work intends to answer three research questions: 1) Do different sensitivity analysis methods yield comparable outputs and attribute importance rankings? 2) Using the same sensitivity analysis method, do different Deep Learning models impact the output of the sensitivity analysis? 3) How well do the results from sensitivity analysis methods align with the ground truth?","['Interpretability', 'Sensitivity Analysis', 'Time-Series Deep Learning']",[],['Zhengguang Wang'],"['University of Virginia, Charlottesville, Virginia']",['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30563,Transparency & Explainability,Enhancing Machine Translation Experiences with Multilingual Knowledge Graphs,"Translating entity names, especially when a literal translation is not correct, poses a significant challenge. Although Machine Translation (MT) systems have achieved impressive results, they still struggle to translate cultural nuances and language-specific context. In this work, we show that the integration of multilingual knowledge graphs into MT systems can address this problem and bring two significant benefits: i) improving the translation of utterances that contain entities by leveraging their human-curated aliases from a multilingual knowledge graph, and, ii) increasing the interpretability of the translation process by providing the user with information from the knowledge graph.","['Artificial Intelligence', 'Natural language processing and speech recognition']",[],"['Simone Conia', 'Daniel Lee', 'Min Li', 'Umar Farooq Minhas', 'Yunyao Li']","['Sapienza University of Rome, Italy', 'University of Calgary, Canada', 'Apple', 'Apple', 'Adobe']","['Italy', 'Canada', '', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30566,Fairness & Bias,LLMGuard: Guarding against Unsafe LLM Behavior,"Although the rise of Large Language Models (LLMs) in enterprise settings brings new opportunities and capabilities, it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns. To alleviate this, we present ""LLMGuard"", a tool that monitors user interactions with an LLM application and flags content against specific behaviours or conversation topics. To do this robustly, LLMGuard employs an ensemble of detectors.",['Artificial Intelligence'],[],"['Shubh Goyal', 'Medha Hira', 'Shubham Mishra', 'Sukriti Goyal', 'Arnav Goel', 'Niharika Dadu', 'Kirushikesh DB', 'Sameep Mehta', 'Nishtha Madaan']","['IIT Jodhpur', 'IIITD', 'IIT Jodhpur', 'IIT Jodhpur', 'IIITD', 'IIT Jodhpur', 'IBM Research India', 'IBM, India Research Lab', 'IBM Research']","['India', 'India', 'India', 'India', 'India', 'India', 'India', 'India', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30579,Transparency & Explainability,"Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning","We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.","['Visual processing', 'Software and testing tools for developing AI technologies', 'Simulation environments for AI agents and multi-agent systems', 'Artificial Intelligence']",[],"['Soumyendu Sarkar', 'Ashwin Ramesh Babu', 'Sajad Mousavi', 'Vineet Gundecha', 'Avisek Naug', 'Sahand Ghorbanpour']","['Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30582,Fairness & Bias,Interactive Human-Centric Bias Mitigation,Bias mitigation algorithms differ in their definition of bias and how they go about achieving that objective. Bias mitigation algorithms impact different cohorts differently and allowing end users and data scientists to understand the impact of these differences in order to make informed choices is a relatively unexplored domain. This demonstration presents an interactive bias mitigation pipeline that allows users to understand the cohorts impacted by their algorithm choice and provide feedback in order to provide a bias mitigated pipeline that most aligns with their goals.,['Artificial Intelligence'],[],"['Inge Vejsbjerg', 'Elizabeth M. Daly', 'Rahul Nair', 'Svetoslav Nizhnichenkov']","['IBM Research', 'IBM Research', 'IBM Research', 'IBM Research']","['', '', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30586,Transparency & Explainability,Generation of Visual Representations for Multi-Modal Mathematical Knowledge,"In this paper we introduce MaRE, a tool designed to generate representations in multiple modalities for a given mathematical problem while ensuring the correctness and interpretability of the transformations between different representations. The theoretical foundation for this tool is Representational Systems Theory (RST), a mathematical framework for studying the structure and transformations of representations. In MaRE’s web front-end user interface, a set of probability equations in Bayesian Notation can be rigorously transformed into Area Diagrams, Contingency Tables, and Probability Trees with just one click, utilising a back-end engine based on RST. A table of cognitive costs, based on the cognitive Representational Interpretive Structure Theory (RIST), that a representation places on a particular profile of user is produced at the same time. MaRE is general and domain independent, applicable to other representations encoded in RST. It may enhance mathematical education and research, facilitating multi-modal knowledge representation and discovery.","['Artificial Intelligence', 'Cognitive systems', 'Educational software and hardware tools for AI', 'Intelligent tutoring systems']",[],"['Lianlong Wu', 'Seewon Choi', 'Daniel Raggi', 'Aaron Stockdill', 'Grecia Garcia Garcia', 'Fiorenzo Colarusso', 'Peter C.H. Cheng', 'Mateja Jamnik']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Sussex', 'University of Sussex', 'University of Sussex', 'University of Sussex', 'University of Cambridge']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/27796,Security,Adversarial Attacks on Federated-Learned Adaptive Bitrate Algorithms,"Learning-based adaptive bitrate (ABR) algorithms have revolutionized video streaming solutions. With the growing demand for data privacy and the rapid development of mobile devices, federated learning (FL) has emerged as a popular training method for neural ABR algorithms in both academia and industry. However, we have discovered that FL-based ABR models are vulnerable to model-poisoning attacks as local updates remain unseen during global aggregation. In response, we propose MAFL (Malicious ABR model based on Federated Learning) to prove that backdooring the learning-based ABR model via FL is practical. Instead of attacking the global policy, MAFL only targets a single ``target client''. Moreover, the unique challenges brought by deep reinforcement learning (DRL) make the attack even more challenging. To address these challenges, MAFL is designed with a two-stage attacking mechanism. Using two representative attack cases with real-world traces, we show that MAFL significantly degrades the model performance on the target client (i.e., increasing rebuffering penalty by 2x and 5x) with a minimal negative impact on benign clients.","['APP: Web', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Rui-Xiao Zhang', 'Tianchi Huang']","['The Unversity of Hong Kong', 'Sony Group Corporation']","['Hong Kong', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/27851,Privacy & Data Governance,Disguise without Disruption: Utility-Preserving Face De-identification,"With the rise of cameras and smart sensors, humanity generates an exponential amount of data. This valuable information, including underrepresented cases like AI in medical settings, can fuel new deep-learning tools. However, data scientists must prioritize ensuring privacy for individuals in these untapped datasets, especially for images or videos with faces, which are prime targets for identification methods. Proposed solutions to de-identify such images often compromise non-identifying facial attributes relevant to downstream tasks. In this paper, we introduce Disguise, a novel algorithm that seamlessly de-identifies facial images while ensuring the usability of the modified data. Unlike previous approaches, our solution is firmly grounded in the domains of differential privacy and ensemble-learning research. Our method involves extracting and substituting depicted identities with synthetic ones, generated using variational mechanisms to maximize obfuscation and non-invertibility. Additionally, we leverage supervision from a mixture-of-experts to disentangle and preserve other utility attributes. We extensively evaluate our method using multiple datasets, demonstrating a higher de-identification rate and superior consistency compared to prior approaches in various downstream tasks.","['CV: Bias', 'Fairness & Privacy', 'CV: Biometrics', 'Face', 'Gesture & Pose']",[],"['Zikui Cai', 'Zhongpai Gao', 'Benjamin Planche', 'Meng Zheng', 'Terrence Chen', 'M. Salman Asif', 'Ziyan Wu']","['University of California, Riverside, CA', 'United Imaging Intelligence, Burlington, MA', 'United Imaging Intelligence, Burlington, MA', 'Rensselaer Polytechnic Institute, Troy, NY', 'United Imaging Intelligence, Burlington, MA', 'University of California, Riverside, CA', 'United Imaging Intelligence, Burlington, MA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/27857,Security,CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection,"Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution. To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain. Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions. In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance.","['CV: 3D Computer Vision', 'CV: Vision for Robotics & Autonomous Driving', 'CV: Object Detection & Categorization', 'CV: Multi-modal Vision', 'ML: Adversarial Learning & Robustness', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning']",[],"['Gyusam Chang', 'Wonseok Roh', 'Sujin Jang', 'Dongwook Lee', 'Daehyun Ji', 'Gyeongrok Oh', 'Jinsun Park', 'Jinkyu Kim', 'Sangpil Kim']","['Department of Artificial Intelligence, Korea University', 'Department of Artificial Intelligence, Korea University', 'Samsung Advanced Institute of Technology (SAIT)', 'Samsung Advanced Institute of Technology (SAIT)', 'Samsung Advanced Institute of Technology (SAIT)', 'Department of Artificial Intelligence, Korea University', 'School of Computer Science and Engineering, Pusan National University', 'Department of Computer Science and Engineering, Korea University', 'Department of Artificial Intelligence, Korea University']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/27971,Fairness & Bias,PICNN: A Pathway towards Interpretable Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) have exhibited great performance in discriminative feature learning for complex visual tasks. Besides discrimination power, interpretability is another important yet under-explored property for CNNs. One difficulty in the CNN interpretability is that filters and image classes are entangled. In this paper, we introduce a novel pathway to alleviate the entanglement between filters and image classes. The proposed pathway groups the filters in a late conv-layer of CNN into class-specific clusters. Clusters and classes are in a one-to-one relationship. Specifically, we use the Bernoulli sampling to generate the filter-cluster assignment matrix from a learnable filter-class correspondence matrix. To enable end-to-end optimization, we develop a novel reparameterization trick for handling the non-differentiable Bernoulli sampling. We evaluate the effectiveness of our method on ten widely used network architectures (including nine CNNs and a ViT) and five benchmark datasets. Experimental results have demonstrated that our method PICNN (the combination of standard CNNs with our proposed pathway) exhibits greater interpretability than standard CNNs while achieving higher or comparable discrimination power.","['CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Wengang Guo', 'Jiayi Yang', 'Huilin Yin', 'Qijun Chen', 'Wei Ye']","['Tongji University', 'Tongji University', 'Tongji University', 'Tongji University', 'Tongji University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28047,Security,Rethinking Robustness of Model Attributions,"For machine learning models to be reliable and trustworthy, their decisions must be interpretable. As these models find increasing use in safety-critical applications, it is important that not just the model predictions but also their explanations (as feature attributions) be robust to small human-imperceptible input perturbations. Recent works have shown that many attribution methods are fragile and have proposed improvements in either these methods or the model training. We observe two main causes for fragile attributions: first, the existing metrics of robustness (e.g., top-k intersection) overpenalize even reasonable local shifts in attribution, thereby making random perturbations to appear as a strong attack, and second, the attribution can be concentrated in a small region even when there are multiple important parts in an image. To rectify this, we propose simple ways to strengthen existing metrics and attribution methods that incorporate locality of pixels in robustness metrics and diversity of pixel locations in attributions. Towards the role of model training in attributional robustness, we empirically observe that adversarially trained models have more robust attributions on smaller datasets, however, this advantage disappears in larger datasets. Code is made available at https://github.com/ksandeshk/LENS.","['CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Sandesh Kamath', 'Sankalp Mittal', 'Amit Deshpande', 'Vineeth N Balasubramanian']","['Indian Institute of Technology, Hyderabad', 'Indian Institute of Technology, Hyderabad', 'Microsoft Research, Bengaluru', 'Indian Institute of Technology, Hyderabad']","['India', 'India', '', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/28108,Fairness & Bias,Causal Representation Learning via Counterfactual Intervention,"Existing causal representation learning methods are based on the causal graph they build. However, due to the omission of bias within the causal graph, they essentially encourage models to learn biased causal effects in latent space. In this paper, we propose a novel causally disentangling framework that aims to learn unbiased causal effects. We first introduce inductive and dataset biases into traditional causal graph for the physical concepts of interest. Then, we eliminate the negative effects from these two biases by counterfactual intervention with reweighted loss function for learning unbiased causal effects. Finally, we employ the causal effects into the VAE to endow the latent representations with causality. In particular, we highlight that removing biases in this paper is regarded as a part of learning process for unbiased causal effects, which is crucial for causal disentanglement performance improvement. Through extensive experiments on real-world and synthetic datasets, we show that our method outperforms different baselines and obtains the state-of-the-art results for achieving causal representation learning.","['CV: Representation Learning for Vision', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],"['Xiutian Li', 'Siqi Sun', 'Rui Feng']","['School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai 200433', 'School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai 200433', 'School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai 200433\nFudan Zhangjiang Institute, Shanghai, 200120\nShanghai Collaborative Innovation Center of Intelligent Visual Computing']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28169,Security,Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise,"The open sourcing of large amounts of image data promotes the development of deep learning techniques. Along with this comes the privacy risk of these image datasets being exploited by unauthorized third parties to train deep learning models for commercial or illegal purposes. To avoid the abuse of data, a poisoning-based technique, ""unlearnable example"", has been proposed to significantly degrade the generalization performance of models by adding imperceptible noise to the data. To further enhance its robustness against adversarial training, existing works leverage iterative adversarial training on both the defensive noise and the surrogate model. However, it still remains unknown whether the robustness of unlearnable examples primarily comes from the effect of enhancement in the surrogate model or the defensive noise. Observing that simply removing the adversarial perturbation on the training process of the defensive noise can improve the performance of robust unlearnable examples, we identify that solely the surrogate model's robustness contributes to the performance. Furthermore, we found a negative correlation exists between the robustness of defensive noise and the protection performance, indicating defensive noise's instability issue. Motivated by this, to further boost the robust unlearnable example, we introduce Stable Error-Minimizing noise (SEM), which trains the defensive noise against random perturbation instead of the time-consuming adversarial perturbation to improve the stability of defensive noise. Through comprehensive experiments, we demonstrate that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet Subset regarding both effectiveness and efficiency.","['CV: Bias', 'Fairness & Privacy', 'PEAI: Privacy & Security', 'CV: Adversarial Attacks & Robustness', 'ML: Privacy']",[],"['Yixin Liu', 'Kaidi Xu', 'Xun Chen', 'Lichao Sun']","['Lehigh University', 'Drexel University', 'Samsung Research America', 'Lehigh University']","['United States', 'United States', '', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28201,Security,Uncertainty-Aware GAN for Single Image Super Resolution,"Generative adversarial network (GAN) has become a popular tool in the perceptual-oriented single image super-resolution (SISR) for its excellent capability to hallucinate details. However, the performance of most GAN-based SISR methods is impeded due to the limited discriminative ability of their discriminators. In specific, these discriminators only focus on the global image reconstruction quality and ignore the more fine-grained reconstruction quality for constraining the generator, as they predict the overall realness of an image instead of the pixel-level realness. Here, we first introduce the uncertainty into the GAN and propose an Uncertainty-aware GAN (UGAN) to regularize SISR solutions, where the challenging pixels with large reconstruction uncertainty and importance (e.g., texture and edge) are prioritized for optimization. The uncertainty-aware adversarial training strategy enables the discriminator to capture the pixel-level SR uncertainty, which constrains the generator to focus on image areas with high reconstruction difficulty, meanwhile, it improves the interpretability of the SR. To balance weights of multiple training losses, we introduce an uncertainty-aware loss weighting strategy to adaptively learn the optimal loss weights. Extensive experiments demonstrate the effectiveness of our approach in extracting the SR uncertainty and the superiority of the UGAN over the state-of-the-arts in terms of the reconstruction accuracy and perceptual quality.","['CV: Low Level & Physics-based Vision', 'CV: Adversarial Attacks & Robustness', 'CV: Applications', 'CV: Interpretability', 'Explainability', 'and Transparency']",[],['Chenxi Ma'],['Fudan University'],['China']
https://ojs.aaai.org/index.php/AAAI/article/view/28199,Privacy & Data Governance,FedST: Federated Style Transfer Learning for Non-IID Image Segmentation,"Federated learning collaboratively trains machine learning models among different clients while keeping data privacy and has become the mainstream for breaking data silos. However, the non-independently and identically distribution (i.e., Non-IID) characteristic of different image domains among different clients reduces the benefits of federated learning and has become a bottleneck problem restricting the accuracy and generalization of federated models. In this work, we propose a novel federated image segmentation method based on style transfer, FedST, by using a denoising diffusion probabilistic model to achieve feature disentanglement and image synthesis of cross-domain image data between multiple clients. Thus it can share style features among clients while protecting structure features of image data, which effectively alleviates the influence of the Non-IID phenomenon. Experiments prove that our method achieves superior segmentation performance compared to state-of-art methods among four different Non-IID datasets in objective and subjective assessment. The code is available at https://github.com/YoferChen/FedST.","['CV: Bias', 'Fairness & Privacy', 'CV: Segmentation']",[],"['Boyuan Ma', 'Xiang Yin', 'Jing Tan', 'Yongfeng Chen', 'Haiyou Huang', 'Hao Wang', 'Weihua Xue', 'Xiaojuan Ban']","['University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'University of Science and Technology Beijing', 'Liaoning Technical University', 'University of Science and Technology Beijing']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28217,Security,Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated Outlier Class Learning,"Existing out-of-distribution (OOD) methods have shown great success on balanced datasets but become ineffective in long-tailed recognition (LTR) scenarios where 1) OOD samples are often wrongly classified into head classes and/or 2) tail-class samples are treated as OOD samples. To address these issues, current studies fit a prior distribution of auxiliary/pseudo OOD data to the long-tailed in-distribution (ID) data. However, it is difficult to obtain such an accurate prior distribution given the unknowingness of real OOD samples and heavy class imbalance in LTR. A straightforward solution to avoid the requirement of this prior is to learn an outlier class to encapsulate the OOD samples. The main challenge is then to tackle the aforementioned confusion between OOD samples and head/tail-class samples when learning the outlier class. To this end, we introduce a novel calibrated outlier class learning (COCL) approach, in which 1) a debiased large margin learning method is introduced in the outlier class learning to distinguish OOD samples from both head and tail classes in the representation space and 2) an outlier-class-aware logit calibration method is defined to enhance the long-tailed classification confidence. Extensive empirical results on three popular benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that COCL substantially outperforms existing state-of-the-art OOD detection methods in LTR while being able to improve the classification accuracy on ID data.  Code is available at https://github.com/mala-lab/COCL.","['CV: Object Detection & Categorization', 'CV: Adversarial Attacks & Robustness', 'CV: Applications']",[],"['Wenjun Miao', 'Guansong Pang', 'Xiao Bai', 'Tianqi Li', 'Jin Zheng']","['School of Computer Science and Engineering, Beihang University', 'School of Computing and Information Systems, Singapore Management University', 'School of Computer Science and Engineering, Beihang University\nState Key Laboratory of Software Development Environment, Jiangxi Research Institute, Beihang University', 'School of Computer Science and Engineering, Beihang University', 'School of Computer Science and Engineering, Beihang University\nState Key Laboratory of Virtual Reality Technology and Systems, Beihang University']","['', 'Singapore', '', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/28228,Security,Adversarial Attacks on the Interpretation of Neuron Activation Maximization,"Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet.","['CV: Interpretability', 'Explainability', 'and Transparency', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Geraldin Nanfack', 'Alexander Fulleringer', 'Jonathan Marty', 'Michael Eickenberg', 'Eugene Belilovsky']","['Concordia University\nMila – Quebec AI Institute', 'Concordia University\nMila – Quebec AI Institute', 'Princeton University', 'Flatiron Institute', 'Concordia University\nMila – Quebec AI Institute']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28235,Fairness & Bias,NeSyFOLD: A Framework for Interpretable Image Classification,"Deep learning models such as CNNs have surpassed human performance in computer vision tasks such as image classi- fication. However, despite their sophistication, these models lack interpretability which can lead to biased outcomes re- flecting existing prejudices in the data. We aim to make pre- dictions made by a CNN interpretable. Hence, we present a novel framework called NeSyFOLD to create a neurosym- bolic (NeSy) model for image classification tasks. The model is a CNN with all layers following the last convolutional layer replaced by a stratified answer set program (ASP) derived from the last layer kernels. The answer set program can be viewed as a rule-set, wherein the truth value of each pred- icate depends on the activation of the corresponding kernel in the CNN. The rule-set serves as a global explanation for the model and is interpretable. We also use our NeSyFOLD framework with a CNN that is trained using a sparse kernel learning technique called Elite BackProp (EBP). This leads to a significant reduction in rule-set size without compromising accuracy or fidelity thus improving scalability of the NeSy model and interpretability of its rule-set. Evaluation is done on datasets with varied complexity and sizes. We also pro- pose a novel algorithm for labelling the predicates in the rule- set with meaningful semantic concept(s) learnt by the CNN. We evaluate the performance of our “semantic labelling algo- rithm” to quantify the efficacy of the semantic labelling for both the NeSy model and the NeSy-EBP model.","['CV: Interpretability', 'Explainability', 'and Transparency', 'KRR: Logic Programming', 'ML: Neuro-Symbolic Learning', 'ML: Transparent', 'Interpretable', 'Explainable ML']",[],"['Parth Padalkar', 'Huaduo Wang', 'Gopal Gupta']","['The University of Texas at Dallas', 'The University of Texas at Dallas', 'The University of Texas at Dallas']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28407,Fairness & Bias,Factorized Diffusion Autoencoder for Unsupervised Disentangled Representation Learning,"Unsupervised disentangled representation learning aims to recover semantically meaningful factors from real-world data without supervision, which is significant for model generalization and interpretability. Current methods mainly rely on assumptions of independence or informativeness of factors, regardless of interpretability. Intuitively, visually interpretable concepts better align with human-defined factors. However, exploiting visual interpretability as inductive bias is still under-explored. Inspired by the observation that most explanatory image factors can be represented by ``content + mask'', we propose a content-mask factorization network (CMFNet) to decompose an image into different groups of content codes and masks, which are further combined as content masks to represent different visual concepts. To ensure informativeness of the representations, the CMFNet is jointly learned with a generator conditioned on the content masks for reconstructing the input image. The conditional generator employs a diffusion model to leverage its robust distribution modeling capability. Our model is called the Factorized Diffusion Autoencoder (FDAE). To enhance disentanglement of visual concepts, we propose a content decorrelation loss and a mask entropy loss to decorrelate content masks in latent space and spatial space, respectively. Experiments on Shapes3d, MPI3D and Cars3d show that our method achieves advanced performance and can generate visually interpretable concept-specific masks. Source code and supplementary materials are available at https://github.com/wuancong/FDAE.","['CV: Representation Learning for Vision', 'ML: Deep Generative Models & Autoencoders', 'ML: Representation Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Ancong Wu', 'Wei-Shi Zheng']","['Sun Yat-sen University, China', 'Sun Yat-sen University, China\nKey Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\nGuangdong Key Laboratory of Information Security Technology, China']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28426,Security,Segment beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation,"Augmented Reality (AR) devices, emerging as prominent mobile interaction platforms, face challenges in user safety, particularly concerning oncoming vehicles. While some solutions leverage onboard camera arrays, these cameras often have limited field-of-view (FoV) with front or downward perspectives. Addressing this, we propose a new out-of-view semantic segmentation task and Segment Beyond View (SBV), a novel audio-visual semantic segmentation method. SBV supplements the visual modality, which miss the information beyond FoV, with the auditory information using a teacher-student distillation model (Omni2Ego). The model consists of a vision teacher utilising panoramic information, an auditory teacher with 8-channel audio, and an audio-visual student that takes views with limited FoV and binaural audio as input and produce semantic segmentation for objects outside FoV. SBV outperforms existing models in comparative evaluations and shows a consistent performance across varying FoV ranges and in monaural audio settings.","['CV: Multi-modal Vision', 'CV: Segmentation', 'HAI: Human-Computer Interaction']",[],"['Renjie Wu', 'Hu Wang', 'Feras Dayoub', 'Hsiang-Ting Chen']","['The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide', 'The University of Adelaide']","['Australia', 'Australia', 'Australia', 'Australia']"
https://ojs.aaai.org/index.php/AAAI/article/view/28444,Security,Learning by Erasing: Conditional Entropy Based Transferable Out-of-Distribution Detection,"Detecting OOD inputs is crucial to deploy machine learning models to the real world safely. However, existing OOD detection methods require an in-distribution (ID) dataset to retrain the models. In this paper, we propose a Deep Generative Models (DGMs) based transferable OOD detection that does not require retraining on the new ID dataset. We first establish and substantiate two hypotheses on DGMs: DGMs exhibit a predisposition towards acquiring low-level features, in preference to semantic information; the lower bound of DGM's log-likelihoods is tied to the conditional entropy between the model input and target output. Drawing on the aforementioned hypotheses, we present an innovative image-erasing strategy, which is designed to create distinct conditional entropy distributions for each individual ID dataset. By training a DGM on a complex dataset with the proposed image-erasing strategy, the DGM could capture the discrepancy of conditional entropy distribution for varying ID datasets, without re-training. We validate the proposed method on the five datasets and show that, without retraining, our method achieves comparable performance to the state-of-the-art group-based OOD detection methods. The project codes will be open-sourced on our project website.","['CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Adversarial Attacks & Robustness', 'CV: Low Level & Physics-based Vision', 'ML: Representation Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'ML: Unsupervised & Self-Supervised Learning']",[],"['Meng Xing', 'Zhiyong Feng', 'Yong Su', 'Changjae Oh']","['Tianjin University; Queen Mary University of London', 'Tianjin University', 'Tianjin Normal University', 'Queen Mary University of London']","['United Kingdom', 'China', 'China', 'United Kingdom']"
https://ojs.aaai.org/index.php/AAAI/article/view/28460,Privacy & Data Governance,Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks,"Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only. However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL.  Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs. Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency. To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL. Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee. And DGP can also significantly improve communication efficiency with a theoretical analysis of its convergence and generalization. Our extensive experiments show that DGP can effectively defend against the most powerful GIAs and reduce the communication cost without sacrificing the model's utility.","['CV: Bias', 'Fairness & Privacy', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Lulu Xue', 'Shengshan Hu', 'Ruizhi Zhao', 'Leo Yu Zhang', 'Shengqing Hu', 'Lichao Sun', 'Dezhong Yao']","['Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Griffith University', 'Huazhong University of Science and Technology', 'Lehigh University', 'Huazhong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28510,Security,Data-Free Hard-Label Robustness Stealing Attack,"The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack.","['CV: Bias', 'Fairness & Privacy', 'ML: Privacy']",[],"['Xiaojian Yuan', 'Kejiang Chen', 'Wen Huang', 'Jie Zhang', 'Weiming Zhang', 'Nenghai Yu']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Nanyang Technological University', 'University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28573,Security,FaceRSA: RSA-Aware Facial Identity Cryptography Framework,"With the flourishing of the Internet, sharing one's photos or automated processing of faces using computer vision technology has become an everyday occurrence. While enjoying the convenience, the concern for identity privacy is also emerging. Therefore, some efforts introduced the concept of ``password'' from traditional cryptography such as RSA into the face anonymization and deanonymization task to protect the facial identity without compromising the usability of the face image. However, these methods either suffer from the poor visual quality of the synthesis results or do not possess the full cryptographic properties, resulting in compromised security. In this paper, we present the first facial identity cryptography framework with full properties analogous to RSA. Our framework leverages the powerful generative capabilities of StyleGAN to achieve megapixel-level facial identity anonymization and deanonymization. Thanks to the great semantic decoupling of StyleGAN's latent space, the identity encryption and decryption process are performed in latent space by a well-designed password mapper in the manner of editing latent code. Meanwhile, the password-related information is imperceptibly hidden in the edited latent code owing to the redundant nature of the latent space. To make our cryptographic framework possesses all the properties analogous to RSA, we propose three types of loss functions: single anonymization loss, sequential anonymization loss, and associated anonymization loss. Extensive experiments and ablation analyses demonstrate the superiority of our method in terms of the quality of synthesis results, identity-irrelevant attributes preservation, deanonymization accuracy, and completeness of properties analogous to RSA.","['CV: Applications', 'CV: Bias', 'Fairness & Privacy', 'CV: Biometrics', 'Face', 'Gesture & Pose']",[],"['Zhongyi Zhang', 'Tianyi Wei', 'Wenbo Zhou', 'Hanqing Zhao', 'Weiming Zhang', 'Nenghai Yu']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28767,Privacy & Data Governance,Poincaré Differential Privacy for Hierarchy-Aware Graph Embedding,"Hierarchy is an important and commonly observed topological property in real-world graphs that indicate the relationships between supervisors and subordinates or the organizational behavior of human groups. As hierarchy is introduced as a new inductive bias into the Graph Neural Networks (GNNs) in various tasks, it implies latent topological relations for attackers to improve their inference attack performance, leading to serious privacy leakage issues. In addition, existing privacy-preserving frameworks suffer from reduced protection ability in hierarchical propagation due to the deficiency of adaptive upper-bound estimation of the hierarchical perturbation boundary. It is of great urgency to effectively leverage the hierarchical property of data while satisfying privacy guarantees. To solve the problem, we propose the Poincar\'e Differential Privacy framework, named PoinDP, to protect the hierarchy-aware graph embedding based on hyperbolic geometry. Specifically, PoinDP first learns the hierarchy weights for each entity based on the Poincar\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware Sensitivity is designed to measure the sensitivity of the hierarchical structure and adaptively allocate the privacy protection strength. Besides, Hyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian mechanism in Euclidean space to hyperbolic space to realize random perturbations that satisfy differential privacy under the hyperbolic space metric. Extensive experiment results on five real-world datasets demonstrate the proposed PoinDP’s advantages of effective privacy protection while maintaining good performance on the node classification task.","['DMKM: Graph Mining', 'Social Network Analysis & Community', 'ML: Privacy']",[],"['Yuecen Wei', 'Haonan Yuan', 'Xingcheng Fu', 'Qingyun Sun', 'Hao Peng', 'Xianxian Li', 'Chunming Hu']","['Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China\nSchool of Software, Beihang University, Beijing, China\nKey Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China', 'Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China', 'Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, China', 'Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, China\nSchool of Software, Beihang University, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28782,Security,RRL: Recommendation Reverse Learning,"As societies become increasingly aware of data privacy, regulations require that private information about users must be removed from both database and ML models, which is more colloquially called `the right to be forgotten`. Such privacy problems of recommendation systems, which hold large amounts of private data, are drawing increasing attention. Recent research suggests dividing the preference data into multiple shards and training submodels with these shards and forgetting users' personal preference data by retraining the submodels of marked shards. Despite the computational efficiency development compared with retraining from scratch, the overall recommendation performance deteriorates after dividing the shards because the collaborative information contained in the training data is broken. In this paper, we aim to propose a forgetting framework for recommendation models that neither separate the training data nor jeopardizes the recommendation performance, named Recommendation Reverse Learning (RRL). Given the trained recommendation model and marked preference data, we devise Reverse BPR Objective (RBPR Objective) to fine-tune the recommendation model to force it to forget the marked data. Nevertheless, as the recommendation model encode the complex collaborative information among users, we propose to utilize Fisher Information Matrix (FIM) to estimate the influence of reverse learning on other users' collaborative information and guide the updates of representations. We conduct experiments on two representative recommendation models and three public benchmark datasets to verify the efficiency of RRL. To verify the forgetting completeness, we use RRL to make the recommendation model poisoned by shilling attacks forget malicious users.","['DMKM: Conversational Systems for Recommendation & Retrieval', 'PEAI: Privacy & Security']",[],"['Xiaoyu You', 'Jianwei Xu', 'Mi Zhang', 'Zechen Gao', 'Min Yang']","['Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28891,Security,Beyond Mimicking Under-Represented Emotions: Deep Data Augmentation with Emotional Subspace Constraints for EEG-Based Emotion Recognition,"In recent years, using Electroencephalography (EEG) to recognize emotions has garnered considerable attention. Despite advancements, limited EEG data restricts its potential. Thus, Generative Adversarial Networks (GANs) are proposed to mimic the observed distributions and generate EEG data. However, for imbalanced datasets, GANs struggle to produce reliable augmentations for under-represented minority emotions by merely mimicking them. Thus, we introduce Emotional Subspace Constrained Generative Adversarial Networks (ESC-GAN) as an alternative to existing frameworks. We first propose the EEG editing paradigm, editing reference EEG signals from well-represented to under-represented emotional subspaces. Then, we introduce diversity-aware and boundary-aware losses to constrain the augmented subspace. Here, the diversity-aware loss encourages a diverse emotional subspace by enlarging the sample difference, while boundary-aware loss constrains the augmented subspace near the decision boundary where recognition models can be vulnerable. Experiments show ESC-GAN boosts emotion recognition performance on benchmark datasets, DEAP, AMIGOS, and SEED, while protecting against potential adversarial attacks. Finally, the proposed method opens new avenues for editing EEG signals under emotional subspace constraints, facilitating unbiased and secure EEG data augmentation.","['HAI: Brain-Sensing and Analysis', 'CMS: Affective Computing', 'ML: Deep Generative Models & Autoencoders']",[],"['Zhi Zhang', 'Shenghua Zhong', 'Yan Liu']","['Shenzhen University\nHong Kong Polytechnic University', 'Shenzhen University', 'The Hong Kong Polytechnic University']","['Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/28922,Security,Linear-Time Verification of Data-Aware Processes Modulo Theories via Covers and Automata,"The need to model and analyse dynamic systems operating over complex data is ubiquitous in AI and neighboring areas, in particular business process management. Analysing such data-aware systems is a notoriously difficult problem, as they are intrinsically infinite-state. Existing approaches work for specific datatypes, and/or limit themselves to the verification of safety properties. In this paper, we lift both such limitations, studying for the first time linear-time verification for so-called data-aware processes modulo theories (DMTs), from the foundational and practical point of view. The DMT model is very general, as it supports processes operating over variables that can store arbitrary types of data, ranging over infinite domains and equipped with domain-specific predicates. Specifically, we provide four contributions. First, we devise a semi-decision procedure for linear-time verification of DMTs, which works for a very large class of datatypes obeying to mild model-theoretic assumptions. The procedure relies on a unique combination of automata-theoretic and cover computation techniques to respectively deal with linear-time properties and datatypes. Second, we identify an abstract, semantic property that guarantees the existence of a faithful finite-state abstraction of the original system, and show that our method becomes a decision procedure in this case. Third, we identify concrete, checkable classes of systems that satisfy this property, generalising several results in the literature. Finally, we present an implementation and an experimental evaluation over a benchmark of real-world data-aware business processes.","['KRR: Action', 'Change', 'and Causality', 'KRR: Automated Reasoning and Theorem Proving', 'KRR: Geometric', 'Spatial', 'and Temporal Reasoning']",[],"['Alessandro Gianola', 'Marco Montali', 'Sarah Winkler']","['INESC-ID/Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal', 'Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy', 'Faculty of Engineering, Free University of Bozen-Bolzano, Bolzano, Italy']","['Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AAAI/article/view/28950,Privacy & Data Governance,No Prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation,"Ensuring fairness in Recommendation Systems (RSs) across demographic groups is critical due to the increased integration of RSs in applications such as personalized healthcare, finance, and e-commerce. Graph-based RSs play a crucial role in capturing intricate higher-order interactions among entities. However, integrating these graph models into the Federated Learning (FL) paradigm with fairness constraints poses formidable challenges as this requires access to the entire interaction graph and sensitive user information (such as gender, age, etc.) at the central server. This paper addresses the pervasive issue of inherent bias within RSs for different demographic groups without compromising the privacy of sensitive user attributes in FL environment with the graph-based model. To address the group bias, we propose F2PGNN (Fair Federated Personalized Graph Neural Network), a novel framework that leverages the power of Personalized Graph Neural Network (GNN) coupled with fairness considerations. Additionally, we use differential privacy techniques to fortify privacy protection. Experimental evaluation on three publicly available datasets showcases the efficacy of F2PGNN in mitigating group unfairness by 47% ∼ 99% compared to the state-of-the-art while preserving privacy and maintaining the utility. The results validate the significance of our framework in achieving equitable and personalized recommendations using GNN within the FL landscape. Source code is at: https://github.com/nimeshagrawal/F2PGNN-AAAI24","['ML: Distributed Machine Learning & Federated Learning', 'ML: Graph-based Machine Learning', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Privacy', 'DMKM: Recommender Systems']",[],"['Nimesh Agrawal', 'Anuj Kumar Sirohi', 'Sandeep Kumar', 'Jayadeva']","['Department of Electrical Engineering, Indian Institute of Technology, Delhi, India', 'Yardi School of Artificial Intelligence, Indian Institute of Technology, Delhi, India', 'Department of Electrical Engineering, Indian Institute of Technology, Delhi, India\nYardi School of Artificial Intelligence, Indian Institute of Technology, Delhi, India', 'Department of Electrical Engineering, Indian Institute of Technology, Delhi, India\nYardi School of Artificial Intelligence, Indian Institute of Technology, Delhi, India']","['India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/28965,Security,Task-Agnostic Privacy-Preserving Representation Learning for Federated Learning against Attribute Inference Attacks,"Federated learning (FL)  has been widely studied recently due to its property to collaboratively train data from different devices without sharing the raw  data. Nevertheless, recent studies show that an adversary can still be possible to infer private information about devices' data, e.g., sensitive attributes such as income, race, and sexual orientation. To mitigate the attribute inference attacks, various existing privacy-preserving FL methods can be adopted/adapted. However, all these existing methods have key limitations: they need to know the FL task in advance, or have intolerable computational overheads or utility losses, or do not have provable privacy guarantees.   We address these issues and design a task-agnostic privacy-preserving presentation learning method for FL (TAPPFL) against attribute inference attacks. TAPPFL is formulated via information theory. Specifically,  TAPPFL has two mutual information goals, where one goal learns task-agnostic data representations that contain the least information about the private attribute in each device's data, and the other goal ensures the learnt data representations include as much information as possible about the device data to maintain FL utility. We also derive privacy guarantees of TAPPFL against worst-case attribute inference attacks, as well as the inherent tradeoff between utility preservation and privacy protection. Extensive results on multiple datasets and applications validate the effectiveness of TAPPFL to protect data privacy, maintain the FL utility, and be efficient as well.  Experimental results also show that TAPPFL outperforms the existing defenses.","['ML: Adversarial Learning & Robustness', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Caridad Arroyo Arevalo', 'Sayedeh Leila Noorbakhsh', 'Yun Dong', 'Yuan Hong', 'Binghui Wang']","['Illinois Institute or Technology', 'Illinois institute of technology', 'Benedictine University', 'University of Connecticut', 'Illinois Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/28990,Security,Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples,"Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted adversarial examples, which are generated through either well-conceived L_p-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer where to attack. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate Counterfactual ADversarial Examples to answer how to attack. The empirical results demonstrate CADE's effectiveness, as evidenced by its competitive performance across diverse attack scenarios, including white-box, transfer-based, and random intervention attacks.","['ML: Adversarial Learning & Robustness', 'ML: Causal Learning']",[],"['Ruichu Cai', 'Yuxuan Zhu', 'Jie Qiao', 'Zefeng Liang', 'Furui Liu', 'Zhifeng Hao']","['School of Computer Science, Guangdong University of Technology, Guangzhou, China\nPeng Cheng Laboratory, Shenzhen, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'School of Computer Science, Guangdong University of Technology, Guangzhou, China', 'Zhejiang Lab, Hangzhou, China', 'College of Science, Shantou University, Shantou, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/28996,Privacy & Data Governance,Learning to Unlearn: Instance-Wise Unlearning for Pre-trained Classifiers,"Since the recent advent of regulations for data protection (e.g., the General Data Protection Regulation), there has been increasing demand in deleting information learned from sensitive data in pre-trained models without retraining from scratch. The inherent vulnerability of neural networks towards adversarial attacks and unfairness also calls for a robust method to remove or correct information in an instance-wise fashion, while retaining the predictive performance across remaining data. To this end, we consider instance-wise unlearning, of which the goal is to delete information on a set of instances from a pre-trained model, by either misclassifying each instance away from its original prediction or relabeling the instance to a different label. We also propose two methods that reduce forgetting on the remaining data: 1) utilizing adversarial examples to overcome forgetting at the representation-level and 2) leveraging weight importance metrics to pinpoint network parameters guilty of propagating unwanted information. Both methods only require the pre-trained model and data instances to forget, allowing painless application to real-life settings where the entire training set is unavailable. Through extensive experimentation on various image classification benchmarks, we show that our approach effectively preserves knowledge of remaining data while unlearning given instances in both single-task and continual unlearning scenarios.","['ML: Classification and Regression', 'CV: Adversarial Attacks & Robustness', 'CV: Applications', 'CV: Learning & Optimization for CV', 'CV: Low Level & Physics-based Vision', 'CV: Other Foundations of Computer Vision', 'CV: Representation Learning for Vision', 'ML: Adversarial Learning & Robustness', 'ML: Other Foundations of Machine Learning', 'ML: Privacy']",[],"['Sungmin Cha', 'Sungjun Cho', 'Dasol Hwang', 'Honglak Lee', 'Taesup Moon', 'Moontae Lee']","['New York University', 'LG AI Research', 'LG AI Research', 'LG AI Research', 'ASRI / INMC / Seoul National University', 'LG AI Research\nUniversity of Illinois at Chicago']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29047,Fairness & Bias,Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction,"Since artificial intelligence has seen tremendous recent successes in many areas, it has sparked great interest in its potential for trustworthy and interpretable risk prediction. However, most models lack causal reasoning and struggle with class imbalance, leading to poor precision and recall. To address this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to transform original feature values into causal feature attributions for the specific risk prediction task. The causal feature attribution helps describe how much contribution the value of this feature can make to the risk prediction result. After the causal feature distillation, a deep neural network is applied to produce trustworthy prediction results with causal interpretability and high precision/recall. We evaluate the performance of our TDCFD method on several synthetic and real datasets, and the results demonstrate its superiority over the state-of-the-art methods regarding precision, recall, interpretability, and causality.","['ML: Classification and Regression', 'APP: Other Applications', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Zhixuan Chu', 'Mengxuan Hu', 'Qing Cui', 'Longfei Li', 'Sheng Li']","['Ant Group', 'University of Virginia', 'Ant Group', 'Ant Group', 'University of Virginia']","['China', 'United States', 'China', 'China', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29048,Security,Resource Efficient Deep Learning Hardware Watermarks with Signature Alignment,"Deep learning intellectual properties (IPs) are high-value assets that are frequently susceptible to theft. This vulnerability has led to significant interest in defending the field's intellectual properties from theft. Recently, watermarking techniques have been extended to protect deep learning hardware from privacy. These technique embed modifications that change the hardware's behavior when activated. In this work, we propose the first method for embedding watermarks in deep learning hardware that incorporates the owner's key samples into the embedding methodology. This improves our watermarks' reliability and efficiency in identifying the hardware over those generated using randomly selected key samples. Our experimental results demonstrate that by considering the target key samples when generating the hardware modifications, we can significantly increase the embedding success rate while targeting fewer functional blocks, decreasing the required hardware overhead needed to defend it.","['ML: Adversarial Learning & Robustness', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Joseph Clements', 'Yingjie Lao']","['Clemson University, Clemson, South Carolina, 29634\nApplied Research Associates, Albuquerque, New Mexico, 87110', 'Clemson University, Clemson, South Carolina, 29634\nTufts University, Medford, Massachusetts, 02155']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29059,Fairness & Bias,Self-Interpretable Graph Learning with Sufficient and Necessary Explanations,"Self-interpretable graph learning methods provide insights to unveil the black-box nature of GNNs by providing predictions with built-in explanations. However, current works suffer from performance degradation compared to GNNs trained without built-in explanations. We argue the main reason is that they fail to generate explanations satisfying both sufficiency and necessity, and the biased explanations further hurt GNNs' performance. In this work, we propose a novel framework for generating SUfficient aNd NecessarY explanations (SUNNY-GNN for short) that benefit GNNs' predictions. The key idea is to conduct augmentations by structurally perturbing given explanations and employ a contrastive loss to guide the learning of explanations toward sufficiency and necessity directions. SUNNY-GNN introduces two coefficients to generate hard and reliable contrastive samples. We further extend SUNNY-GNN to heterogeneous graphs. Empirical results on various GNNs and real-world graphs show that SUNNY-GNN yields accurate predictions and faithful explanations, outperforming the state-of-the-art methods by improving 3.5% prediction accuracy and 13.1% explainability fidelity on average. Our code and data are available at https://github.com/SJTU-Quant/SUNNY-GNN.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Graph-based Machine Learning']",[],"['Jiale Deng', 'Yanyan Shen']","['Department of Computer Science and Engineering Shanghai Jiao Tong University', 'Department of Computer Science and Engineering Shanghai Jiao Tong University']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29070,Fairness & Bias,Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces,"As responsible AI gains importance in machine learning algorithms, properties like fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models (SCMs) in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use SCMs and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation (CAP) and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Adversarial Learning & Robustness']",[],"['Ahmad-Reza Ehyaei', 'Kiarash Mohammadi', 'Amir-Hossein Karimi', 'Samira Samadi', 'Golnoosh Farnadi']","['Max Planck Institute for Intelligent Systems', 'Université de Montréal, Montréal, Canada\nMila - Québec AI Institute, Montréal, Canada', 'Max Planck Institute for Intelligent Systems Germany', 'Max Planck Institute for Intelligent Systems', 'Université de Montréal, Montréal, Canada\nMila - Québec AI Institute, Montréal, Canada\nMcGill University, Montréal, Canada']","['Belgium', 'Canada', 'Germany', 'Belgium', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/29094,Security,REGLO: Provable Neural Network Repair for Global Robustness Properties,"We present REGLO, a novel methodology for repairing pretrained neural networks to satisfy global robustness and individual fairness properties. A neural network is said to be globally robust with respect to a given input region if and only if all the input points in the region are locally robust. This notion of global robustness also captures the notion of individual fairness as a special case. We prove that any counterexample to a global robustness property must exhibit a corresponding large gradient. For ReLU networks, this result allows us to efficiently identify the linear regions that violate a given global robustness property. By formulating and solving a suitable robust convex optimization problem, REGLO then computes a minimal weight change that will provably repair these violating linear regions.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Adversarial Learning & Robustness', 'ML: Privacy']",[],"['Feisi Fu', 'Zhilu Wang', 'Weichao Zhou', 'Yixuan Wang', 'Jiameng Fan', 'Chao Huang', 'Qi Zhu', 'Xin Chen', 'Wenchao Li']","['Boston University', 'Northwestern University', 'Boston University', 'Northwestern University', 'Boston University', 'Univeristy of Liverpool', 'Northwestern University', 'University of New Mexico', 'Boston University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29109,Security,DeepSaDe: Learning Neural Networks That Guarantee Domain Constraint Satisfaction,"As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight updates based on a mix of gradient descent and CSP solving. Evaluation on various machine learning tasks demonstrates that our approach is flexible enough to enforce a wide variety of domain constraints and is able to guarantee them in neural networks.","['ML: Neuro-Symbolic Learning', 'CSO: Constraint Optimization', 'CSO: Constraint Satisfaction', 'CSO: Satisfiability', 'CSO: Satisfiability Modulo Theories', 'ML: Classification and Regression', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Multi-class/Multi-label Learning & Extreme Classification', 'ML: Optimization']",[],"['Kshitij Goyal', 'Sebastijan Dumancic', 'Hendrik Blockeel']","['KU Leuven, Belgium', 'Delft University of Technology, The Netherlands', 'KU Leuven, Belgium']","['Belgium', 'Belgium', 'Belgium']"
https://ojs.aaai.org/index.php/AAAI/article/view/29140,Security,Foreseeing Reconstruction Quality of Gradient Inversion: An Optimization Perspective,"Gradient inversion attacks can leak data privacy when clients share weight updates with the server in federated learning (FL). Existing studies mainly use L2 or cosine distance as the loss function for gradient matching in the attack. Our empirical investigation shows that the vulnerability ranking varies with the loss function used. Gradient norm, which is commonly used as a vulnerability proxy for gradient inversion attack, cannot explain this as it remains constant regardless of the loss function for gradient matching. In this paper, we propose a loss-aware vulnerability proxy (LAVP) for the first time. LAVP refers to either the maximum or minimum eigenvalue of the Hessian with respect to gradient matching loss at ground truth. This suggestion is based on our theoretical findings regarding the local optimization of the gradient inversion in proximity to the ground truth, which corresponds to the worst case attack scenario. We demonstrate the effectiveness of LAVP on various architectures and datasets, showing its consistent superiority over the gradient norm in capturing sample vulnerabilities. The performance of each proxy is measured in terms of Spearman's rank correlation with respect to several similarity scores. This work will contribute to enhancing FL security against any potential loss functions beyond L2 or cosine distance in the future.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Distributed Machine Learning & Federated Learning', 'ML: Privacy', 'PEAI: Privacy & Security']",[],"['Hyeong Gwon Hong', 'Yooshin Cho', 'Hanbyel Cho', 'Jaesung Ahn', 'Junmo Kim']","['KAIST', 'KAIST', 'KAIST', 'KAIST', 'KAIST']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/29167,Security,Fairness without Demographics through Shared Latent Space-Based Debiasing,"Ensuring fairness in machine learning (ML) is crucial, particularly in applications that impact diverse populations. The majority of existing works heavily rely on the availability of protected features like race and gender. However, practical challenges such as privacy concerns and regulatory restrictions often prohibit the use of this data, limiting the scope of traditional fairness research. To address this, we introduce a Shared Latent Space-based Debiasing (SLSD) method that transforms data from both the target domain, which lacks protected features, and a separate source domain, which contains these features, into correlated latent representations. This allows for joint training of a cross-domain protected group estimator on the representations. We then debias the downstream ML model with an adversarial learning technique that leverages the group estimator. We also present a relaxed variant of SLSD, the R-SLSD, that occasionally accesses a small subset of protected features from the target domain during its training phase. Our extensive experiments on benchmark datasets demonstrate that our methods consistently outperform existing state-of-the-art models in standard group fairness metrics.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Classification and Regression', 'ML: Semi-Supervised Learning', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'PEAI: Bias', 'Fairness & Equity']",[],"['Rashidul Islam', 'Huiyuan Chen', 'Yiwei Cai']","['Visa Research', 'Visa Research', 'Visa Research']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29208,Security,Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor Selection,"Machine learning techniques, such as deep learning and ensemble methods, are widely used in various domains due to their ability to handle complex real-world tasks. However, their black-box nature has raised multiple concerns about the fairness, trustworthiness, and transparency of computer-assisted decision-making. This has led to the emergence of local post-hoc explainability methods, which offer explanations for individual decisions made by black-box algorithms. Among these methods, Kernel SHAP is widely used due to its model-agnostic nature and its well-founded theoretical framework. Despite these strengths, Kernel SHAP suffers from high instability: different executions of the method with the same inputs can lead to significantly different explanations, which diminishes the relevance of the explanations. The contribution of this paper is two-fold. On the one hand, we show that Kernel SHAP's instability is caused by its stochastic neighbor selection procedure, which we adapt to achieve full stability without compromising explanation fidelity. On the other hand, we show that by restricting the neighbors generation to perturbations of size 1 -- which we call the coalitions of Layer 1 -- we obtain a novel feature-attribution method that is fully stable, computationally efficient, and still meaningful.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Gwladys Kelodjou', 'Laurence Rozé', 'Véronique Masson', 'Luis Galárraga', 'Romaric Gaudel', 'Maurice Tchuente', 'Alexandre Termier']","['Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, INSA Rennes, CNRS, Inria, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France', 'Sorbonne University, IRD, University of Yaoundé I, UMI 209 UMMISCO, BP 337 Yaoundé, Cameroon', 'Univ Rennes, Inria, CNRS, IRISA - UMR 6074, F35000 Rennes, France']","['France', 'France', 'France', 'France', 'France', 'France', 'France']"
https://ojs.aaai.org/index.php/AAAI/article/view/29223,Fairness & Bias,Pantypes: Diverse Representatives for Self-Explainable Models,"Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions.  Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Interpretability', 'Explainability', 'and Transparency', 'CV: Bias', 'Fairness & Privacy', 'ML: Classification and Regression', 'ML: Clustering', 'ML: Deep Learning Algorithms', 'ML: Dimensionality Reduction/Feature Selection', 'ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Accountability', 'Interpretability & Explainability']",[],"['Rune Kjærsgaard', 'Ahcène Boubekki', 'Line Clemmensen']","['Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark', 'Machine Learning and Uncertainty, Physikalisch-Technische Bundesanstalt, Germany', 'Department of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark']","['Denmark', '', 'Denmark']"
https://ojs.aaai.org/index.php/AAAI/article/view/29321,Security,Backdoor Attacks via Machine Unlearning,"As a new paradigm to erase data from a model and protect user privacy, machine unlearning has drawn significant attention. However, existing studies on machine unlearning mainly focus on its effectiveness and efficiency, neglecting the security challenges introduced by this technique. In this paper, we aim to bridge this gap and study the possibility of conducting malicious attacks leveraging machine unlearning. Specifically, we consider the backdoor attack via machine unlearning, where an attacker seeks to inject a backdoor in the unlearned model by submitting malicious unlearning requests, so that the prediction made by the unlearned model can be changed when a particular trigger presents. In our study, we propose two attack approaches. The first attack approach does not require the attacker to poison any training data of the model. The attacker can achieve the attack goal only by requesting to unlearn a small subset of his contributed training data. The second approach allows the attacker to poison a few training instances with a pre-defined trigger upfront, and then activate the attack via submitting a malicious unlearning request. Both attack approaches are proposed with the goal of maximizing the attack utility while ensuring attack stealthiness. The effectiveness of the proposed attacks is demonstrated with different machine unlearning algorithms as well as different models on different datasets.","['ML: Adversarial Learning & Robustness', 'ML: Classification and Regression', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Zihao Liu', 'Tianhao Wang', 'Mengdi Huai', 'Chenglin Miao']","['Iowa State University', 'University of Virginia', 'Iowa State University', 'Iowa State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29339,Security,PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN in Federated Learning,"Federated learning (FL) has attracted growing attention since it allows for privacy-preserving collaborative training on decentralized clients without explicitly uploading sensitive data to the central server. However, recent works have revealed that it still has the risk of exposing private data to adversaries. In this paper, we conduct reconstruction attacks and enhance inference attacks on various datasets to better understand that sharing trained classification model parameters to a central server is the main problem of privacy leakage in FL. To tackle this problem, a privacy-preserving image distribution sharing scheme with GAN (PPIDSG) is proposed, which consists of a block scrambling-based encryption algorithm, an image distribution sharing method, and local classification training. Specifically, our method can capture the distribution of a target image domain which is transformed by the block encryption algorithm, and upload generator parameters to avoid classifier sharing with negligible influence on model performance. Furthermore, we apply a feature extractor to motivate model utility and train it separately from the classifier. The extensive experimental results and security analyses demonstrate the superiority of our proposed scheme compared to other state-of-the-art defense methods. The code is available at https://github.com/ytingma/PPIDSG.","['ML: Privacy', 'ML: Distributed Machine Learning & Federated Learning']",[],"['Yuting Ma', 'Yuanzhi Yao', 'Xiaohua Xu']","['University of Science and Technology of China', 'Hefei University of Technology', 'University of Science and Technology of China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29352,Fairness & Bias,Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles,"While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'GTEP: Cooperative Game Theory', 'ML: Ensemble Methods', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Maximilian Muschalik', 'Fabian Fumagalli', 'Barbara Hammer', 'Eyke Hüllermeier']","['LMU Munich, MCML Munich, D-80539 Munich, Germany', 'Bielefeld University, CITEC, D-33619 Bielefeld, Germany', 'Bielefeld University, CITEC, D-33619 Bielefeld, Germany', 'LMU Munich, MCML Munich, D-80539 Munich, Germany']","['Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29397,Fairness & Bias,Using Stratified Sampling to Improve LIME Image Explanations,"We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator.  Experiments show the efficacy of the proposed approach.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'RU: Stochastic Optimization', 'SO: Sampling/Simulation-based Search']",[],"['Muhammad Rashid', 'Elvio G. Amparore', 'Enrico Ferrari', 'Damiano Verda']","['University of Torino, Computer Science Department, C.so Svizzera 185, 10149 Torino, Italy', 'University of Torino, Computer Science Department, C.so Svizzera 185, 10149 Torino, Italy', 'Rulex Innovation Labs, Via Felice Romani 9, 16122 Genova, Italy', 'Rulex Innovation Labs, Via Felice Romani 9, 16122 Genova, Italy']","['Japan', 'Japan', 'Colombia', 'Colombia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29403,Fairness & Bias,Limitations of Face Image Generation,"Text-to-image diffusion models have achieved widespread popularity due to their unprecedented image generation capability. In particular, their ability to synthesize and modify human faces has spurred research into using generated face images in both training data augmentation and model performance assessments. In this paper, we study the efficacy and shortcomings of generative models in the context of face generation. Utilizing a combination of qualitative and quantitative measures, including embedding-based metrics and user studies, we present a framework to audit the characteristics of generated faces conditioned on a set of social attributes. We applied our framework on faces generated through state-of-the-art text-to-image diffusion models. We identify several limitations of face image generation that include faithfulness to the text prompt, demographic disparities, and distributional shifts. Furthermore, we present an analytical model that provides insights into how training data selection contributes to the performance of generative models. Our survey data and analytics code can be found online at https://github.com/wi-pi/Limitations_of_Face_Generation","['ML: Ethics', 'Bias', 'and Fairness', 'CV: Bias', 'Fairness & Privacy', 'ML: Deep Generative Models & Autoencoders']",[],"['Harrison Rosenberg', 'Shimaa Ahmed', 'Guruprasad Ramesh', 'Kassem Fawaz', 'Ramya Korlakai Vinayak']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29402,Security,Protect Your Score: Contact-Tracing with Differential Privacy Guarantees,"The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score with epsilon=1 differential privacy, we achieve a two to ten-fold reduction in the infection rate of the virus. To the best of our knowledge, this presents the first contact tracing algorithm with differential privacy guarantees when revealing risk scores for COVID19.","['ML: Distributed Machine Learning & Federated Learning', 'APP: Security', 'MAS: Multiagent Systems under Uncertainty', 'ML: Graph-based Machine Learning']",[],"['Rob Romijnders', 'Christos Louizos', 'Yuki M. Asano', 'Max Welling']","['University of Amsterdam', 'Qualcomm AI research', 'University of Amsterdam', 'University of Amsterdam']","['Netherlands', '', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AAAI/article/view/29414,Fairness & Bias,BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence,"Measuring the coherence of text is a vital aspect of evaluating the quality of written content. Recent advancements in neural coherence modeling have demonstrated their efficacy in capturing entity coreference and discourse relations, thereby enhancing coherence evaluation. However, many existing methods heavily depend on static embeddings or focus narrowly on nearby context, constraining their capacity to measure the overarching coherence of long texts. In this paper, we posit that coherent texts inherently manifest a sequential and cohesive interplay among sentences, effectively conveying the central theme, purpose, or standpoint. To explore this abstract relationship, we introduce the ""BB Score,"" a novel reference-free metric grounded in Brownian bridge theory for assessing text coherence. Our findings showcase that when synergized with a simple additional classification component, this metric attains a performance level comparable to state-of-the-art techniques on standard artificial discrimination tasks. We also establish in downstream tasks that this metric effectively differentiates between human-written documents and text generated by large language models within specific domains. Furthermore, we illustrate the efficacy of this approach in detecting written styles attributed to various large language models, underscoring its potential for generalizability. In summary, we present a novel Brownian bridge coherence metric capable of measuring both local and global text coherence, while circumventing the need for end-to-end model training. This flexibility allows for its application in various downstream tasks.","['ML: Evaluation and Analysis', 'NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Zhecheng Sheng', 'Tianhao Zhang', 'Chen Jiang', 'Dongyeop Kang']","['University of Minnesota, Minneapolis, MN', 'University of Minnesota, Minneapolis, MN', 'University of Minnesota, Minneapolis, MN', 'University of Minnesota, Minneapolis, MN']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29446,Security,FedCompetitors: Harmonious Collaboration in Federated Learning with Competing Participants,"Federated learning (FL) provides a privacy-preserving approach for collaborative training of machine learning models. Given the potential data heterogeneity, it is crucial to select appropriate collaborators for each FL participant (FL-PT) based on data complementarity. Recent studies have addressed this challenge. Similarly, it is imperative to consider the inter-individual relationships among FL-PTs where some FL-PTs engage in competition. Although FL literature has acknowledged the significance of this scenario, practical methods for establishing FL ecosystems remain largely unexplored. In this paper, we extend a principle from the balance theory, namely “the friend of my enemy is my enemy”, to ensure the absence of conflicting interests within an FL ecosystem. The extended principle and the resulting problem are formulated via graph theory and integer linear programming. A polynomial-time algorithm is proposed to determine the collaborators of each FL-PT. The solution guarantees high scalability, allowing even competing FL-PTs to smoothly join the ecosystem without conflict of interest. The proposed framework jointly considers competition and data heterogeneity. Extensive experiments on real-world and synthetic data demonstrate its efficacy compared to five alternative approaches, and its ability to establish efficient collaboration networks among FL-PTs.","['ML: Distributed Machine Learning & Federated Learning', 'MAS: Coordination and Collaboration', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Shanli Tan', 'Hao Cheng', 'Xiaohu Wu', 'Han Yu', 'Tiantian He', 'Yew Soon Ong', 'Chongjun Wang', 'Xiaofeng Tao']","['Beijing University of Posts and Telecommunications', 'Nanjing University', 'Beijing University of Posts and Telecommunications', 'Nanyang Technological University (NTU)', 'Agency for Science, Technology and Research (A*STAR)', 'Nanyang Technological University, Singapore\nA*STAR', 'Nanjing University', 'Beijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China', '', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29451,Privacy & Data Governance,DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction),"The Adam optimizer is a popular choice in contemporary deep learning due to its strong empirical performance. However we observe that in privacy sensitive scenarios, the traditional use of Differential Privacy (DP) with the Adam optimizer leads to sub-optimal performance on several tasks. We find that this performance degradation is due to a DP bias in Adam's second moment estimator, introduced by the addition of independent noise in the gradient computation to enforce DP guarantees. This DP bias leads to a different scaling for low variance parameter updates, that is inconsistent with the behavior of non-private Adam, and Adam's sign descent interpretation. We propose the DP-AdamBC optimization algorithm, which corrects for the bias in the second moment estimation and retrieves the expected behaviour of Adam. Empirically, DP-AdamBC significantly improves the optimization performance of DP-Adam by up to 3.5% in final accuracy in image, text, and graph node classification tasks.",['ML: Privacy'],[],"['Qiaoyue Tang', 'Frederick Shpilevskiy', 'Mathias Lécuyer']","['University of British Columbia', 'University of British Columbia', 'University of British Columbia']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AAAI/article/view/29454,Privacy & Data Governance,z-SignFedAvg: A Unified Stochastic Sign-Based Compression for Federated Learning,"Federated Learning (FL) is a promising privacy-preserving distributed learning paradigm but suffers from high communi- cation cost when training large-scale machine learning models. Sign-based methods, such as SignSGD, have been proposed as a biased gradient compression technique for reducing the communication cost. However, sign-based algorithms could diverge under heterogeneous data, which thus motivated the de- velopment of advanced techniques, such as the error-feedback method and stochastic sign-based compression, to fix this issue. Nevertheless, these methods still suffer from slower convergence rates, and none of them allows multiple local SGD updates like FedAvg. In this paper, we propose a novel noisy perturbation scheme with a general symmetric noise distribution for sign-based compression, which not only al- lows one to flexibly control the bias-variance tradeoff for the compressed gradient, but also provides a unified viewpoint to existing stochastic sign-based methods. More importantly, the proposed scheme enables the development of the very first sign-based FedAvg algorithm (z-SignFedAvg) to accelerate the convergence. Theoretically, we show that z-SignFedAvg achieves a faster convergence rate than existing sign-based methods and, under the uniformly distributed noise, can enjoy the same convergence rate as its uncompressed counterpart. Extensive experiments are conducted to demonstrate that the z-SignFedAvg can achieve competitive empirical performance on real datasets and outperforms existing schemes.","['ML: Distributed Machine Learning & Federated Learning', 'SO: Non-convex Optimization']",[],"['Zhiwei Tang', 'Yanmeng Wang', 'Tsung-Hui Chang']","['School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China\nShenzhen Research Institute of Big Data, Shenzhen, China', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China\nShenzhen Research Institute of Big Data, Shenzhen, China', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China\nShenzhen Research Institute of Big Data, Shenzhen, China']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29458,Fairness & Bias,An Information-Flow Perspective on Algorithmic Fairness,"This work presents insights gained by investigating the relationship between algorithmic fairness and the concept of secure information flow. The problem of enforcing secure information flow is well-studied in the context of information security: If secret information may ""flow"" through an algorithm or program in such a way that it can influence the program’s output, then that is considered insecure information flow as attackers could potentially observe (parts of) the secret.  There is a strong correspondence between secure information flow and algorithmic fairness: if protected attributes such as race, gender, or age are treated as secret program inputs, then secure information flow means that these ""secret"" attributes cannot influence the result of a program. While most research in algorithmic fairness evaluation concentrates on studying the impact of algorithms (often treating the algorithm as a black-box), the concepts derived from information flow can be used both for the analysis of disparate treatment as well as disparate impact w.r.t. a structural causal model.  In this paper, we examine the relationship between quantitative as well as qualitative information-flow properties and fairness. Moreover, based on this duality, we derive a new quantitative notion of fairness called fairness spread, which can be easily analyzed using quantitative information flow and which strongly relates to counterfactual fairness. We demonstrate that off-the-shelf tools for information-flow properties can be used in order to formally analyze a program's algorithmic fairness properties, including the new notion of fairness spread as well as established notions such as demographic parity.","['ML: Ethics', 'Bias', 'and Fairness', 'ML: Information Theory', 'PEAI: Bias', 'Fairness & Equity', 'RU: Causality', 'RU: Graphical Models']",[],"['Samuel Teuber', 'Bernhard Beckert']","['Karlsruhe Institute of Technology', 'Karlsruhe Institute of Technology']","['Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/29476,Security,Practical Privacy-Preserving MLaaS: When Compressive Sensing Meets Generative Networks,"The Machine-Learning-as-a-Service (MLaaS) framework allows one to grab low-hanging fruit of machine learning techniques and data science, without either much expertise for this sophisticated sphere or provision of specific infrastructures. However, the requirement of revealing all training data to the service provider raises new concerns in terms of privacy leakage, storage consumption, efficiency, bandwidth, etc. In this paper, we propose a lightweight privacy-preserving MLaaS framework by combining Compressive Sensing (CS) and Generative Networks. It’s constructed on the favorable facts observed in recent works that general inference tasks could be fulfilled with generative networks and classifier trained on compressed measurements, since the generator could model the data distribution and capture discriminative information which are useful for classification. To improve the performance of the MLaaS framework, the supervised generative models of the server are trained and optimized with prior knowledge provided by the client. In order to prevent the service provider from recovering the original data as well as identifying the queried results, a noise-addition mechanism is designed and adopted into the compressed data domain. Empirical results confirmed its performance superiority in accuracy and resource consumption against the state-of-the-art privacy preserving MLaaS frameworks.","['ML: Classification and Regression', 'PEAI: Privacy & Security']",[],"['Jia Wang', 'Wuqiang Su', 'Zushu Huang', 'Jie Chen', 'Chengwen Luo', 'Jianqiang  Li']","['Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29491,Security,IGAMT: Privacy-Preserving Electronic Health Record Synthesization with Heterogeneity and Irregularity,"Integrating electronic health records (EHR) into machine learning-driven clinical research and hospital applications is important, as it harnesses extensive and high-quality patient data to enhance outcome predictions and treatment personalization. Nonetheless, due to privacy and security concerns, the secondary purpose of EHR data is consistently governed and regulated, primarily for research intentions, thereby constraining researchers' access to EHR data. Generating synthetic EHR data with deep learning methods is a viable and promising approach to mitigate privacy concerns, offering not only a supplementary resource for downstream applications but also sidestepping the confidentiality risks associated with real patient data. While prior efforts have concentrated on EHR data synthesis, significant challenges persist in the domain of generating synthetic EHR data: balancing the heterogeneity of real EHR including temporal and non-temporal features, addressing the missing values and irregular measures, and ensuring the privacy of the real data used for model training. Existing works in this domain only focused on solving one or two aforementioned challenges. In this work, we propose IGAMT, an innovative framework to generate privacy-preserved synthetic EHR data that not only maintain high quality with heterogeneous features, missing values, and irregular measures but also balances the privacy-utility trade-off. Extensive experiments prove that IGAMT significantly outperforms baseline architectures in terms of visual resemblance and comparable performance in downstream applications. Ablation case studies also prove the effectiveness of the techniques applied in IGAMT.","['ML: Deep Generative Models & Autoencoders', 'ML: Privacy', 'ML: Time-Series/Data Streams']",[],"['Wenjie Wang', 'Pengfei Tang', 'Jian Lou', 'Yuanming Shao', 'Lance Waller', 'Yi-an Ko', 'Li Xiong']","['ShanghaiTech University', 'Emory University', 'ZJU-Hangzhou Global Scientific and Technological Innovation Center', 'ShanghaiTech University', 'Emory University', 'Emory Unviversity', 'Emory University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29565,Privacy & Data Governance,Wasserstein Differential Privacy,"Differential privacy (DP) has achieved remarkable results in the field of privacy-preserving machine learning. However, existing DP frameworks do not satisfy all the conditions for becoming metrics, which prevents them from deriving better basic private properties and leads to exaggerated values on privacy budgets. We propose Wasserstein differential privacy (WDP), an alternative DP framework to measure the risk of privacy leakage, which satisfies the properties of symmetry and triangle inequality. We show and prove that WDP has 13 excellent properties, which can be theoretical supports for the better performance of WDP than other DP frameworks.  In addition, we derive a general privacy accounting method called Wasserstein accountant, which enables WDP to be applied in stochastic gradient descent (SGD) scenarios containing subsampling. Experiments on basic mechanisms, compositions and deep learning show that the privacy budgets obtained by Wasserstein accountant are relatively stable and less influenced by order. Moreover, the overestimation on privacy budgets can be effectively alleviated. The code is available at https://github.com/Hifipsysta/WDP.","['ML: Privacy', 'ML: Deep Learning Theory', 'PEAI: Privacy & Security', 'CV: Bias', 'Fairness & Privacy']",[],"['Chengyi Yang', 'Jiayin Qi', 'Aimin Zhou']","['East China Normal University', 'Guangzhou University', 'East China Normal University']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29610,Fairness & Bias,Targeted Activation Penalties Help CNNs Ignore Spurious Signals,"Neural networks (NNs) can learn to rely on spurious signals in the training data, leading to poor generalisation. Recent methods tackle this problem by training NNs with additional ground-truth annotations of such signals. These methods may, however, let spurious signals re-emerge in deep convolutional NNs (CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the same problem by penalising activations to control the re-emergence of spurious signals in deep CNNs, while also lowering training times and memory usage. In addition, ground-truth annotations can be expensive to obtain. We show that TAP still works well with annotations generated by pre-trained models as effective substitutes of ground-truth annotations. We demonstrate the power of TAP against two state-of-the-art baselines on the MNIST benchmark and on two clinical image datasets, using four different CNN architectures.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Bias', 'Fairness & Privacy', 'CV: Interpretability', 'Explainability', 'and Transparency', 'HAI: Human-in-the-loop Machine Learning', 'ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Dekai Zhang', 'Matt Williams', 'Francesca Toni']","['Department of Computing, Imperial College London', 'Department of Radiotherapy, Charing Cross Hospital\nInstitute of Global Health Innovation, Imperial College London', 'Department of Computing, Imperial College London']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29620,Privacy & Data Governance,United We Stand: Accelerating Privacy-Preserving Neural Inference by Conjunctive Optimization with Interleaved Nexus,"Privacy-preserving Machine Learning as a Service (MLaaS) enables the powerful cloud server to run its well-trained neural model upon the input from resource-limited client, with both of server's model parameters and client's input data protected. While computation efficiency is critical for the practical implementation of privacy-preserving MLaaS and it is inspiring to witness recent advances towards efficiency improvement, there still exists a significant performance gap to real-world applications. In general, state-of-the-art frameworks perform function-wise efficiency optimization based on specific cryptographic primitives. Although it is logical, such independent optimization for each function makes noticeable amount of expensive operations unremovable and misses the opportunity to further accelerate the performance by jointly considering privacy-preserving computation among adjacent functions. As such, we propose COIN: Conjunctive Optimization with Interleaved Nexus, which remodels mainstream computation for each function to conjunctive counterpart for composite function, with a series of united optimization strategies. Specifically, COIN jointly computes a pair of consecutive nonlinear-linear functions in the neural model by reconstructing the intermediates throughout the whole procedure, which not only eliminates the most expensive crypto operations without invoking extra encryption enabler, but also makes the online crypto complexity independent of filter size. Experimentally, COIN demonstrates 11.2x to 29.6x speedup over various function dimensions from modern networks, and 6.4x to 12x speedup on the total computation time when applied in networks with model input from small-scale CIFAR10 to large-scale ImageNet.","['ML: Privacy', 'CV: Bias', 'Fairness & Privacy']",[],"['Qiao Zhang', 'Tao Xiang', 'Chunsheng Xin', 'Hongyi Wu']","['Chongqing University', 'Chongqing University', 'Old Dominion University', 'The University of Arizona']","['China', 'China', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29621,Security,A Learnable Discrete-Prior Fusion Autoencoder with Contrastive Learning for Tabular Data Synthesis,"The actual collection of tabular data for sharing involves confidentiality and privacy constraints, leaving the potential risks of machine learning for interventional data analysis unsafely averted. Synthetic data has emerged recently as a privacy-protecting solution to address this challenge. However, existing approaches regard discrete and continuous modal features as separate entities, thus falling short in properly capturing their inherent correlations. In this paper, we propose a novel contrastive learning guided Gaussian Transformer autoencoder, termed GTCoder, to synthesize photo-realistic multimodal tabular data for scientific research. Our approach introduces a transformer-based fusion module that seamlessly integrates multimodal features, permitting for mining more informative latent representations. The attention within the fusion module directs the integrated output features to focus on critical components that facilitate the task of generating latent embeddings. Moreover, we formulate a contrastive learning strategy to implicitly constrain the embeddings from discrete features in the latent feature space by encouraging the similar discrete feature distributions closer while pushing the dissimilar further away, in order to better enhance the representation of the latent embedding. Experimental results indicate that GTCoder is effective to generate photo-realistic synthetic data, with interactive interpretation of latent embedding, and performs favorably against some baselines on most real-world and simulated datasets.","['ML: Deep Learning Algorithms', 'APP: Security', 'ML: Applications', 'ML: Deep Neural Architectures and Foundation Models', 'ML: Privacy']",[],"['Rongchao Zhang', 'Yiwei Lou', 'Dexuan Xu', 'Yongzhi Cao', 'Hanpin Wang', 'Yu Huang']","['Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'School of Software & Microelectronics, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China', 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, School of Computer Science, Peking University, Beijing, China\nNational Engineering Research Center for Software Engineering, Peking University, Beijing, China']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29628,Fairness & Bias,Gaussian Process Neural Additive Models,"Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) poses a major step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that utilize a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensions. It suffers no loss in performance compared with deeper NAM approaches because GPs are well-suited to learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular datasets, showing that it achieves comparable performance in both classification and regression tasks with a massive reduction in the number of parameters.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'ML: Classification and Regression', 'ML: Ethics', 'Bias', 'and Fairness']",[],"['Wei Zhang', 'Brian Barr', 'John Paisley']","['Columbia University', 'Capital One', 'Columbia University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29642,Security,Robust Visual Recognition with Class-Imbalanced Open-World Noisy Data,"Learning from open-world noisy data, where both closed-set and open-set noise co-exist in the dataset, is a realistic but underexplored setting. Only recently, several efforts have been initialized to tackle this problem. However, these works assume the classes are balanced when dealing with open-world noisy data. This assumption often violates the nature of real-world large-scale datasets, where the label distributions are generally long-tailed, i.e. class-imbalanced. In this paper, we study the problem of robust visual recognition with class-imbalanced open-world noisy data. We propose a probabilistic graphical model-based approach: iMRF to achieve label noise correction that is robust to class imbalance via an efficient iterative inference of a Markov Random Field (MRF) in each training mini-batch. Furthermore, we design an agreement-based thresholding strategy to adaptively collect clean samples from all classes that includes corrected closed-set noisy samples while rejecting open-set noisy samples. We also introduce a noise-aware balanced cross-entropy loss to explicitly eliminate the bias caused by class-imbalanced data. Extensive experiments on several benchmark datasets including synthetic and real-world noisy datasets demonstrate the superior performance robustness of our method over existing methods. Our code is available at https://github.com/Na-Z/LIOND.","['ML: Classification and Regression', 'CV: Object Detection & Categorization', 'CV: Adversarial Attacks & Robustness']",[],"['Na Zhao', 'Gim Hee Lee']","['Singapore University of Technology and Design', 'National University of Singapore']","['Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29662,Fairness & Bias,Towards the Disappearing Truth: Fine-Grained Joint Causal Influences Learning with Hidden Variable-Driven Causal Hypergraphs in Time Series,"Causal discovery under Granger causality framework has yielded widespread concerns in time series analysis task. Nevertheless, most previous methods are unaware of the underlying causality disappearing problem, that is, certain weak causalities are less focusable and may be lost during the modeling process, thus leading to biased causal conclusions. Therefore, we propose to introduce joint causal influences (i.e., causal influences from the union of multiple variables) as additional causal indication information to help identify weak causalities. Further, to break the limitation of existing methods that implicitly and coarsely model joint causal influences, we propose a novel hidden variable-driven causal hypergraph neural network to meticulously explore the locality and diversity of joint causal influences, and realize its explicit and fine-grained modeling. Specifically, we introduce hidden variables to construct a causal hypergraph for explicitly characterizing various fine-grained joint causal influences. Then, we customize a dual causal information transfer mechanism (encompassing a multi-level causal path and an information aggregation path) to realize the free diffusion and meticulous aggregation of joint causal influences and facilitate its adaptive learning. Finally, we design a multi-view collaborative optimization constraint to guarantee the characterization diversity of causal hypergraph and capture remarkable forecasting relationships (i.e., causalities). Experiments are conducted to demonstrate the superiority of the proposed model.","['ML: Causal Learning', 'ML: Time-Series/Data Streams']",[],"['Kun Zhu', 'Chunhui Zhao']","['College of Control Science and Engineering, Zhejiang University, Hangzhou, China', 'College of Control Science and Engineering, Zhejiang University, Hangzhou, China']","['China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29669,Security,MFABA: A More Faithful and Accelerated Boundary-Based Attribution Method for Deep Neural Networks,"To better understand the output of deep neural networks (DNN), attribution based methods have been an important approach for model interpretability, which assign a score for each input dimension to indicate its importance towards the model outcome. Notably, the attribution methods use the ax- ioms of sensitivity and implementation invariance to ensure the validity and reliability of attribution results. Yet, the ex- isting attribution methods present challenges for effective in- terpretation and efficient computation. In this work, we in- troduce MFABA, an attribution algorithm that adheres to ax- ioms, as a novel method for interpreting DNN. Addition- ally, we provide the theoretical proof and in-depth analy- sis for MFABA algorithm, and conduct a large scale exper- iment. The results demonstrate its superiority by achieving over 101.5142 times faster speed than the state-of-the-art at- tribution algorithms. The effectiveness of MFABA is thor- oughly evaluated through the statistical analysis in compar- ison to other methods, and the full implementation package is open-source at: https://github.com/LMBTough/MFABA.","['ML: Transparent', 'Interpretable', 'Explainable ML', 'CV: Interpretability', 'Explainability', 'and Transparency', 'ML: Adversarial Learning & Robustness']",[],"['Zhiyu Zhu', 'Huaming Chen', 'Jiayu Zhang', 'Xinyi Wang', 'Zhibo Jin', 'Minhui Xue', 'Dongxiao Zhu', 'Kim-Kwang Raymond Choo']","['The University of Sydney', 'The University of Sydney', 'SuZhouYierqi', 'University of Malaya', 'The University of Sydney', ""CSIRO's Data61"", 'Wayne State University', 'University of Texas at San Antonio']","['Australia', 'Australia', 'China', 'Malawi', 'Australia', 'India', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29717,Security,WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia,"Wikipedia can be edited by anyone and thus contains various quality sentences. Therefore, Wikipedia includes some poor-quality edits, which are often marked up by other editors. While editors' reviews enhance the credibility of Wikipedia, it is hard to check all edited text. Assisting in this process is very important, but a large and comprehensive dataset for studying it does not currently exist. Here, we propose WikiSQE, the first large-scale dataset for sentence quality estimation in Wikipedia. Each sentence is extracted from the entire revision history of English Wikipedia, and the target quality labels were carefully investigated and selected. WikiSQE has about 3.4 M sentences with 153 quality labels. In the experiment with automatic classification using competitive machine learning models, sentences that had problems with citation, syntax/semantics, or propositions were found to be more difficult to detect. In addition, by performing human annotation, we found that the model we developed performed better than the crowdsourced workers. WikiSQE is expected to be a valuable resource for other tasks in NLP.","['NLP: Sentence-level Semantics', 'Textual Inference', 'etc.', 'PEAI: Bias', 'Fairness & Equity', 'ML: Ethics', 'Bias', 'and Fairness', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Kenichiro Ando', 'Satoshi Sekine', 'Mamoru Komachi']","['RIKEN AIP', 'RIKEN AIP', 'Hitotsubashi University']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/29719,Fairness & Bias,All Should Be Equal in the Eyes of LMs: Counterfactually Aware Fair Text Generation,"Fairness in Language Models (LMs) remains a long-standing challenge, given the inherent biases in training data that can be perpetuated by models and affect the downstream tasks. Recent methods employ expensive retraining or attempt debiasing during inference by constraining model outputs to contrast from a reference set of biased templates/exemplars. Regardless, they don’t address the primary goal of fairness to maintain equitability across different demographic groups. In this work, we posit that inferencing LMs to generate unbiased output for one demographic under a context ensues from being aware of outputs for other demographics under the same context. To this end, we propose Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically compares the model’s understanding of diverse demographics to generate more equitable sentences. We conduct an extensive empirical evaluation using base LMs of varying sizes and across three diverse datasets and found that CAFIE outperforms strong baselines. CAFIE produces fairer text and strikes the best balance between fairness and language modeling capability.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'ML: Ethics', 'Bias', 'and Fairness', 'NLP: (Large) Language Models', 'NLP: Safety and Robustness']",[],"['Pragyan Banerjee', 'Abhinav Java', 'Surgan Jandial', 'Simra Shahid', 'Shaz Furniturewala', 'Balaji Krishnamurthy', 'Sumit Bhatia']","['Indian Institute of Technology Guwahati', 'MDSR Labs, Adobe', 'MDSR Labs, Adobe', 'MDSR Labs, Adobe', 'Birla Institute of Technology and Science, Pilani', 'MDSR Labs, Adobe', 'MDSR Labs, Adobe']","['Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia', 'Russia']"
https://ojs.aaai.org/index.php/AAAI/article/view/29741,Fairness & Bias,How to Protect Copyright Data in Optimization of Large Language Models?,"Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.  In this paper, we observe that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'ML: Optimization']",[],"['Timothy Chu', 'Zhao Song', 'Chiwun Yang']","['Google', 'Adobe Research', 'Sun Yat-sen University']","['United States', 'United States', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29742,Fairness & Bias,Unsupervised Layer-Wise Score Aggregation for Textual OOD Detection,"Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on anomaly scores (\textit{e.g.}, Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results can be achieved, provided that an oracle selects the best layer. We propose a data-driven, unsupervised method to leverage this observation to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a more significant number of classes (up to 150), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust and consistent results comparable to using the best layer according to an oracle while removing manual feature selection altogether.","['NLP: Safety and Robustness', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Text Classification']",[],"['Maxime Darrin', 'Guillaume Staerman', 'Eduardo Dadalto Camara Gomes', 'Jackie C. K. Cheung', 'Pablo Piantanida', 'Pierre Colombo']","['International Laboratory on Learning Systems\nMILA - Quebec AI Institute\nMcGill University\nUniversité Paris-Saclay', 'Université Paris-Saclay\nCNRS\nINRIA, CEA, Paris', 'Université Paris-Saclay\nLaboratoire signaux et systèmes\nCNRS\nCentraleSupelec', 'McGill University\nMILA - Quebec AI Institute\nCanada CIFAR AI Chair, Mila', 'International Laboratory on Learning Systems\nMILA - Quebec AI Institute\nUniversité Paris-Saclay\nCNRS', 'Université Paris-Saclay\nCentraleSupelec\nEqual, Paris\nMICS']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29777,Fairness & Bias,Can Large Language Models Understand Real-World Complex Instructions?,"Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs’ ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four criteria and develop corresponding metrics, as current ones are inadequate, biased or too strict and coarse-grained. We compare the performance of representative Chinese-oriented and English-oriented models in following complex instructions through extensive experiments. Resources of CELLO are publicly available at https://github.com/Abbey4799/CELLO.","['NLP: (Large) Language Models', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Qianyu He', 'Jie Zeng', 'Wenhao Huang', 'Lina Chen', 'Jin Xiao', 'Qianxi He', 'Xunzhe Zhou', 'Jiaqing Liang', 'Yanghua Xiao']","['Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'School of Data Science, Fudan University', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\nFudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29795,Fairness & Bias,Debiasing Multimodal Sarcasm Detection with Contrastive Learning,"Despite commendable achievements made by existing work, prevailing multimodal sarcasm detection studies rely more on textual content over visual information. It unavoidably induces spurious correlations between textual words and labels, thereby significantly hindering the models' generalization capability. To address this problem, we define the task of out-of-distribution (OOD) multimodal sarcasm detection, which aims to evaluate models' generalizability when the word distribution is different in training and testing settings. Moreover, we propose a novel debiasing multimodal sarcasm detection framework with contrastive learning, which aims to mitigate the harmful effect of biased textual factors for robust OOD generalization. In particular, we first design counterfactual data augmentation to construct the positive samples with dissimilar word biases and negative samples with similar word biases. Subsequently, we devise an adapted debiasing contrastive learning mechanism to empower the model to learn robust task-relevant features and alleviate the adverse effect of biased words. Extensive experiments show the superiority of the proposed framework.","['NLP: Safety and Robustness', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Language Grounding & Multi-modal NLP']",[],"['Mengzhao Jia', 'Can Xie', 'Liqiang Jing']","['Shandong University', 'Shandong University', 'University of Texas at Dallas']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29834,Fairness & Bias,Robust Evaluation Measures for Evaluating Social Biases in Masked Language Models,"Many evaluation measures are used to evaluate social biases in masked language models (MLMs). However, we find that these previously proposed evaluation measures are lacking robustness in scenarios with limited datasets. This is because these measures are obtained by comparing the pseudo-log-likelihood (PLL) scores of the stereotypical and anti-stereotypical samples using an indicator function. The disadvantage is the limited mining of the PLL score sets without capturing its distributional information. In this paper, we represent a PLL score set as a Gaussian distribution and use Kullback-Leibler (KL) divergence and Jensen–Shannon (JS) divergence to construct evaluation measures for the distributions of stereotypical and anti-stereotypical PLL scores. Experimental results on the publicly available datasets StereoSet (SS) and CrowS-Pairs (CP) show that our proposed measures are significantly more robust and interpretable than those proposed previously.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Safety and Robustness', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],['Yang Liu'],['Tianjin University'],['China']
https://ojs.aaai.org/index.php/AAAI/article/view/29845,Fairness & Bias,Accelerating the Global Aggregation of Local Explanations,"Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a naive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session.    We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-k words with the highest global impact according to different aggregation functions. Some of our techniques are lossless and some are lossy. We show that for a very mild loss of quality, we are able to accelerate the computation by up to 30 times, reducing the computation from hours to minutes. We also devise and study a probabilistic model that accounts for noise in the Anchor algorithm and diminishes the bias toward words that are frequent yet low in impact.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Alon Mor', 'Yonatan Belinkov', 'Benny Kimelfeld']","['Technion, Haifa, Israel', 'Technion, Haifa, Israel', 'Technion, Haifa, Israel']","['Israel', 'Israel', 'Israel']"
https://ojs.aaai.org/index.php/AAAI/article/view/29859,Fairness & Bias,OntoFact: Unveiling Fantastic Fact-Skeleton of LLMs via Ontology-Driven Reinforcement Learning,"Large language models (LLMs) have demonstrated impressive proficiency in information retrieval, while they are prone to generating incorrect responses that conflict with reality, a phenomenon known as intrinsic hallucination. The critical challenge lies in the unclear and unreliable fact distribution within LLMs trained on vast amounts of data. The prevalent approach frames the factual detection task as a question-answering paradigm, where the LLMs are asked about factual knowledge and examined for correctness. However, existing studies primarily focused on deriving test cases only from several specific domains, such as movies and sports, limiting the comprehensive observation of missing knowledge and the analysis of unexpected hallucinations. To address this issue, we propose OntoFact, an adaptive framework for detecting unknown facts of LLMs, devoted to mining the ontology-level skeleton of the missing knowledge. Specifically, we argue that LLMs could expose the ontology-based similarity among missing facts and introduce five representative knowledge graphs (KGs) as benchmarks. We further devise a sophisticated ontology-driven reinforcement learning (ORL) mechanism to produce error-prone test cases with specific entities and relations automatically. The ORL mechanism rewards the KGs for navigating toward a feasible direction for unveiling factual errors. Moreover, empirical efforts demonstrate that dominant LLMs are biased towards answering Yes rather than No, regardless of whether this knowledge is included. To mitigate the overconfidence of LLMs, we leverage a hallucination-free detection (HFD) strategy to tackle unfair comparisons between baselines, thereby boosting the result robustness. Experimental results on 5 datasets, using 32 representative LLMs, reveal a general lack of fact in current LLMs. Notably, ChatGPT exhibits fact error rates of 51.6% on DBpedia and 64.7% on YAGO, respectively. Additionally, the ORL mechanism demonstrates promising error prediction scores, with F1 scores ranging from 70% to 90% across most LLMs. Compared to the exhaustive testing, ORL achieves an average recall of 80% while reducing evaluation time by 35.29% to 63.12%.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models']",[],"['Ziyu Shang', 'Wenjun Ke', 'Nana Xiu', 'Peng Wang', 'Jiajun Liu', 'Yanhui Li', 'Zhizhao Luo', 'Ke Ji']","['School of Computer Science and Engineering, Southeast University', 'School of Computer Science and Engineering, Southeast University\nKey Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'School of Cyber Science and Engineering, Southeast University', 'School of Computer Science and Engineering, Southeast University\nKey Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'School of Computer Science and Engineering, Southeast University', 'State Key Laboratory for Novel Software Technology, Nanjing University', 'Beijing Institute of Computer Technology and Application', 'School of Computer Science and Engineering, Southeast University']","['China', '', 'China', '', 'China', 'China', '', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29897,Fairness & Bias,De-biased Attention Supervision for Text Classification with Causality,"In text classification models, while the unsupervised attention mechanism can enhance performance, it often produces attention distributions that are puzzling to humans, such as assigning high weight to seemingly insignificant conjunctions. Recently, numerous studies have explored Attention Supervision (AS) to guide the model toward more interpretable attention distributions. However, such AS can impact classification performance, especially in specialized domains. In this paper, we address this issue from a causality perspective. Firstly, we leverage the causal graph to reveal two biases in the AS: 1) Bias caused by the label distribution of the dataset. 2) Bias caused by the words' different occurrence ranges that some words can occur across labels while others only occur in a particular label. We then propose a novel De-biased Attention Supervision (DAS) method to eliminate these biases with causal techniques. Specifically, we adopt backdoor adjustment on the label-caused bias and reduce the word-caused bias by subtracting the direct causal effect of the word. Through extensive experiments on two professional text classification datasets (e.g., medicine and law), we demonstrate that our method achieves improved classification accuracy along with more coherent attention distributions.","['NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'NLP: Text Classification']",[],"['Yiquan Wu', 'Yifei Liu', 'Ziyu Zhao', 'Weiming Lu', 'Yating Zhang', 'Changlong Sun', 'Fei Wu', 'Kun Kuang']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Alibaba Group', 'Alibaba Group', 'Zhejiang University', 'Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29901,Fairness & Bias,ALISON: Fast and Effective Stylometric Authorship Obfuscation,"Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features,  allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: (Large) Language Models', 'NLP: Applications', 'NLP: Interpretability', 'Analysis', 'and Evaluation of NLP Models', 'APP: Security']",[],"['Eric Xing', 'Saranya Venkatraman', 'Thai Le', 'Dongwon Lee']","['Washington University in St. Louis', 'The Pennsylvania State University', 'University of Mississippi', 'The Pennsylvania State University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29904,Security,Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification,"Few-shot named entity recognition (NER) aims to recognize novel named entities in low-resource domains utilizing existing knowledge. However, the present few-shot NER models assume that the labeled data are all clean without noise or outliers, and there are few works focusing on the robustness of the cross-domain transfer learning ability to textual adversarial attacks in Few-shot NER. In this work, we comprehensively explore and assess the robustness of few-shot NER models under textual adversarial attack scenario, and found the vulnerability of existing few-shot NER models. Furthermore, we propose a robust two-stage few-shot NER method with Boundary Discrimination and Correlation Purification (BDCP). Specifically, in the span detection stage, the entity boundary discriminative module is introduced to provide a highly distinguishing boundary representation space to detect entity spans. In the entity typing stage, the correlations between entities and contexts are purified by minimizing the interference information and facilitating correlation generalization to alleviate the perturbations caused by textual adversarial attacks. In addition, we construct adversarial examples for few-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive evaluations on those two groups of few-shot NER datasets containing adversarial examples demonstrate the robustness and superiority of the proposed method.","['NLP: Information Extraction', 'NLP: Safety and Robustness']",[],"['Xiaojun Xue', 'Chunxia Zhang', 'Tianxiang Xu', 'Zhendong Niu']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29934,Fairness & Bias,LLMEval: A Preliminary Study on How to Evaluate Large Language Models,"Recently, the evaluation of Large Language Models has emerged as a popular area of research.  The three crucial questions for LLM evaluation are ``what, where, and how to evaluate''. However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with. As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion. In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems.  We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs.  A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results. We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future. The dataset and the results are publicly available at  https://github.com/llmeval. The version with the appendix are publicly available at https://arxiv.org/abs/2312.07398.","['NLP: (Large) Language Models', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: Safety and Robustness']",[],"['Yue Zhang', 'Ming Zhang', 'Haipeng Yuan', 'Shichun Liu', 'Yongyao Shi', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang']","['School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'Shanghai Advanced Institute of Finance, Shanghai Jiaotong University, Shanghai, China', 'Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China', 'School of Computer Science, Fudan University, Shanghai, China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29941,Fairness & Bias,SENCR: A Span Enhanced Two-Stage Network with Counterfactual Rethinking for Chinese NER,"Recently, lots of works that incorporate external lexicon information into character-level Chinese named entity recognition(NER) to overcome the lackness of natural delimiters of words, have achieved many advanced performance. However, obtaining and maintaining high-quality lexicons is costly, especially in special domains. In addition, the entity boundary bias caused by high mention coverage in some boundary characters poses a significant challenge to the generalization of NER models but receives little attention in the existing literature. To address these issues, we propose SENCR, a Span Enhanced Two-stage Network with Counterfactual Rethinking for Chinese NER, that contains a boundary detector for boundary supervision, a convolution-based type classifier for better span representation and a counterfactual rethinking(CR) strategy for debiased boundary detection in inference. The proposed boundary detector and type classifier are jointly trained with the same contextual encoder and then the trained boundary detector is debiased by our proposed CR strategy without modifying any model parameters in the inference stage. Extensive experiments on four Chinese NER datasets show the effectiveness of our proposed approach.","['NLP: Information Extraction', 'NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy']",[],"['Hang Zheng', 'Qingsong Li', 'Shen Chen', 'Yuxuan Liang', 'Li Liu']","['School of Big Data and Software Engineering, Chongqing University, China', 'School of Big Data and Software Engineering, Chongqing University, China', 'School of Big Data and Software Engineering, Chongqing University, China', 'The Hong Kong University of Science and Technology (Guangzhou), China', 'School of Big Data and Software Engineering, Chongqing University, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29948,Fairness & Bias,Quantifying and Analyzing Entity-Level Memorization in Large Language Models,"Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. The results demonstrate that LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.","['NLP: Ethics -- Bias', 'Fairness', 'Transparency & Privacy', 'NLP: (Large) Language Models']",[],"['Zhenhong Zhou', 'Jiuyang Xiang', 'Chaomeng Chen', 'Sen Su']","['Beijing University of Posts and Telecommunications', 'University of Michigan', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29964,Security,MERGE: Fast Private Text Generation,"The drastic increase in language models' parameters has led to a new trend of deploying models in cloud servers, raising growing concerns about private inference for Transformer-based models. Existing two-party privacy-preserving techniques, however, only take into account natural language understanding (NLU) scenarios. Private inference in natural language generation (NLG), crucial for applications like translation and code completion, remains underexplored. In addition, previous privacy-preserving techniques suffer from convergence issues during model training and exhibit poor inference speed when used with NLG models due to the neglect of time-consuming operations in auto-regressive generations. To address these issues, we propose a fast private text generation framework for Transformer-based language models, namely MERGE. MERGE reuses the output hidden state as the word embedding to bypass the embedding computation and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Extensive experiments show that MERGE achieves a 26.5x speedup to the vanilla encrypted model under the sequence length 512, and reduces 80% communication cost, with an up to 10x speedup to state-of-the-art approximated models.","['PEAI: Privacy & Security', 'NLP: (Large) Language Models', 'NLP: Generation']",[],"['Zi Liang', 'Pinghui Wang', 'Ruofei Zhang', 'Nuo Xu', 'Shuo Zhang', 'Lifeng Xing', 'Haitao Bai', 'Ziyang Zhou']","[""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University""]","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29958,Security,Complementary Knowledge Distillation for Robust and Privacy-Preserving Model Serving in Vertical Federated Learning,"Vertical Federated Learning (VFL) enables an active party with labeled data to enhance model performance (utility) by collaborating with multiple passive parties that possess auxiliary features corresponding to the same sample identifiers (IDs). Model serving in VFL is vital for real-world, delay-sensitive applications, and it faces two major challenges: 1) robustness against arbitrarily-aligned data and stragglers; and 2) privacy protection, ensuring minimal label leakage to passive parties. Existing methods fail to transfer knowledge among parties to improve robustness in a privacy-preserving way. In this paper, we introduce a privacy-preserving knowledge transfer framework, Complementary Knowledge Distillation (CKD), designed to enhance the robustness and privacy of multi-party VFL systems. Specifically, we formulate a Complementary Label Coding (CLC) objective to encode only complementary label information of the active party's local model for passive parties to learn. Then, CKD selectively transfers the CLC-encoded complementary knowledge 1) from the passive parties to the active party, and 2) among the passive parties themselves. Experimental results on four real-world datasets demonstrate that CKD outperforms existing approaches in terms of robustness against arbitrarily-aligned data, while also minimizing label privacy leakage.","['PEAI: Safety', 'Robustness & Trustworthiness', 'ML: Privacy', 'ML: Transfer', 'Domain Adaptation', 'Multi-Task Learning', 'PEAI: Privacy & Security']",[],"['Dashan Gao', 'Sheng Wan', 'Lixin Fan', 'Xin Yao', 'Qiang Yang']","['Southern University of Science and Technology, Shenzhen, China\nHong Kong University of Science and Technology, Hong Kong SAR, China', 'Southern University of Science and Technology, Shenzhen, China\nHong Kong University of Science and Technology, Hong Kong SAR, China', 'WeBank AI Lab, Shenzhen, China', 'Southern University of Science and Technology, Shenzhen, China', 'Hong Kong University of Science and Technology, Hong Kong SAR, China']","['China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29965,Security,Does Few-Shot Learning Suffer from Backdoor Attacks?,"The field of few-shot learning (FSL) has shown promising results in scenarios where training data is limited, but its vulnerability to backdoor attacks remains largely unexplored. We first explore this topic by first evaluating the performance of the existing backdoor attack methods on few-shot learning scenarios. Unlike in standard supervised learning, existing backdoor attack methods failed to perform an effective attack in FSL due to two main issues. Firstly, the model tends to overfit to either benign features or trigger features, causing a tough trade-off between attack success rate and benign accuracy. Secondly, due to the small number of training samples, the dirty label or visible trigger in the support set can be easily detected by victims, which reduces the stealthiness of attacks. It seemed that FSL could survive from backdoor attacks.  However, in this paper, we propose the Few-shot Learning Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor attacks. Specifically, we first generate a trigger to maximize the gap between poisoned and benign features. It enables the model to learn both benign and trigger features, which solves the problem of overfitting. To make it more stealthy, we hide the trigger by optimizing two types of imperceptible perturbation, namely attractive and repulsive perturbation, instead of attaching the trigger directly. Once we obtain the perturbations, we can poison all samples in the benign support set into a hidden poisoned support set and fine-tune the model on it. Our method demonstrates a high Attack Success Rate (ASR) in FSL tasks with different few-shot learning paradigms while preserving clean accuracy and maintaining stealthiness. This study reveals that few-shot learning still suffers from backdoor attacks, and its security should be given attention.","['PEAI: Privacy & Security', 'CV: Bias', 'Fairness & Privacy', 'ML: Privacy']",[],"['Xinwei Liu', 'Xiaojun Jia', 'Jindong Gu', 'Yuan Xun', 'Siyuan Liang', 'Xiaochun Cao']","['SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences\nSchool of Cyber Security, University of Chinese Academy of Sciences', 'Nanyang Technological University, Singapore', 'University of Oxford, UK', 'SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences\nSchool of Cyber Security, University of Chinese Academy of Sciences', 'School of Computing, National University of Singapore, Singapore', 'School of Cyber Science and Technology, Shenzhen Campus, Sun Yat-sen University, Shenzhen, China']","['China', 'Singapore', 'United Kingdom', 'China', 'Singapore', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29967,Security,Towards the Robustness of Differentially Private Federated Learning,"Robustness and privacy protection are two important factors of trustworthy federated learning (FL). Existing FL works usually secure data privacy by perturbing local model gradients via the differential privacy (DP) technique, or defend against poisoning attacks by filtering the local gradients in the outlier of the gradient distribution before aggregation. However, these two issues are often addressed independently in existing works, and how to secure federated learning in both privacy and robustness still needs further exploration. In this paper, we unveil that although DP noisy perturbation can improve the learning robustness, DP-FL frameworks are not inherently robust and are vulnerable to a carefully-designed attack method. Furthermore, we reveal that it is challenging for existing robust FL methods to defend against attacks on DP-FL. This can be attributed to the fact that the local gradients of DP-FL are perturbed by random noise, and the selected central gradients inevitably incorporate a higher proportion of poisoned gradients compared to conventional FL. To address this problem, we further propose a new defense method for DP-FL (named Robust-DPFL), which can effectively distinguish poisoned and clean local gradients in DP-FL and robustly update the global model. Experiments on three benchmark datasets demonstrate that baseline methods cannot ensure task accuracy, data privacy, and robustness simultaneously, while Robust-DPFL can effectively enhance the privacy protection and robustness of federated learning meanwhile maintain the task performance.","['PEAI: Privacy & Security', 'ML: Privacy', 'ML: Adversarial Learning & Robustness']",[],"['Tao Qi', 'Huili Wang', 'Yongfeng Huang']","['Tsinghua University', 'Tsinghua University', 'Tsinghua University\nZhongguancun Laboratory']","['China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/29972,Security,"U-trustworthy Models. Reliability, Competence, and Confidence in Decision-Making","With growing concerns regarding bias and discrimination in predictive models, the AI community has increasingly focused on assessing AI system trustworthiness. Conventionally, trustworthy AI literature relies on the probabilistic framework and calibration as prerequisites for trustworthiness. In this work, we depart from this viewpoint by proposing a novel trust framework inspired by the philosophy literature on trust. We present a precise mathematical definition of trustworthiness, termed U-trustworthiness, specifically tailored for a subset of tasks aimed at maximizing a utility function. We argue that a model’s U-trustworthiness is contingent upon its ability to maximize Bayes utility within this task subset. Our first set of results challenges the probabilistic framework by demonstrating its potential to favor less trustworthy models and introduce the risk of misleading trustworthiness assessments. Within the context of U-trustworthiness, we prove that properly-ranked models are inherently U-trustworthy. Furthermore, we advocate for the adoption of the AUC metric as the preferred measure of trustworthiness. By offering both theoretical guarantees and experimental validation, AUC enables robust evaluation of trustworthiness, thereby enhancing model selection and hyperparameter tuning to yield more trustworthy outcomes.","['PEAI: Safety', 'Robustness & Trustworthiness', 'ML: Evaluation and Analysis', 'ML: Other Foundations of Machine Learning']",[],"['Ritwik Vashistha', 'Arya Farahi']","['University of Texas at Austin', 'University of Texas at Austin']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/29974,Security,SAME: Sample Reconstruction against Model Extraction Attacks,"While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during model training. It is compatible with existing active defense methods. Our extensive experiments corroborate the superior efficacy of SAME over state-of-the-art solutions. Our code is available at https://github.com/xythink/SAME.","['PEAI: Privacy & Security', 'CV: Adversarial Attacks & Robustness', 'CV: Bias', 'Fairness & Privacy', 'ML: Privacy', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Yi Xie', 'Jie Zhang', 'Shiqian Zhao', 'Tianwei Zhang', 'Xiaofeng Chen']","['Xidian University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Xidian University']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/29975,Security,High-Fidelity Gradient Inversion in Distributed Learning,"Distributed learning frameworks aim to train global models by sharing gradients among clients while preserving the data privacy of each individual client. However, extensive research has demonstrated that these learning frameworks do not absolutely ensure the privacy, as training data can be reconstructed from shared gradients. Nevertheless, the existing privacy-breaking attack methods have certain limitations. Some are applicable only to small models, while others can only recover images in small batch size and low resolutions, or with low fidelity. Furthermore, when there are some data with the same label in a training batch, existing attack methods usually perform poorly. In this work, we successfully address the limitations of existing attacks by two steps. Firstly, we model the coefficient of variation (CV) of features and design an evolutionary algorithm based on the minimum CV to accurately reconstruct the labels of all training data. After that, we propose a stepwise gradient inversion attack, which dynamically adapts the objective function, thereby effectively and rationally promoting the convergence of attack results towards an optimal solution. With these two steps, our method is able to recover high resolution images (224*224 pixel, from ImageNet and Web) with high fidelity in distributed learning scenarios involving complex models and larger batch size. Experiment results demonstrate the superiority of our approach, reveal the potential vulnerabilities of the distributed learning paradigm, and emphasize the necessity of developing more secure mechanisms. Source code is available at https://github.com/MiLab-HITSZ/2023YeHFGradInv.","['PEAI: Privacy & Security', 'PEAI: Safety', 'Robustness & Trustworthiness']",[],"['Zipeng Ye', 'Wenjian Luo', 'Qi Zhou', 'Yubo Tang']","['School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies', 'School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies\nPeng Cheng Laboratory', 'School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies', 'School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30011,Fairness & Bias,s-ID: Causal Effect Identification in a Sub-population,"Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup, which is distinguished from the whole population through the influence of systematic biases in the sampling process. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.","['RU: Causality', 'KRR: Action', 'Change', 'and Causality', 'ML: Causal Learning', 'RU: Probabilistic Inference']",[],"['Amir Mohammad Abouei', 'Ehsan Mokhtarian', 'Negar Kiyavash']","['EPFL', 'EPFL', 'EPFL']","['Switzerland', 'Switzerland', 'Switzerland']"
https://ojs.aaai.org/index.php/AAAI/article/view/30013,Security,Backward Responsibility in Transition Systems Using General Power Indices,"To improve reliability and the understanding of AI systems, there is increasing interest in the use of formal methods, e.g. model checking. Model checking tools produce a counterexample when a model does not satisfy a property. Understanding these counterexamples is critical for efficient debugging, as it allows the developer to focus  on the parts of the program that caused the issue.  To this end, we present a new technique that ascribes a responsibility value to each state in a transition system that does not satisfy a given safety property. The value is higher if the non-deterministic choices in a state have more power to change the outcome, given the behaviour observed in the counterexample. For this, we employ a concept from cooperative game theory – namely general power indices, such as the Shapley value – to compute the responsibility of the states.  We present an optimistic and pessimistic version of responsibility that differ in how they treat the states that do not lie on the counterexample. We give a characterisation of optimistic responsibility that leads to an efficient algorithm for it and show computational hardness of the pessimistic version. We also present a tool to compute responsibility and show how a stochastic algorithm can be used to approximate responsibility in larger models. These methods can be deployed in the design phase, at runtime and at inspection time to gain insights on causal relations within the behavior of AI systems.","['RU: Causality', 'GTEP: Cooperative Game Theory']",[],"['Christel Baier', 'Roxane van den Bossche', 'Sascha Klüppelholz', 'Johannes Lehmann', 'Jakob Piribauer']","['TU Dresden, Germany\nCentre for Tactile Internet with Human-in-the-Loop (CeTI)', 'Université Paris-Saclay, ENS Paris-Saclay, France', 'TU Dresden, Germany', 'TU Dresden, Germany\nCentre for Tactile Internet with Human-in-the-Loop (CeTI)', 'TU Dresden, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30027,Fairness & Bias,Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach,"This paper studies bandit problems where an agent has access to offline data that might be utilized to potentially improve the estimation of each arm’s reward distribution. A major obstacle in this setting is the existence of compound biases from the observational data. Ignoring these biases and blindly fitting a model with the biased data could even negatively affect the online learning phase. In this work, we formulate this problem from a causal perspective. First, we categorize the biases into confounding bias and selection bias based on the causal structure they imply. Next, we extract the causal bound for each arm that is robust towards compound biases from biased observational data. The derived bounds contain the ground truth mean reward and can effectively guide the bandit agent to learn a nearly-optimal decision policy. We also conduct regret analysis in both contextual and non-contextual bandit settings and show that prior causal bounds could help consistently reduce the asymptotic regret.","['RU: Causality', 'ML: Ethics', 'Bias', 'and Fairness', 'ML: Online Learning & Bandits']",[],"['Wen Huang', 'Xintao Wu']","['University of Arkansas', 'University of Arkansas']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30086,Security,Rethinking the Development of Large Language Models from the Causal Perspective: A Legal Text Prediction Case Study,"While large language models (LLMs) exhibit impressive performance on a wide range of NLP tasks, most of them fail to learn the causality from correlation, which disables them from learning rationales for predicting. Rethinking the whole developing process of LLMs is of great urgency as they are adopted in various critical tasks that need rationales, including legal text prediction (e.g., legal judgment prediction). In this paper, we first explain the underlying theoretical mechanism of their failure and argue that both the data imbalance and the omission of causality in model design and selection render the current training-testing paradigm failed to select the unique causality-based model from correlation-based models. Second, we take the legal text prediction task as the testbed and reconstruct the developing process of LLMs by simultaneously infusing causality into model architectures and organizing causality-based adversarial attacks for evaluation. Specifically, we base our reconstruction on our theoretical analysis and propose a causality-aware self-attention mechanism (CASAM), which prevents LLMs from entangling causal and non-causal information by restricting the interaction between causal and non-causal words. Meanwhile, we propose eight kinds of legal-specific attacks to form causality-based model selection. Our extensive experimental results demonstrate that our proposed CASAM achieves state-of-the-art (SOTA) performances and the strongest robustness on three commonly used legal text prediction benchmarks. We make our code publicly available at https://github.com/Carrot-Red/Rethink-LLM-development.",['General'],[],"['Haotian Chen', 'Lingwei Zhang', 'Yiran Liu', 'Yang Yu']","['Fudan University', 'Johns Hopkins University', 'Tsinghua University', 'Tsinghua University']","['China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30096,Security,From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space,"Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method for model correction on the concept level that explicitly reduces model sensitivity towards biases via gradient penalization. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures. Code and Appendix are available on https://github.com/frederikpahde/rrclarc.",['General'],[],"['Maximilian Dreyer', 'Frederik Pahde', 'Christopher J. Anders', 'Wojciech Samek', 'Sebastian Lapuschkin']","['Fraunhofer Heinrich Hertz Institute', 'Fraunhofer Heinrich Hertz Institute', 'Technical University of Berlin', 'Fraunhofer Heinrich Hertz Institute\nTechnical University of Berlin\nBIFOLD – Berlin Institute for the Foundations of Learning and Data', 'Fraunhofer Heinrich Hertz Institute']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AAAI/article/view/30142,Fairness & Bias,SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models,"Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, we start with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two open source generative language models and we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We demonstrate that the deliberate design of the templates in our benchmark (e.g., adding biasing text to the prompt or using different verbs that change the answer that indicates bias) impacts the model tendencies to generate socially biased output. Additionally, through manual evaluation, we discover problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning.  Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.",['General'],[],"['Manish Nagireddy', 'Lamogha Chiazor', 'Moninder Singh', 'Ioana Baldini']","['IBM Research', 'IBM Research', 'IBM Research', 'IBM Research']","['', '', '', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30158,Security,Bidirectional Contrastive Split Learning for Visual Question Answering,"Visual Question Answering (VQA) based on multi-modal data facilitates real-life applications such as home robots and medical diagnoses. One significant challenge is to devise a robust decentralized learning framework for various client models where centralized data collection is refrained due to confidentiality concerns. This work aims to tackle privacy-preserving VQA by decoupling a multi-modal model into representation modules and a contrastive module, leveraging inter-module gradients sharing and inter-client weight sharing. To this end, we propose Bidirectional Contrastive Split Learning (BiCSL) to train a global multi-modal model on the entire data distribution of decentralized clients. We employ the contrastive loss that enables a more efficient self-supervised learning of decentralized modules. Comprehensive experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models, demonstrating the effectiveness of the proposed method. Furthermore, we inspect BiCSL's robustness against a dual-key backdoor attack on VQA. Consequently, BiCSL shows significantly enhanced resilience when exposed to the multi-modal adversarial attack compared to the centralized learning method, which provides a promising approach to decentralized multi-modal learning.",['General'],[],"['Yuwei Sun', 'Hideya Ochiai']","['The University of Tokyo\nRIKEN AIP', 'The University of Tokyo']","['Japan', 'Japan']"
https://ojs.aaai.org/index.php/AAAI/article/view/30161,Security,Toward More Generalized Malicious URL Detection Models,"This paper reveals a data bias issue that can profoundly hinder the performance of machine learning models in malicious URL detection. We describe how such bias can be diagnosed using interpretable machine learning techniques and further argue that such biases naturally exist in the real world security data for training a classification model. To counteract these challenges, we propose a debiased training strategy that can be applied to most deep-learning based models to alleviate the negative effects of the biased features. The solution is based on the technique of adversarial training to train deep neural networks learning invariant embedding from biased data. Through extensive experimentation, we substantiate that our innovative strategy fosters superior generalization capabilities across both CNN-based and RNN-based detection models. The findings presented in this work not only expose a latent issue in the field but also provide an actionable remedy, marking a significant step forward in the pursuit of more reliable and robust malicious URL detection.",['General'],[],"['Yun-Da Tsai', 'Cayon Liow', 'Yin Sheng Siang', 'Shou-De Lin']","['National Taiwan University', 'National Taiwan University', 'National Taiwan University', 'National Taiwan University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30168,Fairness & Bias,Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks,"A plethora of fair graph neural networks (GNNs) have been proposed to promote algorithmic fairness for high-stake real-life contexts. Meanwhile, explainability is generally proposed to help machine learning practitioners debug models by providing human-understandable explanations. However, seldom work on explainability is made to generate explanations for fairness diagnosis in GNNs. From the explainability perspective, this paper explores the problem of what subgraph patterns cause the biased behavior of GNNs, and what actions could practitioners take to rectify the bias? By answering the two questions, this paper aims to produce compact, diagnostic, and actionable explanations that are responsible for discriminatory behavior. Specifically, we formulate the problem of generating diagnostic and actionable explanations as a multi-objective combinatorial optimization problem. To solve the problem, a dedicated multi-objective evolutionary algorithm is presented to ensure GNNs' explainability and fairness in one go. In particular, an influenced nodes-based gradient approximation is developed to boost the computation efficiency of the evolutionary algorithm. We provide a theoretical analysis to illustrate the effectiveness of the proposed framework. Extensive experiments have been conducted to demonstrate the superiority of the proposed method in terms of classification performance, fairness, and interpretability.",['General'],[],"['Zhenzhong Wang', 'Qingyuan Zeng', 'Wanyu Lin', 'Min Jiang', 'Kay Chen Tan']","['The Hong Kong Polytechnic University', 'Xiamen University', 'The Hong Kong Polytechnic University', 'Xiamen University', 'The Hong Kong Polytechnic University']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://ojs.aaai.org/index.php/AAAI/article/view/30182,Security,Responsible Bandit Learning via Privacy-Protected Mean-Volatility Utility,"For ensuring the safety of users by protecting the privacy,  the traditional privacy-preserving bandit algorithm aiming to maximize the mean reward has been widely studied in scenarios such as online ride-hailing, advertising recommendations, and personalized healthcare. However, classical bandit learning is irresponsible in such practical applications as they fail to account for risks in online decision-making and ignore external system information. This paper firstly proposes  privacy protected mean-volatility utility as the objective of bandit learning and proves its responsibility, because it aims at achieving the maximum probability of utility by considering the risk.  Theoretically, our proposed responsible bandit learning is expected to achieve the fastest convergence rate among current bandit algorithms  and generates more statistical power than classical normality-based test. Finally, simulation studies provide supporting evidence for the theoretical results and demonstrate stronger performance when using stricter privacy budgets.",['General'],[],"['Shanshan Zhao', 'Wenhai Cui', 'Bei Jiang', 'Linglong Kong', 'Xiaodong Yan']","['Mathematics Discipline, Shandong University', 'Mathematics Discipline, Shandong University', 'Mathematics Discipline, University of Alberta', 'Mathematics Discipline, University of Alberta', 'Mathematics Discipline, Shandong University\nShandong National Center for Applied Mathematics']","['China', 'China', 'Canada', 'Canada', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30179,Privacy & Data Governance,LR-XFL: Logical Reasoning-Based Explainable Federated Learning,"Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients’ local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively. The explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model’s robustness to errors. It has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.",['General'],[],"['Yanci Zhang', 'Han Yu']","['Nanyang Technological University', 'Nanyang Technological University']","['Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AAAI/article/view/30197,Security,Referee-Meta-Learning for Fast Adaptation of Locational Fairness,"When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model. Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level.",['General'],[],"['Weiye Chen', 'Yiqun Xie', 'Xiaowei Jia', 'Erhu He', 'Han Bao', 'Bang An', 'Xun Zhou']","['University of Maryland', 'University of Maryland', 'University of Pittsburgh', 'University of Pittsburgh', 'University of Iowa', 'University of Iowa', 'University of Iowa']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30200,Security,Blind-Touch: Homomorphic Encryption-Based Distributed Neural Network Inference for Privacy-Preserving Fingerprint Authentication,"Fingerprint authentication is a popular security mechanism for smartphones and laptops. However, its adoption in web and cloud environments has been limited due to privacy concerns over storing and processing biometric data on servers. This paper introduces Blind-Touch, a novel machine learning-based fingerprint authentication system leveraging homomorphic encryption to address these privacy concerns. Homomorphic encryption allows computations on encrypted data without decrypting. Thus, Blind-Touch can keep fingerprint data encrypted on the server while performing machine learning operations. Blind-Touch combines three strategies to efficiently utilize homomorphic encryption in machine learning: (1) It optimizes the feature vector for a distributed architecture, processing the first fully connected layer (FC-16) in plaintext on the client side and the subsequent layer (FC-1) post-encryption on the server, thereby minimizing encrypted computations; (2) It employs a homomorphic encryption-compatible data compression technique capable of handling 8,192 authentication results concurrently; and (3) It utilizes a clustered server architecture to simultaneously process authentication results, thereby enhancing scalability with increasing user numbers. Blind-Touch achieves high accuracy on two benchmark fingerprint datasets, with a 93.6% F1- score for the PolyU dataset and a 98.2% F1-score for the SOKOTO dataset. Moreover, Blind-Touch can match a fingerprint among 5,000 in about 0.65 seconds. With its privacy-focused design, high accuracy, and efficiency, Blind-Touch is a promising alternative to conventional fingerprint authentication for web and cloud applications.",['General'],[],"['Hyunmin Choi', 'Simon S. Woo', 'Hyoungshick Kim']","['NAVER Cloud, South Korea\nDepartment of Computer Science and Engineering, Sungkyunkwan University, South Korea', 'Department of Artificial Intelligence, Sungkyunkwan University, South Korea\nDepartment of Computer Science and Engineering, Sungkyunkwan University, South Korea', 'Department of Computer Science and Engineering, Sungkyunkwan University, South Korea']","['South Korea', 'South Korea', 'South Korea']"
https://ojs.aaai.org/index.php/AAAI/article/view/30211,Fairness & Bias,Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency,"Predictive analytics has been widely used in various domains, including education, to inform decision-making and improve outcomes. However, many predictive models are proprietary and inaccessible for evaluation or modification by researchers and practitioners, limiting their accountability and ethical design. Moreover, predictive models are often opaque and incomprehensible to the officials who use them, reducing their trust and utility. Furthermore, predictive models may introduce or exacerbate bias and inequity, as they have done in many sectors of society. Therefore, there is a need for transparent, interpretable, and fair predictive models that can be easily adopted and adapted by different stakeholders. In this paper, we propose a fair predictive model based on multivariate adaptive regression splines (MARS) that incorporates fairness measures in the learning process. MARS is a non-parametric regression model that performs feature selection, handles non-linear relationships, generates interpretable decision rules, and derives optimal splitting criteria on the variables. Specifically, we integrate fairness into the knot optimization algorithm and provide theoretical and empirical evidence of how it results in a fair knot placement. We apply our fairMARS model to real-world data and demonstrate its effectiveness in terms of accuracy and equity. Our paper contributes to the advancement of responsible and ethical predictive analytics for social good.",['General'],[],"['Parian Haghighat', 'Denisa Gándara', 'Lulu Kang', 'Hadis Anahideh']","['University of Illinois at Chicago', 'The University of Texas at Austin', 'Department of Mathematics and Statistics, University of Massachusetts Amherst', 'University of Illinois Chicago']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30275,Fairness & Bias,Adventures of Trustworthy Vision-Language Models: A Survey,"Recently, transformers have become incredibly popular in computer vision and vision-language tasks. This notable rise in their usage can be primarily attributed to the capabilities offered by attention mechanisms and the outstanding ability of transformers to adapt and apply themselves to a variety of tasks and domains. Their versatility and state-of-the-art performance have established them as indispensable tools for a wide array of applications. However, in the constantly changing landscape of machine learning, the assurance of the trustworthiness of transformers holds utmost importance. This paper conducts a thorough examination of vision-language transformers, employing three fundamental principles of responsible AI: Bias, Robustness, and Interpretability. The primary objective of this paper is to delve into the intricacies and complexities associated with the practical use of transformers, with the overarching goal of advancing our comprehension of how to enhance their reliability and accountability.","['Vision-Language Models', 'Interpretability', 'Bias', 'Robustness']",[],"['Mayank Vatsa', 'Anubhooti Jain', 'Richa Singh']","['IIT Jodhpur, India', 'IIT Jodhpur, India', 'IIT Jodhpur, India']","['India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30286,Security,Fostering Trustworthiness in Machine Learning Algorithms,"Recent years have seen a surge in research that develops and applies machine learning algorithms to create intelligent learning systems. However, traditional machine learning algorithms have primarily focused on optimizing accuracy and efficiency, and they often fail to consider how to foster trustworthiness in their design. As a result, machine learning models usually face a trust crisis in real-world applications. Driven by these urgent concerns about trustworthiness, in this talk, I will introduce my research efforts towards the goal of making machine learning trustworthy. Specifically, I will delve into the following key research topics: security vulnerabilities and robustness, model explanations, and privacy-preserving mechanisms.","['Artificial Intelligence', 'Machine Learning', 'Trustworthiness']",[],['Mengdi Huai'],['Iowa State University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30298,Fairness & Bias,Towards Trustworthy Deep Learning,"Deep neural networks (DNNs) have achieved unprecedented success across many scientific and engineering fields in the last decades. Despite its empirical success, unfortunately, recent studies have shown that there are various failure modes and blindspots in DNN models which may result in unexpected serious failures and potential harms, e.g. the existence of adversarial examples and small perturbations. This is not acceptable especially for safety critical and high stakes applications in the real-world, including healthcare, self-driving cars, aircraft control systems, hiring and malware detection protocols. Moreover, it has been challenging to understand why and when DNNs will fail due to their complicated structures and black-box behaviors. Lacking interpretability is one critical issue that may seriously hinder the deployment of DNNs in high-stake applications, which need interpretability to trust the prediction, to understand potential failures, and to be able to mitigate harms and eliminate biases in the model.   To make DNNs trustworthy and reliable for deployment, it is necessary and urgent to develop methods and tools that can (i) quantify and improve their robustness against adversarial and natural perturbations, and (ii) understand their underlying behaviors and further correct errors to prevent injuries and damages. These are the important first steps to enable Trustworthy AI and Trustworthy Machine Learning. In this talk, I will survey a series of research efforts in my lab contributed to tackling the grand challenges in (i) and (ii). In the first part of my talk, I will overview our research effort in Robust Machine Learning since 2017, where we have proposed the first attack-agnostic robustness evaluation metric, the first efficient robustness certification algorithms for various types of perturbations, and efficient robust learning algorithms across supervised learning to deep reinforcement learning.    In the second part of my talk, I will survey a series of exciting results in my lab on accelerating interpretable machine learning and explainable AI. Specifically, I will show how we could bring interpretability into deep learning by leveraging recent advances in multi-modal models. I'll present recent works in our group on automatically dissecting neural networks with open vocabulary concepts, designing interpretable neural networks without concept labels, and briefly overview our recent efforts on demystifying black-box DNN training process, automated neuron explanations for Large Language Models and the first robustness evaluation of a family of neuron-level interpretation techniques.","['Deep Learning', 'Trustworthy Machine Learning', 'Robust Machine Learning', 'Interpretable Machine Learning']",[],['Tsui-Wei (Lily) Weng'],['UCSD'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30346,Security,AI Evaluation Authorities: A Case Study Mapping Model Audits to Persistent Standards,"Intelligent system audits are labor-intensive assurance activities that are typically performed once and discarded along with the opportunity to programmatically test all similar products for the market. This study illustrates how several incidents (i.e., harms) involving Named Entity Recognition (NER) can be prevented by scaling up a previously-performed audit of NER systems. The audit instrument's diagnostic capacity is maintained through a security model that protects the underlying data (i.e., addresses Goodhart's Law). An open-source evaluation infrastructure is released along with an example derived from a real-world audit that reports aggregated findings without exposing the underlying data.",['Track: AI Incidents and Best Practices (paper)'],[],"['Arihant Chadda', 'Sean McGregor', 'Jesse Hostetler', 'Andrea Brennen']","['IQT Labs', 'UL Digital Safety Research Institute', 'UL Digital Safety Research Institute', 'IQT Labs']","['', 'India', 'India', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30348,Security,AI Risk Profiles: A Standards Proposal for Pre-deployment AI Risk Disclosures,"As AI systems’ sophistication and proliferation have increased, awareness of the risks has grown proportionally. The AI industry is increasingly emphasizing the need for transparency, with proposals ranging from standardizing use of technical disclosures, like model cards, to regulatory licensing regimes. Since the AI value chain is complicated, with actors bringing varied expertise, perspectives, and values, it is crucial that consumers of transparency disclosures be able to understand the risks of the AI system in question. In this paper we propose a risk profiling standard which can guide downstream decision-making, including triaging further risk assessment, informing procurement and deployment, and directing regulatory frameworks. The standard is built on our proposed taxonomy of AI risks, which distills the wide variety of risks proposed in the literature into a high-level categorization. We outline the myriad data sources needed to construct informative Risk Profiles and propose a template and methodology for collating risk information into a standard, yet flexible, structure. We apply this methodology to a number of prominent AI systems using publicly available information. To conclude, we discuss design decisions for the profiles and future work.","['Assurance', 'Generative AI', 'Track: AI Incidents and Best Practices (paper)', 'Education and Training']",[],"['Eli Sherman', 'Ian Eisenberg']","['Credo AI', 'Credo AI']","['', '']"
https://ojs.aaai.org/index.php/AAAI/article/view/30373,Privacy & Data Governance,A Picture Is Worth a Thousand Words: Co-designing Text-to-Image Generation Learning Materials for K-12 with Educators,"Text-to-image generation (TTIG) technologies are Artificial Intelligence (AI) algorithms that use natural language algorithms in combination with visual generative algorithms. TTIG tools have gained popularity in recent months, garnering interest from non-AI experts, including educators and K-12 students. While they have exciting creative potential when used by K-12 learners and educators for creative learning, they are also accompanied by serious ethical implications, such as data privacy, spreading misinformation, and algorithmic bias. Given the potential learning applications, social implications, and ethical concerns, we designed 6-hour learning materials to teach K-12 teachers from diverse subject expertise about the technical implementation, classroom applications, and ethical implications of TTIG algorithms. We piloted the learning materials titled “Demystify text-to-image generative tools for K-12 educators"" with 30 teachers across two workshops with the goal of preparing them to teach about and use TTIG tools in their classrooms. We found that teachers demonstrated a technical, applied and ethical understanding of TTIG algorithms and successfully designed prototypes of teaching materials for their classrooms.","['Generative AI', 'Text-to-image Generation', 'Creative ML', 'Teacher Education']",[],"['Safinah Ali', 'Prerna Ravi', 'Katherine Moore', 'Hal Abelson', 'Cynthia Breazeal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AAAI/article/view/30395,Fairness & Bias,Towards Trustworthy Autonomous Systems via Conversations and Explanations,"Autonomous systems fulfil an increasingly important role in our societies, however, AI-powered systems have seen less success over the years, as they are expected to tackle a range of social, legal, or technological challenges and modern neural network-based AI systems cannot yet provide guarantees to many of these challenges. Particularly important is that these systems are black box decision makers, eroding human oversight, contestation, and agency. To address this particular concern, my thesis focuses on integrating social explainable AI with cognitive methods and natural language processing to shed light on the internal processes of autonomous systems in a way accessible to lay users. I propose a causal explanation generation model for decision-making called CEMA based on counterfactual simulations in multi-agent systems. I also plan to integrate CEMA with a broader natural language processing pipeline to support targeted and personalised explanations that address people's cognitive biases. I hope that my research will have a positive impact on the public acceptance of autonomous agents by building towards more trustworthy AI.","['Trustworthy Autonomous Systems', 'Social Explainable AI', 'Multi-agent Systems', 'Human-centric AI', 'AI Regulation']",[],['Balint Gyevnar'],['University of Edinburgh'],['United Kingdom']
https://ojs.aaai.org/index.php/AAAI/article/view/30401,Fairness & Bias,Thesis Summary: Operationalizing User-Inclusive Transparency in Artificial Intelligence Systems,"Artificial intelligence system architects can increase user trust by designing systems that are inherently transparent. We propose the idea of representing an AI system as an amalgamation of the AI Model (algorithms), data (input and output, including outcomes), and the user interface with visual interpretations (e.g. graphs, Venn diagrams). By designing human controls and feedback mechanisms for AI systems that allow users to exert control over them we can integrate transparency into existing user interfaces. Our plan is to design prototypes of transparent user interfaces for AI systems using well-known usability principles. By conducting surveys we will study their impact to see if these principles help the user to work with the AI system with confidence and if the user perceives the system to be adequately transparent.","['Transparency', 'Usability Principles', 'Explanations', 'Algorithmic Bias', 'Uncertainity', 'Algorithmic Fairness', 'Data', 'Accuracy', 'Trust']",[],['Deepa Muralidhar'],['Georgia State University'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30454,Fairness & Bias,Explainable Earnings Call Representation Learning (Student Abstract),"Earnings call transcripts hold valuable insights that are vital for investors and analysts when making informed decisions. However, extracting these insights from lengthy and complex transcripts can be a challenging task. The traditional manual examination is not only time-consuming but also prone to errors and biases. Deep learning-based representation learning methods have emerged as promising and automated approaches to tackle this problem. Nevertheless, they may encounter significant challenges, such as the unreliability of the representation encoding process and certain domain-specific requirements in the context of finance. To address these issues, we propose a novel transcript representation learning model. Our model leverages the structural information of transcripts to effectively extract key insights, while endowing model with explainability via variational information bottleneck. Extensive experiments on two downstream financial tasks demonstrate the effectiveness of our approach.","['Representation Learning', 'Contrastive Learning', 'Earnings Call Transcript', 'Information Bottleneck']",[],"['Yanlong Huang', 'Yue Lei', 'Wenxin Tai', 'Zhangtao Cheng', 'Ting Zhong', 'Kunpeng Zhang']","['University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'University of Electronic Science and Technology of China\nKash Institute of Electronics and Information Industry', 'University of Maryland, College Park']","['China', 'China', 'China', 'China', 'China', 'China']"
https://ojs.aaai.org/index.php/AAAI/article/view/30513,Security,Confidence Is All You Need for MI Attacks (Student Abstract),"In this evolving era of machine learning security, membership inference attacks have emerged as a potent threat to the confidentiality of sensitive data. In this attack, adversaries aim to determine whether a particular point was used during the training of a target model. This paper proposes a new method to gauge a data point’s membership in a model’s training set. Instead of correlating loss with membership, as is traditionally done, we have leveraged the fact that training examples generally exhibit higher confidence values when classified into their actual class. During training, the model is essentially being ’fit’ to the training data and might face particular difficulties in generalization to unseen data. This asymmetry leads to the model achieving higher confidence on the training data as it exploits the specific patterns and noise present in the training data. Our proposed approach leverages the confidence values generated by the machine-learning model. These confidence values provide a probabilistic measure of the model’s certainty in its predictions and can further be used to infer the membership of a given data point. Additionally, we also introduce another variant of our method that allows us to carry out this attack without knowing the ground truth(true class) of a given data point, thus offering an edge over existing label-dependent attack methods.","['Confidence Value', 'Likelihood Ratio Attack (LiRA)', 'Membership Inference Attacks', 'Model Privacy', 'Adversarial Attacks']",[],"['Abhishek Sinha', 'Himanshi Tibrewal', 'Mansi Gupta', 'Nikhar Waghela', 'Shivank Garg']","['Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667', 'Indian Institute of Technology Roorkee, Roorkee, Uttarakhand, India - 247667']","['India', 'India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AAAI/article/view/30550,Security,A Novel Approach for Longitudinal Modeling of Aging Health and Predicting Mortality Rates,"Aging is a complex stochastic process that affects healthy functioning through various pathways. In contrast to the more commonly used cross-sectional methods, our research focuses on longitudinal modeling of aging, a less explored but crucial area. We have developed a Stochastic Differential Equation (SDE) model, at the forefront of aging research, designed to accurately forecast the health trajectories and survival rates of individuals. This model adeptly delineates the connections between different health indicators and provides clear, interpretable results. Our approach utilizes the SDE framework to encapsulate the inherent uncertainty in the aging process. Moreover, it incorporates a Recurrent Neural Network (RNN) to integrate past health data into future health projections. We plan to train and test our model using a comprehensive dataset tailored for aging studies. This model is not only computationally cost-effective but also highly relevant in assessing health risks in older populations, particularly for those at high risk. It can serve as an essential tool in anticipating and preparing for challenges like infectious disease outbreaks. Overall, our research aims to improve health equity and global health security significantly, offering substantial benefits to public health and deepening our understanding of the aging process.","['Aging', 'Math Modeling', 'Stochastic Differential Equation', 'Interpretability', 'Health And Mortality']",[],['Hannah Guan'],['Harvard College'],['United States']
https://ojs.aaai.org/index.php/AAAI/article/view/30579,Security,"Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning","We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.","['Visual processing', 'Software and testing tools for developing AI technologies', 'Simulation environments for AI agents and multi-agent systems', 'Artificial Intelligence']",[],"['Soumyendu Sarkar', 'Ashwin Ramesh Babu', 'Sajad Mousavi', 'Vineet Gundecha', 'Avisek Naug', 'Sahand Ghorbanpour']","['Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise', 'Hewlett Packard Enterprise']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"