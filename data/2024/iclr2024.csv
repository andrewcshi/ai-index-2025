link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://openreview.net/forum?id=IEduRUO55F,Security,Eureka: Human-Level Reward Design via Coding Large Language Models,"Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",[],[],"['Yecheng Jason Ma', 'William Liang', 'Guanzhi Wang', 'De-An Huang', 'Osbert Bastani', 'Dinesh Jayaraman', 'Yuke Zhu', 'Linxi Fan', 'Anima Anandkumar']","['', 'University of Pennsylvania', 'California Institute of Technology', 'NVIDIA', 'University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', 'Computer Science Department, University of Texas, Austin', 'NVIDIA', 'Computer Science, California Institute of Technology']","['', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=IGzaH538fz,Security,GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations,"Graph classification, which aims to predict a label for a graph, has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier misclassifies the perturbed graph. Such vulnerability impedes the deployment of graph classification in security/safety-critical applications. Existing empirical defenses lack formal robustness guarantees and could be broken by adaptive or unknown attacks. Existing provable defenses have the following limitations: 1)  they achieve sub-optimal robustness guarantees for graph structure perturbation, 2) they cannot provide robustness guarantees for arbitrarily node feature perturbations, 3) their robustness guarantees are probabilistic, meaning they could be incorrect with a non-zero probability, and 4) they incur large computation costs. We aim to address those limitations in this work. We propose GNNCert, a certified defense against both graph structure and node feature perturbations for graph classification. Our GNNCert provably predicts the same label for a graph when the number of perturbed edges and the number of nodes with perturbed features are bounded. Our results on 8 benchmark datasets show that GNNCert outperforms three state-of-the-art methods.",[],[],"['zaishuo xia', 'Han Yang', 'Binghui Wang', 'Jinyuan Jia']","['Microsoft', 'Sichuan University', 'Illinois Institute of Technology', 'College of IST, Pennsylvania State University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=RIuevDSK5V,Fairness & Bias,ConR: Contrastive Regularizer for Deep Imbalanced Regression,"Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. ConR discerns the disagreements between the label space and feature space, and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategies in a contrastive manner: incorrect proximities are penalized proportionate to the label similarities and the correct ones are encouraged to model local similarities. ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. Moreover, ConR is orthogonal to existing approaches and smoothly extends to uni- and multi-dimensional label spaces. Our comprehensive experiments show that ConR significantly boosts the performance of all the state-of-the-art methods on four large-scale deep imbalanced regression benchmarks.",[],[],"['Mahsa Keramati', 'Lili Meng', 'R. David Evans']","['Simon Fraser University', 'University of British Columbia', '']","['Canada', 'Canada', '']"
https://openreview.net/forum?id=HT2dAhh4uV,Fairness & Bias,Learning to Compose: Improving Object Centric Learning by Injecting Compositionality,"Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations. In this study, we propose a novel objective that explicitly encourages compositionality of the representations. Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data. We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices.",[],[],"['Whie Jung', 'Jaehoon Yoo', 'Sungjin Ahn', 'Seunghoon Hong']","['School of Computing, Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', '', 'School of Computing, Korea Advanced Institute of Science and Technology']","['South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=YLJs4mKJCF,Fairness & Bias,Towards Poisoning Fair Representations,"Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female.  Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks.  Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures.  This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much demographic information as possible by injecting carefully crafted poisoning samples into the training data. This attack entails a prohibitive bilevel optimization, wherefore an effective approximated solution is proposed. A theoretical analysis on the needed number of poisoning samples is derived and sheds light on defending against the attack. Experiments on benchmark fairness datasets and state-of-the-art fair representation learning models demonstrate the superiority of our attack.",[],[],"['Tianci Liu', 'Haoyu Wang', 'Feijie Wu', 'Hengtong Zhang', 'Pan Li', 'Lu Su', 'Jing Gao']","['ECE, Purdue University', 'State University of New York at Albany', 'Purdue University', 'Tencent AI Lab', 'Georgia Institute of Technology', 'Purdue University', 'Purdue University']","['United States', 'United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=9WD9KwssyT,Fairness & Bias,Zipformer: A faster and better encoder for automatic speech recognition,"The Conformer has become the most popular encoder model for automatic speech recognition (ASR).  It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer.  Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4)  new activation functions SwooshR and SwooshL work better than Swish.  We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster converge and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.",[],[],"['Zengwei Yao', 'Liyong Guo', 'Xiaoyu Yang', 'Wei Kang', 'Fangjun Kuang', 'Yifan Yang', 'Zengrui Jin', 'Long Lin', 'Daniel Povey']","['Xiaomi Corporation', ""Northwest Polytechnical University Xi'an"", 'University of Cambridge', 'Xiaomi Corp.', 'Xiaomi', 'Computer Science and Engineering, Shanghai Jiaotong University', '', 'Xiaomi corp.', 'Johns Hopkins University']","['China', 'China', 'United Kingdom', 'China', 'China', 'China', 'China', 'China', 'United States']"
https://openreview.net/forum?id=SdeAPV1irk,Security,Incremental Randomized Smoothing Certification,"Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive.  We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of the approximate model from scratch.",[],[],"['Shubham Ugare', 'Tarun Suresh', 'Debangshu Banerjee', 'Gagandeep Singh', 'Sasa Misailovic']","['Computer Science, University of Illinois at Urbana-Champaign', 'Department of Computer Science', 'Computer Science, University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'School of Computing and Data Science, University of Illinois, Urbana Champaign']","['United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=uFbWHyTlPn,Fairness & Bias,Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach,"Differentially Private Stochastic Gradient Descent with Gradient Clipping (DPSGD-GC) is a powerful tool for training deep learning models using sensitive data, providing both a solid theoretical privacy guarantee and high efficiency. However, existing research has shown that DPSGD-GC only converges when using large clipping thresholds that are dependent on problem-specific parameters that are often unknown in practice. Therefore, DPSGD-GC suffers from degraded performance due to the {\it constant}  bias introduced by the clipping. In our work, we propose a new error-feedback (EF) DP algorithm as an alternative to DPSGD-GC, which offers a diminishing utility bound without inducing a constant clipping bias. More importantly, it allows for an arbitrary choice of clipping threshold that is independent of the problem. We establish an algorithm-specific DP analysis for our proposed algorithm, providing privacy guarantees based on R{\'e}nyi DP. And we demonstrate that under mild conditions, our algorithm can achieve the same utility bound as DPSGD without gradient clipping. Our empirical results on standard datasets show that the proposed algorithm achieves higher accuracies than DPSGD while maintaining the same level of DP guarantee.",[],[],"['Xinwei Zhang', 'Zhiqi Bu', 'Steven Wu', 'Mingyi Hong']","['University of Southern California', 'Amazon', 'School of Computer Science, Carnegie Mellon University', 'AGI, Amazon']","['United States', 'Japan', 'United States', 'Japan']"
https://openreview.net/forum?id=A0HKeKl4Nl,Transparency & Explainability,Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,"Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a `wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such ``wrapped capabilities'' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.",[],[],"['Samyak Jain', 'Robert Kirk', 'Ekdeep Singh Lubana', 'Robert P. Dick', 'Hidenori Tanaka', 'Tim Rocktäschel', 'Edward Grefenstette', 'David Krueger']","['Research, Microsoft', 'UK AI Safety Institute', 'Center for Brain Science, Harvard University, Harvard University', 'Electrical Engineering and Computer Science, University of Michigan', 'Harvard University, Harvard University', 'Google DeepMind', 'Google DeepMind', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal']","['United States', 'United Kingdom', 'United States', 'United States', 'United States', 'United States', 'United States', 'Canada']"
https://openreview.net/forum?id=pOoKI3ouv1,Transparency & Explainability,Robust agents learn causal world models,"It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.",[],[],"['Jonathan Richens', 'Tom Everitt']","['DeepMind', 'DeepMind']","['United States', 'United States']"
https://openreview.net/forum?id=fDaLmkdSKU,Security,Near-Optimal Solutions of Constrained Learning Problems,"With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with generalization guarantees) by formulating constrained learning problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we leverage the fact that non-convex, finite-dimensional constrained learning problems can be seen as parametrizations of convex, functional problems. Our results show that rich parametrizations effectively mitigate the issue of feasibility in dual methods, shedding light on prior empirical successes of dual learning. We illustrate our findings in fair learning tasks.",[],[],"['Juan Elenter', 'Luiz F. O. Chamon', 'Alejandro Ribeiro']","['University of Pennsylvania', 'Applied Mathematics, École Polytechnique', 'University of Pennsylvania']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=ixP76Y33y1,Security,The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images,"This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic ``label sharpness'' ($K_\mathcal{F}$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model's adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our $d_{data}$ formalism to the related metric of learned representation intrinsic dimension ($d_{repr}$), derive a generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$ serves as an upper bound for $d_{repr}$. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes. Our findings offer insights into the influence of intrinsic dataset properties on generalization, representation learning, and robustness in deep neural networks. *Code link: https://github.com/mazurowski-lab/intrinsic-properties*",[],[],"['Nicholas Konz', 'Maciej A Mazurowski']","['Duke University', 'Radiology/Biostatistics&Bioinformatics, Duke University']","['United States', 'United States']"
https://openreview.net/forum?id=i2Phucne30,Fairness & Bias,On Bias-Variance Alignment in Deep Models,"Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \emph{aligned} at a sample level, where squared bias is approximately \emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.",[],[],"['Lin Chen', 'Michal Lukasik', 'Wittawat Jitkrittum', 'Chong You', 'Sanjiv Kumar']","['', 'Google Research', 'Google Research', 'Google', 'Google']","['', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=GEcwtMk1uA,Security,Identifying the Risks of LM Agents with an LM-Emulated Sandbox,"Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.",[],[],"['Yangjun Ruan', 'Honghua Dong', 'Andrew Wang', 'Silviu Pitis', 'Yongchao Zhou', 'Jimmy Ba', 'Yann Dubois', 'Chris J. Maddison', 'Tatsunori Hashimoto']","['Computer Science, Stanford University', 'Department of Computer Science, University of Toronto', 'University of Toronto', 'University of Toronto', 'University of Toronto', 'Department of Computer Science, University of Toronto', 'OpenAI', 'Department of Computer Science, University of Toronto', 'Stanford University']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://openreview.net/forum?id=hTEGyKf0dZ,Security,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!","Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs.  (This paper contains red-teaming data and model-generated content that can be offensive in nature.)",[],[],"['Xiangyu Qi', 'Yi Zeng', 'Tinghao Xie', 'Pin-Yu Chen', 'Ruoxi Jia', 'Prateek Mittal', 'Peter Henderson']","['Princeton University', 'Virginia Tech', 'Electrical and Computer Engineering, Princeton University', 'International Business Machines', 'Virginia Tech', 'Princeton University', 'Princeton University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=HKgRwNhI9R,Fairness & Bias,Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics,"Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH Simulation. We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization. Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary. An implementation of our approach, as well as complete datasets and solver implementations, is available at https://github.com/orgs/tum-pbs/SFBC.",[],[],"['Rene Winchenbach', 'Nils Thuerey']","['Physics Based Simulations, Technische Universität München', 'Computer Science, Technical University Munich']","['Germany', 'Romania']"
https://openreview.net/forum?id=FIGXAxr9E4,Fairness & Bias,CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?,"We study data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP can inadvertently absorb stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns/unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems.",[],[],"['Ibrahim Alabdulmohsin', 'Xiao Wang', 'Andreas Peter Steiner', 'Priya Goyal', ""Alexander D'Amour"", 'Xiaohua Zhai']","['Google', 'Google DeepMind', 'Google DeepMind', 'Google', 'Google', 'Google DeepMind']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=FsVxd9CIlb,Transparency & Explainability,AttEXplore: Attribution for Explanation with model parameters eXploration,"Due to the real-world noise and human-added perturbations, attaining the trustworthiness of deep neural networks (DNNs) is a challenging task. Therefore, it becomes essential to offer explanations for the decisions made by these non-linear and complex parameterized models. Attribution methods are promising for this goal, yet its performance can be further improved. In this paper, for the first time, we present that the decision boundary exploration approaches of attribution are consistent with the process for transferable adversarial attacks. Specifically, the transferable adversarial attacks craft general adversarial samples from the source model, which is consistent with the generation of adversarial samples that can cross multiple decision boundaries in attribution. Utilizing this consistency, we introduce a novel attribution method via model parameter exploration. Furthermore, inspired by the capability of frequency exploration to investigate the model parameters, we provide enhanced explainability for DNNs by manipulating the input features based on frequency information to explore the decision boundaries of different models. Large-scale experiments demonstrate that our \textbf{A}ttribution method for \textbf{E}xplanation with model parameter e\textbf{X}ploration (AttEXplore) outperforms other state-of-the-art interpretability methods. Moreover, by employing other transferable attack techniques, AttEXplore can explore potential variations in attribution outcomes. Our code is available at: https://github.com/LMBTough/ATTEXPLORE.",[],[],"['Zhiyu Zhu', 'Huaming Chen', 'Jiayu Zhang', 'Xinyi Wang', 'Zhibo Jin', 'Jason Xue', 'Flora D. Salim']","['University of Sydney', '', 'Suzhou Yierqi', 'Faculty of Computer Science & Information Technology, Universiti Malaya', 'University of Technology Sydney', '', 'University of New South Wales']","['Australia', '', 'Australia', 'Australia', 'Australia', '', 'Australia']"
https://openreview.net/forum?id=FM5xfcaR2Y,Fairness & Bias,Post-hoc bias scoring is optimal for fair classification,"We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive attributes. We achieve competitive or better performance compared to both in-processing and post-processing methods across three datasets: Adult, COMPAS, and CelebA. Unlike most post-processing methods, we do not require access to sensitive attributes during the inference time.",[],[],"['Wenlong Chen', 'Yegor Klochkov', 'Yang Liu']","['Imperial College London', 'ByteDance, AI Lab', 'Computer Science and Engineering, University of California, Santa Cruz']","['United Kingdom', 'United States', 'United States']"
https://openreview.net/forum?id=hAYHmV1gM8,Privacy & Data Governance,FedWon: Triumphing Multi-domain Federated Learning Without Normalization,"Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, instead of label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated Learning Without Normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while existing normalization techniques possess their own limitations. In order to address these issues, FedWon eliminates the normalization layers in FL and reparameterizes convolution layers with scaled weight standardization. Through extensive experimentation on five datasets and five models, our comprehensive experimental results demonstrate that FedWon surpasses both FedAvg and the current state-of-the-art method (FedBN) across all experimental setups, achieving notable accuracy improvements of more than 10% in certain domains. Furthermore, FedWon is versatile for both cross-silo and cross-device FL, exhibiting robust domain generalization capability, showcasing strong performance even with a batch size as small as 1, thereby catering to resource-constrained devices. Additionally, FedWon can also effectively tackle the challenge of skewed label distribution.",[],[],"['Weiming Zhuang', 'Lingjuan Lyu']","['Sony Research', 'Sony Research, Sony']","['Japan', 'Japan']"
https://openreview.net/forum?id=F76bwRSLeK,Transparency & Explainability,Sparse Autoencoders Find Highly Interpretable Features in Language Models,"One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.",[],[],"['Robert Huben', 'Hoagy Cunningham', 'Logan Riggs Smith', 'Aidan Ewart', 'Lee Sharkey']","['Independent', 'Anthropic', 'Mississippi State University', 'University of Bristol', 'Apollo Research']","['Germany', '', 'United States', 'United Kingdom', 'Burkina Faso']"
https://openreview.net/forum?id=uIKZSStON3,Fairness & Bias,In-context Exploration-Exploitation for Reinforcement Learning,"In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large Transformer models. We address this challenge by introducing an In-context Exploration-Exploitation (ICEE) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a Transformer model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as Gaussian process biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous in-context learning method.",[],[],"['Zhenwen Dai', 'Federico Tomasi', 'Sina Ghiassian']","['Spotify', 'Spotify', 'Spotify']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=lNCnZwcH5Z,Transparency & Explainability,Non-negative Contrastive Learning,"Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these advantages enable NCL to outperform CL significantly on feature disentanglement, feature selection, as well as downstream classification tasks. At last, we show that NCL can be easily extended to other learning scenarios and benefit supervised learning as well. Code is available at https://github.com/PKU-ML/non_neg.",[],[],"['Yifei Wang', 'Qi Zhang', 'Yaoyu Guo', 'Yisen Wang']","['Massachusetts Institute of Technology', 'Peking University', 'Peking University', 'Peking University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=fCeUoDr9Tq,Fairness & Bias,Zero-Shot Robustification of Zero-Shot Models,"Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings---without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and show an average improvement of 15.98% over several zero-shot baselines. Additionally, we demonstrate that RoboShot is compatible with a variety of pretrained and language models and propose a way to further boost performance with a zero-shot adaptation variant.",[],[],"['Dyah Adila', 'Changho Shin', 'Linrong Cai', 'Frederic Sala']","['University of Wisconsin, Madison', 'University of Wisconsin, Madison', 'University of Wisconsin - Madison', 'Computer Sciences, University of Wisconsin, Madison']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=CYmF38ysDa,Transparency & Explainability,FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets,"Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations.",[],[],"['Seonghyeon Ye', 'Doyoung Kim', 'Sungdong Kim', 'Hyeonbin Hwang', 'Seungone Kim', 'Yongrae Jo', 'James Thorne', 'Juho Kim', 'Minjoon Seo']","['Korea Advanced Institute of Science and Technology', 'KAIST AI, KAIST', 'KAIST AI', 'Korea Advanced Institute of Science & Technology', 'Carnegie Mellon University', 'Korea Advanced Institute of Science & Technology', 'KAIST', 'Korea Advanced Institute of Science and Technology', 'Twelve Labs']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=kGteeZ18Ir,Fairness & Bias,Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs,"Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like ‘_You are Yoda. Explain the Theory of Relativity._’ While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs’ capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform _basic reasoning tasks_. Our study covers 24 reasoning datasets (spanning mathematics, law, medicine, morals, and more), 4 LLMs (2 versions of ChatGPT-3.5, GPT-4-Turbo, and Llama-2-70b-chat), and 19 diverse personas (e.g., ‘an Asian person’) spanning 5 socio-demographic groups: race, gender, religion, disability, and political affiliation. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked (‘_Are Black people less skilled at mathematics?_’), they manifest stereotypical and often erroneous presumptions when prompted to answer questions while adopting a persona. These can be observed as abstentions in the model’s response, e.g., ‘_As a Black person, I am unable to answer this question as it requires math knowledge_’, and generally result in a substantial drop in performance on reasoning tasks. Our experiments with ChatGPT-3.5 show that this bias is _ubiquitous_&mdash;80% of our personas demonstrate bias; it is _significant_&mdash;some datasets show performance drops of 70%+; and can be especially _harmful for certain groups_&mdash;some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all four LLMs exhibit persona-induced bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern as they do not always manifest as explicit abstentions, and can also be hard-to-avoid&mdash;we find de-biasing prompts to have minimal to no effect. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs&mdash;a trend on the rise&mdash;can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",[],[],"['Shashank Gupta', 'Vaishnavi Shrivastava', 'Ameet Deshpande', 'Ashwin Kalyan', 'Peter Clark', 'Ashish Sabharwal', 'Tushar Khot']","['Allen Institute for Artificial Intelligence', 'Stanford University', 'Princeton University', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Allen Institute for AI', 'Aristo, Allen Institute for Artificial Intelligence']","['', 'United States', 'United States', '', '', 'United States', '']"
https://openreview.net/forum?id=C61sk5LsK6,Fairness & Bias,InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning,"Data pruning aims to obtain lossless performances with less overall cost. A common approach is to filter out samples that make less contribution to the training. This could lead to gradient expectation bias compared to the original data. To solve this problem, we propose InfoBatch, a novel framework aiming to achieve lossless training acceleration by unbiased dynamic data pruning. Specifically, InfoBatch randomly prunes a portion of less informative samples based on the loss distribution and rescales the gradients of the remaining samples to approximate the original gradient. As a plug-and-play and architecture-agnostic framework, InfoBatch consistently obtains lossless training results on classification, semantic segmentation, vision pertaining, and instruction fine-tuning tasks. On CIFAR10/100, ImageNet- 1K, and ADE20K, InfoBatch losslessly saves 40% overall cost. For pertaining MAE and diffusion model, InfoBatch can respectively save 24.8% and 27% cost. For LLaMA instruction fine-tuning, combining InfoBatch and the recent coreset selection method (DQ) can achieve 10 times acceleration. Our results encourage more exploration on the data efficiency aspect of large model training. Code is publicly available at NUS-HPC-AI-Lab/InfoBatch.",[],[],"['Ziheng Qin', 'Kai Wang', 'Zangwei Zheng', 'Jianyang Gu', 'Xiangyu Peng', 'xu Zhao Pan', 'Daquan Zhou', 'Lei Shang', 'Baigui Sun', 'Xuansong Xie', 'Yang You']","['National University of Singapore', 'soc, national university of singaore, National University of Singapore', 'National University of Singapore', 'Computer Science and Engineering, The Ohio State University', 'National University of Singapore', 'Harbin Institute of Technology', 'NA, Bytedance', 'Alibaba Group', 'Alibaba Group', 'Tongyi, Alibaba Group', 'Computer Science, National University of Singapore']","['Singapore', 'United States', 'Singapore', 'Georgia', 'Singapore', 'China', '', 'China', 'China', 'China', 'Singapore']"
https://openreview.net/forum?id=C36v8541Ns,Security,The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing,"Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius in this context is a crucial indicator of the robustness of models. However how to design an efficient classifier with an associated certified radius? Randomized smoothing provides a promising framework by relying on noise injection into the inputs to obtain a smoothed and robust classifier. In this paper, we first show that the variance introduced by the Monte-Carlo sampling in the randomized smoothing procedure estimate closely interacts with two other important properties of the classifier, \textit{i.e.} its Lipschitz constant and margin.  More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. To increase the certified robust radius, we introduce a different way to convert logits to probability vectors for the base classifier to leverage the variance-margin trade-off. We leverage the use of Bernstein's concentration inequality along with enhanced Lipschitz bounds for randomized smoothing. Experimental results show a significant improvement in certified accuracy compared to current state-of-the-art methods. Our novel certification procedure allows us to use pre-trained models with randomized smoothing, effectively improving the current certification radius in a zero-shot manner.",[],[],"['Blaise Delattre', 'Alexandre Araujo', 'Quentin Barthélemy', 'Alexandre Allauzen']","['Informatique, , Université Paris-Dauphine (Paris IX)', 'New York University', 'Foxstream', 'Ecole Supérieure de Physique et de Chimie Industrielles']","['United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=9nsNyN0vox,Transparency & Explainability,Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks,"Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data. To endow LMs with genuine rule comprehension abilities, we propose ""Neural Comprehension"" - a framework that synergistically integrates compiled neural networks (CoNNs) into the standard transformer architecture. CoNNs are neural modules designed to explicitly encode rules through artificially generated attention weights. By incorporating CoNN modules, the Neural Comprehension framework enables LMs to accurately and robustly execute rule-intensive symbolic tasks. Extensive experiments demonstrate the superiority of our approach over existing techniques in terms of length generalization, efficiency, and interpretability for symbolic operations. Furthermore, it can be applied to LMs across different model scales, outperforming tool-calling methods in arithmetic reasoning tasks while maintaining superior inference efficiency. Our work highlights the potential of seamlessly unifying explicit rule learning via CoNNs and implicit pattern learning in LMs, paving the way for true symbolic comprehension capabilities. The code is released at: \url{https://github.com/wengsyx/Neural-Comprehension}.",[],[],"['Yixuan Weng', 'Minjun Zhu', 'Fei Xia', 'Bin Li', 'Shizhu He', 'Kang Liu', 'Jun Zhao']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Westlake University', 'Institute of automation, Chinese Academy of Sciences', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=CIqjp9yTDq,Fairness & Bias,Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise,"Heavy-ball momentum with decaying learning rates is widely used with SGD for optimizing deep learning models. In contrast to its empirical popularity, the understanding of its theoretical property is still quite limited, especially under the standard anisotropic gradient noise condition for quadratic regression problems. Although it is widely conjectured that heavy-ball momentum method can provide accelerated convergence and should work well in large batch settings, there is no rigorous theoretical analysis. In this paper, we fill this theoretical gap by establishing a non-asymptotic convergence bound for stochastic heavy-ball methods with step decay scheduler on quadratic objectives, under the anisotropic gradient noise condition. As a direct implication, we show that heavy-ball momentum can provide $\tilde{\mathcal{O}}(\sqrt{\kappa})$ accelerated convergence of the bias term of SGD while still achieving near-optimal convergence rate with respect to the stochastic variance term. The combined effect implies an overall convergence rate within log factors from the statistical minimax rate. This means SGD with heavy-ball momentum is useful in the large-batch settings such as distributed machine learning or federated learning, where a smaller number of iterations can significantly reduce the number of communication rounds, leading to acceleration in practice.",[],[],"['Rui Pan', 'Yuxing Liu', 'Xiaoyu Wang', 'Tong Zhang']","['University of Illinois at Urbana-Champaign', 'Computer Science, University of Illinois at Urbana-Champaign', 'Hong Kong University of Science and Technology', 'UIUC']","['United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=7D9X2cFnt1,Fairness & Bias,Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning,"Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting.  To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prototypes used in a novel asymmetric cross entropy loss which effectively balances prototype rehearsal with data from new tasks. Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to learn new tasks by maintaining model plasticity and significantly outperform the state-of-the-art.",[],[],"['Simone Magistri', 'Tomaso Trinci', 'Albin Soutif', 'Joost van de Weijer', 'Andrew D. Bagdanov']","['Information Engineering, University of Florence', 'University of Florence', 'Independant researcher', 'Universitat Autónoma de Barcelona', 'Computer Visione Center, Università degli Studi di Firenze']","['Italy', 'Italy', 'Germany', 'Spain', 'Switzerland']"
https://openreview.net/forum?id=GgEAdqYPNA,Fairness & Bias,Investigating the Benefits of Projection Head for Representation Learning,"An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of  training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized representations. We theoretically characterize scenarios where such representations are more beneficial, highlighting the intricate interplay between data augmentation and input features. Additionally, we demonstrate that introducing non-linearity into the network allows lower layers to learn features that are completely absent in higher layers. Finally, we show how this mechanism improves the robustness in supervised contrastive learning and supervised learning. We empirically validate our results through various experiments on CIFAR-10/100, UrbanCars and shifted versions of ImageNet. We also introduce a potential alternative to projection head, which offers a more interpretable and controllable design.",[],[],"['Yihao Xue', 'Eric Gan', 'Jiayi Ni', 'Siddharth Joshi', 'Baharan Mirzasoleiman']","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles', 'Computer Science, University of California, Los Angeles']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=s56xikpD92,Security,BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection,"We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract **backdoor functionality** of a given backdoored model to a *backdoor expert* model. The approach is straightforward --- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model~(dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, **BaDExpert** (**Ba**ckdoor Input **D**etection with Backdoor **Expert**), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer). Our code is integrated into our research toolbox: [https://github.com/vtu81/backdoor-toolbox](https://github.com/vtu81/backdoor-toolbox).",[],[],"['Tinghao Xie', 'Xiangyu Qi', 'Ping He', 'Yiming Li', 'Jiachen T. Wang', 'Prateek Mittal']","['Electrical and Computer Engineering, Princeton University', 'Princeton University', '', 'Nanyang Technological University', 'Electrical and Computer Engineering, Princeton University', 'Princeton University']","['United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=h57gkDO2Yg,Fairness & Bias,Self-Supervised Dataset Distillation for Transfer Learning,"Dataset distillation aims to optimize a small set so that a model trained on the set achieves performance similar to that of a model trained on the full dataset. While many supervised methods have achieved remarkable success in distilling a large dataset into a small set of representative samples, however,  they are not designed to produce a distilled dataset that can be effectively used to facilitate self-supervised pre-training. To this end, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning (SSL). We first prove that a gradient of synthetic samples with respect to a SSL objective in naive bilevel optimization is \textit{biased} due to the randomness originating from data augmentations or masking for inner optimization. To address this issue, we propose to minimize the mean squared error (MSE) between a model's representations of the synthetic examples and their corresponding learnable target feature representations for the inner objective, which does not introduce any randomness. Our primary motivation is that the model obtained by the proposed inner optimization can mimic the \textit{self-supervised target model}. To achieve this, we also introduce the MSE between representations of the inner model and the self-supervised target model on the original full dataset for outer optimization. We empirically validate the effectiveness of our method on transfer learning. Our code is available at  https://github.com/db-Lee/selfsup_dd",[],[],"['Dong Bok Lee', 'Seanie Lee', 'Joonho Ko', 'Kenji Kawaguchi', 'Juho Lee', 'Sung Ju Hwang']","['AI graduate school, Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'KAIST', 'National University of Singapore', 'Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science & Technology', '']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'Singapore', 'South Korea']"
https://openreview.net/forum?id=BEyEziZ4R6,Fairness & Bias,DP-SGD Without Clipping: The Lipschitz Neural Network Way,"State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on Lipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees.  Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to maximize the gradient-to-noise ratio for fixed privacy guarantees. To facilitate the application of Lipschitz networks and foster robust and certifiable learning under privacy guarantees, we provide a Python package that implements building blocks allowing the construction and private training of such networks.",[],[],"['Louis Béthune', 'Thomas Massena', 'Thibaut Boissin', 'Aurélien Bellet', 'Franck Mamalet', 'Yannick Prudent', 'Corentin Friedrich', 'Mathieu Serrurier', 'David Vigouroux']","['Machine Learning Research, Apple', 'IRIT / SNCF DTIPG', 'IRT Saint exupéry', 'INRIA', 'IRT Saint Exupery', 'IRT Saint-Exupéry', 'IRT Saint Exupéry', 'Irit, university Jean Jaurès', '']","['Brazil', '', 'Brazil', '', 'Brazil', 'Brazil', 'Brazil', 'Brazil', '']"
https://openreview.net/forum?id=9RNfX0ah0K,Security,Leave-one-out Distinguishability in Machine Learning,"We introduce an analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD).  This is key to measuring data **memorization** and information **leakage** as well as the **influence** of training data points in machine learning. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of leakage using membership inference attacks. Our analytical framework enables us to investigate the causes of leakage and where the leakage is high.  For example, we analyze the influence of activation functions, on data memorization.  Additionally, our method allows us to identify queries that disclose the most information about the training data in the leave-one-out setting.  We illustrate how optimal queries can be used for accurate **reconstruction** of training data.",[],[],"['Jiayuan Ye', 'Anastasia Borovykh', 'Soufiane Hayou', 'Reza Shokri']","['National University of Singapore', '', 'Simons Institute, University of California, Berkeley', '']","['Singapore', '', 'United States', '']"
https://openreview.net/forum?id=tm8s3696Ox,Security,Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting,"One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial attack manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodically achieves high-quality data and ensemble models. Extensive experiments demonstrate that Co-Boosting can substantially outperform existing baselines under various settings. Moreover, Co-Boosting eliminates the need for adjustments to the client's local training, requires no additional data or model transmission, and allows client models to have heterogeneous architectures.",[],[],"['Rong Dai', 'Yonggang Zhang', 'Ang Li', 'Tongliang Liu', 'Xun Yang', 'Bo Han']","['University of Science and Technology of China', 'Hong Kong Baptist University', 'University of Maryland, College Park', 'University of Sydney', 'Department of Electronic Engineering and Information Science, University of Science and Technology of China', 'Department of Computer Science, HKBU']","['China', 'Hong Kong', 'United States', 'Australia', 'China', '']"
https://openreview.net/forum?id=awWpHnEJDw,Fairness & Bias,The Hidden Language of Diffusion Models,"Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual prompt. However, the internal representations learned by these models remain an enigma. In this work, we present Conceptor, a novel method to interpret the internal representation of a textual concept by a diffusion model. This interpretation is obtained by decomposing the concept into a small set of human-interpretable textual elements. Applied over the state-of-the-art Stable Diffusion model, Conceptor reveals non-trivial structures in the representations of concepts. For example, we find surprising visual connections between concepts, that transcend their textual semantics. We additionally discover concepts that rely on mixtures of exemplars, biases, renowned artistic styles, or a simultaneous fusion of multiple meanings of the concept. Through a large battery of experiments, we demonstrate Conceptor's ability to provide meaningful, robust, and faithful decompositions for a wide variety of abstract, concrete, and complex textual concepts, while allowing to naturally connect each decomposition element to its corresponding visual impact on the generated images.",[],[],"['Hila Chefer', 'Oran Lang', 'Mor Geva', 'Volodymyr Polosukhin', 'Assaf Shocher', 'michal Irani', 'Inbar Mosseri', 'Lior Wolf']","['Tel Aviv University', 'Google', 'Tel Aviv University', 'Computer Science Departmen, Technion-Israel Institute of Technology', 'Research, NVIDIA', 'Weizmann Institute of SCience', 'Google', 'Tel Aviv University']","['Israel', '', 'Israel', 'Israel', '', 'Israel', '', 'Israel']"
https://openreview.net/forum?id=AcoXPIPh4A,Fairness & Bias,Risk Bounds of Accelerated SGD for Overparameterized Linear Regression,"Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. We establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when our analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result.",[],[],"['Xuheng Li', 'Yihe Deng', 'Jingfeng Wu', 'Dongruo Zhou', 'Quanquan Gu']","['University of California, Los Angeles', 'Computer Science, University of California, Los Angeles', 'University of California, Berkeley', 'Indiana University', 'ByteDance Inc.']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=AJBkfwXh3u,Transparency & Explainability,Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks,"Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity in the research of dynamic graphs, but are limited by the low transparency, such that human-understandable insights can hardly be drawn from their predictions. Although a number of existing research have been devoted to investigating the interpretability of graph neural networks (GNNs), achieving the interpretability of DyGNNs is pivotally challenging due to the complex spatial-temporal correlations in dynamic graphs. To this end, we propose an innovative causality-inspired generative model based on structural causal model (SCM), which explores the underlying philosophies of DyGNN predictions by identifying the trivial, static, and dynamic causal relationships. To reach this goal, two critical tasks need to be accomplished including (1) disentangling the complex causal relationships, and (2) fitting the spatial-temporal explanations of DyGNNs in the SCM architecture. To tackle these challenges, the proposed method incorporates a contrastive learning module to disentangle trivial and causal relationships, and a dynamic correlating module to disentangle dynamic and static causal relationships, respectively. A dynamic VGAE-based framework is further developed, which generates causal-and-dynamic masks for spatial interpretability, and recognizes dynamic relationships along the time horizon through causal invention for temporal interpretability. Comprehensive experiments have been conducted on both synthetic and real-world datasets, where our approach yields substantial improvements, thereby demonstrating significant superiority.",[],[],"['Kesen Zhao', 'Liang Zhang']","['City University', 'Shenzhen Research Institute of Big Data']","['United States', '']"
https://openreview.net/forum?id=vW1SkPl4kp,Security,Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback,"Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we present a novel risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk (CVaR) objective under both linear and general function approximations, enriched by human feedback. These new formulations provide a principled way to guarantee safety in each decision making step throughout the control process. Moreover, integrating human feedback into risk-sensitive RL framework bridges the gap between algorithmic decision-making and human participation, allowing us to also guarantee safety for human-in-the-loop systems. We propose provably sample-efficient algorithms for this Iterated CVaR RL and provide rigorous theoretical analysis. Furthermore, we establish a matching lower bound to corroborate the optimality of our algorithms in a linear context.",[],[],"['Yu Chen', 'Yihan Du', 'Pihe Hu', 'Siwei Wang', 'Desheng Wu', 'Longbo Huang']","['Institution for Interdisciplinary Information Sciences (IIIS), Tsinghua University', 'University of Illinois at Urbana-Champaign', 'Tsinghua University, Tsinghua University', 'Microsoft', 'University of the Chinese Academy of Sciences', 'IIIS, Tsinghua University, Tsinghua University']","['China', 'China', 'China', '', 'China', 'China']"
https://openreview.net/forum?id=PCm1oT8pZI,Privacy & Data Governance,Safe and Robust Watermark Injection with a Single OoD Image,"Training a high-performance deep neural network requires large amounts of data and computational resources.  Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial.  A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning.  To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification.  The independence of training data makes it agnostic to third-party promises of IP security.  We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction.  Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.",[],[],"['Shuyang Yu', 'Junyuan Hong', 'Haobo Zhang', 'Haotao Wang', 'Zhangyang Wang', 'Jiayu Zhou']","['Michigan State University', 'University of Texas at Austin', '', 'Qualcomm Inc, QualComm', 'University of Texas at Austin', 'University of Michigan - Ann Arbor']","['United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Ifz3IgsEPX,Privacy & Data Governance,DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer,"Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local LLM and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning (DP-OPT) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by LLMs themselves can be transferred without compromising performance significantly. To ensure that the prompts do not leak private information, we introduce the first private prompt generation mechanism, by a differentially-private (DP) ensemble of in-context learning with private demonstrations.  With DP-OPT, generating privacy-preserving prompts by Vicuna-7b can yield competitive performance compared to non-private in-context learning on GPT3.5 or local private prompt tuning. Codes are available at https://github.com/VITA-Group/DP-OPT.",[],[],"['Junyuan Hong', 'Jiachen T. Wang', 'Chenhui Zhang', 'Zhangheng LI', 'Bo Li', 'Zhangyang Wang']","['University of Texas at Austin', 'Electrical and Computer Engineering, Princeton University', 'Institute for Data, Systems, and Society, Massachusetts Institute of Technology', 'ECE, University of Texas at Austin', 'CS, University of Illinois, Urbana Champaign', 'University of Texas at Austin']","['United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=buC4E91xZE,Privacy & Data Governance,AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection,"Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/ organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.",[],[],"['Qihang Zhou', 'Guansong Pang', 'Yu Tian', 'Shibo He', 'Jiming Chen']","['Zhejiang University', 'Singapore Management University', 'Harvard University', 'Zhejiang University', 'Dept. of Control, Zhejiang University']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=8F6bws5JBy,Fairness & Bias,Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication,"Tabular data in the wild are frequently afflicted with class-imbalance, biasing machine learning model predictions towards major classes. A data-centric solution to this problem is oversampling - where the classes are balanced by adding synthetic minority samples via generative methods. However, although tabular generative models are capable of generating synthetic samples under a balanced distribution, their integrity suffers when the number of minority samples is low. To this end, pre-trained generative language models with rich prior knowledge are a fitting candidate for the task at hand. Nevertheless, an oversampling strategy tailored for tabular data that utilizes the extensive capabilities of such language models is yet to emerge. In this paper, we propose a novel oversampling framework for tabular data to channel the abilities of generative language models. By leveraging its conditional sampling capabilities, we synthesize minority samples by progressively masking the important features of the majority class samples and imputing them towards the minority distribution. To reduce the inclusion of imperfectly converted samples, we utilize the power of the language model itself to self-authenticate the labels of the samples generated by itself, sifting out ill-converted samples. Extensive experiments on a variety of datasets and imbalance ratios reveal that the proposed method successfully generates reliable minority samples to boost the performance of machine learning classifiers, even under heavy imbalance ratios.",[],[],"['June Yong Yang', 'Geondo Park', 'Joowon Kim', 'Hyeongwon Jang', 'Eunho Yang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=2Oiee202rd,Transparency & Explainability,PerceptionCLIP: Visual Classification by Inferring and Conditioning on Contexts,"Vision-language models like CLIP are widely used in zero-shot image classification due to their ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better performance is still an open question. This paper draws inspiration from the human visual perception process: when classifying an object, humans first infer contextual attributes (e.g., background and orientation) which help separate the foreground object from the background, and then classify the object based on this information. Inspired by it, we observe that providing CLIP with contextual attributes improves zero-shot image classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot classification method PerceptionCLIP. Given an image, it first infers contextual attributes (e.g., background) and then performs object classification conditioning on them. Our experiments show that PerceptionCLIP achieves better generalization, group robustness, and interpretability.",[],[],"['Bang An', 'Sicheng Zhu', 'Michael-Andrei Panaitescu-Liess', 'Chaithanya Kumar Mummadi', 'Furong Huang']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park', 'Bosch Center for Artificial Intelligence', 'Computer Science, University of Maryland']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=7TOs9gjAg1,Fairness & Bias,Removing Biases from Molecular Representations via Information Maximization,"High-throughput drug screening -- using cell imaging or gene expression measurements as readouts of drug effect -- is a critical tool in biotechnology to assess and understand the relationship between the chemical structure and biological activity of a drug. Since large-scale screens have to be divided into multiple experiments, a key difficulty is dealing with batch effects, which can introduce systematic errors and non-biological associations in the data. We propose InfoCORE, an Information maximization approach for COnfounder REmoval, to effectively deal with batch effects and obtain refined molecular representations. InfoCORE establishes a variational lower bound on the conditional mutual information of the latent representations given a batch identifier. It adaptively reweights samples to equalize their implied batch distribution. Extensive experiments on drug screening data reveal InfoCORE's superior performance in a multitude of tasks including molecular property prediction and molecule-phenotype retrieval. Additionally, we show results for how InfoCORE offers a versatile framework and resolves general distribution shifts and issues of data fairness by minimizing correlation with spurious features or removing sensitive attributes.",[],[],"['Chenyu Wang', 'Sharut Gupta', 'Caroline Uhler', 'Tommi S. Jaakkola']","['', 'Massachusetts Institute of Technology', 'Electrical Engineering & Computer Science, Massachusetts Institute of Technology', '']","['', 'United States', 'United States', '']"
https://openreview.net/forum?id=9w3iw8wDuE,Fairness & Bias,Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors,"Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a confidence metric that aims to determine which samples have a lower likelihood of causing error. Through experimental studies, however, we observed the unreliability of entropy as a confidence metric for TTA under biased scenarios and theoretically revealed that it stems from the neglect of the influence of latent disentangled factors of data on predictions. Building upon these findings, we introduce a novel TTA method named Destroy Your Object (DeYO), which leverages a newly proposed confidence metric named Pseudo-Label Probability Difference (PLPD). PLPD quantifies the influence of the shape of an object on prediction by measuring the difference between predictions before and after applying an object-destructive transformation. DeYO consists of sample selection and sample weighting, which employ entropy and PLPD concurrently. For robust adaptation, DeYO prioritizes samples that dominantly incorporate shape information when making predictions. Our extensive experiments demonstrate the consistent superiority of DeYO over baseline methods across various scenarios, including biased and wild. Project page is publicly available at https://whitesnowdrop.github.io/DeYO/.",[],[],"['Jonghyun Lee', 'Dahuin Jung', 'Saehyung Lee', 'Junsung Park', 'Juhyeon Shin', 'Uiwon Hwang', 'Sungroh Yoon']","['Seoul National University', 'Soongsil University', 'Seoul National University', 'Seoul National University', 'Seoul National University', 'Yonsei University - Mirae Campus', 'ECE and AI, Seoul National University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=fpoAYV6Wsk,Transparency & Explainability,Circuit Component Reuse Across Tasks in Transformer Language Models,"Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in (Wang, 2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to ‘repair’ the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.",[],[],"['Jack Merullo', 'Carsten Eickhoff', 'Ellie Pavlick']","['Computer Science, Brown University', 'Eberhard-Karls-Universität Tübingen', 'Brown University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=FWJAmwE0xH,Fairness & Bias,Neural-Symbolic Recursive Machine for Systematic Generalization,"Current learning models often struggle with human-like systematic generalization, particularly in learning compositional rules from limited data and extrapolating them to novel combinations. We introduce the Neural-Symbolic Recursive Ma- chine ( NSR), whose core is a Grounded Symbol System ( GSS), allowing for the emergence of combinatorial syntax and semantics directly from training data. The NSR employs a modular design that integrates neural perception, syntactic parsing, and semantic reasoning. These components are synergistically trained through a novel deduction-abduction algorithm. Our findings demonstrate that NSR’s design, imbued with the inductive biases of equivariance and compositionality, grants it the expressiveness to adeptly handle diverse sequence-to-sequence tasks and achieve unparalleled systematic generalization. We evaluate NSR’s efficacy across four challenging benchmarks designed to probe systematic generalization capabilities: SCAN for semantic parsing, PCFG for string manipulation, HINT for arithmetic reasoning, and a compositional machine translation task. The results affirm NSR ’s superiority over contemporary neural and hybrid models in terms of generalization and transferability.",[],[],"['Qing Li', 'Yixin Zhu', 'Yitao Liang', 'Ying Nian Wu', 'Song-Chun Zhu', 'Siyuan Huang']","['Beijing Institute for General Artificial Intelligence (BIGAI)', 'Institute for Artificial Intelligence, Peking University', 'Peking University', 'Statistics, UCLA', 'Beijing Institute for General Artificial Intelligence', 'Beijing Institute for General Artificial Intelligence']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=XsHqr9dEGH,Fairness & Bias,Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking,"Recent work by Power et al. (2022) highlighted a surprising ""grokking"" phenomenon in learning arithmetic tasks: a neural net first ""memorizes"" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.",[],[],"['Kaifeng Lyu', 'Jikai Jin', 'Zhiyuan Li', 'Simon Shaolei Du', 'Jason D. Lee', 'Wei Hu']","['Simons Institute, University of California, Berkeley', 'ICME, Stanford University', 'Toyota Technological Institute at Chicago', 'University of Washington', 'Princeton University', 'University of Michigan - Ann Arbor']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=fsW7wJGLBd,Security,Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game,"While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to *prompt injection attacks*: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over 563,000 prompt injection attacks and 118,000 prompt-based ""defenses"" against prompt injection, all created by players of an online game called Tensor Trust. To the best of our knowledge, this is the first dataset that includes both human-generated attacks and defenses for instruction-following LLMs. The attacks in our dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. We also use the dataset to create a benchmark for resistance to two types of prompt injection, which we refer to as *prompt extraction* and *prompt hijacking*. Our benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, we show that some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game. We release data and code at [tensortrust.ai/paper](https://tensortrust.ai/paper)",[],[],"['Sam Toyer', 'Olivia Watkins', 'Ethan Adrian Mendes', 'Justin Svegliato', 'Luke Bailey', 'Tiffany Wang', 'Isaac Ong', 'Karim Elmaaroufi', 'Pieter Abbeel', 'Trevor Darrell', 'Alan Ritter', 'Stuart Russell']","['University of California Berkeley', 'University of California Berkeley', 'Georgia Institute of Technology', 'University of California, Berkeley', 'Computer Science, Stanford University', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'Amazon', 'EECS, Electrical Engineering & Computer Science Department', 'Georgia Institute of Technology', 'EECS, University of California Berkeley']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=6ARlSgun7J,Fairness & Bias,Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction,"Extreme Classification (XC) architectures, which utilize a massive One-vs-All (OvA) classifier layer at the output, have demonstrated remarkable performance on problems with large label sets. Nonetheless, these architectures falter on tail labels with few representative samples. This phenomenon has been attributed to factors such as classifier over-fitting and missing label bias, and solutions involving regularization and loss re-calibration have been developed. This paper explores the impact of label variance - a previously unexamined factor - on the tail performance in extreme classifiers. It also develops a method to systematically reduce label variance in XC by transferring the knowledge from a specialized tail-robust teacher model to the OvA classifiers. For this purpose, it proposes a principled knowledge distillation framework, LEVER, which enhances the tail performance in extreme classifiers with formal guarantees on generalization. Comprehensive experiments are conducted on a diverse set of XC datasets, demonstrating that LEVER can enhance tail performance by around 5\% and 6\% points in PSP and coverage metrics, respectively, when integrated with leading extreme classifiers. Moreover, it establishes a new state-of-the-art when added to the top-performing Renee classifier. Extensive ablations and analyses substantiate the efficacy of our design choices. Another significant contribution is the release of two new XC datasets that are different from and more challenging than the available benchmark datasets, thereby encouraging more rigorous algorithmic evaluation in the future. Code for LEVER is available at: aka.ms/lever.",[],[],"['Anirudh Buvanesh', 'Rahul Chand', 'Jatin Prakash', 'Bhawna Paliwal', 'Mudit Dhawan', 'Neelabh Madan', 'Deepesh Hada', 'Vidit Jain', 'SONU MEHTA', 'Yashoteja Prabhu', 'Manish Gupta', 'Ramachandran Ramjee', 'Manik Varma']","['Computer Science, Mila - Quebec Artificial Intelligence Institute', 'Microsoft', 'Computer Science, New York University', 'Microsoft Research', 'Machine Learning Department, Carnegie Mellon University', 'New York University', 'Microsoft', 'Microsoft Research, India', 'Indian Institute of Technology Delhi', '', 'Bing, Microsoft', 'Microsoft', 'Research, Research, Microsoft']","['Canada', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=cVUOnF7iVp,Privacy & Data Governance,Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model,"In this paper, we revisit  the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive LDP (NLDP) algorithms exist. To address these issues,  we  first consider the problem in the $\epsilon$ non-interactive LDP model and provide a lower bound of $\Omega(\frac{\sqrt{dk\log d}}{\sqrt{n}\epsilon})$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space.  We propose an innovative NLDP algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. Our algorithm achieves an upper bound of $\tilde{O}({\frac{d\sqrt{k}}{\sqrt{n}\epsilon}})$ for the estimation error when the data is sub-Gaussian, which can be further improved by a factor of  $O(\sqrt{d})$ if the server has additional public but unlabeled data.  For the sequentially interactive LDP model, we show a similar lower bound of $\Omega({\frac{\sqrt{dk}}{\sqrt{n}\epsilon}})$. As for the upper bound, we rectify a previous method and show that it is possible to achieve a bound of $\tilde{O}(\frac{k\sqrt{d}}{\sqrt{n}\epsilon})$. Our findings reveal fundamental differences between the non-private case, central DP model, and local DP model in the sparse linear regression problem.",[],[],"['Liyang Zhu', 'Meng Ding', 'Vaneet Aggarwal', 'Jinhui Xu', 'Di Wang']","['Duke University', 'State University of New York at Buffalo', 'Purdue University', 'Computer Science and Engineering, University at Buffalo, State University of New York', '']","['United States', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=5RUf9nEdyC,Fairness & Bias,TEDDY: Trimming Edges with Degree-based Discrimination Strategy,"Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce **TEDDY**, a one-shot edge sparsification framework that leverages structural information by incorporating *edge-degree* statistics. Following the edge sparsification, we encourage the parameter sparsity during training via simple projected gradient descent on the $\ell_0$ ball. Given the target sparsity levels for both the graph structure and the model parameters, our TEDDY facilitates efficient and rapid realization of GLT within a *single* training. Remarkably, our experimental results demonstrate that TEDDY significantly surpasses conventional iterative approaches in generalization, even when conducting one-shot sparsification that solely utilizes graph structures, without taking feature information into account.",[],[],"['Hyunjin Seo', 'Jihun Yun', 'Eunho Yang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology']","['South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=5bNYf0CqxY,Fairness & Bias,Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks,"The spiking neural networks are inspired by the biological neurons that employ binary spikes to propagate information in the neural network. It has garnered considerable attention as the next-generation neural network, as the spiking activity simplifies the computation burden of the network to a large extent and is known for its low energy deployment enabled by specialized neuromorphic hardware. One popular technique to feed a static image to such a network is rate encoding, where each pixel is encoded into random binary spikes, following a Bernoulli distribution that uses the pixel intensity as bias. By establishing a novel connection between rate-encoding and randomized smoothing, we give the first provable robustness guarantee for spiking neural networks against adversarial perturbation of inputs bounded under $l_1$-norm. We introduce novel adversarial training algorithms for rate-encoded models that significantly improve the state-of-the-art empirical robust accuracy result. Experimental validation of the method is performed across various static image datasets, including CIFAR-10, CIFAR-100 and ImageNet-100. The code is available at \url{https://github.com/BhaskarMukhoty/CertifiedSNN}.",[],[],"['Bhaskar Mukhoty', 'Hilal AlQuabeh', 'Giulia De Masi', 'Huan Xiong', 'Bin Gu']","['Mohamed bin Zayed University of Artificial Intelligence', 'NLP, Mohamed bin Zayed University of Artificial Intelligence', 'Technology Innovation Institute', 'Harbin Institute of Technology', 'machine learning, Mohamed bin Zayed University of Artificial Intelligence']","['United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates']"
https://openreview.net/forum?id=5Dwqu5urzs,Security,Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings,"This paper proposes the Phy-DRL: a physics-regulated deep reinforcement learning (DRL) framework for safety-critical autonomous systems. The Phy-DRL has three distinguished invariant-embedding designs: i) residual action policy (i.e., integrating data-driven-DRL action policy and physics-model-based action policy), ii) automatically constructed safety-embedded reward, and iii) physics-model-guided neural network (NN) editing, including link editing and activation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically provable safety guarantee and 2) strict compliance of critic and actor networks with physics knowledge about the action-value function and action policy. Finally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot. The experiments validate our theoretical results and demonstrate that Phy-DRL features guaranteed safety compared to purely data-driven DRL and solely model-based design while offering remarkably fewer learning parameters and fast training towards safety guarantee.",[],[],"['Hongpeng Cao', 'Yanbing Mao', 'Lui Sha', 'Marco Caccamo']","['School of engineering and design, Technische Universität München', 'Engineering Technology  , Wayne State University', 'Department of Computer Science', 'Technische Universität München']","['Germany', 'Germany', '', 'Germany']"
https://openreview.net/forum?id=bvjcMvMn7B,Fairness & Bias,Structural Fairness-aware Active Learning for Graph Neural Networks,"Graph Neural Networks (GNNs) have seen significant achievements in semi-supervised node classification. Yet, their efficacy often hinges on access to high-quality labeled node samples, which may not always be available in real-world scenarios. While active learning is commonly employed across various domains to pinpoint and label high-quality samples based on data features, graph data present unique challenges due to their intrinsic structures that render nodes non-i.i.d. Furthermore, biases emerge from the positioning of labeled nodes; for instance, nodes closer to the labeled counterparts often yield better performance. To better leverage graph structure and mitigate structural bias in active learning, we present a unified optimization framework (SCARCE), which is also easily incorporated with node features.  Extensive experiments demonstrate that the proposed method not only improves the GNNs performance but also paves the way for more fair results.",[],[],"['Haoyu Han', 'Xiaorui Liu', 'Li Ma', 'MohamadAli Torkamani', 'Hui Liu', 'Jiliang Tang', 'Makoto Yamada']","['MSE, Michigan State University', 'Computer Science, North Carolina State University', '', 'AWS AI', 'Michigan State University', 'Michigan State University', 'Okinawa Institute of Science and Technology (OIST)']","['United States', 'United States', '', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=4eJDMjYZZG,Security,Language Model Detectors Are Easily Optimized Against,"The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has highlighted the fragility of these systems. In this paper, we demonstrate a data-efficient attack that fine-tunes language models to confuse existing detectors, leveraging recent developments in reinforcement learning of language models. We use the `human-ness' score (often just a log probability) of various open-source and commercial detectors as a reward function for reinforcement learning, subject to a KL-divergence constraint that the resulting model does not differ significantly from the original. For a 7B parameter Llama-2 model, fine-tuning for under a day reduces the AUROC of the OpenAI RoBERTa-Large detector from 0.84 to 0.63, while perplexity on OpenWebText increases from 8.7 to only 9.0; with a larger perplexity budget, we can drive AUROC to 0.30 (worse than random). Similar to traditional adversarial attacks, we find that this increase in 'detector evasion' generalizes to other detectors not used during training. In light of our empirical results, we advise against continued reliance on LLM-generated text detectors. Models, datasets, and selected experiment code will be released at https://github.com/charlottttee/llm-detector-evasion.",[],[],"['Charlotte Nicks', 'Eric Mitchell', 'Rafael Rafailov', 'Archit Sharma', 'Christopher D Manning', 'Chelsea Finn', 'Stefano Ermon']","['Stanford University', 'Computer Science, Stanford University', 'Google', 'Computer Science Department, Stanford University', 'Physical Intelligence', 'Computer Science, Stanford University']","['United States', 'United States', '', 'United States', '', 'United States']"
https://openreview.net/forum?id=4UiLqimGm5,Fairness & Bias,Coordinate-Aware Modulation for Neural Fields,"Neural fields, mapping low-dimensional input coordinates to corresponding signals, have shown promising results in representing various signals. Numerous methodologies have been proposed, and techniques employing MLPs and grid representations have achieved substantial success. MLPs allow compact and high expressibility, yet often suffer from spectral bias and slow convergence speed. On the other hand, methods using grids are free from spectral bias and achieve fast training speed, however, at the expense of high spatial complexity. In this work, we propose a novel way for exploiting both MLPs and grid representations in neural fields. Unlike the prevalent methods that combine them sequentially (extract features from the grids first and feed them to the MLP), we inject spectral bias-free grid representations into the intermediate features in the MLP. More specifically, we suggest a Coordinate-Aware Modulation (CAM), which modulates the intermediate features using scale and shift parameters extracted from the grid representations. This can maintain the strengths of MLPs while mitigating any remaining potential biases, facilitating the rapid learning of high-frequency components. In addition, we empirically found that the feature normalizations, which have not been successful in neural filed literature, proved to be effective when applied in conjunction with the proposed CAM. Experimental results demonstrate that CAM enhances the performance of neural representation and improves learning stability across a range of signals. Especially in the novel view synthesis task, we achieved state-of-the-art performance with the least number of parameters and fast training speed for dynamic scenes and the best performance under 1MB memory for static scenes. CAM also outperforms the best-performing video compression methods using neural fields by a large margin. Our project page is available at https://maincold2.github.io/cam/.",[],[],"['Joo Chan Lee', 'Daniel Rho', 'Seungtae Nam', 'Jong Hwan Ko', 'Eunbyung Park']","['Sungkyunkwan University', 'Department of Computer Science, University of North Carolina at Chapel Hill', 'Sungkyunkwan University', 'Sungkyunkwan University', 'Sungkyunkwan University']","['South Korea', 'United States', 'South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=3QkzYBSWqL,Security,Universal Backdoor Attacks,"Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and reused many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a naïve composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient,   _universal_ data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a slight increase in poison samples. Our idea is to generate triggers with salient characteristics that the model can learn. The triggers we craft exploit a phenomenon we call _inter-class poison transferability_, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. We demonstrate the effectiveness and robustness of our universal backdoor attacks by controlling models with up to 6,000 classes while poisoning only 0.15% of the training dataset.",[],[],"['Benjamin Schneider', 'Nils Lukas', 'Florian Kerschbaum']","['Computer Science, University of Waterloo', 'Machine Learning, Mohamed bin Zayed University of Artificial Intelligence', '']","['Canada', 'United Arab Emirates', '']"
https://openreview.net/forum?id=gmg7t8b4s0,Privacy & Data Governance,Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory,"Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often-overlooked interactive settings where an LLM receives information from multiple sources and generates an output to be shared with other entities, creating the potential of exposing sensitive input data in inappropriate contexts. In these scenarios, humans nat- urally uphold privacy by choosing whether or not to disclose information depending on the context. We ask the question “Can LLMs demonstrate an equivalent discernment and reasoning capability when considering privacy in context?” We propose CONFAIDE, a benchmark grounded in the theory of contextual integrity and designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. CONFAIDE consists of four tiers, gradually increasing in complexity, with the final tier evaluating contextual privacy reasoning and theory of mind capabilities. Our experiments show that even commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively, highlighting the urgent need for a new direction of privacy-preserving approaches as we demonstrate a larger underlying problem stemmed in the models’ lack of reasoning capabilities.",[],[],"['Niloofar Mireshghallah', 'Hyunwoo Kim', 'Xuhui Zhou', 'Yulia Tsvetkov', 'Maarten Sap', 'Reza Shokri', 'Yejin Choi']","['University of Washington', 'Allen Institute for Artificial Intelligence', 'SCS, CMU, Carnegie Mellon University', 'Department of Computer Science, University of Washington', 'Carnegie Mellon University', '', 'Computer Science, Computer Science Department, Stanford University']","['United States', '', 'United States', 'United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=2inBuwTyL2,Fairness & Bias,Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks,"Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack.  In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations: the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably SE(3)-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an SE(3) invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably SE(3) equivariant. We demonstrate that our method can yield substantially more precise placement predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations. Supplementary information and videos can be found at https://sites.google.com/view/reldist-iclr-2023.",[],[],"['Ben Eisner', 'Yi Yang', 'Todor Davchev', 'Mel Vecerik', 'Jonathan Scholz', 'David Held']","['Carnegie Mellon University', 'Google DeepMind', 'DeepMind', 'University College London', 'DeepMind', 'Carnegie Mellon University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=TPZRq4FALB,Fairness & Bias,Test-time Adaptation against Multi-modal Reliability Bias,"Test-time adaptation (TTA) has emerged as a new paradigm for reconciling distribution shifts across domains without accessing source data. However, existing TTA methods mainly concentrate on uni-modal tasks, overlooking the complexity of multi-modal scenarios. In this paper, we delve into the multi-modal test-time adaptation and reveal a new challenge named reliability bias. Different from the definition of traditional distribution shifts, reliability bias refers to the information discrepancies across different modalities derived from intra-modal distribution shifts. To solve the challenge, we propose a novel method, dubbed REliable fusion and robust ADaptation (READ). On the one hand, unlike the existing TTA paradigm that mainly repurposes the normalization layers, READ employs a new paradigm that modulates the attention between modalities in a self-adaptive way, supporting reliable fusion against reliability bias. On the other hand, READ adopts a novel objective function for robust multi-modal adaptation, where the contributions of confident predictions could be amplified and the negative impacts of noisy predictions could be mitigated. Moreover, we introduce two new benchmarks to facilitate comprehensive evaluations of multi-modal TTA under reliability bias. Extensive experiments on the benchmarks verify the effectiveness of our method against multi-modal reliability bias. The code and benchmarks are available at https://github.com/XLearning-SCU/2024-ICLR-READ.",[],[],"['Mouxing Yang', 'Yunfan Li', 'Changqing Zhang', 'Peng Hu', 'Xi Peng']","['Sichuan University', 'Sichuan University', 'College of Intelligence and Computing , Tianjin University', 'Sichuan University', 'Sichuan University']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=Zj12nzlQbz,Security,INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection,"Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.",[],[],"['Chao Chen', 'Kai Liu', 'Ze Chen', 'Yi Gu', 'Yue Wu', 'Mingyuan Tao', 'Zhihang Fu', 'Jieping Ye']","['Alibaba Group', 'National University of Singapore', 'Alibaba Group', 'Shanghai Jiao Tong University', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=2Q8TZWAHv4,Transparency & Explainability,GOAt: Explaining Graph Neural Networks via Graph Output Attribution,"Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-of-the-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.",[],[],"['Shengyao Lu', 'Keith G Mills', 'Jiao He', 'Bang Liu', 'Di Niu']","['Computer Engineering, University of Alberta', 'Department of Electrical and Computer Engineering, University of Alberta', 'Pudong, huawei', 'DIRO, University of Montreal', 'Department of Electrical and Computer Engineering, University of Alberta']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://openreview.net/forum?id=9JQtrumvg8,Fairness & Bias,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis","Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.",[],[],"['Izzeddin Gur', 'Hiroki Furuta', 'Austin V Huang', 'Mustafa Safdari', 'Yutaka Matsuo', 'Douglas Eck', 'Aleksandra Faust']","['CS, Google', 'Google DeepMind', 'Brown University', 'Research, Google', 'The University of Tokyo', 'Google', 'Google Brain']","['United States', 'United States', 'United States', 'United States', 'Japan', 'United States', 'United States']"
https://openreview.net/forum?id=1BuWv9poWz,Security,Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation,"Vision Transformers (ViTs) have been widely used in various domains. Similar to Convolutional Neural Networks (CNNs), ViTs are prone to the impacts of adversarial samples, raising security concerns in real-world applications. As one of the most effective black-box attack methods, transferable attacks can generate adversarial samples on surrogate models to directly attack the target model without accessing the parameters. However, due to the distinct internal structures of ViTs and CNNs, adversarial samples constructed by traditional transferable attack methods may not be applicable to ViTs. Therefore, it is imperative to propose more effective transferability attack methods to unveil latent vulnerabilities in ViTs. Existing methods have found that applying gradient regularization to extreme gradients across different functional regions in the transformer structure can enhance sample transferability. However, in practice, substantial gradient disparities exist even within the same functional region across different layers. Furthermore, we find that mild gradients therein are the main culprits behind reduced transferability. In this paper, we introduce a novel Gradient Normalization Scaling method for fine-grained gradient editing to enhance the transferability of adversarial attacks on ViTs. More importantly, we highlight that ViTs, unlike traditional CNNs, exhibit distinct attention regions in the frequency domain. Leveraging this insight, we delve into exploring the frequency domain to further enhance the algorithm's transferability. Through extensive experimentation on various ViT variants and traditional CNN models, we substantiate that the new approach achieves state-of-the-art performance, with an average performance improvement of 33.54\% and 42.05\% on ViT and CNN models, respectively. Our code is available at: https://github.com/LMBTough/GNS-HFA.",[],[],"['Zhiyu Zhu', 'Xinyi Wang', 'Zhibo Jin', 'Jiayu Zhang', 'Huaming Chen']","['University of Sydney', 'Faculty of Computer Science & Information Technology, Universiti Malaya', 'University of Technology Sydney', 'Suzhou Yierqi', '']","['Australia', 'Malaysia', 'Australia', 'China', '']"
https://openreview.net/forum?id=kIZ3S3tel6,Fairness & Bias,Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization,"We identify a new phenomenon in neural network optimization which arises from the interaction of depth and a particular heavy-tailed structure in natural data. Our result offers intuitive explanations for several previously reported observations about network training dynamics, including a conceptually new cause for progressive sharpening and the edge of stability. We further draw connections to related phenomena including grokking and simplicity bias.  Experimentally, we demonstrate the significant influence of paired groups of outliers in the training data with strong *Opposing Signals*: consistent, large magnitude features which dominate the network output and provide gradients which point in opposite directions. Due to these outliers, early optimization enters a narrow valley which carefully balances the opposing groups; subsequent sharpening causes their loss to rise rapidly, oscillating between high on one group and then the other, until the overall loss spikes. We carefully study these groups' effect on the network's optimization and behavior, and we complement this with a theoretical analysis of a two-layer linear network under a simplified model.  Our finding enables new qualitative predictions of training behavior which we confirm experimentally. It also provides a new lens through which to study and improve modern training practices for stochastic optimization, which we highlight via a case study of Adam versus SGD.",[],[],"['Elan Rosenfeld', 'Andrej Risteski']","['CMU, Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States']"
https://openreview.net/forum?id=pDCublKPmG,Security,Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL,"Most existing works focus on direct perturbations to the victim's state/action or the underlying transition dynamics to demonstrate the vulnerability of reinforcement learning agents to adversarial attacks.  However, such direct manipulations may not be always realizable. In this paper, we consider a multi-agent setting where a well-trained victim agent $\nu$ is exploited by an attacker controlling another  agent $\alpha$ with an \textit{adversarial policy}. Previous models do not account for the possibility that the attacker may only have partial control over  $\alpha$ or that the attack may produce easily detectable ``abnormal'' behaviors. Furthermore, there is a lack of provably efficient defenses against these adversarial policies.  To address these limitations, we introduce a generalized attack framework that has the flexibility to model to what extent the adversary is able to control the agent, and allows the attacker to regulate the state distribution shift and produce stealthier adversarial policies. Moreover, we offer a provably efficient defense with polynomial convergence to the most robust victim policy through adversarial training with timescale separation.  This stands in sharp contrast to supervised learning, where adversarial training typically provides only \textit{empirical} defenses. Using the Robosumo competition experiments, we show that our generalized attack formulation results in much stealthier adversarial policies when maintaining the same winning rate as baselines.  Additionally, our adversarial training approach yields stable learning dynamics and less exploitable victim policies.",[],[],"['Xiangyu Liu', 'Souradip Chakraborty', 'Yanchao Sun', 'Furong Huang']","['University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'Apple AI/ML', 'Computer Science, University of Maryland']","['United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=DFTHW0MyiW,Security,Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies,"In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond merely worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic difficulty in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\Pi$. This finding prompts us to \textit{refine} the baseline policy class $\Pi$ prior to test time, aiming for efficient adaptation within a compact, finite policy class $\tilde{\Pi}$, which can resort to an adversarial bandit subroutine. In light of the importance of a finite and compact $\tilde{\Pi}$, we propose a novel training-time algorithm to iteratively discover \textit{non-dominated policies}, forming a near-optimal and minimal $\tilde{\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios.",[],[],"['Xiangyu Liu', 'Chenghao Deng', 'Yanchao Sun', 'Yongyuan Liang', 'Furong Huang']","['University of Maryland, College Park', 'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Apple AI/ML', 'Computer Science Department, University of Maryland, College Park', 'Computer Science, University of Maryland']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=HXoq9EqR9e,Fairness & Bias,FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs,"Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP’s image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative optimization involving closed-form solvers, which leads to 4×-10× faster training than the existing methods. 3) Sample Efficiency: Under sample-limited conditions, FairerCLIP significantly outperforms baselines when they fail entirely. And, 4) Performance: Empirically, FairerCLIP achieves appreciable accuracy gains on benchmark fairness and spurious correlation datasets over their respective baselines.",[],[],"['Sepehr Dehdashtian', 'Lan Wang', 'Vishnu Boddeti']","['Computer Science and Engineering, Michigan State University', '', 'Michigan State University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=i9Vs5NGDpk,Fairness & Bias,"Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning","We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of asymptotically free sketches under very mild data assumptions. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. This in particular allows construction of prediction intervals with asymptotically correct coverage conditional on the training data. We also propose an ""ensemble trick"" whereby the risk for unsketched ridge regression can be efficiently estimated via GCV using small sketched ridge ensembles. We empirically validate our theoretical results using both synthetic and real large-scale datasets with practical sketches including CountSketch and subsampled randomized discrete cosine transforms.",[],[],"['Pratik Patil', 'Daniel LeJeune']","['University of California, Berkeley', 'Stanford University']","['United States', 'United States']"
https://openreview.net/forum?id=gHAr7ZA1OL,Fairness & Bias,Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects,"To promote the safe deployment of object detectors, a task of unsupervised out-of-distribution object detection (OOD-OD) is recently proposed, aiming to detect unknown objects during training without reliance on any auxiliary OOD data. To alleviate the impact of lacking OOD data, for this task, one feasible solution is to exploit the known in-distribution (ID) data to synthesize proper OOD information for supervision, which strengthens detectors' discrimination. From the frequency perspective, since the phase generally reflects the content of the input, in this paper, we explore leveraging the phase of ID features to generate expected OOD features involving different content. And a method of Modulated Phase Diffusion (MPD) is proposed, containing a shared forward and two different reverse processes. Specifically, after calculating the phase of the extracted features, to prevent the rapid loss of content in the phase, the forward process gradually performs Gaussian Average on the phase instead of adding noise. The averaged phase and original amplitude are combined to obtain the features taken as the input of the reverse process. Next, one OOD branch is defined to synthesize virtual OOD features by continually enlarging the content discrepancy between the OOD features and original ones. Meanwhile, another modulated branch is designed to generate augmented features owning a similar phase as the original features by scaling and shifting the OOD branch. Both original and augmented features are used for training, enhancing the discrimination. Experimental results on OOD-OD, incremental object detection, and open-set object detection demonstrate the superiorities of our method. The source code will be released at https://github.com/AmingWu/MPD.",[],[],"['Aming WU', 'Cheng Deng']","['College of Electronic Engineering, Xidian University', 'School of Electronic Engineering, Xidian University']","['United States', 'China']"
https://openreview.net/forum?id=5KojubHBr8,Fairness & Bias,MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning,"Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity.  However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images, making VLMs less effective in downstream vision-language tasks. In this paper, we address the limitation above by 1) introducing vision-language Model with **M**ulti-**M**odal **I**n-**C**ontext **L**earning(MMICL), a new approach to allow the VLM to deal with multi-modal inputs efficiently; 2) proposing a novel context scheme to augment the in-context learning ability of the VLM; 3) constructing the Multi-modal In-Context Learning (MIC) dataset, designed to enhance the VLM's ability to understand complex multi-modal prompts. Our experiments confirm that MMICL achieves new state-of-the-art zero-shot performance on a wide range of general vision-language tasks, especially for complex benchmarks, including MME and MMBench. Our analysis demonstrates that MMICL effectively tackles the challenge of complex multi-modal prompt understanding and emerges the impressive ICL ability. Furthermore, we observe that MMICL successfully alleviates language bias in VLMs, a common issue for VLMs that often leads to hallucination when faced with extensive textual context. Our code, dataset, dataset tool, and model are available at https://github.com/PKUnlp-icler/MIC.",[],[],"['Haozhe Zhao', 'Zefan Cai', 'Shuzheng Si', 'Xiaojian Ma', 'Kaikai An', 'Liang Chen', 'Zixuan Liu', 'Sheng Wang', 'Wenjuan Han', 'Baobao Chang']","['Peking University', '', 'Tsinghua University', 'University of California, Los Angeles', 'Peking University', 'School of Computer Science, Peking University', 'University of Washington', 'CSE, University of Washington, Seattle', 'Beijing Jiaotong University', 'School of Computer Science, Peking University']","['China', '', 'China', 'United States', 'China', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=dnqPvUjyRI,Fairness & Bias,SemiReward: A General Reward Model for Semi-supervised Learning,"Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks across three modalities, extensive experiments verify that SemiReward achieves significant performance gains and faster convergence speeds upon Pseudo Label, FlexMatch, and Free/SoftMatch. Code and models are available at https://github.com/Westlake-AI/SemiReward.",[],[],"['Siyuan Li', 'Weiyang Jin', 'Zedong Wang', 'Fang Wu', 'Zicheng Liu', 'Cheng Tan', 'Stan Z. Li']","['Westlake University & Zhejiang University', 'New York University', 'The Hong Kong University of Science and Technology', '', 'School of Engineering, Westlake University', 'Zhejiang University & Westlake University', 'AI Department, Westlake University']","['China', 'United States', 'Hong Kong', '', 'United States', 'China', 'United States']"
https://openreview.net/forum?id=8vKknbgXxf,Fairness & Bias,What does automatic differentiation compute for neural networks?,"Forward- or reverse-mode automatic differentiation (AD) is a popular algorithm for computing the derivative of a function expressed by a program. AD always outputs the correct derivative if a program does not use any non-differentiable functions and control flows; however, it may return an arbitrary value otherwise. In this work, we investigate what AD computes for neural networks that may contain non-differentiable functions such as ReLU and maxpools. We first prove that AD always returns a generalized derivative called a Clarke subderivative for networks with pointwise activation functions, if the minibatch size is one and all non-differentiable neurons have distinct bias parameters. We show that the same conclusion does not hold otherwise, but does hold under some mild sufficient conditions. We also prove similar results for more general networks that can use maxpools and bias parameters shared across different neurons. We empirically check our sufficient conditions over popular network architectures and observe that AD almost always computes a Clarke subderivative in practical learning setups.",[],[],"['Sejun Park', 'Sanghyuk Chun', 'Wonyeol Lee']","['Korea University', 'NAVER AI Lab', 'POSTECH']","['Japan', 'Italy', 'Burkina Faso']"
https://openreview.net/forum?id=uWVC5FVidc,Fairness & Bias,Unbiased Watermark for Large Language Models,"The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.",[],[],"['Zhengmian Hu', 'Lichang Chen', 'Xidong Wu', 'Yihan Wu', 'Hongyang Zhang', 'Heng Huang']","['Adobe Systems', 'Google', 'University of Pittsburgh', 'CS, University of Maryland, College Park', 'School of Computer Science, University of Waterloo', 'Department of Computer Science, University of Maryland, College Park']","['United States', '', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=ShjMHfmPs0,Fairness & Bias,Self-Consuming Generative Models Go MAD,"Seismic advances in generative AI algorithms for imagery, text, and other data types have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates an autophagous (""self-consuming"") loop  whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity.  Our primary conclusion across all scenarios is that *without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease.* We term this condition Model Autophagy Disorder (MAD), by analogy to mad cow disease, and show that appreciable MADness arises in just a few generations.",[],[],"['Sina Alemohammad', 'Josue Casco-Rodriguez', 'Lorenzo Luzi', 'Ahmed Imtiaz Humayun', 'Hossein Babaei', 'Daniel LeJeune', 'Ali Siahkoohi', 'Richard Baraniuk']","['Electrical and Computer engineering, Rice University', 'Rice University', 'Rice University', 'Rice University', 'Rice University', 'Stanford University', 'Computational Applied Mathematics and Operations Research, Rice University', 'William Marsh Rice University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=E60SIDItyT,Fairness & Bias,Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions,"Due to the rise of privacy concerns, in many practical applications, the training data is aggregated before being shared with the learner to protect the privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: the bag-level loss and the instance-level loss. In the former, the model is learned by minimizing a loss between the aggregate responses and aggregate model predictions, while in the latter, the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation allows us to compare the two approaches with respect to the bias and variance of the resulting estimators and to introduce a novel interpolating estimator that combines the two approaches. For linear regression tasks, we provide a precise characterization of the risk of the interpolating estimator in an asymptotic regime where the size of the training set grows in proportion to the feature dimension. Our analysis enables us to theoretically understand the effect of different factors, such as bag size, on the model's prediction risk. Additionally, we propose a mechanism for differentially private learning from aggregate responses and derive the optimal bag size in terms of the prediction risk-privacy trade-off. We also carry out thorough experiments to corroborate our theory and show the efficacy of the interpolating estimator.",[],[],"['Adel Javanmard', 'Lin Chen', 'Vahab Mirrokni', 'Ashwinkumar Badanidiyuru', 'Gang Fu']","['Data Sciences and Operations, University of Southern California', '', 'Google Research', 'Google', 'Google Research']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=qup9xD8mW4,Transparency & Explainability,Behaviour Distillation,"Dataset distillation aims to condense large datasets into a small number of synthetic examples that can be used as drop-in replacements when training new models. It has applications to interpretability, neural architecture search, privacy, and continual learning. Despite strong successes in supervised domains, such methods have not yet been extended to reinforcement learning, where the lack of a fixed dataset renders most distillation methods unusable. Filling the gap, we formalize $\textit{behaviour distillation}$, a setting that aims to discover and then condense the information required for training an expert policy into a synthetic dataset of state-action pairs, $\textit{without access to expert data}$.  We then introduce Hallucinating Datasets with Evolution Strategies (HaDES), a method for behaviour distillation that can discover datasets of $\textit{just four}$ state-action pairs which, under supervised learning, train agents to competitive performance levels in continuous control tasks. We show that these datasets generalize out of distribution to training policies with a wide range of architectures and hyperparameters. We also demonstrate application to a downstream task, namely training multi-task agents in a zero-shot fashion. Beyond behaviour distillation, HaDES provides significant improvements in neuroevolution for RL over previous approaches and achieves SoTA results on one standard supervised dataset distillation task. Finally, we show that visualizing the synthetic datasets can provide human-interpretable task insights.",[],[],"['Andrei Lupu', 'Chris Lu', 'Jarek Luca Liesen', 'Robert Tjarko Lange', 'Jakob Nicolaus Foerster']","['University of Oxford', 'University of Oxford', 'Bernstein center for computational neuroscience', 'TU Berlin', 'University of Oxford, University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=9kG7TwgLYu,Fairness & Bias,Time Fairness in Online Knapsack Problems,"The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive algorithms are known for this problem, they may be fundamentally unfair, i.e., individual items may be treated inequitably in different ways. We formalize a practically-relevant notion of time fairness which effectively models a trade off between static and dynamic pricing in a motivating application such as cloud resource allocation, and show that existing algorithms perform poorly under this metric.  We propose a parameterized deterministic algorithm where the parameter precisely captures the Pareto-optimal trade-off between fairness (static pricing) and competitiveness (dynamic pricing). We show that randomization is theoretically powerful enough to be simultaneously competitive and fair; however, it does not work well in experiments. To further improve the trade-off between fairness and competitiveness, we develop a nearly-optimal learning-augmented algorithm which is fair, consistent, and robust (competitive), showing substantial performance improvements in numerical experiments.",[],[],"['Adam Lechowicz', 'Rik Sengupta', 'Bo Sun', 'Shahin Kamali', 'Mohammad Hajiesmaili']","['Manning College of Information and Computer Sciences, University of Massachusetts Amherst', 'International Business Machines', '', 'Electrical Engineering and Computer Science, York University', 'College of Information and Computer Sciences, University of Massachusetts Amherst']","['United States', 'Colombia', '', 'Canada', 'United States']"
https://openreview.net/forum?id=zbOSJ3CATY,Security,A ROBUST DIFFERENTIAL NEURAL ODE OPTIMIZER,"Neural networks and neural ODEs tend to be vulnerable to adversarial attacks, rendering robust optimizers critical to curb the success of such attacks. In this regard, the key insight of this work is to interpret Neural ODE optimization as a min-max optimal control problem. More particularly, we present Game Theoretic Second-Order Neural Optimizer (GTSONO), a robust game theoretic optimizer based on the principles of min-max Differential Dynamic Programming. The proposed method exhibits significant computational benefits due to efficient matrix decompositions and provides convergence guarantees to local saddle points. Empirically, the robustness of the proposed optimizer is demonstrated through greater robust accuracy  compared to benchmark optimizers when trained on clean images. Additionally, its ability to provide a performance increase when adapted to an already existing adversarial defense technique is also illustrated. Finally, the superiority of the proposed update law over its gradient based counterpart highlights the potential benefits of incorporating robust optimal control paradigms into adversarial training methods.",[],[],"['Panagiotis Theodoropoulos', 'Guan-Horng Liu', 'Tianrong Chen', 'Augustinos D Saravanos', 'Evangelos Theodorou']","['Aerospace Engineering, Georgia Institute of Technology', 'FAIR, Meta AI', 'Apple', 'Georgia Institute of Technology', 'Georgia Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=XIZEFyVGC9,Fairness & Bias,Debiasing Algorithm through Model Adaptation,"Large language models are becoming the go-to solution for the ever-growing number of tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey bias. Based on the analysis results, we intervene in the model by applying a linear projection to the weight matrices of these layers. Our titular method DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.",[],[],"['Tomasz Limisiewicz', 'David Mareček', 'Tomáš Musil']","['Charles University', 'Charles University, Prague', 'Charles University, Prague']","['Czech Republic', 'Czech Republic', 'Czech Republic']"
https://openreview.net/forum?id=Koh0i2u8qX,Security,Mitigating Emergent Robustness Degradation while Scaling Graph Learning,"Although graph neural networks have exhibited remarkable performance in various graph tasks, a significant concern is their vulnerability to adversarial attacks. Consequently, many defense methods have been proposed to alleviate the deleterious effects of adversarial attacks and learn robust graph representations. However, most of them are difficult to *simultaneously* avoid two major limitations: (i) an emergent and severe degradation in robustness when exposed to very intense attacks, and (ii) heavy computation complexity hinders them from scaling to large graphs. In response to these challenges, we introduce an innovative graph defense method for unpredictable real-world scenarios by *designing a graph robust learning framework that is resistant to robustness degradation* and *refraining from unscalable designs with heavy computation*: specifically, our method employs a denoising module, which eliminates edges that are associated with attacked nodes to reconstruct a cleaner graph; Then, it applies Mixture-of-Experts to select differentially private noises with varying magnitudes to counteract the hidden features attacked at different intensities toward robust predictions; Moreover, our overall design avoids the reliance on heavy adjacency matrix computations, such as SVD, thus facilitating its applicability even on large graphs. Comprehensive experiments have been conducted to demonstrate the anti-degraded robustness and scalability of our method, as compared to popular graph adversarial learning methods, under diverse attack intensities and various datasets of different sizes.",[],[],"['Xiangchi Yuan', 'Chunhui Zhang', 'Yijun Tian', 'Yanfang Ye', 'Chuxu Zhang']","['Georgia Institute of Technology', 'Dartmouth College', 'Amazon', 'CSE, University of Notre Dame', '']","['United States', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=ORUiqcLpV6,Transparency & Explainability,CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding,"3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question “Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?”. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at github.com/eslambakr/CoT 3DV G.",[],[],"['Eslam Mohamed BAKR', 'Mohamed Ayman Mohamed', 'Mahmoud Ahmed', 'Habib Slim', 'Mohamed Elhoseiny']","['CEMSE, King Abdullah University of Science and Technology', 'Computing Science,  University of Alberta', 'King Abdullah University of Science and Technology', 'VCC, King Abdullah University of Science and Technology', 'Computer Science , KAUST']","['Saudi Arabia', 'Saudi Arabia', 'Saudi Arabia', 'Saudi Arabia', '']"
https://openreview.net/forum?id=ODSgo2m8aE,Fairness & Bias,Aligning Relational Learning with Lipschitz Fairness,"Relational learning has gained significant attention, led by the expressiveness of Graph Neural Networks (GNNs) on graph data. While the inherent biases in common graph data are involved in GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method to examine the output stability of machine learning models without incurring additional computational costs. Recently, its use in controlling the stability of Euclidean neural networks, the calculation of the precise Lipschitz bound remains elusive for non-Euclidean neural networks like GNNs, especially within fairness contexts. However, no existing research has investigated Lipschitz bounds to shed light on stabilizing the GNN outputs, especially when working on graph data with implicit biases. To narrow this gap, we begin with the general GNNs operating on relational data, and formulate a Lipschitz bound to limit the changes in the output regarding biases associated with the input. Additionally, we theoretically analyze how the Lipschitz bound of a GNN model could constrain the output perturbations induced by biases learned from data for fairness training. We experimentally validate the Lipschitz bound's effectiveness in limiting biases of the model output. Finally, from a training dynamics perspective, we demonstrate why the theoretical Lipschitz bound can effectively guide the GNN training to better trade-off between accuracy and fairness.",[],[],"['Yaning Jia', 'Chunhui Zhang', 'Soroush Vosoughi']","['Computer Science, Dartmouth College', 'Dartmouth College', 'Dartmouth College']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=yRrPfKyJQ2,Transparency & Explainability,Conversational Drug Editing Using Retrieval and Domain Feedback,"Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. To bridge this gap, we propose ChatDrug, a framework to facilitate the systematic investigation of drug editing using LLMs. ChatDrug jointly leverages a prompt module, a retrieval and domain feedback module, and a conversation module to streamline effective drug editing. We empirically show that ChatDrug reaches the best performance on all 39 drug editing tasks, encompassing small molecules, peptides, and proteins. We further demonstrate, through 10 case studies, that ChatDrug can successfully identify the key substructures for manipulation, generating diverse and valid suggestions for drug editing. Promisingly, we also show that ChatDrug can offer insightful explanations from a domain-specific perspective, enhancing interpretability and enabling informed decision-making.",[],[],"['Shengchao Liu', 'Jiongxiao Wang', 'Yijin Yang', 'Chengpeng Wang', 'Ling Liu', 'Hongyu Guo', 'Chaowei Xiao']","['EECS, University of California, Berkeley', 'iSchool, University of Wisconsin - Madison', 'Arizona State University', 'University of Illinois at Urbana-Champaign', 'Princeton University', 'University of Ottawa', 'University of Wisconsin - Madison']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=39cPKijBed,Fairness & Bias,Training Unbiased Diffusion Models From Biased Dataset,"With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The experimental evidence supports the usefulness of the proposed method, which outperforms baselines including time-independent importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.",[],[],"['Yeongmin Kim', 'Byeonghu Na', 'Minsang Park', 'JoonHo Jang', 'Dongjun Kim', 'Wanmo Kang', 'Il-chul Moon']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Stanford University', 'Korea Advanced Institute of Science and Technology', '']","['South Korea', 'South Korea', 'South Korea', 'South Korea', 'United States', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=xEJMoj1SpX,Fairness & Bias,Elucidating the Exposure Bias in Diffusion Models,"Diffusion models have demonstrated impressive generative capabilities, but their exposure bias problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output, mitigating the input mismatch between training and sampling. Experiments on various diffusion frameworks (ADM, DDIM, EDM, LDM, DiT, PFGM++) verify the effectiveness of our method. Remarkably, our ADM-ES, as a state-of-the-art stochastic sampler, obtains 2.17 FID on CIFAR-10 under 100-step unconditional generation. The code is at https://github.com/forever208/ADM-ES",[],[],"['Mang Ning', 'Mingxiao Li', 'Jianlin Su', 'Albert Ali Salah', 'Itir Onal Ertugrul']","['Utrecht University', 'Computer Science, KU Leuven', 'Moonshot AI Ltd', 'Utrecht University', 'Utrecht University']","['Netherlands', '', 'Netherlands', 'Netherlands', 'Netherlands']"
https://openreview.net/forum?id=aA33A70IO6,Security,Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis,"The rapid development of large language models (LLMs) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement. Existing alignment methods usually direct LLMs toward the favorable outcomes by utilizing human-annotated, flawless instruction-response pairs. Conversely, this study proposes a novel alignment technique based on mistake analysis, which deliberately exposes LLMs to erroneous content to learn the reasons for mistakes and how to avoid them. In this case, mistakes are repurposed into valuable data for alignment, effectively helping to avoid the production of erroneous responses. Without external models or human annotations, our method leverages a model's intrinsic ability to discern undesirable mistakes and improves the safety of its generated responses. Experimental results reveal that our method outperforms existing alignment approaches in enhancing model safety while maintaining the overall utility.",[],[],"['Kai Chen', 'Chunwei Wang', 'Kuo Yang', 'Jianhua Han', 'Lanqing HONG', 'Fei Mi', 'Hang Xu', 'Zhengying Liu', 'Wenyong Huang', 'Zhenguo Li', 'Dit-Yan Yeung', 'Lifeng Shang']","['The Hong Kong University of Science and Technology', 'Pudong Distinct, Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huawei Noah‘s Ark Lab', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology', 'Huawei Technologies Ltd.']","['Hong Kong', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=92btneN9Wm,Fairness & Bias,SPDER: Semiperiodic Damping-Enabled Object Representation,"We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional implicit neural representation networks. Our proposed architecture, SPDER, is a simple MLP that uses an activation function composed of a sinusoidal multiplied by a sublinear function, called the damping function. The sinusoidal enables the network to automatically learn the positional embedding of an input coordinate while the damping passes on the actual coordinate value by preventing it from being projected down to within a finite range of values. Our results indicate that SPDERs speed up training by 10 times and converge to losses 1,500 to 50,000 times lower than that of the state-of-the-art for image representation. SPDER is also state-of-the-art in audio representation. The superior representation capability allows SPDER to also excel on multiple downstream tasks such as image super-resolution and video frame interpolation. We provide intuition as to why SPDER significantly improves fitting compared to that of other INR methods while requiring no hyperparameter tuning or preprocessing. See code at https://github.com/katop1234/SPDER.",[],[],"['Kathan Shah', 'Chawin Sitawarin']","['EECS, University of California, Berkeley', 'Meta']","['United States', '']"
https://openreview.net/forum?id=2cRzmWXK9N,Security,Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints,"The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative; and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush–Kuhn–Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, our $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).",[],[],"['Chaoqi Wang', 'Yibo Jiang', 'Chenghao Yang', 'Han Liu', 'Yuxin Chen']","['xAI', 'University of Chicago', 'Computer Science, University of Chicago', 'University of Chicago', 'Computer Science, University of Chicago']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Tr3fZocrI6,Fairness & Bias,Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data,"A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias}$ & $\texttt{Feature-Whiten}$ ($\texttt{DFW}$), of the popular alternating minimization-descent (AMD) scheme proposed in Collins et al., (2021), and establish linear convergence to the optimal representation with noise level scaling down with the $\textit{total}$ source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of $\texttt{DFW}$ on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data. Our analysis unifies and generalizes prior work, and provides a flexible framework for a wider range of applications, such as in controls and dynamical systems.",[],[],"['Thomas TCK Zhang', 'Leonardo Felipe Toso', 'James Anderson', 'Nikolai Matni']","['University of Pennsylvania, University of Pennsylvania', 'Columbia University', 'Columbia University', 'School of Engineering and Applied Science, University of Pennsylvania']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=nsNyDvNQTc,Fairness & Bias,Leveraging Uncertainty Estimates To Improve Classifier Performance,"Binary classification typically involves predicting the label of an instance based on whether the model score for the positive class exceeds a threshold chosen based on the application requirements (e.g., maximizing recall for a precision bound). However, model scores are often not aligned with true positivity rate. This is especially true when the training involves a differential sampling of classes or there is distributional drift between train and test settings. In this paper, we provide theoretical analysis and empirical evidence of the dependence of estimation bias on both uncertainty and model score. Further, we  formulate the decision boundary selection using both model score and uncertainty, prove that it is NP-hard, and present  algorithms  based on dynamic programming and isotonic regression.  Evaluation of the proposed algorithms on three real-world datasets yield  25\%-40\%  improvement in recall at high precision bounds over the traditional approach of using model score alone, highlighting the benefits of leveraging uncertainty.",[],[],"['Gundeep Arora', 'Srujana Merugu', 'Anoop Saladi', 'Rajeev Rastogi']","['Amazon', '', 'Amazon', 'Amazon']","['Japan', '', 'Japan', 'Japan']"
https://openreview.net/forum?id=fjf3YenThE,Security,New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions,"Hard-thresholding is an important type of algorithm in machine learning that is used to solve $\ell_0$ constrained optimization problems. However,  the true gradient of the objective function can be difficult to access in certain scenarios, which normally can be approximated by zeroth-order (ZO) methods. SZOHT algorithm is the only algorithm tackling $\ell_0$ sparsity constraints with zeroth-order gradients so far. Unfortunately,  SZOHT  has a notable limitation on the number of random directions due to the inherent conflict between the deviation of ZO gradients and the expansivity of the hard-thresholding operator.  This paper approaches this problem by considering the role of variance and provides a new insight into variance reduction: mitigating the unique conflicts between ZO gradients and hard-thresholding.  Under this perspective, we propose a generalized variance reduced ZO hard-thresholding algorithm as well as the generalized convergence analysis under standard assumptions. The theoretical results demonstrate the new algorithm eliminates the restrictions on the number of random directions, leading to improved convergence rates and broader applicability compared with SZOHT.  Finally, we illustrate the utility of our method on a portfolio optimization problem as well as black-box adversarial attacks.",[],[],"['Xinzhe Yuan', 'William de Vazelhes', 'Bin Gu', 'Huan Xiong']","['IASM, Harbin Institute of Technology', 'Technology Innovation Institute', 'machine learning, Mohamed bin Zayed University of Artificial Intelligence', 'Harbin Institute of Technology']","['China', 'Brazil', 'United Arab Emirates', 'China']"
https://openreview.net/forum?id=7Jwpw4qKkb,Security,AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models,"The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively. Code is available at https://github.com/SheltonLiu-N/AutoDAN.",[],[],"['Xiaogeng Liu', 'Nan Xu', 'Muhao Chen', 'Chaowei Xiao']","['University of Wisconsin - Madison', 'Department of Computer Science, University of Southern California', 'University of California, Davis', 'University of Wisconsin - Madison']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=P1aobHnjjj,Fairness & Bias,Implicit bias of SGD in $L_2$-regularized linear DNNs: One-way jumps from high to low rank,"The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can be avoided since they do not fit the data, GD might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.",[],[],"['Zihan Wang', 'Arthur Jacot']","['New York University', 'Courant Institute, NYU, New York University']","['United States', 'United States']"
https://openreview.net/forum?id=NnYaYVODyV,Transparency & Explainability,Perceptual Group Tokenizer: Building Perception with Iterative Grouping,"Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping. Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-training, and interpretability. Specifically, Perceptual Group Tokenizer achieves 79.7% on ImageNet-1K self-supervised learning benchmark with linear probe evaluation, marking a new progress under this paradigm.",[],[],"['Zhiwei Deng', 'Ting Chen', 'Yang Li']","['Google Deepmind', 'Google', 'Google']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=nZP10evtkV,Security,Optimal transport based adversarial patch to leverage large scale attack transferability,"Adversarial patch attacks, where a small patch is placed in the scene to fool neural networks, have been studied for numerous applications. Focusing on image classification, we consider the setting of a black-box transfer attack where an attacker does not know the target model. Instead of forcing corrupted image representations to cross the nearest decision boundaries or converge to a particular point, we propose a distribution-oriented approach. We rely on optimal transport to push the feature distribution of attacked images towards an already modeled distribution. We show that this new distribution-oriented approach leads to better transferable patches. Through digital experiments conducted on ImageNet-1K, we provide evidence that our new patches are the only ones that can simultaneously influence multiple Transformer models and Convolutional Neural Networks. Physical world experiments demonstrate that our patch can affect systems in deployment without explicit knowledge.",[],[],"['Pol Labarbarie', 'Adrien CHAN-HON-TONG', 'Stéphane Herbin', 'Milad Leyli-abadi']","['Onera - The french aerospace lab', 'Onera - The french aerospace lab', 'ONERA', 'Irt SystemX']","['France', 'France', 'France', 'France']"
https://openreview.net/forum?id=GkJOCga62u,Fairness & Bias,Orbit-Equivariant Graph Neural Networks,"Equivariance is an important structural property that is captured by architectures such as graph neural networks (GNNs). However, equivariant graph functions cannot produce different outputs for similar nodes, which may be undesirable when the function is trying to optimize some global graph property. In this paper, we define orbit-equivariance, a relaxation of equivariance which allows for such functions whilst retaining important structural inductive biases. We situate the property in the hierarchy of graph functions, define a taxonomy of orbit-equivariant functions, and provide four different ways to achieve non-equivariant GNNs. For each, we analyze their expressivity with respect to orbit-equivariance and evaluate them on two novel datasets, one of which stems from a real-world use-case of designing optimal bioisosteres.",[],[],"['Matthew Morris', 'Bernardo Cuenca Grau', 'Ian Horrocks']","['Department of Computer Science, University of Oxford', 'Computer Science, University of Oxford', 'University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=gn0mIhQGNM,Privacy & Data Governance,SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation,"With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often suffer limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' for MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting data points). To the best of our knowledge, SalUn is the first principled MU approach that can effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation tasks. As highlighted below, For example, SalUn yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2% gap compared to exact unlearning on the CIFAR-10 dataset. Moreover, in preventing conditional diffusion models from generating harmful images, SalUn achieves nearly 100% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not. Codes are available at https://github.com/OPTML-Group/Unlearn-Saliency.  **WARNING**: This paper contains model outputs that may be offensive in nature.",[],[],"['Chongyu Fan', 'Jiancheng Liu', 'Yihua Zhang', 'Eric Wong', 'Dennis Wei', 'Sijia Liu']","['Computer Science and Engineering, Michigan State University', 'Computer Science and Engineering, Michigan State University', 'Computer Science and Engineering, Michigan State University', 'University of Pennsylvania', 'International Business Machines', 'CSE, Michigan State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Sf2A2PUXO3,Fairness & Bias,Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation,"Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces.  To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\times \sim 5000\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.",[],[],"['Hsiang Hsu', 'Guihong Li', 'Shaohan Hu', 'Chun-Fu Chen']","['JP Morgan & Chase Bank', 'Advanced Micro Devices', 'J.P. Morgan Chase', 'JPMorganChase, GTAR']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=AfnsTnYphT,Fairness & Bias,"Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs","Vision tasks are characterized by the properties of locality and translation invariance.      The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture.     Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds,      or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks.     To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is determined by a $d$-sparse signal vector that can freely appear in any one of the $k$ patches.      On this task, for any orthogonally equivariant algorithm like gradient descent, we prove that CNNs require $\tilde{O}(k+d)$ samples, whereas LCNs require $\Omega(kd)$ samples, establishing the statistical advantages of weight sharing in translation invariant tasks.      Furthermore, LCNs need $\tilde{O}(k(k+d))$ samples, compared to $\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in local tasks.     Additionally, we develop information theoretic tools for analyzing randomized algorithms, which may be of interest for statistical research.",[],[],"['Aakash Lahoti', 'Stefani Karp', 'Ezra Winston', 'Aarti Singh', 'Yuanzhi Li']","['CMU, Carnegie Mellon University', 'Google', 'Machine Learning Department, School of Computer Science', 'Machine Learning, Carnegie Mellon University', 'CMU, Carnegie Mellon University']","['United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=6IjN7oxjXt,Security,Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training,"Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We, therefore, propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also aids in mitigating ""robust overfitting"". Furthermore, our study provides valuable insights into the mechanisms of selective adversarial training and offers a promising avenue for future research.",[],[],"['Shruthi Gowda', 'Bahram Zonooz', 'Elahe Arani']","['Eindhoven University of Technology', 'Department of Mathematics and Computer Science, Eindhoven University of Technology', 'Mathematics and computer science , Eindhoven University of technology']","['Netherlands', 'Netherlands', 'Netherlands']"
https://openreview.net/forum?id=m52uU0dVbH,Security,Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit,"Vertical federated learning (VFL), where each participating client holds a subset of data features, has found numerous applications in finance, healthcare, and IoT systems. However, adversarial attacks, particularly through the injection of adversarial examples (AEs), pose serious challenges to the security of VFL models. In this paper, we investigate such vulnerabilities through developing a novel attack to disrupt the VFL inference process, under a practical scenario where the adversary is able to *adaptively corrupt a subset of clients*. We formulate the problem of finding optimal attack strategies as an online optimization problem, which is decomposed into an inner problem of adversarial example generation (AEG) and an outer problem of corruption pattern selection (CPS). Specifically, we establish the equivalence between the formulated CPS problem and a multi-armed bandit (MAB) problem, and propose the Thompson sampling with Empirical maximum reward (E-TS) algorithm for the adversary to efficiently identify the optimal subset of clients for corruption. The key idea of E-TS is to introduce an estimation of the expected maximum reward for each arm, which helps to specify a small set of *competitive arms*, on which the exploration for the optimal arm is performed. This significantly reduces the exploration space, which otherwise can quickly become prohibitively large as the number of clients increases. We analytically characterize the regret bound of E-TS, and empirically demonstrate its capability of efficiently revealing the optimal corruption pattern with the highest attack success rate, under various datasets of popular VFL tasks.",[],[],"['Duanyi YAO', 'Songze Li', 'Ye XUE', 'Jin Liu']","['Hong Kong University of Science and Technology', 'Southeast University', 'Shenzhen Research Institute of Big Data, CUHK(SZ）', 'HKUST(GZ)']","['Hong Kong', 'China', 'China', 'Hong Kong']"
https://openreview.net/forum?id=xriGRsoAza,Transparency & Explainability,Inherently Interpretable Time Series Classification via Multiple Instance Learning,"Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains.",[],[],"['Joseph Early', 'Gavin Cheung', 'Kurt Cutajar', 'Hanting Xie', 'Jas Kandola', 'Niall Twomey']","['Helsing', 'Amazon', 'Amazon', 'Amazon', 'Amazon', 'Amazon']","['United Kingdom', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=uNkKaD3MCs,Fairness & Bias,Learning with Mixture of Prototypes for Out-of-Distribution Detection,"Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g. modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) that models each class with multiple prototypes to capture the sample diversities, which learns more faithful and compact samples embeddings for enhanching OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. To learn embeddings with multiple prototypes, PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Compared to previous methods with prototypes, the proposed mixture prototype modeling of PALM promotes the representations of each ID class to be more compact and separable from others and the unseen OOD samples, resulting in more reliable OOD detection. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM over previous methods, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark.",[],[],"['Haodong Lu', 'Dong Gong', 'Shuo Wang', 'Jason Xue', 'Lina Yao', 'Kristen Moore']","['', 'University of New South Wales', 'Shanghai Jiao Tong University', '', 'University of New South Wales', ""Data61, CSIRO's Data61""]","['', 'Australia', 'China', '', 'Australia', 'China']"
https://openreview.net/forum?id=RzY9qQHUXy,Fairness & Bias,Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning,"Real-world tasks are universally associated with training samples that exhibit a long-tailed class distribution, and traditional deep learning models are not suitable for fitting this distribution, thus resulting in a biased trained model. To surmount this dilemma, massive deep long-tailed learning studies have been proposed to achieve inter-class fairness models by designing sophisticated sampling strategies or improving existing model structures and loss functions. Habitually, these studies tend to apply data augmentation strategies to improve the generalization performance of their models. However, this augmentation strategy applied to balanced distributions may not be the best option for long-tailed distributions. For a profound understanding of data augmentation, we first theoretically analyze the gains of traditional augmentation strategies in long-tailed learning, and observe that augmentation methods cause the long-tailed distribution to be imbalanced again, resulting in an intertwined imbalance: inherent data-wise imbalance and extrinsic augmentation-wise imbalance, i.e., two 'birds' co-exist in long-tailed learning. Motivated by this observation, we propose an adaptive Dynamic Optional Data Augmentation (DODA) to address this intertwined imbalance, i.e., one 'stone' simultaneously 'kills' two 'birds', which allows each class to choose appropriate augmentation methods by maintaining a corresponding augmentation probability distribution for each class during training. Extensive experiments across mainstream long-tailed recognition benchmarks (e.g., CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018) prove the effectiveness and flexibility of the DODA in overcoming the intertwined imbalance.",[],[],"['Binwu Wang', 'Pengkun Wang', 'Wei Xu', 'Xu Wang', 'Yudong Zhang', 'Kun Wang', 'Yang Wang']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Suzhou Institute for Advanced Research, University of Science and Technology of China', 'University of Science and Technology of China', 'Nanyang Technological University', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=XJ9vjEAqbx,Security,Adversarial Training Should Be Cast as a Non-Zero-Sum Game,"One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers.  The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function. Our formulation yields a simple algorithmic framework that matches and in some cases outperforms state-of-the-art attacks, attains comparable levels of robustness to standard adversarial training algorithms, and does not suffer from robust overfitting.",[],[],"['Alexander Robey', 'Fabian Latorre', 'George J. Pappas', 'Hamed Hassani', 'Volkan Cevher']","['Machine Learning, CMU, Carnegie Mellon University', 'Swiss Federal Institute of Technology Lausanne', 'Electrical and Systems Engineering, School of Engineering and Applied Science, University of Pennsylvania', 'University of Pennsylvania', 'EPFL - EPF Lausanne']","['United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=hOMVq57Ce0,Transparency & Explainability,Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning,"Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. Linear policies can justify their decisions while interacting in a dynamic environment, but their reduced expressivity prevents them from solving hard tasks. Instead, we argue for the use of piecewise-linear policies. We carefully study to what extent they can retain the interpretable properties of linear policies while reaching competitive performance with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the decision process of the agent without requiring an additional explanation model. We evaluate HC policies in control and navigation experiments, visualize the improved interpretability of the agent and highlight its trade-off with performance. Moreover, we validate that the restricted model class that the HyperCombinator belongs to is compatible with the algorithmic constraints of various reinforcement learning algorithms.",[],[],"['Maxime Wabartha', 'Joelle Pineau']","['', 'Facebook']","['', 'United States']"
https://openreview.net/forum?id=rzBskAEmoc,Transparency & Explainability,CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images,"The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the Context-Aware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5\%, 95.9\%, and 88.1\%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interpretability by identifying regions of high diagnostic value. Our code is available at https://github.com/olgarithmics/ICLR_CAMIL.",[],[],"['Olga Fourkioti', 'Matt De Vries', 'Chris Bakal']","['The Institute of Cancer Research', 'Cancer Biology, Institute of Cancer Research', 'Cancer Biology, Institute of Cancer Research']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=AqN23oqraW,Fairness & Bias,KoLA: Carefully Benchmarking World Knowledge of Large Language Models,"The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models, and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. The KoLA dataset will be updated every three months to provide timely references for developing LLMs and knowledge-related systems.",[],[],"['Jifan Yu', 'Xiaozhi Wang', 'Shangqing Tu', 'Shulin Cao', 'Daniel Zhang-Li', 'Xin Lv', 'Hao Peng', 'Zijun Yao', 'Xiaohan Zhang', 'Hanming Li', 'Chunyang Li', 'Zheyuan Zhang', 'Yushi Bai', 'Yantao Liu', 'Amy Xin', 'Kaifeng Yun', 'Linlu GONG', 'Nianyi Lin', 'Jianhui Chen', 'Zhili Wu', 'Yunjia Qi', 'Weikai Li', 'Yong Guan', 'Kaisheng Zeng', 'Ji Qi', 'Hailong Jin', 'Jinxin Liu', 'Yu Gu', 'Yuan Yao', 'Ning Ding', 'Lei Hou', 'Zhiyuan Liu', 'Xu Bin', 'Jie Tang', 'Juanzi Li']","['Institute of Education, Tsinghua University', 'Department of Computer Science and Technology, Tsinghua University', 'Department of Computer Science and Technology, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Computer Science and Technology, Tsinghua University, Tsinghua University', 'Zhipu AI', 'Tsinghua University, Tsinghua University', 'Department of Computer Science and Technology, Tsinghua University', 'Beijing Knowledge Atlas Technology Co., Ltd.', 'Tsinghua University, Tsinghua University', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Department of Computer Science and Technology , Tsinghua University', 'Computer science and technology, Tsinghua University, Tsinghua University', 'University of the Chinese Academy of Sciences', 'DCST, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Department of Computer Science and Technology,, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Computer Science and Technology, Tsinghua University', ', Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=ARPrtuzAnQ,Fairness & Bias,On the hardness of learning under symmetries,"We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known  symmetries (""equivariance"") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.",[],[],"['Bobak Kiani', 'Thien Le', 'Hannah Lawrence', 'Stefanie Jegelka', 'Melanie Weber']","['Massachusetts Institute of Technology', 'EECS, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Computer Science, Technische Universität München', '']","['United States', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=jUWktnsplU,Fairness & Bias,Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners,"As two prominent strategies for representation learning, Contrastive Learning (CL) and Masked Image Modeling (MIM) have witnessed significant progress. Previous studies have demonstrated the advantages of each approach in specific scenarios. CL, resembling supervised pre-training, excels at capturing longer-range global patterns and enhancing feature discrimination, while MIM is adept at introducing local and diverse attention across transformer layers. Considering the respective strengths, previous studies utilize feature distillation to inherit both discrimination and diversity. In this paper, we thoroughly examine previous feature distillation methods and observe that the increase in diversity mainly stems from asymmetric designs, which may in turn compromise the discrimination ability. To strike a balance between the two properties, we propose a simple yet effective strategy termed Hybrid Distill, which leverages both the CL and MIM teachers to jointly guide the student model. Hybrid Distill emulates the token relations of the MIM teacher at intermediate layers for diversity, while simultaneously distilling the final features of the CL teacher to enhance discrimination. A progressive redundant token masking strategy is employed to reduce the expenses associated with distillation and aid in preventing the model from converging to local optima. Experimental results demonstrate that Hybrid Distill achieves superior performance on various benchmark datasets.",[],[],"['Bowen Shi', 'XIAOPENG ZHANG', 'Yaoming Wang', 'Jin Li', 'Wenrui Dai', 'Junni Zou', 'Hongkai Xiong', 'Qi Tian']","['Shanghai Jiao Tong University', 'CBG, Huawei Technologies Ltd.', '', 'SEIEE, Shanghai Jiao Tong University', 'Department of Computer Science & Engineering, Shanghai Jiao Tong University', 'Department of Computer Science and Engineering, Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Huawei Technologies Ltd.']","['China', 'China', '', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=8FHWkY0SwF,Fairness & Bias,Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients,"Personalized federated learning (PFL) has gained great success in tackling the scenarios where target datasets are heterogeneous across the local clients. However, the application of the existing PFL methods to real-world setting is hindered by the common assumption that the test data on each client is in-distribution (IND) with respect to its training data. Due to the bias of training dataset, the modern machine learning model prefers to rely on shortcut which can perform well on the training data but fail to generalize to the unseen test data that is out-of-distribution (OOD). This pervasive phenomenon is called shortcut learning and has attracted plentiful efforts in centralized situations. In PFL, the limited data diversity on federated clients makes mitigating shortcut and meanwhile preserving personalization knowledge rather difficult. In this paper, we analyse this challenging problem by formulating the structural causal models (SCMs) for heterogeneous federated clients. From the proposed SCMs, we derive two significant causal signatures which inspire a provable shortcut discovery and removal method under federated learning, namely FedSDR. Specifically, FedSDR is divided into two steps: 1) utilizing the available training data distributed among local clients to discover all the shortcut features in a collaborative manner. 2) developing the optimal personalized causally invariant predictor for each client by eliminating the discovered shortcut features. We provide theoretical analysis to prove that our method can draw complete shortcut features and produce the optimal personalized invariant predictor that can generalize to unseen OOD data on each client. The experimental results on diverse datasets validate the superiority of FedSDR over the state-of-the-art PFL methods on OOD generalization performance.",[],[],"['Xueyang Tang', 'Song Guo', 'Jie ZHANG', 'Jingcai Guo']","['The Hong Kong Polytechnic University', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Computer Science and Engineering, Hong Kong University of Science and Technology', 'Computing, The Hong Kong Polytechnic University']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://openreview.net/forum?id=l1U6sEgYkb,Security,DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation,"Accurate 3D lane estimation is crucial for ensuring safety in autonomous driving. However, prevailing monocular techniques suffer from depth loss and lighting variations, hampering accurate 3D lane detection. In contrast, LiDAR points offer geometric cues and enable precise localization. In this paper, we present DV-3DLane, a novel end-to-end **D**ual-**V**iew multi-modal **3D Lane** detection framework that synergizes the strengths of both images and LiDAR points. We propose to learn multi-modal features in dual-view spaces, *i.e.*, *perspective view* (PV) and *bird's-eye-view* (BEV), effectively leveraging the modal-specific information. To achieve this, we introduce three designs: 1) A bidirectional feature fusion strategy that integrates multi-modal features into each view space, exploiting their unique strengths. 2) A unified query generation approach that leverages lane-aware knowledge from both PV and BEV spaces to generate queries. 3) A 3D dual-view deformable attention mechanism, which aggregates discriminative features from both PV and BEV spaces into queries for accurate 3D lane detection. Extensive experiments on the public benchmark, OpenLane, demonstrate the efficacy and efficiency of DV-3DLane. It achieves state-of-the-art performance, with a remarkable 11.2 gain in F1 score and a substantial 53.5% reduction in errors. Code is available on [github](https://github.com/JMoonr/dv-3dlane).",[],[],"['Yueru Luo', 'Shuguang Cui', 'Zhen Li']","['The Chinese University of Hong Kong', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen', 'ECE, The Chinese University of Hong Kong, Shenzhen']","['Hong Kong', 'Hong Kong', 'Hong Kong']"
https://openreview.net/forum?id=DEJIDCmWOz,Security,On the Reliability of Watermarks for Large Language Models,"As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. _Watermarking_ is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed.  For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a $1\mathrm{e}{-5}$ false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",[],[],"['John Kirchenbauer', 'Jonas Geiping', 'Yuxin Wen', 'Manli Shu', 'Khalid Saifullah', 'Kezhi Kong', 'Kasun Fernando', 'Aniruddha Saha', 'Micah Goldblum', 'Tom Goldstein']","['University of Maryland, College Park', 'ELLIS Institute Tübingen', 'Department of Computer Science, University of Maryland, College Park', 'Salesforce Research', 'CS, University of Maryland, College Park', 'University of Maryland, College Park', 'Scuola Normale Superiore', 'University of Maryland, College Park', 'Electrical Engineering, Columbia University', 'Computer Science, University of Maryland, College Park']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=6p8lpe4MNf,Security,A Semantic Invariant Robust Watermark for Large Language Models,"Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness.",[],[],"['Aiwei Liu', 'Leyi Pan', 'Xuming Hu', 'Shiao Meng', 'Lijie Wen']","['Tsinghua University', 'School of Software, Tsinghua University', 'AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)', 'School of Software, Tsinghua University', 'School of Software, Tsinghua University']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=NW2s5XXwXU,Fairness & Bias,Long-tailed Diffusion Models with Oriented Calibration,"Diffusion models are acclaimed for generating high-quality and diverse images. However, their performance notably degrades when trained on data with a long-tailed distribution. For long tail diffusion model generation, current works focus on the calibration and enhancement of the tail generation with head-tail knowledge transfer. The transfer process relies on the abundant diversity derived from the head class and, more significantly, the condition capacity of the model prediction. However, the dependency on the conditional model prediction to realize the knowledge transfer might exhibit bias during training, leading to unsatisfactory generation results and lack of robustness. Utilizing a Bayesian framework, we develop a weighted denoising score-matching technique for knowledge transfer directly from head to tail classes. Additionally, we incorporate a gating mechanism in the knowledge transfer process. We provide statistical analysis to validate this methodology, revealing that the effectiveness of such knowledge transfer depends on both label distribution and sample similarity, providing the insight to consider sample similarity when re-balancing the label proportion in training. We extensively evaluate our approach with experiments on multiple benchmark datasets, demonstrating its effectiveness and superior performance compared to existing methods. Code: \url{https://github.com/MediaBrain-SJTU/OC_LT}.",[],[],"['Tianjiao Zhang', 'Huangjie Zheng', 'Jiangchao Yao', 'Xiangfeng Wang', 'Mingyuan Zhou', 'Ya Zhang', 'Yanfeng Wang']","['SEIEE, Shanghai Jiao Tong University', 'Machine learning research, Apple', 'CMIC, Shanghai Jiaotong University', 'East China Normal University', 'Google', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'School of Artificial Intelligence, Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', '', 'China', 'China']"
https://openreview.net/forum?id=JewzobRhay,Fairness & Bias,When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations,"Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.",[],[],"['Aleksandar Petrov', 'Philip Torr', 'Adel Bibi']","['Google DeepMind', 'University of Oxford', 'Engineering Science, University of Oxford']","['United States', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=F5dhGCdyYh,Security,Illusory Attacks: Information-theoretic detectability matters in adversarial attacks,"Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs.  Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of information-theoretic detectability constraints makes them \textit{detectable} using automated means or human inspection.  Detectability is undesirable to adversaries as it may trigger security escalations. We introduce \textit{\eattacks{}}, a novel form of adversarial attack on sequential decision-makers that is both effective and of $\epsilon-$bounded statistical detectability.  We propose a novel dual ascent algorithm to learn such attacks end-to-end. Compared to existing attacks, we empirically find \eattacks{} to be significantly harder to detect with automated methods, and a small study with human participants\footnote{IRB approval under reference R84123/RE001} suggests they are similarly harder to detect for humans.  Our findings suggest the need for better anomaly detectors, as well as effective hardware- and system-level defenses. The project website can be found at https://tinyurl.com/illusory-attacks.",[],[],"['Tim Franzmeyer', 'Stephen Marcus McAleer', 'Joao F. Henriques', 'Jakob Nicolaus Foerster', 'Philip Torr', 'Adel Bibi', 'Christian Schroeder de Witt']","['University of Oxford', 'Carnegie Mellon University', 'University of Oxford', 'University of Oxford, University of Oxford', 'University of Oxford', 'Engineering Science, University of Oxford', 'Department of Engineering Science, University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=xC8xh2RSs2,Transparency & Explainability,Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace,"Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light on this question, here we take Hugging Face - one of the largest platforms for sharing and collaborating on ML models and datasets -  as a prominent case study. By analyzing all 7,433 dataset documentation on Hugging Face, our investigation provides an overview of the Hugging Face dataset ecosystem and insights into dataset documentation practices, yielding 5 main findings: (1) The dataset card completion rate shows marked heterogeneity correlated with dataset popularity: While 86.0\% of the top 100 downloaded dataset cards fill out all sections suggested by Hugging Face community, only 7.9\% of dataset cards with no downloads complete all these sections. (2) A granular examination of each section within the dataset card reveals that the practitioners seem to prioritize Dataset Description and Dataset Structure sections, accounting for 36.2\% and 33.6\% of the total card length, respectively, for the most downloaded datasets. In contrast, the Considerations for Using the Data section receives the lowest proportion of content, accounting for just 2.1\% of the text. (3) By analyzing the subsections within each section and utilizing topic modeling to identify key topics, we uncover what is discussed in each section, and underscore significant themes encompassing both technical and social impacts, as well as limitations within the Considerations for Using the Data section. (4) Our findings also highlight the need for improved accessibility and reproducibility of datasets in the Usage sections. (5) In addition, our human annotation evaluation emphasizes the pivotal role of comprehensive dataset content in shaping individuals' perceptions of a dataset card's overall quality. Overall, our study offers a unique perspective on analyzing dataset documentation through large-scale data science analysis and underlines the need for more thorough dataset documentation in machine learning research.",[],[],"['Xinyu Yang', 'Weixin Liang', 'James Zou']","['Cornell University', 'Computer Science, Stanford University', 'Stanford University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=mE52zURNGc,Fairness & Bias,An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment,"Direct image alignment is a widely used technique for relative 6DoF pose estimation between two images, but its accuracy strongly depends on pose initialization. Therefore, recent end-to-end frameworks increase the convergence basin of the learned feature descriptors with special training objectives, such as the Gauss-Newton loss. However, the training data may exhibit bias toward a specific type of motion and pose initialization, thus limiting the generalization of these methods. In this work, we derive a closed-form solution to the expected optimum of the Gauss-Newton loss.  The solution is agnostic to the underlying feature representation and allows us to dynamically adjust the basin of convergence according to our assumptions about the uncertainty in the current estimates. These properties allow for effective control over the convergence in the alignment process. Despite using self-supervised feature embeddings, our solution achieves compelling accuracy w.r.t. the state-of-the-art direct image alignment methods trained end-to-end with pose supervision, and demonstrates improved robustness to pose initialization. Our analytical solution exposes some inherent limitations of end-to-end learning with the Gauss-Newton loss, and establishes an intriguing connection between direct image alignment and feature-matching approaches.",[],[],"['Sergei Solonets', 'Daniil Sinitsyn', 'Lukas Von Stumberg', 'Nikita Araslanov', 'Daniel Cremers']","['Technische Universität München', 'CIT, Department of Informatics, Technische Universität München', 'Valve Corporation', 'Technische Universität München', 'Technical University Munich']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://openreview.net/forum?id=IOEEDkla96,Security,Adversarial Feature Map Pruning for Backdoor,"Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attacks, which are achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender cannot reproduce the trigger successfully then the DNN model will not be repaired, as the trigger is not effectively removed.   In this work, we propose Adversarial Feature Map Pruning for Backdoor (FMP) to mitigate backdoor from the DNN. Unlike existing defense strategies, which focus on reproducing backdoor triggers, FMP attempts to prune backdoor feature maps, which are trained to extract backdoor information from inputs. After pruning these backdoor feature maps, FMP will fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMP can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers (e.g., FMP decreases the ASR to 2.86% in CIFAR10, which is 19.2% to 65.41% lower than baselines). Second, unlike conventional defense methods that tend to exhibit low robust accuracy (that is, the accuracy of the model on poisoned data), FMP achieves a higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks (e.g., FMP obtains 87.40% RA in CIFAR10). Our code is publicly available at: https://github.com/hku-systems/FMP.",[],[],"['Dong HUANG', 'Qingwen Bu']","['The University of Hong Kong', 'Shanghai Jiao Tong University']","['Hong Kong', 'China']"
https://openreview.net/forum?id=pFOoOdaiue,Security,Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula,"Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial RL based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium (QRE), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between the entropy-regularized objective and QRE enables free modulation of the rationality of the agents by simply tuning the temperature coefficient. We leverage this insight to propose our novel algorithm, Quantal Adversarial RL (QARL), which gradually increases the rationality of the adversary in a curriculum fashion until it is fully rational, easing the complexity of the optimization problem while retaining robustness. We provide extensive evidence of QARL outperforming RARL and recent baselines across several MuJoCo locomotion and navigation problems in overall performance and robustness.",[],[],"['Aryaman Reddi', 'Maximilian Tölle', 'Jan Peters', 'Georgia Chalvatzaki', ""Carlo D'Eramo""]","['Informatik, Technische Universität Darmstadt', 'CS Department, TU Darmstadt, Technische Universität Darmstadt', 'Systems AI for Robot Learning, German Research Center for AI', 'PEARL, Technische Universität Darmstadt', 'Institute of Computer Science, Bayerische Julius-Maximilians-Universität Würzburg']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://openreview.net/forum?id=oxjeePpgSP,Security,Backdoor Contrastive Learning via Bi-level Trigger Optimization,"Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., 99\% ASR on ImageNet-100) with a very low poisoning rate (1\%). Besides, our attack can effectively evade existing state-of-the-art defenses.",[],[],"['Weiyu Sun', 'Xinyu Zhang', 'Hao LU', 'Ying-Cong Chen', 'Ting Wang', 'Jinghui Chen', 'Lu Lin']","['Georgia Institute of Technology', 'Nanjing University', 'AI, Hong Kong University of Science and Technology', 'AI, The Hong Kong University of Science and Technology', 'Computer Science, State University of New York at Stony Brook', 'Pennsylvania State University', 'Pennsylvania State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=OqTMUPuLuC,Transparency & Explainability,DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models,"Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability.  Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question.  Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.  Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain.  Project page: https://pjlab-adg.github.io/DiLu/",[],[],"['Licheng Wen', 'Daocheng Fu', 'Xin Li', 'Xinyu Cai', 'Tao MA', 'Pinlong Cai', 'Min Dou', 'Botian Shi', 'Liang He', 'Yu Qiao']","['Shanghai AI Lab', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'Autonomous Driving, Shanghai AI Lab', 'The Chinese University of Hong Kong', 'Frontier Discovery Center, Shanghai Artificial Intelligence Laboratory', 'Shanghai AI Laboratory', 'Shanghai AI Lab', '', '']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', '', '']"
https://openreview.net/forum?id=1OfAO2mes1,Security,Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency,"Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, *i.e.*,  without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimization problem, leveraging a novel SPC-based loss function as the primary optimization objective. Our innovation unfolds in several key aspects. First, we revisit the vanilla SPC method, unveiling its limitations in addressing the proposed backdoor identification problem. Subsequently, we develop a bi-level optimization-based approach to precisely identify backdoor data by minimizing the advanced SPC loss. Finally, we demonstrate the efficacy of our proposal against a spectrum of backdoor attacks, encompassing basic label-corrupted attacks as well as more sophisticated clean-label attacks, evaluated across various benchmark datasets. Experiment results show that our approach often surpasses the performance of current baselines in identifying backdoor data points, resulting in about 4\%-36\% improvement in average AUROC. Codes are available at https://github.com/OPTML-Group/BackdoorMSPC.",[],[],"['Soumyadeep Pal', 'Yuguang Yao', 'Ren Wang', 'Bingquan Shen', 'Sijia Liu']","['University of Alberta', 'Michigan State University', 'Illinois Institute of Technology', 'National University of Singapore', 'CSE, Michigan State University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=OF5x1dzWSS,Security,Doubly Robust Instance-Reweighted Adversarial Training,"Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee.  Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement.",[],[],"['Daouda Sow', 'Sen Lin', 'Zhangyang Wang', 'Yingbin Liang']","['Ohio State University', 'University of Houston', 'University of Texas at Austin', 'ECE, The Ohio State University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=gtkFw6sZGS,Transparency & Explainability,Generative Judge for Evaluating Alignment,"The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding *generality* (i.e., assessing performance across diverse scenarios), *flexibility* (i.e., examining under different protocols), and *interpretability* (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, **Auto-J**, designed to address these challenges. Our model is trained on user queries and LLM-generated responses under massive real-world scenarios and accommodates diverse evaluation protocols (e.g., pairwise response comparison and single-response evaluation) with well-structured natural language critiques. To demonstrate the efficacy of our approach, we construct a new testbed covering 58 different scenarios. Experimentally, **Auto-J** outperforms a series of strong competitors, including both open-source and closed-source models, by a large margin. We also provide detailed analysis and case studies to further reveal the potential of our method and make a variety of resources public at https://github.com/GAIR-NLP/auto-j.",[],[],"['Junlong Li', 'Shichao Sun', 'Weizhe Yuan', 'Run-Ze Fan', 'hai zhao', 'Pengfei Liu']","['Shanghai Jiaotong University', 'Amazon', 'Computer Science Department, New York University', 'Generative AI Research Lab, Shanghai Jiao Tong University', 'School of Computer, Shanghai Jiao Tong University', 'Shanghai Jiaotong University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=SKulT2VX9p,Fairness & Bias,Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach,"Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current methods assume that the true causal graph is given, which is often not true in real-world applications. To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known. The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph (PDAG), specifically, a class of causal DAGs that can be learned from observational data combined with domain knowledge. The PDAG is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy. Results on both simulated and real-world datasets demonstrate the effectiveness of this method.",[],[],"['Aoqi Zuo', 'Yiqing Li', 'Susan Wei', 'Mingming Gong']","['University of Melbourne', 'Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence', 'Econometrics and Business Statistics, Monash University', 'School of mathematics and statistics, University of Melbourne']","['Australia', 'Australia', 'Australia', 'Australia']"
https://openreview.net/forum?id=qe49ybvvPs,Fairness & Bias,Diverse Projection Ensembles for Distributional Reinforcement Learning,"In contrast to classical reinforcement learning, distributional RL algorithms aim to learn the distribution of returns rather than their expected value. Since the nature of the return distribution is generally unknown a priori or arbitrarily complex, a common approach finds approximations within a set of representable, parametric distributions. Typically, this involves a projection of the unconstrained distribution onto the set of simplified distributions. We argue that this projection step entails a strong inductive bias when coupled with neural networks and gradient descent, thereby profoundly impacting the generalization behavior of learned models. In order to facilitate reliable uncertainty estimation through diversity, this work studies the combination of several different projections and representations in a distributional ensemble. We establish theoretical properties of such projection ensembles and derive an algorithm that uses ensemble disagreement, measured by the average $1$-Wasserstein distance, as a bonus for deep exploration. We evaluate our algorithm on the behavior suite benchmark and find that diverse projection ensembles lead to significant performance improvements over existing methods on a wide variety of tasks with the most pronounced gains in directed exploration problems.",[],[],"['Moritz Akiya Zanger', 'Wendelin Boehmer', 'Matthijs T. J. Spaan']","['Delft University of Technology', 'Intelligent Systems Department, Delft University of Technology', '']","['Netherlands', 'Netherlands', '']"
https://openreview.net/forum?id=NddKiWtdUm,Security,Training Socially Aligned Language Models on Simulated Social Interactions,"The goal of social alignment for AI systems is to make sure these models can conduct themselves appropriately following social values. Unlike humans who establish a consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly recite the corpus in social isolation, which causes poor generalization in unfamiliar cases and the lack of robustness under adversarial attacks. In this work, we introduce a new training paradigm that enables LMs to learn from simulated social interactions. Compared with existing methods, our method is much more scalable and efficient, and shows superior performance in alignment benchmarks and human evaluation.",[],[],"['Ruibo Liu', 'Ruixin Yang', 'Chenyan Jia', 'Ge Zhang', 'Diyi Yang', 'Soroush Vosoughi']","['Google DeepMind', 'Georgia Institute of Technology', 'Stanford University', 'ByteDance Inc.', 'Stanford University', 'Dartmouth College']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=tsE5HLYtYg,Security,SafeDreamer: Safe Reinforcement Learning with World Models,"The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce SafeDreamer, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in RL tasks.  Further details can be found in the code repository: https://github.com/PKU-Alignment/SafeDreamer.",[],[],"['Weidong Huang', 'Jiaming Ji', 'Chunhe Xia', 'Borong Zhang', 'Yaodong Yang']","['artificial intelligence, Peking University', 'Institute for AI, Peking University', 'Beijing University of Aeronautics and Astronautics', '', 'Peking University']","['United States', 'United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=PczQtTsTIX,Fairness & Bias,CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity,"Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ's contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",[],[],"['Aditya Bhatt', 'Daniel Palenicek', 'Boris Belousov', 'Max Argus', 'Artemij Amiranashvili', 'Thomas Brox', 'Jan Peters']","['', 'Computer Science, Technische Universität Darmstadt', 'German Research Centre for AI', 'TF, University of Freiburg, Albert-Ludwigs-Universität Freiburg', 'Amazon', 'University of Freiburg', 'Systems AI for Robot Learning, German Research Center for AI']","['', 'Germany', 'Côte d’Ivoire', 'Germany', 'Japan', 'Switzerland', 'Vietnam']"
https://openreview.net/forum?id=ccxD4mtkTU,Security,Can LLM-Generated Misinformation Be Detected?,"The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.",[],[],"['Canyu Chen', 'Kai Shu']","['Computer Science, Illinois Institute of Technology', 'Emory University']","['United States', 'United States']"
https://openreview.net/forum?id=LJWizuuBUy,Security,SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning,"This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for network pruning, capitalizing on the geometric properties of the optimal transport problem. The “swap” of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",[],[],"['Lei You', 'Hei Victor Cheng']","['Technical University of Denmark', 'Aarhus University']","['Denmark', 'Denmark']"
https://openreview.net/forum?id=fB1iiH9xo7,Fairness & Bias,Pre-training LiDAR-based 3D Object Detectors through Colorization,"Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and labels by teaching the model to colorize LiDAR point clouds, equipping it with valuable semantic cues. To tackle challenges arising from color variations and selection bias, we incorporate color as ""context"" by providing ground-truth colors as hints during colorization. Experimental results on the KITTI and Waymo datasets demonstrate GPC's remarkable effectiveness. Even with limited labeled data, GPC significantly improves fine-tuning performance; notably, on just 20% of the KITTI dataset, GPC outperforms training from scratch with the entire dataset.  In sum, we introduce a fresh perspective on pre-training for 3D object detection, aligning the objective with the model's intended role and ultimately advancing the accuracy and efficiency of 3D object detection for autonomous vehicles.",[],[],"['Tai-Yu Pan', 'Chenyang Ma', 'Tianle Chen', 'Cheng Perng Phoo', 'Katie Z Luo', 'Yurong You', 'Mark Campbell', 'Kilian Q Weinberger', 'Bharath Hariharan', 'Wei-Lun Chao']","['Ohio State University', 'University of North Carolina at Chapel Hill', 'Computer Science, Boston University, Boston University', 'Apple', 'Cornell University', 'NVIDIA', 'Mechanical & Aerospace Engineering, Cornell University', 'Cornell University', 'Cornell University', 'Ohio State University']","['United States', 'United States', 'United States', '', 'United States', '', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=krx55l2A6G,Security,Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning,"Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding the client-side detectability of MS attacks were raised, questioning their practicality. In this work, for the first time, we thoroughly study client-side detectability. We first demonstrate that all prior MS attacks are detectable by principled checks, and formulate a necessary set of requirements that a practical MS attack must satisfy. Next, we propose SEER, a novel attack framework that satisfies these requirements. The key insight of SEER is the use of a secret decoder, jointly trained with the shared model. We show that SEER can steal user data from gradients of realistic networks, even for large batch sizes of up to 512 and under secure aggregation. Our work is a promising step towards assessing the true vulnerability of federated learning in real-world settings.",[],[],"['Kostadin Garov', 'Dimitar Iliev Dimitrov', 'Nikola Jovanović', 'Martin Vechev']","['INSAIT', 'INSAIT, Sofia University ""St Kliment Ohridski""', 'ETHZ - ETH Zurich', 'Computer Science, Swiss Federal Institute of Technology']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://openreview.net/forum?id=riNuqYiD66,Fairness & Bias,A Branching Decoder for Set Generation,"Generating a set of text is a common challenge for many NLP applications, for example, automatically providing multiple keyphrases for a document to facilitate user reading. Existing generative models use a sequential decoder that generates a single sequence successively, and the set generation problem is converted to sequence generation via concatenating multiple text into a long text sequence. However, the elements of a set are unordered, which makes this scheme suffer from biased or conflicting training signals. In this paper, we propose a branching decoder, which can generate a dynamic number of tokens at each time-step and branch multiple generation paths. In particular, paths are generated individually so that no order dependence is required. Moreover, multiple paths can be generated in parallel which greatly reduces the inference time. Experiments on several keyphrase generation datasets demonstrate that the branching decoder is more effective and efficient than the existing sequential decoder.",[],[],"['Zixian Huang', 'Gengyang Xiao', 'Yu Gu', 'Gong Cheng']","['Nanjing University', 'nanjing university', 'Computer Science and Engineering, Ohio State University', 'Nanjing University']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=kvByNnMERu,Fairness & Bias,Estimating Shape Distances on Neural Representations with Limited Samples,"Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance—a measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a novel method-of-moments estimator with a tunable bias-variance tradeoff parameterized by an upper bound on bias. We show that this estimator achieves superior performance to standard estimators in simulation and on neural data, particularly in high-dimensional settings. Our theoretical work and estimator thus respectively define and dramatically expand the scope of neural data for which geometric similarity can be accurately measured.",[],[],"['Dean A Pospisil', 'Brett W. Larsen', 'Sarah E Harvey', 'Alex H Williams']","['Princeton University', 'Databricks Mosaic Research', 'Flatiron Institute', 'New York University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=SUUrkC3STJ,Fairness & Bias,VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections,"Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global attention) for every pair of nodes to learn node representation vectors, resulting in quadratic computational costs that are unaffordable for large-scale graph data. Therefore, mini-batch training for graph transformers is a promising direction, but limited samples in each mini-batch can not support effective dense attention to encode informative representations. Facing this bottleneck, (1) we start by assigning each node a token list that is sampled by personalized PageRank (PPR) and then apply standard multi-head self-attention only on this list to compute its node representations. This PPR tokenization method decouples model training from complex graph topological information and makes heavy feature engineering offline and independent, such that mini-batch training of graph transformers is possible by loading each node's token list in batches. We further prove this PPR tokenization is viable as a graph convolution network with a fixed polynomial filter and jumping knowledge. However, only using personalized PageRank may limit information carried by a token list, which could not support different graph inductive biases for model training. To this end, (2) we rewire graphs by introducing multiple types of virtual connections through structure- and content-based super nodes that enable PPR tokenization to encode local and global contexts, long-range interaction, and heterophilous information into each node's token list, and then formalize our $\underline{\textbf{V}}$irtual $\underline{\textbf{C}}$onnection $\underline{\textbf{R}}$anking based $\underline{\textbf{Graph}}$ Trans$\underline{\textbf{former}}$ (VCR-Graphormer). Overall, VCR-Graphormer needs $O(m+klogk)$ complexity for graph tokenization as compared to $O(n^{3})$ of previous works. The [code](https://github.com/DongqiFu/VCR-Graphormer) is provided.",[],[],"['Dongqi Fu', 'Zhigang Hua', 'Yan Xie', 'Jin Fang', 'Si Zhang', 'Kaan Sancak', 'Hao Wu', 'Andrey Malevich', 'Jingrui He', 'Bo Long']","['Meta AI, Meta', 'Monetization AI, Meta', 'Facebook', 'Meta', 'Meta', 'Georgia Institute of Technology', 'Facebook', 'Facebook', 'School of Information Sciences, University of Illinois at Urbana-Champaign', 'Monetization AI, Meta']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Ffjc8ApSbt,Fairness & Bias,Debiased Collaborative Filtering with Kernel-Based Causal Balancing,"Collaborative filtering builds personalized models from the collected user feedback. However, the collected data is observational rather than experimental, leading to various biases in the data, which can significantly affect the learned model. To address this issue, many studies have focused on propensity-based methods to combat the selection bias by reweighting the sample loss, and demonstrate that balancing is important for debiasing both theoretically and empirically. However, there are two questions that still need to be addressed: which function class should be balanced and how to effectively balance that function class? In this paper, we first perform theoretical analysis to show the effect of balancing finite-dimensional function classes on the bias of IPS and DR methods, and based on this, we propose a universal kernel-based balancing method to balance functions on the reproducing kernel Hilbert space. In addition, we propose a novel adaptive causal balancing method during the alternating update between unbiased evaluation and training of the prediction model. Specifically, the prediction loss of the model is projected in the kernel-based covariate function space, and the projection coefficients are used to determine which functions should be prioritized for balancing to reduce the estimation bias. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of the proposed approach.",[],[],"['Haoxuan Li', 'Chunyuan Zheng', 'Yanghao Xiao', 'Peng Wu', 'Zhi Geng', 'Xu Chen', 'Peng Cui']","['Center for Data Science, Peking University', 'meituan', 'University of Chinese Academy of Sciences', 'Beijing Technology and Business University', 'School of Matematics asn Statistics, Beijing Technology and Business University', 'AI, Renmin University of China', 'Tsinghua University, Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=JbcwfmYrob,Transparency & Explainability,SEA: Sparse Linear Attention with Estimated Attention Mask,"The transformer architecture has driven breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, long seqeuences pose a problem due to the quadratic complexity of the attention operation. Previous re- search has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowl- edge from a teacher’s attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches lose interpretability if they cannot produce full attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear at- tention, then subsequently creates a sparse attention matrix with a top-k̂ selection to perform a sparse attention operation. For language modeling tasks (Wikitext2), previous linear and sparse attention methods show roughly two-fold worse per- plexity scores over the quadratic OPT-1.3B baseline, while SEA achieves better perplexity than OPT-1.3B, using roughly half the memory of OPT-1.3B. More- over, SEA maintains an interpretable attention matrix and can utilize knowledge distillation to lower the complexity of existing pretrained transformers. We be- lieve that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory. Code: https://github.com/gmlwns2000/sea-attention",[],[],"['Heejun Lee', 'Jina Kim', 'Jeffrey Willette', 'Sung Ju Hwang']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'AI, Korea Advanced Institute of Science and Technology', '']","['South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=DfPtC8uSot,Security,Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks,"Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed graphs and relate it to the classical definition of adversarial robustness in the graph representation learning literature. Our definition allows us to derive an upper bound of the expected robustness of Graph Convolutional Networks (GCNs) and Graph Isomorphism Networks subject to node feature attacks. Building on these findings, we connect the expected robustness of GNNs to the orthonormality of their weight matrices and consequently propose an attack-independent, more robust variant of the GCN, called the Graph Convolutional Orthonormal Robust Networks (GCORNs). We further introduce a probabilistic method to estimate the expected robustness, which allows us to evaluate the effectiveness of GCORN on several real-world datasets. Experimental experiments showed that GCORN outperforms available defense methods. Our code is publicly available at: https://github.com/Sennadir/GCORN .",[],[],"['Yassine ABBAHADDOU', 'Sofiane ENNADIR', 'Johannes F. Lutzeyer', 'Michalis Vazirgiannis', 'Henrik Boström']","['École Polytechnique', 'EECS, KTH Royal Institute of Technology', 'Ecole Polytechique', 'Ecole Polytechnique, France', 'KTH Royal Institute of Technology, Stockholm, Sweden']","['France', 'France', 'France', 'France', 'France']"
https://openreview.net/forum?id=mzyZ4wzKlM,Security,Expressive Losses for Verified Robustness via Convex Combinations,"In order to train networks for verified adversarial robustness, it is common to over-approximate the worst-case loss over perturbation regions, resulting in networks that attain verifiability at the expense of standard performance. As shown in recent work, better trade-offs between accuracy and robustness can be obtained by carefully coupling adversarial training with over-approximations.  We hypothesize that the expressivity of a loss function, which we formalize as the ability to span a range of trade-offs between lower and upper bounds to the worst-case loss through a single parameter (the over-approximation coefficient), is key to attaining state-of-the-art performance.  To support our hypothesis, we show that trivial expressive losses, obtained via convex combinations between adversarial attacks and IBP bounds, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. We provide a detailed analysis of the relationship between the over-approximation coefficient and performance profiles across different expressive losses, showing that, while expressivity is essential, better approximations of the worst-case loss are not necessarily linked to superior robustness-accuracy trade-offs.",[],[],"['Alessandro De Palma', 'Rudy R Bunel', 'Krishnamurthy Dj Dvijotham', 'M. Pawan Kumar', 'Robert Stanforth', 'Alessio Lomuscio']","['INRIA', 'Deepmind', 'Google DeepMind', 'DeepMind', 'DeepMind', 'Computing, Imperial College London']","['France', 'United States', 'United States', 'United States', 'United States', 'United Kingdom']"
https://openreview.net/forum?id=KgaBScZ4VI,Fairness & Bias,Language Model Cascades: Token-Level Uncertainty And Beyond,"Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. A simple strategy to achieve more favorable cost-quality tradeoffs is cascading: here, a small model is invoked for most “easy” instances, while a few “hard” instances are deferred to the large model. While the principles underpinning effective cascading are well-studied for classification tasks — with deferral based on predicted class uncertainty favored theoretically and practically — a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across different examples. To mitigate the length bias, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.",[],[],"['Neha Gupta', 'Harikrishna Narasimhan', 'Wittawat Jitkrittum', 'Ankit Singh Rawat', 'Aditya Krishna Menon', 'Sanjiv Kumar']","['Google', 'Google', 'Google Research', 'Google', 'Google', 'Google']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=kBTzlxM2J1,Security,Faithful Rule Extraction for Differentiable Rule Learning Models,"There is increasing interest in methods for extracting interpretable rules from ML models trained to solve a wide range of tasks over knowledge graphs (KGs), such as KG completion, node classification, question answering and recommendation. Many such approaches, however, lack formal guarantees establishing the precise relationship between the model and the extracted rules, and this lack of assurance becomes especially problematic when the extracted rules are applied in safety-critical contexts or to ensure compliance with legal requirements. Recent research has examined whether the rules derived from the influential Neural-LP model exhibit soundness (or completeness), which means that the results obtained by applying the model to any dataset always contain (or are contained in) the results obtained by applying the rules to the same dataset. In this paper, we extend this analysis to the context of DRUM, an approach that has demonstrated superior practical performance. After observing that the rules currently extracted from a DRUM model can be unsound and/or incomplete, we propose a novel algorithm where the output rules, expressed in an extension of Datalog, ensure both soundness and completeness. This algorithm, however, can be inefficient in practice and hence we propose additional constraints to DRUM models facilitating rule extraction, albeit at the expense of reduced expressive power.",[],[],"['Xiaxia Wang', 'David Jaime Tena Cucala', 'Bernardo Cuenca Grau', 'Ian Horrocks']","['Department of Computer Science, University of Oxford', 'Department of Computer Science, Royal Holloway University of London', 'Computer Science, University of Oxford', 'University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=PQY2v6VtGe,Transparency & Explainability,Confidential-DPproof: Confidential Proof of Differentially Private Training,"Post hoc privacy auditing techniques can be used to test the privacy guarantees of a model, but come with several limitations: (i) they can only establish lower bounds on the privacy loss, (ii) the intermediate model updates and some data must be shared with the auditor to get a better approximation of the privacy loss, and (iii) the auditor typically faces a steep computational cost to run a large number of attacks. In this paper, we propose to proactively generate a cryptographic certificate of privacy during training to forego such auditing limitations. We introduce Confidential-DPproof , a framework for Confidential Proof of Differentially Private Training, which enhances training with a certificate of the $(\varepsilon,\delta)$-DP guarantee achieved. To obtain this certificate without revealing information about the training data or model, we design a customized zero-knowledge proof protocol tailored to the requirements introduced by differentially private training, including random noise addition and privacy amplification by subsampling. In experiments on CIFAR-10, Confidential-DPproof trains a model achieving state-of-the-art $91$% test accuracy with a certified privacy guarantee of $(\varepsilon=0.55,\delta=10^{-5})$-DP in approximately 100 hours.",[],[],"['Ali Shahin Shamsabadi', 'Gefei Tan', 'Tudor Ioan Cebere', 'Aurélien Bellet', 'Hamed Haddadi', 'Nicolas Papernot', 'Xiao Wang', 'Adrian Weller']","['Brave Software, Brave Software', '', 'INRIA', 'INRIA', 'brave', 'University of Toronto', 'Northwestern University', 'Alan Turing Institute']","['France', '', 'France', 'France', 'France', 'Canada', 'United States', 'United States']"
https://openreview.net/forum?id=ArpwmicoYW,Fairness & Bias,FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis,"Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across sub-groups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalisation gap. To manage this tradeoff, we propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets. The code is available at https://github.com/Raman1121/FairTune.",[],[],"['Raman Dutt', 'Ondrej Bohdal', 'Sotirios A. Tsaftaris', 'Timothy Hospedales']","['School of Informatics, University of Edinburgh, University of Edinburgh', 'Samsung Research', 'University of Edinburgh', 'University of Edinburgh']","['United Kingdom', '', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=vESNKdEMGp,Security,Multilingual Jailbreak Challenges in Large Language Models,"While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\% for ChatGPT and 40.71\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation.  Data is available at \url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",[],[],"['Yue Deng', 'Wenxuan Zhang', 'Sinno Jialin Pan', 'Lidong Bing']","['School of Computer Science and  Engineering, Nanyang Technological University', 'Alibaba Group', 'Nanyang Technological University', 'DAMO, Alibaba Group']","['Singapore', 'China', 'Singapore', 'Mali']"
https://openreview.net/forum?id=bxITGFPVWh,Security,Sharpness-Aware Data Poisoning Attack,"Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the  uncertainty of the re-training process after the injection of poisoning samples. It includes the uncertainty of training initialization, algorithm and model architecture. To address this challenge, we propose a new strategy called **Sharpness-Aware Data Poisoning Attack (SAPA)**. In particular, it leverages the concept of DNNs' loss landscape sharpness to optimize the poisoning effect on the (approximately) worst re-trained model. Extensive experiments demonstrate that SAPA offers a general and principled strategy that significantly enhances various types of poisoning attacks against various types of re-training uncertainty.",[],[],"['Pengfei He', 'Han Xu', 'Jie Ren', 'Yingqian Cui', 'Shenglai Zeng', 'Hui Liu', 'Charu C. Aggarwal', 'Jiliang Tang']","['Michigan State University', 'University of Arizona', 'Amazon', 'Search Experience Science, Amazon', 'CSE, Michigan State University', 'Michigan State University', 'International Business Machines', 'Michigan State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=BPHcEpGvF8,Security,Demystifying Poisoning Backdoor Attacks from a Statistical Perspective,"Backdoor attacks pose a significant security risk to machine learning applications due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper derives a fundamental understanding of backdoor attacks that applies to both discriminative and generative models, including diffusion models and large language models. We evaluate the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. We demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios. Our code is available \href{https://github.com/KeyWgh/DemystifyBackdoor}{here}.",[],[],"['Ganghua Wang', 'Xun Xian', 'Ashish Kundu', 'Jayanth Srinivasa', 'Xuan Bi', 'Mingyi Hong', 'Jie Ding']","['Data Science, University of Chicago', 'College of Science & Engineering, University of Minnesota, Minneapolis', '', 'Cisco', 'University of Minnesota - Twin Cities', 'AGI, Amazon', 'University of Minnesota - Twin Cities']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=BGkqypmGvm,Fairness & Bias,A 2-Dimensional State Space Layer for Spatial Inductive Bias,"A central objective in computer vision is to design models with appropriate 2-D inductive bias. Desiderata for 2-D inductive bias include two-dimensional position awareness, dynamic spatial locality, and translation and permutation invariance. To address these goals, we leverage an expressive variation of the multidimensional State Space Model (SSM). Our approach introduces efficient parameterization, accelerated computation, and a suitable normalization scheme. Empirically, we observe that incorporating our layer at the beginning of each transformer block of Vision Transformers (ViT), as well as when replacing the Conv2D filters of ConvNeXT with our proposed layers significantly  enhances performance for multiple backbones and across multiple datasets. The new layer is effective even with a negligible amount of additional parameters and inference time. Ablation studies and visualizations demonstrate that the layer has a strong 2-D inductive bias. For example, vision transformers equipped with our layer exhibit effective performance even without positional encoding. Our code is attached as supplementary.",[],[],"['Ethan Baron', 'Itamar Zimerman', 'Lior Wolf']","['Tel Aviv University', 'Tel Aviv University', 'Tel Aviv University']","['Israel', 'Israel', 'Israel']"
https://openreview.net/forum?id=AZGIwqCyYY,Fairness & Bias,Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning,"Recent advances in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. While effective, these methods are limited to the system domain, where the type of system remains consistent and thus cannot ensure the adaptation to new, or unseen physical systems governed by different laws. For instance, a neural network trained on a mass-spring system cannot guarantee accurate predictions for the behavior of a two-body system or any other system with different physical laws. In this work, we take a significant leap forward by targeting cross domain generalization within the field of Hamiltonian dynamics.  We model our system with a graph neural network (GNN) and employ a meta learning algorithm to enable the model to gain experience over a distribution of systems and make it adapt to new physics. Our approach aims to learn a unified Hamiltonian representation that is generalizable across multiple system domains, thereby overcoming the limitations of system-specific models.  We demonstrate that the meta-trained model captures the generalized Hamiltonian representation that is consistent across different physical domains. Overall, through the use of meta learning, we offer a framework that achieves cross domain generalization, providing a step towards a unified model for understanding a wide array of dynamical systems via deep learning.",[],[],"['Yeongwoo Song', 'Hawoong Jeong']","['Department of Physics, Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology']","['South Korea', 'South Korea']"
https://openreview.net/forum?id=BKXvPDekud,Fairness & Bias,CellPLM: Pre-training of Cell Language Model Beyond Single Cells,"The current state-of-the-art single-cell pre-trained models are greatly inspired by the success of large language models. They trained transformers by treating genes as tokens and cells as sentences. However, three fundamental differences between single-cell data and natural language data are overlooked: (1) scRNA-seq data are presented as bag-of-genes instead of sequences of RNAs; (2) Cell-cell relations are more intricate and important than inter-sentence relations; and (3) The quantity of single-cell data is considerably inferior to text data, and they are very noisy. In light of these characteristics, we propose a new pre-trained model, $\textit{CellPLM}$, which takes cells as tokens and tissues as sentences. In addition, we leverage spatially-resolved transcriptomic data in pre-training to facilitate learning cell-cell relationships and introduce a Gaussian prior distribution as an additional inductive bias to overcome data limitations. $\textit{CellPLM}$ is the first single-cell pre-trained transformer that encodes cell-cell relations and it consistently outperforms existing pre-trained and non-pre-trained models in diverse downstream tasks, with 100 times higher inference speed on generating cell embeddings than previous pre-trained models.",[],[],"['Hongzhi Wen', 'Wenzhuo Tang', 'Xinnan Dai', 'Jiayuan Ding', 'Wei Jin', 'Yuying Xie', 'Jiliang Tang']","['Michigan State University', 'Computer Science, Michigan State University', 'Michigan State University', 'University of Southern California', 'Emory University', 'Michigan State University', 'Michigan State University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=HX5ujdsSon,Fairness & Bias,In-Context Learning through the Bayesian Prism,"In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.",[],[],"['Madhur Panwar', 'Kabir Ahuja', 'Navin Goyal']","['EPFL - EPF Lausanne', 'Microsoft', 'Microsoft']","['Switzerland', 'United States', 'United States']"
https://openreview.net/forum?id=7UhxsmbdaQ,Transparency & Explainability,Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design,"Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration shows that improvements to explainability and sample efficiency for molecular design can be made synergistic.",[],[],"['Jeff Guo', 'Philippe Schwaller']","['EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne']","['Switzerland', 'Switzerland']"
https://openreview.net/forum?id=HE9eUQlAvo,Transparency & Explainability,"""What Data Benefits My Classifier?"" Enhancing Model Performance and Interpretability through Influence-Based Data Selection","Classification models are ubiquitously deployed in society and necessitate high utility, fairness, and robustness performance. Current research efforts mainly focus on improving model architectures and learning algorithms on fixed datasets to achieve this goal. In contrast, in this paper, we address an orthogonal yet crucial problem: given a fixed convex learning model (or a convex surrogate for a non-convex model) and a function of interest, we assess what data benefits the model by interpreting the feature space, and then aim to improve performance as measured by this function. To this end, we propose the use of influence estimation models for interpreting the classifier's performance from the perspective of the data feature space. Additionally, we propose data selection approaches based on influence that enhance model utility, fairness, and robustness. Through extensive experiments on synthetic and real-world datasets, we validate and demonstrate the effectiveness of our approaches not only for conventional classification scenarios, but also under more challenging scenarios such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning.",[],[],"['Anshuman Chhabra', 'Peizhao Li', 'Prasant Mohapatra', 'Hongfu Liu']","['Computer Science and Engineering, University of South Florida', 'Google', 'Computer Science and Engineering, University of South Florida', 'Brandeis University']","['United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=52fz5sUAy2,Fairness & Bias,Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference,"Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect. To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference, and introduces a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect. We further develop two new estimators for estimating the proposed ideal loss. We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effects are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods.",[],[],"['Haoxuan Li', 'Chunyuan Zheng', 'Sihao Ding', 'Peng Wu', 'Zhi Geng', 'Fuli Feng', 'Xiangnan He']","['Center for Data Science, Peking University', 'meituan', 'University of Science and Technology of China', 'Beijing Technology and Business University', 'School of Matematics asn Statistics, Beijing Technology and Business University', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=vY9nzQmQBw,Transparency & Explainability,Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis,"Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced.",[],[],['Hubert Siuzdak'],[''],['']
https://openreview.net/forum?id=ANvmVS2Yr0,Fairness & Bias,Generalization in diffusion models arises from geometry-adaptive harmonic representations,"Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the ""true"" continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough.  In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.",[],[],"['Zahra Kadkhodaie', 'Florentin Guth', 'Eero P Simoncelli', 'Stéphane Mallat']","['CCN, Flatiron Institute', 'New York University', 'New York University', 'College de France']","['Brazil', 'United States', 'United States', 'France']"
https://openreview.net/forum?id=86zAUE80pP,Privacy & Data Governance,CPPO: Continual Learning for Reinforcement Learning with Human Feedback,"The approach of Reinforcement Learning from Human Feedback (RLHF) is widely used for enhancing pre-trained Language Models (LM), enabling them to better align with human preferences. Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is often impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns. To address this limitation, we propose Continual Proximal Policy Optimization (CPPO), a novel method that is able to continually align LM with dynamic human preferences. Specifically, CPPO adopts a weighting strategy to decide which samples should be utilized for enhancing policy learning and which should be used for solidifying past experiences. This seeks a good trade-off between policy learning and knowledge retention. Our experimental results show that CPPO outperforms strong Continuous learning (CL) baselines when it comes to consistently aligning with human preferences. Furthermore, compared to PPO, CPPO offers more efficient and stable learning in non-continual scenarios.",[],[],"['Han Zhang', 'Yu Lei', 'Lin Gui', 'Min Yang', 'Yulan He', 'Hui Wang', 'Ruifeng Xu']","['Harbin Institute of Technology', 'Peng Cheng Laboratory, Shenzhen, China', ""King's College London, University of London"", 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences', ""King's College London, University of London"", 'Department of System Engineering, Cloud Computing', 'School of Computer Science and Technology, Harbin Institute of Technology']","['China', 'China', 'China', '', 'China', '', 'China']"
https://openreview.net/forum?id=vZ6r9GMT1n,Security,Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks,"Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures.",[],[],"['Nguyen Hung-Quang', 'Yingjie Lao', 'Tung Pham', 'Kok-Seng Wong', 'Khoa D Doan']","['VinUniversity', 'Tufts University', 'Machine Learning, VinAI Research', 'VinUniversity', 'College of Engineering and Computer Science, VinUniversity']","['Ukraine', 'United States', 'India', 'Ukraine', 'United States']"
https://openreview.net/forum?id=pxI5IPeWgW,Transparency & Explainability,ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference,"Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has mostly relied on neural networks for inference and simultaneous correction of assignment bias. New approaches typically build on top of previous approaches by proposing new (or refined) architectures and learning algorithms. However, the end result—a neural-network-based inference machine—remains unchallenged. In this paper, we introduce a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While we still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. Doing so yields several advantages such as interpretability, irregular sampling, and a different set of identification assumptions. Above all, we consider the introduction of a completely new type of solution to be our most important contribution as it may spark entirely new innovations in treatment effects in general. We facilitate this by formulating our contribution as a framework that can transform any ODE discovery method into a treatment effects method.",[],[],"['Krzysztof Kacprzyk', 'Samuel Holt', 'Jeroen Berrevoets', 'Zhaozhi Qian', 'Mihaela van der Schaar']","['', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","['', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=0BqyZSWfzo,Transparency & Explainability,One-shot Empirical Privacy Estimation for Federated Learning,"Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks, model architectures, or DP algorithm, and/or require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel “one-shot” approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the model architecture, task, or DP algorithm. We show that our method provides provably correct estimates for the privacy loss under the Gaussian mechanism, and we demonstrate its performance on a well-established FL benchmark dataset under several adversarial threat models.",[],[],"['Galen Andrew', 'Peter Kairouz', 'Sewoong Oh', 'Alina Oprea', 'Hugh Brendan McMahan', 'Vinith Menon Suriyakumar']","['Google', 'Google', 'Allen school, University of Washington', 'Computer Science, Northeastern University', 'Google', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=TYXtXLYHpR,Transparency & Explainability,Towards Transparent Time Series Forecasting,"Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in high-stakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. In particular, currently used bottom-up approaches that focus on the values of the time series at specific time points (usually regularly spaced) do not provide a holistic understanding of the entire time series. This limits the applicability of ML in many critical areas. To open up these domains for ML, we propose a top-down framework of bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. Applying this framework, we develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. Through a series of experiments, we demonstrate the efficacy and interpretability of our approach, paving the way for more transparent and reliable applications of ML in various domains.",[],[],"['Krzysztof Kacprzyk', 'Tennison Liu', 'Mihaela van der Schaar']","['', 'University of Cambridge', 'University of Cambridge']","['', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=Gf15GsnfTy,Fairness & Bias,REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes,"Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.",[],[],"['David Ireland', 'Giovanni Montana']","['University of Warwick', 'The University of Warwick']","['United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=cc8h3I3V4E,Fairness & Bias,Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization,"We propose the first loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms  with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches.",[],[],"['Ian Gemp', 'Luke Marris', 'Georgios Piliouras']","['Google DeepMind', 'DeepMind', 'Google DeepMind']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=bWzxhtl1HP,Fairness & Bias,Exploring Diffusion Time-steps for Unsupervised Representation Learning,"Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. Specifically, the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, e.g., fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). To disentangle the modular attributes, at each time-step t, we learn a t-specific feature to compensate for the newly lost attribute, and the set of all {1,...,t}-specific features, corresponding to the cumulative set of lost attributes, are trained to make up for the reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and Bedroom datasets, the learned feature significantly improves attribute classification and enables faithful counterfactual generation, e.g., interpolating only one specified attribute between two images, validating the disentanglement quality. Codes are in https://github.com/yue-zhongqi/diti.",[],[],"['Zhongqi Yue', 'Jiankun Wang', 'Qianru Sun', 'Lei Ji', 'Eric I-Chao Chang', 'Hanwang Zhang']","['College of Computing and Data Science, Nanyang Technological University', 'School of Computer Science and  Engineering, Nanyang Technological University', 'School of Computing and Information Systems, Singapore Management University', 'Multimodal, Research, Microsoft', 'Massachusetts Institute of Technology', 'computer science, Nanyang Technological University']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://openreview.net/forum?id=uQBW7ELXfO,Security,Unpaired Image-to-Image Translation via Neural Schrödinger Bridge,"Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. While diffusion models have achieved remarkable progress, they have limitations in unpaired image-to-image (I2I) translation tasks due to the Gaussian prior assumption. Schrödinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. Yet, to our best knowledge, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose Unpaired Neural Schrödinger Bridge (UNSB), which expresses the SB problem as a sequence of adversarial learning problems. This allows us to incorporate advanced discriminators and regularization to learn a SB between unpaired data. We show that UNSB is scalable and successfully solves various unpaired I2I translation tasks. Code: \url{https://github.com/cyclomon/UNSB}",[],[],"['Beomsu Kim', 'Gihyun Kwon', 'Kwanyoung Kim', 'Jong Chul Ye']","['Korea Advanced Institute of Science & Technology', 'Bio&Brain Engineering, Korea Advanced Institute of Science & Technology', 'Visual Intelligence Team, Samsung Research', 'Graduate School of AI, Korea Advanced Institute of Science and Technology']","['South Korea', 'South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=VrHiF2hsrm,Security,Understanding Catastrophic Forgetting in Language Models via Implicit Inference,"We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.",[],[],"['Suhas Kotha', 'Jacob Mitchell Springer', 'Aditi Raghunathan']","['Stanford University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=3Y7r6xueJJ,Fairness & Bias,Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline,"Most continual learning (CL) algorithms have focused on tackling the stability-plasticity dilemma, that is, the challenge of preventing the forgetting of past tasks while learning new ones. However, we argue that they have overlooked the impact of knowledge transfer when the training dataset of a certain task is biased — namely, when the dataset contains some spurious correlations that can overly influence the prediction rule of a model. In that case, how would the dataset bias of a certain task affect the prediction rules of a CL model for future or past tasks? In this work, we carefully design systematic experiments using three benchmark datasets to answer the question from our empirical findings. Specifically, we first show through two-task CL experiments that standard CL methods, which are oblivious of the dataset bias, can transfer bias from one task to another, both forward and backward. Moreover, we find out this transfer is exacerbated depending on whether the CL methods focus on stability or plasticity. We then present that the bias is also transferred and even accumulates in longer task sequences. Finally, we offer a standardized experimental setup and a simple, yet strong plug-in baseline method, dubbed as group-class Balanced Greedy Sampling (BGS), which are utilized for the development of more advanced bias-aware CL methods.",[],[],"['Donggyu Lee', 'Sangwon Jung', 'Taesup Moon']","['Sungkyunkwan University', 'Electronical and Electric Engineering, Seoul National University', 'Seoul National University']","['', 'United States', 'United States']"
https://openreview.net/forum?id=3qo1pJHabg,Security,LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks,"Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.",[],[],"['Jianlang Chen', 'Xuhong Ren', 'Qing Guo', 'Felix Juefei-Xu', 'Di Lin', 'Wei Feng', 'Lei Ma', 'Jianjun Zhao']","['', 'Tianjin University of Technology', 'National University of Singapore', 'GenAI, Meta', 'College of Intelligence and Computing, Tianjin University', 'Tianjin University', 'Department of Computer Science, The University of Tokyo', 'Kyushu University']","['', 'China', 'Singapore', 'Japan', 'China', 'China', 'Japan', 'Japan']"
https://openreview.net/forum?id=cObFETcoeW,Transparency & Explainability,Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark,"Saliency-based representation visualization (SRV) ($e.g.$, Grad-CAM) is one of the most classical and widely adopted explainable artificial intelligence (XAI) methods for its simplicity and efficiency. It can be used to interpret deep neural networks by locating saliency areas contributing the most to their predictions. However, it is difficult to automatically measure and evaluate the performance of SRV methods due to the lack of ground-truth salience areas of samples. In this paper, we revisit the backdoor-based SRV evaluation, which is currently the only feasible method to alleviate the previous problem. We first reveal its \emph{implementation limitations} and \emph{unreliable nature} due to the trigger generalization of existing backdoor watermarks. Given these findings, we propose a generalization-limited backdoor watermark (GLBW), based on which we design a more faithful XAI evaluation. Specifically, we formulate the training of watermarked DNNs as a min-max problem, where we find the `worst' potential trigger (with the highest attack effectiveness and differences from the ground-truth trigger) via inner maximization and minimize its effects and the loss over benign and poisoned samples via outer minimization in each iteration. In particular, we design an adaptive optimization method to find desired potential triggers in each inner maximization. Extensive experiments on benchmark datasets are conducted, verifying the effectiveness of our generalization-limited watermark. Our codes are available at \url{https://github.com/yamengxi/GLBW}.",[],[],"['Mengxi Ya', 'Yiming Li', 'Tao Dai', 'Bin Wang', 'Yong Jiang', 'Shu-Tao Xia']","['Tsinghua University, Tsinghua University', 'Nanyang Technological University', 'Department of Software Engineering, Shenzhen University', 'Tsinghua University, Tsinghua University', 'Data and Information Science, Tsinghua University', 'Division of Information Science and Technology, Shenzhen International Graduate School, Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=g7ohDlTITL,Fairness & Bias,Flow Matching on General Geometries,"We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",[],[],"['Ricky T. Q. Chen', 'Yaron Lipman']","['FAIR Labs, Meta AI', '']","['United States', 'United States']"
https://openreview.net/forum?id=9hjVoPWPnh,Privacy & Data Governance,Machine Unlearning for Image-to-Image Generative Models,"Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples.  Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models.",[],[],"['Guihong Li', 'Hsiang Hsu', 'Chun-Fu Chen', 'Radu Marculescu']","['Advanced Micro Devices', 'JP Morgan & Chase Bank', 'JPMorganChase, GTAR', 'University of Texas, Austin']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=wYvuY60SdD,Fairness & Bias,Mixture of Weak and Strong Experts on Graphs,"Realistic graphs contain both (1) rich self-features of nodes and  (2) informative structures of neighborhoods, jointly handled by a Graph Neural Network (GNN) in the typical setup. We propose to decouple the two modalities by **M**ixture **o**f **w**eak and **st**rong experts (**Mowst**), where the weak expert is a light-weight Multi-layer Perceptron (MLP), and the strong expert is an off-the-shelf GNN. To adapt the experts' collaboration to different target nodes, we propose a ""confidence"" mechanism based on the dispersion of the weak expert's prediction logits. The strong expert is conditionally activated in the low-confidence region when either the node's classification relies on neighborhood information, or the weak expert has low model quality. We reveal interesting training dynamics by analyzing the influence of the confidence function on loss: our training algorithm encourages the specialization of each expert by effectively generating soft splitting of the graph. In addition, our ""confidence"" design imposes a desirable bias toward the strong expert to benefit from GNN's better generalization capability. Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN. Empirically, Mowst on 4 backbone GNN architectures show significant accuracy improvement on 6 standard node classification benchmarks, including both homophilous and heterophilous graphs (https://github.com/facebookresearch/mowst-gnn).",[],[],"['Hanqing Zeng', 'Hanjia Lyu', 'Diyi Hu', 'Yinglong Xia', 'Jiebo Luo']","['Meta AI', 'Department of Computer Science, University of Rochester', 'University of Southern California', 'Meta', 'University of Rochester']","['United States', 'United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=Xz13DtbOVW,Transparency & Explainability,Balancing Act: Constraining Disparate Impact in Sparse Models,"Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that _directly addresses the disparate impact of pruning_: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.",[],[],"['Meraj Hashemizadeh', 'Juan Ramirez', 'Rohan Sukumaran', 'Golnoosh Farnadi', 'Simon Lacoste-Julien', 'Jose Gallego-Posada']","['University of Montreal', 'DIRO, University of Montreal', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'School of Computer Science, McGill University', 'University of Montreal', 'University of Montreal']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=8OxL034uEr,Fairness & Bias,MgNO: Efficient Parameterization of Linear Operators via Multigrid,"In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) =  \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons.  This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators. Moreover, it seamlessly accommodates diverse boundary conditions. Our empirical observations reveal that MgNO exhibits superior ease of training compared to CNN-based models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators. We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).",[],[],"['Juncai He', 'Xinliang Liu', 'Jinchao Xu']","['King Abdullah University of Science and Technology', 'King Abdullah University of Science and Technology', 'King Abdullah University of Science and Technology']","['Saudi Arabia', 'Saudi Arabia', 'Saudi Arabia']"
https://openreview.net/forum?id=UMfcdRIotC,Security,Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals,"Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",[],[],"['Yair Ori Gat', 'Nitay Calderon', 'Amir Feder', 'Alexander Chapanin', 'Amit Sharma', 'Roi Reichart']","['Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'Technion - Israel Institute of Technology', 'Columbia University', 'Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'Microsoft Research', 'Data and Decision Sciences , Technion, Israel Institute of Technology']","['Israel', 'Israel', 'Israel', 'Israel', '', '']"
https://openreview.net/forum?id=jFa5KESW65,Security,IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks,"We introduce a novel approach to counter adversarial attacks, namely, image resampling. Image resampling transforms a discrete image into a new one, simulating the process of scene recapturing or rerendering as specified by a geometrical transformation. The underlying rationale behind our idea is that image resampling can alleviate the influence of adversarial perturbations while preserving essential semantic information, thereby conferring an inherent advantage in defending against adversarial attacks. To validate this concept, we present a comprehensive study on leveraging image resampling to defend against adversarial attacks. We have developed basic resampling methods that employ interpolation strategies and coordinate shifting magnitudes. Our analysis reveals that these basic methods can partially mitigate adversarial attacks. However, they come with apparent limitations: the accuracy of clean images noticeably decreases, while the improvement in accuracy on adversarial examples is not substantial.We propose implicit representation-driven image resampling (IRAD) to overcome these limitations. First, we construct an implicit continuous representation that enables us to represent any input image within a continuous coordinate space. Second, we introduce SampleNet, which automatically generates pixel-wise shifts for resampling in response to different inputs. Furthermore, we can extend our approach to the state-of-the-art diffusion-based method, accelerating it with fewer time steps while preserving its defense capability. Extensive experiments demonstrate that our method significantly enhances the adversarial robustness of diverse deep models against various attacks while maintaining high accuracy on clean images.",[],[],"['Yue Cao', 'Tianlin Li', 'Xiaofeng Cao', 'Ivor Tsang', 'Yang Liu', 'Qing Guo']","['National Technological University', 'Nanyang Technological University', 'AI School, Jilin University', 'A*STAR', 'CCDS, Nanyang Technological University', 'National University of Singapore']","['United States', 'Singapore', 'China', 'United States', 'Singapore', 'Singapore']"
https://openreview.net/forum?id=O9nZCwdGcG,Fairness & Bias,Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values,"Multivariate time series forecasting plays an important role in various applications ranging from meteorology study, traffic management to economics planning. In the past decades, many efforts have been made toward accurate and reliable forecasting methods development under the assumption of intact input data. However, the time series data from real-world scenarios is often partially observed due to device malfunction or costly data acquisition, which can seriously impede the performance of the existing approaches. A naive employment of imputation methods unavoidably involves error accumulation and leads to suboptimal solutions. Motivated by this, we propose a Biased Temporal Convolution Graph Network that jointly captures the temporal dependencies and spatial structure. In particular, we inject bias into the two carefully developed modules, the Multi-Scale Instance PartialTCN and Biased GCN, to account for missing patterns. The experimental results show that our proposed model is able to achieve up to $9.93$\% improvements over the existing methods on five real-world benchmark datasets. Our code is available at: https://github.com/chenxiaodanhit/BiTGraph.",[],[],"['Xiaodan Chen', 'Xiucheng Li', 'Bo Liu', 'Zhijun Li']","['Computer Science and Technology, Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Computer Science, Harbin Institute of Technology']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=FAGtjl7HOw,Transparency & Explainability,Explaining Kernel Clustering via Decision Trees,"Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model.",[],[],"['Maximilian Fleissner', 'Leena Chennuru Vankadara', 'Debarghya Ghoshdastidar']","['', 'Amazon Development Center Germany', 'Technical University Munich']","['', 'Germany', 'Romania']"
https://openreview.net/forum?id=FHqAzWl2wE,Fairness & Bias,Multimarginal Generative Modeling with Stochastic Interpolants,"Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption.  In addition, the multimarginal perspective enables an efficient algorithm for optimizing the dynamical transport cost in the ordinary two-marginal setting. We demonstrate these capacities with several numerical examples.",[],[],"['Michael Samuel Albergo', 'Nicholas Matthew Boffi', 'Michael Lindsey', 'Eric Vanden-Eijnden']","['Society of Fellows, Harvard University', 'NYU, New York University', 'University of California, Berkeley', 'New York University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=DRu8PMHgCh,Privacy & Data Governance,FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning,"Federated Learning (FL) is an important privacy-preserving learning paradigm that plays an important role in the Intelligent Internet of Things. Training a global model in FL, however, is vulnerable to the noise in the heterogeneous data across the clients. In this paper, we introduce **FedTrans**, a novel client-transparent client utility estimation method designed to guide client selection for noisy scenarios, mitigating performance degradation problems. To estimate the client utility, we propose a Bayesian framework that models client utility and its relationships with the weight parameters and the performance of local models. We then introduce a variational inference algorithm to effectively infer client utility, given only a small amount of auxiliary data. Our evaluation demonstrates that leveraging FedTrans as a guide for client selection can lead to a better accuracy performance (up to 7.8\%), ensuring robustness in noisy scenarios.",[],[],"['Mingkun Yang', 'Ran Zhu', 'Qing Wang', 'Jie Yang']","['EEMCS, Delft University of Technology', 'EEMCS, Delft University of Technology', 'Department of Software Technology, Delft University of Technology', 'Delft University of Technology']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://openreview.net/forum?id=jKTUlxo5zy,Transparency & Explainability,Less is More: Fewer Interpretable Region via Submodular Subset Selection,"Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate small interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9\% and 2.5\% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0\% and 18.4\% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at https://github.com/RuoyuChen10/SMDL-Attribution.",[],[],"['Ruoyu Chen', 'Hua Zhang', 'Siyuan Liang', 'Jingzhi Li', 'Xiaochun Cao']","['Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'National University of Singapore', 'Institute information of engineering, chinese academy of sciences', 'School of Cyber Science and Technology, SUN YAT-SEN UNIVERSITY']","['China', 'China', 'Singapore', 'China', 'China']"
https://openreview.net/forum?id=iS5ADHNg2A,Fairness & Bias,Deceptive Fairness Attacks on Graphs via Meta Learning,"We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity or individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.",[],[],"['Jian Kang', 'Yinglong Xia', 'Ross Maciejewski', 'Jiebo Luo', 'Hanghang Tong']","['University of Rochester', 'Meta', 'Arizona State University', 'University of Rochester', 'computer science, University of Illinois at Urbana-Champaign']","['United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=rIx1YXVWZb,Transparency & Explainability,Understanding Addition in Transformers,"Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper provides a comprehensive analysis of a one-layer Transformer model trained to perform n-digit integer addition. Our findings suggests that the model dissects the task into parallel streams dedicated to individual digits, employing varied algorithms tailored to different positions within the digits. Furthermore, we identify a rare scenario characterized by high loss, which we explain. By thoroughly elucidating the model’s algorithm, we provide new insights into its functioning. These findings are validated through rigorous testing and mathematical modeling, thereby contributing to the broader fields of model understanding and interpretability. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.",[],[],"['Philip Quirke', 'Fazl Barez']","['Martian', '']","['', '']"
https://openreview.net/forum?id=R7rZUSGOPD,Transparency & Explainability,PAE: Reinforcement Learning from External Knowledge for Efficient Exploration,"Human intelligence is adept at absorbing valuable insights from external knowledge. This capability is equally crucial for artificial intelligence.  In contrast, classical reinforcement learning agents lack such capabilities and often resort to extensive trial and error to explore the environment.  This paper introduces $\textbf{PAE}$: $\textbf{P}$lanner-$\textbf{A}$ctor-$\textbf{E}$valuator, a novel framework for teaching agents to $\textit{learn to absorb external knowledge}$.  PAE integrates the Planner's knowledge-state alignment mechanism, the Actor's mutual information skill control, and the Evaluator's adaptive intrinsic exploration reward to achieve 1) effective cross-modal information fusion, 2) enhanced linkage between knowledge and state, and 3) hierarchical mastery of complex tasks. Comprehensive experiments across  11 challenging tasks from the BabyAI and MiniHack environment suites demonstrate PAE's superior exploration efficiency with good interpretability.",[],[],"['Zhe Wu', 'Haofei Lu', 'Junliang Xing', 'You Wu', 'Renye Yan', 'Yaozhong Gan', 'Yuanchun Shi']","['Qiyuan laboratory', '', 'Department of Computer Science and Technology, Tsinghua University, Tsinghua University', 'nanjing university', 'Peking University', 'Qiyuan Laboratory', 'Department of Computer Science and Technology, , Tsinghua University']","['Serbia', '', 'China', 'China', 'United States', 'Serbia', 'China']"
https://openreview.net/forum?id=Rry1SeSOQL,Transparency & Explainability,MT-Ranker: Reference-free machine translation evaluation by inter-system ranking,"Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score. This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. Unfortunately, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior correlation with human judgments by merely using indirect supervision from natural language inference and weak supervision from our synthetic data. In the context of reference-free evaluation, MT-Ranker, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark, ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors, MT-Ranker marks state-of-the-art against reference-free as well as reference-based baselines.",[],[],"['Ibraheem Muhammad Moosa', 'Rui Zhang', 'Wenpeng Yin']","['Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=wriKDQqiOQ,Security,On the Effect of Batch Size in Byzantine-Robust Distributed Learning,"Byzantine-robust distributed learning (BRDL), in which computing devices are likely to behave abnormally due to accidental failures or malicious attacks, has recently become a hot research topic. However, even in the independent and identically distributed (i.i.d.) case, existing BRDL methods will suffer a significant drop on model accuracy due to the large variance of stochastic gradients. Increasing batch sizes is a simple yet effective way to reduce the variance. However, when the total number of gradient computation is fixed, a too-large batch size will lead to a too-small iteration number (update number), which may also degrade the model accuracy. In view of this challenge, we mainly study the effect of batch size when the total number of gradient computation is fixed in this work. In particular, we show that when the total number of gradient computation is fixed, the optimal batch size corresponding to the tightest theoretical upper bound in BRDL increases with the fraction of Byzantine workers. Therefore, compared to the case without attacks, a larger batch size is preferred when under Byzantine attacks. Motivated by the theoretical finding, we propose a novel method called Byzantine-robust stochastic gradient descent with normalized momentum (ByzSGDnm) in order to further increase model accuracy in BRDL. We theoretically prove the convergence of ByzSGDnm for general non-convex cases under Byzantine attacks. Empirical results show that when under Byzantine attacks, compared to the cases of small batch sizes, setting a relatively large batch size can significantly increase the model accuracy, which is consistent with our theoretical results. Moreover, ByzSGDnm can achieve higher model accuracy than existing BRDL methods when under deliberately crafted attacks. In addition, we empirically show that increasing batch sizes has the bonus of training acceleration.",[],[],"['Yi-Rui Yang', 'Chang-Wei Shi', 'Wu-Jun Li']","['School of Computer Science, Nanjing University', 'School of Computer Science, Nanjing University', 'Nanjing University']","['China', 'China', 'China']"
https://openreview.net/forum?id=vngVydDWft,Fairness & Bias,From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication,"It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction tasks, observing consistent latent similarity and downstream performance improvements in a zero-shot stitching setting. The experimental analysis comprises three modalities (vision, text, and graphs), twelve pretrained foundational models, nine benchmarks, and several architectures trained from scratch.",[],[],"['Irene Cannistraci', 'Luca Moschella', 'Marco Fumero', 'Valentino Maiorca', 'Emanuele Rodolà']","['University of Roma ""La Sapienza""', 'Apple', 'Institute of Science and Technology Austria(ISTA)', 'Computer Science, University of Roma ""La Sapienza""', 'Sapienza University of Rome']","['Italy', '', 'Austria', 'Italy', 'United States']"
https://openreview.net/forum?id=SoismgeX7z,Fairness & Bias,Generalized Schrödinger Bridge Matching,"Modern distribution matching algorithms for training diffusion or flow models directly prescribe the time evolution of the marginal distributions between two boundary distributions. In this work, we consider a generalized distribution matching setup, where these marginals are only implicitly described as a solution to some task-specific objective function. The problem setup, known as the Generalized Schrödinger Bridge (GSB), appears prevalently in many scientific areas both within and without machine learning. We propose Generalized Schödinger Bridge Matching (GSBM), a new matching algorithm inspired by recent advances, generalizing them beyond kinetic energy minimization and to account for nonlinear state costs. We show that such a generalization can be cast as solving conditional stochastic optimal control, for which efficient variational approximations can be used, and further debiased with the aid of path integral theory. Compared to prior methods for solving GSB problems, our GSBM algorithm always preserves a feasible transport map between the boundary distributions throughout training, thereby enabling stable convergence and significantly improved scalability. We empirically validate our claims on an extensive suite of experimental setups, including crowd navigation, opinion depolarization, LiDAR manifolds, and image domain transfer. Our work brings new algorithmic opportunities for training diffusion models enhanced with task-specific optimality structures.",[],[],"['Guan-Horng Liu', 'Yaron Lipman', 'Maximilian Nickel', 'Brian Karrer', 'Evangelos Theodorou', 'Ricky T. Q. Chen']","['FAIR, Meta AI', '', 'Facebook', 'Meta Fundamental AI Research (FAIR)', 'Georgia Institute of Technology', 'FAIR Labs, Meta AI']","['United States', 'United States', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=3d0OmYTNui,Privacy & Data Governance,Privately Aligning Language Models with Reinforcement Learning,"Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",[],[],"['Fan Wu', 'Huseyin A Inan', 'Arturs Backurs', 'Varun Chandrasekaran', 'Janardhan Kulkarni', 'Robert Sim']","['University of Illinois, Urbana Champaign', 'Microsoft', 'Microsoft', 'Electrical and Computer Engineering, University of Illinois Urbana-Champaign', 'Microsoft Research, Redmond', 'Microsoft']","['United States', '', '', 'United States', 'United States', '']"
https://openreview.net/forum?id=EcetCr4trp,Privacy & Data Governance,Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory,"Federated Learning (FL) has attracted significant attention as an efficient privacy-preserving approach to distributed learning across multiple clients. Despite extensive empirical research and practical applications, a systematic way to theoretically understand the convergence and generalization properties in FL remains limited. This work aims to establish a unified theoretical foundation for understanding FL through feature learning theory. We focus on a scenario where each client employs a two-layer convolutional neural network (CNN) for local training on their own data. Many existing works analyze the convergence of Federated Averaging (FedAvg) under lazy training with linearizing assumptions in weight space. In contrast, our approach tracks the trajectory of signal learning and noise memorization in FL, eliminating the need for these assumptions. We further show that FedAvg can achieve near-zero test error by effectively increasing signal-to-noise ratio (SNR) in feature learning, while local training without communication achieves a large constant test error. This finding highlights the benefits of communication for generalization in FL. Moreover, our theoretical results suggest that a weighted FedAvg method, based on the similarity of input features across clients, can effectively tackle data heterogeneity issues in FL. Experimental results on both synthetic and real-world datasets verify our theoretical conclusions and emphasize the effectiveness of the weighted FedAvg approach.",[],[],"['Wei Huang', 'Ye Shi', 'Zhongyi Cai', 'Taiji Suzuki']","['RIKEN AIP', 'School of Science Information and Technology, ShanghaiTech University', 'ShanghaiTech University', 'The University of Tokyo']","['Japan', 'China', 'China', 'China']"
https://openreview.net/forum?id=uXjfOmTiDt,Security,Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches,"The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be confronted with unseen or adaptive adversarial attacks and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. To achieve this, EAD develops two central recurrent sub-modules, i.e., a perception module and a policy module, to implement two critical functions of active vision. These models recurrently process a series of beliefs and observations, facilitating progressive refinement of their comprehension of the target object and enabling the development of strategic actions to counter adversarial patches in 3D environments. To optimize learning efficiency, we incorporate a differentiable approximation of environmental dynamics and deploy patches that are agnostic to the adversary’s strategies. Extensive experiments demonstrate that EAD substantially enhances robustness against a variety of patches within just a few steps through its action policy in safety-critical tasks (e.g., face recognition and object detection), without compromising standard accuracy. Furthermore, due to the attack-agnostic characteristic, EAD facilitates excellent generalization to unseen attacks, diminishing the averaged attack success rate by 95% across a range of unseen adversarial attacks.",[],[],"['Lingxuan Wu', 'Xiao Yang', 'Yinpeng Dong', 'Liuwei XIE', 'Hang Su', 'Jun Zhu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Hong Kong University of Science and Technology', 'Computer Science, Tsinghua University', 'Computer Science, Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=7W3GLNImfS,Fairness & Bias,Human Feedback is not Gold Standard,"Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.",[],[],"['Tom Hosking', 'Phil Blunsom', 'Max Bartolo']","['', 'Google', 'Cohere']","['', 'United States', 'United States']"
https://openreview.net/forum?id=c93SBwz1Ma,Transparency & Explainability,BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models,"Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger is embedded in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in COT prompting. Consequently, given any query prompt containing the backdoor trigger, the LLM will be misled to output unintended content. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. In addition, our findings reveal that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0\% across the six benchmark tasks on GPT-4. We also demonstrate the interpretability of BadChain by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses.",[],[],"['Zhen Xiang', 'Fengqing Jiang', 'Zidi Xiong', 'Bhaskar Ramasubramanian', 'Radha Poovendran', 'Bo Li']","['University of Georgia', 'University of Washington', 'Harvard University', 'Western Washington University', 'University of Washington, Seattle', 'CS, University of Illinois, Urbana Champaign']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=wg8NPfeMF9,Transparency & Explainability,$\texttt{NAISR}$: A 3D Neural Additive Model for Interpretable Shape Representation,"Deep implicit functions (DIFs) have emerged as a powerful paradigm for many computer vision tasks such as 3D shape reconstruction, generation, registration, completion, editing, and understanding. However, given a set of 3D shapes with associated covariates there is at present no shape representation method which allows to precisely represent the shapes while capturing the individual dependencies on each covariate. Such a method would be of high utility to researchers to discover knowledge hidden in a population of shapes. For scientific shape discovery purpose, we propose a 3D Neural Additive Model for Interpretable Shape Representation ($\texttt{NAISR}$) which describes individual shapes by deforming a shape atlas in accordance to the effect of disentangled covariates. Our approach captures shape population trends and allows for patient-specific predictions through shape transfer. $\texttt{NAISR}$ is the first approach to combine the benefits of deep implicit shape representations with an atlas deforming according to specified covariates. We evaluate $\texttt{NAISR}$ with respect to shape reconstruction, shape disentanglement, shape evolution, and shape transfer on three datasets, i.e. 1) $\textit{Starman}$, a simulated 2D shape dataset; 2) ADNI hippocampus 3D shape dataset; 3) pediatric airway 3D shape dataset. Our experiments demonstrate that $\texttt{NAISR}$ achieves competitive shape reconstruction performance while retaining interpretability. Our code is available at https://github.com/uncbiag/NAISR.",[],[],"['Yining Jiao', 'Carlton Jude ZDANSKI', 'Julia S Kimbell', 'Andrew Prince', 'Cameron P Worden', 'Samuel Kirse', 'Christopher Rutter', 'Benjamin Shields', 'William Alexander Dunn', 'Jisan Mahmud', 'Marc Niethammer']","['Computer Science, University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'Spring Hill College', 'University of North Carolina at Chapel Hill', 'Wake Forest University School of Medicine', 'Cardiothoracic Surgery, Ohio State University, Columbus', 'Department of Otolaryngology/Head and Neck Surgery, University of North Carolina at Chapel Hill', 'Otolaryngology, University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'Department of Computer Science and Engineering; and Neurological Surgery, University of California, San Diego']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=BBD6KXIGJL,Fairness & Bias,Hybrid Directional Graph Neural Network for Molecules,"Equivariant message passing neural networks have emerged as the prevailing approach for predicting chemical properties of molecules due to their ability to leverage translation and rotation symmetries, resulting in a strong inductive bias. However, the equivariant operations in each layer can impose excessive constraints on the function form and network flexibility. To address these challenges, we introduce a novel network called the Hybrid Directional Graph Neural Network (HDGNN), which effectively combines strictly equivariant operations with learnable modules. We evaluate the performance of HDGNN on the QM9 dataset and the IS2RE dataset of OC20, demonstrating its state-of-the-art performance on several tasks and competitive performance on others. Our code is anonymously released on https://github.com/ajy112/HDGNN.",[],[],"['Junyi An', 'Chao Qu', 'Zhipeng Zhou', 'Fenglei Cao', 'Xu Yinghui', 'Yuan Qi', 'Furao Shen']","['Shanghai Academy of Artificial Intelligence for Science', 'Inftech', 'INF (Shanghai) Technology Co., LTD', 'inf tech', 'Fudan University', 'Fudan University', 'Artificial Intelligence, Nanjing University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=BRdEBlwUW6,Fairness & Bias,DAFA: Distance-Aware Fair Adversarial Training,"The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its performance on harder ones. However, we observe that under adversarial attacks, the majority of the model's predictions for samples from the worst class are biased towards classes similar to the worst class, rather than towards the easy classes. Through theoretical and empirical analysis, we demonstrate that robust fairness deteriorates as the distance between classes decreases. Motivated by these insights, we introduce the Distance-Aware Fair Adversarial Training (DAFA) methodology, which addresses robust fairness by taking into account the similarities between classes. Specifically, our method assigns distinct adversarial margins and loss weights to each class and adjusts them to encourage a trade-off in robustness among similar classes. Experimental results across various datasets demonstrate that our method not only maintains average robust accuracy but also significantly improves the worst robust accuracy, indicating a marked improvement in robust fairness compared to existing methods.",[],[],"['Hyungyu Lee', 'Saehyung Lee', 'Hyemi Jang', 'Junsung Park', 'Ho Bae', 'Sungroh Yoon']","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University', 'Ewha Womans University', 'ECE and AI, Seoul National University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=kvcbV8KQsi,Transparency & Explainability,"Successor Heads: Recurring, Interpretable Attention Heads In The Wild","In this work we describe successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days. For example, successor heads increment 'Monday' into 'Tuesday'. We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms. Existing research in this area has struggled to find recurring, mechanistically interpretable large language model (LLM) components beyond small toy models. Further, existing results have led to very little insight to explain the internals of the larger models that are used in practice. In this paper, we analyze the behavior of successor heads in LLMs and find that they implement abstract representations that are common to different architectures.  Successor heads form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2. We find a set of 'mod 10' features that underlie how successor heads increment in LLMs across different architectures and sizes. We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, where we find that successor heads are important for achieving a low loss on examples involving succession, and also identify interpretable polysemanticity in a Pythia successor head.",[],[],"['Rhys Gould', 'Euan Ong', 'George Ogden', 'Arthur Conmy']","['Department of Applied Mathematics and Theoretical Physics, University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'Google DeepMind']","['United Kingdom', 'United Kingdom', 'United Kingdom', '']"
https://openreview.net/forum?id=tEgrUrUuwA,Fairness & Bias,Partitioning Message Passing for Graph Fraud Detection,"Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment graph structure to accommodate the inductive bias of GNNs towards homophily, by excluding heterophilic neighbors during message passing. In our work, we argue that the key to applying GNNs for GFD is not to exclude but to {\em distinguish} neighbors with different labels. Grounded in this perspective, we introduce Partitioning Message Passing (PMP), an intuitive yet effective message passing paradigm expressly crafted for GFD. Specifically, in the neighbor aggregation stage of PMP, neighbors with different classes are aggregated with distinct node-specific aggregation functions. By this means, the center node can adaptively adjust the information aggregated from its heterophilic and homophilic neighbors, thus avoiding the model gradient being dominated by benign nodes which occupy the majority of the population. We theoretically establish a connection between the spatial formulation of PMP and spectral analysis to characterize that PMP operates an adaptive node-specific spectral graph filter, which demonstrates the capability of PMP to handle heterophily-homophily mixed graphs. Extensive experimental results show that PMP can significantly boost the performance on GFD tasks.",[],[],"['Wei Zhuo', 'Zemin Liu', 'Bryan Hooi', 'Bingsheng He', 'Guang Tan', 'Rizal Fathony', 'Jia Chen']","['Nanyang Technological University', 'College of Computer Science and Technology, Zhejiang University', 'Computer Science, National University of Singapore', 'National University of Singapore', 'SUN YAT-SEN UNIVERSITY', 'Capital One', '']","['Singapore', 'China', 'Singapore', 'Singapore', 'China', 'Paraguay', '']"
https://openreview.net/forum?id=yarUvgEXq3,Fairness & Bias,Safe Collaborative Filtering,"Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for personalized recommender systems to reduce the risk of losing users with low satisfaction. This study introduces a ""safe"" collaborative filtering method that prioritizes recommendation quality for less-satisfied users rather than focusing on the average performance. Our approach minimizes the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. To overcome computational challenges for web-scale recommender systems, we develop a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of our approach while maintaining competitive computational efficiency.",[],[],"['Riku Togashi', 'Tatsushi Oka', 'Naoto Ohsaka', 'Tetsuro Morimura']","['CyberAgent', '', 'CyberAgent, Inc.', 'CyberAgent, Inc.']","['Belgium', '', 'Belgium', 'Belgium']"
https://openreview.net/forum?id=5nM2AHzqUj,Fairness & Bias,Linear Log-Normal Attention with Unbiased Concentration,"Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models.",[],[],"['Yury Nahshan', 'Joseph Kampeas', 'Emir Haleva']","['Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Tel Aviv University, Tel Aviv University']","['China', 'China', 'China']"
https://openreview.net/forum?id=WOiOzHG2zD,Fairness & Bias,TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields,"Recent works learn 3D representation explicitly under text-3D guidance. However, limited text-3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing open-vocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D. Specifically, rather than using the text prompts as input directly, we suggest to inject dynamic noise into the latent space of given text prompts, i.e., Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the appropriate range of textual latent space that is expanded by NTFs. To this end, an NTFGen module is proposed to model general text latent code in noisy fields. Meanwhile, an NTFBind module is proposed to align view-invariant image latent code to noisy fields, further supporting image-conditional 3D generation. To guide the conditional generation in both geometry and texture, multi-modal discrimination is constructed with a text-3D discriminator and a text-2.5D discriminator. Compared to previous methods, TextField3D includes three merits: 1) large vocabulary, 2) text consistency, and 3) low latency. Extensive experiments demonstrate that our method achieves a potential open-vocabulary 3D generation capability.",[],[],"['Tianyu Huang', 'Yihan Zeng', 'Bowen Dong', 'Hang Xu', 'Songcen Xu', 'Rynson W. H. Lau', 'Wangmeng Zuo']","['', 'Huawei Technologies Ltd.', 'Computing, Hong Kong Polytechnic University', 'Huawei Noah‘s Ark Lab', ""Huawei Noah's Ark Lab"", 'Department of Computer Science, City University of Hong Kong', 'Harbin Institute of Technology']","['', 'China', 'Hong Kong', 'China', 'China', 'Hong Kong', 'China']"
https://openreview.net/forum?id=ADDCErFzev,Fairness & Bias,Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems,"According to the efficient coding hypothesis, neural populations encode information optimally when representations are high-dimensional and uncorrelated. However, such codes may carry a cost in terms of generalization and robustness. Past empirical studies of early visual cortex (V1) in rodents have suggested that this tradeoff indeed constrains sensory representations. However, it remains unclear whether these insights generalize across the hierarchy of the human visual system, and particularly to object representations in high-level occipitotemporal cortex (OTC). To gain new empirical clarity, here we develop a family of object recognition models with parametrically varying dropout proportion $p$, which induces systematically varying dimensionality of internal responses (while controlling all other inductive biases). We find that increasing dropout produces an increasingly smooth, low-dimensional representational space. Optimal robustness to lesioning is observed at around 70% dropout, after which both accuracy and robustness decline. Representational comparison to large-scale 7T fMRI data from occipitotemporal cortex in the Natural Scenes Dataset reveals that this optimal degree of dropout is also associated with maximal emergent neural predictivity. Finally, using new techniques for achieving denoised estimates of the eigenspectrum of human fMRI responses, we compare the rate of eigenspectrum decay between model and brain feature spaces. We observe that the match between model and brain representations is associated with a common balance between efficiency and robustness in the representational space. These results suggest that varying dropout may reveal an optimal point of balance between the efficiency of high-dimensional codes and the robustness of low dimensional codes in hierarchical vision systems.",[],[],"['Jacob S. Prince', 'Gabriel Fajardo', 'George A. Alvarez', 'Talia Konkle']","['Harvard University', 'Harvard University', 'Harvard University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=rmg0qMKYRQ,Fairness & Bias,Intriguing Properties of Generative Classifiers,"What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.",[],[],"['Priyank Jaini', 'Kevin Clark', 'Robert Geirhos']","['Google', 'Google', 'Google DeepMind']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=BXY6fe7q31,Fairness & Bias,Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions,"Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs’ underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. To address this issue, we introduce a generic and lightweight Visual Prompt Generator Complete module (VPG-C), which can infer and complete the missing details essential for comprehending demonstrative instructions. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C. The code and models are available at https://github.com/DCDmllm/Cheetah.",[],[],"['Juncheng Li', 'Kaihang Pan', 'Zhiqi Ge', 'Minghe Gao', 'Wei Ji', 'Wenqiao Zhang', 'Tat-Seng Chua', 'Siliang Tang', 'Hanwang Zhang', 'Yueting Zhuang']","['Zhejiang University', 'Zhejiang University', 'Computer Science, Zhejiang University', 'College of computer science and technology, Zhejiang University', 'School of Intelligence Science and Technology, Nanjing University', 'Computer, Zhejiang University', 'Department of Computer Science, National University of Singapore', 'College of Computer Science and Technology, Zhejiang University', 'computer science, Nanyang Technological University', 'Computer science , Zhejiang University']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=fq1wNrC2ai,Fairness & Bias,Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation,"We study infinite-horizon average-reward Markov decision processes (AMDPs) in the context of general function approximation. Specifically, we propose a novel algorithmic framework named Local-fitted Optimization with OPtimism (LOOP), which incorporates both model-based and value-based incarnations. In particular, LOOP features a novel construction of confidence sets and a low-switching policy updating scheme, which are tailored to the average-reward and function approximation setting. Moreover, for AMDPs, we propose a novel complexity measure --- average-reward generalized eluder coefficient (AGEC) --- which captures the challenge of exploration in AMDPs with general function approximation. Such a complexity measure encompasses almost all previously known tractable AMDP models, such as linear AMDPs and linear mixture AMDPs, and also includes newly identified cases such as kernel AMDPs and AMDPs with Bellman eluder dimensions. Using AGEC, we prove that LOOP achieves a sublinear  $\tilde{\mathcal{O}}(\mathrm{poly}(d, \mathrm{sp}(V^*)) \sqrt{T\beta} )$ regret, where $d$ and $\beta$ correspond to  AGEC and log-covering number of the hypothesis class respectively,  $\mathrm{sp}(V^*)$ is the span of the optimal state bias function, $T$ denotes the number of steps, and $\tilde{\mathcal{O}} (\cdot) $ omits logarithmic factors. When specialized to concrete AMDP models, our regret bounds are comparable to those established by the existing algorithms designed specifically for these special cases.  To the best of our knowledge, this paper presents the first comprehensive theoretical framework capable of handling nearly all AMDPs.",[],[],"['Jianliang He', 'Han Zhong', 'Zhuoran Yang']","['Department of Statistics and Data Science, Yale University', 'Peking University', 'Yale University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=ztpy1gsUpT,Privacy & Data Governance,Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting,"Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",[],[],"['Xinlu Zhang', 'Shiyang Li', 'Xianjun Yang', 'Chenxin Tian', 'Yao Qin', 'Linda Ruth Petzold']","['LinkedIn', 'Amazon', '', 'Peking Union Medical Collage', 'University of California, Santa Barbara', '']","['United States', 'Japan', '', 'United States', '', '']"
https://openreview.net/forum?id=z6KS9D1dxt,Security,Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game,"In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with identified allies and minimizing vulnerability to adversarial manipulation. We define the optimal solution to the BARDec-POMDP as an ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist and the corresponding policy weakly dominates previous approaches as time goes to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experiments on matrix game, Level-based Foraging and StarCraft II indicate that, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations, showing resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.",[],[],"['Simin Li', 'Jun Guo', 'Jingqiao Xiu', 'Ruixiao Xu', 'Xin Yu', 'Jiakai Wang', 'Aishan Liu', 'Yaodong Yang', 'Xianglong Liu']","['School of Computer Science and Engineering, Beihang University', 'Department of Computer Science and Technology, Tsinghua University', 'National University of Singapore', 'Beijing University of Aeronautics and Astronautics', 'School of Computer Science and Engineering, Beihang University', 'Zhongguancun Laboratory', 'Beihang University', 'Peking University', '']","['China', 'China', 'Singapore', 'China', '', 'India', 'Egypt', 'United States', '']"
https://openreview.net/forum?id=DCDT918ZkI,Security,Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective,"Current defenses against graph attacks often rely on certain properties to eliminate structural perturbations by identifying adversarial edges from normal edges. However, this dependence makes defenses vulnerable to adaptive (white-box) attacks from adversaries with the same knowledge. Adversarial training seems to be a feasible way to enhance robustness without reliance on artificially designed properties. However, in this paper, we show that it can lead to models learning incorrect information. To solve this issue, we re-examine graph attacks from the out-of-distribution (OOD) perspective for poisoning and evasion attacks and introduce a novel adversarial training paradigm incorporating OOD detection. This approach strengthens the robustness of Graph Neural Networks (GNNs) without reliance on prior knowledge. To further evaluate adaptive robustness, we develop adaptive attacks against our methods, revealing a trade-off between graph attack efficacy and defensibility. Through extensive experiments over 25,000 perturbed graphs, our method could still maintain good robustness against both adaptive and non-adaptive attacks. The code is provided at https://github.com/likuanppd/GOOD-AT.",[],[],"['Kuan Li', 'YiWen Chen', 'Yang Liu', 'Jin Wang', 'Qing He', 'Minhao Cheng', 'Xiang Ao']","['Hong Kong University of Science and Technology', 'School of Artificial Intelligence, Beijing University of Aeronautics and Astronautics', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Megagon Labs', 'Institute of Computing Technology, CAS', 'College of Information Sciences and Technology, Pennsylvania State University', 'University of the Chinese Academy of Sciences']","['Hong Kong', 'China', '', 'Ethiopia', '', 'United States', 'United States']"
https://openreview.net/forum?id=jLIUfrAcMQ,Fairness & Bias,Debiasing Attention Mechanism in Transformer without Demographics,"Although transformers demonstrate impressive capabilities in a variety of tasks, the fairness issue remains a significant concern when deploying these models. Existing works to address fairness issues in transformers require sensitive labels (such as age, gender, etc.), which can raise privacy concerns or violate legal regulations. An alternative way is through fairness without demographics. However, existing works that improve Rawlsian Max-Min fairness may impose overly restrictive constraints. Other methods that use auxiliary networks could be parameter inefficient. In this paper, we present a new approach to debiasing transformers by leveraging their inherent structure.  By reconsidering the roles of important components (queries, keys, and values) in the attention mechanism, we introduce a simple yet effective debiasing strategy from two perspectives: 1) Grounded in theoretical analysis, we normalize and apply absolute value operations to queries and keys to minimize the bias in attention weight allocation; 2) We reduce the bias within values through local alignment via contrastive learning. Throughout the entire process, our approach does not require any sensitive labels. Furthermore, to enhance memory efficiency in the training phase, we propose a strategy that debias only the last encoder to improve fairness in pre-trained models. We conduct experiments in computer vision and natural language processing tasks and show that our method is comparable and even outperforms the state-of-the-art method with substantially lower energy consumption.",[],[],"['Shenyu Lu', 'Yipei Wang', 'Xiaoqian Wang']","['Purdue University', 'ECE, Purdue University', 'Purdue University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=yozwqhIHXj,Transparency & Explainability,Image Translation as Diffusion Visual Programmers,"We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs ($i.e.$, computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP’s remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the frame work enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols ($e.g.$, [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.",[],[],"['Cheng Han', 'James Chenhao Liang', 'Qifan Wang', 'MAJID RABBANI', 'Sohail Dianat', 'Raghuveer Rao', 'Ying Nian Wu', 'Dongfang Liu']","['Computer Science, University of Missouri - Kansas City', 'U. S. Naval Research Laboratory', 'Meta AI', 'EME, Rochester Institute of Technology', 'Electrical and Microelectronic Engineering, Rochester Institute of Technology', 'Intelligent Perception , DEVCOM Army Research Laboratory', 'Statistics, UCLA', 'Rochester Institute of Technology']","['United States', 'United States', '', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=xv8iGxENyI,Security,Threaten Spiking Neural Networks through Combining Rate and Temporal Information,"Spiking Neural Networks (SNNs) have received widespread attention in academic communities due to their superior spatio-temporal processing capabilities and energy-efficient characteristics. With further in-depth application in various fields, the vulnerability of SNNs under adversarial attack has become a focus of concern.  In this paper, we draw inspiration from two mainstream learning algorithms of SNNs and observe that SNN models reserve both rate and temporal information. To better understand the capabilities of these two types of information, we conduct a quantitative analysis separately for each. In addition, we note that the retention degree of temporal information is related to the parameters and input settings of spiking neurons. Building on these insights, we propose a hybrid adversarial attack based on rate and temporal information (HART), which allows for dynamic adjustment of the rate and temporal attributes. Experimental results demonstrate that compared to previous works, HART attack can achieve significant superiority under different attack scenarios, data types, network architecture, time-steps, and model hyper-parameters. These findings call for further exploration into how both types of information can be effectively utilized to enhance the reliability of SNNs. Code is available at [https://github.com/hzc1208/HART_Attack](https://github.com/hzc1208/HART_Attack).",[],[],"['Zecheng Hao', 'Tong Bu', 'Xinyu Shi', 'Zihan Huang', 'Zhaofei Yu', 'Tiejun Huang']","['School of Computer Science, Peking University', '', '', 'Peking University', 'Peking University', 'School of Computer Science, Peking University']","['United States', '', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=xUzWmFdglP,Privacy & Data Governance,Privacy Amplification for Matrix Mechanisms,"Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning (ML), but, is not readily applicable to the newer state-of-the-art (SOTA) algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD.  In this paper, we propose ""MMCC'' (matrix mechanism conditional composition), the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as $\epsilon\to0$.  To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our ""conditional composition theorem'' has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification. Our algorithm also has practical empirical utility. We show that amplification leads to significant improvement in the privacy/utility trade-offs for DP-FTRL style algorithms for standard benchmark tasks.",[],[],"['Christopher A. Choquette-Choo', 'Arun Ganesh', 'Thomas Steinke', 'Abhradeep Guha Thakurta']","['Google DeepMind', 'Google', 'Google DeepMind', 'Google']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=vy42bYs1Wo,Security,Off-Policy Primal-Dual Safe Reinforcement Learning,"Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose conservative policy optimization, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce local policy convexification to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and further verify them by extensive experiments. Results on benchmark tasks show that our method not only achieves an asymptotic performance comparable to state-of-the-art on-policy methods while using much fewer samples, but also significantly reduces constraint violation during training. Our code is available at https://github.com/ZifanWu/CAL.",[],[],"['Zifan Wu', 'Bo Tang', 'Qian Lin', 'Chao Yu', 'Shangqin Mao', 'Qianlong Xie', 'Xingxing Wang', 'Dong Wang']","['Kahlert School of Computing, University of Utah', 'Institute for Advanced Algorithms Research, Shanghai', 'Computer science, SUN YAT-SEN UNIVERSITY', 'Sun Yat-sen University', 'Meituan', '', 'meituan', 'Tsinghua University, Tsinghua University']","['United States', '', 'China', 'China', 'Brazil', '', 'Brazil', 'China']"
https://openreview.net/forum?id=vqIH0ObdqL,Transparency & Explainability,Can Large Language Models Infer Causation from Correlation?,"Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize – they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs’ pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",[],[],"['Zhijing Jin', 'Jiarui Liu', 'Zhiheng LYU', 'Spencer Poff', 'Mrinmaya Sachan', 'Rada Mihalcea', 'Mona T. Diab', 'Bernhard Schölkopf']","['', 'Carnegie Mellon University', 'University of Waterloo', 'Facebook', 'Swiss Federal Institute of Technology', 'University of Michigan', 'Language Technologies Institute, Carnegie Mellon University', '']","['', 'United States', 'Canada', 'Vietnam', 'India', 'Italy', 'United States', '']"
https://openreview.net/forum?id=vRyp2dhEQp,Security,Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",[],[],"['Ziqiang Li', 'Hong Sun', 'Pengfei Xia', 'Heng Li', 'Beihao Xia', 'Yi Wu', 'Bin Li']","['Nanjing University of Information Science and Technology', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Huazhong University of Science and Technology', 'EIC, Huazhong University of Science and Technology', 'University of Science and Technology of China', 'School of Information Science and Technology, University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=vLJcd43U7a,Transparency & Explainability,SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning,"Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present SYMBOL, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within SYMBOL, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by SYMBOL not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem dimensions, population sizes, and optimization horizons. Furthermore, we conduct in-depth analyses of our SYMBOL framework and the optimization rules that it generates, underscoring its desirable flexibility and interpretability.",[],[],"['Jiacheng Chen', 'Zeyuan Ma', 'Hongshu Guo', 'Yining Ma', 'Jie Zhang', 'Yue-Jiao Gong']","['South China University of Technology', 'South China University of Technology', 'South China University of Technology', 'Massachusetts Institute of Technology', 'Nanyang Technological University', 'Computer Science, South China University of Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=vE1e1mLJ0U,Fairness & Bias,The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks.,"Biological cortical neurons are remarkably sophisticated computational devices, temporally integrating their vast synaptic input over an intricate dendritic tree, subject to complex, nonlinearly interacting internal biological processes.  A recent study proposed to characterize this complexity by fitting accurate surrogate models to replicate the input-output relationship of a detailed biophysical cortical pyramidal neuron model and discovered it needed temporal convolutional networks (TCN) with millions of parameters.  Requiring these many parameters, however, could stem from a misalignment between the inductive biases of the TCN and cortical neuron's computations. In light of this, and to explore the computational implications of leaky memory units and nonlinear dendritic processing, we introduce the Expressive Leaky Memory (ELM) neuron model, a biologically inspired phenomenological model of a cortical neuron. Remarkably, by exploiting such slowly decaying memory-like hidden states and two-layered nonlinear integration of synaptic input, our ELM neuron can accurately match the aforementioned input-output relationship with under ten thousand trainable parameters. To further assess the computational ramifications of our neuron design, we evaluate it on various tasks with demanding temporal structures, including the Long Range Arena (LRA) datasets, as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits dataset (SHD-Adding). Leveraging a larger number of memory units with sufficiently long timescales, and correspondingly sophisticated synaptic integration, the ELM neuron displays substantial long-range processing capabilities, reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA, and even solving the Pathfinder-X task with over 70\% accuracy (16k context length). These findings raise further questions about the computational sophistication of individual cortical neurons and their role in extracting complex long-range temporal dependencies.",[],[],"['Aaron Spieler', 'Nasim Rahaman', 'Georg Martius', 'Bernhard Schölkopf', 'Anna Levina']","['Friedrich Miescher Institute', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Eberhard-Karls-Universität Tübingen', '', 'Eberhard-Karls-Universität Tübingen']","['Brazil', 'Brazil', 'Germany', '', 'Germany']"
https://openreview.net/forum?id=v1VvCWJAL8,Transparency & Explainability,Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models,"Answering counterfactual queries has important applications such as explainability, robustness, and fairness but is challenging when the causal variables are unobserved and the observations are non-linear mixtures of these latent variables, such as pixels in images. One approach is to recover the latent Structural Causal Model (SCM), which may be infeasible in practice due to requiring strong assumptions, e.g., linearity of the causal mechanisms or perfect atomic interventions. Meanwhile, more practical ML-based approaches using naive domain translation models to generate counterfactual samples lack theoretical grounding and may construct invalid counterfactuals. In this work, we strive to strike a balance between practicality and theoretical guarantees by analyzing a specific type of causal query called *domain counterfactuals*, which hypothesizes what a sample would have looked like if it had been generated in a different domain (or environment). We show that recovering the latent SCM is unnecessary for estimating domain counterfactuals, thereby sidestepping some of the theoretic challenges. By assuming invertibility and sparsity of intervention, we prove domain counterfactual estimation error can be bounded by a data fit term and intervention sparsity term. Building upon our theoretical results, we develop a theoretically grounded practical algorithm that simplifies the modeling process to generative model estimation under autoregressive and shared parameter constraints that enforce intervention sparsity. Finally, we show an improvement in counterfactual estimation over baseline methods through extensive simulated and image-based experiments.",[],[],"['Zeyu Zhou', 'Ruqi Bai', 'Sean Kulinski', 'Murat Kocaoglu', 'David I. Inouye']","['Purdue University', 'Electrical and Computer Engineering, Purdue University', 'Purdue University', 'Purdue University', 'Purdue University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=up6hr4hIQH,Transparency & Explainability,Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks,"Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes --- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.",[],[],"['Xu Zheng', 'Farhad Shirani', 'Tianchun Wang', 'Wei Cheng', 'Zhuomin Chen', 'Haifeng Chen', 'Hua Wei', 'Dongsheng Luo']","['', 'Florida International University', 'Pennsylvania State University', 'Data Science, NEC-Labs', '', 'NEC-Labs', 'Arizona State University', '']","['', 'United States', 'United States', 'United States', 'United States', '', 'United States', '']"
https://openreview.net/forum?id=Hf17y6u9BC,Transparency & Explainability,Towards Best Practices of Activation Patching in Language Models: Metrics and Methods,"Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization—identifying the important model components—is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.",[],[],"['Fred Zhang', 'Neel Nanda']","['Google DeepMind', 'Google DeepMind']","['United States', 'United States']"
https://openreview.net/forum?id=u7559ZMvwY,Security,Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization,"The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline to acquire the robust purifier model, named Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks, resulting in the robustness generalization to unseen attacks, and FT is essential for the improvement of robustness.  To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves optimal robustness and exhibits generalization ability against unseen attacks.",[],[],"['Guang Lin', 'Chao Li', 'Jianhai Zhang', 'Toshihisa Tanaka', 'Qibin Zhao']","['Tokyo University of Agriculture and Technology', 'RIKEN', 'College of Computer Science, Hangzhou Dianzi University', 'Tokyo University of Agriculture and Technology, Tokyo Institute of Technology', 'AIP, RIKEN']","['Japan', 'Japan', 'Japan', 'Japan', 'Japan']"
https://openreview.net/forum?id=u859gX7ADC,Transparency & Explainability,Augmenting Transformers with Recursively Composed Multi-grained Representations,"We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference.  Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained with Transformers, making ReCAT enjoy scaling ability, strong performance, and interpretability at the same time. We conduct experiments on various sentence-level and span-level tasks. Evaluation results indicate that ReCAT can significantly outperform vanilla Transformer models on all span-level tasks and recursive models on natural language inference tasks. More interestingly, the hierarchical structures induced by ReCAT exhibit strong consistency with human-annotated syntactic trees, indicating good interpretability brought by the CIO layers.",[],[],"['Xiang Hu', 'Qingyang Zhu', 'Kewei Tu', 'Wei Wu']","['Alibaba Group', 'ShanghaiTech University', 'ShanghaiTech University', 'Ant Research']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=tiiAzqi6Ol,Transparency & Explainability,Compositional Preference Models for Aligning LMs,"As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs. Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.",[],[],"['Dongyoung Go', 'Tomasz Korbak', 'Germán Kruszewski', 'Jos Rozen', 'Marc Dymetman']","['NAVER', 'UK AI Safety Institute', 'Naver Labs Europe', 'Naver Labs Europe', 'Naver Labs Europe']","['Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://openreview.net/forum?id=SBj2Qdhgew,Fairness & Bias,Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition,"This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either $\textit{global fairness}$ (overall disparity of the model across all clients) or $\textit{local fairness}$ (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding regarding the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, $\textit{Unique Disparity}$, $\textit{Redundant  Disparity}$, and $\textit{Masked Disparity}$. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree.  We introduce the $\textit{Accuracy and Global-Local Fairness Optimality Problem}$ (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",[],[],"['Faisal Hamman', 'Sanghamitra Dutta']","['Electrical and Computer Engineering, University of Maryland, College Park', 'Electrical and Computer Engineering, University of Maryland, College Park']","['United States', 'United States']"
https://openreview.net/forum?id=sqRgz88TM3,Security,VFLAIR: A Research Library and Benchmark for Vertical Federated Learning,"Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. To address this need, we present an extensible and lightweight VFL framework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which supports VFL training with a variety of models, datasets and protocols, along with standardized modules for comprehensive evaluations of attacks and defense strategies. We also benchmark $11$ attacks and $8$ defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenarios.",[],[],"['Tianyuan Zou', 'Zixuan GU', 'Yu He', 'Hideaki Takahashi', 'Yang Liu', 'Ya-Qin Zhang']","['Tsinghua University, Institute for AI Industry Research, Tsinghua University', 'School of Software, Tsinghua University', 'Fudan University', 'ISB, The University of Tokyo', 'Tsinghua University, Tsinghua University', 'AIR, Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=lNZJyEDxy4,Transparency & Explainability,MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data,"This paper addresses the problem of anomaly detection in tabular data, which is usually implemented in an one-class classification setting where the training set only contains normal samples. Inspired by the success of masked image/language modeling in vision and natural language domains, we extend masked modeling methods to address this problem by capturing intrinsic correlations between features in training set. Thus, a sample deviate from such correlations is related to a high possibility of anomaly. To obtain multiple and diverse correlations, we propose a novel masking strategy which generates multiple masks by learning, and design a diversity loss to reduce the similarity of different masks. Extensive experiments show our method achieves state-of-the-art performance. We also discuss the interpretability from the perspective of each individual feature and correlations between features.",[],[],"['Jiaxin Yin', 'Yuanyuan Qiao', 'Zitang Zhou', 'Xiangchao Wang', 'Jie Yang']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Hangzhou Dianzi University', 'Beijing University of Posts and Telecommunications']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=slSmYGc8ee,Fairness & Bias,How connectivity structure shapes rich and lazy learning in neural circuits,"In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights — in particular their effective rank — influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics. Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting.",[],[],"['Yuhan Helena Liu', 'Aristide Baratin', 'Jonathan Cornford', 'Stefan Mihalas', 'Eric Todd SheaBrown', 'Guillaume Lajoie']","['Princeton University', '', '', 'Allen Institute', 'University of Washington', 'Mila, Quebec AI institute']","['United States', '', '', 'United States', 'United States', 'Canada']"
https://openreview.net/forum?id=samyfu6G93,Security,NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks,"Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To train NeuroBack, a new dataset called DataBack containing 120,286 data samples is created. Finally, NeuroBack is implemented as an enhancement to a state-of-the-art SAT solver called Kissat. As a result, it allowed Kissat to solve 5.2% more problems on the recent SAT competition problem set, SATCOMP-2022. NeuroBack therefore shows how machine learning can be harnessed to improve SAT solving in an effective and practical manner.",[],[],"['Wenxi Wang', 'Yang Hu', 'Mohit Tiwari', 'Sarfraz Khurshid', 'Kenneth McMillan', 'Risto Miikkulainen']","['University of Texas, Austin', 'Amazon', 'University of Texas at Austin', 'ITU of Punjab Lahore, Pakistan', 'University of Texas, Austin', 'The University of Texas, Austin']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=rp0EdI8X4e,Transparency & Explainability,Faithful Vision-Language Interpretation via Concept Bottleneck Models,"The demand for transparency in healthcare and finance has led to interpretable machine learning (IML) models, notably the concept bottleneck models (CBMs), valued for their potential in performance and insights into deep neural networks. However, CBM's reliance on manually annotated data poses challenges. Label-free CBMs have emerged to address this, but they remain unstable, affecting their faithfulness as explanatory tools. To address this issue of inherent instability, we introduce a formal definition for an alternative concept called the Faithful Vision-Language Concept (FVLC) model. We present a methodology for constructing an FVLC that satisfies four critical properties. Our extensive experiments on four benchmark datasets using Label-free CBM model architectures demonstrate that our FVLC outperforms other baselines regarding stability against input and concept set perturbations. Our approach incurs minimal accuracy degradation compared to the vanilla CBM, making it a promising solution for reliable and faithful model interpretation.",[],[],"['Songning Lai', 'Lijie Hu', 'Junxiao Wang', 'Laure Berti-Equille', 'Di Wang']","['The Hong Kong University of Science and Technology', 'KAUST', 'CIAT, Guangzhou University', 'IRD - Institute of Research for Sustainable Development, France', '']","['Hong Kong', '', 'China', 'France', '']"
https://openreview.net/forum?id=wZWTHU7AsQ,Security,Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations,"Deploying reinforcement learning (RL) systems requires robustness to uncertainty and model misspecification, yet prior robust RL methods typically only study noise introduced independently across time. However, practical sources of uncertainty are usually coupled across time. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium within this game, GRAD optimizes for general robustness against temporally-coupled perturbations. Experiments on continuous control tasks demonstrate that, compared with prior methods, our approach achieves a higher degree of robustness to various types of attacks on different attack domains, both in settings with temporally-coupled perturbations and decoupled perturbations.",[],[],"['Yongyuan Liang', 'Yanchao Sun', 'Ruijie Zheng', 'Xiangyu Liu', 'Benjamin Eysenbach', 'Tuomas Sandholm', 'Furong Huang', 'Stephen Marcus McAleer']","['Computer Science Department, University of Maryland, College Park', 'Apple AI/ML', 'Computer Science, University of Maryland, College Park', 'University of Maryland, College Park', 'Princeton University', 'Carnegie Mellon University', 'Computer Science, University of Maryland', 'Carnegie Mellon University']","['United States', '', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=qoYogklIPz,Transparency & Explainability,Demystifying Embedding Spaces using Large Language Models,"Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing large language models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.",[],[],"['Guy Tennenholtz', 'Yinlam Chow', 'ChihWei Hsu', 'Jihwan Jeong', 'Lior Shani', 'Azamat Tulepbergenov', 'Deepak Ramachandran', 'Martin Mladenov', 'Craig Boutilier']","['Research, Google', 'Google Research', 'Google Research', 'University of Toronto', 'Google Research', 'Google', 'Google', 'CS Department, TU Dortmund University, Germany, TU Dortmund', 'Google']","['United States', 'United States', 'United States', 'Canada', 'United States', 'United States', 'United States', 'Germany', 'United States']"
https://openreview.net/forum?id=qBWhjsNPEY,Security,DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training,"Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled and practical ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model pruning methodology using only finite differences to explore and exploit the sparse DL prior in CGE. Third, we develop the methods of feature reuse and forward parallelization to advance the practical implementations of ZO training. Our extensive experiments show that DeepZero achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10, approaching FO training performance for the first time. Furthermore, we show the practical utility of DeepZero in applications of certified adversarial defense and DL-based partial differential equation error correction, achieving 10-20% improvement over SOTA. We believe our results will inspire future research on scalable ZO optimization and contribute to advancing deep learning.",[],[],"['Aochuan Chen', 'Yimeng Zhang', 'Jinghan Jia', 'James Diffenderfer', 'Konstantinos Parasyris', 'Jiancheng Liu', 'Yihua Zhang', 'Zheng Zhang', 'Bhavya Kailkhura', 'Sijia Liu']","['DSA, Hong Kong University of Science and Technology', 'ByteDance Inc.', 'Michigan State University', 'Lawrence Livermore National Labs', 'Lawrence Livermore National Labs', 'Computer Science and Engineering, Michigan State University', 'Computer Science and Engineering, Michigan State University', 'Electrical and Computer Engineering, University of California, Santa Barbara', 'CASC, Lawrence Livermore National Laboratory', 'CSE, Michigan State University']","['United States', '', 'United States', 'United States', 'United States', 'United States', 'United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=q4AEBLHuA6,Fairness & Bias,Solving High Frequency and Multi-Scale PDEs with Gaussian Processes,"Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by  Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next,  we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remaining toward the ground truth. Third, to enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. We use the GP conditional mean to predict the solution and its derivatives so as to fit the boundary condition and the equation itself.  As a result, we can derive a Kronecker product structure in the covariance matrix. We use Kronecker product properties and multilinear algebra to promote computational efficiency and scalability, without low-rank approximations. We show the advantage of our method in systematic experiments. The code is released at {https://github.com/xuangu-fang/Gaussian-Process-Slover-for-High-Freq-PDE}.",[],[],"['Shikai Fang', 'Madison Cooley', 'Da Long', 'Shibo Li', 'Mike Kirby', 'Shandian Zhe']","['Microsoft Research , Microsoft', '', 'Kahlert School of Computing , The University of Utah', 'Department of Computer Science, Florida State University', ', University of Utah', '']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=q3KNrmW6Ql,Fairness & Bias,Adversarial Attacks on Fairness of Graph Neural Networks,"Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs and guides further research on the robustness of GNNs in terms of fairness.",[],[],"['Binchi Zhang', 'Yushun Dong', 'Chen Chen', 'Yada Zhu', 'Minnan Luo', 'Jundong Li']","['Electrical and Computer Engineering, University of Virginia, Charlottesville', 'CS, Florida State University', 'University of Virginia, Charlottesville', 'IBM Research', ""School of Computer Science and Technology, Xi'an Jiaotong University"", 'University of Virginia']","['United States', 'United States', 'United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=pmweVpJ229,Privacy & Data Governance,Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy,"Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\varepsilon,\delta)$-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing $\delta$-approximation error into the privacy guarantees. To bridge this gap, we propose the Approximate SAample Perturbation (abbr. ASAP) algorithm which perturbs an MCMC sample with noise proportional to its Wasserstein-infinity ($W_\infty$) distance from a reference distribution that satisfies pure DP or pure Gaussian DP (i.e., $\delta=0$). We then leverage a Metropolis-Hastings algorithm to generate the sample and prove that the algorithm converges in W$_\infty$ distance. We show that by combining our new techniques with a localization step, we obtain the first nearly linear-time algorithm that achieves the optimal rates in the DP-ERM problem with strongly convex and smooth losses.",[],[],"['Yingyu Lin', 'Yian Ma', 'Yu-Xiang Wang', 'Rachel Emily Redberg', 'Zhiqi Bu']","['University of California, San Diego', 'University of California, San Diego', 'University of California, San Diego', 'Northeastern University', 'Amazon']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=plmBsXHxgR,Security,Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models,"We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",[],[],"['Erfan Shayegani', 'Yue Dong', 'Nael Abu-Ghazaleh']","['Computer Science, University of California, Riverside', 'University of California, Riverside', 'University of California, Riverside']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=oGNdBvymod,Fairness & Bias,Entropy-MCMC: Sampling from Flat Basins with Ease,"Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, targeting at the original posterior can lead to suboptimal performance, as some samples may become trapped in ""bad"" modes and suffer from overfitting. Leveraging the observation that ""good"" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our method and further show that it converges faster than several existing flatness-aware methods in the strongly convex setting. Empirical results demonstrate that our method can successfully sample from flat basins of the posterior, and outperforms all compared baselines on multiple benchmarks including classification, calibration, and out-of-distribution detection.",[],[],"['Bolian Li', 'Ruqi Zhang']","['Purdue University', 'Purdue University']","['United States', 'United States']"
https://openreview.net/forum?id=nTNgkEIfeb,Privacy & Data Governance,FedInverse: Evaluating Privacy Leakage in Federated Learning,"Federated Learning (FL) is a distributed machine learning technique where multiple devices (such as smartphones or IoT devices) train a shared global model by using their local data. FL claims that the data privacy of local participants is preserved well because local data will not be shared with either the server-side or other training participants. However, this paper discovers a pioneering finding that a model inversion (MI) attacker, who acts as a benign participant, can invert the shared global model and obtain the data belonging to other participants. This will lead to severe data-leakage risk in FL because it is difficult to identify attackers from benign participants. In addition, we found even the most advanced defense approaches could not effectively address this issue. Therefore, it is important to evaluate such data-leakage risks of an FL system before using it. To alleviate this issue, we propose FedInverse to evaluate whether the FL global model can be inverted by MI attackers. In particular, FedInverse can be optimized by leveraging the Hilbert-Schmidt independence criterion (HSIC) as a regularizer to adjust the diversity of the MI attack generator. We test FedInverse with three typical MI attackers, GMI, KED-MI, and VMI, and the experiments show our FedInverse method can successfully obtain the data belonging to other participants. The code of this work is available at https://github.com/Jun-B0518/FedInverse",[],[],"['Di Wu', 'Jun Bai', 'Yiliao Song', 'Junjun Chen', 'Wei Zhou', 'Yong Xiang', 'Atul Sajjanhar']","['School of Mathematics, Physics and Computing, University of Southern Queensland', 'School of Information technology, Deakin University', 'University of Adelaide', '', 'Monash University', 'Deakin University', 'School of Information Technology, Deakin University']","['Australia', 'Australia', 'Australia', '', 'Australia', 'Australia', 'Australia']"
https://openreview.net/forum?id=nnicaG5xiH,Transparency & Explainability,Interpretable Meta-Learning of Physical Systems,"Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural networks, resulting in high computational costs and limited interpretability. We introduce CAMEL, a new meta-learning architecture capable of learning efficiently from multiple environments, with an affine structure with respect to the learning task. We prove that CAMEL can identify the physical parameters of the system, enabling interpreable learning. We demonstrate the competitive generalization performance and the low computational cost of our method by comparing it to state-of-the-art algorithms on physical systems, ranging from toy models to complex, non-analytical systems. The interpretability of our method is illustrated with original applications to parameter identification and to adaptive control and system identification.",[],[],"['Matthieu Blanke', 'Marc Lelarge']","['Inria', 'ens']","['France', 'France']"
https://openreview.net/forum?id=nAs4LdaP9Y,Privacy & Data Governance,Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning,"Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is \textit{Global Catastrophic Forgetting}, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting in CFL. Our algorithm extracts the global input subspace of each layer for old tasks and modifies the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer. This decreases the interference between tasks, which is the main cause for forgetting. Our method is almost computation-free on the client side and has negligible communication cost. We empirically show that FOT outperforms state-of-the-art continual learning methods in the CFL setting, achieving an average accuracy gain of up to 15% with 27% lower forgetting while only incurring a minimal computation and communication cost. Code can be found [here ](https://github.com/duygunuryldz/Federated_Orthogonal_Training)",[],[],"['Yavuz Faruk Bakman', 'Duygu Nur Yaldiz', 'Yahya H. Ezzeldin', 'Salman Avestimehr']","['Computer Science, University of Southern California', 'University of Southern California', 'MediaTek USA Inc', '']","['United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=aFWUY3E7ws,Transparency & Explainability,Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction,"With the continuous advancement of neural network methodologies, time series prediction has attracted substantial interest over the past decades. Nonetheless, the interpretability of neural networks is insufficient and the utilization of deep learning techniques for prediction necessitates significant computational expenditures, rendering its application arduous in numerous scenarios. In order to tackle this challenge, an interpretable sparse system identification method which does not require a time-consuming training through back-propagation is proposed in this study. This method integrates advantages from both knowledge-based and data-driven approaches, and constructs dictionary functions by leveraging Fourier basis and taking into account both the long-term trends and the short-term fluctuations behind data. By using the $l_1$ norm for sparse optimization, prediction results can be gained with an explicit sparse expression function and an extremely high accuracy. The performance evaluation of the proposed method is conducted on comprehensive benchmark datasets, including ETT, Exchange, and ILI. Results reveal that our proposed method attains a significant overall improvement of more than 20\% in accordance with the most recent state-of-the-art deep learning methodologies. Additionally, our method demonstrates the efficient training capability on only CPUs. Therefore, this study may shed some light onto the realm of time series reconstruction and prediction.",[],[],"['Xiaoyi Liu', 'Duxin Chen', 'Wenjia Wei', 'Xia Zhu', 'Wenwu Yu']","['Southeast University', 'Southeast University', '', 'Huawei Technologies Ltd.', 'Southeast University']","['Bangladesh', 'Bangladesh', '', 'Bangladesh', 'Bangladesh']"
https://openreview.net/forum?id=mYhH0CDFFa,Security,Rethinking CNN’s Generalization to Backdoor Attack from Frequency Domain,"Convolutional  neural network  (CNN) is easily affected by backdoor injections, whose models perform normally on clean samples but produce specific outputs on poisoned ones. Most of the existing studies have focused on the effect of trigger feature changes of poisoned samples on model generalization in spatial domain. We focus on the mechanism of CNN memorize poisoned samples in frequency domain, and find that CNN generate generalization to poisoned samples by memorizing the frequency domain distribution of trigger changes. We also explore the influence of trigger perturbations in different frequency domain components on the generalization of poisoned models from visible and invisible backdoor attacks, and prove that high-frequency components are more susceptible to perturbations than low-frequency components. Based on the above fundings, we propose a universal invisible strategy for visible triggers, which can achieve trigger invisibility while maintaining raw attack performance. We also design a novel frequency domain backdoor attack method based on low-frequency semantic information, which can achieve 100\% attack accuracy on multiple models and multiple datasets, and can bypass multiple defenses.",[],[],"['Quanrui Rao', 'Lin Wang', 'Wuying Liu']","['Ludong University', 'Xianda College of Economics and Humanities Shanghai International Studies University', 'Ludong University']","['China', 'China', 'China']"
https://openreview.net/forum?id=8iTpB4RNvP,Security,Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection,"The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes \emph{Poisoned Forgery Face} framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses SoTA backdoor baselines with a significant improvement in attack success rate (+16.39\% BD-AUC) and reduction in visibility (-12.65\% $L_\infty$). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. Our codes will be made available at \url{https://github.com/JWLiang007/PFF}.",[],[],"['Jiawei Liang', 'Siyuan Liang', 'Aishan Liu', 'Xiaojun Jia', 'Junhao Kuang', 'Xiaochun Cao']","['SUN YAT-SEN UNIVERSITY', 'National University of Singapore', 'Beihang University', 'College of Computing and Data Science, Nanyang Technological University', 'SUN YAT-SEN UNIVERSITY', 'School of Cyber Science and Technology, SUN YAT-SEN UNIVERSITY']","['China', 'Singapore', 'Egypt', 'Singapore', 'China', 'China']"
https://openreview.net/forum?id=DqD59dQP37,Transparency & Explainability,Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework,"Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of observed confounding. This enables practitioners to audit the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framework in a series of experiments, including a real-world case study about predicting prison sentences. To the best of our knowledge, ours is the first work to study causal fairness under observed confounding. To this end, our work is of direct practical value for auditing and ensuring the fairness of predictions in high-stakes applications.",[],[],"['Maresa Schröder', 'Dennis Frauen', 'Stefan Feuerriegel']","['', 'Ludwig-Maximilians-Universität München', 'LMU Munich']","['', 'Germany', 'Germany']"
https://openreview.net/forum?id=m3RRWWFaVe,Fairness & Bias,DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCTION LEARNING,"Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs’ value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.",[],[],"['Shitong Duan', 'Xiaoyuan Yi', 'Peng Zhang', 'Tun Lu', 'Xing Xie', 'Ning Gu']","['Fudan University', 'Research, Microsoft', 'Fudan University', 'Computer Science, Fudan University', 'Microsoft Research Asia', 'School of computer, Fudan University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=TlyiaPXaVN,Security,Generative Adversarial Equilibrium Solvers,"We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds for Lipschitz-smooth function approximators in a large class of concave pseudo-games, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism.",[],[],"['Denizalp Goktas', 'David C. Parkes', 'Ian Gemp', 'Luke Marris', 'Georgios Piliouras', 'Romuald Elie', 'Guy Lever', 'Andrea Tacchetti']","['Brown University', 'Computer Science, Harvard University', 'Google DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'Google', 'DeepMind']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=kIP0duasBb,Fairness & Bias,Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models,"One fascinating aspect of pre-trained vision-language models (VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation (TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed \textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.",[],[],"['Shuai Zhao', 'Xiaohan Wang', 'Linchao Zhu', 'Yi Yang']","['University of Technology Sydney', 'Stanford University', 'College of Computer Science and Technology, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University']","['Australia', 'United States', 'China', 'China']"
https://openreview.net/forum?id=NLPzL6HWNl,Privacy & Data Governance,Improving LoRA in Privacy-preserving Federated Learning,"Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module. However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs. The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL.  Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.",[],[],"['Youbang Sun', 'Zitao Li', 'Yaliang Li', 'Bolin Ding']","['Northeastern University', 'Alibaba Group (U.S.)', 'Alibaba Group', 'Alibaba Group']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=jr03SfWsBS,Fairness & Bias,Unprocessing Seven Years of Algorithmic Fairness,"Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation.",[],[],"['André Cruz', 'Moritz Hardt']","['Max Planck Institute for Intelligent Systems', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute']","['Belgium', 'Brazil']"
https://openreview.net/forum?id=jlEjB8MVGa,Security,How Does Unlabeled Data Provably Help Out-of-Distribution Detection?,"Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier. Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. Code is publicly available at https://github.com/deeplearning-wisc/sal.",[],[],"['Xuefeng Du', 'Zhen Fang', 'Ilias Diakonikolas', 'Yixuan Li']","['University of Wisconsin, Madison', 'AAII, University of Technology Sydney', 'Computer Sciences, University of Wisconsin - Madison', 'University of Wisconsin, Madison']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Cu5wJa5LGO,Transparency & Explainability,LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer,"Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents). To address these challenges, we introduce a causal framework for video reasoning, termed Learning Latent Causal Processes (LLCP). At the heart of LLCP lies a multivariate generative model designed to analyze the spatial-temporal dynamics of objects within events. Leveraging the inherent modularity of causal mechanisms, we train the model through self-supervised local auto-regression eliminating the need for annotated question-answer pairs. During inference, the model is applied to answer two types of reasoning questions: accident attribution, which infers the cause from observed effects, and counterfactual prediction, which predicts the effects of counterfactual conditions given the factual evidence. In the first scenario, we identify variables that deviate from the established distribution by the learned model, signifying the root cause of accidents. In the second scenario, we replace embeddings of previous variables with counterfactual ones, enabling us to forecast potential developments. Once we have identified these cause/effect variables, natural language answers are derived through a combination of grammatical parsing and a pre-trained vision-language model. We assess the efficacy of LLCP on both synthetic and real-world data, demonstrating comparable performance to supervised methods despite our framework using no paired textual annotations.",[],[],"['Guangyi Chen', 'Yuke Li', 'Xiao Liu', 'Zijian Li', 'Eman Al Suradi', 'Donglai Wei', 'Kun Zhang']","['Machine Learning, Mohamed bin Zayed University of Artificial Intelligence', '', 'Computer Science, Technische Universität Darmstadt', 'Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence', 'Boston College', 'Mohamed bin Zayed University of Artificial Intelligence']","['United Arab Emirates', '', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates']"
https://openreview.net/forum?id=j5JvZCaDM0,Security,Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model,"Safe offline reinforcement learning is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning, which can be effectively extracted with a guided diffusion model thanks to its expressiveness.  We compare FISOR against baselines on DSRL benchmark for safe offline RL. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks. Code: https://github.com/ZhengYinan-AIR/FISOR.",[],[],"['Yinan Zheng', 'Jianxiong Li', 'Dongjie Yu', 'Yujie Yang', 'Shengbo Eben Li', 'Xianyuan Zhan', 'Jingjing Liu']","['Tsinghua University, Tsinghua University', 'AIR, Tsinghua University', 'University of Hong Kong', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Institute for AI Industry Research, Tsinghua University, Tsinghua University', 'Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=vrE2fqAInO,Privacy & Data Governance,Fixed-Budget Differentially Private Best Arm Identification,"We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval.  Given a finite budget $T$ and a privacy parameter  $\varepsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\em $\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}), based on the principle of {\em maximum absolute determinants}, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and (c) the problem complexity that is expressible as the sum of two terms, one characterising the complexity of standard fixed-budget BAI (without privacy constraints), and the other accounting for the $\varepsilon$-DP constraint. Additionally, we present some auxiliary results that contribute to the derivation of the lower bound on the error probability. These results, we posit, may be of independent interest and could prove instrumental in proving lower bounds on error probabilities in several other bandit problems. Whereas prior works provide results for BAI in the fixed-budget regime without privacy constraints or in the fixed-confidence regime with privacy constraints, our work fills the gap in the literature by providing the results for BAI in the fixed-budget regime under the $\varepsilon$-DP constraint.",[],[],"['Zhirui Chen', 'P. N. Karthik', 'Yeow Meng Chee', 'Vincent Tan']","['National University of Singapore', 'Artificial Intelligence, Indian Institute of Technology, Hyderabad', 'National University of Singapore', 'Mathematics, National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://openreview.net/forum?id=CEkIyshNbC,Fairness & Bias,Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space,"Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory for its occurring mechanism in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that the presence of noisy data strongly influences its occurrence. By comprehensively analysing the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that while small and intermediate models before the interpolation threshold follow the traditional bias-variance trade-off, over-parameterized models interpolate noisy samples among robust data thus acquiring the capability to separate the information from the noise. The source code is available at \url{https://github.com/Yufei-Gu-451/double_descent_inference.git}.",[],[],"['Yufei Gu', 'Xiaoqing Zheng', 'Tomaso Aste']","['Data Science and Analytics Thrust, The Hong Kong University of Science and Technology', '', 'University College London, University of London']","['Hong Kong', '', 'United States']"
https://openreview.net/forum?id=CGlczSBBSj,Fairness & Bias,SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution,"Real-world Super-Resolution (Real-SR) methods focus on dealing with diverse real-world images and have attracted increasing attention in recent years. The key idea is to use a complex and high-order degradation model to mimic real-world degradations.  Although they have achieved impressive results in various scenarios, they are faced with the obstacle of evaluation. Currently, these methods are only assessed by their average performance on a small set of degradation cases randomly selected from a large space, which fails to provide a comprehensive understanding of their overall performance and often yields inconsistent and potentially misleading results. To overcome the limitation in evaluation, we propose SEAL, a framework for systematic evaluation of real-SR. In particular, we cluster the extensive degradation space to create a set of representative degradation cases, which serves as a comprehensive test set. Next, we propose a coarse-to-fine evaluation protocol to measure the distributed and relative performance of real-SR methods on the test set. The protocol incorporates two new metrics: acceptance rate (AR) and relative performance ratio (RPR), derived from acceptance and excellence lines. Under SEAL, we benchmark existing real-SR methods, obtain new observations and insights into their performance, and develop a new strong baseline. We consider SEAL as the first step towards creating an unbiased and comprehensive real-SR evaluation platform, which can promote the development of real-SR.",[],[],"['Wenlong Zhang', 'Xiaohui Li', 'Xiangyu Chen', 'Xiaoyun Zhang', 'Yu Qiao', 'Xiao-Ming Wu', 'Chao Dong']","['AI4Science, Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'University of Macau', 'Cooperative Medianet Innovation Center, Shanghai Jiaotong University', '', 'Hong Kong Polytechnic University', 'Institute of Advanced Computing and Digital Engineering, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences']","['China', 'China', 'China', 'China', '', 'China', '']"
https://openreview.net/forum?id=iCNOK45Csv,Security,Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.",[],[],"['Ming-Yu Chung', 'Sheng-Yen Chou', 'Chia-Mu Yu', 'Pin-Yu Chen', 'Sy-Yen Kuo', 'Tsung-Yi Ho']","['Electrical & Computer Engineering, Duke University', 'Computer Science and Engineering, Cornell University', 'Electronics and Electrical Engineering, National Yang Ming Chiao Tung University', 'International Business Machines', 'Electrical Engineering, National Taiwan University', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong']","['United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=mMaQvkMzDi,Transparency & Explainability,Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning,"Following the success of Large Language Models (LLMs), Large Multimodal Models (LMMs), such as the Flamingo model and its subsequent competitors, have started to emerge as natural steps towards generalist agents. However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations. To refine our understanding of those flaws, we deviate from the current evaluation paradigm, and (1) evaluate 10 recent open-source LMMs from 3B up to 80B parameter scale,  on 5 different axes; hallucinations, abstention, compositionality, explainability and instruction following. Our evaluation on these axes reveals major flaws in LMMs. While the current go-to solution to align these models is based on training, such as instruction tuning or RLHF, we rather (2) explore the training-free in-context learning (ICL) as a solution, and study how it affects these limitations. Based on our ICL study, (3) we push ICL further and propose new multimodal ICL variants such as; Multitask-ICL, Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows; (1) Despite their success, LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, answer abstention, ICL only slightly improves instruction following, does not improve compositional abilities, and actually even amplifies hallucinations. (3) The proposed ICL variants are promising as post-hoc approaches to efficiently tackle some of those flaws. The code is available here: https://github.com/mshukor/EvALign-ICL.",[],[],"['Mustafa Shukor', 'Alexandre Rame', 'Corentin Dancette', 'Matthieu Cord']","['Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Google', 'Raidium', 'Sorbonne Université']","['United States', '', 'India', 'Germany']"
https://openreview.net/forum?id=I4wB3HA3dJ,Fairness & Bias,Domain-Inspired Sharpness-Aware Minimization Under Domain Shifts,"This paper presents a Domain-Inspired Sharpness-Aware Minimization (DISAM) algorithm for optimization under domain shifts. It is motivated by the inconsistent convergence degree of SAM across different domains, which induces optimization bias towards certain domains and thus impairs the overall convergence. To address this issue, we consider the domain-level convergence consistency in the sharpness estimation to prevent the overwhelming (deficient) perturbations for less (well) optimized domains. Specifically, DISAM introduces the constraint of minimizing variance in the domain loss, which allows the elastic gradient calibration in perturbation generation: when one domain is optimized above the averaging level w.r.t. loss, the gradient perturbation towards that domain will be weakened automatically, and vice versa. Under this mechanism, we theoretically show that DISAM can achieve faster overall convergence and improved generalization in principle when inconsistent convergence emerges. Extensive experiments on various domain generalization benchmarks show the superiority of DISAM over a range of state-of-the-art methods. Furthermore, we show the superior efficiency of DISAM in parameter-efficient fine-tuning combined with the pretraining models. The source code is released at https://github.com/MediaBrain-SJTU/DISAM.",[],[],"['Ruipeng Zhang', 'Ziqing Fan', 'Jiangchao Yao', 'Ya Zhang', 'Yanfeng Wang']","[""Institute of Diagnostic and Interventional Radiology, Shanghai Sixth People's Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai Jiaotong University"", '', 'CMIC, Shanghai Jiaotong University', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'School of Artificial Intelligence, Shanghai Jiao Tong University']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=E1NxN5QMOE,Fairness & Bias,Enhancing Group Fairness in Online Settings Using Oblique Decision Forests,"Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion – one instance at a time – optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes. We also present an efficient framework to train Aranyani and theoretically analyze several of its properties. We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches.",[],[],"['Somnath Basu Roy Chowdhury', 'Nicholas Monath', 'Ahmad Beirami', 'Rahul Kidambi', 'Kumar Avinava Dubey', 'Amr Ahmed', 'Snigdha Chaturvedi']","['Department of Computer Science, University of North Carolina, Chapel Hill', 'Google', 'Google', '', 'Google Research', 'Research, Google', 'Computer Science, Department of Computer Science, University of North Carolina at Chapel Hill']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=gMLQwKDY3N,Security,An Unforgeable Publicly Verifiable Watermark for Large Language Models,"Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection. To address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently. Experiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at https://github.com/THU-BPM/unforgeable_watermark",[],[],"['Aiwei Liu', 'Leyi Pan', 'Xuming Hu', 'Shuang Li', 'Lijie Wen', 'Irwin King', 'Philip S. Yu']","['Tsinghua University', 'School of Software, Tsinghua University', 'AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)', 'Tencent', 'School of Software, Tsinghua University', 'Computer Science and Engineering, The Chinese University of Hong Kong', 'Computer Science, University of Illinois, Chicago']","['China', 'China', 'Hong Kong', '', 'China', 'Hong Kong', 'United States']"
https://openreview.net/forum?id=sGVmr7KHfn,Transparency & Explainability,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.",[],[],"['Yuxiang Lai', 'Yi Zhou', 'Xinghong Liu', 'Tao Zhou']","['Emory University', 'School of Computer Science and Engineering, Southeast University', '', 'Computer science, Nanjing University of Science and Technology']","['United States', 'Bangladesh', '', 'China']"
https://openreview.net/forum?id=BOfDKxfwt0,Security,LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset,"Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",[],[],"['Lianmin Zheng', 'Wei-Lin Chiang', 'Ying Sheng', 'Tianle Li', 'Siyuan Zhuang', 'Zhanghao Wu', 'Yonghao Zhuang', 'Zhuohan Li', 'Zi Lin', 'Eric Xing', 'Joseph E. Gonzalez', 'Ion Stoica', 'Hao Zhang']","['University of California Berkeley', 'University of California, Berkeley', 'Stanford University', 'EECS, University of California, Berkeley', 'EECS, University of California Berkeley', 'University of California Berkeley', 'CMU, Carnegie Mellon University', 'University of California Berkeley', 'University of California, San Diego', 'Mohamed bin Zayed Univeristy of AI', 'EECS, University of California, Berkeley', 'EECS, University of California, Berkeley', 'University of California, San Diego']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=qz3mcn99cu,Security,A Recipe for Improved Certifiable Robustness,"Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training.  However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards \emph{underfitting} than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large ``Cholesky-orthogonalized residual dense'' layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points.",[],[],"['Kai Hu', 'Klas Leino', 'Zifan Wang', 'Matt Fredrikson']","['Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Scale AI', 'Gray Swan AI']","['United States', 'United States', '', '']"
https://openreview.net/forum?id=ezscMer8L0,Fairness & Bias,Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model,"The Segment-Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM’s local prior assumption. Notably, Conv-LoRA not only preserves SAM’s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM’s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA’s superiority in adapting SAM to real-world semantic segmentation tasks.",[],[],"['Zihan Zhong', 'Zhiqiang Tang', 'Tong He', 'Haoyang Fang', 'Chun Yuan']","['Tsinghua University, Tsinghua University', 'AWS', 'Amazon', 'Amazon', 'SIGS, Tsinghua University, Tsinghua University']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=lUYY2qsRTI,Fairness & Bias,Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding,"A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive experiments and ablations the efficacy of our approach on a sepsis management benchmark, as well as on electronic health records. Our results suggest that nonidentifiable hidden confounding bias can be mitigated to improve offline RL solutions in practice.",[],[],"['Alizée Pace', 'Hugo Yèche', 'Bernhard Schölkopf', 'Gunnar Ratsch', 'Guy Tennenholtz']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', '', 'Swiss Federal Institute of Technology', 'Research, Google']","['Switzerland', 'Switzerland', '', 'Switzerland', 'United States']"
https://openreview.net/forum?id=MREQ0k6qvD,Transparency & Explainability,One-hot Generalized Linear Model for Switching Brain State Discovery,"Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel prior-informed state-switching GLMs, called Gaussian HMM-GLM (Gaussian prior) and one-hot HMM-GLM (Gumbel-Softmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data. The code is available at \url{https://github.com/JerrySoybean/onehot-hmmglm}.",[],[],"['Chengrui Li', 'Soon Ho Kim', 'Chris Rodgers', 'Hannah Choi', 'Anqi Wu']","['School of Computational Science & Engineering, Georgia Institute of Technology', 'Georgia Institute of Technology', 'Emory University', 'Georgia Institute of Technology', 'Georgia Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=eMHn77ZKOp,Fairness & Bias,Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback,"We investigate the combinatorial multi-armed bandit problem where an action is to select $k$ arms from a set of base arms, and its reward is the maximum of the sample values of these $k$ arms, under a weak feedback structure that only returns the value and index of the arm with the maximum value. This novel feedback structure is much weaker than the semi-bandit feedback previously studied and is only slightly stronger than the full-bandit feedback, and thus it presents a new challenge for the online learning task. We propose an algorithm and derive a regret bound for instances where arm outcomes follow distributions with finite supports. Our algorithm introduces a novel concept of biased arm replacement to address the weak feedback challenge, and it achieves a distribution-dependent regret bound of $O((k/\Delta)\log(T))$ and a distribution-independent regret bound of $\tilde{O}(\sqrt{T})$, where $\Delta$ is the reward gap and $T$ is the time horizon.  Notably, our regret bound is comparable to the bounds obtained under the more informative semi-bandit feedback.  We demonstrate the effectiveness of our algorithm through experimental results.",[],[],"['Yiliu Wang', 'Wei Chen', 'Milan Vojnovic']","['Allen Institute', 'Microsoft Research', 'London School of Economics']","['United States', 'United States', 'United Kingdom']"
https://openreview.net/forum?id=Ad81awoBVS,Fairness & Bias,Rotation Has Two Sides: Evaluating Data Augmentation for Deep One-class Classification,"One-class classification (OCC) involves predicting whether a new data is normal or anomalous based solely on the data from a single class during training. Various attempts have been made to learn suitable representations for OCC within a self-supervised framework. Notably, discriminative methods that use geometric visual transformations, such as rotation, to generate pseudo-anomaly samples have exhibited impressive detection performance. Although rotation is commonly viewed as a distribution-shifting transformation and is widely used in the literature, the cause of its effectiveness remains a mystery. In this study, we are the first to make a surprising observation: there exists a strong linear relationship (Pearson's Correlation, $r > 0.9$) between the accuracy of rotation prediction and the performance of OCC. This suggests that a classifier that effectively distinguishes different rotations is more likely to excel in OCC, and vice versa. The root cause of this phenomenon can be attributed to the transformation bias in the dataset, where representations learned from transformations already present in the dataset tend to be less effective, making it essential to accurately estimate the transformation distribution before utilizing pretext tasks involving these transformations for reliable self-supervised representation learning. To the end, we propose a novel two-stage method to estimate the transformation distribution within the dataset. In the first stage, we learn general representations through standard contrastive pre-training. In the second stage, we select potentially semantics-preserving samples from the entire augmented dataset, which includes all rotations, by employing density matching with the provided reference distribution. By sorting samples based on semantics-preserving versus shifting transformations, we achieve improved performance on OCC benchmarks.",[],[],"['Guodong Wang', 'Yunhong Wang', 'Xiuguo Bao', 'Di Huang']","['Beijing University of Aeronautics and Astronautics', '', 'Coordination Center of China', 'School of Computer Science and Engineering, Beihang University']","['China', '', 'China', '']"
https://openreview.net/forum?id=kUveo5k1GF,Fairness & Bias,Improving equilibrium propagation without weight symmetry through Jacobian homeostasis,"Equilibrium propagation (EP) is a compelling alternative to the back propagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates.  Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to yield unbiased gradient estimates. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry contributes to bias is unknown because, in practice, its contribution may be masked by a finite nudge.  To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that bias due to finite nudge can be avoided by estimating exact derivatives via a Cauchy integral. In contrast, weight asymmetry induces residual bias  through poor alignment of EP's neuronal error vectors compared to BP resulting in low task performance. To mitigate the latter issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point.  This homeostatic objective dramatically improves the network's ability to solve complex tasks such as ImageNet 32$\times$32.  Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate's relaxation dynamics.",[],[],"['Axel Laborieux', 'Friedemann Zenke']","['Neuromorphic, Huawei Technologies Ltd.', 'University of Basel']","['Hungary', 'Switzerland']"
https://openreview.net/forum?id=RvfPnOkPV4,Transparency & Explainability,What's In My Big Data?,"Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities---count and search---*at scale*, which allows us to analyze more than 35 terabytes on a standard compute node.  We apply WIMBD to ten different corpora used to train popular language models, including *C4*, *The Pile*, and *RedPajama*. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.  For instance, we find that about 50% of the documents in *RedPajama* and *LAION-2B-en* are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",[],[],"['Yanai Elazar', 'Akshita Bhagia', 'Ian Helgi Magnusson', 'Abhilasha Ravichander', 'Dustin Schwenk', 'Alane Suhr', 'Evan Pete Walsh', 'Dirk Groeneveld', 'Luca Soldaini', 'Sameer Singh', 'Hannaneh Hajishirzi', 'Noah A. Smith', 'Jesse Dodge']","['Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', '', 'Computer Science, University of Washington', 'Allen Institute for Artificial Intelligence', 'EECS, University of California, Berkeley', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'University of California, Irvine', 'CS, Allen Institute for Artificial Intelligence', 'Computer Science & Engineering, University of Washington', 'Allen Institute for Artificial Intelligence']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=dKl6lMwbCy,Fairness & Bias,Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models,"Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments, for a particular comparison instance. To our surprise, we observe that the choice of feedback protocol has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs that leverage rankings data for alignment (say model X) are preferred over those that leverage ratings data (say model Y), with a rank-based evaluation protocol (is X/Y's response better than reference response?) but not with a rating-based evaluation protocol (score Rank X/Y's response on a scale of 1-7). Our findings thus shed light on critical gaps in methods for evaluating the real-world utility of language models and their strong dependence on the feedback protocol used for alignment. Our code and data are available at \url{https://github.com/Hritikbansal/sparse_feedback}.",[],[],"['Hritik Bansal', 'John Dang', 'Aditya Grover']","['University of California, Los Angeles', 'Cohere For AI, Cohere', 'Computer Science, University of California, Los Angeles']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=dLoAdIKENc,Security,Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks,"In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with watermarking emerging as a promising approach. In this paper, we analyze the robustness of various AI-image detectors including watermarking and classifier-based deepfake detectors. For watermarking methods that introduce subtle image perturbations (i.e., low perturbation budget methods), we reveal a fundamental trade-off between the evasion error rate (i.e., the fraction of watermarked images detected as non-watermarked ones) and the spoofing error rate (i.e., the fraction of non-watermarked images detected as watermarked ones) upon an application of a diffusion purification attack. In this regime, we also empirically show that diffusion purification effectively removes watermarks with minimal changes to images. For high perturbation watermarking methods where notable changes are applied to images, the diffusion purification attack is not effective. In this case, we develop a model substitution adversarial attack that can successfully remove watermarks. Moreover, we show that watermarking methods are vulnerable to spoofing attacks where the attacker aims to have real images (potentially obscene) identified as watermarked ones, damaging the reputation of the developers. In particular, by just having black-box access to the watermarking method, we show that one can generate a watermarked noise image which can be added to the real images to have them falsely flagged as watermarked ones. Finally, we extend our theory to characterize a fundamental trade-off between the robustness and reliability of classifier-based deep fake detectors and demonstrate it through experiments. Code is available at https://github.com/mehrdadsaberi/watermark_robustness.",[],[],"['Mehrdad Saberi', 'Vinu Sankar Sadasivan', 'Keivan Rezaei', 'Aounon Kumar', 'Atoosa Chegini', 'Wenxiao Wang', 'Soheil Feizi']","['Computer Science, Department of Computer Science, University of Maryland, College Park', 'University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'School of Engineering and Applied Sciences, Harvard University', 'University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=cxfPefbu1s,Fairness & Bias,Procedural Fairness Through Decoupling Objectionable Data Generating Components,"We reveal and address the frequently overlooked yet important issue of _disguised procedural unfairness_, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for _pure procedural justice_ (Rawls, 1971; 2001), we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing _disguised procedural unfairness_, drawing attention not only to the objectionable data generating components that we aim to mitigate, but also more importantly, to the neutral components that we intend to keep unaffected.",[],[],"['Zeyu Tang', 'Jialu Wang', 'Yang Liu', 'Peter Spirtes', 'Kun Zhang']","['Carnegie Mellon University', 'University of California, Santa Cruz', 'Computer Science and Engineering, University of California, Santa Cruz', 'Philosophy, Carnegie Mellon University', 'Mohamed bin Zayed University of Artificial Intelligence']","['United States', '', '', 'United States', 'United States']"
https://openreview.net/forum?id=cXbnGtO0NZ,Fairness & Bias,Latent 3D Graph Diffusion,"Generating 3D graphs of symmetry-group equivariance is of intriguing potential in broad applications from machine vision to molecular discovery. Emerging approaches adopt diffusion generative models (DGMs) with proper re-engineering to capture 3D graph distributions. In this paper, we raise an orthogonal and fundamental question of in what (latent) space we should diffuse 3D graphs. ❶ We motivate the study with theoretical analysis showing that the performance bound of 3D graph diffusion can be improved in a latent space versus the original space, provided that the latent space is of (i) low dimensionality yet (ii) high quality (i.e., low reconstruction error) and DGMs have (iii) symmetry preservation as an inductive bias. ❷ Guided by the theoretical guidelines, we propose to perform 3D graph diffusion in a low-dimensional latent space, which is learned through cascaded 2D–3D graph autoencoders for low-error reconstruction and symmetry-group invariance. The overall pipeline is dubbed latent 3D graph diffusion. ❸ Motivated by applications in molecular discovery, we further extend latent 3D graph diffusion to conditional generation given SE(3)-invariant attributes or equivariant 3D objects. ❹ We also demonstrate empirically that out-of-distribution conditional generation can be further improved by regularizing the latent space via graph self-supervised learning. We validate through comprehensive experiments that our method generates 3D molecules of higher validity / drug-likeliness and comparable or better conformations / energetics, while being an order of magnitude faster in training. Codes are released at https://github.com/Shen-Lab/LDM-3DG.",[],[],"['Yuning You', 'Ruida Zhou', 'Jiwoong Park', 'Haotian Xu', 'Chao Tian', 'Zhangyang Wang', 'Yang Shen']","['California Institute of Technology', 'University of California, Los Angeles', 'Northeastern University', 'Department of Applied Mathematics and Statistics, Stony Brook University', '', 'University of Texas at Austin', 'Texas A&M University - College Station']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=HhfcNgQn6p,Fairness & Bias,Towards a statistical theory of data selection under weak supervision,"Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning.  Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $x_{i}$, and to be given access to a  'surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by {$x_{i}$}$_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization. By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$ Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$ Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.",[],[],"['Germain Kolossov', 'Andrea Montanari', 'Pulkit Tandon']","['Granica', 'Stanford University', 'Granica']","['Peru', 'Peru', 'Peru']"
https://openreview.net/forum?id=sLdVl0q68X,Transparency & Explainability,NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling,"Spatio-temporal (ST) prediction plays a pivotal role in earth sciences, such as meteorological prediction, urban computing. Adequate high-quality data, coupled with deep models capable of inference, are both indispensable and prerequisite for achieving meaningful results. However, the sparsity of data and the high costs associated with deploying sensors lead to significant data imbalances. Models that are overly tailored and lack causal relationships further compromise the generalizabilities of inference methods. Towards this end, we first establish a causal concept for ST predictions, named  NuwaDynamics, which targets to identify causal regions in data and endow model with causal reasoning ability in a two-stage process. Concretely, we initially leverage upstream self-supervision to discern causal important patches, imbuing the model with generalized information and conducting informed interventions on complementary trivial patches to extrapolate potential test distributions. This phase is referred to as the discovery step. Advancing beyond discovery step, we transfer the data to downstream tasks for targeted ST objectives, aiding the model in recognizing a broader potential distribution and fostering its causal perceptual capabilities (refer as Update step). Our concept aligns seamlessly with the contemporary backdoor adjustment mechanism in causality theory. Extensive experiments on six real-world ST benchmarks showcase that models can gain outcomes upon the integration of the NuwaDynamics concept. NuwaDynamics also can significantly benefit a wide range of changeable ST tasks like extreme weather and long temporal step super-resolution predictions.",[],[],"['Kun Wang', 'Hao Wu', 'Yifan Duan', 'Guibin Zhang', 'Kai Wang', 'Xiaojiang Peng', 'Yu Zheng', 'Yuxuan Liang', 'Yang Wang']","['Nanyang Technological University', '', 'Software Engineering, University of Science and Technology of China', '', 'soc, national university of singaore, National University of Singapore', 'Shenzhen Technology University', 'JD.COM', 'INTR & DSA, The Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China']","['Singapore', '', 'China', '', 'United States', 'China', '', 'Hong Kong', 'China']"
https://openreview.net/forum?id=DmD1wboID9,Fairness & Bias,BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction,"As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs. Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks.",[],[],"['Jiangmeng Li', 'Fei Song', 'Yifan Jin', 'Wenwen Qiang', 'Changwen Zheng', 'Fuchun Sun', 'Hui Xiong']","['National Key Laboratory of Space Integrated Information System, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'University of the Chinese Academy of Sciences', 'Institute of Software Chinese Academy of Sciences', 'National Key Laboratory of Space Integrated Information System, Institute of Software, Chinese Academy of Sciences', 'Department of Computer Science and Technology, Tsinghua University', 'Hong Kong University of Science and Technology (Guangzhou)']","['China', 'United States', 'United States', 'China', 'China', 'China', 'Hong Kong']"
https://openreview.net/forum?id=k9SVcrmXL8,Fairness & Bias,BECLR: Batch Enhanced Contrastive Few-Shot Learning,"Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios where FSL approaches suffer the most from sample bias. We later on discuss that DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we coin as BECLR), constructively magnifying each other's impact. We then present a suite of extensive quantitative and qualitative experimentation to corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL benchmarks (to the best of our knowledge), and significantly outperforms the best of the current baselines (codebase available at https://github.com/stypoumic/BECLR).",[],[],"['Stylianos Poulakakis-Daktylidis', 'Hadi Jamali-Rad']","['Delft University of Technology', 'Delft University of Technology']","['Netherlands', 'Netherlands']"
https://openreview.net/forum?id=b3Cu426njo,Transparency & Explainability,Meta-Learning Priors Using Unrolled Proximal Networks,"Relying on prior knowledge accumulated from related tasks, meta-learning offers a powerful approach to learning a novel task from a limited number of training data. Recent approaches use a family of prior probability density functions or recurrent neural network models, whose parameters can be optimized by utilizing labeled data from the observed tasks. While these approaches have appealing empirical performance, expressiveness of their prior is relatively low, which limits generalization and interpretation of meta-learning. Aiming at expressive yet meaningful priors, this contribution puts forth a novel prior representation model that leverages the notion of algorithm unrolling.  The key idea is to unroll the proximal gradient descent steps, where learnable piecewise linear functions are developed to approximate the desired proximal operators within *tight* theoretical error bounds established for both smooth and non-smooth proximal functions. The resultant multi-block neural network not only broadens the scope of learnable priors, but also enhances interpretability from an optimization viewpoint. Numerical tests conducted on few-shot learning datasets demonstrate markedly improved performance with flexible, visualizable, and understandable priors.",[],[],"['Yilang Zhang', 'Georgios B. Giannakis']","['Electrical and Computer Engineering, University of Minnesota - Twin Cities', 'University of Minnesota, Minneapolis']","['United States', 'United States']"
https://openreview.net/forum?id=ag3o2T51Ht,Security,Circumventing Concept Erasure Methods For Text-To-Image Generative Models,"Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to ""erase"" sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we leverage the existence of special learned word embeddings that can retrieve ""erased"" concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety.",[],[],"['Minh Pham', 'Kelly O. Marshall', 'Niv Cohen', 'Govind Mittal', 'Chinmay Hegde']","['Computer Science, New York University', 'New York University', 'Computer Science, New York University', 'Computer Science and Engineering, New York University', 'New York University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=s8cMuxI5gu,Security,Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks,"Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction. We believe soft labels in classification tasks are worth further attention in gradient inversion attacks.",[],[],"['Yanbo Wang', 'Jian Liang', 'Ran He']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'NLPR, Institute of automation, Chinese academy of science, Chinese Academy of Sciences']","['China', 'China', 'China']"
https://openreview.net/forum?id=plebgsdiiV,Fairness & Bias,Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies,"We consider off-policy evaluation (OPE) of deterministic target policies for reinforcement learning (RL) in environments with continuous action spaces. While it is common to use importance sampling for OPE, it suffers from high variance when the behavior policy deviates significantly from the target policy. In order to address this issue, some recent works on OPE proposed in-sample learning with importance resampling. Yet, these approaches are not applicable to deterministic target policies for continuous action spaces. To address this limitation, we propose to relax the deterministic target policy using a kernel and learn the kernel metrics that minimize the overall mean squared error of the estimated temporal difference update vector of an action value function, where the action value function is used for policy evaluation. We derive the bias and variance of the estimation error due to this relaxation and provide analytic solutions for the optimal kernel metric. In empirical studies using various test domains, we show that the OPE with in-sample learning using the kernel with optimized metric achieves significantly improved accuracy than other baselines.",[],[],"['Haanvid Lee', 'Tri Wahyu Guntara', 'Jongmin Lee', 'Yung-Kyun Noh', 'Kee-Eung Kim']","['Borealis AI', 'KRAFTON', 'University of California, Berkeley', 'Hanyang University', 'Korea Advanced Institute of Science and Technology']","['Serbia', '', 'United States', 'China', '']"
https://openreview.net/forum?id=aGH43rjoe4,Transparency & Explainability,Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data,"Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning latent variability into components that are either shared between or independent to each modality. We parameterize the latents of our model in the Fourier domain, and show improved latent identification using this approach over standard GP-VAE methods. We validate our model on simulated multi-modal data consisting of Poisson spike counts and MNIST images that scale and rotate smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to not only identify the shared and independent latent structure across modalities accurately, but provides good reconstructions of both images and neural rates on held-out trials. Finally, we demonstrate our framework on two real world multi-modal experimental settings: Drosophila whole-brain calcium imaging alongside tracked limb positions, and Manduca sexta spike train measurements from ten wing muscles as the animal tracks a visual stimulus.",[],[],"['Rabia Gondur', 'Usama Bin Sikandar', 'Evan Schaffer', 'Mikio Christian Aoi', 'Stephen L Keeley']","['Cold Spring Harbor Laboratory', 'Georgia Institute of Technology', 'Columbia University', 'University of California, San Diego', 'Fordham University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=eUgS9Ig8JG,Fairness & Bias,SaNN: Simple Yet Powerful Simplicial-aware Neural Networks,"Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in SNNs is enormous. In this work, we propose a scalable simplicial-aware neural network (SaNN) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. SaNN is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which SaNN is provably more powerful than the Weisfeiler-Lehman (WL) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman (SWL) test. We also show that SaNN is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories,  simplicial closures, and classifying graphs.",[],[],"['Sravanthi Gurugubelli', 'Sundeep Prabhakar Chepuri']","['Indian Institute of Science, Bangalore', 'Indian Institute of Science']","['India', 'India']"
https://openreview.net/forum?id=d3xKPQVjSc,Fairness & Bias,Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation,"State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional representations can lose information about the observed confounders and thus lead to bias, because of which the validity of representation learning for CATE estimation is typically violated. In this paper, we propose a new, representation-agnostic refutation framework for estimating bounds on the representation-induced confounding bias that comes from dimensionality reduction (or other constraints on the representations) in CATE estimation. First, we establish theoretically under which conditions CATE is non-identifiable given low-dimensional (constrained) representations. Second, as our remedy, we propose a neural refutation framework which performs partial identification of CATE or, equivalently, aims at estimating lower and upper bounds of the representation-induced confounding bias. We demonstrate the effectiveness of our bounds in a series of experiments. In sum, our refutation framework is of direct relevance in practice where the validity of CATE estimation is of importance.",[],[],"['Valentyn Melnychuk', 'Dennis Frauen', 'Stefan Feuerriegel']","['Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', 'LMU Munich']","['Germany', 'Germany', 'Germany']"
https://openreview.net/forum?id=ZULjcYLWKe,Security,DMBP: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations,"Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of ran- dom noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP.",[],[],"['Zhihe YANG', 'Yunjian Xu']","['Mechanical and Automation Engineering, The Chinese University of Hong Kong', 'The Chinese University of Hong Kong']","['Hong Kong', 'Hong Kong']"
https://openreview.net/forum?id=YgMdDQB09U,Fairness & Bias,AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning,"Self-supervised learning through contrastive representations is an emergent and promising avenue, aiming at alleviating the availability of labeled data. Recent research in the field also demonstrates its viability for several downstream tasks, henceforth leading to works that implement the contrastive principle through innovative loss functions and methods. However, despite achieving impressive progress, most methods depend on prohibitively large batch sizes and compute requirements for good performance.  In this work, we propose the $\textbf{AUC}$-$\textbf{C}$ontrastive $\textbf{L}$earning, a new approach to contrastive learning that demonstrates robust and competitive performance in compute-limited regimes.  We propose to incorporate the contrastive objective within the AUC-maximization framework, by noting that the AUC metric is maximized upon enhancing the probability of the network's binary prediction difference between positive and negative samples which inspires adequate embedding space arrangements in representation learning. Unlike standard contrastive methods, when performing stochastic optimization, our method maintains unbiased stochastic gradients and thus is more robust to batchsizes as opposed to standard stochastic optimization problems. Remarkably, our method with a batch size of 256, outperforms several state-of-the-art methods that may need much larger batch sizes (e.g., 4096), on ImageNet and other standard datasets. Experiments on transfer learning, few-shot learning, and other downstream tasks also demonstrate the viability of our method.",[],[],"['Rohan Sharma', 'Kaiyi Ji', 'zhiqiang xu', 'Changyou Chen']","['State University of New York at Buffalo', 'Computer Science and Engineering, State University of New York at Buffalo', 'Mohamed bin Zayed University of Artificial Intelligence', 'State University of New York, Buffalo']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=34STseLBrQ,Fairness & Bias,Polynomial Width is Sufficient for Set Representation with High-dimensional Features,"Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to complex analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activations (LE). We demonstrate that $L$ being $\operatorname{poly}(N, D)$ is sufficient for set representation using both embedding layers. We also provide a lower bound of $L$ for the LP embedding layer. Furthermore, we extend our results to permutation-equivariant set functions and the complex field.",[],[],"['Peihao Wang', 'Shenghao Yang', 'Shu Li', 'Zhangyang Wang', 'Pan Li']","['Department of Electrical and Computer Engineering, University of Texas, Austin', 'University of Waterloo', 'Purdue University', 'University of Texas at Austin', 'Georgia Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=lrQlLqQase,Transparency & Explainability,A Dynamical View of the Question of Why,"We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.",[],[],"['Mehdi Fatemi', 'Sindhu C. M. Gowda']","['Microsoft', 'University of Toronto']","['United States', 'Canada']"
https://openreview.net/forum?id=gEwKAZZmSw,Fairness & Bias,Efficient Backpropagation with Variance Controlled Adaptive Sampling,"Sampling-based algorithms, which eliminate ""unimportant"" computations during forward and/or backpropagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to minimize the computational load of BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance introduced by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process. The implementation is available at https://github.com/thu-ml/VCAS.",[],[],"['Ziteng Wang', 'Jianfei Chen', 'Jun Zhu']","['', 'Tsinghua University', 'Computer Science, Tsinghua University']","['', 'China', 'China']"
https://openreview.net/forum?id=9W6KaAcYlr,Fairness & Bias,Most discriminative stimuli for functional cell type clustering,"Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully find discriminative stimuli across species, stages of the visual system and recording techniques. The resulting most discriminative stimuli can be used to assign functional cell types fast and on the fly, without the need to train complex predictive models or show a large natural scene dataset, paving the way for experiments that were previously limited by experimental time. Crucially, MDS are interpretable: they visualize the distinctive stimulus patterns that most unambiguously identify a specific type of neuron.",[],[],"['Max F Burg', 'Thomas Zenkel', 'Michaela Vystrčilová', 'Jonathan Oesterle', 'Larissa Höfling', 'Konstantin Friedrich Willeke', 'Jan Lause', 'Sarah Müller', 'Paul G. Fahey', 'Zhiwei Ding', 'Kelli Restivo', 'Shashwat Sridhar', 'Tim Gollisch', 'Philipp Berens', 'Andreas S. Tolias', 'Thomas Euler', 'Matthias Bethge', 'Alexander S Ecker']","['', 'Eberhard-Karls-Universität Tübingen', 'Georg-August Universität Göttingen', 'Eberhard-Karls-Universität Tübingen', 'University of Tuebingen', 'University of Tuebingen', '', 'Hertie Institute for AI in Brain Health, University of Tuebingen', 'BioX, Stanford University', 'Baylor College of Medicine', 'Baylor College of Medicine', 'Georg-August Universität Göttingen', 'Georg-August Universität Göttingen', 'University of Tuebingen', 'Baylor College of Medicine', 'University of Tuebingen', 'Tübingen AI Center, University of Tuebingen', 'Institute of Computer Science, Georg-August Universität Göttingen']","['', 'Germany', 'Germany', 'Germany', 'Netherlands', 'Netherlands', '', '', 'United States', 'United States', 'United States', 'Germany', 'Germany', 'Netherlands', 'United States', 'Netherlands', 'United States', 'Germany']"
https://openreview.net/forum?id=ZlQRiFmq7Y,Transparency & Explainability,Retrieval-based Disentangled Representation Learning with Natural Language Supervision,"Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitating disentanglement. We extensively assess the performance of VDR across 15 retrieval benchmark datasets, covering text-to-text and cross-modal retrieval scenarios, as well as human evaluation. Our experimental results compellingly demonstrate the superiority of VDR over previous bi-encoder retrievers with comparable model size and training costs, achieving an impressive 8.7% improvement in NDCG@10 on the BEIR benchmark, a 5.3\% increase on MS COCO, and a 6.0% increase on Flickr30k in terms of mean recall in the zero-shot setting. Moreover, The results from human evaluation indicate that interpretability of our method is on par with SOTA captioning models.",[],[],"['Jiawei Zhou', 'Xiaoguang Li', 'Lifeng Shang', 'Xin Jiang', 'Qun Liu', 'Lei Chen']","['Hong Kong University of Science and Technology', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Noah’s Ark Lab, Huawei Technologies', ""Huawei Noah's Ark Lab"", 'Hong Kong University of Science and Technology']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=XlTDBZFXWp,Privacy & Data Governance,The importance of feature preprocessing for differentially private linear optimization,"Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints?   As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization.  In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called *DPSGD-F*, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.",[],[],"['Ziteng Sun', 'Ananda Theertha Suresh', 'Aditya Krishna Menon']","['Google', 'Google', 'Google']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=qDhq1icpO8,Fairness & Bias,Conditional Instrumental Variable Regression with Representation Learning for Causal Inference,"This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard IV is too strong to be practical. To address these challenging and practical problems of the standard IV method (linearity assumption and the strict condition), in this paper, we use a conditional IV (CIV) to relax the unconfounded instrument condition of standard IV and propose a non-linear \underline{CIV} regression with \underline{C}onfounding \underline{B}alancing \underline{R}epresentation \underline{L}earning,  CBRL.CIV, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. We theoretically demonstrate the soundness of CBRL.CIV. Extensive experiments on synthetic and two real-world datasets show the competitive performance of CBRL.CIV against state-of-the-art IV-based estimators and superiority in dealing with the non-linear situation.",[],[],"['Debo Cheng', 'Ziqi Xu', 'Jiuyong Li', 'Lin Liu', 'Jixue Liu', 'Thuc Duy Le']","['University of South Australia, Australia', 'Royal Melbourne Institute of Technology', 'University of South Australia, Australia', '', 'Information Technology, University of South Australia, Australia', 'University of South Australia, Australia']","['Australia', 'Australia', 'Australia', '', 'Australia', 'Australia']"
https://openreview.net/forum?id=XNa6r6ZjoB,Fairness & Bias,Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers,"An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the *Abstractor*. At the core of the Abstractor is a variant of attention called *relational cross-attention*. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from object-level features. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The Abstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the Abstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard Transformers. Finally, Abstractors are evaluated on a collection of tasks based on mathematical problem solving, where consistent improvements in performance and sample efficiency are observed.",[],[],"['Awni Altabaa', 'Taylor Whittington Webb', 'Jonathan D. Cohen', 'John Lafferty']","['Yale University', 'Microsoft Research, Research, Microsoft', 'Princeton University', 'Yale University']","['United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=qoHeuRAcSl,Transparency & Explainability,Grounding Language Plans in Demonstrations Through Counterfactual Perturbations,"Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide/",[],[],"['Yanwei Wang', 'Tsun-Hsuan Wang', 'Jiayuan Mao', 'Michael Hagenow', 'Julie Shah']","['CSAIL, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=auKAUJZMO6,Fairness & Bias,Adaptive Chameleon  or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts,"By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory?  We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs. Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",[],[],"['Jian Xie', 'Kai Zhang', 'Jiangjie Chen', 'Renze Lou', 'Yu Su']","['Fudan University', 'Ohio State University, Columbus', 'ByteDance Inc.', 'Pennsylvania State University', 'Computer Science and Engineering, Ohio State University']","['United States', 'United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=jKhNBulNMh,Transparency & Explainability,Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework,"Machine learning (ML) has been shown to successfully accelerate solving NP-hard combinatorial optimization (CO) problems under the branch and bound framework.  However, the high training and inference cost and limited interpretability of ML approaches severely limit their wide application to modern exact CO solvers. In contrast, human-designed policies---though widely integrated in modern CO solvers due to their compactness and reliability---can not capture data-driven patterns for higher performance. To combine the advantages of the two paradigms, we propose the first symbolic discovery framework---namely, deep symbolic discovery for exact combinatorial optimization solver (Symb4CO)---to learn high-performance symbolic policies on the branching task. Specifically, we show the potential existence of small symbolic policies empirically, employ a large neural network to search in the high-dimensional discrete space, and compile the learned symbolic policies directly for fast deployment. Experiments show that the Symb4CO learned purely CPU-based policies consistently achieve *comparable* performance to previous GPU-based state-of-the-art approaches.  Furthermore, the appealing features of Symb4CO include its high training (*ten training instances*) and inference (*one CPU core*) efficiency and good interpretability (*one-line expressions*), making it simple and reliable for deployment. The results show encouraging potential for the *wide* deployment of ML to modern CO solvers.",[],[],"['Yufei Kuang', 'Jie Wang', 'Haoyang Liu', 'Fangzhou Zhu', 'Xijun Li', 'Jia Zeng', 'Jianye HAO', 'Bin Li', 'Feng Wu']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Huawei Technologies Ltd.', 'Shanghai Jiao Tong University', ""Huawei Noah's Ark Lab"", 'Tianjin University', 'School of Information Science and Technology, University of Science and Technology of China', 'University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=QcMdPYBwTu,Fairness & Bias,Scalable and Effective Implicit Graph Neural Networks on Large Graphs,"Graph Neural Networks (GNNs) have become the de facto standard for modeling graph-structured data in various applications. Among them, implicit GNNs have shown a superior ability to effectively capture long-range dependencies in underlying graphs. However, implicit GNNs tend to be computationally expensive and have high memory usage, due to 1) their use of full-batch training; and 2) they require a large number of iterations to solve a fixed-point equation. These compromise the scalability and efficiency of implicit GNNs especially on large graphs. In this paper, we aim to answer the question: how can we efficiently train implicit GNNs to provide effective predictions on large graphs? We propose a new scalable and effective implicit GNN (SEIGNN) with a mini-batch training method and a stochastic solver, which can be trained efficiently on large graphs. Specifically, SEIGNN can more effectively incorporate global and long-range information by introducing coarse-level nodes in the mini-batch training method. It also achieves reduced training time by obtaining unbiased approximate solutions with fewer iterations in the proposed solver. Comprehensive experiments on various large graphs demonstrate that SEIGNN outperforms baselines and achieves higher accuracy with less training time compared with existing implicit GNNs.",[],[],"['Juncheng Liu', 'Bryan Hooi', 'Kenji Kawaguchi', 'Yiwei Wang', 'Chaosheng Dong', 'Xiaokui Xiao']","['National University of Singapore', 'Computer Science, National University of Singapore', 'National University of Singapore', 'Computer Science, University of California, Merced', 'Amazon', '']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', '']"
https://openreview.net/forum?id=cH3oufN8Pl,Fairness & Bias,Label-Focused Inductive Bias over Latent Object Features in Visual Classification,"Most neural networks for classification primarily learn features differentiated by input-domain related information such as visual similarity of objects in an image. While this focus is natural behavior, it can inadvertently introduce an inductive bias that conflicts with unseen relations in an implicit output-domain determined by human labeling based on their own world knowledge. Such conflicts can limit generalization of models by potential dominance of the input-domain focused bias in inference. To overcome this limitation without external resources, we introduce Output-Domain focused Biasing (ODB) training strategy that constructs inductive biases on features differentiated by only output labels. It has four steps: 1) it learns intermediate latent object features in an unsupervised manner; 2) it decouples their visual dependencies by assigning new independent embedding parameters; 3) it captures structured features optimized for the original classification task; and 4) it integrates the structured features with the original visual features for the final prediction. We implement the ODB on a vision transformer architecture, and achieved significant improvements on image classification benchmarks. This paper offers a straightforward and effective method to obtain and utilize output-domain focused inductive bias for classification mapping two different domains.",[],[],"['Ilmin Kang', 'HyounYoung Bae', 'Kangil Kim']","['AI Graduate School, Gwangju Institute of Science and Technology', 'Gwangju Institute of Science and Technology', 'AI Graduate School, Gwangju Institute of Science and Technology']","['South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=gFR4QwK53h,Fairness & Bias,Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View,"Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias GRNI by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, Causal Dropout Model. We provide a simple yet effective theoretical result: interestingly, the conditional independence (CI) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptotically identical to the CI relations in the original data without dropouts. This particular test-wise deletion procedure, in which we perform CI tests on the samples without zeros for the conditioned variables, can be seamlessly integrated with existing structure learning approaches including constraint-based and greedy score-based methods, thus giving rise to a principled framework for GRNI in the presence of dropouts. We further show that the causal dropout model can be validated from data, and many existing statistical models to handle dropouts fit into our model as specific parametric instances. Empirical evaluation on synthetic, curated, and real-world experimental transcriptomic data comprehensively demonstrate the efficacy of our method.",[],[],"['Haoyue Dai', 'Ignavier Ng', 'Gongxu Luo', 'Peter Spirtes', 'Petar Stojanov', 'Kun Zhang']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Mohamed bin Zayed University of Artificial Intelligence', 'Philosophy, Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Mohamed bin Zayed University of Artificial Intelligence']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=m7tJxajC3G,Privacy & Data Governance,Federated Causal Discovery from Heterogeneous Data,"Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method. The code is available at https://github.com/lokali/FedCDH.git.",[],[],"['Loka Li', 'Ignavier Ng', 'Gongxu Luo', 'Biwei Huang', 'Guangyi Chen', 'Tongliang Liu', 'Bin Gu', 'Kun Zhang']","['Mohamed bin Zayed University of Artificial Intelligence', 'Carnegie Mellon University', 'Mohamed bin Zayed University of Artificial Intelligence', 'University of California, San Diego', 'Machine Learning, Mohamed bin Zayed University of Artificial Intelligence', 'University of Sydney', 'machine learning, Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence']","['United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates', 'United Arab Emirates']"
https://openreview.net/forum?id=VLFhbOCz5D,Privacy & Data Governance,"Tangent Transformers for Composition,Privacy and Removal","We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy. Our code is available at: https://github.com/tianyu139/tangent-model-composition",[],[],"['Tian Yu Liu', 'Aditya Golatkar', 'Stefano Soatto']","['', 'Amazon', 'Amazon Web Services']","['', 'United States', 'United States']"
https://openreview.net/forum?id=MO5PiKHELW,Transparency & Explainability,"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs","Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.",[],[],"['Angelica Chen', 'Ravid Shwartz-Ziv', 'Kyunghyun Cho', 'Matthew L Leavitt', 'Naomi Saphra']","['Center for Data Science, New York University', 'New York University', 'Genentech', 'Facebook', 'Harvard University']","['United States', 'United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=9ROuKblmi7,Privacy & Data Governance,NECO: NEural Collapse Based Out-of-distribution detection,"Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ""neural collapse"", a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of “neural collapse” and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.",[],[],"['Mouïn Ben Ammar', 'Nacim Belkhir', 'Sebastian Popescu', 'Antoine Manzanera', 'Gianni Franchi']","['ENSTA Paris', 'Safran', 'Find&Order', ""Unité d'Informatique et d'Ingénierie des Systèmes, Ecole Nationale Supérieure de Techniques Avancées"", 'ENSTA Paris']","['United States', '', '', 'United States', 'United States']"
https://openreview.net/forum?id=ycv2z8TYur,Fairness & Bias,EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision,"We present EmerNeRF, a simple yet powerful approach for learning spatial-temporal representations of dynamic driving scenes. Grounded in neural fields, EmerNeRF simultaneously captures scene geometry, appearance, motion, and semantics via self-bootstrapping. EmerNeRF hinges upon two core components: First, it stratifies scenes into static and dynamic fields. This decomposition emerges purely from self-supervision, enabling our model to learn from general, in-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field from the dynamic field and uses this flow field to further aggregate multi-frame features, amplifying the rendering precision of dynamic objects. Coupling these three fields (static, dynamic, and flow) enables EmerNeRF to represent highly-dynamic scenes self-sufficiently, without relying on ground truth object annotations or pre-trained models for dynamic object segmentation or optical flow estimation. Our method achieves state-of-the-art performance in sensor simulation, significantly outperforming previous methods when reconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In addition, to bolster EmerNeRF's semantic generalization, we lift 2D visual foundation model features into 4D space-time and address a general positional bias in modern Transformers, significantly boosting 3D perception performance (e.g., 37.50% relative improvement in occupancy prediction accuracy on average). Finally, we construct a diverse and challenging 120-sequence dataset to benchmark neural fields under extreme and highly-dynamic settings. See the project page for code, data, and request pre-trained models: https://emernerf.github.io",[],[],"['Jiawei Yang', 'Boris Ivanovic', 'Or Litany', 'Xinshuo Weng', 'Seung Wook Kim', 'Boyi Li', 'Tong Che', 'Danfei Xu', 'Sanja Fidler', 'Marco Pavone', 'Yue Wang']","['Computer Science, University of Southern California', 'NVIDIA Research, NVIDIA', 'Computer Science, Technion - Israel Institute of Technology, Technion', 'NVIDIA', 'NVIDIA', 'NVIDIA Research', 'NVIDIA', 'School of Interactive Computing, Georgia Institute of Technology', 'Department of Computer Science, University of Toronto', 'NVIDIA', 'Computer Science, University of Southern California']","['United States', 'United States', 'Israel', 'United States', 'United States', 'United States', 'United States', 'United States', 'Canada', 'United States', 'United States']"
https://openreview.net/forum?id=ShQrnAsbPI,Fairness & Bias,Accurate Forgetting for Heterogeneous Federated Continual Learning,"Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain under- explored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL). Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients. We argue that forgetting phenomena are not invariably detrimental. In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients. In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning. While existing CL strategies focus on the complete utilization of previous knowledge, we found that forgetting biased information was beneficial in our study. Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method AF-FCL that selectively utilizes previous knowledge in federated networks. We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge. Comprehensive experiments affirm the superiority of our method over baselines.",[],[],"['Abudukelimu Wuerkaixi', 'Sen Cui', 'Jingfeng Zhang', 'Kunda Yan', 'Bo Han', 'Gang Niu', 'Lei Fang', 'Changshui Zhang', 'Masashi Sugiyama']","['Department of Automation, Tsinghua University, Beijing', 'Automation, Tsinghua University, Tsinghua University', 'AIP, RIKEN', 'Auto, Tsinghua University, Tsinghua University', 'Department of Computer Science, HKBU', 'RIKEN', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Center for Advanced Intelligence Project, RIKEN']","['China', 'China', 'China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=lsvlvWB9vz,Fairness & Bias,EControl: Fast Distributed Optimization with Compression and Error Control,"Modern distributed training relies heavily on communication compression to reduce the communication overhead. In this work, we study algorithms employing a popular class of contractive compressors in order to reduce communication overhead. However, the naive implementation often leads to unstable convergence or even exponential divergence due to the compression bias. Error Compensation (EC) is an extremely popular mechanism to mitigate the aforementioned issues during the training of models enhanced by contractive compression operators. Compared to the effectiveness of EC in the data homogeneous regime, the understanding of the practicality and theoretical foundations of EC in the data heterogeneous regime is limited. Existing convergence analyses typically rely on strong assumptions such as bounded gradients, bounded data heterogeneity, or large batch accesses, which are often infeasible in modern Machine Learning Applications. We resolve the majority of current issues by proposing EControl, a novel mechanism that can regulate error compensation by controlling the strength of the feedback signal. We prove fast convergence for EControl in standard strongly convex, general convex, and nonconvex settings without any additional assumptions on the problem or data heterogeneity. We conduct extensive numerical evaluations to illustrate the efficacy of our method and support our theoretical findings.",[],[],"['Yuan Gao', 'Rustem Islamov', 'Sebastian U Stich']","['CISPA, saarland university, saarland informatics campus', 'Department of Mathematics and Informatics, University of Basel', 'CISPA Helmholtz Center for Information Security']","['Germany', 'Germany', 'Germany']"
https://openreview.net/forum?id=7erlRDoaV8,Security,Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks,"Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover “deleted” information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe implications for the deployment of language models in a world where individuals enjoy ownership of their personal data, a right to privacy, and safety from harmful model outputs.",[],[],"['Vaidehi Patil', 'Peter Hase', 'Mohit Bansal']","['Department of Computer Science, Department of Computer Science, University of North Carolina at Chapel Hill', '', 'University of North Carolina at Chapel Hill']","['United States', '', 'United States']"
https://openreview.net/forum?id=8EyRkd3Qj2,Privacy & Data Governance,CLAP: Collaborative Adaptation for Patchwork Learning,"In this paper, we investigate a new practical learning scenario, where the data distributed in different sources/clients are typically generated with various modalities. Existing research on learning from multi-source data mostly assume that each client owns the data of all modalities, which may largely limit its practicability. In light of the expensiveness and sparsity of multimodal data, we propose patchwork learning to jointly learn from fragmented multimodal data in distributed clients. Considering the concerns on data privacy, patchwork learning aims to impute incomplete multimodal data for diverse downstream tasks without accessing the raw data directly. Local clients could miss different modality combinations. Due to the statistical heterogeneity induced by non-i.i.d. data, the imputation is more challenging since the learned dependencies fail to adapt to the imputation of other clients. In this paper, we provide a novel imputation framework to tackle modality combination heterogeneity and statistical heterogeneity simultaneously, called ``collaborative adaptation''. In particular, for two observed modality combinations from two clients, we learn the transformations between their maximal intersection and other modalities by proposing a novel ELBO. We improve the worst-performing required transformations through a Pareto min-max optimization framework. In extensive experiments, we demonstrate the superiority of the proposed method compared to existing related methods on benchmark data sets and a real-world clinical data set.",[],[],"['Sen Cui', 'Abudukelimu Wuerkaixi', 'Weishen Pan', 'Jian Liang', 'Lei Fang', 'Changshui Zhang', 'Fei Wang']","['Automation, Tsinghua University, Tsinghua University', 'Department of Automation, Tsinghua University, Beijing', 'Weill Cornell Medicine, Cornell University', 'Kuaishou Technology', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Population Health Sciences, Cornell University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=vEfmVS5ywF,Fairness & Bias,Learning in reverse causal strategic environments with ramifications on two sided markets,"Motivated by equilibrium models of labor markets, we develop a formulation of causal strategic classification in which strategic agents can directly manipulate their outcomes. As an application, we consider employers that seek to anticipate the strategic response of a labor force when developing a hiring policy. We show theoretically that employers with performatively optimal hiring policies improve employer reward, labor force skill level, and labor force equity (compared to employers that do not anticipate the strategic labor force response) in the classic Coate-Loury labor market model. Empirically, we show that these desirable properties of performative hiring policies do generalize to our own formulation of a general equilibrium labor market. On the other hand, we also observe that the benefits of performatively optimal hiring policies are brittle in some aspects. We demonstrate that in our formulation a performative employer both harms workers by reducing their aggregate welfare and fails to prevent discrimination when more sophisticated wage and cost structures are introduced.",[],[],"['Seamus Somerstep', 'Yuekai Sun', 'Yaacov Ritov']","['University of Michigan - Ann Arbor', 'University of Michigan', 'Statiscs, University of Michigan - Ann Arbor']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=EWTFMkTdkT,Fairness & Bias,Invariance-based Learning of Latent Dynamics,"We propose a new model class aimed at predicting dynamical trajectories from high-dimensional empirical data. This is done by combining variational autoencoders and (spatio-)temporal transformers within a  framework designed to enforce certain scientifically-motivated invariances. The models allow inference of system behavior at any continuous time and generalization well beyond the data distributions seen during training. Furthermore, the models do not require an explicit neural ODE formulation, making them efficient and highly scalable in practice. We study  behavior through simple theoretical analyses and  extensive empirical experiments. The latter investigate  the ability to predict the trajectories of complicated  systems based on finite data and show that the proposed approaches can outperform existing neural-dynamical models. We study also more general inductive bias in the context of transfer to data obtained under entirely novel system interventions. Overall, our results provide a new framework for efficiently learning complicated dynamics in a data-driven manner, with potential applications in a wide range of fields including physics, biology, and engineering.",[],[],"['Kai Lagemann', 'Christian Lagemann', 'Sach Mukherjee']","['DZNE', 'Rheinisch Westfälische Technische Hochschule Aachen', '']","['Germany', 'Germany', '']"
https://openreview.net/forum?id=XEFWBxi075,Fairness & Bias,GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data,"Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We conducted an extensive evaluation on a predefined benchmark with 19 classification datasets and demonstrate that our method outperforms existing gradient-boosting and deep learning frameworks on most datasets. The method is available under: https://github.com/s-marton/GRANDE",[],[],"['Sascha Marton', 'Stefan Lüdtke', 'Christian Bartelt', 'Heiner Stuckenschmidt']","['Technische Universität Clausthal', '', 'Institute for Software and Systems Engineering, Technische Universität Clausthal', 'University of Mannheim']","['Germany', '', 'Germany', 'Germany']"
https://openreview.net/forum?id=oZtt0pRnOl,Privacy & Data Governance,Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation,"We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets.  This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions.  Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.",[],[],"['Xinyu Tang', 'Richard Shin', 'Huseyin A Inan', 'Andre Manoel', 'Fatemehsadat Mireshghallah', 'Zinan Lin', 'Sivakanth Gopi', 'Janardhan Kulkarni', 'Robert Sim']","['Apple', 'Google', 'Microsoft', 'Gretel.ai', 'University of Washington', 'Microsoft', 'Microsoft Research', 'Microsoft Research, Redmond', 'Microsoft']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=cuAxSHcsSX,Privacy & Data Governance,On Differentially Private Federated Linear Contextual Bandits,"We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user's privacy. We identify three issues in the state-of-the-art~\citep{dubey2020differentially}: (i) failure of claimed privacy protection, (ii) incorrect regret bound due to noise miscalculation and (iii) ungrounded communication cost.  To resolve these issues, we take a two-step approach. First, we design an algorithmic framework consisting of a generic federated LCB algorithm and flexible privacy protocols. Then, leveraging the proposed framework, we study federated LCBs under two different privacy constraints. We first establish privacy and regret guarantees under silo-level local differential privacy, which fix the issues present in state-of-the-art algorithm. To further improve the regret performance, we next consider shuffle model of differential privacy, under which we show that our algorithm can achieve nearly ``optimal'' regret without a trusted server.  We accomplish this via two different schemes --  one relies on a new result on privacy amplification via shuffling for DP mechanisms and another one leverages the integration of a shuffle protocol for vector sum into the tree-based mechanism, both of which might be of independent interest. Finally, we support our theoretical results with numerical evaluations over contextual bandit instances generated from both synthetic and real-life data.",[],[],"['Xingyu Zhou', 'Sayak Ray Chowdhury']","['ECE, Wayne State University', 'Indian Institute of Technology, Kanpur']","['United States', 'India']"
https://openreview.net/forum?id=7wY67ZDQTE,Transparency & Explainability,Cauchy-Schwarz Divergence Information Bottleneck for Regression,"The information bottleneck (IB) approach is popular to improve the generalization, robustness and explainability of deep neural networks. Essentially, it aims to find a minimum sufficient representation $\mathbf{t}$ by striking a trade-off between a compression term $I(\mathbf{x};\mathbf{t})$ and a prediction term $I(y;\mathbf{t})$, where $I(\cdot;\cdot)$ refers to the mutual information (MI). MI is for the IB for the most part expressed in terms of the Kullback-Leibler (KL) divergence, which in the regression case corresponds to prediction based on mean squared error (MSE) loss with Gaussian assumption and compression approximated by variational inference.  In this paper, we study the IB principle for the regression problem and develop a new way to parameterize the IB with deep neural networks by exploiting favorable properties of the Cauchy-Schwarz (CS) divergence. By doing so, we move away from MSE-based regression and ease estimation by avoiding variational approximations or distributional assumptions. We investigate the improved generalization ability of our proposed CS-IB and demonstrate strong adversarial robustness guarantees. We demonstrate its superior performance on six real-world regression tasks over other popular deep IB approaches. We additionally observe that the solutions discovered by CS-IB always achieve the best trade-off between prediction accuracy and compression ratio in the information plane. The code is available at \url{https://github.com/SJYuCNEL/Cauchy-Schwarz-Information-Bottleneck}.",[],[],"['Shujian Yu', 'Xi Yu', 'Sigurd Løkse', 'Robert Jenssen', 'Jose C Principe']","['Vrije Universiteit Amsterdam', 'Computational Science Initiative, Brookhaven National Laboratory', 'NORCE Norwegian Research Centre', 'Dept. Physics and Technology, UiT The Arctic University of Norway', 'University of Florida']","['Netherlands', 'United States', '', 'Norway', 'United States']"
https://openreview.net/forum?id=WIzzXCVYiH,Transparency & Explainability,GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries,"While Graph Neural Networks (GNNs) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts to explain the decision-making process of GNNs. These efforts often focus on explaining why a certain prediction is made for a particular instance, or what discriminative features the GNNs try to detect for each class. However, to the best of our knowledge, there is no existing study on understanding the decision boundaries of GNNs, even though the decision-making process of GNNs is directly determined by the decision boundaries. To bridge this research gap, we propose a model-level explainability method called GNNBoundary, which attempts to gain deeper insights into the decision boundaries of graph classifiers. Specifically, we first develop an algorithm to identify the pairs of classes whose decision regions are adjacent. For an adjacent class pair, the near-boundary graphs between them are effectively generated by optimizing a novel objective function specifically designed for boundary graph generation. Thus, by analyzing the nearboundary graphs, the important characteristics of decision boundaries can be uncovered. To evaluate the efficacy of GNNBoundary, we conduct experiments on both synthetic and public real-world datasets. The results demonstrate that, via the analysis of faithful near-boundary graphs generated by GNNBoundary, we can thoroughly assess the robustness and generalizability of the explained GNNs. The official implementation can be found at https://github.com/yolandalalala/GNNBoundary.",[],[],"['Xiaoqi Wang', 'Han Wei Shen']","['Bosch', 'Ohio State University, Columbus']","['Vietnam', 'United States']"
https://openreview.net/forum?id=U7iiF79kI3,Security,CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception,"Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D data.  Although previous research has investigated pretraining methods for both LiDAR and camera-based 3D object detection, a unified pretraining framework for multimodal BEV perception is missing. In this study, we introduce CALICO, a novel framework that applies contrastive objectives to both LiDAR and camera backbones. Specifically, CALICO incorporates two stages: point-region contrast (PRC) and region-aware distillation (RAD). PRC better balances the region- and scene-level representation learning on the LiDAR modality and offers significant performance improvement compared to existing methods. RAD effectively achieves contrastive distillation on our self-trained teacher model. CALICO's efficacy is substantiated by extensive evaluations on 3D object detection and BEV map segmentation tasks, where it delivers significant performance improvements. Notably, CALICO outperforms the baseline method by 10.5\% and 8.6\% on NDS and mAP. Moreover, CALICO boosts the robustness of multimodal 3D object detection against adversarial attacks and corruption. Additionally, our framework can be tailored to different backbones and heads, positioning it as a promising approach for multimodal BEV perception.",[],[],"['Jiachen Sun', 'Haizhong Zheng', 'Qingzhao Zhang', 'Atul Prakash', 'Zhuoqing Mao', 'Chaowei Xiao']","['University of Michigan', 'ECE, CMU, Carnegie Mellon University', 'Computer Science and Engineering, University of Michigan - Ann Arbor', 'University of Michigan', 'Google', 'University of Wisconsin - Madison']","['United States', 'United States', 'United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=o2IEmeLL9r,Transparency & Explainability,Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning,"Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM’s superiority in sample efficiency and task performance compared to baselines. Moreover, PTGM exemplifies enhanced interpretability and generalization of the acquired low-level skills.",[],[],"['Haoqi Yuan', 'Zhancun Mu', 'Feiyang Xie', 'Zongqing Lu']","['Peking University', 'Yuanpei College, Peking University', 'Peking University', '']","['United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=AJBGSVSTT2,Security,Backdoor Federated Learning by Poisoning Backdoor-Critical Layers,"Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that our BC layer-aware backdoor attacks can successfully backdoor FL under seven SOTA defenses with only 10% malicious clients and outperform the latest backdoor attack methods.",[],[],"['Haomin Zhuang', 'Mingxian Yu', 'Hao Wang', 'Yang Hua', 'Jian Li', 'Xu Yuan']","['Computer Science Engineering, University of Notre Dame', 'SUN YAT-SEN UNIVERSITY', 'Department of Electrical and Computer Engineering, Stevens Institute of Technology', ""Queen's University Belfast"", 'State University of New York at Stony Brook', 'Computer and Information Sciences, University of Delaware']","['United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=SsmT8aO45L,Security,Provable Robust Watermarking for AI-Generated Text,"We study the problem of watermarking large language models (LLMs) generated text — one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.",[],[],"['Xuandong Zhao', 'Prabhanjan Vijendra Ananth', 'Lei Li', 'Yu-Xiang Wang']","['University of California, Berkeley', ', University of California, Santa Barbara', 'Language Technology Institute, School of Computer Science, Carnegie Mellon University', 'University of California, San Diego']","['United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=U7VW3KBm34,Transparency & Explainability,Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition,"The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model's inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",[],[],"['Sangyu Han', 'Yearim Kim', 'Nojun Kwak']","['Seoul National University', 'Department of intelligence and information, Seoul National University', 'Seoul National University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=nFI3wFM9yN,Privacy & Data Governance,Communication-Efficient Federated Non-Linear Bandit Optimization,"Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization preserves data privacy and allows for large-scale computing, which makes it a promising decentralized machine learning paradigm. Though it is often deployed for tasks that are online in nature, e.g., next-word prediction on keyboard apps, most works formulate it as an offline problem. The few exceptions that consider federated bandit optimization are limited to very simplistic function classes, e.g., linear, generalized linear, or non-parametric function class with bounded RKHS norm, which severely hinders its practical usage. In this paper, we propose a new algorithm, named Fed-GO-UCB, for federated bandit optimization with generic non-linear objective function. Under some mild conditions, we rigorously prove that Fed-GO-UCB is able to achieve sub-linear rate for both cumulative regret and communication cost. At the heart of our theoretical analysis are distributed regression oracle and individual confidence set construction, which can be of independent interests. Empirical evaluations also demonstrate the effectiveness of the proposed algorithm.",[],[],"['Chuanhao Li', 'Chong Liu', 'Yu-Xiang Wang']","['Yale University', 'University at Albany, State University of New York', 'University of California, San Diego']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=VVgGbB9TNV,Transparency & Explainability,An LLM can Fool Itself: A Prompt-Based Adversarial Attack,"The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions. Our source code is available at https://github.com/GodXuxilie/PromptAttack.",[],[],"['Xilie Xu', 'Keyi Kong', 'Ning Liu', 'Lizhen Cui', 'Di Wang', 'Jingfeng Zhang', 'Mohan Kankanhalli']","['School of Computing, National University of Singapore', 'Taishan College, Shandong University', 'School of Software, Shandong University', 'Shandong University', '', 'AIP, RIKEN', 'School of Computing, National University of Singapore']","['China', 'China', 'China', 'China', '', 'Japan', 'Singapore']"
https://openreview.net/forum?id=n7Sr8SW4bn,Security,Mayfly: a Neural Data Structure for Graph Stream Summarization,"A graph is a structure made up of vertices and edges used to represent complex relationships between entities, while a graph stream is a continuous flow of graph updates that convey evolving relationships between entities. The massive volume and high dynamism of graph streams promote research on data structures of graph summarization, which provides a concise and approximate view of graph streams with sub-linear space and linear construction time, enabling real-time graph analytics in various domains, such as social networking, financing, and cybersecurity. In this work, we propose the Mayfly, the first neural data structure for summarizing graph streams. The Mayfly replaces handcrafted data structures with better accuracy and adaptivity. To cater to practical applications, Mayfly incorporates two offline training phases. During the larval phase, the Mayfly learns basic summarization abilities from automatically and synthetically constituted meta-tasks, and in the metamorphosis phase, it rapidly adapts to real graph streams via meta-tasks. With specific configurations of information pathways, the Mayfly enables flexible support for miscellaneous graph queries, including edge, node, and connectivity queries. Extensive empirical studies show that the Mayfly significantly outperforms its handcrafted competitors.",[],[],"['Yuan Feng', 'Yukun Cao', 'Wang Hairu', 'Xike Xie', 'S Kevin Zhou']","['Computer Science, University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'BME School, University of Science and Technology of China']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=TyFrPOKYXw,Security,Safe RLHF: Safe Reinforcement Learning from Human Feedback,"With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.  Code is available at https://github.com/PKU-Alignment/safe-rlhf.   Warning: This paper contains example data that may be offensive or harmful.",[],[],"['Josef Dai', 'Xuehai Pan', 'Ruiyang Sun', 'Jiaming Ji', 'Xinbo Xu', 'Mickel Liu', 'Yizhou Wang', 'Yaodong Yang']","['Institute for AI, Peking University', 'School of Computer Science, Peking University', 'Peking University', 'Institute for AI, Peking University', 'Beijing University of Posts and Telecommunications', 'Paul G. Allen School, University of Washington', 'Computer Science, Peking University', 'Peking University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=iriEqxFB4y,Fairness & Bias,DOS: Diverse Outlier Sampling for Out-of-Distribution Detection,"Modern neural networks are known to give overconfident predictions for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier datasets. However, the OOD samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for OOD detection performance. Motivated by the observation, we propose a straightforward and novel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse and informative outliers. Specifically, we cluster the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with absent category loss. With DOS, the sampled outliers efficiently shape a globally compact decision boundary between ID and OOD data. Extensive experiments demonstrate the superiority of DOS, reducing the average FPR95 by up to 25.79% on CIFAR-100 with TI-300K.",[],[],"['Wenyu Jiang', 'Hao Cheng', 'MingCai Chen', 'Chongjun Wang', 'Hongxin Wei']","['Department of Statistics and Data Science, Southern University of Science and Technology', 'Nanjing University', 'Nanjing University', 'School of Computer Science, nanjing university', 'Southern University of Science and Technology']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=pE6gWrASQm,Security,On Adversarial Training without Perturbing all Examples,"Adversarial training is the de-facto standard for improving robustness against adversarial examples. This usually involves a multi-step adversarial attack applied on each example during training. In this paper, we explore only constructing adversarial examples (AE) on a subset of the training examples. That is, we split the training set in two subsets $A$ and $B$, train models on both ($A\cup B$) but construct AEs only for examples in $A$. Starting with $A$ containing only a single class, we systematically increase the size of $A$ and consider splitting by class and by examples. We observe that: (i) adv. robustness transfers by difficulty and to classes in $B$ that have never been adv. attacked during training, (ii) we observe a tendency for hard examples to provide better robustness transfer than easy examples, yet find this tendency to diminish with increasing complexity of datasets (iii) generating AEs on only $50$% of training data is sufficient to recover most of the baseline AT performance even on ImageNet. We observe similar transfer properties across tasks, where generating AEs on only $30$% of data can recover baseline robustness on the target task. We evaluate our subset analysis on a wide variety of image datasets like CIFAR-10, CIFAR-100, ImageNet-200 and show transfer to SVHN, Oxford-Flowers-102 and Caltech-256. In contrast to conventional practice, our experiments indicate that the utility of computing AEs varies by class and examples and that weighting examples from $A$ higher than $B$ provides high transfer performance. Code is available at [http://github.com/mlosch/SAT](http://github.com/mlosch/SAT).",[],[],"['Max Losch', 'Mohamed Omran', 'David Stutz', 'Mario Fritz', 'Bernt Schiele']","['Max-Planck Institute for Informatics', 'Qualcomm Inc, QualComm', 'DeepMind', 'CISPA Helmholtz Center for Information Security', 'Max Planck Institute for Informatics, Saarland Informatics Campus']","['Germany', 'United States', 'United States', 'United States', 'Germany']"
https://openreview.net/forum?id=9m02ib92Wz,Transparency & Explainability,DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models,"Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.",[],[],"['Yongchan Kwon', 'Eric Wu', 'Kevin Wu', 'James Zou']","['', 'Electrical Engineering, Stanford University', 'Stanford University', 'Stanford University']","['', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Ebt7JgMHv1,Transparency & Explainability,Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching,"Mechanistic interpretability aims to attribute high-level model behaviors to specific, interpretable learned features. It is hypothesized that these features manifest as directions or low-dimensional subspaces within activation space. Accordingly, recent studies have explored the identification and manipulation of such subspaces to reverse-engineer computations, employing methods such as activation patching. In this work, we demonstrate that naïve approaches to subspace interventions can give rise to interpretability illusions.  Specifically, even if patching along a subspace has the intended end-to-end causal effect on model behavior, this effect may be achieved by activating \emph{a dormant parallel pathway} using a component that is \textit{causally disconnected} from the model output. We demonstrate this in a mathematical example, realize the example empirically in two different settings (the Indirect Object Identification (IOI) task and factual recall), and argue that activating dormant pathways ought to be prevalent in practice. In the context of factual recall, we further show that the illusion is related to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localisation.  However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (IOI) where prior manual circuit analysis allows an understanding of the location of the ground truth feature. We explore the additional evidence needed to argue that a patched subspace is faithful.",[],[],"['Aleksandar Makelov', 'Georg Lange', 'Atticus Geiger', 'Neel Nanda']","['Research, OpenAI', 'Independent', 'Pr(Ai)²R Group', 'Google DeepMind']","['India', 'Germany', '', '']"
https://openreview.net/forum?id=Tvwf4Vsi5F,Security,PubDef: Defending Against Transfer Attacks From Public Models,"Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under this threat model, our defense, PubDef, outperforms the state-of-the-art white-box adversarial training by a large margin with almost no loss in the normal accuracy. For instance, on ImageNet, our defense achieves 62% accuracy under the strongest transfer attack vs only 36% of the best adversarially trained model. Its accuracy when not under attack is only 2% lower than that of an undefended model (78% vs 80%). We release our code at https://github.com/wagner-group/pubdef.",[],[],"['Chawin Sitawarin', 'Jaewon Chang', 'David Huang', 'Wesson Altoyan', 'David Wagner']","['Meta', '', 'University of California Berkeley']","['United States', '', 'United States']"
https://openreview.net/forum?id=OCqyFVFNeF,Transparency & Explainability,Defining and extracting generalizable interaction primitives from DNNs,"Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2024) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.",[],[],"['Lu Chen', 'Siyu Lou', 'Benhao Huang', 'Quanshi Zhang']","['Computer Science, Shanghai Jiao Tong University', 'Department of computer science and engineering, Shanghai Jiaotong University', 'IEEE Pilot Class, CS, Shanghai Jiaotong University', 'Shanghai Jiao Tong University']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=m7aPLHwsLr,Security,DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness,"Machine Learning (ML) models have been utilized for malware detection for over two decades. Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks. However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and/or can defend against only specific attacks, which makes them less practical in real-world settings. In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the *de-randomized smoothing* technique for the domain of malware detection. Specifically, we propose a *window ablation* scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables. After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness. To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables. More surprisingly, through evaluating DRSM against $9$ empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model. In addition, we collected $15.5K$ recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time. Our code and dataset are available at - https://github.com/ShoumikSaha/DRSM",[],[],"['Shoumik Saha', 'Wenxiao Wang', 'Yigitcan Kaya', 'Soheil Feizi', 'Tudor Dumitras']","['University of Maryland, College Park', 'University of Maryland, College Park', 'Computer Science, University of California, Santa Barbara', 'University of Maryland, College Park', 'University of Maryland, College Park']","['United States', 'United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=Wd47f7HEXg,Fairness & Bias,Quasi-Monte Carlo for 3D Sliced Wasserstein,"Monte Carlo (MC) integration has been employed as the standard approximation method for the Sliced Wasserstein (SW) distance, whose analytical expression involves an intractable expectation. However, MC integration is not optimal in terms of absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically evaluate various methods to construct QMC point sets on the 3D unit-hypersphere, including the Gaussian-based and equal area mappings, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimator for stochastic optimization, we extend QSW to Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness in the discussed point sets. Theoretically, we prove the asymptotic convergence of QSW and the unbiasedness of RQSW. Finally, we conduct experiments on various 3D tasks, such as point-cloud comparison, point-cloud interpolation, image style transfer, and training deep point-cloud autoencoders, to demonstrate the favorable performance of the proposed QSW and RQSW variants.",[],[],"['Khai Nguyen', 'Nicola Bariletto', 'Nhat Ho']","['University of Texas, Austin', 'Department of Statistics and Data Sciences, University of Texas at Austin', 'University of Texas, Austin']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=yKksu38BpM,Transparency & Explainability,Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models,"A recent trend in explainable AI research has focused on surrogate modeling, where neural networks are approximated as simpler ML algorithms such as kernel machines. A second trend has been to utilize kernel functions in various explain-by-example or data attribution tasks. In this work, we combine these two trends to analyze approximate empirical neural tangent kernels (eNTK) for data attribution. Approximation is critical for eNTK analysis due to the high computational cost to compute the eNTK. We define new approximate eNTK and perform novel analysis on how well the resulting kernel machine surrogate models correlate with the underlying neural network. We introduce two new random projection variants of approximate eNTK which allow users to tune the time and memory complexity of their calculation. We conclude that kernel machines using approximate neural tangent kernel as the kernel function are effective surrogate models, with the introduced trace NTK the most consistent performer.",[],[],"['Andrew William Engel', 'Zhichao Wang', 'Natalie Frank', 'Ioana Dumitriu', 'Sutanay Choudhury', 'Anand Sarwate', 'Tony Chiang']","['Physics, Ohio State University, Columbus', 'University of California, San Diego', 'New York University', 'University of California, San Diego', 'Pacific Northwest National Lab', 'Electrical and Computer Engineering, Rutgers University', 'Pacific Northwest National Laboratory']","['United States', 'Colombia', 'United States', 'Colombia', 'Chad', 'United States', 'Serbia']"
https://openreview.net/forum?id=TTrzgEZt9s,Fairness & Bias,Distributionally Robust Optimization with Bias and Variance Reduction,"We consider the distributionally robust optimization (DRO) problem, wherein a learner optimizes the worst-case empirical risk achievable by reweighing the observed training examples. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3x faster than baselines such as SGD and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.",[],[],"['Ronak Mehta', 'Vincent Roulet', 'Krishna Pillutla', 'Zaid Harchaoui']","['', 'Google', 'Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology', '']","['India', 'India', 'India', 'India']"
https://openreview.net/forum?id=VoLDkQ6yR3,Security,Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation,"Modern deep learning requires large volumes of data, which could contain sensitive or private information that cannot be leaked. Recent work has shown for homogeneous neural networks a large portion of this training data could be reconstructed with only access to the trained network parameters. While the attack was shown to work empirically, there exists little formal understanding of its effective regime and which datapoints are susceptible to reconstruction. In this work, we first build a stronger version of the dataset reconstruction attack and show how it can provably recover the \emph{entire training set} in the infinite width regime. We then empirically study the characteristics of this attack on two-layer networks and reveal that its success heavily depends on deviations from the frozen infinite-width Neural Tangent Kernel limit. Next, we study the nature of easily-reconstructed images. We show that both theoretically and empirically, reconstructed images tend to ``outliers'' in the dataset, and that these reconstruction attacks can be used for \textit{dataset distillation}, that is, we can retrain on reconstructed images and obtain high predictive accuracy.",[],[],"['Noel Loo', 'Ramin Hasani', 'Mathias Lechner', 'Alexander Amini', 'Daniela Rus']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'CSAIL, Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=hnrB5YHoYu,Fairness & Bias,Finetuning Text-to-Image Diffusion Models for Fairness,"The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a 75% young and 25% old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.",[],[],"['Xudong Shen', 'Chao Du', 'Tianyu Pang', 'Min Lin', 'Yongkang Wong', 'Mohan Kankanhalli']","['National University of Singapore', 'Sea AI Lab', 'Sea AI Lab', 'Sea AI Lab', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://openreview.net/forum?id=YH5w12OUuU,Fairness & Bias,TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting,"The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. TEMPO expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of TEMPO over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets. This performance gain is observed not only in scenarios involving previously unseen datasets but also in scenarios with multi-modal inputs. This compelling finding highlights TEMPO's potential to constitute a foundational model-building framework.",[],[],"['Defu Cao', 'Furong Jia', 'Sercan O Arik', 'Tomas Pfister', 'Yixiang Zheng', 'Wen Ye', 'Yan Liu']","['University of Southern California', 'Computer Science, Duke University', 'Google', 'Google', 'University of Southern California', 'Computer Science, University of Southern California', 'University of Southern California']","['United States', 'United States', '', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=SQGUDc9tC8,Fairness & Bias,The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models,"Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train PLMs on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of {\sc Social Bias Neurons}. Specifically, we propose {\sc Integrated Gap Gradients (IG$^2$)} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias.  By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG$^2$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, {\sc Bias Neuron Suppression (BNS)} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG$^2$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost\footnote{This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.}.",[],[],"['Yan Liu', 'Yu Liu', 'Xiaokang Chen', 'Pin-Yu Chen', 'Daoguang Zan', 'Min-Yen Kan', 'Tsung-Yi Ho']","['', 'University of Eastern Finland', 'Peking University', 'International Business Machines', 'ByteDance Inc.', 'National University of Singapore', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong']","['', 'Finland', 'China', 'China', 'Singapore', 'Singapore', 'Hong Kong']"
https://openreview.net/forum?id=SLw9fp4yI6,Fairness & Bias,Controlled Text Generation via Language Model Arithmetic,"As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction. We release an open source easy-to-use implementation of our framework at https://github.com/eth-sri/language-model-arithmetic.",[],[],"['Jasper Dekoninck', 'Marc Fischer', 'Luca Beurer-Kellner', 'Martin Vechev']","['Department of Computer Science, ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Computer Science, Swiss Federal Institute of Technology']","['Switzerland', 'India', 'Switzerland', 'India']"
https://openreview.net/forum?id=S46Knicu56,Fairness & Bias,A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error,"Estimating treatment effects has numerous real-world applications in various fields, such as epidemiology and political science. While much attention has been devoted to addressing the challenge using fully observational data, there has been comparatively limited exploration of this issue in cases when the treatment is not directly observed. In this paper, we tackle this problem by developing a general variational framework, which is flexible to integrate with advanced neural network-based approaches, to identify the average dose-response function (ADRF) with the continuously valued error-contaminated treatment. Our approach begins with the formulation of a probabilistic data generation model, treating the unobserved treatment as a latent variable. In this model, we leverage a learnable density estimation neural network to derive its prior distribution conditioned on covariates. This module also doubles as a generalized propensity score estimator, effectively mitigating selection bias arising from observed confounding variables. Subsequently, we calculate the posterior distribution of the treatment, taking into account the observed measurement and outcome. To mitigate the impact of treatment error, we introduce a re-parametrized treatment value, replacing the error-affected one, to make more accurate predictions regarding the outcome. To demonstrate the adaptability of our framework, we incorporate two state-of-the-art ADRF estimation methods and rigorously assess its efficacy through extensive simulations and experiments using semi-synthetic data.",[],[],"['Erdun Gao', 'Howard Bondell', 'Wei Huang', 'Mingming Gong']","['The Australian Institute for Machine Learning, The University of Adelaide', '', 'School of Mathematics and Statistics, University of Melbourne', 'School of mathematics and statistics, University of Melbourne']","['Australia', '', 'Australia', 'Australia']"
https://openreview.net/forum?id=SIZWiya7FE,Privacy & Data Governance,Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models,"Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge prompts us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further address the issue of lacking supervision information, which hinders alignment with ground truth, we introduce a contrastive loss to facilitate the matching of representations between the remaining data and those of the original model, thus preserving predictive performance. Experimental results across various unlearning tasks demonstrate the effectiveness of our proposed method, Label-Agnostic Forgetting (LAF) without using any labels, which achieves comparable performance to state-of-the-art methods that rely on full supervision information. Furthermore, our approach excels in semi-supervised scenarios, leveraging limited supervision information to outperform fully supervised baselines. This work not only showcases the viability of supervision-free unlearning in deep models but also opens up a new possibility for future research in unlearning at the representation level.",[],[],"['Shaofei Shen', 'Chenhao Zhang', 'Yawen Zhao', 'Alina Bialkowski', 'Weitong Tony Chen', 'Miao Xu']","['University of Queensland', '', 'University of Queensland', 'University of Queensland', '', 'School of EECS, University of Queensland']","['Australia', '', 'Australia', 'Australia', '', 'Australia']"
https://openreview.net/forum?id=xnhvVtZtLD,Fairness & Bias,On the Fairness ROAD: Robust Optimization for Adversarial Debiasing,"In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages,  have raised concerns about persistent local disparities between sensitive groups. In this work, we address the problem of local fairness, which ensures that the predictor is unbiased not only in terms of expectations over the whole population, but also within any subregion of the feature space, unknown at training time. To enforce this objective, we introduce ROAD, a novel approach that leverages the Distributionally Robust Optimization (DRO) framework  within a fair adversarial learning objective, where an adversary tries to infer the sensitive attribute from the predictions. Using an instance-level re-weighting strategy, ROAD is designed to prioritize inputs that are likely to be locally unfair, i.e. where the adversary faces the least difficulty in reconstructing the sensitive attribute. Numerical experiments demonstrate the effectiveness of our method: it achieves Pareto dominance with respect to local fairness and accuracy for a given global fairness level across three standard datasets, and also enhances fairness generalization under distribution shift.",[],[],"['Vincent Grari', 'Thibault Laugel', 'Tatsunori Hashimoto', 'sylvain lamprier', 'Marcin Detyniecki']","['Stanford University', 'AXA', 'Stanford University', ""Université d'Angers"", 'AXA']","['United States', '', 'United States', 'United States', '']"
https://openreview.net/forum?id=QrEHs9w5UF,Transparency & Explainability,PRIME: Prioritizing Interpretability in Failure Mode Extraction,"In this work, we study the challenge of providing human-understandable descriptions for failure modes in trained image classification models. Existing works address this problem by first identifying clusters (or directions) of incorrectly classified samples in a latent space and then aiming to provide human-understandable text descriptions for them. We observe that in some cases, describing text does not match well with identified failure modes, partially owing to the fact that shared interpretable attributes of failure modes may not be captured using clustering in the feature space. To improve on these shortcomings, we propose a novel approach that prioritizes interpretability in this problem: we start by obtaining human-understandable concepts (tags) of images in the dataset and then analyze the model's behavior based on the presence or absence of combinations of these tags. Our method also ensures that the tags describing a failure mode form a minimal set, avoiding redundant and noisy descriptions. Through several experiments on different datasets, we show that our method successfully identifies failure modes and generates high-quality text descriptions associated with them. These results highlight the importance of prioritizing interpretability in understanding model failures.",[],[],"['Keivan Rezaei', 'Mehrdad Saberi', 'Mazda Moayeri', 'Soheil Feizi']","['Computer Science, University of Maryland, College Park', 'Computer Science, Department of Computer Science, University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=gT5hALch9z,Security,Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions,"Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3\% safety examples (a few hundred demonstrations) when fine-tuning a model like LLaMA can substantially improve its safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones. As a whole, our results illustrate trade-offs in training LLMs to be helpful and training them to be safe.",[],[],"['Federico Bianchi', 'Mirac Suzgun', 'Giuseppe Attanasio', 'Paul Rottger', 'Dan Jurafsky', 'Tatsunori Hashimoto', 'James Zou']","['Stanford University', 'Stanford University', 'Instituto de Telecomunicações', 'Bocconi University', 'Computer Science, Stanford University', 'Stanford University', 'Stanford University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=3Z1gxuAQrA,Fairness & Bias,PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training,"Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens using a 2k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128k.",[],[],"['Dawei Zhu', 'Nan Yang', 'Liang Wang', 'Yifan Song', 'Wenhao Wu', 'Furu Wei', 'Sujian Li']","['Peking University', 'Microsoft Research Asia', 'Microsoft Research', 'Peking University', 'Peking University', 'Microsoft Research', 'Peking University']","['China', 'China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=6cFcw1Rxww,Fairness & Bias,Local Search GFlowNets,"Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space.  This paper proposes to train GFlowNets with local search, which focuses on exploiting high-rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via backtracking and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme, which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.",[],[],"['Minsu Kim', 'Taeyoung Yun', 'Emmanuel Bengio', 'Dinghuai Zhang', 'Yoshua Bengio', 'Sungsoo Ahn', 'Jinkyoo Park']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Recursion', '', 'computer science and operations research, University of Montreal', 'Graduate School of AI, Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology']","['South Korea', 'South Korea', 'Switzerland', 'Canada', 'United States', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=PKICZXVY9M,Fairness & Bias,Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization,"Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as prompt learning, that not only study the discrimination between in-distribution (ID) and out-of-distribution (OOD) samples, but also show some improvements in both ID and OOD accuracies. In this paper, we first demonstrate that vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes. Then we propose a novel approach OGEN to address this pitfall, with the main focus on improving the OOD GENeralization of finetuned models. Specifically, a class-conditional feature generator is introduced to synthesize OOD features using just the class name of any unknown class. Such synthesized features will provide useful knowledge about unknowns and help regularize the decision boundary between ID and OOD data when optimized jointly. Equally important is our adaptive self-distillation mechanism to regularize our feature generation model during joint optimization, i.e., adaptively transferring knowledge between model states to further prevent overfitting. Experiments validate that our method yields convincing gains in OOD generalization performance in different settings. Code: https://github.com/apple/ml-ogen.",[],[],"['Yuhang Zang', 'Hanlin Goh', 'Joshua M. Susskind', 'Chen Huang']","['Shanghai Artificial Intelligence Laboratory', 'Apple', 'Cognitive Science, Apple', 'Apple']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=PAfnMGXief,Security,BRUSLEATTACK: A QUERY-EFFICIENT SCORE- BASED BLACK-BOX SPARSE ADVERSARIAL ATTACK,"We study the unique, less-well understood problem of generating sparse adversarial samples simply by observing the score-based replies to model queries. Sparse attacks aim to discover a minimum number—the $l_0$ bounded—perturbations to model inputs to craft adversarial examples and misguide model decisions. But, in contrast to query-based dense attack counterparts against black-box models, constructing sparse adversarial perturbations, even when models serve confidence score information to queries in a score-based setting, is non-trivial. Because, such an attack leads to: i) an NP-hard problem; and ii) a non-differentiable search space. We develop the BRUSLEATTACK—a new, faster (more query-efficient) algorithm formulation for the problem. We conduct extensive attack evaluations including an attack demonstration against a Machine Learning as a Service (MLaaS) offering exemplified by __Google Cloud Vision__ and robustness testing of adversarial training regimes and a recent defense against black-box attacks. The proposed attack scales to achieve state-of-the-art attack success rates and query efficiency on standard computer vision tasks such as ImageNet across different model architectures. Our artifacts and DIY attack samples are available on GitHub. Importantly, our work facilitates faster evaluation of model vulnerabilities and raises our vigilance on the safety, security and reliability of deployed systems.",[],[],"['Quoc Viet Vo', 'Ehsan Abbasnejad', 'Damith Ranasinghe']","['University of Adelaide', 'Monash University', 'University of Adelaide']","['Australia', 'Australia', 'Australia']"
https://openreview.net/forum?id=Oju2Qu9jvn,Transparency & Explainability,Estimating Conditional Mutual Information for Dynamic Feature Selection,"Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary feature sets and learning a policy to identify the most valuable selections. Here, we take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is implementing this policy, and we design a new approach that estimates the mutual information in a discriminative rather than a generative fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform costs between features, incorporating prior information, and exploring modern architectures to handle partial input information. We find that our method provides consistent gains over recent state-of-the-art methods across a variety of datasets.",[],[],"['Soham Gadgil', 'Ian Connick Covert', 'Su-In Lee']","['University of Washington', 'Stanford University', 'University of Washington, Seattle']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=O9PArxKLe1,Security,Leveraging Optimization for Adaptive Attacks on Image Watermarks,"Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in unethical activities. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. When evaluating watermarking algorithms and their (adaptive) attacks, it is challenging to determine whether an adaptive attack is optimal, i.e., the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's parameters. We demonstrate for Stable Diffusion models that such an attacker can break all five surveyed watermarking methods at no visible degradation in image quality. Optimizing our attacks is efficient and requires less than 1 GPU hour to reduce the detection accuracy to 6.3% or less. Our findings emphasize the need for more rigorous robustness testing against adaptive, learnable attackers.",[],[],"['Nils Lukas', 'Abdulrahman Diaa', 'Lucas Fenaux', 'Florian Kerschbaum']","['Machine Learning, Mohamed bin Zayed University of Artificial Intelligence', 'CrySP Lab, University of Waterloo', 'University of Waterloo', '']","['Canada', 'Canada', 'Canada', '']"
https://openreview.net/forum?id=jJvXNpvOdM,Fairness & Bias,Task Planning for Visual Room Rearrangement under Partial Observability,"This paper presents a novel hierarchical task planner under partial observability that empowers an embodied agent to use visual input to efficiently plan a sequence of actions for simultaneous object search and rearrangement in an untidy room, to achieve a desired tidy state. The paper introduces (i) a novel Search Network that utilizes commonsense knowledge from large language models to find unseen objects, (ii) a Deep RL network trained with proxy reward, along with (iii) a novel graph-based state representation to produce a scalable and effective planner that interleaves object search and rearrangement to minimize the number of steps taken and overall traversal of the agent, as well as to resolve blocked goal and swap cases, and (iv) a sample-efficient cluster-biased sampling for simultaneous training of the proxy reward network along with the Deep RL network. Furthermore, the paper presents new metrics and a benchmark dataset - RoPOR, to measure the effectiveness of rearrangement planning. Experimental results show that our method significantly outperforms the state-of-the-art rearrangement methods Weihs et al. (2021a); Gadre et al. (2022); Sarch et al. (2022); Ghosh et al. (2022).",[],[],"['Karan Mirakhor', 'Sourav Ghosh', 'Dipanjan Das', 'Brojeshwar Bhowmick']","['Robotics Institute, CMU, Carnegie Mellon University', 'Tata Consultancy Services Limited, India', 'Jadavpur University', 'TCS Research & Innovation']","['United States', 'India', 'India', 'Chad']"
https://openreview.net/forum?id=O8ouVV8PjF,Transparency & Explainability,CNN Kernels Can Be the Best Shapelets,"Shapelets and CNN are two typical approaches to model time series. Shapelets aim at finding a set of sub-sequences that extract feature-based interpretable shapes, but may suffer from accuracy and efficiency issues. CNN performs well by encoding sequences with a series of hidden representations, but lacks interpretability. In this paper, we demonstrate that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling. Based on this finding, we propose ShapeConv, an interpretable CNN layer with its kernel serving as shapelets to conduct time-series modeling tasks in both supervised and unsupervised settings. By incorporating shaping regularization, we enforce the similarity for maximum interpretability. We also find human knowledge can be easily injected to ShapeConv by adjusting its initialization and model performance is boosted with it. Experiments show that ShapeConv can achieve state-of-the-art performance on time-series benchmarks without sacrificing interpretability and controllability.",[],[],"['Eric Qu', 'Yansen Wang', 'Xufang Luo', 'Wenqiang He', 'Kan Ren', 'Dongsheng Li']","['Meta FAIR', 'Microsoft Research Asia', 'Microsoft Research', 'University of Science and Technology of China', 'ShanghaiTech University', 'Microsoft Research Asia']","['United States', 'China', 'United States', 'China', 'China', 'China']"
https://openreview.net/forum?id=jH67LHVOIO,Fairness & Bias,LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses,"A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs.",[],[],"['Xin Liu', 'Muhammad Khalifa', 'Lu Wang']","['University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor', 'Northeastern University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=NnyD0Rjx2B,Fairness & Bias,fairret: a Framework for Differentiable Fairness Regularization Terms,"Current tools for machine learning fairness only admit a limited range of fairness definitions and have seen little integration with automatic differentiation libraries, despite the central role these libraries play in modern machine learning pipelines.  We introduce a framework of fairness regularization terms (fairret) which quantify bias as modular objectives that are easily integrated in automatic differentiation pipelines. By employing a general definition of fairness in terms of linear-fractional statistics, a wide class of fairrets can be computed efficiently. Experiments show the behavior of their gradients and their utility in enforcing fairness with minimal loss of predictive power compared to baselines. Our contribution includes a PyTorch implementation of the fairret framework.",[],[],"['Maarten Buyl', 'MaryBeth Defrance', 'Tijl De Bie']","['Universiteit Gent', 'Universiteit Gent', 'Ghent University']","['Belgium', 'Belgium', 'Belgium']"
https://openreview.net/forum?id=NDkpxG94sF,Fairness & Bias,V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection,"We introduce a highly performant 3D object detector for point clouds using the DETR framework. The prior attempts all end up with suboptimal results because they fail to learn accurate inductive biases from the limited scale of training data. In particular, the queries often attend to points that are far away from the target objects, violating the locality principle in object detection. To address the limitation, we introduce a novel 3D Vertex Relative Position Encoding (3DV-RPE) method which computes position encoding for each point based on its relative position to the 3D boxes predicted by the queries in each decoder layer, thus providing clear information to guide the model to focus on points near the objects, in accordance with the principle of locality. Furthermore, we have systematically refined our pipeline, including data normalization, to better align with the task requirements. Our approach demonstrates remarkable performance on the demanding ScanNetV2 benchmark, showcasing substantial enhancements over the prior state-of-the-art CAGroup3D. Specifically, we achieve an increase in $AP_{25}$  from $75.1\%$ to $77.8\%$ and in  ${AP}_{50}$ from $61.3\%$ to $66.0\%$.",[],[],"['Yichao Shen', 'Zigang Geng', 'Yuhui Yuan', 'Yutong Lin', 'Ze Liu', 'Chunyu Wang', 'Han Hu', 'Nanning Zheng', 'Baining Guo']","[""Xi'an Jiaotong University"", 'University of Science and Technology of China', 'Visual Computing Group, Microsoft Research', ""Xi'an Jiaotong University"", 'University of Science and Technology of China', 'Microsoft Research Asia', 'Microsft Research Asia', ""Xi'an Jiaotong University"", 'Microsoft Research']","['China', 'China', 'China', 'China', 'China', '', 'China', 'China', '']"
https://openreview.net/forum?id=L3FHMoKZcS,Fairness & Bias,Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering,"Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.",[],[],"['Han Zhou', 'Xingchen Wan', 'Lev Proleev', 'Diana Mincu', 'Jilin Chen', 'Katherine A Heller', 'Subhrajit Roy']","['University of Cambridge', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","['United Kingdom', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=MEGQGNUfPx,Security,The Effectiveness of Random Forgetting for Robust Generalization,"Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called ""Forget to Mitigate Overfitting (FOMO)"". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between the standard and robust accuracy outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.",[],[],"['Vijaya Raghavan T Ramkumar', 'Bahram Zonooz', 'Elahe Arani']","['', 'Department of Mathematics and Computer Science, Eindhoven University of Technology', 'Mathematics and computer science , Eindhoven University of technology']","['', 'Netherlands', 'Netherlands']"
https://openreview.net/forum?id=iI7hZSczxE,Transparency & Explainability,Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference,"Learning disentangled representations for time series is a promising path to facilitate reliable generalization to in- and out-of distribution (OOD), offering benefits like feature derivation and improved interpretability and fairness, thereby enhancing downstream tasks. We focus on disentangled representation learning for home appliance electricity usage, enabling users to understand and optimize their consumption for a reduced carbon footprint. Our approach frames the problem as disentangling each attribute's role in total consumption. Unlike existing methods assuming attribute independence which leads to non-identiability, we acknowledge real-world time series attribute correlations, learned up to a smooth bijection using contrastive learning and a single autoencoder. To address this, we propose a Disentanglement under Independence-Of-Support via Contrastive Learning (DIOSC), facilitating representation generalization across diverse correlated scenarios. Our method utilizes innovative \textit{l}-variational inference layers with self-attention, effectively addressing temporal dependencies across bottom-up and top-down networks. We find that DIOSC can enhance the task of representation of time series electricity consumption. We introduce TDS (Time Disentangling Score) to gauge disentanglement quality. TDS reliably reflects disentanglement performance, making it a valuable metric for evaluating time series representations disentanglement. Code available at https://institut-polytechnique-de-paris.github.io/time-disentanglement-lib.",[],[],"['Khalid Oublal', 'Said Ladjal', 'David Benhaiem', 'Emmanuel LE BORGNE', 'François Roueff']","['Computer Science, École Polytechnique', 'Telecom Paristech', 'Linear accelerator Laboratory', 'R&D, TotalEnergies', 'Télécom Paris']","['France', 'United States', 'Serbia', 'Japan', 'United States']"
https://openreview.net/forum?id=JnYaF3vv3G,Fairness & Bias,LabelDP-Pro: Learning with Label Differential Privacy via Projections,"Label differentially private (label DP) algorithms seek to preserve the privacy of the labels in a training dataset in settings where the features are known to the adversary. In this work, we study a new family of label DP training algorithms. Unlike most prior label DP algorithms that have been based on label randomization, our algorithm naturally leverages the power of the central model of DP. It interleaves gradient projection operations with private stochastic gradient descent steps in order to improve the utility of the trained model while guaranteeing the privacy of the labels. We show that such projection-based algorithms can be made practical and that they improve on the state-of-the art for label DP training in the high-privacy regime. We complement our empirical evaluation with theoretical results shedding light on the efficacy of our method through the lens of bias-variance trade-offs.",[],[],"['Badih Ghazi', 'Yangsibo Huang', 'Pritish Kamath', 'Ravi Kumar', 'Pasin Manurangsi', 'Chiyuan Zhang']","['Google', 'Google', 'Google Research', 'Research, Google', 'Google', 'Google']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=MNShbDSxKH,Transparency & Explainability,GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules,"Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate languages into module descriptions, thus achieving strong visual reasoning results while maintaining the model’s transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. On the contrary, human beings gradually acquire knowledge that can be reused and grow into more profound skills for fast generalization to new tasks since we are an infant. Inspired by this, we propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module’s ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed GENOME model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the visual modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.",[],[],"['Zhenfang Chen', 'Rui Sun', 'Wenjun Liu', 'Yining Hong', 'Chuang Gan']","['MIT-IBM Watson AI lab', 'University of California, Los Angeles', 'Tsinghua University, Tsinghua University', 'University of California, Los Angeles', 'University of Massachusetts at Amherst']","['United States', 'United States', 'China', 'China', 'United States', 'United States']"
https://openreview.net/forum?id=J7ioefqDPw,Security,Rethinking Label Poisoning for GNNs: Pitfalls and Attacks,"Node labels for graphs are usually generated using an automated process or crowd-sourced from human users. This opens up avenues for malicious users to compromise the training labels, making it unwise to blindly rely on them. While robustness against noisy labels is an active area of research, there are only a handful of papers in the literature that address this for graph-based data. Even more so, the effects of adversarial label perturbations is sparsely studied. More critically, we reveal that the entire literature on label poisoning for GNNs is plagued by serious evaluation pitfalls. Thus making it hard to conclude how robust GNNs are against label perturbations. After course correcting the state of label poisoning attacks with our faithful evaluation, we identify a discrepancy in attack efficiency of $\sim9\%$ on average. Additionally, we introduce two new simple yet effective attacks that are significantly stronger (up to $\sim8\%$) than the previous strongest attack. Our strongest proposed attack can be efficiently computed and is theoretically backed.",[],[],"['Vijay Lingam', 'Mohammad Sadegh Akhondzadeh', 'Aleksandar Bojchevski']","['Amazon', 'Universität Köln', 'University of Cologne']","['Japan', 'Germany', 'Italy']"
https://openreview.net/forum?id=hiHZVUIYik,Fairness & Bias,"A path-norm toolkit for modern networks: consequences, promises and challenges","This work introduces the first toolkit around path-norms that fully encompasses general DAG ReLU networks with biases, skip connections and any operation based on the extraction of order statistics: max pooling, GroupSort etc. This toolkit notably allows us to establish generalization bounds for modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type.  These extended path-norms further enjoy the usual benefits of path-norms: ease of computation,  invariance under the symmetries of the network, and improved sharpness on layered fully-connected networks compared to the product of operator norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.",[],[],"['Antoine Gonon', 'Nicolas Brisebarre', 'Elisa Riccietti', 'Rémi Gribonval']","['EPFL - EPF Lausanne', 'CNRS', 'Computer Science, ENS Lyon', 'INRIA']","['Switzerland', '', 'France', '']"
https://openreview.net/forum?id=JW3jTjaaAB,Transparency & Explainability,AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction,"Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships within the air quality data. Experiments on two real-world benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art models for different testing scenarios including different lead time (24h, 48h, 72h), sparse data and sudden change prediction, achieving reduction in prediction errors up to 10\%. Moreover, a case study further validates that our model captures underlying physical processes of particle movement and generates accurate predictions with real physical meaning. The code is available at: https://github.com/kethmih/AirPhyNet",[],[],"['Kethmi Hirushini Hettige', 'Jiahao Ji', 'Shili Xiang', 'Cheng Long', 'Gao Cong', 'Jingyuan Wang']","['', '', 'Institute for Infocomm Research, A*STAR', 'College of Computing and Data Science, Nanyang Technological University', 'Nanyang Technological University', 'School of Computer Science and Engineering, Beihang University']","['', '', 'Singapore', 'Singapore', 'Singapore', '']"
https://openreview.net/forum?id=IzqZbNMZ0M,Privacy & Data Governance,Private Zeroth-Order Nonsmooth Nonconvex Optimization,"We introduce a new zeroth-order algorithm for private stochastic optimization on nonconvex and nonsmooth objectives. Given a dataset of size $M$, our algorithm ensures $(\alpha,\alpha\rho^2/2)$-Renyi differential privacy and finds a $(\delta,\epsilon)$-stationary point so long as $M=\tilde\Omega(\frac{d}{\delta\epsilon^3} + \frac{d^{3/2}}{\rho\delta\epsilon^2})$. This matches the optimal complexity found in its non-private zeroth-order analog.  Notably, although the objective is not smooth, we have privacy ``for free'' when $\rho \ge \sqrt{d}\epsilon$.",[],[],"['Qinzi Zhang', 'Hoang Tran', 'Ashok Cutkosky']","['Boston University, Boston University', 'Department of Electrical and Computer Engineering, Boston University', 'Boston University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=jX2DT7qDam,Fairness & Bias,Jointly-Learned Exit and Inference for a Dynamic Neural Network,"Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for $\textit{every}$ inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs are decoupled during training, leading to a train-test mismatch; and 2) the thresholding gating mechanism introduces a positive bias into the predictive probabilities, making it difficult to readily extract uncertainty information. We propose a novel architecture that connects these two modules. This leads to significant performance improvements on classification datasets and enables better uncertainty characterization capabilities.",[],[],"['florence regol', 'Joud Chataoui', 'Mark Coates']","['McGill University', 'McGill University', 'Electrical and Computer Engineering, McGill University']","['Canada', 'Canada', 'Canada']"
https://openreview.net/forum?id=m50eKHCttz,Privacy & Data Governance,Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model,"Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other – independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such ""complementary"" knowledge from one model to another without performance degradation – a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer in scenarios agnostic to pretrained model pairings would unlock auxiliary gains and knowledge fusion from any model repository without restrictions on model and problem specifics - including from weaker, lower-performance models. This work therefore provides an initial, in-depth exploration on the viability of such general-purpose knowledge transfer. Across large-scale experiments, we first reveal the shortcomings of standard knowledge distillation techniques, and then propose a much more general extension through data partitioning for successful transfer between nearly all pretrained models, which we show can also be done unsupervised. Finally, we assess both the scalability and impact of fundamental model properties on successful model-agnostic knowledge transfer.",[],[],"['Karsten Roth', 'Lukas Thede', 'A. Sophia Koepke', 'Oriol Vinyals', 'Olivier J Henaff', 'Zeynep Akata']","['Computer Science, University of Tuebingen', 'Eberhard-Karls-Universität Tübingen', 'Technische Universität München', 'Google', 'DeepMind', 'Technische Universität München']","['Germany', 'Germany', 'Germany', 'United States', 'United States', 'Germany']"
https://openreview.net/forum?id=mXpNp8MMr5,Security,On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks,"Adversarial robustness is an important standard for measuring the quality of learned models, and adversarial training is an effective strategy for improving the adversarial robustness of models. In this paper, we disclose that adversarially trained models are vulnerable to two-faced attacks, where slight perturbations in input features are crafted to make the model exhibit a false sense of robustness in the verification phase. Such a threat is significantly important as it can mislead our evaluation of the adversarial robustness of models, which could cause unpredictable security issues when deploying substandard models in reality. More seriously, this threat seems to be pervasive and tricky: we find that many types of models suffer from this threat, and models with higher adversarial robustness tend to be more vulnerable. Furthermore, we provide the first attempt to formulate this threat, disclose its relationships with adversarial risk, and try to circumvent it via a simple countermeasure. These findings serve as a crucial reminder for practitioners to exercise caution in the verification phase, urging them to refrain from blindly trusting the exhibited adversarial robustness of models.",[],[],"['Shengjie Zhou', 'Lue Tao', 'Yuzhou Cao', 'Tao Xiang', 'Bo An', 'Lei Feng']","['School of Computer Science, Chongqing University', 'Nanjing University', 'Nanyang Technological University', 'Colllege of Computer Science, Chongqing University', 'School of Computer Science and Engineering, Nanyang Technological University', 'Singapore University of Technology and Design']","['China', 'China', 'Singapore', 'China', 'Singapore', 'Singapore']"
https://openreview.net/forum?id=VmGRoNDQgJ,Security,Influencer Backdoor Attack on Semantic Segmentation,"When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification, backdoor attacks on semantic segmentation have been largely overlooked. Unlike classification, semantic segmentation aims to classify every pixel within a given image. In this work, we explore backdoor attacks on segmentation models to misclassify all pixels of a victim class by injecting a specific trigger on non-victim pixels during inferences, which is dubbed Influencer Backdoor Attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and mislead classifications of all victim pixels in every single inference and could be easily applied to real-world scenes. Based on the context aggregation ability of segmentation models, we proposed a simple, yet effective, Nearest-Neighbor trigger injection strategy. We also introduce an innovative Pixel Random Labeling strategy which maintains optimal performance even when the trigger is placed far from the victim pixels. Our extensive experiments reveal that current segmentation models do suffer from backdoor attacks, demonstrate IBA real-world applicability, and show that our proposed techniques can further increase attack performance.",[],[],"['Haoheng Lan', 'Jindong Gu', 'Philip Torr', 'Hengshuang Zhao']","['', '', 'University of Oxford', 'The University of Hong Kong']","['', '', 'United Kingdom', 'Hong Kong']"
https://openreview.net/forum?id=5h0qf7IBZZ,Fairness & Bias,MiniLLM: Knowledge Distillation of Large Language Models,"Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",[],[],"['Yuxian Gu', 'Li Dong', 'Furu Wei', 'Minlie Huang']","['Computer Science and Technology, Tsinghua University, Tsinghua University', 'Microsoft Research', 'Microsoft Research', 'Tsinghua University, Tsinghua University']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=5liV2xUdJL,Fairness & Bias,Time-Efficient Reinforcement Learning with Stochastic Stateful Policies,"Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time (BPTT), which comes with significant drawbacks, such as slow training due to sequential gradient propagation and the occurrence of vanishing or exploding gradients. The gradient is often truncated to address these issues, resulting in a biased policy update. We present a novel approach for training stateful policies by decomposing the latter into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, enabling us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms. Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, e.g. humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.",[],[],"['Firas Al-Hafez', 'Guoping Zhao', 'Jan Peters', 'Davide Tateo']","['Technische Universität Darmstadt', 'Technische Universität Darmstadt', 'Systems AI for Robot Learning, German Research Center for AI', 'Technische Universität Darmstadt']","['Germany', 'Germany', 'Germany', 'Germany']"
https://openreview.net/forum?id=5HCnKDeTws,Fairness & Bias,"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method","While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.",[],[],"['Biao Zhang', 'Zhongtao Liu', 'Colin Cherry', 'Orhan Firat']","['Google DeepMind', 'Google', 'Google']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=4Ua4hKiAJX,Fairness & Bias,Locality-Aware Graph Rewiring in GNNs,"Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and  (iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii). We propose a novel rewiring framework that satisfies all of (i)--(iii) through a locality-aware sequence of rewiring operations. We then discuss a specific instance of such rewiring framework and  validate its effectiveness on several real-world benchmarks, showing that it either matches or significantly outperforms existing rewiring approaches.",[],[],"['Federico Barbero', 'Ameya Velingker', 'Amin Saberi', 'Michael M. Bronstein', 'Francesco Di Giovanni']","['University of Oxford', 'Google', 'Stanford University', 'University of Oxford', 'Valence Labs powered by recursion']","['United Kingdom', 'United States', 'United States', 'United Kingdom', 'France']"
https://openreview.net/forum?id=4KqkizXgXU,Security,Curiosity-driven Red-teaming for Large Language Models,"Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a $\textit{red team}$ of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs.  However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty.  Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at https://github.com/Improbable-AI/curiosity_redteam.",[],[],"['Zhang-Wei Hong', 'Idan Shenfeld', 'Tsun-Hsuan Wang', 'Yung-Sung Chuang', 'Aldo Pareja', 'James R. Glass', 'Akash Srivastava', 'Pulkit Agrawal']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Delft University of Technology', '', 'International Business Machines', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States', 'United States', '', 'United States', 'United States']"
https://openreview.net/forum?id=4DoSULcfG6,Security,Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning,"The integration of Machine Learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for ML training purposes. One such privacy risk is Membership Inference (MI), in which an adversary seeks to determine whether a particular data point was included in the training dataset of a model. Current state-of-the-art MI approaches capitalize on access to the model’s predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness.  In this work, we  focus on the less explored and more realistic label-only setting, where the model provides only the predicted label as output. We show that existing label-only attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge,  we propose a new attack Chameleon that leverages a novel data poisoning strategy and an efficient query selection method to achieve significantly more accurate membership inference than existing label-only attacks, especially for low FPRs.",[],[],"['Harsh Chaudhari', 'Giorgio Severi', 'Alina Oprea', 'Jonathan Ullman']","['Northeastern University', 'Northeastern University', 'Computer Science, Northeastern University', 'Computer Science, University of Michigan - Ann Arbor']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=jj5ZjZsWJe,Fairness & Bias,Stochastic Controlled Averaging for Federated Learning with Communication Compression,"Communication compression has been an important topic in Federated Learning (FL) for alleviating the communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression. In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs, building upon which we propose two compressed FL algorithms, SCALLION and  SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover,SCALLION and SCAFCOM attain fast convergence rates under arbitrary data heterogeneity without any additional assumptions on compression errors. Experiments show that \scallion and  \scafcom outperform recent compressed FL methods under the same communication budget.",[],[],"['Xinmeng Huang', 'Ping Li', 'Xiaoyun Li']","['University of Pennsylvania', 'Rutgers University', 'LinkedIn']","['United States', 'United States', '']"
https://openreview.net/forum?id=3xDaj4pRna,Fairness & Bias,Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning,"Sharpness-Aware Minimization (SAM) has emerged as a promising alternative optimizer to stochastic gradient descent (SGD). The originally-proposed motivation behind SAM was to bias neural networks towards flatter minima that are believed to generalize better. However, recent studies have shown conflicting evidence on the relationship between flatness and generalization, suggesting that flatness does fully explain SAM's success. Sidestepping this debate, we identify an orthogonal effect of SAM that is beneficial out-of-distribution: we argue that SAM implicitly balances the quality of diverse features. SAM achieves this effect by adaptively suppressing well-learned features which gives remaining features opportunity to be learned. We show that this mechanism is beneficial in datasets that contain redundant or spurious features where SGD falls for the simplicity bias and would not otherwise learn all available features. Our insights are supported by experiments on real data: we demonstrate that SAM improves the quality of features in datasets containing redundant or spurious features, including CelebA, Waterbirds, CIFAR-MNIST, and DomainBed.",[],[],"['Jacob Mitchell Springer', 'Vaishnavh Nagarajan', 'Aditi Raghunathan']","['Carnegie Mellon University', 'Google', 'Carnegie Mellon University']","['United States', '', 'United States']"
https://openreview.net/forum?id=3oTPsORaDH,Fairness & Bias,SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases,"Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. To incorporate these inductive biases, we propose the Second-order Equivariant Graph Neural Ordinary Differential Equation (SEGNO). Specifically, we show how the second-order continuity can be incorporated into GNNs while maintaining the equivariant property. Furthermore, we offer theoretical insights into SEGNO, highlighting that it can learn a unique trajectory between adjacent states, which is crucial for model generalization. Additionally, we prove that the discrepancy between this learned trajectory of SEGNO and the true trajectory is bounded. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that our model yields a significant improvement over the state-of-the-art baselines.",[],[],"['Yang Liu', 'Jiashun Cheng', 'Haihong Zhao', 'Tingyang Xu', 'Peilin Zhao', 'Fugee Tsung', 'Jia Li', 'Yu Rong']","['The Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology', 'DSA, Hong Kong University of Science and Technology', 'Alibaba Group', 'Tencent AI Lab', 'Hong Kong University of Science and Technology (Guangzhou)', 'Information Hub, The Hong Kong University of Science and Technology', 'Alibaba Group']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', '', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://openreview.net/forum?id=3y2TfP966N,Transparency & Explainability,T-Rep: Representation Learning for Time Series using Time-Embeddings,"Multivariate time series present challenges to standard machine learning techniques, as they are often unlabeled, high dimensional, noisy, and contain missing data. To address this, we propose T-Rep, a self-supervised method to learn time series representations at a timestep granularity. T-Rep learns vector embeddings of time alongside its feature extractor, to extract temporal features such as trend, periodicity, or distribution shifts from the signal. These time-embeddings are leveraged in pretext tasks, to incorporate smooth and fine-grained temporal dependencies in the representations, as well as reinforce robustness to missing data. We evaluate T-Rep on downstream classification, forecasting, and anomaly detection tasks. It is compared to existing self-supervised algorithms for time series, which it outperforms in all three tasks. We test T-Rep in missing data regimes, where it proves more resilient than its counterparts. Finally, we provide latent space visualisation experiments, highlighting the interpretability of the learned representations.",[],[],"['Archibald Felix Fraikin', 'Adrien Bennetot', 'Stephanie Allassonniere']","['INRIA', 'Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Ecole Polytechnique ( )']","['France', 'France', 'France']"
https://openreview.net/forum?id=3VD4PNEt5q,Security,Fusion Is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection,"Multi-sensor fusion (MSF) is widely used in autonomous vehicles (AVs) for perception, particularly for 3D object detection with camera and LiDAR sensors. The purpose of fusion is to capitalize on the advantages of each modality while minimizing its weaknesses. Advanced deep neural network (DNN)-based fusion techniques have demonstrated the exceptional and industry-leading performance. Due to the redundant information in multiple modalities, MSF is also recognized as a general defence strategy against adversarial attacks.  In this paper, we attack fusion models from the camera modality that is considered to be of lesser importance in fusion but is more affordable for attackers. We argue that the weakest link of fusion models depends on their most vulnerable modality and propose an attack framework that targets advanced camera-LiDAR fusion-based 3D object detection models through camera-only adversarial attacks.  Our approach employs a two-stage optimization-based strategy that first thoroughly evaluates vulnerable image areas under adversarial attacks, and then applies dedicated attack strategies for different fusion models to generate deployable patches. The evaluations with six advanced camera-LiDAR fusion models and one camera-only model indicate that our attacks successfully compromise all of them. Our approach can either decrease the mean average precision (mAP) of detection performance from 0.824 to 0.353 or degrade the detection score of a target object from 0.728 to 0.156, demonstrating the efficacy of our proposed attack framework. Code is available.",[],[],"['Zhiyuan Cheng', 'Hongjun Choi', 'Shiwei Feng', 'James Chenhao Liang', 'Guanhong Tao', 'Dongfang Liu', 'Michael Zuzak', 'Xiangyu Zhang']","['Dept. of Computer Science, Purdue University', 'Daegu Gyeongbuk Institute of Science and Technology', 'Purdue University', 'U. S. Naval Research Laboratory', '', 'Rochester Institute of Technology', 'Rochester Institute of Technology', 'Computer Science, Purdue University']","['United States', '', 'United States', 'United States', '', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=jvveGAbkVx,Fairness & Bias,Fair Classifiers that Abstain without Harm,"In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints. To generalize the abstaining decisions to test samples, we then train a surrogate model to learn the abstaining decisions based on the IP solutions in an end-to-end manner. We analyze the feasibility of the IP procedure to determine the possible abstention rate for different levels of unfairness tolerance and accuracy constraint for achieving no harm. To the best of our knowledge, this work is the first to identify the theoretical relationships between the constraint parameters and the required abstention rate. Our theoretical results are important since a high abstention rate is often infeasible in practice due to a lack of human resources. Our framework outperforms existing methods in terms of fairness disparity without sacrificing accuracy at similar abstention rates.",[],[],"['Tongxin Yin', 'Jean-Francois Ton', 'Ruocheng Guo', 'Yuanshun Yao', 'Mingyan Liu', 'Yang Liu']","['University of Michigan - Ann Arbor', 'R&D, Bytedance', 'Bytedance Research/TikTok UK', 'Meta GenAI', 'EECS, University of Michigan - Ann Arbor', 'Computer Science and Engineering, University of California, Santa Cruz']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=cmcD05NPKa,Transparency & Explainability,Learning the greatest common divisor: explaining transformer predictions,"The predictions of small transformers, trained to calculate the greatest common divisor (GCD) of two positive integers, can be fully characterized by looking at model inputs and outputs. As training proceeds, the model learns a list $\mathcal D$ of integers, products of divisors of the base used to represent integers and small primes, and predicts the largest element of $\mathcal D$ that divides both inputs.  Training distributions impact performance. Models trained from uniform operands only learn a handful of GCD (up to $38$ GCD $\leq100$). Log-uniform operands boost performance to $73$ GCD $\leq 100$, and a log-uniform distribution of outcomes (i.e. GCD) to $91$. However, training from uniform (balanced) GCD breaks explainability.",[],[],['Francois Charton'],['Facebook'],['United States']
https://openreview.net/forum?id=31IOmrnoP4,Fairness & Bias,Repelling Random Walks,"We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able  to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.",[],[],"['Isaac Reid', 'Eli Berger', 'Krzysztof Marcin Choromanski', 'Adrian Weller']","['University of Cambridge', 'University of Haifa', 'Google Brain Robotics & Columbia University', 'Alan Turing Institute']","['United Kingdom', 'Israel', 'United States', 'Italy']"
https://openreview.net/forum?id=k581sTMyPt,Transparency & Explainability,Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making,"Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1) while PubMedBERT, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of SUFO in enhancing trust and safety when using transformers in medicine, and we believe SUFO can aid practitioners in evaluating fine-tuned language models (LMs) for other applications in medicine and in more critical domains.",[],[],"['Aliyah R. Hsu', 'Yeshwanth Cherapanamjeri', 'Briton Park', 'Tristan Naumann', 'Anobel Odisho', 'Bin Yu']","['University of California, Berkeley', 'Massachusetts Institute of Technology', 'Citadel Securities', 'Microsoft Research', 'University of California, San Francisco', 'University of California Berkeley']","['United States', 'United States', 'United States', '', '', 'United States']"
https://openreview.net/forum?id=2iGiSHmeAN,Fairness & Bias,BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics,"Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as Newtonian or Hamiltonian. Here, we propose a framework, namely Brownian graph neural networks (BroGNet), combining stochastic differential equations (SDEs) and GNNs to learn Brownian dynamics directly from the trajectory. We modify the architecture of BroGNet to enforce linear momentum conservation of the system, which, in turn, provides superior performance on learning dynamics as revealed empirically. We demonstrate this approach on several systems, namely, linear spring, linear spring with binary particle types, and non-linear spring systems, all following Brownian dynamics at finite temperatures. We show that BroGNet significantly outperforms proposed baselines across all the benchmarked Brownian systems. In addition, we demonstrate zero-shot generalizability of BroGNet to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training. Finally, we show that BroGNet conserves the momentum of the system resulting in superior performance and data efficiency. Altogether, our study contributes to advancing the understanding of the intricate dynamics of Brownian motion and demonstrates the effectiveness of graph neural networks in modeling such complex systems.",[],[],"['Suresh Bishnoi', 'Jayadeva Jayadeva', 'Sayan Ranu', 'N M Anoop Krishnan']","['Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'Computer Science & Engineering, Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi']","['India', 'India', 'India', 'India']"
https://openreview.net/forum?id=1pSL2cXWoz,Fairness & Bias,ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection,"Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints. To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions. Leveraging the conjugation constraint revealed in our theorem, we introduce a \textsc{ConjNorm} method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset. In light of the computational challenges of normalization, we devise an unbiased and analytically tractable estimator of the partition function using the Monte Carlo-based importance sampling technique. Extensive experiments across OOD detection benchmarks empirically demonstrate that our proposed \textsc{ConjNorm} has established a new state-of-the-art in a variety of OOD detection setups, outperforming the current best method by up to 13.25\% and 28.19\% (FPR95) on CIFAR-100 and ImageNet-1K, respectively.",[],[],"['Bo Peng', 'Yadan Luo', 'Yonggang Zhang', 'Yixuan Li', 'Zhen Fang']","['', 'The University of Queensland', 'Hong Kong Baptist University', 'University of Wisconsin, Madison', 'AAII, University of Technology Sydney']","['', 'Australia', 'Hong Kong', 'United States', 'Australia']"
https://openreview.net/forum?id=1YPfmglNRU,Fairness & Bias,Defining Expertise: Applications to Treatment Effect Estimation,"Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and “expertise” is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise—particularly the type of expertise the decision-makers of a domain are likely to have—can be informative in designing and selecting methods for treatment effect estimation. We formally define two types of expertise, predictive and prognostic, and demonstrate empirically that: (i) the prominent type of expertise in a domain significantly influences the performance of different methods in treatment effect estimation, and (ii) it is possible to predict the type of expertise present in a dataset, which can provide a quantitative basis for model selection.",[],[],"['Alihan Hüyük', 'Qiyao Wei', 'Alicia Curth', 'Mihaela van der Schaar']","['Harvard University', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://openreview.net/forum?id=14rn7HpKVk,Transparency & Explainability,SALMONN: Towards Generic Hearing Abilities for Large Language Models,"Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as  automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",[],[],"['Changli Tang', 'Wenyi Yu', 'Guangzhi Sun', 'Xianzhao Chen', 'Tian Tan', 'Wei Li', 'Lu Lu', 'Zejun MA', 'Chao Zhang']","['Electronic Engineering, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'University of Cambridge', 'Tianjin University', 'Shanghai Jiaotong University', 'Tiktok, Bytedance', 'Louisiana State University', 'ByteDance Inc.', 'Tsinghua University']","['China', 'China', 'China', 'China', 'China', '', 'China', '', 'China']"
https://openreview.net/forum?id=0w42S2Gp70,Security,LipSim: A Provably Robust Perceptual Similarity Metric,"Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees.  By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an $\ell_2$ ball. Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application.",[],[],"['Sara Ghazanfari', 'Alexandre Araujo', 'Prashanth Krishnamurthy', 'Farshad Khorrami', 'Siddharth Garg']","['Electrical and Computer Engineering, New York University', 'New York University', 'Electrical and Computer Engineering, New York University', 'ECE, New York University', 'New York University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=1SbkubNdbW,Security,Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks,"Label smoothing – using softened labels instead of hard ones – is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against MIAs.",[],[],"['Lukas Struppek', 'Dominik Hintersdorf', 'Kristian Kersting']","['Technische Universität Darmstadt', 'German Research Center for AI', 'German Research Center for AI']","['Côte d’Ivoire', 'Côte d’Ivoire', 'Côte d’Ivoire']"
https://openreview.net/forum?id=f6CBQYxXvr,Fairness & Bias,Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features,"Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most of a very small target dataset, we propose a lightweight, sample-efficient approach that learns a diverse set of features and adapts to a target distribution by interpolating these features. Our approach, Project and Probe (Pro$^2$), first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro$^2$ then learns a linear classifier on top of these projected features using a small target dataset. Theoretically, we find that Pro$^2$ results in more sample-efficient generalization by inducing a favorable bias-variance tradeoff. Our experiments on four datasets, with multiple distribution shift settings for each, show that Pro$^2$ improves performance by 5-15% when given limited target data compared to prior methods such as standard linear probing.",[],[],"['Annie S Chen', 'Yoonho Lee', 'Amrith Setlur', 'Sergey Levine', 'Chelsea Finn']","['Google', 'Stanford University', 'School of Computer Science, Carnegie Mellon University', 'University of California Berkeley', 'Physical Intelligence']","['', 'United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=0j9ZDzMPqr,Transparency & Explainability,UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models,"Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning, explaining unsupervised models has been less explored. To address this gap, we propose a method for generating counterfactual (CF) explanations in unsupervised node representation learning, aiming to identify the most important subgraphs that cause a significant change in the $k$-nearest neighbors of a node of interest in the learned embedding space upon perturbation. The $k$-nearest neighbor-based CF explanation method provides simple, yet pivotal, information for understanding unsupervised downstream tasks, such as top-$k$ link prediction and clustering. Furthermore, we introduce a Monte Carlo Tree Search (MCTS)-based explainability method for generating expressive CF explanations for **U**nsupervised **N**ode **R**epresentation learning methods, which we call **UNR-Explainer**. The proposed method demonstrates improved performance on six datasets for both unsupervised GraphSAGE and DGI.",[],[],"['Hyunju Kang', 'Geonhee Han', 'Hogun Park']","['Artificial Intelligence, Sungkyunkwan University', 'Sung Kyun Kwan University', 'Computer Science Engineering, Sungkyunkwan University']","['South Korea', 'South Korea', 'South Korea']"
https://openreview.net/forum?id=ZSD3MloKe6,Fairness & Bias,Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps,"Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the synthesis of high-quality images. However, their inference process characteristically requires numerous, potentially hundreds, of iterative steps, which could exaggerate the problem of exposure bias due to the training and inference discrepancy. Previous work has attempted to mitigate this issue by perturbing inputs during training, which consequently mandates the retraining of the DPM. In this work, we conduct a systematic study of exposure bias in DPM and, intriguingly, we find that the exposure bias could be alleviated with a novel sampling method that we propose, without retraining the model. We empirically and theoretically show that, during inference, for each backward time step t and corresponding state ˆxt, there might exist another time step $t_s$ which exhibits superior coupling with $\hat{x}_t$. Based on this finding, we introduce a sampling method named Time-Shift Sampler. Our framework can be seamlessly integrated to existing sampling algorithms, such as DDPM, DDIM and other high-order solvers, inducing merely minimal additional computations. Experimental results show our method brings significant and consistent improvements in FID scores on different datasets and sampling methods. For example, integrating Time-Shift Sampler to F-PNDM yields a FID=3.88, achieving 44.49% improvements as compared to F-PNDM, on CIFAR-10 with 10 sampling steps, which is more performant than the vanilla DDIM with 100 sampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.",[],[],"['Mingxiao Li', 'Tingyu Qu', 'Ruicong Yao', 'Wei Sun', 'Marie-Francine Moens']","['Computer Science, KU Leuven', 'KU Leuven', 'KU Leuven', 'KU Leuven', 'Department of Computer Science, KU Leuven, KU Leuven']","['Belgium', 'Belgium', 'Belgium', 'Belgium', 'Belgium']"
https://openreview.net/forum?id=kILAd8RdzA,Fairness & Bias,On the Generalization and Approximation Capacities of Neural Controlled Differential Equations,"Neural Controlled Differential Equations (NCDE) are a state-of-the-art tool for supervised learning with irregularly sampled time series (Kidger 2020). However, no theoretical analysis of their performance has been provided yet, and it remains unclear in particular how the roughness of the sampling affects their predictions. By merging the rich theory of controlled differential equations (CDE) and Lipschitz-based measures of the complexity of deep neural nets, we take a first step towards the theoretical understanding of NCDE. Our first result is a sampling-dependant generalization bound for this class of predictors. In a second time, we leverage the continuity of the flow of CDEs to provide a detailed analysis of both the sampling-induced bias and the approximation bias. Regarding this last result, we show how classical approximation results on neural nets may transfer to NCDE. Our theoretical results are validated through a series of experiments.",[],[],"['Linus Bleistein', 'Agathe Guilloux']","['Mathematics, INRIA', 'INRIA']","['Philippines', '']"
https://openreview.net/forum?id=mutJBk3ILg,Fairness & Bias,Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation,"Supervised learning methods have been found to exhibit inductive biases favoring simpler features. When such features are spuriously correlated with the label, this can result in suboptimal performance on minority subgroups. Despite the growing popularity of methods which learn from unlabeled data, the extent to which these representations rely on spurious features for prediction is unclear. In this work, we explore the impact of spurious features on Self-Supervised Learning (SSL) for visual representation learning. We first empirically show that commonly used augmentations in SSL can cause undesired invariances in the image space, and illustrate this with a simple example. We further show that classical approaches in combating spurious correlations, such as dataset re-sampling during SSL, do not consistently lead to invariant representations. Motivated by these findings, we propose LateTVG to remove spurious information from these representations during pre-training, by regularizing later layers of the encoder via pruning. We find that our method produces representations which outperform the baselines on several benchmarks, without the need for group or label information during SSL.",[],[],"['Kimia Hamidieh', 'Haoran Zhang', 'Swami Sankaranarayanan', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Flagship Pioneering Labs 97', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=nxnbPPVvOG,Fairness & Bias,Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem,"We consider the problem of linear estimation, and establish an extension of the Gauss-Markov theorem, in which the bias operator is allowed to be non-zero but bounded with respect to a matrix norm of Schatten type.  We derive simple and explicit formulas for the optimal estimator in the cases of Nuclear and  Spectral norms (with the Frobenius case recovering ridge regression). Additionally, we analytically derive the generalization error in multiple random matrix ensembles, and compare with Ridge regression. Finally, we conduct an extensive simulation study, in which we show that the cross-validated Nuclear and Spectral regressors can outperform Ridge in several circumstances.",[],[],['Simon Segert'],['Independent'],['Germany']
https://openreview.net/forum?id=XTHfNGI3zT,Transparency & Explainability,Quantifying the Plausibility of Context Reliance in Neural Machine Translation,"Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings. However, the questions of $\textit{when}$ and $\textit{which parts}$ of the context affect model generations are typically tackled separately, and current plausibility evaluations are practically limited to a handful of artificial benchmarks. To address this, we introduce $\textbf{P}$lausibility $\textbf{E}$valuation of $\textbf{Co}$ntext $\textbf{Re}$liance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use PECoRe to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level phenomena. Finally, we apply our method to unannotated model translations to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.",[],[],"['Gabriele Sarti', 'Grzegorz Chrupała', 'Malvina Nissim', 'Arianna Bisazza']","['Center for Language and Cognition, University of Groningen', 'Tilburg University', 'University of Groningen', 'University of Groningen']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://openreview.net/forum?id=mQYHXUUTkU,Fairness & Bias,BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity,"Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -- Semantic Captioning Using Brain Alignments (""BrainSCUBA"") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high predicted activations. Finally, to demonstrate how our method enables scientific discovery, we perform exploratory investigations on the distribution of ""person"" representations in the brain, and discover fine-grained semantic selectivity in body-selective areas. Unlike earlier studies that decode text, our method derives *voxel-wise captions of semantic selectivity*. Our results show that BrainSCUBA is a promising means for understanding functional preferences in the brain, and provides motivation for further hypothesis-driven investigation of visual cortex.",[],[],"['Andrew Luo', 'Margaret Marie Henderson', 'Michael J. Tarr', 'Leila Wehbe']","['University of Hong Kong', 'Psychology, Carnegie Mellon University', 'Psychology/Neuroscience, Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=pETSfWMUzy,Security,RAIN: Your Language Models Can Align Themselves without Finetuning,"Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",[],[],"['Yuhui Li', 'Fangyun Wei', 'Jinjing Zhao', 'Chao Zhang', 'Hongyang Zhang']","['Peking University', 'Microsoft Research', 'University of Sydney', 'Peking University', 'School of Computer Science, University of Waterloo']","['United States', '', 'Australia', 'United States', 'Canada']"
https://openreview.net/forum?id=r42tSSCHPh,Security,Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation,"The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as ``jailbreaks"". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the attack success rate from $0\%$ to more than $95\%$ across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computational cost. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the attack success rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.",[],[],"['Yangsibo Huang', 'Samyak Gupta', 'Mengzhou Xia', 'Kai Li', 'Danqi Chen']","['Google', 'Computer Science, Princeton University', 'Department of Computer Science, Princeton University', 'Princeton University', 'Department of Computer Science, Princeton University']","['', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=p34fRKp8qA,Fairness & Bias,Lie Group Decompositions for Equivariant Neural Networks,"Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \textnormal{GL}^{+}(n, \mathbb{R})$ and $G = \textnormal{SL}(n, \mathbb{R})$, as well as their representation as affine transformations $\mathbb{R}^{n} \rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the ""larger"" groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.",[],[],"['Mircea Mironenco', 'Patrick Forré']","['', '']","['', '']"
https://openreview.net/forum?id=w1JanwReU6,Fairness & Bias,Are Models Biased on Text without Gender-related Language?,"Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: *Do language models still exhibit gender bias in non-stereotypical settings?* To do so, we introduce **UnStereoEval (USE)**, a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language.  By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models.  Concretely, models demonstrate fair behavior in only 9%-41% of  stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at [ucinlp.github.io/unstereo-eval](https://ucinlp.github.io/unstereo-eval).",[],[],"['Catarina G Belém', 'Preethi Seshadri', 'Yasaman Razeghi', 'Sameer Singh']","['Department of Computer Science, University of California, Irvine', 'University of California, Irvine', 'University of California, Irvine', 'University of California, Irvine']","[''United States', ''United States', ''United States', ''United States']"
https://openreview.net/forum?id=viftsX50Rt,Fairness & Bias,General Graph Random Features,"We propose a novel random walk-based algorithm for unbiased estimation of arbitrary functions of a weighted adjacency matrix, coined  general graph random features (g-GRFs). This includes many of the most popular examples of kernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time complexity with respect to the number of nodes, overcoming the notoriously prohibitive cubic scaling of exact graph kernel evaluation. It can also be trivially distributed across machines, permitting learning on much larger networks. At the heart of the algorithm is a modulation function which upweights or downweights the contribution from different random walks depending on their lengths. We show that by parameterising it with a neural network we can obtain g-GRFs that give higher-quality kernel estimates or perform efficient, scalable kernel learning. We provide robust theoretical analysis and support our findings with experiments including pointwise estimation of fixed graph kernels, solving non-homogeneous graph ordinary differential equations, node clustering and kernel regression on triangular meshes.",[],[],"['Isaac Reid', 'Krzysztof Marcin Choromanski', 'Eli Berger', 'Adrian Weller']","['University of Cambridge', 'Google Brain Robotics & Columbia University', 'University of Haifa', 'Alan Turing Institute']","['United Kingdom', 'United States', 'Israel', 'Italy']"
https://openreview.net/forum?id=u3dX2CEIZb,Fairness & Bias,Scaling physics-informed hard constraints with mixture-of-experts,"Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an ``expert'' through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.",[],[],"['Nithin Chalapathi', 'Yiheng Du', 'Aditi S. Krishnapriyan']","['EECS, University of California, Berkeley', 'Peking University', 'University of California, Berkeley']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=thbtoAkCe9,Fairness & Bias,$\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning,"In recent years, data quality has emerged as an important factor for training massive models. Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing *data diversity* in the coreset, and (2) functions that assign *difficulty scores* to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset selection. In this work, we represent a dataset as an undirected graph and propose a novel pruning algorithm, $\mathbb{D}^2$ Pruning, that uses message passing over this dataset graph for coreset selection. $\mathbb{D}^2$ Pruning updates the difficulty scores of each example by incorporating the difficulty of its neighboring examples in the dataset graph. Then, these updated difficulty scores direct a graph-based sampling method to select a coreset that encapsulates both diverse and difficult regions of the dataset space. We evaluate supervised and self-supervised versions of our method on various vision and NLP datasets. Results show that $\mathbb{D}^2$ Pruning improves coreset selection over previous state-of-the-art methods at low-to-medium pruning rates. Additionally, we find that using $\mathbb{D}^2$ Pruning for filtering large multimodal datasets leads to increased diversity in the dataset and improved generalization of pretrained models. Our work shows that $\mathbb{D}^2$ Pruning is a versatile framework for understanding and processing datasets.",[],[],"['Adyasha Maharana', 'Prateek Yadav', 'Mohit Bansal']","['Department of Computer Science, University of North Carolina, Chapel Hill', 'Department of Computer Science, University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=t8cBsT9mcg,Security,Classification with Conceptual Safeguards,"We propose a new approach to promote safety in classification tasks with concept annotations. Our approach – called a *conceptual safeguard* – acts as a verification layer for models that predict a target outcome by first predicting the presence of intermediate concepts. Given this architecture, a safeguard ensures that a model meets a minimal level of accuracy by abstaining from uncertain predictions. In contrast to a standard selective classifier, a safeguard provides an avenue to improve coverage by allowing a human to confirm the presence of uncertain concepts on instances on which it abstains. We develop methods to build safeguards that maximize coverage without compromising safety, namely techniques to propagate the uncertainty in concept predictions and to flag salient concepts for human review. We benchmark our approach on a collection of real-world and synthetic datasets, showing that it can improve performance and coverage in deep learning tasks.",[],[],"['Hailey Joren', 'Charles Thomas Marx', 'Berk Ustun']","['', 'Stanford University', 'University of California, San Diego']","['', 'United States', 'Colombia']"
https://openreview.net/forum?id=x4OPJ7lHVU,Privacy & Data Governance,Privacy-Preserving In-Context Learning for Large Language Models,"In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars.  To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks.  The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets.  Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation.  We experiment on four text classification benchmarks and two language generation tasks, and our empirical findings suggest that our DP-ICL achieves a strong utility-privacy tradeoff.",[],[],"['Tong Wu', 'Ashwinee Panda', 'Jiachen T. Wang', 'Prateek Mittal']","['ECE, Princeton University', 'CS, University of Maryland, College Park', 'Electrical and Computer Engineering, Princeton University', 'Princeton University']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=Zh2iqiOtMt,Fairness & Bias,Towards the Fundamental Limits of Knowledge Transfer over Finite Domains,"We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal{S}$ over labels $\mathcal{A}$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|\mathcal{S}||\mathcal{A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|\mathcal{S}||\mathcal{A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (complete logits) on $\mathcal{A}$ given every sampled input, thereby provably enables the student to enjoy a rate ${|\mathcal{S}|}/{n}$ free of $|\mathcal{A}|$. We find any Kullback-Leibler divergence minimizer to be optimal in the last case. Numerical simulations distinguish the four learners and corroborate our theory.",[],[],"['Qingyue Zhao', 'Banghua Zhu']","['Department of Computer Science, University of California, Los Angeles', 'University of Washington']","[''United States', 'United States']"
https://openreview.net/forum?id=XyrB1Ay44j,Security,Quantifying and Enhancing Multi-modal Robustness with Modality Preference,"Multi-modal models have shown a promising capability to effectively integrate information from various sources, yet meanwhile, they are found vulnerable to pervasive perturbations, such as uni-modal attacks and missing conditions. To counter these perturbations, robust multi-modal representations are highly expected, which are positioned well away from the discriminative multi-modal decision boundary. In this paper, different from conventional empirical studies, we focus on a commonly used joint multi-modal framework and theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness. This discovery can further explain the limitation of multi-modal robustness and the phenomenon that multi-modal models are often vulnerable to attacks on the specific modality. Moreover, our analysis reveals how the widespread issue, that the model has different preferences for modalities, limits the multi-modal robustness by influencing the essential components and could lead to attacks on the specific modality highly effective. Inspired by our theoretical finding, we introduce a training procedure called Certifiable Robust Multi-modal Training (CRMT), which can alleviate this influence from modality preference and explicitly regulate essential components to significantly improve robustness in a certifiable manner. Our method demonstrates substantial improvements in performance and robustness compared with existing methods. Furthermore, our training procedure can be easily extended to enhance other robust training strategies, highlighting its credibility and flexibility.",[],[],"['Zequn Yang', 'Yake Wei', 'Ce Liang', 'Di Hu']","['Gaoling School of Artificial Intelligence, Renmin University of China', 'Renmin University of China', 'Renmin University of China', 'Renmin University of China']","['China', 'China', 'China', 'China']"
https://openreview.net/forum?id=WjRPZsfeBO,Transparency & Explainability,A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data,"Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",[],[],"['Saptarshi Chakraborty', 'Peter Bartlett']","['', 'University of California - Berkeley']","['', 'United States']"
https://openreview.net/forum?id=djM3WzpOmK,Fairness & Bias,Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries,"The inductive bias of a graph neural network (GNN) is largely encoded in its specified graph. Latent graph inference relies on latent geometric representations to dynamically rewire or infer a GNN's graph to maximize the GNN's predictive downstream performance, but it lacks solid theoretical foundations in terms of embedding-based representation guarantees. This paper addresses this issue by introducing a trainable deep learning architecture, coined \textit{neural snowflake}, that can adaptively implement fractal-like metrics on $\mathbb{R}^d$. We prove that any given finite weights graph can be isometrically embedded by a standard MLP encoder. Furthermore, when the latent graph can be represented in the feature space of a sufficiently regular kernel, we show that the combined neural snowflake and MLP encoder do not succumb to the curse of dimensionality by using only a low-degree polynomial number of parameters in the number of nodes. This implementation enables a low-dimensional isometric embedding of the latent graph. We conduct synthetic experiments to demonstrate the superior metric learning capabilities of neural snowflakes when compared to more familiar spaces like Euclidean space.  Additionally, we carry out latent graph inference experiments on graph benchmarks. Consistently, the neural snowflake model achieves predictive performance that either matches or surpasses that of the state-of-the-art latent graph inference models. Importantly, this performance improvement is achieved without requiring random search for optimal latent geometry. Instead, the neural snowflake model achieves this enhancement in a differentiable manner.",[],[],"['Haitz Sáez de Ocáriz Borde', 'Anastasis Kratsios']","['University of Oxford', 'Vector Institute']","['United Kingdom', 'Brazil']"
https://openreview.net/forum?id=WNzy9bRDvG,Fairness & Bias,Improved Techniques for Training Consistency Models,"Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step. These scores mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",[],[],"['Yang Song', 'Prafulla Dhariwal']","['OpenAI', 'OpenAI']","['India', 'India']"
https://openreview.net/forum?id=Tj3xLVuE9f,Fairness & Bias,On the Foundations of Shortcut Learning,"Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on *predictivity*---how reliably a feature indicates training-set labels---but also on *availability*---how easily the feature can be extracted from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and we quantify a model's shortcut bias---its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less available, more predictive) feature. We find that linear models are relatively unbiased, but introducing a single hidden layer with ReLU or Tanh units yields a bias. Our empirical findings are consistent with a theoretical account based on Neural Tangent Kernels. Finally, we study how models used in practice trade off predictivity and availability in naturalistic datasets, discovering availability manipulations which increase models' degree of shortcut bias. Taken together, these findings suggest that the propensity to learn shortcut features is a fundamental characteristic of deep nonlinear architectures warranting systematic study given its role in shaping how models solve tasks.",[],[],"['Katherine Hermann', 'Hossein Mobahi', 'Thomas FEL', 'Michael Curtis Mozer']","['Google', 'Google', 'Kempner Institute, Harvard University', 'Research, Google DeepMind']","['United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=7gDENzTzw1,Security,Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations,"Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty. Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods. Our code is available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.",[],[],"['Xiaolin Sun', 'Zizhan Zheng']","['Computer Science, Tulane University', 'Computer Science, Tulane University']","['United States', 'United States']"
https://openreview.net/forum?id=VJvbOSXRUq,Transparency & Explainability,GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking,"Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",[],[],"['Mert Kosan', 'Samidha Verma', 'Burouj Armgaan', 'Khushbu Pahwa', 'Ambuj Singh', 'Sourav Medya', 'Sayan Ranu']","['VISA', 'Yardi School of Artificial Intelligence, Indian Institute of Technology, Delhi', 'Indian Institute of Technology, Delhi', 'Computer Science, Rice University', '', 'University of Illinois at Chicago', 'Computer Science & Engineering, Indian Institute of Technology Delhi']","['United States', 'India', 'India', 'United States', '', 'United States', 'India']"
https://openreview.net/forum?id=NgaLU2fP5D,Transparency & Explainability,"Predictive, scalable and interpretable knowledge tracing on structured domains","Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (""knowledge tracing""; KT), and the prerequisite structure of the learning domain (""knowledge mapping""). While recent deep learning models achieve high KT accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. PSI-KT is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable Bayesian inference, PSI-KT targets the real-world need for efficient personalization even with a growing body of learners and interaction data. Evaluated on three datasets from online learning platforms, PSI-KT achieves superior multi-step **p**redictive accuracy and **s**calable inference in continual-learning settings, all while providing **i**nterpretable representations of learner-specific traits and the prerequisite structure of knowledge that causally supports learning. In sum, predictive, scalable and interpretable knowledge tracing with solid knowledge mapping lays a key foundation for effective personalized learning to make education accessible to a broad, global audience.",[],[],"['Hanqi Zhou', 'Robert Bamler', 'Charley M Wu', 'Álvaro Tejero-Cantero']","['Eberhard-Karls-Universität Tübingen', 'University of Tuebingen', 'Eberhard-Karls-Universität Tübingen', 'University of Tuebingen']","['Germany', 'Netherlands', 'Germany', 'Netherlands']"
https://openreview.net/forum?id=AcJrSoArlh,Security,Rethinking Model Ensemble in Transfer-based Adversarial Attacks,"It is widely recognized that deep learning models lack robustness to adversarial examples. An intriguing property of adversarial examples is that they can transfer across different models, which enables black-box attacks without any knowledge of the victim model. An effective strategy to improve the transferability is attacking an ensemble of models. However, previous works simply average the outputs of different models, lacking an in-depth analysis on how and why model ensemble methods can strongly improve the transferability. In this paper, we rethink the ensemble in adversarial attacks and define the common weakness of model ensemble with two properties: 1) the flatness of loss landscape; and 2) the closeness to the local optimum of each model. We empirically and theoretically show that both properties are strongly correlated with the transferability and propose a Common Weakness Attack (CWA) to generate more transferable adversarial examples by promoting these two properties. Experimental results on both image classification and object detection tasks validate the effectiveness of our approach to improving the adversarial transferability, especially when attacking adversarially trained models. We also successfully apply our method to attack a black-box large vision-language model -- Google's Bard, showing the practical effectiveness. Code is available at \url{https://github.com/huanranchen/AdversarialAttacks}.",[],[],"['Huanran Chen', 'Yichi Zhang', 'Yinpeng Dong', 'Xiao Yang', 'Hang Su', 'Jun Zhu']","['AI, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Computer Science, Tsinghua University', 'Computer Science, Tsinghua University']","['China', 'China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=wxJ0eXwwda,Security,The Unlocking Spell on Base LLMs:  Rethinking Alignment via In-Context Learning,"Alignment tuning has become the de facto standard practice for enabling base large language models (LLMs) to serve as open-domain AI assistants. The alignment tuning process typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al., 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be ""superficial."" This raises questions about how exactly the alignment tuning transforms a base LLM.   We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterparts (e.g., Llama-2 and Llama-2-chat). Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions (i.e., they share the top-ranked tokens). Most distribution shifts occur with stylistic tokens (e.g., discourse markers, safety disclaimers). This direct evidence strongly supports the hypothesis that alignment tuning primarily learns to adopt the language style of AI assistants, and that the knowledge required for answering user queries predominantly comes from the base LLMs themselves.   Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL (Untuned LLMs with Restyled In-context Alignment). URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named just-eval-instruct. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT (Mistral-7b-Instruct) or SFT+RLHF (Llama-2-70b-chat). We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",[],[],"['Bill Yuchen Lin', 'Abhilasha Ravichander', 'Ximing Lu', 'Nouha Dziri', 'Melanie Sclar', 'Khyathi Chandu', 'Chandra Bhagavatula', 'Yejin Choi']","['xAI', 'Computer Science, University of Washington', 'Department of Computer Science, University of Washington', '', 'Facebook', '', 'Allen Institute for Artificial Intelligence', 'Computer Science, Computer Science Department, Stanford University']","['United States', 'United States', 'United States', '', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=lm7MRcsFiS,Security,Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?,"Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming scheme for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents. In essence, Ring-A-Bell could serve as a red-teaming tool to understand the limitations of deployed safety mechanisms and to explore the risk under plausible attacks. Our codes are available at https://github.com/chiayi-hsu/Ring-A-Bell.",[],[],"['Yu-Lin Tsai', 'Chia-Yi Hsu', 'Chulin Xie', 'Chih-Hsun Lin', 'Jia You Chen', 'Bo Li', 'Pin-Yu Chen', 'Chia-Mu Yu', 'Chun-Ying Huang']","['Arete Honors Program, National Yang Ming Chiao Tung University', 'Computer Science , National Yang Ming Chiao Tung University', 'University of Illinois, Urbana Champaign', 'National Chiao Tung University', 'National Chiao Tung University', 'CS, University of Illinois, Urbana Champaign', 'International Business Machines', 'Electronics and Electrical Engineering, National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', '', 'United States']"
https://openreview.net/forum?id=MbfAK4s61A,Security,GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher,"Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time in bypassing the safety alignment of GPT-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that LLMs seem to have a ''secret cipher'', and propose a novel SelfCipher that uses only role play and several unsafe demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases.",[],[],"['Youliang Yuan', 'Wenxiang Jiao', 'Wenxuan Wang', 'Jen-tse Huang', 'Pinjia He', 'Shuming Shi', 'Zhaopeng Tu']","['The Chinese University of Hong Kong-Shenzhen', 'Tencent AI Lab', 'The Chinese University of Hong Kong', 'Johns Hopkins University', 'School of Data Science, The Chinese University of Hong Kong, Shenzhen', 'Tencent AI Lab', 'Tencent AI Lab']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'United States', 'Hong Kong', 'Hong Kong', 'Hong Kong']"
https://openreview.net/forum?id=5iENGLEJKG,Fairness & Bias,INViTE: INterpret and Control Vision-Language Models with Text Explanations,"Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges. We present INViTE: a framework for INterpreting Vision Transformer’s latent tokens with Text Explanations. Given a latent token, INViTE retains its semantic information to the final layer using transformer’s local operations and retrieves the closest text for explanation. INViTE enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, INViTE allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations. Our code is available at https://github.com/tonychenxyz/vit-interpret.",[],[],"['Haozhe Chen', 'Junfeng Yang', 'Carl Vondrick', 'Chengzhi Mao']","['Columbia University', 'Columbia University', 'Computer Science, Columbia University', 'Google']","['United States', 'United States', 'United States', '']"
https://openreview.net/forum?id=H3UayAQWoE,Security,On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs,"Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.",[],[],"['Jen-tse Huang', 'Wenxuan Wang', 'Eric John Li', 'Man Ho LAM', 'Shujie Ren', 'Youliang Yuan', 'Wenxiang Jiao', 'Zhaopeng Tu', 'Michael Lyu']","['Johns Hopkins University', 'The Chinese University of Hong Kong', 'The Chinese University of Hong', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Tianjin Medical University', 'The Chinese University of Hong Kong-Shenzhen', 'Tencent AI Lab', 'Tencent AI Lab', 'Computer Science , The Chinese University of Hong Kong']","['Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', 'Hong Kong', '', '', 'Hong Kong']"
https://openreview.net/forum?id=TzAJbTClAz,Fairness & Bias,FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods,"This paper introduces the Fair Fairness Benchmark (FFB), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is important for ethical compliance. However, there exist challenges in comparing and developing fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from 45,079 experiments, 14,428 GPU hours. We believe that our work will significantly facilitate the growth and development of the fairness research community. The benchmark is available at https://github.com/ahxt/fair_fairness_benchmark.",[],[],"['Xiaotian Han', 'Jianfeng Chi', 'Yu Chen', 'Qifan Wang', 'Han Zhao', 'Na Zou', 'Xia Hu']","['Case Western Reserve University', 'Meta AI', 'Anytime.AI', 'Meta AI', 'University of Illinois, Urbana Champaign', 'Industrial Engineering, University of Houston', 'Computer Science , Rice University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://openreview.net/forum?id=shr9PXz7T0,Fairness & Bias,Large Language Models Are Not Robust Multiple Choice Selectors,"Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option IDs as answers (like “Option A”). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs’ token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model’s prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.",[],[],"['Chujie Zheng', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Minlie Huang']","['Computer Science and Technology, Tsinghua University', 'Tencent, Wechat AI', 'WeChat AI, Tencent Inc.', 'WeChat AI, WeChat AI, Tencent Inc.', 'Tsinghua University, Tsinghua University']","['China', 'China', 'China', 'China', 'China']"
https://openreview.net/forum?id=Djw0XhjHZb,Transparency & Explainability,Simplicial Representation Learning with Neural $k$-Forms,"Geometric deep learning extends deep learning to incorporate information about the geometry and topology data, especially in complex domains like graphs. Despite the popularity of message passing in this field, it has limitations such as the need for graph rewiring, ambiguity in interpreting data, and over-smoothing. In this paper, we take a different approach, focusing on leveraging geometric information from simplicial complexes embedded in $\mathbb{R}^n$ using node coordinates. We use differential $k$-forms in $\mathbb{R}^n$ to create representations of simplices, offering interpretability and geometric consistency without message passing. This approach also enables us to apply differential geometry tools and achieve universal approximation. Our method is efficient, versatile, and applicable to various input complexes, including graphs, simplicial complexes, and cell complexes. It outperforms existing message passing neural networks in harnessing information from geometrical graphs with node features serving as coordinates.",[],[],"['Kelly Maggs', 'Celia Hacker', 'Bastian Rieck']","['Mathematics, EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'University of Fribourg']","['Switzerland', '', 'Switzerland']"
https://openreview.net/forum?id=SA19ijj44B,Fairness & Bias,A Study of Bayesian Neural Network Surrogates for Bayesian Optimization,"Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs, linearized Laplace approximations, and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate models on diverse problems with varying dimensionality, number of objectives, non-stationarity, and discrete and continuous inputs. We find: (i) the ranking of methods is highly problem dependent, suggesting the need for tailored inductive biases; (ii) HMC is the most successful approximate inference procedure for fully stochastic BNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive; (iv) deep ensembles perform relatively poorly; (v) infinite-width BNNs are particularly promising, especially in high dimensions.",[],[],"['Yucen Lily Li', 'Tim G. J. Rudner', 'Andrew Gordon Wilson']","['New York University', 'New York University', 'New York University']","['United States', 'United States', 'United States']"
https://openreview.net/forum?id=vN9fpfqoP1,Fairness & Bias,Fine-Tuned Language Models Generate Stable Inorganic Materials as Text,"We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90\% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned  LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49\% vs 28\%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models' ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained LLMs are surprisingly well-suited for atomistic data.",[],[],"['Nate Gruver', 'Anuroop Sriram', 'Andrea Madotto', 'Andrew Gordon Wilson', 'C. Lawrence Zitnick', 'Zachary Ward Ulissi']","['New York University', 'Facebook', 'FAIR', 'New York University', 'Meta', 'Meta Fundamental AI Research']","['United States', 'Vietnam', '', 'United States', '', 'Mali']"