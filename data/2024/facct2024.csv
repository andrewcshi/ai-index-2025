link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://doi.org/10.1145/3630106.3658537,Transparency & Explainability,Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations,"Decision processes of computer vision models—especially deep neural networks—are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are—as the explainability methods themselves—often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.","['eXplainable AI', 'XAI', 'saliency maps', 'saliency metrics', 'heatmaps', 'quantitative evaluation', 'psychometric testing', 'validity', 'reliability', 'objective XAI evaluation']",Computing methodologies → Interest point and salient region detections General and reference → Metrics Computing methodologies → Computer vision Human-centered computing~Human computer interaction (HCI),"['Benjamin Fresz', 'Lena Lörcher', 'Marco Huber']","['Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658538,Fairness & Bias,Designing Long-term Group Fair Policies in Dynamical Systems,"Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term—even if fairness considerations were taken into account in the policy design process. In this paper, we propose a novel framework for studying long-term group fairness in dynamical systems, in which current decisions may affect an individual’s features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long-term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov Chain Convergence Theorem to ensure unique convergence. Our framework enables the utilization of historical temporal data to tackle challenges associated with delayed feedback when learning long-term fair policies in practice. Importantly, our framework shows that interventions on the data distribution (e.g., subsidies) can be used to achieve policy learning that is both short- and long-term fair. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. In semi-synthetic simulations based on real-world datasets, we show how our approach facilitates identifying effective interventions for long-term fairness.","['fairness', 'long-term', 'dynamical system', 'equilibrium', 'policy learning']",Computing methodologies → Machine learning Social and professional topics,"['Miriam Rateike', 'Isabel Valera', 'Patrick Forré']","['Saarland University, Germany', 'Saarland University, Germany and Max Planck Institute for Software Systems, Germany', 'AI4Science Lab, AMLab, Informatics Institute, University of Amsterdam, Germany']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658539,Fairness & Bias,Learning Fairness from Demonstrations via Inverse Reinforcement Learning,"Defining fairness in algorithmic contexts is challenging, particularly when adapting to new domains. Our research introduces a novel method for learning and applying group fairness preferences across different classification domains, without the need for manual fine-tuning. Utilizing concepts from inverse reinforcement learning (IRL), our approach enables the extraction and application of fairness preferences from human experts or established algorithms. We propose the first technique for using IRL to recover and adapt group fairness preferences to new domains, offering a low-touch solution for implementing fair classifiers in settings where expert-established fairness tradeoffs are not yet defined.",[],[],"['Jack Blandin', 'Ian A. Kash']","['Department of Computer Science, University of Illinois at Chicago, United States of America', 'Department of Computer Science, University of Illinois at Chicago, United States of America']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3658540,Fairness & Bias,Using Property Elicitation to Understand the Impacts of Fairness Regularizers,"Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.",[],[],['Jessie Finocchiaro'],"['Center for Research on Computation and Society, Harvard University, United States of America']",['United States']
https://doi.org/10.1145/3630106.3658543,Fairness & Bias,Data Feminism for AI,"This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.","['feminism', 'data feminism', 'data justice', 'ai ethics', 'responsible ai']","Computing methodologies → Artificial intelligence Human-centered computing → Collaborative and social computing theory, concepts and paradigms Applied computing → Arts and humanities","['Lauren Klein', ""Catherine D'Ignazio""]","['Quantitative Theory & Methods and English, Emory University, United States', 'Department of Urban Studies and Planning, MIT, USA']","['United States', '']"
https://doi.org/10.1145/3630106.3658547,Transparency & Explainability,"Why is ""Problems"" Predictive of Positive Sentiment? A Case Study of Explaining Unintuitive Features in Sentiment Classification","Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions. To this end, many approaches explain which input features are most predictive of a target label. However, such explanations can still be puzzling to users (e.g., in product reviews, the word “problems” is predictive of positive sentiment). If left unexplained, puzzling explanations can have negative impacts. Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research. We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study. We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive. Results from a crowdsourced study (N = 300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification.",[],[],"['Jiaming Qu', 'Jaime Arguello', 'Yue Wang']","['School of Information and Library Science, University of North Carolina at Chapel Hill, United States', 'School of Information and Library Science, University of North Carolina at Chapel Hill, USA', 'School of Information and Library Science, University of North Carolina at Chapel Hill, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658548,Transparency & Explainability,"Regulating AI-Based Remote Biometric Identification. Investigating the Public Demand for Bans, Audits, and Public Database Registrations","AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but at the same time they are criticised for inheriting biases and violating fundamental human rights. As a result, the use of RBI poses risks to society. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a broad consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI for the Common Good. As a possible counterweight, public opinion can have a decisive influence on policymakers (e.g. through voter demands) to establish boundaries and conditions under which AI systems should be used – if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems.","['Regulation', 'Trust', 'Discrimination Perception', 'Survey Research', 'Artificial Intelligence', 'Remote Biometric Identification']",Human-centered computing → Empirical studies in HCI Social and professional topics → Governmental regulations Applied computing → Sociology Applied computing → Law,"['Kimon Kieslich', 'Marco Lünich']","['Institute for Information Law, University of Amsterdam, Netherlands', 'Department of Social Sciences, Heinrich Heine University Düsseldorf, Germany']","['Netherlands', 'Germany']"
https://doi.org/10.1145/3630106.3658899,Fairness & Bias,Algorithmic Pluralism: A Structural Approach To Equal Opportunity,"We present a structural approach toward achieving equal opportunity in systems of algorithmic decision-making called algorithmic pluralism. Algorithmic pluralism describes a state of affairs in which no set of algorithms severely limits access to opportunity, allowing individuals the freedom to pursue a diverse range of life paths. To argue for algorithmic pluralism, we adopt Joseph Fishkin’s theory of bottlenecks, which focuses on the structure of decision-points that determine how opportunities are allocated. The theory contends that each decision-point or “bottleneck’’ limits access to opportunities with some degree of severity and legitimacy. We extend Fishkin’s structural viewpoint and use it to reframe existing systemic concerns about equal opportunity in algorithmic decision-making, such as patterned inequality and algorithmic monoculture. In proposing algorithmic pluralism, we argue for the urgent priority of alleviating severe bottlenecks in algorithmic-decision-making. We contend that there must be a pluralism of opportunity available to many different individuals in order to promote equal opportunity in a systemic way. We further show how this framework has several implications for system design and regulation through current debates about equal opportunity in algorithmic hiring.","['algorithmic fairness', 'bottlenecks', 'equal opportunity', 'homogenization', 'algorithmic monoculture', 'structural injustice']",Computing methodologies → Philosophical/theoretical foundations of artificial intelligence,"['Shomik Jain', 'Vinith Suriyakumar', 'Kathleen Creel', 'Ashia Wilson']","['Institute for Data, Systems, and Society, MIT, United States of America', 'Department of Electrical Engineering and Computer Science, MIT, United States of America', 'Department of Philosophy and Religion and Khoury College of Computer Sciences, Northeastern University, USA', 'Department of Electrical Engineering and Computer Science, MIT, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658902,Fairness & Bias,"Ethnic Classifications in Algorithmic Fairness: Concepts, Measures and Implications in Practice","We address the challenges and implications of ensuring fairness in algorithmic decision-making (ADM) practices related to ethnicity. Expanding beyond the U.S.-centric approach to race, we provide an overview of ethnic classification schemes in European countries and emphasize how the distinct approaches to ethnicity in Europe can impact fairness assessments in ADM. Drawing on large-scale German survey data, we highlight differences in ethnic disadvantage across subpopulations defined by different measures of ethnicity. We build prediction models in the labor market, health, and finance domain and investigate the fairness implications of different ethnic classification schemes across multiple prediction tasks and fairness metrics. Our results show considerable variation in fairness scores across ethnic classifications, where error disparities for the same model can be twice as large when using different operationalizations of ethnicity. We argue that ethnic classifications differ in their ability to identify ethnic disadvantage across ADM domains and advocate for context-sensitive operationalizations of ethnicity and its transparent reporting in fair machine learning (ML) applications.","['Ethnic Classifications', 'Fairness Evaluation', 'Protected Attributes', 'Ethnicity']","Security and privacy → Human and societal aspects of security and privacy Computing methodologies → Machine learning Applied computing → Law, social and behavioral sciences","['Sofia Jaime', 'Christoph Kern']","['University of California, USA', 'LMU Munich, Germany and Munich Center for Machine Learning (MCML), Germany and University of Mannheim, Germany']","['United States', 'Germany']"
https://doi.org/10.1145/3630106.3658903,Fairness & Bias,Algorithmic Reproductive Justice,"Reproductive justice is an intersectional feminist framework and movement which argues all people have the right to have a child, to not have a child, to parent in safe and healthy environments, and to own their bodies and control their futures. We identify increasing surveillance, assessing worth, datafication and monetisation, and decimating planetary health as forms of structural violence associated with emerging digital technologies. These trends are implicated in the (re)production of inequities, creating barriers to the realisation of reproductive justice. We call for algorithmic reproductive justice, and highlight the potential for both acts of resistance and industry reform to advance that aim.","['Artificial intelligence', 'AI', 'fairness', 'human rights', 'reproductive rights', 'reproductive justice', 'social justice', 'reproductive coercion', 'eugenics', 'algorithmic violence', 'structural violence']",Social and professional topics → Surveillance Security and privacy → Social aspects of security and privacy Social and professional topics → User characteristics,"['Jasmine Fledderjohann', 'Bran Knowles', 'Esmorie Miller']","['Lancaster University, United Kingdom', 'Lancaster University, United Kingdom', 'Lancaster University, United Kingdom']","['Ghana', 'Ghana', 'Ghana']"
https://doi.org/10.1145/3630106.3658904,Fairness & Bias,"""Like rearranging deck chairs on the Titanic""? Feasibility, Fairness, and Ethical Concerns of a Citizen Carbon Budget for Reducing CO2 Emissions","Radical and disruptive interventions are needed to reach ""Net Zero"" by 2050 to avert the climate catastrophe. Although governments, companies, cities, and institutions have pledged to take action and reduce their carbon emissions, the idea of personal carbon allowances or budgets for individuals has also been proposed as a potential national policy in the UK. In this paper, we employ a Research through Design approach to explore the notion of a carbon budget. We present combined results from two studies: firstly a workshop with members of environmental organisations (industry, charity, and policymaking) discussing the concept of a Citizen Carbon Budget (CCB) and app, from the wide perspective of societal desirability drawn from Responsible Research and Innovation (RRI); and secondly, a one-month deployment of a CCB mobile app with twelve members of the public based in the UK. Key findings from the combination of these approaches showed that the CCB app was fruitful in supporting awareness of personal carbon emissions and reflections about people’s lifestyles. However, several concerns were raised, including the unfairness of treating all people equally in environmental policy, regardless of their background and context. We provide considerations for policymaking and design, including intertwined perspectives drawn from the differing approaches of individual and collective action.","['carbon emissions', 'carbon footprint', 'sustainability', 'ethics', 'personal tracking', 'policy', 'research through design', 'HCI', 'responsible research and innovation']",Human-centered computing → Empirical studies in HCI,"['Gisela Reyes-Cruz', 'Peter Craigon', 'Anna-Maria Piskopani', 'Liz Dowthwaite', 'Yang Lu', 'Justyna Lisinska', 'Elnaz Shafipour', 'Sebastian Stein', 'Joel Fischer']","['University of Nottingham, UK, United Kingdom', 'University of Nottingham, UK, United Kingdom', 'University of Nottingham, UK, United Kingdom', 'University of Nottingham, UK, United Kingdom', 'York St John University, UK, United Kingdom', ""King's College London, UK, United Kingdom"", 'University of Southampton, UK, United Kingdom', 'University of Southampton, UK, United Kingdom', 'University of Nottingham, UK, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3630106.3658905,Transparency & Explainability,A preprocessing Shapley value-based approach to detect relevant and disparity prone features in machine learning,"Decision support systems became ubiquitous in every aspect of human lives. Their reliance on increasingly complex and opaque machine learning models raises transparency and fairness concerns with respect to unprivileged groups of people. This motivated several efforts to estimate importance of features towards the models’ performance and to detect unfair/disparate decisions. The latter is often dealt with by means of fairness metrics that rely on performance metrics with respect to predefined features that are considered protected (salient features such as age, gender, ethnicity, etc.) and/or sensitive (such as education, /occupation, banking information). However, such an approach is subjective (as fairness metrics depend on the choice features), there may be other features that lead to unfair (disparate) decisions and that may ask for suitable interpretations.",[],[],"['Guilherme Dean Pelegrina', 'Miguel Couceiro', 'Leonardo Tomazeli Duarte']","['School of Applied Sciences, University of Campinas, Brazil and Mackenzie Presbyterian University, Brazil', 'CNRS, LORIA, Université de Lorraine, France and INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal', 'School of Applied Sciences, University of Campinas, Brazil']","['Brazil', 'Portugal', 'Italy']"
https://doi.org/10.1145/3630106.3658906,Transparency & Explainability,"Failing Our Youngest: On the Biases, Pitfalls, and Risks in a Decision Support Algorithm Used for Child Protection","In recent years, Danish child protective services have experienced increasing pressure, prompting the adoption of a decision-support algorithm to aid caseworkers in identifying children at heightened risk of maltreatment, named Decision Support. Despite its critical role, this algorithm has not undergone formal evaluation. Through a freedom of information request, we were able to partially access the algorithm and conduct an audit. We find that the algorithm has significant methodological flaws, suffers from information leakage, relies on inappropriate proxy values for maltreatment assessment, generates inconsistent risk scores, and exhibits age-based discrimination. Given these serious issues, we strongly advise against the use of this kind of algorithms in local government, municipal, and child protection settings, and we call for rigorous evaluation of such tools before implementation and for continual monitoring post-deployment by listing a series of specific recommendations.","['Algorithmic audit', 'technological fairness', 'algorithmic decision making', 'algorithmic discrimination']",Applied computing Social and professional topics → Technology audits,"['Therese Moreau', 'Roberta Sinatra', 'Vedran Sekara']","['NEtwoRks, Data, and Society (NERDS), IT University of Copenhagen, Denmark', 'Center for Social Data Science (SODAS), University of Copenhagen, Denmark and NEtwoRks, Data, and Society (NERDS), IT University of Copenhagen, Denmark and ISI Foundation, Italy', 'NEtwoRks, Data, and Society (NERDS), IT University of Copenhagen, Denmark and Pioneer Centre for AI (P1), Denmark']","['Denmark', 'Denmark', 'Denmark']"
https://doi.org/10.1145/3630106.3658911,Security,Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators,"The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens’ homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.","['Generative AI', 'Multimodal', 'Harms', 'Taxonomy', 'Speech Generation', 'Speech Synthesis', 'Voice Cloning', 'Deepfakes']",Computing methodologies → Artificial intelligence Social and professional topics → Computing / technology policy General and reference → Evaluation,"['Wiebke Hutiri', 'Orestis Papakyriakopoulos', 'Alice Xiang']","['Sony AI, Switzerland', 'Sony AI, Switzerland and Technical University of Munich, Germany', 'Sony AI, USA']","['Switzerland', 'Germany', 'Japan']"
https://doi.org/10.1145/3630106.3658912,Transparency & Explainability,Operationalizing the Search for Less Discriminatory Alternatives in Fair Lending,"The Less Discriminatory Alternative is a key provision of the disparate impact doctrine in the United States. In fair lending, this provision mandates that lenders must adopt models that reduce discrimination when they do not compromise their business interests. In this paper, we develop practical methods to audit for less discriminatory alternatives. Our approach is designed to verify the existence of less discriminatory machine learning models – by returning an alternative model that can reduce discrimination without compromising performance (discovery) or by certifying that an alternative model does not exist (refutation). We develop a method to fit the least discriminatory linear classification model in a specific lending task – by minimizing an exact measure of disparity (e.g., the maximum gap in group FNR) and enforcing hard performance constraints for business necessity (e.g., on FNR and FPR). We apply our method to study the prevalence of less discriminatory alternatives on real-world datasets from consumer finance applications. Our results highlight how models may inadvertently lead to unnecessary discrimination across common deployment regimes, and demonstrate how our approach can support lenders, regulators, and plaintiffs by reliably detecting less discriminatory alternatives in such instances.",[],[],"['Talia B Gillis', 'Vitaly Meursault', 'Berk Ustun']","['Columbia University, USA', 'Federal Reserve Bank of Philadelphia Research Department, United States', 'Halicioglu Data Science Institute, UCSD, United States']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658913,Transparency & Explainability,Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation,"With text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on “implicitly adversarial” prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. We present an in-depth account of our methodology, a systematic study of novel attack strategies and safety failures, and a visualization tool for easy exploration of the dataset. The first challenge round resulted in over 10k prompt-image pairs with machine annotations for safety. A subset of 1.5k samples contains rich human annotations of harm types and attack styles. Our findings emphasize the necessity of continual auditing and adaptation as new vulnerabilities emerge. This work will enable proactive, iterative safety assessments and promote responsible development of T2I models.","['Red teaming', 'Data-centric AI', 'Text-to-image', 'Adversarial Testing', 'Crowdsourcing']",Human-centered computing → Interaction techniques Human-centered computing → Interaction paradigms Social and professional topics → User characteristics Human-centered computing → Visual analytics,"['Jessica Quaye', 'Alicia Parrish', 'Oana Inel', 'Charvi Rastogi', 'Hannah Rose Kirk', 'Minsuk Kahng', 'Erin Van Liemt', 'Max Bartolo', 'Jess Tsang', 'Justin White', 'Nathan Clement', 'Rafael Mosquera', 'Juan Ciro', 'Vijay Janapa Reddi', 'Lora Aroyo']","['Harvard University, USA', 'Google, United States of America', 'University of Zürich, Switzerland', 'Google, USA', 'University of Oxford, United Kingdom', 'Google, USA', 'Google, USA', 'University College London, Cohere, United Kingdom', 'Google, USA', 'Google, USA', 'Google, United Kingdom', 'ML Commons, Colombia', 'ML Commons, Colombia', 'Harvard University, United States of America', 'Google, USA']","['United States', 'Switzerland', 'United States', 'Japan', 'United Kingdom', 'Japan', 'Japan', 'United States', 'Japan', 'Japan', 'United Kingdom', 'Colombia', 'Colombia', 'United States', 'Japan']"
https://doi.org/10.1145/3630106.3658914,Fairness & Bias,Insights From Insurance for Fair Machine Learning,"We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.",[],[],"['Christian Fröhlich', 'Robert C. Williamson']","['University of Tübingen, Germany', 'University of Tübingen, Germany']","['Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658915,Security,No Simple Fix: How AI Harms Reflect Power and Jurisdiction in the Workplace,"The introduction of AI into working processes has resulted in workers increasingly being subject to AI-related harms. By analyzing incidents of worker-related AI harms between 2008 and 2023 in the AI Incident Database, we find that harms get addressed under considerably restricted scenarios. Results from a Qualitative Comparative Analysis (QCA) show that workers with more power resources, either in the form of expertise or labor market power, have a greater likelihood of seeing harms fixed, all else equal. By contrast, workers lacking expertise or labor market power, have lower success rates and must resort to legal or regulatory mechanisms to get fixes through. These findings suggest that the workplace is another arena in which AI has the potential to reproduce existing inequalities among workers and that stronger legal frameworks and regulations can empower more vulnerable worker populations.","['Social and professional topics', 'Socio-technical systems', 'artificial intelligence', 'harms', 'work', 'expertise', 'algorithmic management', 'governance', 'regulation', 'safety']",,"['Nataliya Nedzhvetskaya', 'JS Tan']","['University of California, Berkeley, United States of America', 'Massachusetts Institute of Technology, USA']","['', 'United States']"
https://doi.org/10.1145/3630106.3658916,Transparency & Explainability,Algorithmic Misjudgement in Google Search Results: Evidence from Auditing the US Online Electoral Information Environment,"Google Search is an important way that people seek information about politics [8], and Google states that it is “committed to providing timely and authoritative information on Google Search to help voters understand, navigate, and participate in democratic processes.”1 This paper studies the extent to which government-maintained web domains are represented in the online electoral information environment, as captured through 3.45 Google Search result pages collected during the 2022 US midterm elections for 786 locations across the United States. Focusing on state, county, and local government domains that provide locality-specific information, we study not only the extent to which these sources appear in organic search results, but also the extent to which these sources are correctly targeted to their respective constituents. We label misalignment between the geographic area that non-federal domains serve and the locations for which they appear in search results as algorithmic mistargeting, a subtype of algorithmic misjudgement in which the search algorithm targets locality-specific information to users in different (incorrect) locations. In the context of the 2022 US midterm elections, we find that 71% of all occurrences of state, county, and local government sources were mistargeted, with some domains appearing disproportionately often among organic results despite providing locality-specific information that may not be relevant to all voters. However, we also find that mistargeting often occurs in low ranks. We conclude by considering the potential consequences of extensive mistargeting of non-federal government sources and argue that ensuring the correct targeting of these sources to their respective constituents is a critical part of Google’s role in facilitating access to authoritative and locally-relevant electoral information.","['algorithm auditing', 'problematic behavior', 'google search audit', 'elections', 'algorithmic misjudgement']",Information systems → Web search engines Information systems → Personalization,"['Brooke Perreault', 'Johanna Hoonsun Lee', 'Ropafadzo Shava', 'Eni Mustafaraj']","['Wellesley College, United States of America', 'Wellesley College, USA', 'Wellesley College, USA', 'Wellesley College, United States of America']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658917,Fairness & Bias,To See or Not to See: Understanding the Tensions of Algorithmic Curation for Visual Arts,"Algorithmic recommendation is one of the most popular applications of machine learning (ML) systems. While the implication of algorithmic recommendation has been studied in the context of high-stakes domains such as finance and healthcare, there has been very little focus in understanding its impacts with respect to the arts domain. Given that ML is increasingly finding place in the arts domain such as in generative arts and content analysis, in this paper, we examine the tensions of algorithmic curation in the context of visual arts. Through case studies, we describe how curatorial algorithms that are oblivious of broader socio-cultural contexts could potentially result in ethical concerns such as over-representation and misattribution, to name a few. Towards addressing some of these concerns, the paper offers design guidelines. Specifically, the paper outlines repair strategies that suggest ways 1) to engage with cultural stakeholders in building visual art curatorial algorithms, 2) to unlearn biases embedded in digital artworks and their meta-data, and 3) emphasize the need to establish regulatory norms specific to the use of ML in visual art curation. Taking cue from the process employed by artwork curators, the paper also describes how authenticity can be prioritized by re-calibrating visual art curatorial algorithms. The paper also suggest ways through which the potential of state-of-the-art ML curatorial algorithms can be re-imagined towards empowering the audience of artworks. We hope the insights presented in the paper spark interdisciplinary discussions and pave way for fostering reformation in algorithmic curation of visual arts.","['algorithmic recommendation', 'visual arts', 'case studies', 'machine learning', 'curation']",Computing methodologies → Machine learning algorithms Human-centered computing → Human computer interaction (HCI),['Ramya Srinivasan'],"['Fujitsu Research of America, United States of America']",['United States']
https://doi.org/10.1145/3630106.3658918,Transparency & Explainability,In the Walled Garden: Challenges and Opportunities for Research on the Practices of the AI Tech Industry,"Research on technology companies and their workers can externalize otherwise invisible and tacit workplace approaches, identify organizational constraints to creating more ethical AI systems, help ground interventions in real-world organizational realities, and result in the co-creation of better business practices for organizations. However, getting access to technology companies is difficult for external researchers. In this paper, I draw from insights gained by conducting research on and with industry professionals. I present four challenges when conducting industry-focused research on responsible AI. I also present methods I used to navigate each challenge. Finally, I highlight opportunities for the tech industry to lower the barriers to external research. This work aims to share ways of navigating methodological challenges and encourage better transparency in the tech industry.","['Human subjects research', 'methods', 'big tech', 'tech industry', 'transparency', 'work studies', 'AI ethics']",Social and professional topics → Computing industry Human-centered computing → Empirical studies in collaborative and social computing Computing methodologies~Artificial intelligence,['Morgan Klaus Scheuerman'],"['University of Colorado Boulder, United States of America']",['United States']
https://doi.org/10.1145/3630106.3658920,Fairness & Bias,Overriding (in)justice: pretrial risk assessment administration on the frontlines,"A small but growing number of empirical studies have attempted to measure the impacts of algorithmic pretrial risk assessments on discrete policy goals such as decarceration, racial equity, and public safety. A separate but related body of work explores frontline worker resistance and discretion related to sociotechnical systems in criminal legal contexts. I build on work that aims to bridge the gaps between these literatures by offering an ethnographic account of pretrial risk assessment administration across the United States. I draw on semi-structured interviews with 74 pretrial actors and site observations across 8 jurisdictions. I highlight the process of risk assessment administration and the frontline workers who perform that labor. Like judges, pretrial officers have the autonomy to override risk assessment recommendations, unlike judges however, their decisions are made outside the courtroom and far removed from public scrutiny. This paper makes three contributions. First, it provides a detailed account of the personal, professional, and organizational dynamics that lead pretrial officers to override risk assessment recommendations. Second, it presents a taxonomy of override behavior among pretrial officers in an effort to promote more effective policy decisions. Lastly, it provides further empirical evidence that pretrial risk assessments are unlikely to guarantee racial or economic equity or decarceration in the long term.",[],[],['Sarah Riley'],"['Stanford University, United States of America']",['United States']
https://doi.org/10.1145/3630106.3658921,Fairness & Bias,Benchmarking the Fairness of Image Upsampling Methods,"Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics—inspired by their supervised fairness counterparts—to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results. All experiments can be reproduced using our provided repository.1","['Conditional Generative Models', 'Computer Vision', 'Image Upsampling', 'Fairness']",Computing methodologies → Reconstruction,"['Mike Laszkiewicz', 'Imant Daunhawer', 'Julia E. Vogt', 'Asja Fischer', 'Johannes Lederer']","['Faculty of Computer Science, Ruhr University Bochum, Germany', 'Department of Computer Science, ETH Zurich, Switzerland', 'Department of Computer Science, ETH Zurich, Switzerland', 'Faculty of Computer Science, Ruhr University Bochum, Germany', 'Department of Mathematics, Computer Science, and Natural Sciences, University of Hamburg, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658922,Fairness & Bias,Analyzing the Relationship Between Difference and Ratio-Based Fairness Metrics,"In research studying the fairness of machine learning algorithms and models, fairness often means that a metric is the same when computed for two different groups of people. For example, one might define fairness to mean that the false positive rate of a classifier is the same for people of different genders, ages, or races. However, it is usually not possible to make this metric identical for all groups. Instead, algorithms ensure that the metric is similar—for example, that the false positive rates are similar. Researchers usually measure this similarity or dissimilarity using either the difference or ratio between the metric values for different groups of people. Although these two approaches are known to be different, there has been little work analyzing their differences and respective benefits. In this paper we examine this relationship analytically and empirically, and conclude that unless there are application-specific reasons to prefer the difference approach, the ratio approach should be preferred.","['Fair Machine Learning', 'Bias', 'Fairness Metrics', 'Classification']",Computing methodologies → Machine learning General and reference → Metrics,"['Min-Hsuan Yeh', 'Blossom Metevier', 'Austin Hoag', 'Philip Thomas']","['University of Massachusetts, United States', 'University of Massachusetts, USA', 'Berkeley Existential Risk Initiative, USA', 'University of Massachusetts, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658923,Fairness & Bias,Diversified Ensembling: An Experiment in Crowdsourced Machine Learning,"Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In [12], the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants’ efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams’ approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.",[],[],"['Ira Globus-Harris', 'Declan Harrison', 'Michael Kearns', 'Pietro Perona', 'Aaron Roth']","['Computer and Information Sciences, University of Pennsylvania, USA and Amazon Web Services Artificial Intelligence (AWS AI), USA', 'Computer and Information Sciences, University of Pennsylvania, USA and Amazon Web Services Artificial Intelligence (AWS AI), USA', 'Computer and Information Sciences, University of Pennsylvania, USA and Amazon Web Services Artificial Intelligence (AWS AI), USA', 'California Institute of Technology, USA and Amazon Web Services Artificial Intelligence (AWS AI), USA', 'Computer and Information Sciences, University of Pennsylvania, Amazon AWS AI, USA and Amazon Web Services Artificial Intelligence (AWS AI), USA']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658925,Fairness & Bias,Tackling Language Modelling Bias in Support of Linguistic Diversity,"Current AI-based language technologies—language models, machine translation systems, multilingual dictionaries and corpora—are known to focus on the world’s 2–3% most widely spoken languages. Research efforts of the past decade have attempted to expand this coverage to ‘under-resourced languages.’ The goal of our paper is to bring attention to a corollary phenomenon that we call language modelling bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. We define language modelling bias as uneven per-language performance under similar test conditions. We show that bias stems not only from technology but also from ethically problematic research and development methodologies that disregard the needs of language communities. Moving towards diversity-aware alternatives, we present an initiative that aims at reducing language modelling bias within lexical resources through both technology design and methodology, based on an eye-level collaboration with local communities.",[],[],"['Gábor Bella', 'Paula Helm', 'Gertraud Koch', 'Fausto Giunchiglia']","['Lab-STICC, CNRS UMR 6285, IMT Atlantique, France', 'Faculty of Humanities, University of Amsterdam, Netherlands', 'Institute of Anthropological Studies on Culture and History, University of Hamburg, Germany', 'Department of Information Engineering and Computer Science, University of Trento, Italy']","['France', 'Netherlands', '', 'Italy']"
https://doi.org/10.1145/3630106.3658927,Fairness & Bias,Towards Geographic Inclusion in the Evaluation of Text-to-Image Models,"Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases. In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling. However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures. In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs. We collect over 65,000 image annotations and 20 survey responses. We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity. For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative. In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of “appeal” captured in reference datasets used to ground evaluations. We recommend steps for improved automatic and human evaluations. This includes collecting annotations from people located inside and outside the region of interest, instructing annotators on whether they should follow specific definitions of evaluation criteria or utilize their own interpretation, and reporting assumptions underlying automatic evaluations.","['text-to-image generation', 'geography', 'evaluation']",Social and professional topics Applied computing Computing methodologies → Computer vision,"['Melissa Hall', 'Samuel J. Bell', 'Candace Ross', 'Adina Williams', 'Michal Drozdzal', 'Adriana Romero Soriano']","['Meta (FAIR), United States', 'Meta (FAIR), France', 'Meta (FAIR), USA', 'Meta (FAIR), USA', 'Meta (FAIR), Canada', 'Meta (FAIR), Canada and Mila, Canada and McGill University, Canada']","['United States', 'France', 'Japan', 'Japan', 'Canada', 'Canada']"
https://doi.org/10.1145/3630106.3658928,Transparency & Explainability,D-hacking,"Recent regulatory efforts, including Executive Order 14110 and the AI Bill of Rights, have focused on mitigating discrimination in AI systems through novel and traditional application of anti-discrimination laws. While these initiatives rightly emphasize fairness testing and mitigation, we argue that they pay insufficient attention to robust bias measurement and mitigation—and that without doing so, the frameworks cannot effectively achieve the goal of reducing discrimination in deployed AI models. This oversight is particularly concerning given the instability and brittleness of current algorithmic bias mitigation and fairness optimization methods, as highlighted by growing evidence in the algorithmic fairness literature. This instability heightens the risk of what we term discrimination-hacking or d-hacking, a scenario where, inadvertently or deliberately, the selection of models based on favorable fairness metrics within specific samples could lead to misleading or non-generalizable fairness performance. We term this effect d-hacking because systematically selecting among numerous models to find the least discriminatory one parallels the concept of p-hacking in social science research of selectively reporting outcomes that appear statistically significant resulting in misleading conclusions. In light of these challenges, we argue that AI fairness regulation should not only call for fairness measurement and bias mitigation, but also specify methods to ensure robust solutions to discrimination in AI systems. Towards the goal of arguing for robust fairness assessment and bias mitigation in AI regulation, this paper (1) synthesizes evidence of d-hacking in the computer science literature and provides experimental demonstrations of d-hacking, (2) analyzes current legal frameworks to understand the treatment of robust fairness and non-discriminatory behavior, both in recent AI regulation proposals and traditional U.S. discrimination law, and (3) outlines policy recommendations for preventing d-hacking in high-stakes domains.","['algorithmic fairness', 'machine learning', 'AI regulation', 'disparate impact', 'algorithmic discrimination', 'distribution shift', 'artificial intelligence auditing', 'anti-discrimination law']",Social and professional topics → Government technology policy Computing methodologies → Machine learning Applied computing → Law General and reference → Evaluation General and reference → Measurement General and reference → Reliability,"['Emily Black', 'Talia Gillis', 'Zara Yasmine Hall']","['Barnard College, USA', 'Columbia University, USA', 'Columbia University, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658929,Fairness & Bias,Algorithmic Fairness in Performative Policy Learning: Escaping the Impossibility of Group Fairness,"In many prediction problems, the predictive model affects the distribution of the prediction target. This phenomenon is known as performativity and is often caused by the behavior of individuals with vested interests in the outcome of the predictive model. Although performativity is generally problematic because it manifests as distribution shifts, we develop algorithmic fairness practices that leverage performativity to achieve stronger group fairness guarantees in social classification problems (compared to what is achievable in non-performative settings). In particular, we leverage the policymaker’s ability to steer the population to remedy inequities in the long term. A crucial benefit of this approach is that it is possible to resolve the incompatibilities between conflicting group fairness definitions.",[],[],"['Seamus Somerstep', ""Ya'acov Ritov"", 'Yuekai Sun']","['Department of Statistics, University of Michigan, United States of America', 'Department of Statistics, University of Michigan, USA', 'Department of Statistics, University of Michigan, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658930,Fairness & Bias,Data Agency Theory: A Precise Theory of Justice for AI Applications,"Data collection methods for AI applications have been heavily scrutinized by researchers, policymakers, and the general public. In this paper, we propose data agency theory (DAT), a precise theory of justice to evaluate and improve current consent procedures used in AI applications. We argue that data agency is systematically defined by consent policies. Therefore, data agency is a matter of justice. DAT claims data agency ought to be afforded in a way that minimizes the oppression of data contributors by data collectors. We then apply DAT to two salient consent procedures in AI applications: Reddit’s Terms of Service agreement and the United States’s IRB protocols. Through these cases, we demonstrate how our theory helps evaluate justice and generate ideas for improvement. Finally, we discuss the implications of using justice as an evaluation metric, comparing consent procedures, and adopting DAT in future research.",[],[],"['Leah Ajmani', 'Logan Stapleton', 'Mo Houtti', 'Stevie Chancellor']","['Computer Science & Engineering, University of Minnesota, United States of America', 'Computer Science & Engineering, University of Minnesota, Uniter States of America', 'Computer Science & Engineering, University of Minnesota, USA', 'Computer Science & Engineering, University of Minnesota, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658931,Fairness & Bias,Lazy Data Practices Harm Fairness Research,"Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications.","['critical data studies', 'protected groups', 'fair ML generalization', 'reproducibility']",Social and professional topics → User characteristics Computing methodologies → Machine learning,"['Jan Simson', 'Alessandro Fabris', 'Christoph Kern']","['Institute of Statistics, LMU Munich, Germany and Munich Center for Machine Learning (MCML), Germany', 'Max Planck Institute for Security and Privacy, Germany', 'Institute of Statistics, LMU Munich, Germany and Munich Center for Machine Learning (MCML), Germany']","['Germany', '', 'Germany']"
https://doi.org/10.1145/3630106.3658932,Transparency & Explainability,Auditing GPT's Content Moderation Guardrails: Can ChatGPT Write Your Favorite TV Show?,"Large language models (LLMs) are increasingly appearing in consumer-facing products. To prevent problematic use, the organizations behind these systems have put content moderation guardrails in place that prevent the models from generating content they consider harmful. However, most of these enforcement standards and processes are opaque. Although they play a major role in the user experience of these tools, automated content moderation tools have received relatively less attention than other aspects of the models. This study undertakes an algorithm audit of OpenAI’s ChatGPT with the goal of better understanding its content moderation guardrails and their potential biases. To evaluate performance on a broad cultural range of content, we generate a dataset of 100 popular United States television shows with one to three synopses for each episode in the first season of each show (3,309 total synopses). We probe GPT’s content moderation endpoint (ME) to identify violating content both in the synopses themselves, and in GPT’s own outputs when asked to generate a script based on each synopsis, also comparing with ME outputs on 81 real scripts from the same TV shows (269,578 total ME outputs). Our findings show that a large number of GPT-generated and real scripts flag as content violations (about 18% of GPT scripts and 69% of real ones). Using metadata, we find that TV maturity ratings, as well as certain genres (Animation, Crime, Fantasy, and others) are statistically significantly related to a script’s likelihood of flagging. We conclude by discussing the implications of LLM self-censorship and directions for future research on their moderation procedures.","['AI system audit', 'content moderation', 'text generation']",Human-centered computing → Empirical studies in HCI Computing methodologies → Natural language generation Applied computing → Arts and humanities,"['Yaaseen Mahomed', 'Charlie M. Crawford', 'Sanjana Gautam', 'Sorelle A. Friedler', 'Danaë Metaxa']","['University of Pennsylvania, USA', 'Haverford College, USA', 'Pennsylvania State University, USA', 'Haverford College, United States of America', 'University of Pennsylvania, United States of America']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658933,Transparency & Explainability,Identifying and Improving Disability Bias in GPT-Based Resume Screening,"As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability-related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.","['Resume Audit', 'Bias', 'Ableism', 'GPT']",Social and professional topics → People with disabilities Social and professional topics → Employment issues Computing methodologies → Artificial intelligence,"['Kate Glazko', 'Yusuf Mohammed', 'Ben Kosa', 'Venkatesh Potluri', 'Jennifer Mankoff']","['University of Washington, United States of America', 'University of Washington, USA', 'University of Washington, USA', 'University of Washington, USA', 'University of Washington, United States of America']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658934,Fairness & Bias,The Digital Faces of Oppression and Domination: A Relational and Egalitarian Perspective on the Data-driven Society and its Regulation,"Drawing from Iris Marion Young's politics of difference and democratic theory, this contribution formulates a relational and egalitarian account of digital justice to understand and help counter, the social and technical conditions under which data-driven decision-making systems are liable to reinforce and introduce social injustice. To do so, this contribution is structured alongside three axes. First, I present data-driven decision-making systems as socio-technical systems that both take meaning from and co-shape people's relationships and the social structures they are part of. Due to this relational push and pull, I argue, data-driven systems have the potential to restructure society and, consequently, the conditions that govern people's exposure to, and experience of, injustice therein. Second, I transpose Young's ideation of oppression and domination onto the digital ecosystem. Both notions are used to locate within complex, dynamic and automated environments, a series of social and technological conditions that unjustifiably limit people's actions and behaviours. Third, I build on Young's model for an inclusive democracy to propose a series of institutional and procedural practices to ensure that, within the digital ecosystem, each person has the effective opportunity to pursue the life projects they value and to communicate their needs, concerns and experiences in ways that are heard and recognized by others.","['Digital Justice', 'Structural Injustice', 'Algorithmic Fairness', 'Political Philosophy', 'Relational Equality', 'Democratic Theory', 'Inequality Oppression', 'Domination']","Computing methodologies → Philosophical/theoretical foundations of artificial intelligence Applied computing → Law, social and behavioral sciences",['Laurens Naudts'],"['AI, Media & Democracy Lab - Institute for Information Law, University of Amsterdam, Netherlands and Centre for IT & IP Law, KU Leuven, Belgium']",['Netherlands']
https://doi.org/10.1145/3630106.3658936,Transparency & Explainability,"Silencing the Risk, Not the Whistle: A Semi-automated Text Sanitization Tool for Mitigating the Risk of Whistleblower Re-Identification","Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors. However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously. The specific content of their disclosures and their distinct writing style may re-identify them as the source. Legal measures, such as the EU Whistleblower Directive, are limited in their scope and effectiveness. Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward. However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity. They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other labels of named entities) and combinations thereof with placeholders. Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including the whistleblower’s writing style. Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility. Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text. We then use a Large Language Model (LLM) that we fine-tuned for paraphrasing to render this text coherent and style-neutral. We evaluate our tool’s effectiveness using court cases from the European Court of Human Rights (ECHR) and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution attacks and utility loss statistically using the popular IMDb62 movie reviews dataset, which consists of 62 individuals. Our method can significantly reduce authorship attribution accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content’s semantics, as measured by the established cosine similarity of sentence embeddings.","['Text Sanitization', 'Whistleblower Anonymity', 'Authorship Obfuscation', 'Fine-tuning Language Models', 'LLM-based Rephrasing']","Information systems → Retrieval tasks and goals Security and privacy → Privacy protections Information systems → Web applications Information systems → Data mining Human-centered computing → Collaborative and social computing systems and tools Applied computing → Law, social and behavioral sciences Security and privacy → Pseudonymity, anonymity and untraceability Computing methodologies → Natural language processing Security and privacy → Human and societal aspects of security and privacy","['Dimitri Staufer', 'Frank Pallas', 'Bettina Berendt']","['TU Berlin, Germany', 'Paris Lodron University Salzburg, Austria', 'TU Berlin, Germany and Weizenbaum Institute, Germany and KU Leuven, Belgium']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658937,Fairness & Bias,Gender Bias Detection in Court Decisions: A Brazilian Case Study,"Data derived from the realm of the social sciences is often produced in digital text form, which motivates its use as a source for natural language processing methods. Researchers and practitioners have developed and relied on artificial intelligence techniques to collect, process, and analyze documents in the legal field, especially for tasks such as text summarization and classification. While increasing procedural efficiency is often the primary motivation behind natural language processing in the field, several works have proposed solutions for human rights-related issues, such as assessment of public policy and institutional social settings. One such issue is the presence of gender biases in court decisions, which has been largely studied in social sciences fields; biased institutional responses to gender-based violence are a violation of international human rights dispositions since they prevent gender minorities from accessing rights and hamper their dignity. Natural language processing-based approaches can help detect these biases on a larger scale. Still, the development and use of such tools require researchers and practitioners to be mindful of legal and ethical aspects concerning data sharing and use, reproducibility, domain expertise, and value-charged choices. In this work, we (a) present an experimental framework developed to automatically detect gender biases in court decisions issued in Brazilian Portuguese and (b) describe and elaborate on features we identify to be critical in such a technology, given its proposed use as a support tool for research and assessment of court activity.","['gender bias', 'natural language processing', 'social computing', 'legal text']",Applied computing → Law Computing methodologies → Information extraction Computing methodologies → Supervised learning by classification,"['Raysa Benatti', 'Fabiana Severi', 'Sandra Avila', 'Esther Luna Colombini']","['Faculty of Science, Department of Computer Science, University of Tübingen, Germany and Institute of Computing, University of Campinas, Brazil', 'Law School of Ribeirão Preto, University of São Paulo, Brazil', 'Institute of Computing, University of Campinas, Brazil', 'Institute of Computing, University of Campinas, Brazil']","['Brazil', 'Brazil', 'Brazil', 'Brazil']"
https://doi.org/10.1145/3630106.3658938,Fairness & Bias,The four-fifths rule is not disparate impact: A woeful tale of epistemic trespassing in algorithmic fairness,"Computer scientists are trained in the art of creating abstractions that simplify and generalize. However, a premature abstraction that omits crucial contextual details creates the risk of epistemic trespassing, by falsely asserting its relevance into other contexts. We study how the field of responsible AI has created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the <Formula format=""inline""><TexMath><?TeX $\nicefrac {4}{5}$?></TexMath><AltText>Math 1</AltText><File name=""facct24-53-inline1"" type=""svg""/></Formula> rule or 80% rule), a single part of disparate impact discrimination law, into the disparate impact metric. This metric incorrectly introduces a new deontic nuance and new potentials for ethical harms that were absent in the original <Formula format=""inline""><TexMath><?TeX $\nicefrac {4}{5}$?></TexMath><AltText>Math 2</AltText><File name=""facct24-53-inline2"" type=""svg""/></Formula> rule. We also survey how the field has amplified the potential for harm in codifying the <Formula format=""inline""><TexMath><?TeX $\nicefrac {4}{5}$?></TexMath><AltText>Math 3</AltText><File name=""facct24-53-inline3"" type=""svg""/></Formula> rule into popular AI fairness software toolkits. The harmful erasure of legal nuances is a wake-up call for computer scientists to self-critically re-evaluate the abstractions they create and use, particularly in the interdisciplinary field of AI ethics.",[],[],"['Elizabeth Anne Watkins', 'Jiahao Chen']","['Intel Labs, USA', 'Responsible AI LLC, United States of America']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3658939,Fairness & Bias,"Mapping the individual, social and biospheric impacts of Foundation Models","Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK’s AI Safety Summit and the G7’s Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.","['risks', 'harms', 'foundation models', 'AI governance', 'responsible AI']",Social and professional topics → Socio-technical systems Applied computing → Sociology General and reference → General literature,"['Andrés Domínguez Hernández', 'Shyam Krishna', 'Antonella Maia Perini', 'Michael Katell', 'SJ Bennett', 'Ann Borda', 'Youmna Hashem', 'Semeli Hadjiloizou', 'Sabeehah Mahomed', 'Smera Jayadeva', 'Mhairi Aitken', 'David Leslie']","['Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom and The Digital Environment Research Institute, Queen Mary University London, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Department of Geography, Durham University, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'Public Policy Programme, The Alan Turing Institute, United Kingdom', 'The Digital Environment Research Institute, Queen Mary University London, United Kingdom and Public Policy Programme, The Alan Turing Institute, United Kingdom']","['Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://doi.org/10.1145/3630106.3658942,Security,Escalation Risks from Language Models in Military and Diplomatic Decision-Making,"Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models’ reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.","['Natural Language Processing', 'Multi-Agent Security', 'Language Model Agents', 'Evaluation', 'Safety', 'Socio-Technical Impact', 'Military Applications']",Computing methodologies → Natural language generation Computing methodologies → Natural language processing Applied computing → Military,"['Juan-Pablo Rivera', 'Gabriel Mukobi', 'Anka Reuel', 'Max Lamparth', 'Chandler Smith', 'Jacquelyn Schneider']","['Georgia Institute of Technology, United States of America', 'Stanford University, USA', 'Stanford University, USA', 'Stanford University, USA', 'Northeastern University, USA', 'Hoover Wargaming and Crisis Simulation Initiative, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658944,Fairness & Bias,More than the Sum of its Parts: Susceptibility to Algorithmic Disadvantage as a Conceptual Framework,"Algorithmic systems are increasingly being applied in contexts of state action to, in some capacity, mediate the relations between state and individual. Disadvantageous effects, such as potential discriminatory outcomes brought forth by different kinds of biases, have been the locus of severe critique by academic scholarship and political activism. There has been scholarly work conceptualizing biases and types of biases, as well as types of harm. Drawing from Elizabeth Anderson’s conceptualization of relational equality, this paper emphasizes the relationality of the encounters between state and individual. This paper introduces ""susceptibility to algorithmic disadvantage"" as a conceptual framework to address the relational constellation at play. Susceptibility to algorithmic disadvantage has a vertical dimension that addresses the relation between a state actor and an individual and a horizontal dimension that is characterized by intersectional inequalities that prevail in societal contexts. Intersectional feminist scholarship has established that interlocking systems of oppression amount to more than the sum of their single-axis parts. Paralleling this argument, this paper argues that susceptibility to algorithmic disadvantage amounts to more than the sum of the vertical and the horizontal dimension: the dimensions co-constitute and reinforce each other. The proposed framework is applied to four international case studies situated in crucial areas of state action: facial recognition in law enforcement in the USA, biometric identification in social welfare in India, dialect recognition in the asylum system in Germany, and grade prediction in the education system in the UK. Viewed through the lens of the proposed framework, heterogeneous use cases in different locations and areas of state action emerge as similar considering the inquiry into questions of justice, rendering the proposed framework a useful tool for analysis.",[],"Computing methodologies → Machine learning Applied computing → Law, social and behavioral sciences Computing methodologies → Philosophical/theoretical foundations of artificial intelligence",['Paola Lopez'],"['University of Vienna, Austria and University of Bremen, Germany']",['Italy']
https://doi.org/10.1145/3630106.3658947,Fairness & Bias,Gender Representation Across Online Retail Products,"We present a broad characterization of gender representation in a large heterogeneous sample of retail products. In particular, we study online product textual information, such as titles and descriptions. Our goal is to understand from a semantic perspective, differences and similarities in how girls (women) and boys (men) are represented. We perform a comparative analysis of the language used in gendered products (i.e., products that mention exclusively either of these two genders), and additionally compare it to products that are explicitly gender neutral or inclusive. We found that the adjectives, skills, occupations, and values described in gendered products tended to reinforce stereotypes. Some of these stereotypes are aligned with historical findings from research on traditional off-line retail stores, and others are new owing to the up-to-date product dataset our research is based on. By leveraging additional existing resources we were able to gain insight into how certain product descriptions reflect stereotypes that are related to soft-skills and hierarchical occupational information. Conversely, we found that a large segment of products present explicitly as gender neutral or inclusive. We explore whether the language used by gender-inclusive products can be useful to improve stereotypes reflected in gendered product text. Specifically, we study its effect in word embedding fairness through debiasing techniques.",[],[],"['Dana Pessach', 'Barbara Poblete']","['Amazon, Israel', 'Department of Computer Science, University of Chile, Chile and Amazon, USA']","['Israel', 'Chile']"
https://doi.org/10.1145/3630106.3658948,Transparency & Explainability,Visibility into AI Agents,"Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.","['visibility', 'transparency', 'ai agents', 'ai deployment', 'ai oversight', 'ai monitoring']",Computing methodologies → Artificial intelligence Applied computing → Law Social and professional topics → Governmental regulations,"['Alan Chan', 'Carson Ezell', 'Max Kaufmann', 'Kevin Wei', 'Lewis Hammond', 'Herbie Bradley', 'Emma Bluemke', 'Nitarshan Rajkumar', 'David Krueger', 'Noam Kolt', 'Lennart Heim', 'Markus Anderljung']","['Centre for the Governance of AI, United Kingdom and Mila (Quebec AI Institute), Canada', 'Harvard University, United Kingdom', 'Independent, United Kingdom', 'Harvard Law School, USA', 'University of Oxford, United Kingdom and Cooperative AI Foundation, United Kingdom', 'University of Cambridge, United Kingdom', 'Centre for the Governance of AI, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Toronto, Canada', 'Centre for the Governance of AI, United Kingdom', 'Centre for the Governance of AI, United Kingdom']","['Canada', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3630106.3658949,Transparency & Explainability,The tensions of data sharing for human rights: A modern slavery case study,"There are calls for greater data sharing to address human rights issues. Advocates claim this will provide an evidence-base to increase transparency, improve accountability, enhance decision-making, identify abuses, and offer remedies for rights violations. However, these well-intentioned efforts have been found to sometimes enable harms against the people they seek to protect. This paper shows issues relating to fairness, accountability, or transparency (FAccT) in and around data sharing can produce such ‘ironic’ consequences. It does so using an empirical case study: efforts to tackle modern slavery and human trafficking in the UK. We draw on a qualitative analysis of expert interviews, workshops, ecosystem mapping exercises, and a desk-based review. The findings show how, in the UK, a large ecosystem of data providers, hubs, and users emerged to process and exchange data from across the country. We identify how issues including legal uncertainties, non-transparent sharing procedures, and limited accountability regarding downstream uses of data may undermine efforts to tackle modern slavery and place victims of abuses at risk of further harms. Our findings help explain why data sharing activities can have negative consequences for human rights, even within human rights initiatives. Moreover, our analysis offers a window into how FAccT principles for technology relate to the human rights implications of data sharing. Finally, we discuss why these tensions may be echoed in other areas where data sharing is pursued for human rights concerns, identifying common features which may lead to similar results, especially where sensitive data is shared to achieve social goods or policy objectives.","['Data sharing', 'data governance', 'human rights', 'modern slavery', 'fairness', 'transparency', 'accountability']",Applied computing → Sociology Applied computing → Law Social and professional topics → Governmental regulations Social and professional topics → Privacy policies Security and privacy → Social aspects of security and privacy Information systems → Data exchange,"['Jamie Hancock', 'Sarada Mahesh', 'Jennifer Cobbe', 'Jatinder Singh', 'Anjali Mazumder']","['The Alan Turing Institute, United Kingdom', 'The Alan Turing Institute, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom and The Alan Turing Institute, United Kingdom', 'The Alan Turing Institute, United Kingdom']","['Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://doi.org/10.1145/3630106.3658950,Transparency & Explainability,Trouble at Sea: Data and digital technology challenges for maritime human rights concerns,"Recent years have revealed the severity and scale of human rights abuses at sea. Yet maritime human rights investigations remain challenging due to an array of difficulties, including physical inaccessibility and a complex legal environment. Improving the availability of data has been framed as a solution that will enhance transparency in marine-related activities and improve accountability for rights violations. Such enthusiasm has fuelled the development of technological solutions promising to identify abuses and safeguard vulnerable individuals. However, these efforts clash with concerns over the use of data and technology in human rights practice. In the context of such tensions, this paper studies how data and technology have been integrated within investigations into rights abuses at sea. We examine the challenges posed for transparency, accountability, and fairness regarding communities affected by rights violations. We ask: do data and digital technologies offer effective means for helping to expose rights abuses and hold malicious actors accountable? Or do they introduce new threats to autonomy, privacy, and dignity? We present empirical research based on qualitative engagements with expert practitioners. We find: 1) an increased availability of datasets did not necessarily prevent harm or improve safeguarding for vulnerable people; 2) many tech solutions were detached from affected individuals’ lived experiences and appeared not to meet communities’ needs; 3) uses of data and technology could introduce or aggravate risks to fairness and accountability within human rights investigations. We contribute a much-needed reflection on the actual implications of the use of data and technological tools for communities affected by human rights violations. Regarding maritime human rights, we argue that prioritising large-scale, top-down monitoring to collect larger datasets or market more tech solutions is not the best way for data and technology to contribute to transparency and accountability. Instead, we advocate for deeper engagement with affected communities.","['Data', 'technology', 'remote sensing', 'open-source intelligence (OSINT)', 'human rights', 'human rights practice', 'investigation', 'seas', 'fishing', 'maritime', 'ethics', 'fairness', 'transparency', 'accountability']",Applied computing → Law Applied computing~Sociology Information systems~Open source software Information systems → Data analytics Information systems → Geographic information systems Information systems → Global positioning systems Social and professional topics → Codes of ethics,"['Jamie Hancock', 'Ruoyun Hui', 'Jatinder Singh', 'Anjali Mazumder']","['The Alan Turing Institute, United Kingdom', 'The Alan Turing Institute, United Kingdom', 'Compliant & Accountable Systems Group, University of Cambridge, United Kingdom and The Alan Turing Institute, United Kingdom', 'The Alan Turing Institute, United Kingdom']","['Italy', 'Italy', 'Italy', 'Italy']"
https://doi.org/10.1145/3630106.3658951,Transparency & Explainability,From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap,"Two goals – improving replicability and accountability of Machine Learning research respectively, have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers - replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the responsibility gap – holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application, this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability’s advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles, practical implications on Circulating Reference, Interpretative Labor, and research communication.",[],[],['Tianqi Kou'],"['College of Information Sciences and Technology, Pennsylvania State University, United States of America']",['United States']
https://doi.org/10.1145/3630106.3658952,Fairness & Bias,Structural Interventions and the Dynamics of Inequality,"Recent conversations in the algorithmic fairness literature have raised several concerns with standard conceptions of fairness. First, constraining predictive algorithms to satisfy fairness benchmarks may sometimes lead to non-optimal outcomes for disadvantaged groups. Second, technical interventions are often ineffective by themselves, especially when divorced from an understanding of structural processes that generate social inequality. Inspired by both these critiques, we construct a common decision-making model, using mortgage loans as a running example. We show that under some conditions, any choice of decision threshold will inevitably perpetuate existing disparities in financial stability unless one deviates from the Pareto optimal policy. This confirms the intuition that technical interventions, such as fairness constraints, often do not sufficiently address persistent underlying inequities. Then, we model the effects of three different types of interventions: (1) policy changes in the algorithm’s decision threshold, and external changes to parameters that govern the downstream effects of late payment for (2) the whole population or (3) disadvantaged subgroups. We show how different interventions are recommended depending on the difficulty of enacting structural change upon external parameters and depending on the policymaker’s preferences for equity or efficiency. Counterintuitively, we demonstrate that preferences for efficiency over equity may sometimes lead to recommendations for interventions that target the under-resourced group alone. Finally, we simulate the effects of interventions on a dataset that combines HMDA and Fannie Mae loan data. This research highlights the ways that structural inequality can be perpetuated by seemingly unbiased decision mechanisms, and it shows that in many situations, technical solutions must be paired with external, context-aware interventions to enact social change.","['algorithmic justice', 'social change', 'structural injustice', 'interventions', 'housing policy']",Applied computing~Sociology Human-centered computing → Collaborative and social computing design and evaluation methods,"['Aurora Zhang', 'Anette Hosoi']","['Institute for Data, Systems, and Society, Massachusetts Institute of Technology, United States of America', 'Massachusetts Institute of Technology, USA']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3658953,Transparency & Explainability,Explainable Artificial Intelligence for Academic Performance Prediction. An Experimental Study on the Impact of Accuracy and Simplicity of Decision Trees on Causability and Fairness Perceptions,"The rising adoption of learning analytics and academic performance prediction technologies in higher education highlights the urgent need for transparency and explainability. This demand, rooted in ethical concerns and fairness considerations, converges with Explainable Artificial Intelligence (XAI) principles. Despite the recognized importance of transparency and fairness in learning analytics, empirical studies examining student fairness perceptions, particularly within academic performance prediction, remain limited. We conducted a pre-registered factorial survey experiment involving 1,047 German students to investigate how decision tree features (simplicity and accuracy) influence perceived distributive and informational fairness, mediated by causability (i.e., the self-assessed understandability of a machine learning model’s cause-effect linkages). Additionally, we examined the moderating role of institutional trust in these relationships. Our results indicate that decision tree simplicity positively affects fairness perceptions, mediated by causability. In contrast, prediction accuracy neither directly nor indirectly influences these perceptions. Even if the hypothesized effects of interest are either minor or non-existent, results show that the medium positive effect of causability on the distributive fairness assessment depends on institutional trust. These findings substantially impact the crafting of transparent machine learning models in educational settings. We discuss important implications for fairness and transparency in implementing academic performance prediction systems.","['Explainable AI', 'Trustworthy AI', 'ML Fairness', 'Academic Performance Prediction', 'Experiment', 'Causability']",Human-centered computing → Empirical studies in visualization Human-centered computing → User studies Human-centered computing → Laboratory experiments Computing methodologies → Artificial intelligence,"['Marco Lünich', 'Birte Keller']","['Faculty of Arts and Humanities, Heinrich Heine University, Germany', 'Faculty of Arts and Humanities, Heinrich Heine University, Germany']","['Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658954,Fairness & Bias,Mitigating Group Bias in Federated Learning for Heterogeneous Devices,"Federated learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature, i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients. It produces biased global models, i.e., models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity.","['Federated Learning', 'Algorithmic Fairness', 'Group Fairness']",Computing methodologies → Machine learning Computing methodologies → Distributed computing methodologies,"['Khotso Selialia', 'Yasra Chandio', 'Fatima M. Anwar']","['University of Massachusetts Amherst, United States of America', 'University of Massachusetts Amherst, United States of America', 'University of Massachusetts Amherst, United States of America']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658955,Transparency & Explainability,Machine learning data practices through a data curation lens: An evaluation framework,"Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.",[],[],"['Eshta Bhardwaj', 'Harshit Gujral', 'Siyi Wu', 'Ciara Zogheib', 'Tegan Maharaj', 'Christoph Becker']","['University of Toronto, Canada', 'University of Toronto, Canada', 'University of Toronto, Canada', 'University of Toronto, Canada', 'University of Toronto, Canada', 'University of Toronto, Canada']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://doi.org/10.1145/3630106.3658957,Transparency & Explainability,A Framework for Assurance Audits of Algorithmic Systems,"The authors have requested minor, non-substantive changes to the Version of Record and, in accordance with ACM policies, a Corrected Version of Record was published on July 17, 2024. For reference purposes, the VoR may still be accessed via the Supplemental Material section on this page.",[],[],"['Khoa Lam', 'Benjamin Lange', 'Borhane Blili-Hamelin', 'Jovana Davidovic', 'Shea Brown', 'Ali Hasan']","['BABL AI Inc., USA', 'BABL AI Inc., USA, Ludwig Maximilians University (LMU), Germany & Munich Center for Machine Learning, Germany', 'BABL AI Inc., United States of America and AI Risk and Vulnerability Alliance (ARVA), USA', 'BABL AI Inc., USA and University of Iowa, USA', 'BABL AI Inc., USA and University of Iowa, USA', 'BABL AI Inc., USA and University of Iowa, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658958,Transparency & Explainability,"Building, Shifting, & Employing Power: A Taxonomy of Responses From Below to Algorithmic Harm","A large body of research has attempted to ensure that algorithmic systems adhere to notions of fairness and transparency. Increasingly, researchers have highlighted that mitigating algorithmic harms requires explicitly taking power structures into account. Those with power over algorithmic systems often fail to sufficiently address algorithmic harms and rarely consult those directly harmed by algorithmic systems. Left to their own devices, people respond to algorithmic harms they encounter in a wide variety of ways, but we lack broader, overarching understandings of these responses. In this work, we synthesize documented, historical cases into a taxonomy of responses “from below” to algorithmic harm. Our taxonomy connects different types of responses to existing theorizations of power from fields including anthropology, human-computer interaction, and communication, centering how people employ, shift, and build power in their responses to algorithmic harm. Based on our taxonomy, we highlight an opportunity space for the FAccT community to engage with and support such action from below.","['algorithmic harm', 'power', 'algorithmic bias', 'critical algorithm studies', 'data leverage', 'feminist refusal', 'algorithmic resistance', 'everyday algorithm auditing', 'AI ethics', 'accountability']","Human-centered computing → HCI theory, concepts and models Human-centered computing → Collaborative and social computing theory, concepts and paradigms","['Alicia DeVrio', 'Motahhare Eslami', 'Kenneth Holstein']","['Carnegie Mellon University, USA', 'Carnegie Mellon University, USA', 'Carnegie Mellon University, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658959,Transparency & Explainability,Auditing Work: Exploring the New York City algorithmic bias audit regime,"In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law’s practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of ‘auditor roles’ that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes.","['algorithm audit', 'algorithmic bias', 'AI policy', 'Local Law 144']",Social and professional topics Social and professional topics → Governmental regulations Social and professional topics → Government technology policy,"['Lara Groves', 'Jacob Metcalf', 'Alayna Kennedy', 'Briana Vecchione', 'Andrew Strait']","['Ada Lovelace Institute, United Kingdom', 'Data & Society Research Institute, United States of America', 'Independent researcher, USA', 'Data & Society Research Institute, United States of America', 'Ada Lovelace Institute, United Kingdom']","['United Kingdom', '', 'United Kingdom', '', 'United Kingdom']"
https://doi.org/10.1145/3630106.3658960,Transparency & Explainability,Automated Transparency: A Legal and Empirical Analysis of the Digital Services Act Transparency Database,"The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (‘statements of reasons’ - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.",[],[],"['Rishabh Kaushal', 'Jacob Van De Kerkhof', 'Catalina Goanta', 'Gerasimos Spanakis', 'Adriana Iamnitchi']","['Institute of Data Science, Maastricht University, Netherlands and Indira Gandhi Delhi Technical University for Women, India', 'Faculty of Law, Economics and Governance, Utrecht University, Netherlands', 'Faculty of Law, Economics and Governance, Utrecht University, Netherlands', 'Department of Advanced Computing Sciences, Maastricht University, Netherlands', 'Institute of Data Science, Maastricht University, Netherlands']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://doi.org/10.1145/3630106.3658961,Fairness & Bias,PreFAIR: Combining Partial Preferences for Fair Consensus Decision-making,"Preference aggregation mechanisms help decision-makers combine diverse preference rankings produced by multiple voters into a single consensus ranking. Prior work has developed methods for aggregating multiple rankings into a fair consensus over the same set of candidates. Yet few real-world problems present themselves as such precisely formulated aggregation tasks with each voter fully ranking all candidates. Instead, preferences are often expressed as rankings over partial and even disjoint subsets of candidates. For instance, hiring committee members typically opt to rank their top choices instead of exhaustively ordering every single job applicant. However, the existing literature does not offer a framework for characterizing nor ensuring group fairness in such partial preference aggregation tasks. Unlike fully ranked settings, partial preferences imply both a selection decision of whom to rank plus an ordering decision of how to rank the selected candidates. Our work fills this gap by conceptualizing the open problem of fair partial preference aggregation. We introduce an impossibility result for fair selection from partial preferences and design a computational framework showing how we can navigate this obstacle. Inspired by Single Transferable Voting, our proposed solution PreFair produces consensus rankings that are fair in the selection of candidates and also in their relative ordering. Our experimental study demonstrates that PreFair achieves the best performance in this dual fairness objective compared to state-of-the-art alternatives adapted to this new problem while still satisfying voter preferences.",[],[],"['Kathleen Cachel', 'Elke Rundensteiner']","['Worcester Polytechnic Institute, USA', 'Worcester Polytechnic Institute, USA']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3658962,Transparency & Explainability,Algorithmic Transparency and Participation through the Handoff Lens: Lessons Learned from the U.S. Census Bureau’s Adoption of Differential Privacy,"Emerging discussions on the responsible government use of algorithmic technologies propose transparency and public participation as key mechanisms for preserving accountability and trust. But in practice, the adoption and use of any technology shifts the social, organizational, and political context in which it is embedded. Therefore translating transparency and participation efforts into meaningful, effective accountability must take into account these shifts. We adopt two theoretical frames, Mulligan and Nissenbaum’s handoff model and Star and Griesemer’s boundary objects, to reveal such shifts during the U.S. Census Bureau’s adoption of differential privacy (DP) in its updated disclosure avoidance system (DAS) for the 2020 census. This update preserved (and arguably strengthened) the confidentiality protections that the Bureau is mandated to uphold, and the Bureau engaged in a range of activities to facilitate public understanding of and participation in the system design process. Using publicly available documents concerning the Census’ implementation of DP, this case study seeks to expand our understanding of how technical shifts implicate values, how such shifts can afford (or fail to afford) greater transparency and participation in system design, and the importance of localized expertise throughout. We present three lessons from this case study toward grounding understandings of algorithmic transparency and participation: (1) efforts towards transparency and participation in algorithmic governance must center values and policy decisions, not just technical design decisions; (2) the handoff model is a useful tool for revealing how such values may be cloaked beneath technical decisions; and (3) boundary objects alone cannot bridge distant communities without trusted experts traveling alongside to broker their adoption.","['critical transparency studies', 'participatory AI', 'differential privacy', 'census']","Social and professional topics → Government technology policy Applied computing → Law, social and behavioral sciences Security and privacy → Human and societal aspects of security and privacy","['Amina A. Abdu', 'Lauren M. Chambers', 'Deirdre K. Mulligan', 'Abigail Z. Jacobs']","['University of Michigan, United States of America', 'University of California, Berkeley, United States of America', 'University of California, Berkeley, United States of America', 'University of Michigan, United States of America']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658963,Transparency & Explainability,Auditing Image-based NSFW Classifiers for Content Filtering,"This paper examines NSFW (Not Safe For Work) image classifiers for content filtering. Through an audit of three prevalent NSFW classifiers, we analyze the relationship between NSFW predictions and three demographic factors: gender, skin-tone, and age. Our study reveals that women are disproportionately more frequently misclassified as NSFW compared to men, even when they appear conducting common daily-life activities. Additionally, we find that NSFW classifiers tend to mispredict images of people with lighter skin-tones and images depicting younger people. We explore the causes of such mispredictions by analyzing the explanatory pixel maps, which reveal some of the reasons behind the misclassifications. Overall, the implications of our findings become particularly salient when considering the application of filters based on NSFW classifiers, which we identified to have a direct impact on image datasets, computer vision models, generative AI, user experience, and artistic creativity. In summary, we hope our study brings attention to the inherent biases within NSFW classifiers and underscores the importance of addressing these issues to ensure fair and equitable outcomes in content filtering.","['audit', 'computer vision', 'content filtering', 'content moderation', 'NSFW classification']",Social and professional topics → Pornography Social and professional topics → Censoring filters Social and professional topics → Technology and censorship Social and professional topics → Women Computing methodologies → Computer vision,"['Warren Leu', 'Yuta Nakashima', 'Noa Garcia']","['University of California, Irvine, USA', 'Osaka University, Japan', 'Osaka University, Japan']","['United States', 'Japan', 'Japan']"
https://doi.org/10.1145/3630106.3658965,Fairness & Bias,Impact Charts: A Tool for Identifying Systematic Bias in Social Systems and Data,"We introduce impact charts and apply them to problems of systematic bias encoded in three different data sets. Impact charts are highly visual, making the effects they find easy to understand by both domain experts and non-experts. Impact charts are nonlinear and non-parametric, so they are able to identify structural biases whose functional forms are not a priori well understood.",[],[],['Darren Erik Vengroff'],"['Independent Researcher, United States']",['United States']
https://doi.org/10.1145/3630106.3658966,Transparency & Explainability,Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings,"The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.","['Generative AI', 'Language Models', 'Open Models', 'Transparency', 'Chatbots', 'ChatGPT', 'GPT-4', 'qLoRA']",Computing methodologies → Natural language processing Computing methodologies → Machine learning Computing methodologies → Artificial intelligence Applied computing Human-centered computing,"['Robert Wolfe', 'Isaac Slaughter', 'Bin Han', 'Bingbing Wen', 'Yiwei Yang', 'Lucas Rosenblatt', 'Bernease Herman', 'Eva Brown', 'Zening Qu', 'Nic Weber', 'Bill Howe']","['University of Washington, United States', 'University of Washington, USA', 'University of Washington, USA', 'University of Washington, USA', 'University of Washington, USA', 'New York University, USA', 'University of Washington, USA', 'University of Washington, USA', 'University of Washington, USA', 'University of Washington, United States', 'University of Washington, United States']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658967,Fairness & Bias,WorldBench: Quantifying Geographic Disparities in LLM Factual Recall,"As large language models (LLMs) continue to improve and gain popularity, some may use the models to recall facts, despite well documented limitations with LLM factuality. Towards ensuring that models work reliably for all, we seek to uncover if geographic disparities emerge when asking an LLM the same question about different countries. To this end, we present WorldBench, a dynamic and flexible benchmark composed of per-country data from the World Bank. In extensive experiments on state of the art open and closed source models, including GPT-4, Gemini, Llama-2, and Vicuna, to name a few, we find significant biases based on region and income level. For example, error rates are 1.5 times higher for countries from Sub-Saharan Africa compared to North American countries. We observe these disparities to be consistent over 20 LLMs and 11 individual World Bank indicators (i.e. specific statistics, such as population or CO2 emissions). WorldBench also enables automatic detection of citation hallucination, where models cite the World Bank itself while providing false statistics, and a manner to assess when an LLM’s stored facts begin to go out of date. We hope our benchmark will draw attention to geographic disparities in existing LLMs and facilitate the remedying of these biases.","['Geographic Disparity', 'Bias', 'Fairness', 'Large Language Models', 'Factuality']",Computing methodologies → Artificial intelligence Computing methodologies → Machine learning,"['Mazda Moayeri', 'Elham Tabassi', 'Soheil Feizi']","['University of Maryland, United States of America', 'Michigan State University, USA', 'University of Maryland, United States of America']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658968,Transparency & Explainability,The Dark Side of Dataset Scaling: Evaluating Racial Classification in Multimodal Models,"‘Scale the model, scale the data, scale the GPU farms’ is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.","['Audits', 'Evaluations', 'Scale', 'Visio-Linguistic Models', 'Multimodal Datasets', 'CLIP', 'Racism', 'Bias']",General and reference → Evaluation,"['Abeba Birhane', 'Sepehr Dehdashtian', 'Vinay Prabhu', 'Vishnu Boddeti']","['School of Computer Science and Statistics, Mozilla Foundation and Trinity College Dublin, Ireland', 'Department of Computer Science and Engineering, Michigan State University, USA', 'HAL51 Inc, USA', 'Computer Science and Engineering, Michigan State University, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658969,Fairness & Bias,Speaking of accent: A content analysis of accent misconceptions in ASR research,"Automatic speech recognition (ASR) researchers are working to address the differing transcription performance of ASR by accent or dialect. However, research often has a limited view of accent in ways that reproduce discrimination and limit the scope of potential solutions. In this paper we present a content analysis of 22 papers published in 2022 in top conferences and journals on the topic of accent and ASR. We report on how accent is sometimes mistakenly viewed as something some people don’t have; as having a default; and being an attribute only of the speaker, and not of the listener. We discuss the implications on research and provide recommendations to researchers who hope to reduce ASR biases by accent.","['accent', 'discrimination', 'speech recognition', 'AI fairness']","Human-centered computing → HCI theory, concepts and models Computing methodologies → Speech recognition","['Kerri Prinos', 'Neal Patwari', 'Cathleen A. Power']","['Washington University in St. Louis, USA', 'Washington University in St. Louis, United States of America', 'Relational Communities, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658970,Transparency & Explainability,Law and the Emerging Political Economy of Algorithmic Audits,"For almost a decade now, scholarship in and beyond the ACM FAccT community has been focusing on novel and innovative ways and methodologies to audit the functioning of algorithmic systems. Over the years, this research idea and technical project has matured enough to become a regulatory mandate. Today, the Digital Services Act (DSA) and the Online Safety Act (OSA) have established the framework within which technology corporations and (traditional) auditors will develop the ‘practice’ of algorithmic auditing thereby presaging how this ‘ecosystem’ will develop. In this paper, we systematically review the auditing provisions in the DSA and the OSA in light of observations from the emerging industry of algorithmic auditing. Who is likely to occupy this space? What are some political and ethical tensions that are likely to arise? How are the mandates of ‘independent auditing’ or ‘the evaluation of the societal context of an algorithmic function’ likely to play out in practice? By shaping the picture of the emerging political economy of algorithmic auditing, we draw attention to strategies and cultures of traditional auditors that risk eroding important regulatory pillars of the DSA and the OSA. Importantly, we warn that ambitious research ideas and technical projects of/for algorithmic auditing may end up crashed by the standardising grip of traditional auditors and/or diluted within a complex web of (sub-)contractual arrangements, diverse portfolios, and tight timelines.","['auditing', 'algorithmic audits', 'Digital Services Act', 'Online Safety Act', 'political economy']",Social and professional topics → Computing / technology policy Information systems → Social networking sites,"['Petros Terzis', 'Michael Veale', 'Noëlle Gaumann']","['Institute for Information Law, University of Amsterdam, Netherlands and Faculty of Laws, University College London, United Kingdom', 'Faculty of Laws, University College London, United Kingdom and Institute for Information Law, University of Amsterdam, Netherlands', 'Faculty of Laws, University College London, United Kingdom']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658971,Transparency & Explainability,Meaningful Transparency for Clinicians: Operationalising HCXAI Research with Gynaecologists,"AI systems can bring great benefits to our healthcare systems, e.g. by improving patient outcomes. Yet implementing them into clinical practice remains challenging. To bridge the gap between academic research and design implementation, we argue clinicians need transparency about such systems that is meaningful—i.e. contextually appropriate—to them. Towards this, we explore recent HCXAI recommendations for building transparent AI systems for users in a specific domain: gynaecology. By better understanding clinicians’ perspectives on meaningful transparency, our aim is to complement and help operationalise such recommendations. We conduct a co-design workshop and interviews with n=15 gynaecologists in the UK and the Netherlands. We show that HCXAI must better account for clinical teams with different types of gynaecologist users, and that the timeliness and relevance of the information provided about the AI-based tool throughout its design lifecycle—in particular before a tool is implemented into clinical practice—is crucial for transparency to become meaningful. Our contributions include: i) testing recommendations from the latest HCXAI literature with a prospective, real-life AI application in a relatively less-studied clinical domain; ii) describing and visualising gynaecologists’ understanding of meaningful transparency for clinicians; iii) outlining four design recommendations towards realising meaningful transparency for clinicians and opportunities for research; and iv) expanding HCI and AI research in women’s health by directly engaging with gynaecologists as users and co-designers. Exploring such issues is key to facilitate the implementation of AI systems that meet clinicians’ information needs and that they can trust.","['clinicians', 'transparency', 'clinical Artificial Intelligence', 'explainable AI', 'gynaecologists', 'co-design']","Applied computing → Life and medical sciences Social and professional topics → User characteristics Human-centered computing → Empirical studies in HCI Computing methodologies → Philosophical/theoretical foundations of artificial intelligence Human-centered computing → HCI theory, concepts and models","['Bianca Giulia Sarah Schor', 'Emma Kallina', 'Jatinder Singh', 'Alan Blackwell']","['University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom', 'University of Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3630106.3658972,Fairness & Bias,A Causal Perspective on Label Bias,"Predictive models developed with machine learning techniques are commonly used to inform decision making and resource allocation in high-stakes contexts, such as healthcare and public health. One means through which this practice may propagate equity-related harms is when the data used for model development or evaluation exhibits label bias. In such cases, the target of prediction is a proxy label of a construct of interest that may be difficult or impossible to measure, while the relationship between the proxy and the construct of interest differs systematically across subgroups. Label bias can be especially challenging to identify and mitigate in practice because consequential fairness violations are masked when the model is evaluated with respect to the proxy label. In this work, we aim to develop further formal understanding of label bias to inform the development of approaches for the identification and mitigation of it. To do so, we present desiderata for unbiased and biased proxy labels, introduce candidate causal graphical criteria for label bias, and consider the extent to which proxy labels can be used to reason about fairness with respect to a true construct of interest. We validate our findings with a simulation study and experiments with synthetic health insurance data used in the context of a care management system.","['Label bias', 'Proxy', 'Model Evaluation', 'Fairness']",Computing methodologies → Machine learning,"['Vishwali Mhasawade', ""Alexander D'Amour"", 'Stephen R Pfohl']","['New York University, United States of America', 'Google, United States of America', 'Google, United States of America']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658973,Security,Epistemic Power in AI Ethics Labor: Legitimizing Located Complaints,"What counts as legitimate AI ethics labor, and consequently, what are the epistemic terms on which AI ethics claims are rendered legitimate? Based on 75 interviews with technologists including researchers, developers, open source contributors, and activists, this paper explores the various epistemic bases from which AI ethics is discussed and practiced. In the context of outside attacks on AI ethics as an impediment to “progress,” I show how some AI ethics practices have reached toward authority from automation and quantification, and achieved some legitimacy as a result, while those based on richly embodied and situated lived experience have not. This paper draws together the work of feminist Anthropology and Science and Technology Studies scholars Diana Forsythe and Lucy Suchman with the works of postcolonial feminist theorist Sara Ahmed and Black feminist theorist Kristie Dotson to examine the implications of dominant AI ethics practices.",[],[],['David Gray Widder'],"['Digital Life Initiative, Cornell Tech, USA']",['Paraguay']
https://doi.org/10.1145/3630106.3658974,Fairness & Bias,One Model Many Scores: Using Multiverse Analysis to Prevent Fairness Hacking and Evaluate the Influence of Model Design Decisions,"A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. The downstream effects of ADM systems critically depend on the decisions made during a systems’ design, implementation, and evaluation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these decisions are made implicitly, without knowing exactly how they will influence the final system. To study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. In our proposed method, we turn implicit decisions during design and evaluation into explicit ones and demonstrate their fairness implications. By combining decisions, we create a grid of all possible “universes” of decision combinations. For each of these universes, we compute metrics of fairness and performance. Using the resulting dataset, one can investigate the variability and robustness of fairness scores and see how and which decisions impact fairness. We demonstrate how multiverse analyses can be used to better understand fairness implications of design and evaluation decisions using an exemplary case study of predicting public health care coverage for vulnerable populations. Our results highlight how decisions regarding the evaluation of a system can lead to vastly different fairness metrics for the same model. This is problematic, as a nefarious actor could optimise or “hack” a fairness metric to portray a discriminating model as fair merely by changing how it is evaluated. We illustrate how a multiverse analysis can help to address this issue.","['algorithmic fairness', 'multiverse analysis', 'automated decision making', 'robustness', 'reliable machine learning']",Social and professional topics → User characteristics Computing methodologies → Machine learning,"['Jan Simson', 'Florian Pfisterer', 'Christoph Kern']","['Institute of Statistics, LMU Munich, Germany and Munich Center for Machine Learning (MCML), Germany', 'Institute of Statistics, LMU Munich, Germany', 'Institute of Statistics, LMU Munich, Germany and Munich Center for Machine Learning (MCML), Germany']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658975,Fairness & Bias,"Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans","Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.","['Large Language Models', 'AI Bias', 'Homogeneity Bias', 'Perceived Variability', 'Stereotyping']",Computing methodologies → Natural language processing Applied computing → Psychology,"['Messi H.J. Lee', 'Jacob M. Montgomery', 'Calvin K. Lai']","['Division of Computational and Data Sciences, Washington University in St.Louis, United States', 'Department of Political Science, Washington University in St.Louis, USA', 'Department of Psychological & Brain Sciences, Washington University in St.Louis, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658976,Transparency & Explainability,Perceptive Visual Urban Analytics is Not (Yet) Suitable for Municipalities,"The use of Computer Vision, through a Perceptive Visual Urban Analytics (VUA) paradigm, has been proposed as a way for municipalities to more easily monitor their cities. However, prior studies fall short of actually investigating whether Perceptive VUA is ready for municipal use. In this paper we take a critical look at this paradigm by comparing key methods and evaluating them on usability and trustworthiness with municipal experts as well as Responsible AI and Computer Vision researchers. Based on on this evaluation we find that Perceptive VUA is not (yet) ready for municipal use as they do not incorporate domain knowledge and overly rely on spurious correlations. We conclude by providing recommendations for how to progress Perceptive VUA such that it may actually contribute to improving the liveability and quality of urban environments.","['Explainability', 'Computer Vision', 'Trustworthiness']",Computing methodologies → Computer vision tasks Applied computing,"['Tim Alpherts', 'Sennay Ghebreab', 'Yen-Chia Hsu', 'Nanne Van Noord']","['University of Amsterdam, Netherlands', 'University of Amsterdam, Netherlands', 'University of Amsterdam, Netherlands', 'University of Amsterdam, Netherlands']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://doi.org/10.1145/3630106.3658977,Fairness & Bias,The Impact of Differential Feature Under-reporting on Algorithmic Fairness,"Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such “differential feature under-reporting” as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, little is known about the setting of data missingness absent indicators (i.e. differential feature under-reporting). In this work, we study an analytically tractable model of differential feature under-reporting to characterizethe impact of under-report on algorithmic fairness. We demonstrate how standard missing data methods typically fail to mitigate bias in this setting, and propose a new set of augmented loss and imputation methods. Our results show that, in real world data settings, under-reporting typically exacerbates disparities. The proposed solution methods show some success in mitigating disparities attributable to feature under-reporting.",[],[],"['Nil-Jana Akpinar', 'Zachary Lipton', 'Alexandra Chouldechova']","['Carnegie Mellon University, USA and Amazon Web Services, USA', 'Carnegie Mellon Univeristy, United States of America', 'Carnegie Mellon University, United States of America']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658979,Fairness & Bias,Collective Constitutional AI: Aligning a Language Model with Public Input,"There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.","['human-centered AI', 'participatory AI', 'reinforcement learning from human feedback', 'AI ethics', 'value alignment', 'collective alignment', 'AI alignment', 'generative AI', 'AI bias']","Computing methodologies → Machine learning Computing methodologies → Natural language processing Human-centered computing → HCI design and evaluation methods Human-centered computing → HCI theory, concepts and models Human-centered computing → Collaborative and social computing design and evaluation methods","['Saffron Huang', 'Divya Siddarth', 'Liane Lovitt', 'Thomas I. Liao', 'Esin Durmus', 'Alex Tamkin', 'Deep Ganguli']","['Collective Intelligence Project, USA', 'Collective Intelligence Project, USA', 'Anthropic, USA', 'Unaffiliated, USA', 'Anthropic, USA', 'Anthropic, USA', 'Anthropic, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658980,Fairness & Bias,Fairness in Online Ad Delivery,"Advertising funds a number of services that play a major role in our everyday online experiences, from social networking, to maps, search, and news. As the power and reach of advertising platforms grow, so do the concerns about the potential for discrimination associated with targeted advertising. However, despite our ever-improving ability to measure and describe instances of unfair distribution of high-stakes ads—such as employment, housing, or credit—we lack the tools to model and predict the extent to which alternative systems could address such problems. In this paper, we simulate an ad distribution system to model the effects that enforcing popularly proposed fairness approaches would have on the utility of the advertising platforms and their users. We show that in many realistic scenarios, achieving statistical parity would come at a much higher utility cost to platforms than enforcing predictive parity or equality of opportunity. Additionally, we identify a tradeoff between different notions of fairness, i.e., enforcing one criterion leads to worse outcomes with respect to other criteria. We further describe how pursuing fairness in situations where one group of users is more expensive to advertise to is likely to result in “leveling down” effects, i.e., not benefiting any group of users. We show that these negative effects can be prevented by ensuring that it is the platforms that carry the cost of fairness rather than passing it on to their users or advertisers. Overall, our findings contribute to ongoing discussions on fair ad delivery. We show that fairness is not satisfied by default, that limiting targeting options is not sufficient to address potential discrimination and bias in online ad delivery, and that choices made by regulators and platforms may backfire if potential side-effects are not properly considered.","['algorithmic fairness', 'online advertising', 'bias mitigation', 'leveling down']",Information systems → Online advertising Computing methodologies → Machine learning Social and professional topics → Computing / technology policy,"['Joachim Baumann', 'Piotr Sapiezynski', 'Christoph Heitz', 'Aniko Hannak']","['University of Zurich, Switzerland and Zurich University of Applied Sciences, Switzerland', 'Northeastern University, United States of America', 'Zurich University of Applied Sciences, Switzerland', 'University of Zurich, Switzerland']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://doi.org/10.1145/3630106.3658981,Fairness & Bias,Knowledge-Enhanced Language Models Are Not Bias-Proof: Situated Knowledge and Epistemic Injustice in AI,"The factual inaccuracies (""hallucinations"") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of ""objective"" or ""neutral"" knowledge.","['natural language processing', 'language models', 'knowledge graphs', 'knowledge enhancement', 'bias', 'fairness', 'representation', 'epistemology', 'feminism']",Computing methodologies → Natural language generation Computing methodologies → Reasoning about belief and knowledge,"['Angelie Kraft', 'Eloïse Soulier']","['Universität Hamburg, Germany and Leuphana Universität Lüneburg, Germany', 'Universität Hamburg, Germany']","['Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658982,Fairness & Bias,NLP for Maternal Healthcare: Perspectives and Guiding Principles in the Age of LLMs,"Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of guiding principles. We propose nine principles for ethical use of NLP for maternal healthcare, grouped into three themes: (i) recognizing contextual significance (ii) holistic measurements, and (iii) who/what is valued. For each principle, we describe its underlying rationale and provide practical advice. This set of principles can provide a methodological pattern for other researchers and serve as a resource to practitioners working on maternal health and other healthcare fields to emphasize the importance of technical nuance, historical context, and inclusive design when developing NLP technologies for clinical use.","['maternal health', 'natural language processing', 'large language models', 'ethical guidelines']",Applied computing → Health informatics Computing methodologies → Natural language processing Social and professional topics → Medical technologies,"['Maria Antoniak', 'Aakanksha Naik', 'Carla S. Alvarado', 'Lucy Lu Wang', 'Irene Y. Chen']","['Allen Institute for AI, USA', 'Allen Institute for AI, USA', 'Association of American Medical Colleges, Center for Health Justice, USA', 'University of Washington, Allen Institute for AI, USA', 'University of California, Berkeley and University of California, San Francisco, USA']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658985,Transparency & Explainability,Transparency in the Wild: Navigating Transparency in a Deployed AI System to Broaden Need-Finding Approaches,"Transparency is a critical component when building artificial intelligence (AI) decision-support tools, especially for contexts in which AI outputs impact people or policy. Effectively identifying and addressing user transparency needs in practice remains a challenge. While a number of guidelines and processes for identifying transparency needs have emerged, existing methods tend to approach need-finding with a limited focus that centers around a narrow set of stakeholders and transparency techniques. To broaden this perspective, we employ numerous need-finding methods to investigate transparency mechanisms in a widely deployed AI-decision support tool developed by a wildlife conservation non-profit. Throughout our 5-month case study, we conducted need-finding through semi-structured interviews with end-users, analysis of the tool’s community forum, experiments with their ML model, and analysis of training documents created by end-users. We also held regular meetings with the tool’s product and machine learning teams. By approaching transparency need-finding from a broad lens, we uncover insights into end-users’ transparency needs as well as unexpected uses and challenges with current transparency mechanisms. Our study is one of the first to incorporate such diverse perspectives to reveal an unbiased and rich view of transparency needs. Lastly, we offer the FAccT community recommendations on broadening transparency need-finding approaches, contributing to the evolving field of transparency research.","['Transparency Mechanisms', 'Need-Finding', 'AI-Supported Decision-Making', 'Case Study', 'Computer Vision']",Computing methodologies → Computer vision Human-centered computing → Empirical studies in collaborative and social computing Applied computing,"['Violet Turri', 'Katelyn Morrison', 'Katherine-Marie Robinson', 'Collin Abidi', 'Adam Perer', 'Jodi Forlizzi', 'Rachel Dzombak']","['Carnegie Mellon University Software Engineering Institute, United States', 'Carnegie Mellon University, USA', 'Carnegie Mellon University Software Engineering Institute, United States', 'Carnegie Mellon University Software Engineering Institute, USA', 'Carnegie Mellon University, USA', 'Carnegie Mellon University, USA', 'Carnegie Mellon University Software Engineering Institute, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658986,Fairness & Bias,How the Types of Consequences in Social Scoring Systems Shape People's Perceptions and Behavioral Reactions,"In the context of the rise of algorithmic decision-making (ADM) systems, social scoring systems are particularly controversial. They aim to encourage socially desirable behaviors by rewarding people with a good score in various decision-making contexts. In this paper, we report the results of a survey following a social scoring experiment, to predominantly understand the impact of the scoring outcome and the decision importance on people’s perceptions and behavioral intentions within an abstract social scoring system. We find that the outcome was pivotal for creating opinion differences regarding people’s perceptions, and behavioral reactions. In contrast, the decision importance did not exert a systematic impact on people’s perceptions and behavioral reactions, but exacerbated existing opinion differences in terms of perceived effectiveness. Specifically, the outcome strongly shaped the structural relationship between people’s experiences, perceptions, and behavioral reactions, creating a substantial outcome favorability bias for people with a bad outcome. Although people with a bad outcome reported an intention to adapt their behaviors, their intention to engage in desired behaviors could not be attributed to a perceived legitimacy of the system. For those with a good outcome, perceptions of procedural justice and legitimacy were weakened by the privacy-invading character of the social scoring system. Our work shows that the outcome people receive might create a pivotal disparate impact on people’s overall attitudes towards social scoring, shape their behavioral reactions, and create divergent behavioral motives, suggesting that very distinct societal dynamics may arise.","['social scoring systems', 'experiment', 'procedural justice', 'legitimacy']",Human-centered computing → Empirical studies in HCI Security and privacy → Economics of security and privacy Security and privacy → Social aspects of security and privacy,"['Carmen Loefflad', 'Jens Grossklags']","['School of Computation, Information and Technology, Technical University of Munich, Germany', 'School of Computation, Information and Technology, Technical University of Munich, Germany']","['Germany', 'Germany']"
https://doi.org/10.1145/3630106.3658987,Transparency & Explainability,The Impact and Opportunities of Generative AI in Fact-Checking,"Generative AI appears poised to transform white collar professions, with more than 90% of Fortune 500 companies using OpenAI’s flagship GPT models, which have been characterized as “general purpose technologies” capable of effecting epochal changes in the economy. But how will such technologies impact organizations whose job is to verify and report factual information, and to ensure the health of the information ecosystem? To investigate this question, we conducted 30 interviews with N=38 participants working at 29 fact-checking organizations across six continents, asking about how they use generative AI and the opportunities and challenges they see in the technology. We found that uses of generative AI envisioned by fact-checkers differ based on organizational infrastructure, with applications for quality assurance in Editing, for trend analysis in Investigation, and for information literacy in Advocacy. We used the TOE framework to describe participant concerns ranging from the Technological (lack of transparency), to the Organizational (resource constraints), to the Environmental (uncertain and evolving policy). Building on the insights of our participants, we describe value tensions between fact-checking and generative AI, and propose a novel Verification dimension to the design space of generative models for information verification work. Finally, we outline an agenda for fairness, accountability, and transparency research to support the responsible use of generative AI in fact-checking. Throughout, we highlight the importance of human infrastructure and labor in producing verified information in collaboration with AI. We expect that this work will inform not only the scientific literature on fact-checking, but also contribute to understanding of organizational adaptation to a powerful but unreliable new technology.","['Generative AI', 'Fact-Checking', 'Transparency', 'Sociotechnical Infrastructure', 'Design']",Social and professional topics → Computing industry Computing methodologies → Artificial intelligence Human-centered computing Applied computing → Computers in other domains,"['Robert Wolfe', 'Tanushree Mitra']","['University of Washington, United States of America', 'University of Washington, USA']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3658989,Fairness & Bias,Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness,"Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different – a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving the chances of a positive outcome available to individuals from another sub-population, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model’s predictive power, individual fairness and robustness while ensuring group fairness.","['Individual Fairness', 'Group Fairness', 'Machine Learning', 'Threshold Optimisation', 'Calibration']","Human-centered computing → Collaborative and social computing theory, concepts and paradigms Mathematics of computing → Probabilistic algorithms Computing methodologies → Machine learning Social and professional topics → User characteristics","['Edward Small', 'Kacper Sokol', 'Daniel Manning', 'Flora D. Salim', 'Jeffrey Chan']","['RMIT University, Australia', 'ETH Zürich, Switzerland', 'RMIT University, Australia', 'University of New South Wales, Australia', 'RMIT University, Australia']","['India', 'India', 'India', 'India', 'India']"
https://doi.org/10.1145/3630106.3658990,Transparency & Explainability,A Critical Survey on Fairness Benefits of Explainable AI,"In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 scientific articles on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. Importantly, we notice that claims are often (i) vague and simplistic, (ii) lacking normative grounding, or (iii) poorly aligned with the actual capabilities of XAI. We suggest to conceive XAI not as an ethical panacea but as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness. Moreover, when making a claim about XAI and fairness, we emphasize the need to be more specific about what kind of XAI method is used, which fairness desideratum it refers to, how exactly it enables fairness, and who is the stakeholder that benefits from XAI.",[],[],"['Luca Deck', 'Jakob Schoeffer', 'Maria De-Arteaga', 'Niklas Kühl']","['University of Bayreuth, Germany', 'University of Texas at Austin, United States', 'University of Texas at Austin, USA', 'University of Bayreuth, Germany']","['Germany', 'United States', 'United States', 'Germany']"
https://doi.org/10.1145/3630106.3658991,Fairness & Bias,"Evidence of What, for Whom? The Socially Contested Role of Algorithmic Bias in a Predictive Policing Tool","This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur. Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool’s algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders’ positionality and political ends. Drawing inspiration from Catherine D’Ignazio’s taxonomy of “refusing and using” data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures. We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI. This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures. We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo.",['criminal justice; predictive policing; algorithmic bias; qualitative'],Human-centered computing → Empirical studies in HCI,"['Marta Ziosi', 'Dasha Pruss']","['University of Oxford, United Kingdom', 'Harvard University, USA']","['United Kingdom', 'United States']"
https://doi.org/10.1145/3630106.3658992,Fairness & Bias,Participation in the age of foundation models,"Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of services, from banking to healthcare. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to historically marginalized groups. The larger scale and domain-agnostic manner in which these models operate further heightens the stakes: any errors or harms are liable to reoccur across use cases. In AI & ML more broadly, participatory approaches hold promise to lend agency and decision-making power to marginalized stakeholders, leading to systems that better benefit justice through equitable and distributed governance. But existing approaches in participatory AI/ML are typically grounded in a specific application and set of relevant stakeholders, and it is not straightforward how to apply these lessons to the context of foundation models. Our paper aims to fill this gap.","['Foundation models', 'public participation', 'governance', 'communities', 'stakeholders']","Computing methodologies → Machine learning Computing methodologies → Artificial intelligence Computing methodologies → Natural language generation Computing methodologies → Computer vision Human-centered computing → Human computer interaction (HCI) Human-centered computing → HCI theory, concepts and models Social and professional topics → Computing / technology policy","['Harini Suresh', 'Emily Tseng', 'Meg Young', 'Mary Gray', 'Emma Pierson', 'Karen Levy']","['Cornell Tech, United States of America and Brown University, USA', 'Cornell University, United States of America', 'Data & Society Research Institute, United States of America', 'Microsoft Research, USA', 'Cornell Tech, USA', 'Cornell University, United States of America']","['United States', 'United States', '', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658993,Fairness & Bias,A Robot Walks into a Bar: Can Language Models Serve as Creativity SupportTools for Comedy? An Evaluation of LLMs’ Humour Alignment with Comedians,"We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on “AI x Comedy” conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians’ motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to “cruise ship comedy material from the 1950s, but a bit less racist”. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, “offensive” language as a practice of resistance, satire and “punching up”. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists’ needs. Warning: this study may contain offensive language and discusses self-harm.","['Large Language Models', 'Comedy', 'Creativity', 'Offensive speech', 'Censorship', 'Value Alignment']",Applied computing → Performing arts Human-centered computing → Empirical studies in HCI Social and professional topics → Censoring filters,"['Piotr Mirowski', 'Juliette Love', 'Kory Mathewson', 'Shakir Mohamed']","['Google DeepMind, United Kingdom', 'Google DeepMind, United Kingdom', 'Google DeepMind, Canada', 'Google DeepMind, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3630106.3658996,Fairness & Bias,Careless Whisper: Speech-to-Text Hallucination Harms,"Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI’s Whisper, a state-of-the-art automated speech recognition service outperforming industry competitors, as of 2023. While many of Whisper’s transcriptions were highly accurate, we find that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as perpetuating violence, making up inaccurate associations, or implying false authority. We then study why hallucinations occur by observing the disparities in hallucination rates between speakers with aphasia (who have a lowered ability to express themselves using speech and voice) and a control group. We find that hallucinations disproportionately occur for individuals who speak with longer shares of non-vocal durations—a common symptom of aphasia. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases amplified by hallucinations in downstream applications of speech-to-text models.","['Automated Speech Recognition', 'Generative AI', 'Algorithmic Fairness', 'Thematic Coding']",Human-centered computing → Collaborative and social computing design and evaluation methods Applied computing → Sound and music computing,"['Allison Koenecke', 'Anna Seo Gyeong Choi', 'Katelyn X. Mei', 'Hilke Schellmann', 'Mona Sloane']","['Cornell University, USA', 'Cornell University, USA', 'University of Washington, USA', 'New York University, USA', 'University of Virginia, USA']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658997,Fairness & Bias,Actionable Recourse for Automated Decisions: Examining the Effects of Counterfactual Explanation Type and Presentation on Lay User Understanding,"Automated decision-making systems are increasingly deployed in domains such as hiring and credit approval where negative outcomes can have substantial ramifications for decision subjects. Thus, recent research has focused on providing explanations that help decision subjects understand the decision system and enable them to take actionable recourse to change their outcome. Popular counterfactual explanation techniques aim to achieve this by describing alterations to an instance that would transform a negative outcome to a positive one. Unfortunately, little user evaluation has been performed to assess which of the many counterfactual approaches best achieve this goal. In this work, we conduct a crowd-sourced between-subjects user study (N = 252) to examine the effects of counterfactual explanation type and presentation on lay decision subjects’ understandings of automated decision systems. We find that the region-based counterfactual type significantly increases objective understanding, subjective understanding, and response confidence as compared to the point-based type. We also find that counterfactual presentation significantly effects response time and moderates the effect of counterfactual type for response confidence, but not understanding. A qualitative analysis reveals how decision subjects interact with different explanation configurations and highlights unmet needs for explanation justification. Our results provide valuable insights and recommendations for the development of counterfactual explanation techniques towards achieving practical actionable recourse and empowering lay users to seek justice and opportunity in automated decision workflows.",[],[],"['Peter M. VanNostrand', 'Dennis M. Hofmann', 'Lei Ma', 'Elke A. Rundensteiner']","['Worcester Polytechnic Institute, United States', 'Worcester Polytechnic Institute, USA', 'Worcester Polytechnic Institute, USA', 'Worcester Polytechnic Institute, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3658998,Transparency & Explainability,Null Compliance: NYC Local Law 144 and the challenges of algorithm accountability,"In July 2023, New York City became the first jurisdiction globally to mandate bias audits for commercial algorithmic systems, specifically for automated employment decisions systems (AEDTs) used in hiring and promotion. Local Law 144 (LL 144) requires AEDTs to be independently audited annually for race and gender bias, and the audit report must be publicly posted. Additionally, employers are obligated to post a transparency notice with the job listing. In this study, 155 student investigators recorded 391 employers’ compliance with LL 144 and the user experience for prospective job applicants. Among these employers, 18 posted audit reports and 13 posted transparency notices. These rates could potentially be explained by a significant limitation in the accountability mechanisms enacted by LL 144. Since the law grants employers substantial discretion over whether their system is in scope of the law, a null result cannot be said to indicate non-compliance, a condition we call ""null compliance."" Employer discretion may also explain our finding that nearly all audits reported an impact factor over 0.8, a rule of thumb often used in employment discrimination cases. We also find that the benefit of LL 144 to ordinary job seekers is limited due to shortcomings in accessibility and usability. Our findings offer important lessons for policy-makers as they consider regulating algorithmic systems, particularly the degree of discretion to grant to regulated parties and the limitations of relying on transparency and end-user accountability.","['algorithm audit', 'compliance', 'transparency', 'bias']",Social and professional topics → Governmental regulations Computing methodologies → Artificial intelligence,"['Lucas Wright', 'Roxana Mika Muenster', 'Briana Vecchione', 'Tianyao Qu', 'Pika (Senhuang) Cai', 'Alan Smith', 'Jacob Comm 2450 Student Investigators', 'J. Nathan Metcalf', 'Lucas Matias']","['Citizens and Technology Lab, Cornell University, USA', 'Department of Communication, Cornell University, United States of America', 'Data & Society Research Institute, United States of America', 'Department of Sociology, Cornell University, United States of America', 'Department of Information Science, Cornell University, USA', 'Consumer Reports, USA', 'Cornell University, USA', 'Data & Society Research Institute, United States of America', 'Citizens and Technology Lab, Cornell University, USA']","['United States', 'United States', '', 'United States', 'United States', 'United States', 'United States', '', 'United States']"
https://doi.org/10.1145/3630106.3658999,Transparency & Explainability,SIDEs: Separating Idealization from Deceptive 'Explanations' in xAI,"Explainable AI (xAI) methods are important for establishing trust in using black-box models. However, recent criticism has mounted against current xAI methods that they disagree, are necessarily false, and can be manipulated, which has started to undermine the deployment of black-box models. Rudin (2019) goes so far as to say that we should stop using black-box models altogether in high-stakes cases because xAI explanations ‘must be wrong’. However, strict fidelity to the truth is historically not a desideratum in science. Idealizations–the intentional distortions introduced to scientific theories and models–are commonplace in the natural sciences and are seen as a successful scientific tool. Thus, it is not falsehood qua falsehood that is the issue. In this paper, I outline the need for xAI research to engage in idealization evaluation. Drawing on the use of idealizations in the natural sciences and philosophy of science, I introduce a novel framework for evaluating whether xAI methods engage in successful idealizations or deceptive explanations (SIDEs). SIDEs evaluates whether the limitations of xAI methods, and the distortions that they introduce, can be part of a successful idealization or are indeed deceptive distortions as critics suggest. I discuss the role that existing research can play in idealization evaluation and where innovation is necessary. Through a qualitative analysis we find that leading feature importance methods and counterfactual explanations are subject to idealization failure and suggest remedies for ameliorating idealization failure.","['explainable AI', 'idealization', 'philosophy of science', 'qualitative evaluation']",Computing methodologies → Machine learning Social and professional topics → Technology audits,['Emily Sullivan'],"['Utrecht University, Netherlands']",['Netherlands']
https://doi.org/10.1145/3630106.3659001,Transparency & Explainability,Algorithmic Harms and Algorithmic Wrongs,"New artificial intelligence (AI) systems grounded in machine learning are being integrated into our lives at a rapid rate, but not without consequence: scholars across domains have increasingly pointed out issues related to privacy, transparency, bias, discrimination, exploitation, and exclusion associated with algorithmic systems in both public and private sector contexts. Concerns surrounding the adverse impacts of these technologies have spurred discussion on the topics of algorithmic harm. However, the overwhelming majority of articles on said harms offer no definition as to what constitutes ‘harm’ in these contexts. This paper aims to address this omission by introducing one criterion for a suitable account of algorithmic harm. More specifically, we follow Joel Feinberg in understanding harms as distinct from wrongs, where only the latter necessarily carry a normative dimension. This distinction highlights issues in the current scholarship surrounding the conflation of algorithmic harms and wrongs. In response to these issues, we put forth two requirements for upholding the harms/wrongs distinction when analyzing the increasingly far-reaching impacts of these technologies and suggest how this distinction can be useful in design, engineering, and policymaking.",[],[],"['Nathalie Diberardino', 'Clair Baleshta', 'Luke Stark']","['Department of Philosophy, Western University, Canada', 'Department of Philosophy, Western University, Canada', 'Faculty of Information and Media Studies, Western University, Canada and Azrieli Global Scholars Program, Canadian Institute for Advanced Research (CIFAR), Canada']","['Canada', 'Canada', 'Canada']"
https://doi.org/10.1145/3630106.3659003,Transparency & Explainability,CARMA: A practical framework to generate recommendations for causal algorithmic recourse at scale,"Algorithms are increasingly used to automate large-scale decision-making processes, e.g., online platforms that make instant decisions in lending, hiring, and education. When such automated systems yield unfavorable decisions, it is imperative to allow for recourse by accompanying the instantaneous negative decisions with recommendations that can help affected individuals to overturn them. However, the practical challenges of providing algorithmic recourse in large-scale settings are not negligible: giving recourse recommendations that are actionable requires not only causal knowledge of the relationships between applicant features but also solving a complex combinatorial optimization problem for each rejected applicant. In this work, we introduce CARMA, a novel framework to generate causal recourse recommendations at scale. For practical settings with limited causal information, CARMA leverages pre-trained state-of-the-art causal generative models to find recourse recommendations. More importantly, CARMA addresses the scalability of finding these recommendations by casting the complex recourse optimization problem as a prediction task. By training a novel neural-network-based framework, CARMA efficiently solves the prediction task without requiring supervision for optimal recourse actions. Our extensive evaluations show that post-training, running inference on CARMA reliably amortizes causal recourse, generating optimal and instantaneous recommendations. CARMA exhibits flexibility, as its optimization is versatile with respect to the algorithmic decision-making and pre-trained causal generative models, provided their differentiability is ensured. Furthermore, we showcase CARMA in a case study, illustrating its ability to tailor causal recourse recommendations by readily incorporating population-level feature preferences based on factors such as difficulty or time needed.","['Recourse', 'Causality', 'Neural Networks', 'Counterfactual Explanations']",Social and professional topics Computing methodologies → Machine learning,"['Ayan Majumdar', 'Isabel Valera']","['MPI-SWS, Germany and Saarland University, Germany', 'Saarland University, Germany and MPI-SWS, Germany']","['Germany', 'Germany']"
https://doi.org/10.1145/3630106.3659006,Fairness & Bias,Group Fairness via Group Consensus,"Ensuring equitable impact of machine learning models across different societal groups is of utmost importance for real-world machine learning applications. Prior research in fairness has predominantly focused on adjusting model outputs through pre-processing, in-processing, or post-processing techniques. These techniques focus on correcting bias in either the data or the model. However, we argue that the bias in the data and model should be addressed in conjunction. To achieve this, we propose an algorithm called GroupDebias to reduce unfairness in the data in a model-guided fashion, thereby enabling models to exhibit more equitable behavior. Even though it is model-aware, the core idea of GroupDebias is independent of the model architecture, making it a versatile and effective approach that can be broadly applied across various domains and model types. Our method focuses on systematically addressing biases present in the training data itself by adaptively dropping samples that increase the biases in the model. Theoretically, the proposed approach enjoys a guaranteed improvement in demographic parity at the expense of a bounded reduction in balanced accuracy. A comprehensive evaluation of the GroupDebias algorithm through extensive experiments on diverse datasets and machine learning models demonstrates that GroupDebias consistently and significantly outperforms existing fairness enhancement techniques, achieving a more substantial reduction in unfairness with minimal impact on model performance.","['Fairness', 'Machine Learning', 'Historical Bias', 'Sampling']",Computing methodologies → Supervised learning Computing methodologies~Ensemble methods Computing methodologies → Artificial intelligence,"['Eunice Chan', 'Zhining Liu', 'Ruizhong Qiu', 'Yuheng Zhang', 'Ross Maciejewski', 'Hanghang Tong']","['University of Illinois, Urbana-Champaign, United States of America', 'University of Illinois, Urbana-Champaign, USA', 'University of Illinois, Urbana-Champaign, United States of America', 'University of Illinois, Urbana-Champaign, USA', 'Arizona State University, USA', 'University of Illinois, Urbana-Champaign, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659008,Transparency & Explainability,Intervening to Increase Community Trust for Fair Network Outcomes,"Refugees or immigrants who arrive in new countries often feel isolated. In this work, we examine how a resource-bounded public entity can make recommendations to increase integration of these new arrivals into a community. The community is made up of agents who engage in a strategic network formation process; agents join periodically — new arrivals are the refugees. The public entity meanwhile makes a limited number of edge-formation recommendations (according to its resource constraint) per iteration in order to increase integration of refugees. This work investigates the relationship between community trust and network fairness. First, we show that increasing the public entity’s resource allocation will not compensate for low trust in the community. Then, we introduce two trust-increasing interventions by the public entity: a targeted advertising campaign, and an announcement to increase transparency. We find that diverting a fraction (20%) of the public entity’s resources to a targeted advertising campaign can increase trust and fairness in the community, especially in low trust scenarios. We find that personalized, local announcements are more effective at increasing fairness than global announcements in low trust scenarios; they almost double our fairness metric in some cases. Importantly, the transparent announcement requires no extra resource expenditure on the part of the public entity. Our work underscores the importance of community trust — low trust cannot be compensated for with resources. This work provides theoretical support for these trust-increasing interventions, which we show can lead to increased integration of refugees in communities.","['trust', 'fairness', 'refugee resettlement', 'network formation']",Human-centered computing → Social recommendation Computing methodologies → Agent / discrete models Theory of computation → Social networks,"['Naina Balepur', 'Hari Sundaram']","['University of Illinois Urbana-Champaign, United States of America', 'University of Illinois Urbana-Champaign, USA']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3659010,Fairness & Bias,Unlawful Proxy Discrimination: A Framework for Challenging Inherently Discriminatory Algorithms,"Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts. This has important implications: unlike indirect discrimination, there is generally no ‘objective justification’ stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se. In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic. We draw on computer science literature to suggest that, in the algorithmic context, ‘treatment on the grounds of’ needs to be understood in terms of two steps: proxy capacity and proxy use. Only where both elements can be made out can direct discrimination be said to be ‘on grounds of’ a protected characteristic. We analyse the legal conditions of our proposed proxy capacity and proxy use tests. Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making.","['direct discrimination', 'disparate treatment', 'algorithmic fairness', 'machine learning', 'EU non-discrimination law', 'proxy discrimination']",Social and professional topics → Computing / technology policy Computing methodologies → Artificial intelligence Computing methodologies → Machine learning Applied computing → Law,"['Hilde Weerts', 'Aislinn Kelly-Lyth', 'Reuben Binns', 'Jeremias Adams-Prassl']","['Eindhoven University of Technology, Netherlands', 'Blackstone Chambers, United Kingdom', 'University of Oxford, United Kingdom', 'University of Oxford, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3630106.3659011,Transparency & Explainability,MiMICRI: Towards Domain-centered Counterfactual Explanations of Cardiovascular Image Classification Models,"The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis. At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs. However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge. In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models. MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures. From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts. We evaluate this library with two medical experts. Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge. However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated. We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts.","['explainable AI', 'human-centered AI', 'interactive visualizations', 'counterfactual explanation']",Human-centered computing → Visualization systems and tools Human-centered computing → Visual analytics Human-centered computing → Human computer interaction (HCI),"['Grace Guo', 'Lifu Deng', 'Animesh Tandon', 'Alex Endert', 'Bum Chul Kwon']","['Georgia Institute of Technology, United States of America', 'Cleveland Clinic, USA', 'Cleveland Clinic, USA', 'Georgia Institute of Technology, USA', 'IBM Research, United States of America']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659014,Fairness & Bias,Fairness without Sensitive Attributes via Knowledge Sharing,"While model fairness improvement has been explored previously, existing methods invariably rely on adjusting explicit sensitive attribute values in order to improve model fairness in downstream tasks. However, we observe a trend in which sensitive demographic information becomes inaccessible as public concerns around data privacy grow. In this paper, we propose a confidence-based hierarchical classifier structure called “Reckoner” for reliable fair model learning under the assumption of missing sensitive attributes. We first present results showing that if the dataset contains biased labels or other hidden biases, classifiers significantly increase the bias gap across different demographic groups in the subset with higher prediction confidence. Inspired by these findings, we devised a dual-model system in which a version of the model initialised with a high-confidence data subset learns from a version of the model initialised with a low-confidence data subset, enabling it to avoid biased predictions. Our experimental results show that Reckoner consistently outperforms state-of-the-art baselines in COMPAS dataset and New Adult dataset, considering both accuracy and fairness metrics.",[],[],"['Hongliang Ni', 'Lei Han', 'Tong Chen', 'Shazia Sadiq', 'Gianluca Demartini']","['University of Queensland, Australia', 'University of Queensland, Australia', 'University of Queensland, Australia', 'University of Queensland, Australia', 'University of Queensland, Australia']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://doi.org/10.1145/3630106.3659015,Fairness & Bias,The Conflict Between Algorithmic Fairness and Non-Discrimination: An Analysis of Fair Automated Hiring,"AI-based automated hiring systems cover a wide range of tools of varying complexity, from resume parsing tools to candidate selection models. Their close interference in economic and social life faces raising demands and investigations aiming to reduce the potential discrimination they may cause. This article covers the intersection of EU non-discrimination law and algorithmic fairness in the context of automated hiring systems. The paper analyzes the balance between equality of opportunity (formal and substantive) and equality of outcome, critiques the focus on non-conservative group fairness in machine learning, and discusses the legal implications of automated hiring systems under EU law. Additionally, it highlights often committed fallacies in relation to the process of de-biasing and advocates for a broader understanding of fairness in machine learning that aligns with EU legal standards and societal values.",[],[],"['Robert Lee Poe', 'Soumia Zohra El Mestari']","[""Laboratorio Interdisciplinare Diritti e Regole (LIDER-Lab), Sant'Anna School of Advanced Studies, Italy"", 'Interdisciplinary Research Group in Socio-technical Cybersecurity (IRISC), Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg']","['Italy', 'Luxembourg']"
https://doi.org/10.1145/3630106.3659016,Fairness & Bias,BaBE: Enhancing Fairness via Estimation of Explaining Variables,"We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each group. The decision can then be based directly on the estimated E. We show, by experiments on synthetic and real data sets, that our approach provides a good level of fairness as well as high accuracy.",[],[],"['Ruta Binkyte', 'Daniele Gorla', 'Catuscia Palamidessi']","['CISPA ? Helmholtz Center for Information Security, Germany', 'Università di Roma, Italy', 'Inria and École Polytechnique (IPP), France']","['United States', 'Italy', 'France']"
https://doi.org/10.1145/3630106.3659017,Fairness & Bias,Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology,"Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South. This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally. Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies. Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences. Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches. This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies. Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages. By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North.",[],[],"['Rishav Hada', 'Safiya Husain', 'Varun Gumma', 'Harshita Diddee', 'Aditya Yadavalli', 'Agrima Seth', 'Nidhi Kulkarni', 'Ujwal Gadiraju', 'Aditya Vashistha', 'Vivek Seshadri', 'Kalika Bali']","['Microsoft Research, India', 'Karya, India', 'Microsoft Research, India', 'Carnegie Mellon University, USA', 'Karya, India', 'University of Michigan, United States of America', 'Karya, India', 'Delft University of Technology, Netherlands', 'Cornell University, United States of America', 'Microsoft Research, India and Karya, India', 'Microsoft Research, India']","['India', 'India', 'India', 'India', 'India', '', 'India', 'India', 'India', 'India', 'India']"
https://doi.org/10.1145/3630106.3659018,Fairness & Bias,Balancing Act: Evaluating People’s Perceptions of Fair Ranking Metrics,"Algorithmic decision-making using rankings— prevalent in areas from hiring and bail to university admissions— raises concerns of potential bias. In this paper, we explore the alignment between people’s perceptions of fairness and two popular fairness metrics designed for rankings. In a crowdsourced experiment with 480 participants, people rated the perceived fairness of a hypothetical scholarship distribution scenario. Results suggest a strong inclination towards relying on explicit score values. There is also evidence of people’s preference for one fairness metric, NDKL, over the other metric, ARP. Qualitative results paint a more complex picture: some participants endorse meritocratic award schemes and express concerns about fairness metrics being used to modify rankings; while other participants acknowledge socio-economic factors in score-based rankings as justification for adjusting rankings. In summary, we find that operationalizing algorithmic fairness in practice is a balancing act between mitigating harms towards marginalized groups and societal conventions of leveraging traditional performance scores such as grades in decision-making contexts.",[],[],"['Mallak Alkhathlan', 'Kathleen Cachel', 'Hilson Shrestha', 'Lane Harrison', 'Elke Rundensteiner']","['Worcester Polytechnic Institute, United States', 'Worcester Polytechnic Institute, USA', 'Worcester Polytechnic Institute, USA', 'Worcester Polytechnic Institute, USA', 'Worcester Polytechnic Institute, USA']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659020,Fairness & Bias,From the Fair Distribution of Predictions to the Fair Distribution of Social Goods: Evaluating the Impact of Fair Machine Learning on Long-Term Unemployment,"Deploying an algorithmically informed policy is a significant intervention in society. Prominent methods for algorithmic fairness focus on the distribution of predictions at the time of training, rather than the distribution of social goods that arises after deploying the algorithm in a specific social context. However, requiring a ‘fair’ distribution of predictions may undermine efforts at establishing a fair distribution of social goods. First, we argue that addressing this problem requires a notion of prospective fairness that anticipates the change in the distribution of social goods after deployment. Second, we provide formal conditions under which this change is identified from pre-deployment data. That requires accounting for different kinds of performative effects. Here, we focus on the way predictions change policy decisions and, consequently, the causally downstream distribution of social goods. Throughout, we are guided by an application from public administration: the use of algorithms to predict who among the recently unemployed will remain unemployed in the long term and to target them with labor market programs. Third, using administrative data from the Swiss public employment service, we simulate how such algorithmically informed policies would affect gender inequalities in long-term unemployment. When risk predictions are required to be ‘fair’ according to statistical parity and equality of opportunity, targeting decisions are less effective, undermining efforts to both lower overall levels of long-term unemployment and to close the gender gap in long-term unemployment.","['Algorithmic Fairness', 'Inequality', 'Active Labor Market Programs', 'Performativity', 'Heterogeneous Treatment Effects']",Computing methodologies → Machine learning Computing methodologies → Philosophical/theoretical foundations of artificial intelligence Social and professional topics → Computing / technology policy,"['Sebastian Zezulka', 'Konstantin Genin']","['University of Tübingen, Germany', 'University of Tübingen, Germany']","['Germany', 'Germany']"
https://doi.org/10.1145/3630106.3659021,Security,Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates,"Recent years have seen substantial investments in AI-based tools designed to detect offensive language at scale, aiming to moderate social media platforms, and ensure safety of conversational AI technologies such as ChatGPT and Bard. These efforts largely treat this task as a technical endeavor, relying on data annotated for offensiveness by a global crowd workforce, without considering crowd workers’ socio-cultural backgrounds or the values their perceptions reflect. Existing research that examines systematic variations in annotators’ judgments often reduces these differences to socio-demographic categories along racial, or gender dimensions, overlooking the diversity of perspectives within such groups. On the other hand, social psychology literature highlights the crucial role that both cultural and psychological factors play in human perceptions and judgments. Through a large-scale cross-cultural study of 4309 participants from 21 countries across eight cultural regions, we demonstrate substantial cross-cultural and individual moral value-based differences in interpretations of offensiveness. Our study reveals specific regions that are significantly more sensitive to offensive language. Furthermore, using the Moral Foundations Theory, we study the underlying moral values that contribute to these cross-cultural differences. Notably, we find that participants’ moral values play a far more important role in shaping their perceptions of offensiveness than geo-cultural distinctions. Our investigation, using a non-monolithic framework to understand cross-cultural moral concerns, reveals crucial insights that can be extrapolated to building AI models for the pluralistic world. Our results call for more extensive consideration of diverse human moral values when deploying AI models across diverse geo-cultural contexts.","['Pluralism', 'Value Alignment', 'Annotation', 'Subjectivity', 'Offensiveness']",General and reference → General conference proceedings,"['Aida Davani', 'Mark Díaz', 'Dylan Baker', 'Vinodkumar Prabhakaran']","['Google Research, United States of America', 'Google Research, United States of America', 'Distributed AI Research Institute (DAIR), United States of America', 'Google, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659022,Fairness & Bias,"Perceptions of Policing Surveillance Technologies in Detroit: Moving Beyond ""Better than Nothing""","In Detroit, the largest Black-majority city in the United States, municipal authorities have deployed an array of surveillance technologies with the promise of containing crime and improving community safety. This article draws from a cross-sectional survey of over two thousand Detroit residents and multi-year community-based fieldwork in Detroit’s Eastside to examine local perceptions of policing surveillance technologies. Our survey reveals that respondents, notably those in more vulnerable positions, report higher perceived safety levels with policing surveillance cameras in their neighborhoods. However, when triangulating these results with insights from our fieldwork, we argue that these survey findings should not be taken as public support for surveillance. Alongside this seeming buy-in is a widely shared “better than nothing” imaginary among residents from impacted communities. “Better than nothing,” for the residents, is a pragmatic compromise and maneuver between being aware of the inherent flaws of surveillance technologies and settling for any available resource or hope. This notion of “better than nothing” unveils residents’ prolonged wait for digital justice and institutional accountability, which we show is where racialized infrastructural harm and exploitation are enacted along the temporal dimension. Our findings offer practical insights for counter-surveillance advocacy efforts.","['policing surveillance', 'Detroit', 'counter-surveillance advocacy', 'techno-failure', 'mixed methods']",Human-centered computing → Empirical studies in collaborative and social computing Social and professional topics → Surveillance,[],[],[]
https://doi.org/10.1145/3630106.3659024,Fairness & Bias,The unfair side of Privacy Enhancing Technologies: addressing the trade-offs between PETs and fairness,"Data sharing in the European Union (EU) has gained new momentum, among others for machine learning (ML) and artificial intelligence (AI) training purposes. By enabling models’ training whilst preserving the privacy of data, Privacy Enhancing Technologies (PETs) have therefore gained popularity, especially among policymakers. So far, computer science research has focused on advancing state-of-the-art privacy engineering and exploring trade-offs between privacy and accuracy. Meanwhile, legal scholarship began investigating the challenges arising therefrom. Yet, few works have delved into the fairness implications of PETs. Further research is essential to both prevent the propagation of bias and discrimination and to limit the accumulation of market power within very few economic entities suitable to undermine fair competition and consumer rights. In our work, we will address this knowledge gap by adopting a legal and computer science point of view. After scoping our understanding of possible unfair sides of PETs based on technical and socio-legal understandings of fairness (Section 2), we provide an overview of PETs mostly relevant for ML and AI training (Section 3). We then discuss fairness-related challenges arising from their use (Section 4) and we suggest possible technical and regulatory (e.g., impact assessment, new rights) solutions to address the shortcomings identified (Section 5). We finally provide conclusions and ideas for future research (Section 6).","['Security and privacy', 'Privacy-preserving protocols', 'Law', 'AI', 'competition', 'data protection', 'fairness', 'PETs', 'privacy']",,"['Alessandra Calvi', 'Gianclaudio Malgieri', 'Dimitris Kotzinos']","['LSTS, d.pia.lab, Vrije Universiteit Brussel, Belgium and Lab. ETIS UMR 8051, CY Cergy Paris University, ENSEA, CNRS, France', 'eLaw - Center for Law and Digital Technologies, Leiden University, Netherlands and Brussels Privacy Hub, Belgium', 'Lab. ETIS UMR 8051, CY Cergy Paris University, ENSEA, CNRS, France']","['France', 'France', 'France']"
https://doi.org/10.1145/3630106.3659025,Fairness & Bias,The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action,"Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to ‘algorithmic positive action’ under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of (fairness-aware) algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to ‘not do harm’ towards a positive obligation to actively ‘do no harm’ as a more adequate framework for algorithmic decision-making and fair ml-interventions.","['algorithmic decision-making', 'discrimination', 'positive action', 'EU law']",Social and professional topics → Computing / technology policy Computing methodologies → Artificial intelligence Computing methodologies → Machine learning Applied computing → Law,"['Hilde Weerts', 'Raphaële Xenidis', 'Fabien Tarissan', 'Henrik Palmer Olsen', 'Mykola Pechenizkiy']","['Eindhoven University of Technology, Netherlands', 'Sciences Po Law School, France', 'Université Paris-Saclay, CNRS, ISP, ENS Paris-Saclay, France', 'University of Copenhagen, Denmark and IEA-Paris, France', 'Eindhoven University of Technology, Netherlands']","['Netherlands', 'France', 'France', 'Denmark', 'Netherlands']"
https://doi.org/10.1145/3630106.3659027,Transparency & Explainability,"The Impact of iBuying is About More Than Just Racial Disparities: Evidence from Mecklenburg County, NC","Instant buyers (iBuyers)—companies that buy and sell homes based on automated valuation models (AVMs)—now hold more than 5% market share in some USA cities. In this work, we investigate the fairness of iBuyers by constructing a dataset that links racial demographics from voter records with detailed property information on over 50,000 real estate transactions. Using Bayesian hierarchical modeling we find that: 1. iBuyers Decrease the Racial Sales Price Gap Between Black and White Home Sellers. Controlling for over 50 property features we find that iBuyers reduce the racial price gap that otherwise exists between homes sold by Black and White homeowners. This is not, however, a result of equity achieved through proprietary AVMs, but rather a result of both Black and White homeowners being similarly disadvantaged by iBuyers’ low purchase prices; and, 2. iBuyers Increase Property Conversion Rates from Individual to Institutional Ownership. We trace iBuyers’ purchases as well as their follow-on sales of homes in Mecklenburg County. In doing so, we show that iBuyers increase the rate at which properties are converted from being individually owned to institutionally owned. The eventual purchasers of iBuyer homes include national and international rental companies that have been tied to high eviction rates and poor property management. As with sale prices, we find that rather than reapportioning this social harm more equitably, iBuyers are simply increasing the rate at which homes bought from White homeowners are converted to institutional ownership. Ultimately, our analysis suggests that iBuyers are Equalizing Housing Outcomes by Extending Real Estate Harms Typically Isolated to Black Homeowners to White homeowners as Well.","['iBuying', 'Automated Valuation Models', 'Algorithm Auditing', 'Bayesian Hierarchical Modeling', 'Housing Discrimination']",Applied computing → Economics Social and professional topics → Race and ethnicity Social and professional topics → Socio-technical systems Social and professional topics → Technology audits,"['Isaac Slaughter', 'Eva Maxfield Brown', 'Nic Weber']","['University of Washington, USA', 'University of Washington, USA', 'University of Washington, United States of America']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659028,Transparency & Explainability,Regulating Explainability in Machine Learning Applications -- Observations from a Policy Design Experiment,"With the rise of artificial intelligence (AI), concerns about AI applications causing unforeseen harms to safety, privacy, security, and fairness are intensifying. While attempts to create regulations are underway, with initiatives such as the EU AI Act and the 2023 White House executive order, skepticism abounds as to the efficacy of such regulations. This paper explores an interdisciplinary approach to designing policy for the explainability of AI applications, as the widely discussed ""right to explanation"" associated with the EU General Data Protection Regulation is ambiguous. To develop practical guidance for explainability, we conducted an experimental study that involved continuous collaboration among a team of researchers with AI and policy backgrounds over the course of ten weeks. The objective was to determine whether, through interdisciplinary effort, we can reach consensus on a policy for explainability in AI–one that is clearer, and more actionable and enforceable than current guidelines. We share nine observations, derived from an iterative policy design process, which included drafting the policy, attempting to comply with it (or circumvent it), and collectively evaluating its effectiveness on a weekly basis. Key observations include: iterative and continuous feedback was useful to improve policy drafts over time, discussing evidence of compliance was necessary during policy design, and human-subject studies were found to be an important form of evidence. We conclude with a note of optimism, arguing that meaningful policies can be achieved within a moderate time frame and with limited experience in policy design, as demonstrated by our student researchers on the team. This holds promising implications for policymakers, signaling that practical and effective regulation for AI applications is attainable.",[],Software and its engineering → Collaboration in software development Computing methodologies → Machine learning Applied computing → Law,"['Nadia Nahar', 'Jenny Rowlett', 'Matthew Bray', 'Zahra Abba Omar', 'Xenophon Papademetris', 'Alka Menon', 'Christian Kästner']","['Software and Societal Systems Department, School of Computer Science, Carnegie Mellon University, United States', 'Oberlin College, USA', 'Yale University, USA', 'Yale University, USA', 'Yale University, USA', 'Yale University, USA', 'Carnegie Mellon University, United States']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659029,Fairness & Bias,Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias,"Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.",[],[],"['Sierra Wyllie', 'Ilia Shumailov', 'Nicolas Papernot']","['University of Toronto and Vector Institute, Canada', 'University of Oxford, United Kingdom', 'University of Toronto and Vector Institute, Canada']","['Canada', 'Canada', 'Canada']"
https://doi.org/10.1145/3630106.3659030,Fairness & Bias,"Data, Annotation, and Meaning-Making: The Politics of Categorization in Annotating a Dataset of Faith-based Communal Violence","Data annotation is a process of meaning-making and is inherently political. The literature on ethics in data-driven technologies explores these political aspects, primarily focusing on questions of bias and power. This paper argues that the politics of annotation often overemphasize secular and modern values and overlooks faith-based, religious, and spiritual aspects (FRS) in data annotation. This oversight particularly affects the postcolonial regions of the Global South, where FRS are intertwined with people’s everyday experiences and ethics. We conducted a focus group discussion and contextual inquiries with six annotators who annotated a faith-related “violence” dataset from South Asian YouTube content. Our analysis reveals that FRS blindness in data annotation manifests through the politics of achieving objectivity and the “scientific” process of meaning-making. Due to these goals, which are predominantly shaped by Western values, FRS sensitivities are overlooked from the initial stages of data curation through annotation, ultimately leading to a context collapse within the annotation process. Finally, we advocate for the adaptation of FRS sensitivities into the annotation process and data infrastructure, particularly when the dataset clearly pertains to FRS, to promote greater cultural and contextual inclusivity in annotation practices.",[],[],"['Mohammad Rashidujjaman Rifat', 'Abdullah Hasan Safir', 'Sourav Saha', 'Jahedul Alam Junaed', 'Maryam Saleki', 'Mohammad Ruhul Amin', 'Syed Ishtiaque Ahmed']","['Computer Science, University of Toronto, Canada', 'Leverhulme Centre for the Future of Intelligence, University of Cambridge, United Kingdom', 'Computer Science & Engineering, Shahjalal University Of Science And Technology, Bangladesh', 'Computer Science and Engineering, Shahjalal University of Science and Technology, Bangladesh', 'Computer and Info Science, Fordham University, USA', 'Computer and Information Science, Fordham University, USA', 'Computer Science, University of Toronto, Toronto, Canada, Canada']","['Canada', 'United Kingdom', 'Bangladesh', 'Bangladesh', 'United States', 'United States', 'Canada']"
https://doi.org/10.1145/3630106.3659031,Transparency & Explainability,The Role of Explainability in Collaborative Human-AI Disinformation Detection,"Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users’ background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.","['Collaborative disinformation detection', 'XAI', 'transparent AI systems', 'expert and lay people evaluation']",Human-centered computing → Empirical studies in HCI Human-centered computing → HCI design and evaluation methods Information systems → Decision support systems Human-centered computing → User studies Human-centered computing → Empirical studies in collaborative and social computing Human-centered computing → Interactive systems and tools,"['Vera Schmitt', 'Luis-Felipe Villa-Arenas', 'Nils Feldhus', 'Joachim Meyer', 'Robert P. Spang', 'Sebastian Möller']","['TU Berlin, DFKI, Germany', 'Telekom, TU Berlin, Germany', 'DFKI, Germany', 'Tel Aviv University, Israel', 'TU Berlin, Germany', 'TU Berlin, DFKI, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3630106.3659032,Transparency & Explainability,An Information Bottleneck Characterization of the Understanding-Workload Tradeoff in Human-Centered Explainable AI,"Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.","['explainable AI', 'workload', 'human factors', 'information bottleneck']","Human-centered computing → User studies Human-centered computing → User models Human-centered computing → HCI theory, concepts and models Human-centered computing → Empirical studies in HCI Mathematics of computing → Information theory","['Lindsay Sanneman', 'Mycal Tucker', 'Julie A. Shah']","['CSAIL, Massachusetts Institute of Technology, United States of America', 'CSAIL, Massachusetts Institute of Technology, USA', 'CSAIL, Massachusetts Institute of Technology, USA']","['United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659033,Transparency & Explainability,A Critical Analysis of the Largest Source for Generative AI Training Data: Common Crawl,"Common Crawl is the largest freely available collection of web crawl data and one of the most important sources of pre-training data for large language models (LLMs). It is used so frequently and makes up such large proportions of the overall pre-training data in many cases that it arguably has become a foundational building block for LLM development, and subsequently generative AI products built on top of LLMs. Despite its pivotal role, Common Crawl itself is not widely understood, nor is there much reflection evident among LLM builders about the implications of using Common Crawl's data. This paper discusses what Common Crawl's popularity for LLM development means for fairness, accountability, and transparency in generative AI by highlighting the organization's values and practices, as well as how it views its own role within the AI ecosystem. Our qualitative analysis is based on in-depth interviews with Common Crawl staffers and relevant online documents.",[],[],['Stefan Baack'],"['Mozilla Foundation, Germany']",['Germany']
https://doi.org/10.1145/3630106.3659034,Transparency & Explainability,Seeing through opacity: The limitations of digital ad transparency in Brazil,"Digital platforms provide a deregulated and opaque environment suited to the maintenance of their business model, in which ads are efficiently served by opaque algorithms to meticulously profiled users based on their behavioral data. The advertising infrastructure provided by these platforms made advertising more segmented and scalable, creating new opportunities and allowing for a profit-oriented influence industry to develop worldwide. Some platforms have invested in transparency measures for digital advertising, but there is still a gap between what is applied in the Global South and the Global North. In Brazil, despite evidence of an online ecosystem of suspicious, inauthentic, scam, and other types of fraudulent ads, regulatory proposals have faced a hard opposition from tech companies. Against this backdrop, there is a need to evaluate advertising transparency archives currently offered by online platforms in Brazil as a means to measure the quality of libraries and the available data.","['Advertising regulation', 'Data access', 'Political advertising', 'Transparency']",Information systems → Computational advertising Information systems → Digital libraries and archives Human-centered computing → Social networking sites,"['Rose Marie Santini', 'Débora Salles', 'Bruno Maurício Martins', 'Alékis Moreira', 'João Gabriel Haddad']","['School of Communication, Federal University of Rio de Janeiro, Brazil', 'School of Communication, Federal University of Rio de Janeiro, Brazil', 'School of Communication, Federal University of Rio de Janeiro, Brazil', 'Fluminense Federal University, Brazil', 'School of Communication, Federal University of Rio de Janeiro, Brazil']","['Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil']"
https://doi.org/10.1145/3630106.3659036,Transparency & Explainability,Algorithmic Arbitrariness in Content Moderation,"Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices made during training, which do not meaningfully change the accuracy of the ML model, but can nevertheless change what the model gets wrong. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as “toxic,” leading to arbitrary restrictions on speech. We use the principles set by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice to interpret the effects of these findings in terms of Human Rights. We analyze (i) the extent of predictive multiplicity among popular state-of-the-art LLMs used for detecting “toxic” content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) the magnitude of model multiplicity on content that is unanimously recognized as toxic by human annotators. Our findings indicate that the up-scaled algorithmic moderation risks legitimizing an “algorithmic leviathan”, where an algorithm disproportionately manages human rights. To mitigate such risks, our study underscores the need to identify and increase the transparency of arbitrariness in content moderation applications. Our findings have implications to content moderation and intermediary liability laws being discussed and passed in many countries, such as the Digital Services Act in the European Union, the Online Safety Act in the United Kingdom, and the recent TSE resolutions in Brazil.","['content moderation', 'predictive multiplicity', 'Rashomon effect']","Social and professional topics → Hate speech Applied computing → Law, social and behavioral sciences Computing methodologies → Natural language processing","['Juan Felipe Gomez', 'Caio Machado', 'Lucas Monteiro Paes', 'Flavio Calmon']","['Department of Physics, Harvard University, United States of America', 'Centre for Socio-Legal Studies, School of Engineering and Applied Sciences, Law School, Oxford University; Harvard University; Universidade de São Paulo, United Kingdom', 'School of Engineering and Applied Sciences, Harvard University, United States of America', 'School of Engineering and Applied Sciences, Harvard University, United States of America']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659037,Transparency & Explainability,Black-Box Access is Insufficient for Rigorous AI Audits,"External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system’s inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.","['Auditing', 'Evaluation', 'Governance', 'Regulation', 'Policy', 'Risk', 'Fairness', 'Black-Box Access', 'White-Box Access', 'Adversarial Attacks', 'Interpretability', 'Explainability', 'Fine-Tuning']",Security and privacy~Social aspects of security and privacy Social and professional topics → Governmental regulations,"['Stephen Casper', 'Carson Ezell', 'Charlotte Siegmann', 'Noam Kolt', 'Taylor Lynn Curtis', 'Benjamin Bucknall', 'Andreas Haupt', 'Kevin Wei', 'Jérémy Scheurer', 'Marius Hobbhahn', 'Lee Sharkey', 'Satyapriya Krishna', 'Marvin Von Hagen', 'Silas Alberti', 'Alan Chan', 'Qinyi Sun', 'Michael Gerovitch', 'David Bau', 'Max Tegmark', 'David Krueger', 'Dylan Hadfield-Menell']","['Massachusetts Institute of Technology, United States of America', 'Harvard University, USA', 'Massachusetts Institute of Technology, USA', 'University of Toronto, Canada', 'Massachusetts Institute of Technology, United States of America', 'Centre for the Governance of AI, United Kingdom', 'Massachusetts Institute of Technology, United States of America', 'Harvard University, USA', 'Apollo Research, United Kingdom', 'Apollo Research, United Kingdom', 'Apollo Research, United Kingdom', 'Harvard University, United States of America', 'Massachusetts Institute of Technology, United States of America', 'Stanford University, USA', 'Centre for the Governance of AI, Mila (Quebec AI Institute), Canada', 'Massachusetts Institute of Technology, USA', 'Massachusetts Institute of Technology, USA', 'Northeastern University, USA', 'Massachusetts Institute of Technology, USA', 'University of Cambridge, United Kingdom', 'Massachusetts Institute of Technology, United States of America']","['United States', 'United States', 'United States', 'United States', '', 'United States', '', 'United States', 'United States', 'United States', 'United States', 'United States', '', 'United States', '', 'United States', 'United States', 'United States', 'United States', 'United States', '']"
https://doi.org/10.1145/3630106.3659038,Security,"Attitudes Toward Facial Analysis AI: A Cross-National Study Comparing Argentina, Kenya, Japan, and the USA","Computer vision AI systems present one of the most radical technical transformations of our time. Such systems are given unparalleled epistemic power to impose meaning on visual data, despite their inherent semantic ambiguity. This epistemic power is particularly evident in computer vision AI that interprets the meaning of human faces. The goal of this work is to empirically document laypeople’s perceptions of the epistemic and ethical complexity of computer vision AI through a large-scale qualitative study with participants in Argentina, Japan, Kenya, and the USA (N=4,468). We developed a vignette scenario about a fictitious company that analyzes people’s portraits using computer vision AI to make a variety of inferences about people based on their faces. For each inference that the fictitious company draws (e.g., age, skin color, intelligence), we ask participants from all countries to reason about how they evaluate computer vision AI inference-making. In a series of workshops, we collaborated as a multinational research team to develop a codebook that captures people’s different justifications of facial analysis AI inferences to create a comprehensive justification portfolio. Our study reveals similarities in justification patterns, but also significant intra-country and inter-country diversity in response to different facial inferences. For example, participants from Argentina, Japan, Kenya, and the USA vastly disagree over the reasonableness of AI classifications such as beautiful or skin color. They tend to agree in their opposition to AI-drawn inferences intelligence and trustworthiness. Adding much-needed non-Western perspectives to debates on computer vision ethics, our results suggest that, contrary to popular justifications for facial classification technologies, there is no such thing as a “common sense” facial classification that accords simply with a general, homogeneous “human intuition.”","['artificial intelligence', 'computer vision', 'facial analysis AI', 'human faces', 'participatory AI ethics']",Computing methodologies → Computer vision Social and professional topics → User characteristics Security and privacy~Human and societal aspects of security and privacy,"['Chiara Ullstein', 'Severin Engelmann', 'Orestis Papakyriakopoulos', 'Yuko Ikkatai', 'Naira Paola Arnez-Jordan', 'Rose Caleno', 'Brian Mboya', 'Shuichiro Higuma', 'Tilman Hartwig', 'Hiromi Yokoyama', 'Jens Grossklags']","['Chair of Cyber Trust, Technical University of Munich, Germany', 'Digital Life Initiative, Cornell Tech, United States of America', 'Professorship of Societal Computing, Technical University of Munich, Germany', 'Kanazawa University, Japan', 'Technical University of Munich, Germany', 'Unaffiliated, Kenya', 'Computer Science, Dedan Kimathi University of Technology, Kenya', 'The University of Tokyo, Japan', 'AI Lab, Umweltbundesamt, Germany', 'Kavli IPMU, CD3, The University of Tokyo, Japan', 'Chair of Cyber Trust, Technical University of Munich, Germany']","['Germany', 'United States', 'Germany', 'Japan', 'Germany', 'Kenya', 'Kenya', 'Japan', 'Germany', 'Japan', 'Germany']"
https://doi.org/10.1145/3630106.3659039,Transparency & Explainability,Embracing Diversity: Interpretable Zero-shot Classification Beyond One Vector Per Class,"Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today’s best models exhibit skewed performance when objects are dissimilar from their typical depiction. Real world objects such as pears appear in a variety of forms — from diced to whole, on a table or in a bowl — yet standard VLM classifiers map all instances of a class to a single vector based on the class label. We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector. We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining. We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent. Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency. We also find our method scales efficiently to a large number of attributes to account for diversity—leading to more accurate predictions for atypical instances. Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method. We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance.","['Bias', 'Fairness', 'Vision Language Models (VLMs)', 'Zero-shot', 'Classification']",Computing methodologies → Artificial intelligence Computing methodologies → Machine learning,"['Mazda Moayeri', 'Michael Rabbat', 'Mark Ibrahim', 'Diane Bouchacourt']","['University of Maryland, United States of America', 'Meta AI, Canada', 'Meta AI, USA', 'Meta AI, Canada']","['United States', 'Canada', 'Canada', 'Canada']"
https://doi.org/10.1145/3630106.3659041,Transparency & Explainability,Auditing for Racial Discrimination in the Delivery of Education Ads,"Digital ads on social-media platforms play an important role in shaping access to economic opportunities. Our work proposes and implements a new third-party auditing method that can evaluate racial bias in the delivery of ads for education opportunities. Third-party auditing is important because it allows external parties to demonstrate presence or absence of bias in social-media algorithms. Education is a domain with legal protections against discrimination and concerns of racial-targeting, but bias induced by ad delivery algorithms has not been previously explored in this domain. Prior audits demonstrated discrimination in platforms’ delivery of ads to users for housing and employment ads. These audit findings supported legal action that prompted Meta to change their ad-delivery algorithms to reduce bias, but only in the domains of housing, employment, and credit. In this work, we propose a new methodology that allows us to measure racial discrimination in a platform’s ad delivery algorithms for education ads. We apply our method to Meta using ads for real schools and observe the results of delivery. We find evidence of racial discrimination in Meta’s algorithmic delivery of ads for education opportunities, posing legal and ethical concerns. Our results extend evidence of algorithmic discrimination to the education domain, showing that current bias mitigation mechanisms are narrow in scope, and suggesting a broader role for third-party auditing of social media in areas where ensuring non-discrimination is important.","['algorithmic auditing', 'targeted advertising', 'ad delivery', 'education ads', 'racial discrimination']",Social and professional topics → Technology audits Social and professional topics → Socio-technical systems Social and professional topics → Systems analysis and design,"['Basileal Imana', 'Aleksandra Korolova', 'John Heidemann']","['Princeton University, USA', 'Princeton University, USA', 'University of Southern California / Information Sciences Institute, United States of America']","['United States', 'United States', '']"
https://doi.org/10.1145/3630106.3659042,Security,Analyzing And Editing Inner Mechanisms of Backdoored Language Models,"Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets. Trigger warning: Offensive language.",[],[],"['Max Lamparth', 'Anka Reuel']","['Stanford University, USA', 'Stanford University, USA']","['United States', 'United States']"
https://doi.org/10.1145/3630106.3659043,Fairness & Bias,Understanding Disparities in Post Hoc Machine Learning Explanation,"Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across “race” and “gender” as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. We also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the Adult income dataset. Overall, results indicate that disparities in model explanations can also depend on data and model properties. Based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.",[],[],"['Vishwali Mhasawade', 'Salman Rahman', 'Zoé Haskell-Craig', 'Rumi Chunara']","['New York University, United States of America', 'New York University, United States of America', 'New York University, USA', 'New York University, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659044,Fairness & Bias,Recommend Me? Designing Fairness Metrics with Providers,"Fairness metrics have become a useful tool to measure how fair or unfair a machine learning system may be for its stakeholders. In the context of recommender systems, previous research has explored how various stakeholders experience algorithmic fairness or unfairness, but it is also important to capture these experiences in the design of fairness metrics. Therefore, we conducted four focus groups with providers (those whose items, content, or profiles are being recommended) of two different domains: content creators and dating app users. We explored how our participants experience unfairness on their associated platforms, and worked with them to co-design fairness goals, definitions, and metrics that might capture these experiences. This work represents an important step towards designing fairness metrics with the stakeholders who will be impacted by their operationalizations. We analyze the efficacy and challenges of enacting these metrics in practice and explore how future work might benefit from this methodology.","['Fairness', 'Recommendation', 'Focus Groups', 'Metrics']",Information systems Human-centered computing → User studies,"['Jessie J. Smith', 'Aishwarya Satwani', 'Robin Burke', 'Casey Fiesler']","['University of Colorado Boulder, United States of America', 'University of Colorado Boulder, USA', 'University of Colorado Boulder, United States of America', 'University of Colorado Boulder, USA']","['United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659046,Transparency & Explainability,Drivers and persuasive strategies to influence user intention to learn about manipulative design,"The proliferation of e-commerce, game, and social networking sites, has brought to light the use of ""dark patterns"" or, more generally, manipulative designs (MDs), which exploit psychological effects and cognitive biases of users to channel their behavior toward outcomes that benefit the company or owner of the site, against the users’ best interests. Previous research has categorized MDs, assessed their impact on users, gauged their prevalence, and attempted automated detection using computer vision and natural language processing techniques. However, limited attention has been given to understanding how to warn and educate users about MDs, guiding them to recognize and resist such manipulative tactics. To address this gap, we carried out a controlled study with n=134 participants, using a survey based on the Protection Motivation Theory (PMT) to better understand the motivations of people to learn about MDs. We also explored the effectiveness of two persuasive strategies, based on Cialdini’s principles of influence (social influence and authority), to trigger attention towards MDs and intention to learn more about MDs and to avoid them. For this, we created a simulated application in a mobile app distribution platform modeled like Google Play Store containing a visual signal, a warning based on one of the two strategies, and simulated reviews from other users. The results indicate that two of the five PMT constructs - a higher Perceived Severity of MDs and a lower Perceived Response Cost of learning about MDs - have the most significant influence on the Intention to learn more about MDs. The participants in the experimental group, exposed to the two persuasive strategies exhibited a larger increase in their intention to seek information about MDs than the participants in the control group. Our study showcases the potential of a persuasive intervention, illustrating how mobile app distribution platforms can enhance user protection against MD exploitation. By implementing such interventions, these platforms can boost accountability and transparency of applications existing on their platform, and MD awareness among their users.",[],[],"['Pooria Babaei', 'Julita Vassileva']","['University of Saskatchewan, Canada', 'University of Saskatchewan, Canada']","['Canada', 'Canada']"
https://doi.org/10.1145/3630106.3659048,Privacy & Data Governance,"(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice","Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore when and why LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries (“cases”) allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts’ recommendations for LLM response strategies, which center around helping users identify ‘right questions to ask’ and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.","['large language model (LLM)', 'human-AI interaction', 'case-based reasoning', 'legal advice', 'professional ethics', 'AI chatbots', 'AI policy', 'AI ethics', 'lawyers', 'responsible AI', 'AI regulation']","Computing methodologies → Discourse, dialogue and pragmatics Applied computing → Law","['Inyoung Cheong', 'King Xia', 'K. J. Kevin Feng', 'Quan Ze Chen', 'Amy X. Zhang']","['University of Washington, United States', 'Independent Attorney, USA', 'University of Washington, United States', 'University of Washington, USA', 'University of Washington, USA']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://doi.org/10.1145/3630106.3659049,Fairness & Bias,Transforming Dutch: Debiasing Dutch Coreference Resolution Systems for Non-binary Pronouns,"Gender-neutral pronouns are increasingly being introduced across Western languages. Recent evaluations have however demonstrated that English NLP systems are unable to correctly process gender-neutral pronouns, with the risk of erasing and misgendering non-binary individuals. This paper examines a Dutch coreference resolution system’s performance on gender-neutral pronouns, specifically hen and die. In Dutch, these pronouns were only introduced in 2016, compared to the longstanding existence of singular they in English. We additionally compare two debiasing techniques for coreference resolution systems in non-binary contexts: Counterfactual Data Augmentation (CDA) and delexicalisation. Moreover, because pronoun performance can be hard to interpret from a general evaluation metric like lea, we introduce an innovative evaluation metric, the pronoun score, which directly represents the portion of correctly processed pronouns. Our results reveal diminished performance on gender-neutral pronouns compared to gendered counterparts. Nevertheless, although delexicalisation fails to yield improvements, CDA substantially reduces the performance gap between gendered and gender-neutral pronouns. We further show that CDA remains effective in low-resource settings, in which a limited set of debiasing documents is used. This efficacy extends to previously unseen neopronouns, which are currently infrequently used but may gain popularity in the future, underscoring the viability of effective debiasing with minimal resources and low computational costs.",[],[],"['Goya van Boven', 'Yupei Du', 'Dong Nguyen']","['Utrecht University, Netherlands', 'Utrecht University, Netherlands', 'Utrecht University, Netherlands']","['Netherlands', 'Netherlands', 'Netherlands']"
https://doi.org/10.1145/3630106.3659050,Fairness & Bias,Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter and What They Represent,"Racial diversity has become increasingly discussed within the AI and algorithmic fairness literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories. Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model. An unclear understanding of who comprises the racial categories chosen and how people are racialized into these categories can lead to varying interpretations of these categories. These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used. Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied.","['racial categories', 'racialization', 'algorithmic fairness', 'race and ethnicity']",,['Jennifer Mickel'],"['The University of Texas at Austin, United States of America']",['United States']