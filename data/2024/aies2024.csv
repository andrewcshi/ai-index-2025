link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://ojs.aaai.org/index.php/AIES/article/view/31612,Fairness & Bias,PoliTune: Analyzing the Impact of Data Selection and Fine-Tuning on Economic and Political Biases in Large Language Models,"In an era where language models are increasingly integrated into decision-making and communication, understanding the biases within Large Language Models (LLMs) becomes imperative, especially when these models are applied in the economic and political domains. This work investigates the impact of fine-tuning and data selection on economic and political biases in LLMs. In this context, we introduce PoliTune, a fine-tuning methodology to explore the systematic aspects of aligning LLMs with specific ideologies, mindful of the biases that arise from their extensive training on diverse datasets. Distinct from earlier efforts that either focus on smaller models or entail resource-intensive pre-training, PoliTune employs Parameter-Efficient Fine-Tuning (PEFT) techniques, which allow for the alignment of LLMs with targeted ideologies by modifying a small subset of parameters. We introduce a systematic method for using the open-source LLM Llama3-70B for dataset selection, annotation, and synthesizing a preferences dataset for Direct Preference Optimization (DPO) to align the model with a given political ideology. We assess the effectiveness of PoliTune through both quantitative and qualitative evaluations of aligning open-source LLMs (Llama3-8B and Mistral-7B) to different ideologies. Our work analyzes the potential of embedding specific biases into LLMs and contributes to the dialogue on the ethical application of AI, highlighting the importance of deploying AI in a manner that aligns with societal values.",[],[],"['Ahmed Agiza', 'Mohamed Mostagir', 'Sherief Reda']","['Brown University', 'University of Michigan', 'Brown University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31617,Fairness & Bias,Nothing Comes Without Its World – Practical Challenges of Aligning LLMs to Situated Human Values through RLHF,"Work on value alignment aims to ensure that human values are respected by AI systems. However, existing approaches tend to rely on universal framings of human values that obscure the question of which values the systems should capture and align with, given the variety of operational situations. This often results in AI systems that privilege only a selected few while perpetuating problematic norms grounded on biases, ultimately causing equity and justice issues. In this perspective paper, we unpack the limitations of predominant alignment practices of reinforcement learning from human feedback (RLHF) for LLMs through the lens of situated values. We build on feminist epistemology to argue that at the design-time, RLHF has problems with representation in the subjects providing feedback and implicitness in the conceptualization of values and situations of real-world users while lacking system adaptation to real user situations at the use time. To address these shortcomings, we propose three research directions: 1) situated annotation to capture information about the crowdworker’s and user’s values and judgments in relation to specific situations at both the design and use-time, 2) expressive instruction to encode plural values for instructing LLMs systems at design-time, and 3) reflexive adaptation to leverage situational knowledge for system adaption at use-time. We conclude by reflecting on the practical challenges of pursuing these research directions and situated value alignment of AI more broadly.",[],[],"['Anne Arzberger', 'Stefan Buijsman', 'Maria Luce Lupetti', 'Alessandro Bozzon', 'Jie Yang']","['Delft University of Technology', 'Delft University of Technology', 'Politechnic University of Turin', 'Delft University of Technology', 'Delft University of Technology']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AIES/article/view/31620,Transparency & Explainability,Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation,"The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.",[],[],"['Julia Barnett', 'Kimon Kieslich', 'Nicholas Diakopoulos']","['Northwestern University', 'Institute for Information Law, University of Amsterdam', 'Northwestern University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31616,Fairness & Bias,Understanding Intrinsic Socioeconomic Biases in Large Language Models,"Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications. Warning: This paper discusses and contains content that can be offensive or upsetting.",[],[],"['Mina Arzaghi', 'Florian Carichon', 'Golnoosh Farnadi']","['HEC Montreal\nMila', 'HEC Montreal', 'McGill University\nMila']","['Canada', 'Canada', 'Canada']"
https://ojs.aaai.org/index.php/AIES/article/view/31624,Transparency & Explainability,A Formal Account of Trustworthiness: Connecting Intrinsic and Perceived Trustworthiness,"This paper proposes a formal account of AI trustworthiness, connecting both intrinsic and perceived trustworthiness in an operational schematization. We argue that trustworthiness extends beyond the inherent capabilities of an AI system to include significant influences from observers' perceptions, such as perceived transparency, agency locus, and human oversight. While the concept of perceived trustworthiness is discussed in the literature, few attempts have been made to connect it with the intrinsic trustworthiness of AI systems. Our analysis introduces a novel schematization to quantify trustworthiness by assessing the discrepancies between expected and observed behaviors and how these affect perceived uncertainty and trust. The paper provides a formalization for measuring trustworthiness, taking into account both perceived and intrinsic characteristics. By detailing the factors that influence trust, this study aims to foster more ethical and widely accepted AI technologies, ensuring they meet both functional and ethical criteria.",[],[],"['Piercosma Bisconti', 'Letizia Aquilino', 'Antonella Marchetti', 'Daniele Nardi']","['Consorzio Interuniversitario Nazionale per l’Informatica, AIIS Lab', 'Università Cattolica del Sacro Cuore, CeRiToM - UniToM - Department of Psychology', 'Università Cattolica del Sacro Cuore, CeRiToM - UniToM - Department of Psychology', 'Sapienza University of Rome, Dipartimento di Ingegneria Informatica, Automatica e Gestionale ""A. Ruberti""']","['Italy', 'Italy', 'Italy', 'Italy']"
https://ojs.aaai.org/index.php/AIES/article/view/31622,Fairness & Bias,Gender in Pixels: Pathways to Non-binary Representation in Computer Vision,"In the field of Computer Vision (CV), the study of bias, including gender bias, has received a significant area of attention in recent years. However, these studies predominantly operate within a binary, cisnormative framework, often neglecting the complexities of non-binary gender identities. To date, there is no comprehensive analysis of how CV is addressing the mitigation of bias for non-binary individuals or how it seeks solutions that transcend a binary view of gender. This systematic scoping review aims to fill this gap by analyzing over 60 papers that delve into gender biases in CV, with a particular emphasis on non-binary perspectives. Our findings indicate that despite the increasing recognition of gender as a multifaceted and complex construct, practical applications of this understanding in CV remain limited and fragmented. The review critically examines the foundational research critiquing the binarism in CV and explores emerging approaches that challenge and move beyond this limited perspective. We highlight innovative solutions, including algorithmic adaptations and the creation of more inclusive and diverse datasets. Furthermore, the study emphasizes the importance of integrating gender theory into CV practices to develop more accurate and representative models. Our recommendations advocate for interdisciplinary collaboration, particularly with Gender Studies, to foster a more nuanced understanding of gender in CV. This study serves as a pivotal step towards redefining gender representation in CV, encouraging researchers and practitioners to embrace and incorporate a broader spectrum of gender identities in their work.",[],[],['Elena Beretta'],['Vrije Universiteit Amsterdam'],['Netherlands']
https://ojs.aaai.org/index.php/AIES/article/view/31621,Security,The Origin and Opportunities of Developers’ Perceived Code Accountability in Open Source AI Software Development,"Open source (OS) software projects in artificial intelligence (AI), such as TensorFlow and scikit-learn, depend on developers' continuous, voluntary code contributions. However, recent security incidents highlighted substantial risks in such software, requiring examinations of factors motivating developers to continuously contribute high-quality code (i.e., providing secure and reliable code fulfilling its functions). Prior research suggests code accountability (i.e., requirements to explain and justify contributed code) to improve code quality, enforced through external accountability mechanisms such as sanctions and rewards. However, the OS domain often lacks such mechanisms, questioning whether and how code accountability arises in this domain and how it affects code contributions. To address these questions, we conducted 26 semi-structured interviews with developers contributing to OS AI software projects. Our findings reveal that despite the absence of external accountability mechanisms, system-, project-, and individual-related factors evoke developers' perceived code accountability. Notably, we discovered a trade-off as high perceived code accountability is associated with higher code quality but discourages developers from participating in OS AI software projects. Overall, this study contributes to understanding the nuanced roles of perceived code accountability in continuously contributing high-quality code without external accountability mechanisms and highlights the complex trade-offs developers face in OS AI software projects.",[],[],"['Sebastian Clemens Bartsch', 'Moritz Lother', 'Jan-Hendrik Schmidt', 'Martin Adam', 'Alexander Benlian']","['Technical University of Darmstadt', 'Technical University of Darmstadt', 'Technical University of Darmstadt', 'Georg-August-University of Göttingen', 'Technical University of Darmstadt']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31625,Fairness & Bias,Unsocial Intelligence: An Investigation of the Assumptions of AGI Discourse,"Dreams of machines rivaling human intelligence have shaped the field of AI since its inception. Yet, the very meaning of human-level AI or artificial general intelligence (AGI) remains elusive and contested. Definitions of AGI embrace a diverse range of incompatible values and assumptions. Contending with the fractured worldviews of AGI discourse is vital for critiques that pursue different values and futures. To that end, we provide a taxonomy of AGI definitions, laying the ground for examining the key social, political, and ethical assumptions they make. We highlight instances in which these definitions frame AGI or human-level AI as a technical topic and expose the value-laden choices being implicitly made. Drawing on feminist, STS, and social science scholarship on the political and social character of intelligence in both humans and machines, we propose contextual, democratic, and participatory paths to imagining future forms of machine intelligence. The development of future forms of AI must involve explicit attention to the values it encodes, the people it includes or excludes, and a commitment to epistemic justice.",[],[],"['Borhane Blili-Hamelin', 'Leif Hancox-Li', 'Andrew Smart']","['AI Risk and Vulnerability Alliance', 'Vijil', 'Google Research']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31628,Transparency & Explainability,Foundation Model Transparency Reports,"Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g. the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.",[],[],"['Rishi Bommasani', 'Kevin Klyman', 'Shayne Longpre', 'Betty Xiong', 'Sayash Kapoor', 'Nestor Maslej', 'Arvind Narayanan', 'Percy Liang']","['Stanford University', 'Stanford University', 'Massachusetts Institute of Technology', 'Stanford University', 'Princeton University', 'Stanford University', 'Princeton University', 'Stanford University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31630,Fairness & Bias,Trustworthy Social Bias Measurement,"How do we design measures of social bias that we trust? While prior work has introduced several measures, no measure has gained widespread trust: instead, mounting evidence argues we should distrust these measures. In this work, we design bias measures that warrant trust based on the cross-disciplinary theory of measurement modeling. To combat the frequently fuzzy treatment of social bias in natural language processing, we explicitly define social bias, grounded in principles drawn from social science research. We operationalize our definition by proposing a general bias measurement framework DivDist, which we use to instantiate 5 concrete bias measures. To validate our measures, we propose a rigorous testing protocol with 8 testing criteria (e.g. predictive validity: do measures predict biases in US employment?). Through our testing, we demonstrate considerable evidence to trust our measures, showing they overcome conceptual, technical, and empirical deficiencies present in prior measures.",[],[],"['Rishi Bommasani', 'Percy Liang']","['Stanford University', 'Stanford University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31631,Security,Views on AI Aren't Binary — They’re Plural (Extended Abstract),"Recent developments in AI have brought broader attention to tensions between two overlapping communities, “AI Ethics” and “AI Safety.” In this article we (i) characterize this false binary, (ii) argue that a simple binary is not an accurate model of AI discourse, and (iii) provide concrete suggestions for how individuals can help avoid the emergence of us-vs-them conflict in the broad community of people working on AI development and governance. While we focus on “AI Ethics” and “AI Safety,” the general lessons apply to related tensions, including those between accelerationist (“e/acc”) and cautious stances on AI development.",[],[],"['Thorin Bristow', 'Luke Thorburn', 'Diana Acosta-Navas']","['Independent Researcher', ""King's College London"", 'Loyola University Chicago']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31632,Fairness & Bias,A Qualitative Study on Cultural Hegemony and the Impacts of AI,"Understanding the future consequences of artificial intelligence requires a holistic consideration of its cultural dimensions, on par with its technological intricacies and potential applications. Individuals and institutions working closely with AI, and with considerable resources, have significant influence on how impact is considered, particularly with regard to how much attention is paid to epistemic concerns (including issues of bias in datasets or potential misinterpretations of data, for example) versus normative concerns (such as societal and ecological effects of AI in the medium- and long-term). In this paper we review qualitative studies conducted with AI researchers and developers to understand how they position themselves relative to each of these two dimensions of impact, and how geographies and conditions of work influence their positions. Our findings underscore the need to gather more perspectives from low- and middle-income countries, whose notions of impact extend beyond the immediate technical concerns or impacts in the short- to medium-term. Rather, they encapsulate a broader spectrum of impact considerations, including the deleterious effects perpetrated by global corporate entities, the unwarranted influence of wealthy nations, the encroachment of philanthrocapitalism, and the adverse consequences of excluding communities affected by these phenomena from active participation in discussions surrounding impact.",[],[],"['Venetia Brown', 'Retno Larasati', 'Aisling Third', 'Tracie Farrell']","['The Open University UK', 'The Open University UK', 'The Open University UK', 'The Open University UK']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AIES/article/view/31634,Transparency & Explainability,Why Am I Still Seeing This: Measuring the Effectiveness of Ad Controls and Explanations in AI-Mediated Ad Targeting Systems,"Recently, Meta has shifted towards AI-mediated ad targeting mechanisms that do not require advertisers to provide detailed targeting criteria. The shift is likely driven by excitement over AI capabilities as well as the need to address new data privacy policies and targeting changes agreed upon in civil rights settlements. At the same time, in response to growing public concern about the harms of targeted advertising, Meta has touted their ad preference controls as an effective mechanism for users to exert control over the advertising they see. Furthermore, Meta markets their ""Why this ad"" targeting explanation as a transparency tool that allows users to understand the reasons for seeing particular ads and inform their actions to control what ads they see in the future.   Our study evaluates the effectiveness of Meta's ""See less"" ad control, as well as the actionability of ad targeting explanations following the shift to AI-mediated targeting. We conduct a large-scale study, randomly assigning participants the intervention of marking ""See less"" to either Body Weight Control or Parenting topics, and collecting the ads Meta shows to participants and their targeting explanations before and after the intervention. We find that utilizing the ""See less"" ad control for the topics we study does not significantly reduce the number of ads shown by Meta on these topics, and that the control is less effective for some users whose demographics are correlated with the topic. Furthermore, we find that the majority of ad targeting explanations for local ads made no reference to location-specific targeting criteria, and did not inform users why ads related to the topics they requested to ""See less"" of continued to be delivered. We hypothesize that the poor effectiveness of controls and lack of actionability and comprehensiveness in explanations are the result of the shift to AI-mediated targeting, for which explainability and transparency tools have not yet been developed by Meta. Our work thus provides evidence for the need of new methods for transparency and user control, suitable and reflective of how the increasingly complex and AI-mediated ad delivery systems operate.",[],[],"['Jane Castleman', 'Aleksandra Korolova']","['Princeton University', 'Princeton University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31635,Transparency & Explainability,Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities,"Harm reporting in Artificial Intelligence (AI) currently lacks a structured process for disclosing and addressing algorithmic flaws, relying largely on an ad-hoc approach. This contrasts sharply with the well-established Coordinated Vulnerability Disclosure (CVD) ecosystem in software security. While global efforts to establish frameworks for AI transparency and collaboration are underway, the unique challenges presented by machine learning (ML) models demand a specialized approach. To address this gap, we propose implementing a Coordinated Flaw Disclosure (CFD) framework tailored to the complexities of ML and AI issues. This paper reviews the evolution of ML disclosure practices, from ad hoc reporting to emerging participatory auditing methods, and compares them with cybersecurity norms. Our framework introduces innovations such as extended model cards, dynamic scope expansion, an independent adjudication panel, and an automated verification process. We also outline a forthcoming real-world pilot of CFD. We argue that CFD could significantly enhance public trust in AI systems. By balancing organizational and community interests, CFD aims to improve AI accountability in a rapidly evolving technological landscape.",[],[],"['Sven Cattell', 'Avijit Ghosh', 'Lucie-Aimée Kaffee']","['AI Village', 'Hugging Face', 'Hugging Face']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31636,Fairness & Bias,Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool,"The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource.  To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide.  Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. In response to these criticisms, some jurisdictions have developed alternative tools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to assess clients' risk levels. Drawing on data from its deployment, we conduct descriptive and quantitative analyses to evaluate whether replacing the VI-SPDAT with the AHA affects racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery.",[],[],"['Lingwei Cheng', 'Cameron Drayton', 'Alexandra Chouldechova', 'Rhema Vaithianathan']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Auckland University of Technology']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31639,Transparency & Explainability,Sponsored is the New Organic: Implications of Sponsored Results on Quality of Search Results in the Amazon Marketplace,"Interleaving sponsored results (advertisements) amongst organic results on search engine result pages (SERP) has become a common practice across multiple digital platforms. Advertisements have catered to consumer satisfaction and fostered competition in digital public spaces; making them an appealing gateway for businesses to reach their consumers. However, especially in the context of digital marketplaces, due to the competitive nature of the sponsored results with the organic ones, multiple unwanted repercussions have surfaced affecting different stakeholders. From the consumers' perspective the sponsored ads/results may cause degradation of search quality and nudge consumers to potentially irrelevant and costlier products. The sponsored ads may also affect the level playing field of the competition in the marketplaces among sellers. To understand and unravel these potential concerns, we analyse the Amazon digital marketplace in four different countries by simulating 4,800 search operations. Our analyses over SERPs consisting 2M organic and 638K sponsored results show items with poor organic ranks (beyond 100th position) appear as sponsored results even before the top organic results on the first page of Amazon SERP. Moreover, we also observe that in majority of the cases, these top sponsored results are costlier and are of poorer quality than the top organic results. We believe these observations can motivate researchers for further deliberation to bring in more transparency and guard rails in the advertising practices followed in digital marketplaces.",[],[],"['Abhisek Dash', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Abhijnan Chakraborty', 'Krishna P. Gummadi']","['Max Planck Institute for Software Systems, Germany', 'Indian Institute of Technology Kharagpur, India', 'Indian Institute of Technology Kharagpur, India', 'Indian Institute of Technology Kharagpur, India', 'Max Planck Institute for Software Systems, Germany']","['Germany', 'India', 'India', 'India', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31638,Privacy & Data Governance,"MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks","The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MoJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting  jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.",[],[],"['Giandomenico Cornacchia', 'Giulio Zizzo', 'Kieran Fraser', 'Muhammad Zaid Hameed', 'Ambrish Rawat', 'Mark Purcell']","['IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe']","['', '', '', '', '', '']"
https://ojs.aaai.org/index.php/AIES/article/view/31644,Transparency & Explainability,Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through Data-centric Factors,"The astonishing successes of ML  have raised growing concern for the fairness of modern methods when deployed in real world settings. However, studies on fairness have mostly focused on supervised ML, while unsupervised outlier detection (OD), with numerous applications in finance, security, etc., have attracted little attention. While a few studies proposed fairness-enhanced OD algorithms, they remain agnostic to the underlying driving mechanisms or sources of unfairness. Even within the supervised ML literature, there exists debate on whether unfairness stems solely from algorithmic biases (i.e. design choices)  or from the  biases encoded in the data on which they are trained.    To close this gap, this work aims to shed light on the possible sources of unfairness in OD by auditing detection models under different data-centric factors.By injecting various known biases into the input data---as pertain to sample size disparity, under-representation, feature measurement noise, and group membership obfuscation---we find that the OD algorithms under the study all exhibit fairness pitfalls, although differing in which types of data bias they are more susceptible to. Most notable of our study is to demonstrate that OD algorithm bias is not merely a data bias problem. A key realization is that the data properties that emerge from bias injection could as well be organic---as pertain to natural group differences w.r.t. sparsity, base rate, variance, and multi-modality. Either natural or biased, such data properties can  give rise to unfairness as they interact with certain algorithmic design choices.  Our work provides a deeper  understanding of the possible sources of OD unfairness, and serves as a framework for assessing the unfairness of future OD algorithms under specific data-centric factors. It also paves the way for future work on mitigation strategies by underscoring the susceptibility of various design choices.",[],[],"['Xueying Ding', 'Rui Xi', 'Leman Akoglu']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31645,Security,Legitimating Emotion Tracking Technologies in Driver Monitoring Systems,"Contemporary automobiles are now incorporating digital technologies, including emotion recognition technologies intended to monitor and sometimes intervene on the driver’s mood, attentiveness, or emotional state. We investigate how the firms producing these technologies justify and legitimate their design, production, and use, and how these discourses of legitimation paint a picture of the desired social role of emotion recognition in the automotive sector. Through a critical discourse analysis of patents, advertising, and promotional materials from industry-leading companies Cerence and Affectiva/Smart Eye, we argue both companies use  potentially spurious arguments about the accuracy of emotion recognition to rationalize their products. Both companies also use a variety of other legitimation techniques around driver safety, individual personalization, and increased productivity to re-frame the social aspects of digitally mediated autonomous vehicles on their terms.",[],[],"['Aaron Doerfler', 'Luke Stark']","['Western University, London ON', 'Western University, London ON\nCanadian Institute for Advanced Research, Toronto ON']","['Canada', 'Canada']"
https://ojs.aaai.org/index.php/AIES/article/view/31647,Security,Red-Teaming for Generative AI: Silver Bullet or Security Theater?,"In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming’s central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.",[],[],"['Michael Feffer', 'Anusha Sinha', 'Wesley H. Deng', 'Zachary C. Lipton', 'Hoda Heidari']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31649,Fairness & Bias,Surviving in Diverse Biases: Unbiased Dataset Acquisition in Online Data Market for Fair Model Training,"The online data markets have emerged as a valuable source of diverse datasets for training machine learning (ML) models.  However, datasets from different data providers may exhibit varying levels of bias with respect to certain sensitive attributes in the population (such as race, sex, age, and marital status).  Recent dataset acquisition research has focused on maximizing accuracy improvements for downstream model training,  ignoring the negative impact of biases in the acquired datasets, which can lead to an unfair model.  Can a consumer obtain an unbiased dataset from datasets with diverse biases? In this work, we propose a fairness-aware data acquisition framework  (FAIRDA) to acquire high-quality datasets that maximize both accuracy and fairness for consumer local classifier training while remaining within a limited budget.  Given the biases of data commodities remain opaque to consumers,  the data acquisition in FAIRDA employs explore-exploit strategies.  Based on whether exploration and exploitation are conducted sequentially or alternately, we introduce two algorithms: the knowledge-based offline data acquisition (KDA) and the reward-based online data acquisition algorithms (RDA).  Each algorithm is tailored to specific customer needs, giving the former an advantage in computational efficiency and the latter an advantage in robustness.  We conduct experiments to demonstrate the effectiveness of the proposed data acquisition framework in steering users toward fairer model training compared to existing baselines under varying market settings.",[],[],"['Jiashi Gao', 'Ziwei Wang', 'Xiangyu Zhao', 'Xin Yao', 'Xuetao Wei']","['Southern University of Science and Technology (SUSTech), Shenzhen, China', 'Southern University of Science and Technology (SUSTech), Shenzhen, China\nUniversity of Birmingham (UoB), UK', 'City University of Hong Kong (CityU), Hong Kong SAR, China', 'Lingnan University (LU), Hong Kong SAR, China', 'Southern University of Science and Technology (SUSTech), Shenzhen, China']","['China', 'United Kingdom', 'Hong Kong', 'Hong Kong', 'China']"
https://ojs.aaai.org/index.php/AIES/article/view/31648,Transparency & Explainability,How Should AI Decisions Be Explained? Requirements for Explanations from the Perspective of European Law,"This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, which was adopted by the European Parliament in March 2024, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary duties, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI methods are derived from each of the legal fields, resulting in the conclusion that each legal field requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI methods.",[],[],"['Benjamin Fresz', 'Elena Dubovitskaya', 'Danilo Brajovic', 'Marco F. Huber', 'Christian Horz']","['Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany\nInstitute of Industrial Manufacturing and Management (IFF), University of Stuttgart, Germany', 'University of Giessen, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany\nInstitute of Industrial Manufacturing and Management (IFF), University of Stuttgart, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany\nInstitute of Industrial Manufacturing and Management (IFF), University of Stuttgart, Germany', 'University of Giessen, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31646,Privacy & Data Governance,Representation Magnitude Has a Liability to Privacy Vulnerability,"The privacy-preserving approaches to machine learning (ML) models have made substantial progress in recent years. However, it is still opaque in which circumstances and conditions the model becomes privacy-vulnerable, leading to a challenge for ML models to maintain both performance and privacy. In this paper, we first explore the disparity between member and non-member data in the representation of models under common training frameworks.We identify how the representation magnitude disparity correlates with privacy vulnerability and address how this correlation impacts privacy vulnerability. Based on the observations, we propose Saturn Ring Classifier Module (SRCM), a plug-in model-level solution to mitigate membership privacy leakage. Through a confined yet effective representation space, our approach ameliorates models’ privacy vulnerability while maintaining generalizability. The code of this work can be found here: https://github.com/JEKimLab/AIES2024SRCM",[],[],"['Xingli Fang', 'Jung-Eun Kim']","['North Carolina State University', 'North Carolina State University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31650,Fairness & Bias,“I Don’t See Myself Represented Here at All”: User Experiences of Stable Diffusion Outputs Containing Representational Harms across Gender Identities and Nationalities,"Though research into text-to-image generators (T2Is) such as Stable Diffusion has demonstrated their amplification of societal biases and potentials to cause harm, such research has primarily relied on computational methods instead of seeking information from real users who experience harm, which is a significant knowledge gap. In this paper, we conduct the largest human subjects study of Stable Diffusion, with a combination of crowdsourced data from 133 crowdworkers and 14 semi-structured interviews across diverse countries and genders. Through a mixed-methods approach of intra-set cosine similarity hierarchies (i.e., comparing multiple Stable Diffusion outputs for the same prompt with each other to examine which result is `closest' to the prompt) and qualitative thematic analysis, we first demonstrate a large disconnect between user expectations for Stable Diffusion outputs with those generated, evidenced by a set of Stable Diffusion renditions of `a Person' providing images far away from such expectations. We then extend this finding of general dissatisfaction into highlighting representational harms caused by Stable Diffusion upon our subjects, especially those with traditionally marginalized identities, subjecting them to incorrect and often dehumanizing stereotypes about their identities. We provide recommendations for a harm-aware approach to (re)design future versions of Stable Diffusion and other T2Is.",[],[],"['Sourojit Ghosh', 'Nina Lutz', 'Aylin Caliskan']","['University of Washington', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31653,Security,The PPOu Framework: A Structured Approach for Assessing the Likelihood of Malicious Use of Advanced AI Systems,"The diffusion of increasingly capable AI systems has produced concern that bad actors could intentionally misuse current or future AI systems for harm. Governments have begun to create new entities—such as AI Safety Institutes—tasked with assessing these risks. However, approaches for risk assessment are currently fragmented and would benefit from broader disciplinary expertise. As it stands, it is often unclear whether concerns about malicious use misestimate the likelihood and severity of the risks. This article advances a conceptual framework to review and structure investigation into the likelihood of an AI system (X) being applied to a malicious use (Y). We introduce a three-stage framework of (1) Plausibility (can X be used to do Y at all?), (2) Performance (how well does X do Y?), and (3) Observed use (do actors use X to do Y in practice?). At each stage, we outline key research questions, methodologies, benefits and limitations, and the types of uncertainty addressed. We also offer ideas for directions to improve risk assessment moving forward.",[],[],"['Josh A. Goldstein', 'Girish Sastry']","['Georgetown University, Center for Security and Emerging Technology', 'OpenAI']","['United States', 'India']"
https://ojs.aaai.org/index.php/AIES/article/view/31651,Fairness & Bias,Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach,"Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a Non-Western community-centered approach and grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English input prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, contributing to the development of more equitable and representative GAI technologies globally. Our work underscores the necessity of adopting a community-centered approach to comprehend the sociotechnical dynamics of these models, complementing existing work in this space while identifying and addressing the potential negative repercussions and harms that may arise as these models are deployed on a global scale.",[],[],"['Sourojit Ghosh', 'Pranav Narayanan Venkit', 'Sanjana Gautam', 'Shomir Wilson', 'Aylin Caliskan']","['University of Washington', 'The Pennsylvania State University', 'The Pennsylvania State University', 'The Pennsylvania State University', 'University of Washington']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31654,Security,Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation (Extended Abstract),"In the United States and other countries exists a “national mental health crisis”: Rates of suicide, depression, anxiety, substance use, and more continue to increase – exacerbated by isolation, the COVID pandemic, and, most importantly, lack of access to mental healthcare. Therefore, many are looking to AI-enabled digital mental health tools, which have the potential to reach many patients who would otherwise remain on wait lists or without care. The main drive behind these new tools is the focus on large language models that could enable real-time, personalized support and advice for patients. With a trend towards language models entering the mental healthcare delivery apparatus, questions arise about how a robust, high-level framework to guide ethical implementations would look like and whether existing language models are ready for this high-stakes application where individual failures can lead to dire consequences.  This paper addresses the ethical and practical challenges custom to mental health applications and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s) with defined rubrics and criteria for each question that would define ""safe,"" ""unsafe,"" and ""borderline"" (between safe and unsafe) for reproducibility.  We find that all tested language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context.  This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models based on system prompt engineering and model-generated self-critiques.  Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.  Our code and the redacted data set are available on Github (github.com/maxlampe/taimh_eval, MIT License). The full, unredacted data set is available upon request due to the harmful content contained.",[],[],"['Declan Grabb', 'Max Lamparth', 'Nina Vasan']","['Northwestern University\nStanford University', 'Stanford University', 'Stanford University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31656,Security,A Conceptual Framework for Ethical Evaluation of Machine Learning Systems,"Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.",[],[],"['Neha R. Gupta', 'Jessica Hullman', 'Hariharan Subramonyam']","['Carnegie Mellon University', 'Northwestern University', 'Stanford University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31657,Transparency & Explainability,Identifying Implicit Social Biases in Vision-Language Models,"Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-It, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a ""terrorist"". Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.",[],[],"['Kimia Hamidieh', 'Haoran Zhang', 'Walter Gerych', 'Thomas Hartvigsen', 'Marzyeh Ghassemi']","['MIT', 'MIT', 'MIT', 'University of Virginia', 'MIT']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31659,Fairness & Bias,"Contributory Injustice, Epistemic Calcification and the Use of AI Systems in Healthcare","AI systems have long been touted as a means to transform the healthcare system and improve service user outcomes. However, these claims frequently ignore the social context that leaves service users subject to epistemic oppression. This paper introduces the term “epistemic calcification” to describe how the use of AI systems leads to our epistemological systems becoming stuck in fixed frameworks for understanding the world. Epistemic calcification leads to contributory injustice as it reduces the ability of healthcare systems to meaningfully consider alternative understandings of people’s health experiences. By analysing examples of algorithmic prognosis and diagnosis, this paper demonstrates the challenges of addressing contributory injustice in AI systems and the need for contestability to focus on more than the AI system and on the underlying epistemologies of AI systems.",[],[],['Mahi Hardalupas'],['Independent researcher'],['Germany']
https://ojs.aaai.org/index.php/AIES/article/view/31658,Fairness & Bias,A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems,"We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the ""causal chain of interactions"" rather than simply focusing on the end outcome; this can help guide reforms.   In this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and criminality. Our framework identifies three canonical scenarios with distinct characteristics: in settings like (1) airport security, the primary source of observed bias against a race is likely to be bias in law enforcement against innocents of that race; (2) AI-empowered policing, the primary source of observed bias against a race is likely to be bias in law enforcement against criminals of that race; and (3) police-civilian interaction, the primary source of observed bias against a race could be bias in law enforcement against that race or bias from the general public in reporting (e.g. via 911 calls) against the other race. Through an extensive empirical study using police-civilian interaction (stop) data and 911 call data, we And an instance of such a counter-intuitive phenomenon: in New Orleans, the observed bias is against the majority race and the likely reason for it is the over-reporting (via 911 calls) of incidents involving the minority race by the general public.",[],[],"['Jessy Xinyi Han', 'Andrew Cesare Miller', 'S. Craig Watkins', 'Christopher Winship', 'Fotini Christia', 'Devavrat Shah']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'United States Naval Academy, Annapolis, MD, USA', 'University of Texas at Austin, Austin, TX, USA', 'Harvard University, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31661,Fairness & Bias,What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from a Perspective of Approximate Justice,"In the field of algorithmic fairness, many fairness criteria have been proposed. Oftentimes, their proposal is only accompanied by a loose link to ideas from moral philosophy -- which makes it difficult to understand when the proposed criteria should be used to evaluate the fairness of a decision-making system. More recently, researchers have thus retroactively tried to tie existing fairness criteria to philosophical concepts. Group fairness criteria have typically been linked to egalitarianism, a theory of distributive justice. This makes it tempting to believe that fairness criteria mathematically represent ideals of distributive justice and this is indeed how they are typically portrayed. In this paper, we will discuss why the current approach of linking algorithmic fairness and distributive justice is too simplistic and, hence, insufficient. We argue that in the context of imperfect decision-making systems -- which is what we deal with in algorithmic fairness -- we should not only care about what the ideal distribution of benefits/harms among individuals would look like but also about how deviations from said ideal are distributed. Our claim is that algorithmic fairness is concerned with unfairness in these deviations. This requires us to rethink the way in which we, as algorithmic fairness researchers, view distributive justice and use fairness criteria.",[],[],"['Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']","['Zurich University of Applied Sciences\nUniversity of Zurich', 'Zurich University of Applied Sciences', 'AlgorithmWatch']","['Switzerland', 'Germany', 'Mali']"
https://ojs.aaai.org/index.php/AIES/article/view/31664,Security,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins,"Large language model (LLM) platforms, such as ChatGPT, have recently begun offering an app ecosystem to interface with third-party services on the internet. While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Apps also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future third-party integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin (apps) ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms. The full version of this paper is available online at https://arxiv.org/abs/2309.10254",[],[],"['Umar Iqbal', 'Tadayoshi Kohno', 'Franziska Roesner']","['Washington University in St. Louis', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31665,Fairness & Bias,"As an AI Language Model, ""Yes I Would Recommend Calling the Police"": Norm Inconsistency in LLM Decision-Making","We investigate the phenomenon of norm inconsistency: where LLMs apply different norms in similar situations. Specifically, we focus on the high-risk application of deciding whether to call the police in Amazon Ring home surveillance videos. We evaluate the decisions of three state-of-the-art LLMs — GPT-4, Gemini 1.0, and Claude 3 Sonnet — in relation to the activities portrayed in the videos, the subjects' skin-tone and gender, and the characteristics of the neighborhoods where the videos were recorded. Our analysis reveals significant norm inconsistencies: (1) a discordance between the recommendation to call the police and the actual presence of criminal activity, and (2) biases influenced by the racial demographics of the neighborhoods. These results highlight the arbitrariness of model decisions in the surveillance context and the limitations of current bias detection and mitigation strategies in normative decision-making.",[],[],"['Shomik Jain', 'D. Calacci', 'Ashia Wilson']","['Massachusetts Institute of Technology', 'Penn State University', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31666,Transparency & Explainability,Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems,"Facial Recognition Systems (FRSs) are being developed and deployed all around the world at unprecedented rates. Most platforms are designed in a limited set of countries, but deployed in other regions too, without adequate checkpoints for region-specific requirements. This is especially problematic for Global South countries which lack strong legislation to safeguard persons facing disparate performance of these systems. A combination of unavailability of datasets, lack of understanding of how FRSs function and low-resource bias mitigation measures accentuate the problems at hand. In this work, we propose a self-curated face dataset composed of 6,579 unique male and female sports-persons (cricket players) from eight countries around the world. More than 50% of the dataset is composed of individuals from the Global South countries and is demographically diverse. To aid adversarial audits and robust model training, we curate four adversarial variants of each image in the dataset, leading to more than 40,000 distinct images. We also use this dataset to benchmark five popular facial recognition systems (FRSs), including both commercial and open-source FRSs, for the task of gender prediction (and country prediction for one of the open-source models as an example of red-teaming). Experiments on industrial FRSs reveal accuracies ranging from 98.2% (in case of Azure) to 38.1% (in case of Face++), with a large disparity between males and females in the Global South (max difference of 38.5% in case of Face++). Biases are also observed in all FRSs between females of the Global North and South (max difference of ~50%). A Grad-CAM analysis shows that the nose, forehead and mouth are the regions of interest for one of the open-source FRSs.  Based on this crucial observation, we design simple, low-resource bias mitigation solutions using few-shot and novel contrastive learning techniques that demonstrate a significant improvement in accuracy with disparity between males and females reducing from 50% to 1.5% in one of the settings. For the red-teaming experiment using the open-source Deepface model we observe that simple fine-tuning is not very useful while contrastive learning brings steady benefits.",[],[],"['Siddharth Jaiswal', 'Animesh Ganai', 'Abhisek Dash', 'Saptarshi Ghosh', 'Animesh Mukherjee']","['Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Kharagpur, India', 'Max Planck Institute for Software Systems, Germany', 'Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Kharagpur, India']","['India', 'India', '', 'India', 'India']"
https://ojs.aaai.org/index.php/AIES/article/view/31669,Transparency & Explainability,Do Responsible AI Artifacts Advance Stakeholder Goals? Four Key Barriers Perceived by Legal and Civil Stakeholders,"The responsible AI (RAI) community has introduced numerous processes and artifacts---such as Model Cards, Transparency Notes, and Data Cards---to facilitate transparency and support the governance of AI systems.  While originally designed to scaffold and document AI development processes in technology companies, these artifacts are becoming central components of regulatory compliance under recent regulations such as the EU AI Act. Much of the existing literature has focussed primarily on the design of new RAI artifacts, or an examination of their use by practitioners within technology companies.  However, as RAI artifacts begin to play key roles in enabling external oversight, it becomes critical to understand how stakeholders---particularly stakeholders situated outside of technology companies who govern and audit industry AI deployments---perceive the efficacy of RAI artifacts.  In this study, we conduct semi-structured interviews and design activities with 19 government, legal, and civil society stakeholders who inform policy and advocacy around responsible AI efforts. While participants believe that RAI artifacts are a valuable contribution to the RAI ecosystem, many have concerns around their potential unintended and longer-term impacts on actors outside of technology companies (e.g., downstream end-users, policymakers, civil society stakeholders). We organized these beliefs into four barriers that help explain how RAI artifacts may (inadvertently) reconfigure power relations across civil society, government, and industry, impeding civil society and legal stakeholders' ability to protect downstream end-users from potential AI harms. Participants envision how structural changes, along with changes in how RAI artifacts are designed, used, and governed, could help re-direct the role and impacts of artifacts in the RAI ecosystem. Drawing on these findings, we discuss research and policy implications for RAI artifacts.",[],[],"['Anna Kawakami', 'Daricia Wilkinson', 'Alexandra Chouldechova']","['Carnegie Mellon University', 'Arizona State University', 'Microsoft Research']","['United States', 'United States', '']"
https://ojs.aaai.org/index.php/AIES/article/view/31668,Fairness & Bias,Virtual Assistants Are Unlikely to Reduce Patient Non-Disclosure,"The ethical use of AI typically involves setting boundaries on its deployment. Ethical guidelines advise against practices that involve deception, privacy infringement, or discriminatory actions. However, ethical considerations can also identify areas where using AI is desirable and morally necessary. For instance, it has been argued that AI could contribute to more equitable justice systems. Another area where ethical considerations can make AI deployment imperative is healthcare. For example, patients often withhold pertinent details from healthcare providers due to fear of judgment. However, utilizing virtual assistants to gather patients' health histories could be a potential solution. Ethical imperatives support using such technology if patients are more inclined to disclose information to an AI system. This article presents findings from several survey studies investigating whether virtual assistants can reduce non-disclosure behaviors. Unfortunately, the evidence suggests that virtual assistants are unlikely to minimize non-disclosure. Therefore, the potential benefits of virtual assistants due to reduced non-disclosure are unlikely to outweigh their ethical risks.",[],[],"['Corinne Jorgenson', 'Ali I. Ozkes', 'Jurgen Willems', 'Dieter Vanderelst']","['University of Cincinnati', 'SKEMA Business School, GREDEG', 'Vienna University of Economics and Business', 'University of Cincinnati']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31671,Fairness & Bias,Epistemic Injustice in Generative AI,"This paper investigates how generative AI can potentially undermine the integrity of collective knowledge and the processes we rely on to acquire, assess, and trust information, posing a significant threat to our knowledge ecosystem and democratic discourse. Grounded in social and political philosophy, we introduce the concept of generative algorithmic epistemic injustice. We identify four key dimensions of this phenomenon: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate each dimension with real-world examples that reveal how generative AI can produce or amplify misinformation, perpetuate representational harm, and create epistemic inequities, particularly in multilingual contexts. By highlighting these injustices, we aim to inform the development of epistemically just generative AI systems, proposing strategies for resistance, system design principles, and two approaches that leverage generative AI to foster a more equitable information ecosystem, thereby safeguarding democratic values and the integrity of knowledge production.",[],[],"['Jackie Kay', 'Atoosa Kasirzadeh', 'Shakir Mohamed']","['Google Deepmind\nUniversity College London', 'University of Edinburgh\nGoogle Research', 'Google Deepmind']","['United States', 'United Kingdom', '']"
https://ojs.aaai.org/index.php/AIES/article/view/31672,Security,Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety,"Operationalizing AI ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by AI systems. To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. These include novel “holistic” methods that go beyond exclusive reliance on technical benchmarking. However, our paper argues that such taxonomies are still too general to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or “high-risk” sectors. This is because many sectors are constituted by discourses, norms, and values that “refract” or even directly conflict with those operating in society more broadly. Drawing from emerging anthropological theories of human rights, we propose that the process of “vernacularization”—a participatory, decolonial practice distinct from doctrinary “translation” (the dominant mode of AI safety operationalization)—can help bridge this gap. To demonstrate this point, we consider the education sector, and identify precisely how vernacularizing a leading  taxonomy of harm leads to a clearer view of how harms AI systems may cause are substantially intensified when deployed in educational spaces. We conclude by discussing the generalizability of vernacularization as a useful AI safety methodology.",[],[],"['Wm. Matthew Kennedy', 'Daniel Vargas Campos']","['University of Sussex', 'Independent Researcher']","['United Kingdom', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31674,Fairness & Bias,Algorithmic Fairness From the Perspective of Legal Anti-discrimination Principles,"Real-world applications of machine learning (ML) algorithms often propagate negative stereotypes and social biases against marginalized groups. In response, the field of fair machine learning has proposed technical solutions for a variety of settings that aim to correct the biases in algorithmic predictions. These solutions remove the dependence of the final prediction on the protected attributes (like gender or race) and/or ensure that prediction performance is similar across demographic groups. Yet, recent studies assessing the impact of these solutions in practice demonstrate their ineffectiveness in tackling real-world inequalities. Given this lack of real-world success, it is essential to take a step back and question the design motivations of algorithmic fairness interventions.   We use popular legal anti-discriminatory principles, specifically anti-classification and anti-subordination principles, to study the motivations of fairness interventions and their applications. The anti-classification principle suggests addressing discrimination by ensuring that decision processes and outcomes are independent of the protected attributes of individuals. The anti-subordination principle, on the other hand, argues that decision-making policies can provide equal protection to all only by actively tackling societal hierarchies that enable structural discrimination, even if that requires using protected attributes to address historical inequalities. Through a survey of the fairness mechanisms and applications, we assess different components of fair ML approaches from the perspective of these principles. We argue that the observed shortcomings of fair ML algorithms are similar to the failures of anti-classification policies and that these shortcomings constitute violations of the anti-subordination principle. Correspondingly, we propose guidelines for algorithmic fairness interventions to adhere to the anti-subordination principle. In doing so, we hope to bridge critical concepts between legal frameworks for non-discrimination and fairness in machine learning.",[],[],"['Vijay Keswani', 'L. Elisa Celis']","['Duke University', 'Yale University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31677,Transparency & Explainability,Acceptable Use Policies for Foundation Models,"As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies—legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers’ acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Companies also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the AI ecosystem.",[],[],['Kevin Klyman'],"['Center for Research on Foundation Models, Stanford University\nBelfer Center for Science and International Affairs, Harvard University']",['United States']
https://ojs.aaai.org/index.php/AIES/article/view/31678,Security,Responsible Reporting for Frontier AI Development,"Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.",[],[],"['Noam Kolt', 'Markus Anderljung', 'Joslyn Barnhart', 'Asher Brass', 'Kevin Esvelt', 'Gillian K. Hadfield', 'Lennart Heim', 'Mikel Rodriguez', 'Jonas B. Sandbrink', 'Thomas Woodside']","['University of Toronto', 'Centre for the Governance of AI', 'Google DeepMind', 'Institute for AI Policy and Strategy', 'Massachusetts Institute of Technology', 'University of Toronto\nVector Institute for AI', 'Centre for the Governance of AI', 'Google DeepMind', 'University of Oxford', 'Center for Security and Emerging Technology']","['Canada', 'United States', 'United States', 'Brazil', 'United States', 'Canada', 'United States', 'United States', 'United Kingdom', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31680,Fairness & Bias,Observing Context Improves Disparity Estimation when Race is Unobserved,"In many domains, it is difficult to obtain the race data that is required to estimate racial disparity.  To address this problem, practitioners have adopted the use of proxy methods which predict race using non-protected covariates.  However, these proxies often yield biased estimates, especially for minority groups, limiting their real-world utility.  In this paper, we introduce two new contextual proxy models that advance existing methods by incorporating contextual features in order to improve race estimates. We show that these algorithms demonstrate significant performance improvements in estimating disparities, on real-world home loan and voter data. We establish that achieving unbiased disparity estimates with contextual proxies relies on mean-consistency, a calibration-like condition.",[],[],"['Kweku Kwegyir-Aggrey', 'Naveen Durvasula', 'Jennifer Wang', 'Suresh Venkatasubramanian']","['Brown University', 'Columbia University', 'Brown University', 'Brown University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31681,Fairness & Bias,Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations,"To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to examine crisis escalation in a fictional US-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as “pacifist” or “aggressive sociopath.” When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.",[],[],"['Max Lamparth', 'Anthony Corso', 'Jacob Ganz', 'Oriana Skylar Mastro', 'Jacquelyn Schneider', 'Harold Trinkunas']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31682,Fairness & Bias,"Racial and Neighborhood Disparities in Legal Financial Obligations in Jefferson County, Alabama","Legal financial obligations (LFOs) such as court fees and fines are commonly levied on individuals who are convicted of crimes. It is expected that LFO amounts should be similar across social, racial, and geographic subpopulations convicted of the same crime.  This work analyzes the distribution of LFOs in Jefferson County, Alabama and highlights disparities across different individual and neighborhood demographic characteristics. Data-driven discovery methods are used to detect subpopulations that experience higher LFOs than the overall population of offenders. Critically, these discovery methods do not rely on pre-specified groups and can assist scientists and researchers investigate socially-sensitive hypotheses in a disciplined way.  Some findings, such as individuals who are Black, live in Black-majority neighborhoods, or live in low-income neighborhoods tending to experience higher LFOs, are commensurate with prior expectation. However others, such as high LFO amounts in worthless instrument (bad check) cases experienced disproportionately by individuals living in affluent majority-white neighborhoods, are more surprising. More broadly than the specific findings, the methodology is shown to identify structural weaknesses that undermine the goal of equal justice under law that can be addressed through policy interventions.",[],[],"['Óscar Lara Yejas', 'Aakanksha Joshi', 'Andrew Martinez', 'Leah Nelson', 'Skyler Speakman', 'Krysten Thompson', 'Yuki Nishimura', 'Jordan Bond', 'Kush R. Varshney']","['IBM Silicon Valley Lab', 'IBM Innovation Studio', 'Center for Justice Innovation', 'Alabama Appleseed', 'IBM Research - Africa', 'IBM Silicon Valley Lab', 'IBM Chief Analytics Office', 'IBM Innovation Studio', 'IBM Research - Thomas J Watson Research Center']","['Brazil', 'Chad', 'United States', 'China', 'Peru', 'Brazil', '', 'Chad', 'Mali']"
https://ojs.aaai.org/index.php/AIES/article/view/31685,Security,On Feasibility of Intent Obfuscating Attacks,"Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose using intent obfuscation to generate adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors---YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN---using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attacks. Finally, we discuss main takeaways and legal repercussions. If you are reading the AAAI/ACM version, please download the technical appendix on arXiv at https://arxiv.org/abs/2408.02674",[],[],"['Zhaobin Li', 'Patrick Shafto']","['Rutgers University—Newark', 'Rutgers University—Newark\nInstitute for Advanced Study']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31684,Transparency & Explainability,How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies,"With the widespread availability of LLMs since the release of ChatGPT and increased public scrutiny, commercial model development appears to have focused their efforts on `safety' training concerning legal liabilities at the expense of social impact evaluation. This mimics a similar trend which we could observe for search engine autocompletion some years prior. We draw on scholarship from NLP and search engine auditing and present a novel evaluation task in the style of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by using four metrics, namely refusal rates, toxicity, sentiment and regard, with and without safety system prompts. Our findings indicate an improvement to stereotyping outputs with the system prompt, but overall a lack of attention by LLMs under study to certain harms classified as toxic, particularly for prompts about peoples/ethnicities and sexual orientation. Mentions of intersectional identities trigger a disproportionate amount of stereotyping. Finally, we discuss the implications of these findings about stereotyping harms in light of the coming intermingling of LLMs and search and the choice of stereotyping mitigation policy to adopt. We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.",[],[],"['Alina Leidinger', 'Richard Rogers']","['University of Amsterdam', 'University of Amsterdam']","['Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AIES/article/view/31686,Fairness & Bias,“Democratizing AI” and the Concern of Algorithmic Injustice (Extended Abstract),"The call to make artificial intelligence (AI) more democratic, or to “democratize AI,” is sometimes framed as a promising response for mitigating algorithmic injustice or making AI more aligned with social justice. However, the notion of “democratizing AI” is elusive, as the phrase has been associated with multiple meanings and practices, and the extent to which it may help mitigate algorithmic injustice is still underexplored. In this paper, based on a socio-technical understanding of algorithmic injustice, I examine three notable notions of democratizing AI and their associated measures—democratizing AI use, democratizing AI development, and democratizing AI governance—regarding their respective prospects and limits in response to algorithmic injustice. My examinations reveal that while some versions of democratizing AI bear the prospect of mitigating the concern of algorithmic injustice, others are somewhat limited and might even function to perpetuate unjust power hierarchies. This analysis thus urges a more fine-grained discussion on how to democratize AI and suggests that closer scrutiny of the power dynamics embedded in the socio-technical structure can help guide such explorations.",[],[],['Ting-an Lin'],"['University of Connecticut, Storrs, CT\nStanford University, Stanford, CA']",['United States']
https://ojs.aaai.org/index.php/AIES/article/view/31687,Fairness & Bias,Foundations for Unfairness in Anomaly Detection - Case Studies in Facial Imaging Data,"Deep anomaly detection (AD) is perhaps the most controversial of data analytic tasks as it identifies entities that are specifically targeted for further investigation or exclusion. Also controversial is the application of AI to facial data, in particular facial recognition. This work explores the intersection of these two areas to understand two core questions: Who these algorithms are being unfair to and equally important why. Recent work has shown that deep AD can be unfair to different groups despite being unsupervised with a recent study showing that for portraits of people: men of color are far more likely to be chosen to be outliers. We study the two main categories of AD algorithms: autoencoder-based and single-class-based which effectively try to compress all the instances and those that can not be easily compressed are deemed to be outliers. We experimentally verify sources of unfairness such as the under-representation of a group (e.g people of color are relatively rare), spurious group features (e.g. men are often photographed with hats) and group labeling noise (e.g. race is subjective). We conjecture that lack of compressibility is the main foundation and the others cause it but experimental results show otherwise and we present a natural hierarchy amongst them.",[],[],"['Michael Livanos', 'Ian Davidson']","['University of California, Davis', 'University of California, Davis']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31689,Transparency & Explainability,Examining the Behavior of LLM Architectures Within the Framework of Standardized National Exams in Brazil,"The Exame Nacional do Ensino Médio (ENEM) is a pivotal test for Brazilian students, required for admission to a significant number of universities in Brazil. The test consists of four objective high-school level tests on Math, Humanities, Natural Sciences and Languages, and one writing essay. Students' answers to the test and to the accompanying socioeconomic status questionnaire are made public every year (albeit anonymized) due to transparency policies from the Brazilian Government. In the context of large language models (LLMs), these data lend themselves nicely to comparing different groups of humans with AI, as we can have access to human and machine answer distributions. We leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4, and MariTalk, a model trained using Portuguese data, to humans, aiming to ascertain how their answers relate to real societal groups and what that may reveal about the model biases. We divide the human groups by using socioeconomic status (SES), and compare their answer distribution with LLMs for each question and for the essay. We find no significant biases when comparing LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as the distance between model and human answers is mostly determined by the human accuracy. A similar conclusion is found by looking at the generated text as, when analyzing the essays, we observe that human and LLM essays differ in a few key factors, one being the choice of words where model essays were easily separable from human ones. The texts also differ syntactically, with LLM generated essays exhibiting, on average, smaller sentences and less thought units, among other differences. These results suggest that, for Brazilian Portuguese in the ENEM context, LLM outputs represent no group of humans, being significantly different from the answers from Brazilian students across all tests. The appendices may be found at https://arxiv.org/abs/2408.05035.",[],[],"['Marcelo Sartori Locatelli', 'Matheus Prado Miranda', 'Igor Joaquim da Silva Costa', 'Matheus Torres Prates', 'Victor Thomé', 'Mateus Zaparoli Monteiro', 'Tomas Lacerda', 'Adriana Pagano', 'Eduardo Rios Neto', 'Wagner Meira Jr.', 'Virgilio Almeida']","['Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais']","['Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil']"
https://ojs.aaai.org/index.php/AIES/article/view/31691,Transparency & Explainability,"Foregrounding Artist Opinions: A Survey Study on Transparency, Ownership, and Fairness in AI Generative Art","Generative AI tools are used to create art-like outputs and sometimes aid in the creative process. These tools have potential benefits for artists, but they also have the potential to harm the art workforce and infringe upon artistic and intellectual property rights. Without explicit consent from artists, Generative AI creators scrape artists' digital work to train Generative AI models and produce art-like outputs at scale. These outputs are now being used to compete with human artists in the marketplace as well as being used by some artists in their generative processes to create art. We surveyed 459 artists to investigate the tension between artists' opinions on Generative AI art's potential utility and harm. This study surveys artists' opinions on the utility and threat of Generative AI art models, fair practices in the disclosure of artistic works in AI art training models, ownership and rights of AI art derivatives, and fair compensation. Results show that a majority of artists believe creators should disclose what art is being used in AI training, that AI outputs should not belong to model creators, and express concerns about AI's impact on the art workforce and who profits from their art. We hope the results of this work will further meaningful collaboration and alignment between the art community and Generative AI researchers and developers.",[],[],"['Juniper Lovato', 'Julia Witte Zimmerman', 'Isabelle Smith', 'Peter Dodds', 'Jennifer L. Karson']","['University of Vermont', 'University of Vermont', 'University of Vermont', 'University of Vermont', 'University of Vermont']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31690,Transparency & Explainability,Social Scoring Systems for Behavioral Regulation: An Experiment on the Role of Transparency in Determining Perceptions and Behaviors,"Recent developments in artificial intelligence research have advanced the spread of automated decision-making (ADM) systems used for regulating human behaviors. In this context, prior work has focused on the determinants of human trust in and the legitimacy of ADM systems, e.g., when used for decision support. However, studies assessing people's perceptions of ADM systems used for behavioral regulation, as well as the effect on behaviors and the overall impact on human communities are largely absent. In this paper, we experimentally investigate people's behavioral adaptations to, and their perceptions of an institutionalized decision-making system, which resembled a social scoring system. Using social scores as incentives, the system aimed at ensuring mutual fair treatment between members of experimental communities. We explore how the provision of transparency affected people’s perceptions, behaviors, as well as the well-being of the communities. While a non-transparent scoring system led to disparate impacts both within as well as across communities, transparency helped people develop trust in each other, create wealth, and enabled them to benefit from the system in a more uniform manner. A transparent system was perceived as more effective, procedurally just, and legitimate, and led people to rely more strongly on the system. However, transparency also made people strongly discipline those with a low score. This suggests that social scoring systems that precisely disclose past behaviors may also impose significant discriminatory consequences on individuals deemed non-compliant.",[],[],"['Carmen Loefflad', 'Mo Chen', 'Jens Grossklags']","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich']","['Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31696,Fairness & Bias,Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance,"Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.",[],[],"['Franck Michel', 'Fabien Gandon']","['Université Côte d’Azur, CNRS, Inria', 'Université Côte d’Azur, Inria, CNRS']","['United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AIES/article/view/31695,Transparency & Explainability,Lessons from Clinical Communications for Explainable AI,"One of the major challenges in the use of opaque, complex AI models is the need or desire to provide an explanation to the end-user (and other stakeholders) as to how the system arrived at the answer it did. While there is significant research in the development of explainability techniques for AI, the question remains as to who needs an explanation, what an explanation consists of, and how to communicate this to a lay user who lacks direct expertise in the area. In this position paper, an interdisciplinary team of researchers argue that the example of clinical communications offers lessons to those interested in improving the transparency and interpretability of AI systems. We identify five lessons from clinical communications: (1) offering explanations for AI systems and disclosure of their use recognizes the dignity of those using and impacted by it; (2) AI explanations can be productively targeted rather than totally comprehensive; (3) AI explanations can be enforced through codified rules but also norms, guided by core values; (4) what constitutes a “good” AI explanation will require repeated updating due to changes in technology and social expectations; 5) AI explanations will have impacts beyond defining any one AI system, shaping and being shaped by broader perceptions of AI. We review the history, debates and consequences surrounding the institutionalization of one type of clinical communication, informed consent, in order to illustrate the challenges and opportunities that may await attempts to offer explanations of opaque AI models. We highlight takeaways and implications for computer scientists and policymakers in the context of growing concerns and moves toward AI governance.",[],[],"['Alka V. Menon', 'Zahra Abba Omar', 'Nadia Nahar', 'Xenophon Papademetris', 'Lynn E. Fiellin', 'Christian Kästner']","['Yale University', 'Yale University', 'Carnegie Mellon University', 'Yale University', 'Dartmouth College', 'Carnegie Mellon University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31700,Transparency & Explainability,Habemus a Right to an Explanation: so What? – A Framework on Transparency-Explainability Functionality and Tensions in the EU AI Act,"The European Union's Artificial Intelligence Act (AI Act), finalized in February 2024, mandates comprehensive transparency and explainability requirements for AI systems to enable effective oversight and safeguard fundamental rights. However, the practical implementation of these requirements faces challenges due to tensions between the need for meaningful explanations and the potential risks to intellectual property and commercial interests of AI providers. This research proposes the Transparency-Explainability Functionality and Tensions (TEFT) framework to systematically analyze the complex interplay of legal, technical, and socio-ethical factors shaping the realization of algorithmic transparency and explainability in the EU context. Through a two-pronged approach combining a focused literature review and an in-depth examination of the AI Act's provisions, we identify key friction points and challenges in operationalizing the right to explanation. The TEFT framework maps the interests and incentives of various stakeholders, including AI providers & deployers, oversight bodies, and affected individuals, while considering their goals, expected benefits, risks, possible negative impacts, and context to algorithmic explainability.",[],[],['Luca Nannini'],"['Minsait by Indra\nCiTIUS, University of Santiago de Compostela']",['Spain']
https://ojs.aaai.org/index.php/AIES/article/view/31703,Transparency & Explainability,Measuring Human-AI Value Alignment in Large Language Models,"This paper seeks to quantify the human-AI value alignment in large language models. Alignment between humans and AI has become a critical area of research to mitigate potential harm posed by AI. In tandem with this need, developers have incorporated a values-based approach towards model development where ethical principles are integrated from its inception. However, ensuring that these values are reflected in outputs remains a challenge. In addition, studies have noted that models lack consistency when producing outputs, which in turn can affect their function. Such variability in responses would impact human-AI value alignment as well, particularly where consistent alignment is critical. Fundamentally, the task of uncovering a model’s alignment is one of explainability – where understanding how these complex models behave is essential in order to assess their alignment.   This paper examines the problem through a case study of GPT-3.5. By repeatedly prompting the model with scenarios based on a dataset of moral stories, we aggregate the model’s alignment with human values to produce a human-AI value alignment metric. Moreover, by using a comprehensive taxonomy of human values, we uncover the latent value profile represented by these outputs, thereby determining the extent of human-AI value alignment.",[],[],"['Hakim Norhashim', 'Jungpil Hahn']","['National University of Singapore\nAI Singapore', 'National University of Singapore\nAI Singapore']","['Singapore', 'Singapore']"
https://ojs.aaai.org/index.php/AIES/article/view/31702,Transparency & Explainability,"AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations","For many use-cases, it is often important to explain the prediction of a black-box model by identifying the most influential training data samples. Existing approaches lack customization for user intent and often provide a homogeneous set of explanation samples, failing to reveal the model's reasoning from different angles.   In this paper, we propose AIDE, an approach for providing antithetical (i.e., contrastive), intent-based, diverse explanations for opaque and complex models. AIDE distinguishes three types of explainability intents: interpreting a correct, investigating a wrong, and clarifying an ambiguous prediction. For each intent, AIDE selects an appropriate set of influential training samples that support or oppose the prediction either directly or by contrast. To provide a succinct summary, AIDE uses diversity-aware sampling to avoid redundancy and increase coverage of the training data.   We demonstrate the effectiveness of AIDE on image and text classification tasks, in three ways:  quantitatively, assessing correctness and continuity;  qualitatively, comparing anecdotal evidence from AIDE and other example-based approaches; and via a user study, evaluating multiple aspects of AIDE. The results show that AIDE addresses the limitations of existing methods and exhibits desirable traits for an explainability method.",[],[],"['Ikhtiyor Nematov', 'Dimitris Sacharidis', 'Katja Hose', 'Tomer Sagi']","['Université Libre de Bruxelles, Belgium\nAalborg University, Denmark', 'Université Libre de Bruxelles, Belgium', 'TU Wien, Austria', 'Aalborg University, Denmark']","['Denmark', 'Belgium', 'Austria', 'Denmark']"
https://ojs.aaai.org/index.php/AIES/article/view/31707,Fairness & Bias,Face the Facts: Using Face Averaging to Visualize Gender-by-Race Bias in Facial Analysis Algorithms,"We applied techniques from psychology --- typically used to visualize human bias --- to facial analysis systems, providing novel approaches for diagnosing and communicating algorithmic bias. First, we aggregated a diverse corpus of human facial images (N=1492) with self-identified gender and race. We tested four automated gender recognition (AGR) systems and found that some exhibited intersectional gender-by-race biases. Employing a technique developed by psychologists --- face averaging --- we created composite images to visualize these systems' outputs. For example, we visualized what an ""average woman"" looks like, according to a system's output. Second, we conducted two online experiments wherein participants judged the bias of hypothetical AGR systems. The first experiment involved participants (N=228) from a convenience sample. When depicting the same results in different formats, facial visualizations communicated bias to the same magnitude as statistics. In the second experiment with only Black participants (N=223), facial visualizations communicated bias significantly more than statistics, suggesting that face averages are meaningful for communicating algorithmic bias.",[],[],"['Kentrell Owens', 'Erin Freiburger', 'Ryan Hutchings', 'Mattea Sim', 'Kurt Hugenberg', 'Franziska Roesner', 'Tadayoshi Kohno']","['Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Psychological and Brain Sciences, Indiana University Bloomington', 'Psychological and Brain Sciences, Indiana University Bloomington', 'Psychological and Brain Sciences, Indiana University Bloomington', 'Psychological and Brain Sciences, Indiana University Bloomington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31705,Fairness & Bias,Hidden or Inferred: Fair Learning-To-Rank With Unknown Demographics,"As learning-to-rank models are increasingly deployed for decision-making in areas with profound life implications, the FairML community has been developing fair learning-to-rank (LTR) models. These models rely on the availability of sensitive demographic features such as race or sex. However, in practice, regulatory obstacles and privacy concerns protect this data from collection and use. As a result, practitioners may either need to promote fairness despite the absence of these features or turn to demographic inference tools to attempt to infer them. Given that these tools are fallible, this paper aims to further understand how errors in demographic inference impact the fairness performance of popular fair LTR strategies. In which cases would it be better to keep such demographic attributes hidden from models versus infer them? We examine a spectrum of fair LTR strategies ranging from fair LTR with and without demographic features hidden versus inferred to fairness-unaware LTR followed by fair re-ranking. We conduct a controlled empirical investigation modeling different levels of inference errors by systematically perturbing the inferred sensitive attribute. We also perform three case studies with real-world datasets and popular open-source inference methods. Our findings reveal that as inference noise grows, LTR-based methods that incorporate fairness considerations into the learning process may increase bias. In contrast, fair re-ranking strategies are more robust to inference errors. All source code, data, and experimental artifacts of our experimental study are available here: https://github.com/sewen007/hoiltr.git",[],[],"['Oluseun Olulana', 'Kathleen Cachel', 'Fabricio Murai', 'Elke Rundensteiner']","['Worcester Polytechnic Institute', 'Worcester Polytechnic Institute', 'Worcester Polytechnic Institute', 'Worcester Polytechnic Institute']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31708,Privacy & Data Governance,Proxy Fairness under the European Data Protection Regulation and the AI Act: A Perspective of Sensitivity and Necessity,"This paper navigates the convergence of the European Data Protection Regulation and the AI Act within the paradigm of computational methods that operationalise fairness in the absence of demographic data, notably through the use of proxy variables and inferential techniques (Proxy Fairness). Particularly, it explores the legal nature of the data involved in Proxy Fairness under the European Data Protection Regulation, focusing on the legal notion of Sensitivity. Moreover, it examines the lawfulness of processing sensitive personal data for Proxy Fairness purposes under the AI Act, particularly focusing on the legal requirement of Necessity. Through this analysis, the paper aims to shed light on core aspects of the legitimacy of Proxy Fairness in the context of EU law, providing a normative foundation to this line of Fair-AI approaches.",[],[],['Ioanna Papageorgiou'],"['Leibniz University Hannover, Hannover, Germany\nInstitute for Legal Informatics, Hannover, Germany']",['Germany']
https://ojs.aaai.org/index.php/AIES/article/view/31709,Fairness & Bias,A Model- and Data-Agnostic Debiasing System for Achieving Equalized Odds,"As reliance on Machine Learning (ML) systems in real-world decision-making processes grows, ensuring these systems are free of bias against sensitive demographic groups is of increasing importance. Existing techniques for automatically debiasing ML models generally require access to either the models’ internal architectures, the models’ training datasets, or both. In this paper we outline the reasons why such requirements are disadvantageous, and present an alternative novel debiasing system that is both data- and model-agnostic. We implement this system as a Reinforcement Learning Agent and through extensive experiments show that we can debias a variety of target ML model architectures over three benchmark datasets. Our results show performance comparable to data- and/or model-gnostic state-of-the-art debiasers.",[],[],"['Thomas Pinkava', 'Jack McFarland', 'Afra Mashhadi']","['University of Washington', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31711,Transparency & Explainability,Disengagement through Algorithms: How Traditional Organizations Aim for Experts' Satisfaction,"This study examines the use of algorithmic tools in traditional organizational decision-making processes. Through forty semi-structured interviews with managers, engineers, and (expert) users across six European projects, we suggest that initiators deploy algorithms not to automate actions or replace users, but to disengage themselves from prescriptive decision-making. Consequently, the responsibility to choose, select, and decide falls upon the users; they become engaged. Therefore, algorithm evaluation is oriented towards utility, interpretability, and, more broadly, user satisfaction. Further research is encouraged to analyze the advent of a 'satisfaction regime', from platforms to traditional organizations.",[],[],['Jérémie Poiroux'],['The University of Tokyo'],['Japan']
https://ojs.aaai.org/index.php/AIES/article/view/31710,Transparency & Explainability,CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models,"This paper introduces the ""CIVICS: Culturally-Informed \& Values-Inclusive Corpus for Societal impacts"" dataset, designed to evaluate the social and cultural variation of Large Language Models (LLMs) towards socially sensitive topics across multiple languages and cultures. The hand-crafted, multilingual dataset of statements addresses value-laden topics, including LGBTQI rights, social welfare, immigration, disability rights, and surrogacy. CIVICS is designed to elicit responses from LLMs to shed light on how values encoded in their parameters shape their behaviors. Through our dynamic annotation processes, tailored prompt design, and experiments, we investigate how open-weight LLMs respond to these issues, exploring their behavior across diverse linguistic and cultural contexts. Using two experimental set-ups based on log-probabilities and long-form responses, we show social and cultural variability across different LLMs. Specifically, different topics and sources lead to more pronounced differences across model answers, particularly on immigration, LGBTQI rights, and social welfare. Experiments on generating long-form responses from models tuned for user chat demonstrate that refusals are triggered disparately across different models, but consistently and more frequently in English or translated statements. As shown by our initial experimentation, the CIVICS dataset can serve as a tool for future research, promoting reproducibility and transparency across broader linguistic settings, and furthering the development of AI technologies that respect and reflect global cultural diversities and value pluralism.  The CIVICS dataset and tools are made available under open licenses at hf.co/CIVICS-dataset.",[],[],"['Giada Pistilli', 'Alina Leidinger', 'Yacine Jernite', 'Atoosa Kasirzadeh', 'Alexandra Sasha Luccioni', 'Margaret Mitchell']","['Hugging Face', 'University of Amsterdam', 'Hugging Face', 'Carnegie Mellon University', 'Hugging Face', 'Hugging Face']","['United States', 'Netherlands', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31713,Transparency & Explainability,What to Trust When We Trust Artificial Intelligence (Extended Abstract),"What to Trust When We Trust Artificial Intelligence  Abstract: So-called “trustworthy AI” has emerged as a guiding aim of industry leaders, computer and data science researchers, and policy makers in the US and Europe. Often, trustworthy AI is characterized in terms of a list of criteria. These lists usually include at least fairness, accountability, and transparency. Fairness, accountability, and transparency are valuable objectives, and they have begun to receive attention from philosophers and legal scholars. However, those who put forth criteria for trustworthy AI have failed to explain why satisfying the criteria makes an AI system—or the organizations that make use of the AI system—worthy of trust. Nor do they explain why the aim of trustworthy AI is important enough to justify devoting resources to achieve it. It even remains unclear whether an AI system is the sort of thing that can be trustworthy or not.  To explain why fairness, accountability, and transparency are suitable criteria for trustworthy AI one needs an analysis of trustworthy AI. Providing an analysis of trustworthy AI is a distinct task from providing criteria. Criteria are diagnostic; they provide a useful test for the phenomenon of interest, but they do not purport to explain the nature of the phenomenon. It is conceivable that an AI system could lack transparency, accountability, or fairness while remaining trustworthy. An analysis of trustworthy AI provides the fundamental features of an AI system in virtue of which it is (or is not) worthy of trust. An AI system that lacks these features will, necessarily, fail to be worthy of trust. This paper puts forward an analysis of trustworthy AI that can be used to critically evaluate criteria for trustworthy AI such as fairness, accountability, and transparency.   In this paper we first make clear the target concept to be analyzed: trustworthy AI. We argue that AI, at least in its current form, should be understood as a distributed, complex system embedded in a larger institutional context. This characterization of AI is consistent with recent definitions proposed by national and international regulatory bodies, and it eliminates some unhappy ambiguity in the common usage of the term. We further limit the scope of our discussion to AI systems which are used to inform decision-making about qualification problems, problems wherein a decision-maker must decide whether an individual is qualified for some beneficial or harmful treatment. We argue that, given reasonable assumptions about the nature of trust and trustworthiness, only AI systems that are used to inform decision-making about qualification problems are appropriate candidates for attributions of (un)trustworthiness.  We then distinguish between two models of trust and trustworthiness that we find in the existing literature. We motivate our account by highlighting this as a dilemma in in the accounts of trustworthy AI that have previously been offered. These accounts claim that trustworthiness is either exclusive to full agents (and it is thus nonsense when we talk of trustworthy AI), or they offer an account of trustworthiness that collapses into mere reliability. The first sort of account we refer to as an agential account and the second sort we refer to as a reliability account. We offer that one of the core challenges of putting forth an account of trustworthy AI is to avoid reducing to one of these two camps. It is thus a desideratum of our account that it avoids being exclusive to full moral agents, while it simultaneously avoids capturing things such as mere tools. We go on to propose our positive account which we submit avoids these twin pitfalls.  We subsequently argue that if AI can be trustworthy, then it will be trustworthy on an institutional model. Starting from an account of institutional trust offered by Purves and Davis, we argue that trustworthy AI systems have three features: they are competent with regard to the task they are assigned, they are responsive to the morally salient facts governing the decision-making context in which they are deployed, and they publicly provide evidence of these features. As noted, this account builds on a model of institutional trust offered by Purves and Davis and an account of default trust from Margaret Urban Walker. The resulting account allows us to accommodate the core challenge of finding a balance between agential accounts and reliability accounts. We go on to refine our account, answer objections, and revisit the list criteria from above as explained in terms of competence, responsiveness, and evidence.",[],[],"['Duncan Purves', 'Schuyler Sturm', 'John Madock']","['University of Florida', 'University of Florida', 'University of Florida']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31712,Security,Not Oracles of the Battlefield: Safety Considerations for AI-Based Military Decision Support Systems,"AI-based military decision support systems that help commanders observe, orient, decide, and act on the battlefield are highly sought after by military leadership. With the advent of large language models, AI developers have begun advertising automated AI-based decision support systems designed to both analyze and act on data from the battlefield. While the desire to use decision support systems to make better decisions on the battlefield is unsurprising, the responsible deployment of such systems requires a clear understanding of the capabilities and limitations of modern machine learning models. This paper reviews recently proposed uses of AI-enables decision support systems (DSS), provides a simplified framework for considering AI-DSS capabilities and limitations, and recommends practical risk mitigations commanders might employ when operating with an AI-enabled DSS.",[],[],"['Emelia Probasco', 'Matthew Burtell', 'Helen Toner', 'Tim G. J. Rudner']","['Georgetown University', 'Georgetown University', 'Georgetown University', 'New York University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31715,Fairness & Bias,"Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis","Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model’s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",[],[],"['Chahat Raj', 'Anjishnu Mukherjee', 'Aylin Caliskan', 'Antonios Anastasopoulos', 'Ziwei Zhu']","['George Mason University', 'George Mason University', 'University of Washington', 'George Mason University', 'George Mason University']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31714,Transparency & Explainability,PPS: Personalized Policy Summarization for Explaining Sequential Behavior of Autonomous Agents,"AI-enabled agents designed to assist humans are gaining traction in a variety of domains such as healthcare and disaster response. It is evident that, as we move forward, these agents will play increasingly vital roles in our lives. To realize this future successfully and mitigate its unintended consequences, it is imperative that humans have a clear understanding of the agents that they work with. Policy summarization methods help facilitate this understanding by showcasing key examples of agent behaviors to their human users. Yet, existing methods produce “one-size-fits-all” summaries for a generic audience ahead of time. Drawing inspiration from research in pedagogy, we posit that personalized policy summaries can more effectively enhance user understanding. To evaluate this hypothesis, this paper presents and benchmarks a novel technique: Personalized Policy Summarization (PPS). PPS discerns a user’s mental model of the agent through a series of algorithmically generated questions and crafts customized policy summaries to enhance user understanding. Unlike existing methods, PPS actively engages with users to gauge their comprehension of the agent behavior, subsequently generating tailored explanations on the fly. Through a combination of numerical and human subject experiments, we confirm the utility of this personalized approach to explainable AI.",[],[],"['Peizhu Qian', 'Harrison Huang', 'Vaibhav Unhelkar']","['Rice University', 'Rice University', 'Rice University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31716,Fairness & Bias,Learning When Not to Measure: Theorizing Ethical Alignment in LLMs,"LLMs and other forms of generative AI have shown immense promise in producing highly accurate epistemic judgements in domains as varied as law, education, and medicine – with GPT notably passing the legal Bar exam and various medical licensing exams. The safe extension of LLMs into safety-critical professional domains requires assurance not only of epistemic but ethical alignment. This paper adopts a theoretical and philosophical approach, drawing from metaethical theories to argue for a distinction hinging around quantitative, axiological comparability that separates Kantian ethics from not only the utilitarianism it is well-known to oppose, but from just distribution theories as well, which are key to debiasing LLM models. It presents the novel hypothesis that LLM ethical acquisition from both corpus induction and RLHF may encounter value conflicts between Kantian and just distribution principles that intensify as they come into improved alignment with both theories, hinging around the variability by which self-attention may statistically attend to the same characterizations as more person-like or more resource-like under distinct prompting strategies.",[],[],['William Rathje'],"['University of California, Berkeley']",['United States']
https://ojs.aaai.org/index.php/AIES/article/view/31717,Security,Gaps in the Safety Evaluation of Generative AI,"Generative AI systems produce a range of ethical and social risks. Evaluation of these risks is a critical step on the path to ensuring the safety of these systems. However, evaluation requires the availability of validated and established measurement approaches and tools. In this paper, we provide an empirical review of the methods and tools that are available for evaluating known safety of generative AI systems to date. To this end, we review more than 200 safety-related evaluations that have been applied to generative AI systems. We categorise each evaluation along multiple axes to create a detailed snapshot of the safety evaluation landscape to date. We release this data for researchers and AI safety practitioners (https://bitly.ws/3hUzu). Analysing the current safety evaluation landscape reveals three systemic ”evaluation gaps”. First, a ”modality gap” emerges as few safety evaluations exist for non-text modalities. Second, a ”risk coverage gap” arises as evaluations for several ethical and social risks are simply lacking. Third, a ”context gap” arises as most safety evaluations are model-centric and fail to take into account the broader context in which AI systems operate. Devising next steps for safety practitioners based on these findings, we present tactical ”low-hanging fruit” steps towards closing the identified evaluation gaps and their limitations. We close by discussing the role and limitations of safety evaluation to ensure the safety of generative AI systems.",[],[],"['Maribeth Rauh', 'Nahema Marchal', 'Arianna Manzini', 'Lisa Anne Hendricks', 'Ramona Comanescu', 'Canfer Akbulut', 'Tom Stepleton', 'Juan Mateos-Garcia', 'Stevie Bergman', 'Jackie Kay', 'Conor Griffin', 'Ben Bariach', 'Iason Gabriel', 'Verena Rieser', 'William Isaac', 'Laura Weidinger']","['Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31722,Fairness & Bias,Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes,"Biased information (recently termed bisinformation) continues to be taught in medical curricula, often long after having been debunked. In this paper, we introduce bricc, a first-in-class initiative that seeks to mitigate medical bisinformation using machine learning to systematically identify and flag text with potential biases, for subsequent review in an expert-in-the-loop fashion, thus greatly accelerating an otherwise labor-intensive process. We have developed a gold-standard bricc dataset throughout several years containing over 12K pages of instructional materials.  Medical experts meticulously annotated these documents for bias according to comprehensive coding guidelines, emphasizing gender, sex, age, geography, ethnicity, and race. Using this labeled dataset, we trained, validated, and tested medical bias classifiers. We test three classifier approaches: a binary type-specific classifier, a general bias classifier; an ensemble combining bias type-specific classifiers independently-trained; and a multi-task learning (MTL) model tasked with predicting both general and type-specific biases. While MTL led to some improvement on race bias detection in terms of F1-score, it did not outperform binary classifiers trained specifically on each task. On general bias detection, the binary classifier achieves up to 0.923 of AUC, a 27.8% improvement over the baseline. This work lays the foundations for debiasing medical curricula by exploring a novel dataset and evaluating different training model strategies. Hence, it offers new pathways for more nuanced and effective mitigation of bisinformation.",[],[],"['Chiman Salavati', 'Shannon Song', 'Willmar Sosa Diaz', 'Scott A. Hale', 'Roberto E. Montenegro', 'Fabricio Murai', 'Shiri Dori-Hacohen']","['University of Connecticut', 'Worcester Polytechnic Institute', 'University of Connecticut', 'Meedan\nUniversity of Oxford', 'University of Washington', 'Worcester Polytechnic Institute', 'University of Connecticut']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31724,Fairness & Bias,Algorithms and Recidivism: A Multi-disciplinary Systematic Review,"The adoption of algorithms across different jurisdictions have transformed the workings of the criminal justice system, particularly in predicting recidivism risk for bail, sentencing, and parole decisions. This shift from human decision-making to statistical or algorithmic tool-assisted decision-making has prompted discussions regarding the legitimacy of such adoption. Our paper presents the results of a systematic review of the literature on criminal recidivism, spanning both legal and empirical perspectives. By coalescing different approaches, we highlight the most prominent themes that have garnered the attention of researchers so far and some that warrant further investigation.",[],[],"['Arul George Scaria', 'Vidya Subramanian', 'Nevin K. George', 'Nandana Sengupta']","['National Law School of India University, Bengaluru', 'National Law School of India University, Bengaluru', 'Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi']","['India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AIES/article/view/31729,Transparency & Explainability,Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges,"Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.",[],[],"['Ishana Shastri', 'Shomik Jain', 'Barbara Engelhardt', 'Ashia Wilson']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Stanford University', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31731,Fairness & Bias,Individual Fairness in Graphs Using Local and Global Structural Information,"Graph neural networks are powerful graph representation learners in which node representations are highly influenced by features of neighboring nodes. Prior work on individual fairness in graphs has focused only on node features rather than structural issues. However, from the perspective of fairness in high-stakes applications, structural fairness is also important, and the learned representations may be systematically and undesirably biased against unprivileged individuals due to a lack of structural awareness in the learning process. In this work, we propose a pre-processing bias mitigation approach for individual fairness that gives importance to local and global structural features. We mitigate the local structure discrepancy of the graph embedding via a locally fair PageRank method. We address the global structure disproportion between pairs of nodes by introducing truncated singular value decomposition-based pairwise node similarities. Empirically, the proposed pre-processed fair structural features have superior performance in individual fairness metrics compared to the state-of-the-art methods while maintaining prediction performance.",[],[],"['Yonas Sium', 'Qi Li', 'Kush R. Varshney']","['Iowa State University', 'Iowa State University', 'International Business Machines']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31730,Transparency & Explainability,Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates,"When designing agents for operation in uncertain environments, designers need tools to automatically reason about what agents ought to do, how that conflicts with what is actually happening, and how a policy might be modified to remove the conflict. These obligations include ethical and social obligations, permissions and prohibitions, which constrain how the agent achieves its mission and executes its policy. We propose a new deontic logic, Expected Act Utilitarian deontic logic, for enabling this reasoning at design time: for specifying and verifying the agent's strategic obligations, then modifying its policy from a reference policy to meet those obligations. Unlike approaches that work at the reward level, working at the logical level increases the transparency of the trade-offs. We introduce two algorithms: one for model-checking whether an RL agent has the right strategic obligations, and one for modifying a reference decision policy to make it meet obligations expressed in our logic. We illustrate our algorithms on DAC-MDPs which accurately abstract neural decision policies, and on toy gridworld environments.",[],[],"['Colin Shea-Blymyer', 'Houssam Abbas']","['Georgetown University', 'Oregon State University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31732,Fairness & Bias,Fairness in AI-Based Mental Health: Clinician Perspectives and Bias Mitigation,"There is limited research on fairness in automated decision-making systems in the clinical domain, particularly in the mental health domain. Our study explores clinicians' perceptions of AI fairness through two distinct scenarios: violence risk assessment and depression phenotype recognition using textual clinical notes. We engage with clinicians through semi-structured interviews to understand their fairness perceptions and to identify appropriate quantitative fairness objectives for these scenarios. Then, we compare a set of bias mitigation strategies developed to improve at least one of the four selected fairness objectives. Our findings underscore the importance of carefully selecting fairness measures, as prioritizing less relevant measures can have a detrimental rather than a beneficial effect on model behavior in real-world clinical use.",[],[],"['Gizem Sogancioglu', 'Pablo Mosteiro', 'Albert Ali Salah', 'Floortje Scheepers', 'Heysem Kaya']","['Utrecht University', 'Utrecht University', 'Utrecht University', 'University Medical Center Utrecht', 'Utrecht University']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AIES/article/view/31733,Transparency & Explainability,Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries,"Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both.  Auditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms’ compliance with regulation. Auditing is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively.  First, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions. On the basis of advanced AI’s risk profile, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits. Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight. Secondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities. Public bodies’ capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences.",[],[],"['Merlin Stein', 'Milan Gandhi', 'Theresa Kriecherbauer', 'Amin Oueslati', 'Robert Trager']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'Hertie School', 'University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AIES/article/view/31734,Transparency & Explainability,"Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance","Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.",[],[],"['Mohammad Tahaei', 'Daricia Wilkinson', 'Alisa Frik', 'Michael Muller', 'Ruba Abu-Salma', 'Lauren Wilcox']","['International Computer Science Institute\neBay', 'Arizona State University', 'International Computer Science Institute', 'IBM Research', 'King’s College London', 'eBay']","['Brazil', 'United States', 'Brazil', '', 'United States', 'Philippines']"
https://ojs.aaai.org/index.php/AIES/article/view/31736,Security,Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents,"Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents: a promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents; however, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., focused on maximizing outcomes over time), norm-based (i.e., conforming to specific norms), or virtue-based (i.e., considering a combination of different virtues). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using an Iterated Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain types of moral agents are able to steer selfish agents towards more cooperative behavior.",[],[],"['Elizaveta Tennant', 'Stephen Hailes', 'Mirco Musolesi']","['University College London', 'University College London', 'University College London\nUniversity of Bologna']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31740,Security,"Medical AI, Categories of Value Conflict, and Conflict Bypasses","It is becoming clear that, in the process of aligning AI with human values, one glaring ethical problem is that of value conflict. It is not obvious what we should do when two compelling values (such as autonomy and safety) come into conflict with one another in the design or implementation of a medical AI technology. This paper shares findings from a scoping review at the intersection of three concepts—AI, moral value, and health—that have to do with value conflict and arbitration. The paper looks at some important and unique cases of value conflict, and then describes three possible categories of value conflict: personal value conflict, interpersonal or intercommunal value conflict, and definitional value conflict. It then describes three general paths forward in addressing value conflict: additional ethical theory, additional empirical evidence, and bypassing the conflict altogether. Finally, it reflects on the efficacy of these three paths forward as ways of addressing the three categories of value conflict, and motions toward what is needed for better approaching value conflicts in medical AI.",[],[],"['Gavin Victor', 'Jean-Christophe Bélisle-Pipon']","['Simon Fraser University', 'Simon Fraser University']","['Canada', 'Canada']"
https://ojs.aaai.org/index.php/AIES/article/view/31741,Fairness & Bias,Decoding Multilingual Moral Preferences: Unveiling LLM's Biases through the Moral Machine Experiment,"Large language models (LLMs) increasingly find their way into the most diverse areas of our everyday lives. They indirectly influence people's decisions or opinions through their daily use. Therefore, understanding how and which moral judgements these LLMs make is crucial. However, morality is not universal and depends on the cultural background. This raises the question of  whether these cultural preferences are also reflected in LLMs when prompted in different languages or whether moral decision-making is consistent across different languages. So far, most research has focused on investigating the inherent values of LLMs in English. While a few works conduct multilingual analyses of moral bias in LLMs in a multilingual setting, these analyses do not go beyond atomic actions. To the best of our knowledge, a multilingual analysis of moral bias in dilemmas has not yet been conducted.  To address this, our paper builds on the moral machine experiment (MME) to investigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and MPT, in a multilingual setting and compares them with the preferences collected from humans belonging to different cultures. To accomplish this, we generate 6500 scenarios of the MME and prompt the models in ten languages on which action to take. Our analysis reveals that all LLMs inhibit different moral biases to some degree and that they not only differ from the human preferences but also across multiple languages within the models themselves. Moreover, we find that almost all models, particularly Llama 3, divert greatly from human values and, for instance, prefer saving fewer people over saving more.",[],[],"['Karina Vida', 'Fabian Damken', 'Anne Lauscher']","['Data Science Group, Universität Hamburg, Germany', 'Technical University of Darmstadt, Germany', 'Data Science Group, Universität Hamburg, Germany']","['Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31742,Security,PICE: Polyhedral Complex Informed Counterfactual Explanations,"Polyhedral geometry can be used to shed light on the behaviour of piecewise linear neural networks, such as ReLU-based architectures.  Counterfactual explanations are a popular class of methods for examining model behaviour by comparing a query to the closest point with a different label, subject to constraints.  We present a new algorithm, Polyhedral-complex Informed Counterfactual Explanations (PICE), which leverages the decomposition of the piecewise linear neural network into a polyhedral complex to find counterfactuals that are provably minimal in the Euclidean norm and exactly on the decision boundary for any given query.  Moreover, we develop variants of the algorithm that target popular counterfactual desiderata such as sparsity, robustness, speed, plausibility, and actionability.  We empirically show on four publicly available real-world datasets that our method outperforms other popular techniques to find counterfactuals and adversarial attacks by distance to decision boundary and distance to query. Moreover, we successfully improve our baseline method in the dimensions of the desiderata we target, as supported by experimental evaluations.",[],[],"['Mattia Jacopo Villani', 'Emanuele Albini', 'Shubham Sharma', 'Saumitra Mishra', 'Salim Ibrahim Amoukou', 'Daniele Magazzeni', 'Manuela Veloso']","[""King's College London\nJ.P. Morgan Chase"", 'J.P. Morgan Chase', 'J.P. Morgan Chase', 'J.P. Morgan Chase', 'J.P. Morgan Chase', 'J.P. Morgan Chase', 'J.P. Morgan Chase']","['India', 'India', 'India', 'India', 'India', 'India', 'India']"
https://ojs.aaai.org/index.php/AIES/article/view/31745,Transparency & Explainability,How Do AI Companies “Fine-Tune” Policy? Examining Regulatory Capture in AI Governance,"Industry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.",[],[],"['Kevin Wei', 'Carson Ezell', 'Nick Gabrieli', 'Chinmay Deshpande']","['RAND\nHarvard Law School', 'Harvard University', 'Harvard University', 'Harvard University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31748,Transparency & Explainability,"Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval","Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1% of cases and female-associated names in only 11.1% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have  implications for widely used AI tools that are automating employment, fairness, and tech policy.",[],[],"['Kyra Wilson', 'Aylin Caliskan']","['University of Washington', 'University of Washington']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31747,Fairness & Bias,A Relational Justification of AI Democratization,"While much has been written about what democratized AI should look like, there has been surprisingly little attention for the normative grounds of AI democratization. Existing calls for AI democratization that do make explicit arguments broadly fall into two categories: outcome-based and legitimacy-based, corresponding to outcome-based and process-based views of procedural justice respectively. This paper argues that we should favor relational justifications of AI democratization to outcome-based ones, because the former additionally provide outcome-independent reasons for AI democratization. Moreover, existing legitimacy-based arguments often leave the why of AI democratization implicit and instead focus on the how. We present two relational arguments for AI democratization: one based on empirical findings regarding the perceived importance of relational features of decision-making procedures, and one based on Iris Marion Young’s conception of justice, according to which the main forms of injustice are domination and oppression. We show how these arguments lead to requirements for procedural fairness and thus also offer guidance on the how of AI democratization. Finally, we consider several objections to AI democratization, including worries concerning epistemic exploitation.",[],[],"['Bauke Wielinga', 'Stefan Buijsman']","['Delft University of Technology', 'Delft University of Technology']","['Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AIES/article/view/31746,Fairness & Bias,Automate or Assist? The Role of Computational Models in Identifying Gendered Discourse in US Capital Trial Transcripts,"The language used by US courtroom actors in criminal trials has long been studied for biases. However, systematic studies for bias in high-stakes court trials have been difficult, due to the nuanced nature of bias and the legal expertise required.  Large language models offer the possibility to automate annotation. But validating the computational approach requires both an understanding of how automated methods fit in existing annotation workflows and what they really offer.  We present a case study of adding a computational model to a complex and high-stakes problem: identifying gender-biased language in US capital trials for women defendants. Our team of experienced death-penalty lawyers and NLP technologists pursue a three-phase study: first annotating manually, then training and evaluating computational models, and finally comparing expert annotations to model predictions.  Unlike many typical NLP tasks, annotating for gender bias in months-long capital trials is complicated, with many individual judgment calls.  Contrary to standard arguments for automation that are based on efficiency and scalability, legal experts find the computational models most useful in providing opportunities to reflect on their own bias in annotation and to build consensus on annotation rules.  This experience suggests that seeking to replace experts with computational models for complex annotation is both unrealistic and undesirable. Rather, computational models offer valuable opportunities to assist the legal experts in annotation-based studies.",[],[],"['Andrea W Wen-Yi', 'Kathryn Adamson', 'Nathalie Greenfield', 'Rachel Goldberg', 'Sandra Babcock', 'David Mimno', 'Allison Koenecke']","['Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Cornell University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31751,Transparency & Explainability,ML-EAT: A Multilevel Embedding Association Test  for Interpretable and Transparent Social Science,"This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.",[],[],"['Robert Wolfe', 'Alexis Hiniker', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31750,Transparency & Explainability,The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations,"Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.",[],[],"['Robert Wolfe', 'Tanushree Mitra']","['University of Washington', 'University of Washington']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31752,Fairness & Bias,"Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study","Popular and news media often portray teenagers with sensationalism, as both a risk to society and at risk from society. As AI begins to absorb some of the epistemic functions of traditional media, we study how teenagers in two countries speaking two languages: 1) are depicted by AI, and 2) how they would prefer to be depicted. Specifically, we study the biases about teenagers learned by static word embeddings (SWEs) and generative language models (GLMs), comparing these with the perspectives of adolescents living in the U.S. and Nepal. We find English-language SWEs associate teenagers with societal problems, and more than 50% of the 1,000 words most associated with teenagers in the pretrained GloVe SWE reflect such problems. Given prompts about teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss societal problems, most commonly violence, but also drug use, mental illness, and sexual taboo. Nepali models, while not free of such associations, are less dominated by social problems. Data from workshops with N=13 U.S. adolescents and N=18 Nepalese adolescents show that AI presentations are disconnected from teenage life, which revolves around activities like school and friendship. Participant ratings of how well 20 trait words describe teens are decorrelated from SWE associations, with Pearson's rho=.02, n.s. in English FastText and rho=.06, n.s. GloVe; and rho=.06, n.s. in Nepali FastText and rho=-.23, n.s. in GloVe. U.S. participants suggested AI could fairly present teens by highlighting diversity, while Nepalese participants centered positivity. Participants were optimistic that, if it learned from adolescents, rather than media sources, AI could help mitigate stereotypes. Our work offers an understanding of the ways SWEs and GLMs misrepresent a developmentally vulnerable group and provides a template for less sensationalized characterization.",[],[],"['Robert Wolfe', 'Aayushi Dangol', 'Bill Howe', 'Alexis Hiniker']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31754,Fairness & Bias,Stable Diffusion Exposed: Gender Bias from Prompt to Image,"Several studies have raised awareness about social biases in image generative models, demonstrating their predisposition towards stereotypes and imbalances. This paper contributes to this growing body of research by introducing an evaluation protocol that analyzes the impact of gender indicators at every step of the generation process on Stable Diffusion images. Leveraging insights from prior work, we explore how gender indicators not only affect gender presentation but also the representation of objects and layouts within the generated images. Our findings include the existence of differences in the depiction of objects, such as instruments tailored for specific genders, and shifts in overall layouts. We also reveal that neutral prompts tend to produce images more aligned with masculine prompts than their feminine counterparts. We further explore where bias originates through representational disparities and how it manifests in the images via prompt-image dependencies, and provide recommendations for developers and users to mitigate potential bias in image generation.",[],[],"['Yankun Wu', 'Yuta Nakashima', 'Noa Garcia']","['Osaka University', 'Osaka University', 'Osaka University']","['Japan', 'Japan', 'Japan']"
https://ojs.aaai.org/index.php/AIES/article/view/31753,Fairness & Bias,Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI,"Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we use a hierarchical clustering approach to show that dataset size predicts the extent to which the underlying structure of facial impression bias resembles that of facial impression bias in humans. Finally, we show that Stable Diffusion models employing CLIP as a text encoder learn facial impression biases, and that these biases intersect with racial biases in Stable Diffusion XL-Turbo. While pretrained CLIP models may prove useful for scientific studies of bias, they will also require significant dataset curation when intended for use as general-purpose models in a zero-shot setting.",[],[],"['Robert Wolfe', 'Aayushi Dangol', 'Alexis Hiniker', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31758,Transparency & Explainability,LLM Voting: Human Choices and AI Collective Decision-Making,"This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.",[],[],"['Joshua C. Yang', 'Damian Dailisan', 'Marcin Korecki', 'Carina I. Hausladen', 'Dirk Helbing']","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://ojs.aaai.org/index.php/AIES/article/view/31757,Transparency & Explainability,Tracing the Evolution of Information Transparency for OpenAI’s GPT Models through a Biographical Approach,"Information transparency, the open disclosure of information about models, is crucial for proactively evaluating the potential societal harm of large language models (LLMs) and developing effective risk mitigation measures. Adapting the biographies of artifacts and practices (BOAP) method from science and technology studies, this study analyzes the evolution of information transparency within OpenAI’s Generative Pre-trained Transformers (GPT) model reports and usage policies from its inception in 2018 to GPT-4, one of today’s most capable LLMs. To assess the breadth and depth of transparency practices, we develop a 9-dimensional, 3-level analytical framework to evaluate the comprehensiveness and accessibility of information disclosed to various stakeholders. Findings suggest that while model limitations and downstream usages are increasingly clarified, model development processes have become more opaque. Transparency remains minimal in certain aspects, such as model explainability and real-world evidence of LLM impacts, and the discussions on safety measures such as technical interventions and regulation pipelines lack in-depth details. The findings emphasize the need for enhanced transparency to foster accountability and ensure responsible technological innovations.",[],[],"['Zhihan Xu', 'Eni Mustafaraj']","['Wellesley College', 'Wellesley College']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31759,Privacy & Data Governance,You Still See Me: How Data Protection Supports the Architecture of  AI Surveillance,"Data forms the backbone of artificial intelligence (AI). Privacy and data protection laws thus have strong bearing on AI systems. Shielded by the rhetoric of compliance with data protection and privacy regulations, privacy-preserving techniques have enabled the extraction of more and new forms of data. We illustrate how the application of privacy-preserving techniques in the development of AI systems--from private set intersection as part of dataset curation to homomorphic encryption and federated learning as part of model computation--can further support surveillance infrastructure under the guise of regulatory permissibility. Finally, we propose technology and policy strategies to evaluate privacy-preserving techniques in light of the protections they actually confer. We conclude by highlighting the role that technologists could play in devising policies that combat surveillance AI technologies.",[],[],"['Rui-Jie Yew', 'Lucy Qin', 'Suresh Venkatasubramanian']","['Brown University', 'Georgetown University', 'Brown University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31760,Fairness & Bias,Mitigating Urban-Rural Disparities in Contrastive Representation Learning with Satellite Imagery,"Satellite imagery is being leveraged for many societally critical tasks across climate, economics, and public health. Yet, because of heterogeneity in landscapes (e.g. how a road looks in different places), models can show disparate performance across geographic areas. Given the important potential of disparities in algorithmic systems used in societal contexts, here we consider the risk of urban-rural disparities in identification of land-cover features. This is via semantic segmentation (a common computer vision task in which image regions are labelled according to what is being shown) which uses pre-trained image representations generated via contrastive self-supervised learning. We propose fair dense representation with contrastive learning (FairDCL) as a method for de-biasing the multi-level latent space of a convolution neural network. The method improves feature identification by removing spurious latent representations which are disparately distributed across urban and rural areas, and is achieved in an unsupervised way by contrastive pre-training. The pre-trained image representation mitigates downstream urban-rural prediction disparities and outperforms state-of-the-art baselines on real-world satellite images. Embedding space evaluation and ablation studies further demonstrate FairDCL’s robustness. As generalizability and robustness in geographic imagery is a nascent topic, our work motivates researchers to consider metrics beyond average accuracy in such applications.",[],[],"['Miao Zhang', 'Rumi Chunara']","['New York University', 'New York University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31761,Transparency & Explainability,Ontology of Belief Diversity: A Community-Based Epistemological Approach,"AI applications across classification, fairness, and human interaction  often  implicitly  require  ontologies  of  social  concepts.  Constructing  these  well  –  especially  when  there  are many relevant categories – is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which isa complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological  methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology’s utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models",[],[],"['Richard Zhang', 'Erin Van Liemt', 'Tyler Fischella']","['Google\nGoogle Deepmind', 'Google\nGoogle Research', 'Google']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31620,Security,Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation,"The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.",[],[],"['Julia Barnett', 'Kimon Kieslich', 'Nicholas Diakopoulos']","['Northwestern University', 'Institute for Information Law, University of Amsterdam', 'Northwestern University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31634,Privacy & Data Governance,Why Am I Still Seeing This: Measuring the Effectiveness of Ad Controls and Explanations in AI-Mediated Ad Targeting Systems,"Recently, Meta has shifted towards AI-mediated ad targeting mechanisms that do not require advertisers to provide detailed targeting criteria. The shift is likely driven by excitement over AI capabilities as well as the need to address new data privacy policies and targeting changes agreed upon in civil rights settlements. At the same time, in response to growing public concern about the harms of targeted advertising, Meta has touted their ad preference controls as an effective mechanism for users to exert control over the advertising they see. Furthermore, Meta markets their ""Why this ad"" targeting explanation as a transparency tool that allows users to understand the reasons for seeing particular ads and inform their actions to control what ads they see in the future.   Our study evaluates the effectiveness of Meta's ""See less"" ad control, as well as the actionability of ad targeting explanations following the shift to AI-mediated targeting. We conduct a large-scale study, randomly assigning participants the intervention of marking ""See less"" to either Body Weight Control or Parenting topics, and collecting the ads Meta shows to participants and their targeting explanations before and after the intervention. We find that utilizing the ""See less"" ad control for the topics we study does not significantly reduce the number of ads shown by Meta on these topics, and that the control is less effective for some users whose demographics are correlated with the topic. Furthermore, we find that the majority of ad targeting explanations for local ads made no reference to location-specific targeting criteria, and did not inform users why ads related to the topics they requested to ""See less"" of continued to be delivered. We hypothesize that the poor effectiveness of controls and lack of actionability and comprehensiveness in explanations are the result of the shift to AI-mediated targeting, for which explainability and transparency tools have not yet been developed by Meta. Our work thus provides evidence for the need of new methods for transparency and user control, suitable and reflective of how the increasingly complex and AI-mediated ad delivery systems operate.",[],[],"['Jane Castleman', 'Aleksandra Korolova']","['Princeton University', 'Princeton University']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31635,Security,Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities,"Harm reporting in Artificial Intelligence (AI) currently lacks a structured process for disclosing and addressing algorithmic flaws, relying largely on an ad-hoc approach. This contrasts sharply with the well-established Coordinated Vulnerability Disclosure (CVD) ecosystem in software security. While global efforts to establish frameworks for AI transparency and collaboration are underway, the unique challenges presented by machine learning (ML) models demand a specialized approach. To address this gap, we propose implementing a Coordinated Flaw Disclosure (CFD) framework tailored to the complexities of ML and AI issues. This paper reviews the evolution of ML disclosure practices, from ad hoc reporting to emerging participatory auditing methods, and compares them with cybersecurity norms. Our framework introduces innovations such as extended model cards, dynamic scope expansion, an independent adjudication panel, and an automated verification process. We also outline a forthcoming real-world pilot of CFD. We argue that CFD could significantly enhance public trust in AI systems. By balancing organizational and community interests, CFD aims to improve AI accountability in a rapidly evolving technological landscape.",[],[],"['Sven Cattell', 'Avijit Ghosh', 'Lucie-Aimée Kaffee']","['AI Village', 'Hugging Face', 'Hugging Face']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31638,Security,"MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks","The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MoJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting  jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.",[],[],"['Giandomenico Cornacchia', 'Giulio Zizzo', 'Kieran Fraser', 'Muhammad Zaid Hameed', 'Ambrish Rawat', 'Mark Purcell']","['IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe', 'IBM Research Europe']","['', '', '', '', '', '']"
https://ojs.aaai.org/index.php/AIES/article/view/31644,Fairness & Bias,Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through Data-centric Factors,"The astonishing successes of ML  have raised growing concern for the fairness of modern methods when deployed in real world settings. However, studies on fairness have mostly focused on supervised ML, while unsupervised outlier detection (OD), with numerous applications in finance, security, etc., have attracted little attention. While a few studies proposed fairness-enhanced OD algorithms, they remain agnostic to the underlying driving mechanisms or sources of unfairness. Even within the supervised ML literature, there exists debate on whether unfairness stems solely from algorithmic biases (i.e. design choices)  or from the  biases encoded in the data on which they are trained.    To close this gap, this work aims to shed light on the possible sources of unfairness in OD by auditing detection models under different data-centric factors.By injecting various known biases into the input data---as pertain to sample size disparity, under-representation, feature measurement noise, and group membership obfuscation---we find that the OD algorithms under the study all exhibit fairness pitfalls, although differing in which types of data bias they are more susceptible to. Most notable of our study is to demonstrate that OD algorithm bias is not merely a data bias problem. A key realization is that the data properties that emerge from bias injection could as well be organic---as pertain to natural group differences w.r.t. sparsity, base rate, variance, and multi-modality. Either natural or biased, such data properties can  give rise to unfairness as they interact with certain algorithmic design choices.  Our work provides a deeper  understanding of the possible sources of OD unfairness, and serves as a framework for assessing the unfairness of future OD algorithms under specific data-centric factors. It also paves the way for future work on mitigation strategies by underscoring the susceptibility of various design choices.",[],[],"['Xueying Ding', 'Rui Xi', 'Leman Akoglu']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31648,Privacy & Data Governance,How Should AI Decisions Be Explained? Requirements for Explanations from the Perspective of European Law,"This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, which was adopted by the European Parliament in March 2024, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary duties, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI methods are derived from each of the legal fields, resulting in the conclusion that each legal field requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI methods.",[],[],"['Benjamin Fresz', 'Elena Dubovitskaya', 'Danilo Brajovic', 'Marco F. Huber', 'Christian Horz']","['Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany\nInstitute of Industrial Manufacturing and Management (IFF), University of Stuttgart, Germany', 'University of Giessen, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany\nInstitute of Industrial Manufacturing and Management (IFF), University of Stuttgart, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany\nInstitute of Industrial Manufacturing and Management (IFF), University of Stuttgart, Germany', 'University of Giessen, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://ojs.aaai.org/index.php/AIES/article/view/31657,Fairness & Bias,Identifying Implicit Social Biases in Vision-Language Models,"Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-It, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a ""terrorist"". Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.",[],[],"['Kimia Hamidieh', 'Haoran Zhang', 'Walter Gerych', 'Thomas Hartvigsen', 'Marzyeh Ghassemi']","['MIT', 'MIT', 'MIT', 'University of Virginia', 'MIT']","['United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31658,Security,A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems,"We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the ""causal chain of interactions"" rather than simply focusing on the end outcome; this can help guide reforms.   In this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and criminality. Our framework identifies three canonical scenarios with distinct characteristics: in settings like (1) airport security, the primary source of observed bias against a race is likely to be bias in law enforcement against innocents of that race; (2) AI-empowered policing, the primary source of observed bias against a race is likely to be bias in law enforcement against criminals of that race; and (3) police-civilian interaction, the primary source of observed bias against a race could be bias in law enforcement against that race or bias from the general public in reporting (e.g. via 911 calls) against the other race. Through an extensive empirical study using police-civilian interaction (stop) data and 911 call data, we And an instance of such a counter-intuitive phenomenon: in New Orleans, the observed bias is against the majority race and the likely reason for it is the over-reporting (via 911 calls) of incidents involving the minority race by the general public.",[],[],"['Jessy Xinyi Han', 'Andrew Cesare Miller', 'S. Craig Watkins', 'Christopher Winship', 'Fotini Christia', 'Devavrat Shah']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'United States Naval Academy, Annapolis, MD, USA', 'University of Texas at Austin, Austin, TX, USA', 'Harvard University, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31666,Fairness & Bias,Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems,"Facial Recognition Systems (FRSs) are being developed and deployed all around the world at unprecedented rates. Most platforms are designed in a limited set of countries, but deployed in other regions too, without adequate checkpoints for region-specific requirements. This is especially problematic for Global South countries which lack strong legislation to safeguard persons facing disparate performance of these systems. A combination of unavailability of datasets, lack of understanding of how FRSs function and low-resource bias mitigation measures accentuate the problems at hand. In this work, we propose a self-curated face dataset composed of 6,579 unique male and female sports-persons (cricket players) from eight countries around the world. More than 50% of the dataset is composed of individuals from the Global South countries and is demographically diverse. To aid adversarial audits and robust model training, we curate four adversarial variants of each image in the dataset, leading to more than 40,000 distinct images. We also use this dataset to benchmark five popular facial recognition systems (FRSs), including both commercial and open-source FRSs, for the task of gender prediction (and country prediction for one of the open-source models as an example of red-teaming). Experiments on industrial FRSs reveal accuracies ranging from 98.2% (in case of Azure) to 38.1% (in case of Face++), with a large disparity between males and females in the Global South (max difference of 38.5% in case of Face++). Biases are also observed in all FRSs between females of the Global North and South (max difference of ~50%). A Grad-CAM analysis shows that the nose, forehead and mouth are the regions of interest for one of the open-source FRSs.  Based on this crucial observation, we design simple, low-resource bias mitigation solutions using few-shot and novel contrastive learning techniques that demonstrate a significant improvement in accuracy with disparity between males and females reducing from 50% to 1.5% in one of the settings. For the red-teaming experiment using the open-source Deepface model we observe that simple fine-tuning is not very useful while contrastive learning brings steady benefits.",[],[],"['Siddharth Jaiswal', 'Animesh Ganai', 'Abhisek Dash', 'Saptarshi Ghosh', 'Animesh Mukherjee']","['Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Kharagpur, India', 'Max Planck Institute for Software Systems, Germany', 'Indian Institute of Technology, Kharagpur, India', 'Indian Institute of Technology, Kharagpur, India']","['India', 'India', '', 'India', 'India']"
https://ojs.aaai.org/index.php/AIES/article/view/31681,Security,Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations,"To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to examine crisis escalation in a fictional US-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as “pacifist” or “aggressive sociopath.” When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.",[],[],"['Max Lamparth', 'Anthony Corso', 'Jacob Ganz', 'Oriana Skylar Mastro', 'Jacquelyn Schneider', 'Harold Trinkunas']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","['United States', 'United States', 'United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31684,Security,How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies,"With the widespread availability of LLMs since the release of ChatGPT and increased public scrutiny, commercial model development appears to have focused their efforts on `safety' training concerning legal liabilities at the expense of social impact evaluation. This mimics a similar trend which we could observe for search engine autocompletion some years prior. We draw on scholarship from NLP and search engine auditing and present a novel evaluation task in the style of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by using four metrics, namely refusal rates, toxicity, sentiment and regard, with and without safety system prompts. Our findings indicate an improvement to stereotyping outputs with the system prompt, but overall a lack of attention by LLMs under study to certain harms classified as toxic, particularly for prompts about peoples/ethnicities and sexual orientation. Mentions of intersectional identities trigger a disproportionate amount of stereotyping. Finally, we discuss the implications of these findings about stereotyping harms in light of the coming intermingling of LLMs and search and the choice of stereotyping mitigation policy to adopt. We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.",[],[],"['Alina Leidinger', 'Richard Rogers']","['University of Amsterdam', 'University of Amsterdam']","['Netherlands', 'Netherlands']"
https://ojs.aaai.org/index.php/AIES/article/view/31689,Fairness & Bias,Examining the Behavior of LLM Architectures Within the Framework of Standardized National Exams in Brazil,"The Exame Nacional do Ensino Médio (ENEM) is a pivotal test for Brazilian students, required for admission to a significant number of universities in Brazil. The test consists of four objective high-school level tests on Math, Humanities, Natural Sciences and Languages, and one writing essay. Students' answers to the test and to the accompanying socioeconomic status questionnaire are made public every year (albeit anonymized) due to transparency policies from the Brazilian Government. In the context of large language models (LLMs), these data lend themselves nicely to comparing different groups of humans with AI, as we can have access to human and machine answer distributions. We leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4, and MariTalk, a model trained using Portuguese data, to humans, aiming to ascertain how their answers relate to real societal groups and what that may reveal about the model biases. We divide the human groups by using socioeconomic status (SES), and compare their answer distribution with LLMs for each question and for the essay. We find no significant biases when comparing LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as the distance between model and human answers is mostly determined by the human accuracy. A similar conclusion is found by looking at the generated text as, when analyzing the essays, we observe that human and LLM essays differ in a few key factors, one being the choice of words where model essays were easily separable from human ones. The texts also differ syntactically, with LLM generated essays exhibiting, on average, smaller sentences and less thought units, among other differences. These results suggest that, for Brazilian Portuguese in the ENEM context, LLM outputs represent no group of humans, being significantly different from the answers from Brazilian students across all tests. The appendices may be found at https://arxiv.org/abs/2408.05035.",[],[],"['Marcelo Sartori Locatelli', 'Matheus Prado Miranda', 'Igor Joaquim da Silva Costa', 'Matheus Torres Prates', 'Victor Thomé', 'Mateus Zaparoli Monteiro', 'Tomas Lacerda', 'Adriana Pagano', 'Eduardo Rios Neto', 'Wagner Meira Jr.', 'Virgilio Almeida']","['Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais', 'Universidade Federal de Minas Gerais']","['Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil', 'Brazil']"
https://ojs.aaai.org/index.php/AIES/article/view/31716,Security,Learning When Not to Measure: Theorizing Ethical Alignment in LLMs,"LLMs and other forms of generative AI have shown immense promise in producing highly accurate epistemic judgements in domains as varied as law, education, and medicine – with GPT notably passing the legal Bar exam and various medical licensing exams. The safe extension of LLMs into safety-critical professional domains requires assurance not only of epistemic but ethical alignment. This paper adopts a theoretical and philosophical approach, drawing from metaethical theories to argue for a distinction hinging around quantitative, axiological comparability that separates Kantian ethics from not only the utilitarianism it is well-known to oppose, but from just distribution theories as well, which are key to debiasing LLM models. It presents the novel hypothesis that LLM ethical acquisition from both corpus induction and RLHF may encounter value conflicts between Kantian and just distribution principles that intensify as they come into improved alignment with both theories, hinging around the variability by which self-attention may statistically attend to the same characterizations as more person-like or more resource-like under distinct prompting strategies.",[],[],['William Rathje'],"['University of California, Berkeley']",['United States']
https://ojs.aaai.org/index.php/AIES/article/view/31729,Fairness & Bias,Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges,"Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.",[],[],"['Ishana Shastri', 'Shomik Jain', 'Barbara Engelhardt', 'Ashia Wilson']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Stanford University', 'Massachusetts Institute of Technology']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31733,Security,Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries,"Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both.  Auditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms’ compliance with regulation. Auditing is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively.  First, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions. On the basis of advanced AI’s risk profile, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits. Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight. Secondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities. Public bodies’ capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences.",[],[],"['Merlin Stein', 'Milan Gandhi', 'Theresa Kriecherbauer', 'Amin Oueslati', 'Robert Trager']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'Hertie School', 'University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://ojs.aaai.org/index.php/AIES/article/view/31745,Security,How Do AI Companies “Fine-Tune” Policy? Examining Regulatory Capture in AI Governance,"Industry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.",[],[],"['Kevin Wei', 'Carson Ezell', 'Nick Gabrieli', 'Chinmay Deshpande']","['RAND\nHarvard Law School', 'Harvard University', 'Harvard University', 'Harvard University']","['United States', 'United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31748,Fairness & Bias,"Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval","Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1% of cases and female-associated names in only 11.1% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have  implications for widely used AI tools that are automating employment, fairness, and tech policy.",[],[],"['Kyra Wilson', 'Aylin Caliskan']","['University of Washington', 'University of Washington']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31751,Fairness & Bias,ML-EAT: A Multilevel Embedding Association Test  for Interpretable and Transparent Social Science,"This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.",[],[],"['Robert Wolfe', 'Alexis Hiniker', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington']","['United States', 'United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31750,Privacy & Data Governance,The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations,"Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.",[],[],"['Robert Wolfe', 'Tanushree Mitra']","['University of Washington', 'University of Washington']","['United States', 'United States']"
https://ojs.aaai.org/index.php/AIES/article/view/31758,Fairness & Bias,LLM Voting: Human Choices and AI Collective Decision-Making,"This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.",[],[],"['Joshua C. Yang', 'Damian Dailisan', 'Marcin Korecki', 'Carina I. Hausladen', 'Dirk Helbing']","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://ojs.aaai.org/index.php/AIES/article/view/31757,Security,Tracing the Evolution of Information Transparency for OpenAI’s GPT Models through a Biographical Approach,"Information transparency, the open disclosure of information about models, is crucial for proactively evaluating the potential societal harm of large language models (LLMs) and developing effective risk mitigation measures. Adapting the biographies of artifacts and practices (BOAP) method from science and technology studies, this study analyzes the evolution of information transparency within OpenAI’s Generative Pre-trained Transformers (GPT) model reports and usage policies from its inception in 2018 to GPT-4, one of today’s most capable LLMs. To assess the breadth and depth of transparency practices, we develop a 9-dimensional, 3-level analytical framework to evaluate the comprehensiveness and accessibility of information disclosed to various stakeholders. Findings suggest that while model limitations and downstream usages are increasingly clarified, model development processes have become more opaque. Transparency remains minimal in certain aspects, such as model explainability and real-world evidence of LLM impacts, and the discussions on safety measures such as technical interventions and regulation pipelines lack in-depth details. The findings emphasize the need for enhanced transparency to foster accountability and ensure responsible technological innovations.",[],[],"['Zhihan Xu', 'Eni Mustafaraj']","['Wellesley College', 'Wellesley College']","['United States', 'United States']"