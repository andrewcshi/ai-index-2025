link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://papers.nips.cc/paper_files/paper/2019/hash/07cb5f86508f146774a2fac4373a8e50-Abstract.html,Transparency & Explainability,A Normative Theory for Causal Inference and Bayes Factor Computation in Neural Circuits,"This study provides a normative theory for how Bayesian causal inference can be implemented in neural circuits. In both cognitive processes such as causal reasoning and perceptual inference such as cue integration, the nervous systems need to choose different models representing the underlying causal structures when making inferences on external stimuli. In multisensory processing, for example, the nervous system has to choose whether to integrate or segregate inputs from different sensory modalities to infer the sensory stimuli, based on whether the inputs are from the same or different sources. Making this choice is a model selection problem requiring the computation of Bayes factor, the ratio of likelihoods between the integration and the segregation models. In this paper, we consider the causal inference in multisensory processing and propose a novel generative model based on neural population code that takes into account both stimulus feature and stimulus reliability in the inference. In the case of circular variables such as heading direction, our normative theory yields an analytical solution for computing the Bayes factor, with a clear geometric interpretation, which can be implemented by simple additive mechanisms with neural population code. Numerical simulation shows that the tunings of the neurons computing Bayes factor are consistent with the ""opposite neurons"" discovered in dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas for visual-vestibular processing. This study illuminates a potential neural mechanism for causal inference in the brain.",[],[],"['Wenhao Zhang', 'Si Wu', 'Brent Doiron', 'Tai Sing Lee']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/11b9842e0a271ff252c1903e7132cd68-Abstract.html,Transparency & Explainability,CPM-Nets: Cross Partial Multi-View Networks,"Despite multi-view learning progressed fast in past decades, it is still challenging due to the difficulty in modeling complex correlation among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets). In this framework, we first give a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the latent representation learned from our algorithm. To achieve the completeness, the task of learning latent multi-view representation is specifically translated to degradation process through mimicking data transmitting, such that the optimal tradeoff between consistence and complementarity across different views could be achieved. In contrast with methods that either complete missing views or group samples according to view-missing patterns, our model fully exploits all samples and all views to produce structured representation for interpretability. Extensive experimental results validate the effectiveness of our algorithm over existing state-of-the-arts.",[],[],"['Changqing Zhang', 'Zongbo Han', 'yajie cui', 'Huazhu Fu', 'Joey Tianyi Zhou', 'Qinghua Hu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html,Transparency & Explainability,On the Fairness of Disentangled Representations,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations.We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable.We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent.Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.",[],[],"['Francesco Locatello', 'Gabriele Abbati', 'Thomas Rainforth', 'Stefan Bauer', 'Bernhard Schölkopf', 'Olivier Bachem']","['Dept. of Computer Science, ETH Zurich and Max-Planck Institute for Intelligent Systems', 'Dept. of Engineering Science, University of Oxford', 'Dept. of Statistics, University of Oxford', 'Max-Planck Institute for Intelligent Systems', 'Max-Planck Institute for Intelligent Systems', 'Google Research, Brain Team']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html,Transparency & Explainability,Calibration tests in multi-class classification: A unifying framework,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.",[],[],"['David Widmann', 'Fredrik Lindsten', 'Dave Zachariah']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4206e38996fae4028a26d43b24f68d32-Abstract.html,Transparency & Explainability,Provably robust boosted decision stumps and trees against adversarial attacks,"The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per input, where $T$ is the number of decision stumps and the optimal update step of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5\% for $\epsilon_\infty=0.3$), FMNIST (23.2\% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7\% for $\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at \url{http://github.com/max-andr/provably-robust-boosting}.",[],[],"['Maksym Andriushchenko', 'Matthias Hein']","['University of Tübingen', 'University of Tübingen']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html,Transparency & Explainability,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,"A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.",[],[],"['Yongkai Wu', 'Lu Zhang', 'Xintao Wu', 'Hanghang Tong']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5352696a9ca3397beb79f116f3a33991-Abstract.html,Transparency & Explainability,Bat-G net: Bat-inspired High-Resolution 3D Image Reconstruction using Ultrasonic Echoes,"In this paper, a bat-inspired high-resolution ultrasound 3D imaging system is presented. Live bats demonstrate that the properly used ultrasound can be used to perceive 3D space. With this in mind, a neural network referred to as a Bat-G network is implemented to reconstruct the 3D representation of target objects from the hyperbolic FM (HFM) chirped ultrasonic echoes. The Bat-G network consists of an encoder emulating a bat's central auditory pathway, and a 3D graphical visualization decoder. For the acquisition of the ultrasound data, a custom-made Bat-I sensor module is used. The Bat-G network shows the uniform 3D reconstruction results and achieves precision, recall, and F1-score of 0.896, 0.899 and 0.895, respectively. The experimental results demonstrate the implementation feasibility of a high-resolution non-optical sound-based imaging system being used by live bats. The project web page (https://sites.google.com/view/batgnet) contains additional content summarizing our research.",[],[],"['Gunpil Hwang', 'Seohyeon Kim', 'Hyeon-Min Bae']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/54ebdfbbfe6c31c39aaba9a1ee83860a-Abstract.html,Transparency & Explainability,Discriminative Topic Modeling with Logistic LDA,"Despite many years of research into latent Dirichlet allocation (LDA), applying LDA to collections of non-categorical items is still challenging for practitioners. Yet many problems with much richer data share a similar structure and could benefit from the vast literature on LDA. We propose logistic LDA, a novel discriminative variant of latent Dirichlet allocation which is easy to apply to arbitrary inputs. In particular, our model can easily be applied to groups of images, arbitrary text embeddings, or integrate deep neural networks. Although it is a discriminative model, we show that logistic LDA can learn from unlabeled data in an unsupervised manner by exploiting the group structure present in the data. In contrast to other recent topic models designed to handle arbitrary inputs, our model does not sacrifice the interpretability and principled motivation of LDA.",[],[],"['Iryna Korshunova', 'Hanchen Xiong', 'Mateusz Fedoryszak', 'Lucas Theis']","['Ghent University', 'Twitter', 'Twitter', 'Twitter']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/55a988dfb00a914717b3000a3374694c-Abstract.html,Transparency & Explainability,Disentangling Influence: Using disentangled representations to audit model predictions,"Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled influence audits, a procedure to audit the indirect influence of features. Specifically, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classifier being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature influence.",[],[],"['Charles Marx', 'Richard Phillips', 'Sorelle Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/567b8f5f423af15818a068235807edc0-Abstract.html,Transparency & Explainability,Demystifying Black-box Models with Symbolic Metamodels,"Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework — a general methodology for interpreting predictions by converting ""black-box"" models into ""white-box"" functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize symbolic metamodels using Meijer G-functions — a class of complex-valued contour integrals that depend on scalar parameters, and whose solutions reduce to familiar elementary, algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a machine learning model with minimal a priori assumptions. We show that symbolic metamodeling provides an all-encompassing framework for model interpretation — all common forms of global and local explanations of a model can be analytically derived from its symbolic metamodel.",[],[],"['Ahmed M. Alaa', 'Mihaela van der Schaar']","['ECE Department, UCLA', 'University of Cambridge, UCLA, and Alan Turing Institute']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5e2b66750529d8ae895ad2591118466f-Abstract.html,Transparency & Explainability,Paraphrase Generation with Latent Bag of Words,"Paraphrase generation is a longstanding important problem in natural language processing.   Recent progress in deep generative models has shown promising results on discrete latent variables for text generation.   Inspired by variational autoencoders with discrete latent structures,   in this work, we propose a latent bag of words (BOW) model for paraphrase generation.  We ground the semantics of a discrete latent variable by the target BOW.   We use this latent variable to build a fully differentiable content planning and surface realization pipeline.   Specifically, we use source words to predict their neighbors and model the target BOW with a mixture of softmax.   We use gumbel top-k reparameterization to perform differentiable subset sampling from the predicted BOW distribution.  We retrieve the sampled word embeddings and use them to augment the decoder and guide its generation search space.   Our latent BOW model not only enhances the decoder, but also exhibits clear interpretability.  We show the model interpretability with regard to (1). unsupervised learning of word neighbors (2). the step-by-step generation procedure.   Extensive experiments demonstrate the model's transparent and effective generation process.",[],[],"['Yao Fu', 'Yansong Feng', 'John P. Cunningham']","['Department of Computer Science, Columbia University', 'Institute of Computer Science and Technology, Peking University', 'Department of Statistics, Columbia University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/6395ebd0f4b478145ecfbaf939454fa4-Abstract.html,Transparency & Explainability,Variational Denoising Network: Toward Blind Noise Modeling and Removal,"Blind image denoising is an important yet very challenging problem in computervision due to the complicated acquisition process of real images. In this work wepropose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.",[],[],"['Zongsheng Yue', 'Hongwei Yong', 'Qian Zhao', 'Deyu Meng', 'Lei Zhang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html,Transparency & Explainability,Deliberative Explanations: visualizing network insecurities,"A new approach to explainable AI, denoted {\it deliberative explanations,\/}  is proposed. Deliberative explanations are a visualization technique  that aims to go beyond the simple visualization of the image regions  (or, more generally, input variables) responsible for a network  prediction. Instead, they aim to expose the deliberations carried  by the network to arrive at that prediction, by uncovering the  insecurities of the network about the latter. The  explanation consists of a list of insecurities, each composed of  1) an image region (more generally, a set of input variables), and 2)  an ambiguity formed by the pair of classes responsible for the network  uncertainty about the region. Since insecurity detection requires  quantifying the difficulty of network predictions, deliberative  explanations combine ideas from the literatures on visual explanations and  assessment of classification difficulty. More specifically,  the proposed implementation  combines attributions with respect to both class  predictions and a difficulty score.  An evaluation protocol that leverages object recognition (CUB200)  and scene classification (ADE20K) datasets that combine part and  attribute annotations is also introduced to evaluate the accuracy of  deliberative explanations. Finally, an experimental evaluation shows that  the most accurate explanations are achieved by combining non self-referential  difficulty scores and second-order attributions. The resulting  insecurities are shown to correlate with regions of attributes that  are shared by different classes. Since these regions are also ambiguous  for humans, deliberative explanations are intuitive, suggesting that  the deliberative process of modern networks correlates with human  reasoning.",[],[],"['Pei Wang', 'Nuno Nvasconcelos']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/704cddc91e28d1a5517518b2f12bc321-Abstract.html,Transparency & Explainability,Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis,"Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However, popular methods disregard temporal structure, rendering them prone to extracting noise rather than meaningful dynamics when applied to time series data. At the same time, many successful unsupervised learning methods for temporal, sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches, we introduce Dynamical Components Analysis (DCA), a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information, defined as the mutual information between the past and future. We test DCA on synthetic examples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets, showing that the dimensions extracted by DCA are more useful than those extracted by other methods for predicting future states and decoding auxiliary variables. Overall, DCA robustly extracts dynamical structure in noisy, high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionality reduction methods.",[],[],"['David Clark', 'Jesse Livezey', 'Kristofer Bouchard']","['Center for Theoretical Neuroscience, Columbia University and Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory', 'Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory and Redwood Center for Theoretical Neuroscience, University of California, Berkeley', 'Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory and Redwood Center for Theoretical Neuroscience, University of California, Berkeley and Helen Wills Neuroscience Institute, University of California, Berkeley']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html,Transparency & Explainability,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric,"Where machine-learned predictive risk scores inform high-stakes decisions, such as bail and sentencing in criminal justice, fairness has been a serious concern. Recent work has characterized the disparate impact that such risk scores can have when used for a binary classification task. This may not account, however, for the more diverse downstream uses of risk scores and their non-binary nature. To better account for this, in this paper, we investigate the fairness of predictive risk scores from the point of view of a bipartite ranking task, where one seeks to rank positive examples higher than negative ones. We introduce the xAUC disparity as a metric to assess the disparate impact of risk scores and define it as the difference in the probabilities of ranking a random positive example from one protected group above a negative one from another group and vice versa. We provide a decomposition of bipartite ranking loss into components that involve the discrepancy and components that involve pure predictive ability within each group. We use xAUC analysis to audit predictive risk scores for recidivism prediction, income prediction, and cardiac arrest prediction, where it describes disparities that are not evident from simply comparing within-group predictive performance.",[],[],"['Nathan Kallus', 'Angela Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html,Transparency & Explainability,Towards Automatic Concept-based Explanations,"Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions.     Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph{concept} based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.",[],[],"['Amirata Ghorbani', 'James Wexler', 'James Y. Zou', 'Been Kim']","['Stanford University and Google Brain', 'Google Brain', 'Stanford University', 'Google Brain']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/79a3308b13cd31f096d8a4a34f96b66b-Abstract.html,Transparency & Explainability,Sobolev Independence Criterion,"We propose the Sobolev Independence Criterion (SIC), an interpretable dependency measure between a high dimensional random variable X and a response variable Y. SIC decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. SIC can be seen as a gradient regularized Integral Probability Metric (IPM) between the joint distribution of the two random variables and the product of their marginals. We use sparsity inducing gradient penalties to promote input sparsity of the critic of the IPM. In the kernel version we show that SIC can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. We then present a neural version of SIC where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. We conduct experiments validating SIC for feature selection in synthetic and real-world experiments. We show that SIC enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the False Discovery Rate. Code is available at http://github.com/ibm/sic.",[],[],"['Youssef Mroueh', 'Tom Sercu', 'Mattia Rigotti', 'Inkit Padhi', 'Cicero Nogueira dos Santos']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7f278ad602c7f47aa76d1bfc90f20263-Abstract.html,Transparency & Explainability,Coresets for Archetypal Analysis,"Archetypal analysis represents instances as linear mixtures of prototypes (the archetypes) that lie on the boundary of the convex hull of the data. Archetypes are thus  often better interpretable than factors computed by other matrix factorization techniques. However, the interpretability comes with high computational cost due to additional convexity-preserving constraints. In this paper, we propose efficient coresets for archetypal analysis. Theoretical guarantees are derived by showing that quantization errors of k-means upper bound archetypal analysis; the computation of a provable absolute-coreset can be performed in only two passes over the data. Empirically, we show that the coresets lead to improved performance on several data sets.",[],[],"['Sebastian Mair', 'Ulf Brefeld']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html,Transparency & Explainability,Full-Gradient Representation for Neural Network Visualization,"We introduce a new tool for interpreting neural nets, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. Using full-gradients, we also propose an approximate saliency map representation for convolutional nets dubbed FullGrad, obtained by aggregating the full-gradient components.We experimentally evaluate the usefulness of FullGrad in explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behavior correctly, and more comprehensively than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.",[],[],"['Suraj Srinivas', 'François Fleuret']","['Idiap Research Institute & EPFL', 'Idiap Research Institute & EPFL']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/82161242827b703e6acf9c726942a1e4-Abstract.html,Transparency & Explainability,Chasing Ghosts: Instruction Following as Bayesian State Tracking,"A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) within the framework of Bayesian state tracking - learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet baseline when predicting the goal location on the map. On the full VLN task, i.e. navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.",[],[],"['Peter Anderson', 'Ayush Shrivastava', 'Devi Parikh', 'Dhruv Batra', 'Stefan Lee']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html,Transparency & Explainability,Hierarchical Optimal Transport for Document Representation,"The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora. Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues. As an alternative, we introduce hierarchical optimal transport as a meta-distance between documents, where documents are modeled as distributions over topics, which themselves are modeled as distributions over words. We then solve an optimal transport problem on the smaller topic space to compute a similarity score. We give conditions on the topics under which this construction defines a distance, and we relate it to the word mover's distance. We evaluate our technique for k-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.",[],[],"['Mikhail Yurochkin', 'Sebastian Claici', 'Edward Chien', 'Farzaneh Mirzazadeh', 'Justin M. Solomon']","['IBM Research and MIT-IBM Watson AI Lab', 'MIT CSAIL and MIT-IBM Watson AI Lab', 'MIT CSAIL and MIT-IBM Watson AI Lab', 'IBM Research and MIT-IBM Watson AI Lab', 'MIT CSAIL and MIT-IBM Watson AI Lab']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/8dd48d6a2e2cad213179a3992c0be53c-Abstract.html,Transparency & Explainability,Saccader: Improving Accuracy of Hard Attention Models for Vision,"Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks, their decisions are difficult to interpret. One approach that offers some level of interpretability by design is \textit{hard attention}, which uses only relevant portions of the image. However, training hard attention models with only class label supervision is challenging, and hard attention has proved difficult to scale to complex datasets. Here, we propose a novel hard attention model, which we term Saccader. Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines, achieving $75\%$  top-1 and $91\%$ top-5 while attending to less than one-third of the image.",[],[],"['Gamaleldin Elsayed', 'Simon Kornblith', 'Quoc V. Le']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/90cc440b1b8caa520c562ac4e4bbcb51-Abstract.html,Transparency & Explainability,Structured Graph Learning Via Laplacian Spectral Constraints,"Learning a graph with a specific structure is essential for interpretability and identification of the relationships among data. But structured graph learning from observed samples is an NP-hard combinatorial problem. In this paper, we first show, for a set of important graph families it is possible to convert the combinatorial constraints of structure into eigenvalue constraints of the graph Laplacian matrix. Then we introduce a unified graph learning framework lying at the integration of the spectral properties of the Laplacian matrix with Gaussian graphical modeling, which is capable of learning structures of a large class of graph families. The proposed algorithms are provably convergent and practically amenable for big-data specific tasks. Extensive numerical experiments with both synthetic and real datasets demonstrate the effectiveness of the proposed methods. An R package containing codes for all the experimental results is submitted as a supplementary file.",[],[],"['Sandeep Kumar', 'Jiaxi Ying', 'Jose Vinicius de Miranda Cardoso', 'Daniel Palomar']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/947018640bf36a2bb609d3557a285329-Abstract.html,Transparency & Explainability,Causal Confusion in Imitation Learning,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive ""causal misidentification"" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.",[],[],"['Pim de Haan', 'Dinesh Jayaraman', 'Sergey Levine']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/95b431e51fc53692913da5263c214162-Abstract.html,Transparency & Explainability,Learning Compositional Neural Programs with Recursive Tree Search and Planning,"We propose a novel reinforcement learning algorithm, AlphaNPI, that incorpo-rates the strengths of Neural Programmer-Interpreters (NPI) and AlphaZero. NPIcontributes structural biases in the form of modularity, hierarchy and recursion,which are helpful to reduce sample complexity, improve generalization and in-crease interpretability. AlphaZero contributes powerful neural network guidedsearch algorithms, which we augment with recursion. AlphaNPI only assumesa hierarchical program specification with sparse rewards: 1 when the programexecution satisfies the specification, and 0 otherwise. This specification enablesus to overcome the need for strong supervision in the form of execution tracesand consequently train NPI models effectively with reinforcement learning. Theexperiments show that AlphaNPI can sort as well as previous strongly supervisedNPI variants. The AlphaNPI agent is also trained on a Tower of Hanoi puzzle withtwo disks and is shown to generalize to puzzles with an arbitrary number of disks.The experiments also show that when deploying our neural network policies, it isadvantageous to do planning with guided Monte Carlo tree search.",[],[],"['Thomas PIERROT', 'Guillaume Ligner', 'Scott E. Reed', 'Olivier Sigaud', 'Nicolas Perrin', 'Alexandre Laterre', 'David Kas', 'Karim Beguir', 'Nando de Freitas']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a2186aa7c086b46ad4e8bf81e2a3a19b-Abstract.html,Transparency & Explainability,Learning Disentangled Representations for Recommendation,"User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users’ decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user’s preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",[],[],"['Jianxin Ma', 'Chang Zhou', 'Peng Cui', 'Hongxia Yang', 'Wenwu Zhu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ac27b77292582bc293a51055bfc994ee-Abstract.html,Transparency & Explainability,Metamers of neural networks reveal divergence from human perceptual systems,"Deep neural networks have been embraced as models of sensory systems, instantiating representational transformations that appear to resemble those in the visual and auditory systems. To more thoroughly investigate their similarity to biological systems, we synthesized model metamers – stimuli that produce the same responses at some stage of a network’s representation. We generated model metamers for natural stimuli by performing gradient descent on a noise signal, matching the responses of individual layers of image and audio networks to a natural image or speech signal. The resulting signals reflect the invariances instantiated in the network up to the matched layer. We then measured whether model metamers were recognizable to human observers – a necessary condition for the model representations to replicate those of humans. Although model metamers from early network layers were recognizable to humans, those from deeper layers were not. Auditory model metamers became more human-recognizable with architectural modifications that reduced aliasing from pooling operations, but those from the deepest layers remained unrecognizable. We also used the metamer test to compare model representations. Cross-model metamer recognition dropped off for deeper layers, roughly at the same point that human recognition deteriorated, indicating divergence across model representations. The results reveal discrepancies between model and human representations, but also show how metamers can help guide model refinement and elucidate model representations.",[],[],"['Jenelle Feather', 'Alex Durango', 'Ray Gonzalez', 'Josh McDermott']","['Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology', 'Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology', 'Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology', 'Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology and McGovern Institute, Massachusetts Institute of Technology and Center for Brains Minds and Machines, Massachusetts Institute of Technology and Speech and Hearing Bioscience and Technology, Harvard University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html,Transparency & Explainability,This Looks Like That: Deep Learning for Interpretable Image Recognition,"When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.",[],[],"['Chaofan Chen', 'Oscar Li', 'Daniel Tao', 'Alina Barnett', 'Cynthia Rudin', 'Jonathan K. Su']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'MIT Lincoln Laboratory', 'Duke University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/bbc12a3a98d8487f58a87d3a3070516e-Abstract.html,Transparency & Explainability,Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks,"Recent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features to predictions made by models. Work on interpretability using saliency-based methods on Recurrent Neural Networks (RNNs) has mostly targeted language tasks, and their applicability to time series data is less understood. In this work we analyze saliency-based methods for RNNs, both classical and gated cell architectures. We show that RNN saliency vanishes over time, biasing detection of salient features only to later time steps and are, therefore, incapable of reliably detecting important features at arbitrary time intervals. To address this vanishing saliency problem, we propose a novel RNN cell structure (input-cell attention), which can extend any RNN cell architecture. At each time step, instead of only looking at the current input vector, input-cell attention uses a fixed-size matrix embedding, each row of the matrix attending to different inputs from current or previous time steps.  Using synthetic data, we show that the saliency map produced by the input-cell attention RNN is able to faithfully detect important features regardless of their occurrence in time. We also apply the input-cell attention RNN on a neuroscience task analyzing functional Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety of tasks. In this case, we use saliency to characterize brain regions (input features) for which activity is important to distinguish between tasks. We show that standard RNN architectures are only capable of detecting important brain regions in the last few time steps of the fMRI data, while the input-cell attention model is able to detect important brain region activity across time without latter time step biases.",[],[],"['Aya Abdelsalam Ismail', 'Mohamed Gunady', 'Luiz Pessoa', 'Hector Corrada Bravo', 'Soheil Feizi']","['Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Psychology, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/c20a7ce2a627ba838cfbff082db35197-Abstract.html,Transparency & Explainability,Learning by Abstraction: The Neural State Machine,"We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.",[],[],"['Drew Hudson', 'Christopher D. Manning']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/cf34645d98a7630e2bcca98b3e29c8f2-Abstract.html,Transparency & Explainability,Self-attention with Functional Time Representation Learning,"Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capture useful time-event interactions.",[],[],"['Da Xu', 'Chuanwei Ruan', 'Evren Korpeoglu', 'Sushant Kumar', 'Kannan Achan']","['Walmart Labs, California, CA', 'Walmart Labs, California, CA', 'Walmart Labs, California, CA', 'Walmart Labs, California, CA', 'Walmart Labs, California, CA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d54e99a6c03704e95e6965532dec148b-Abstract.html,Transparency & Explainability,Assessing Disparate Impact of Personalized Interventions: Identifiability and Bounds,"Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption via sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.",[],[],"['Nathan Kallus', 'Angela Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html,Transparency & Explainability,GNNExplainer: Generating Explanations for Graph Neural Networks,"Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models,and explaining predictions made by GNNs remains unsolved. Herewe propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer  can generate consistent and concise explanations for an entire class of instances.We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.",[],[],"['Zhitao Ying', 'Dylan Bourgeois', 'Jiaxuan You', 'Marinka Zitnik', 'Jure Leskovec']","['Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University and Robust.AI', 'Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Computer Science, Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/dca5672ff3444c7e997aa9a2c4eb2094-Abstract.html,Transparency & Explainability,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components,"Abstract Neural networks are state-of-the-art classification approaches but are generally difficult to interpret. This issue can be partly alleviated by constructing a precise decision process within the neural network. In this work, a network architecture, denoted as Classification-By-Components network (CBC), is proposed. It is restricted to follow an intuitive reasoning based decision process inspired by Biederman's recognition-by-components theory from cognitive psychology. The network is trained to learn and detect generic components that characterize objects. In parallel, a class-wise reasoning strategy based on these components is learned to solve the classification problem. In contrast to other work on reasoning, we propose three different types of reasoning: positive, negative, and indefinite. These three types together form a probability space to provide a probabilistic classifier. The decomposition of objects into generic components combined with the probabilistic reasoning provides by design a clear interpretation of the classification decision process. The evaluation of the approach on MNIST shows that CBCs are viable classifiers. Additionally, we demonstrate that the inherent interpretability offers a profound understanding of the classification behavior such that we can explain the success of an adversarial attack. The method's scalability is successfully tested using the ImageNet dataset.",[],[],"['Sascha Saralajew', 'Lars Holdijk', 'Maike Rees', 'Ebubekir Asan', 'Thomas Villmann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ddf354219aac374f1d40b7e760ee5bb7-Abstract.html,Transparency & Explainability,Learning Representations by Maximizing Mutual Information Across Views,"We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views – e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation.  This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.",[],[],"['Philip Bachman', 'R Devon Hjelm', 'William Buchwalter']","['Microsoft Research', 'Microsoft Research, MILA', 'Microsoft Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e2db7186375992e729165726762cb4c1-Abstract.html,Transparency & Explainability,Untangling in Invariant Speech Recognition,"Encouraged by the success of deep convolutional neural networks on a variety of visual tasks, much theoretical and experimental work has been aimed at understanding and interpreting how vision networks operate.  At the same time, deep neural networks have also achieved impressive performance in audio processing applications, both as sub-components of larger systems and as complete end-to-end systems by themselves.  Despite their empirical successes, comparatively little is understood about how these audio models accomplish these tasks.In this work, we employ a recently developed statistical mechanical theory that connects geometric properties of network representations and the separability of classes to probe how information is untangled within neural networks trained to recognize speech.  We observe that speaker-specific nuisance variations are discarded by the network's hierarchy, whereas task-relevant properties such as words and phonemes are untangled in later layers. Higher level concepts such as parts-of-speech and context dependence also emerge in the later layers of the network. Finally, we find that the deep representations carry out significant temporal untangling by efficiently extracting task-relevant features at each time step of the computation.  Taken together, these findings shed light on how deep auditory models process their time dependent input signals to carry out invariant speech recognition, and show how different concepts emerge through the layers of the network.",[],[],"['Cory Stephenson', 'Jenelle Feather', 'Suchismita Padhy', 'Oguz Elibol', 'Hanlin Tang', 'Josh McDermott', 'SueYeon Chung']","['Intel AI Lab', 'MIT', 'Intel AI Lab', 'Intel AI Lab', 'Intel AI Lab', 'MIT/ Center for Brains, Minds, and Machines', 'Columbia University/ MIT']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/e6872f5bbe75073f8c7cfb93de7f6f3a-Abstract.html,Transparency & Explainability,Constraint-based Causal Structure Learning with Consistent Separating Sets,"We consider constraint-based methods for causal structure learning, such as the PC algorithm or any PC-derived algorithms whose ﬁrst step consists in pruning a complete graph to obtain an undirected graph skeleton, which is subsequently oriented. All constraint-based methods perform this ﬁrst step of removing dispensable edges, iteratively, whenever a separating set and corresponding conditional independence can be found. Yet, constraint-based methods lack robustness over sampling noise and are prone to uncover spurious conditional independences in ﬁnite datasets. In particular, there is no guarantee that the separating sets identiﬁed during the iterative pruning step remain consistent with the ﬁnal graph. In this paper, we propose a simple modiﬁcation of PC and PC-derived algorithms so as to ensure that all separating sets identiﬁed to remove dispensable edges are consistent with the ﬁnal graph,thus enhancing the explainability of constraint-basedmethods. It is achieved by repeating the constraint-based causal structure learning scheme, iteratively, while searching for separating sets that are consistent with the graph obtained at the previous iteration. Ensuring the consistency of separating sets can be done at a limited complexity cost, through the use of block-cut tree decomposition of graph skeletons, and is found to increase their validity in terms of actual d-separation. It also signiﬁcantly improves the sensitivity of constraint-based methods while retaining good overall structure learning performance. Finally and foremost, ensuring sepset consistency improves the interpretability of constraint-based models for real-life applications.",[],[],"['Honghao Li', 'Vincent Cabeli', 'Nadir Sella', 'Herve Isambert']","['Institut Curie, PSL Research University, CNRS UMR168, Paris', 'Institut Curie, PSL Research University, CNRS UMR168, Paris', 'Institut Curie, PSL Research University, CNRS UMR168, Paris', 'Institut Curie, PSL Research University, CNRS UMR168, Paris']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fbefa505c8e8bf6d46f38f5277fed8d6-Abstract.html,Transparency & Explainability,Online Convex Matrix Factorization with Representative Regions,"Matrix factorization (MF) is a versatile learning method that has found wide applications in various data-driven disciplines. Still, many MF algorithms do not adequately scale with the size of available datasets and/or lack interpretability. To improve the computational efficiency of the method, an online (streaming) MF algorithm was proposed in Mairal et al., 2010. To enable data interpretability, a constrained version of MF, termed convex MF, was introduced in Ding et al., 2010. In the latter work, the basis vectors are required to lie in the convex hull of the data samples, thereby ensuring that every basis can be interpreted as a weighted combination of data samples. No current algorithmic solutions for online convex MF are known as it is challenging to find adequate convex bases without having access to the complete dataset. We address both problems by proposing the first online convex MF algorithm that maintains a collection of constant-size sets of representative data samples needed for interpreting each of the basis (Ding et al., 2010) and has the same almost sure convergence guarantees as the online learning algorithm of Mairal et al., 2010. Our proof techniques combine random coordinate descent algorithms with specialized quasi-martingale convergence analysis. Experiments on synthetic and real world datasets show significant computational savings of the proposed online convex MF method compared to classical convex MF. Since the proposed method maintains small representative sets of data samples needed for convex interpretations, it is related to a body of work in theoretical computer science, pertaining to generating point sets (Blum et al., 2016), and in computer vision, pertaining to archetypal analysis (Mei et al., 2018). Nevertheless, it differs from these lines of work both in terms of the objective and algorithmic implementations.",[],[],"['Jianhao Peng', 'Olgica Milenkovic', 'Abhishek Agarwal']","['Electrical and Computer Engineering, University of Illinois Urbana-Champaign', 'Electrical and Computer Engineering, University of Illinois Urbana-Champaign', 'Electrical and Computer Engineering, University of Illinois Urbana-Champaign']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/fe4b8556000d0f0cae99daa5c5c5a410-Abstract.html,Transparency & Explainability,A Benchmark for Interpretability Methods in Deep Neural Networks,"We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.",[],[],"['Sara Hooker', 'Dumitru Erhan', 'Pieter-Jan Kindermans', 'Been Kim']","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/0e1feae55e360ff05fef58199b3fa521-Abstract.html,Fairness & Bias,"Average Individual Fairness: Algorithms, Generalization and Experiments","We propose a new family of fairness definitions for classification problems that combine some of the best properties of both statistical and individual notions of fairness. We posit not only a distribution over individuals, but also a distribution over (or collection of) classification tasks. We then ask that standard statistics (such as error or false positive/negative rates) be (approximately) equalized across individuals, where the rate is defined as an expectation over the classification tasks. Because we are no longer averaging over coarse groups (such as race or gender), this is a semantically meaningful individual-level constraint. Given a sample of individuals and problems, we design an oracle-efficient algorithm (i.e. one that is given access to any standard, fairness-free learning heuristic) for the fair empirical risk minimization task. We also show that given sufficiently many samples, the ERM solution generalizes in two directions: both to new individuals, and to new classification tasks, drawn from their corresponding distributions. Finally we implement our algorithm and empirically verify its effectiveness.",[],[],"['Saeed Sharifi-Malvajerdi', 'Michael Kearns', 'Aaron Roth']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1b486d7a5189ebe8d8c46afc64b0d1b4-Abstract.html,Fairness & Bias,On the Fairness of Disentangled Representations,"Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations.We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable.We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent.Analyzing the representations of more than 12600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.",[],[],"['Francesco Locatello', 'Gabriele Abbati', 'Thomas Rainforth', 'Stefan Bauer', 'Bernhard Schölkopf', 'Olivier Bachem']","['Dept. of Computer Science, ETH Zurich and Max-Planck Institute for Intelligent Systems', 'Dept. of Engineering Science, University of Oxford', 'Dept. of Statistics, University of Oxford', 'Max-Planck Institute for Intelligent Systems', 'Max-Planck Institute for Intelligent Systems', 'Google Research, Brain Team']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1d7c2aae840867027b7edd17b6aaa0e9-Abstract.html,Fairness & Bias,Exploring Algorithmic Fairness in Robust Graph Covering Problems,"Fueled by algorithmic advances, AI algorithms are increasingly being deployed in settings subject to unanticipated challenges with complex social effects. Motivated by real-world deployment of AI driven, social-network based suicide prevention and landslide risk management interventions, this paper focuses on a robust graph covering problem subject to group fairness constraints. We show that, in the absence of fairness constraints, state-of-the-art algorithms for the robust graph covering problem result in biased node coverage: they tend to discriminate individuals (nodes) based on membership in traditionally marginalized groups. To remediate this issue, we propose a novel formulation of the robust covering problem with fairness constraints and a tractable approximation scheme applicable to real world instances. We provide a formal analysis of the price of group fairness (PoF) for this problem, where we show that uncertainty can lead to greater PoF. We demonstrate the effectiveness of our approach on several real-world social networks. Our method yields competitive node coverage while significantly improving group fairness relative to state-of-the-art methods.",[],[],"['Aida Rahmattalabi', 'Phebe Vayanos', 'Anthony Fulginiti', 'Eric Rice', 'Bryan Wilder', 'Amulya Yadav', 'Milind Tambe']","['University of Southern California', 'University of Southern California', 'University of Denver', 'University of Southern California', 'Harvard University', 'Pennsylvania State University', 'Harvard University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/201d546992726352471cfea6b0df0a48-Abstract.html,Fairness & Bias,Assessing Social and Intersectional Biases in Contextualized Word Representations,"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.",[],[],"['Yi Chern Tan', 'L. Elisa Celis']","['Yale University', 'Yale University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/373e4c5d8edfa8b74fd4b6791d0cf6dc-Abstract.html,Fairness & Bias,Unlocking Fairness: a Trade-off Revisited,"The prevailing wisdom is that a model's fairness and its accuracy  are in tension with one another.  However, there is a pernicious  {\em modeling-evaluating dualism} bedeviling fair machine learning  in which phenomena such as label bias are appropriately acknowledged  as a source of unfairness when designing fair models,  only to be tacitly abandoned when evaluating them.  We investigate  fairness and accuracy, but this time under a variety of controlled  conditions in which we vary the amount and type of bias.  We find,  under reasonable assumptions, that the tension between fairness and  accuracy is illusive, and vanishes as soon as we account for these  phenomena during evaluation.  Moreover, our results are consistent  with an opposing conclusion: fairness and accuracy are sometimes in  accord.  This raises the question, {\em might there be a way to    harness fairness to improve accuracy after all?}  Since most  notions of fairness are with respect to the model's predictions and  not the ground truth labels, this provides an opportunity to see if  we can improve accuracy by harnessing appropriate notions of  fairness over large quantities of {\em unlabeled} data with  techniques like posterior regularization and generalized  expectation.  Indeed, we find that semi-supervision not only  improves fairness, but also accuracy and has advantages over  existing in-processing methods that succumb to selection bias on the  training set.",[],[],"['Michael Wick', 'swetasudha panda', 'Jean-Baptiste Tristan']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/44a2e0804995faf8d2e3b084a1e2db1d-Abstract.html,Fairness & Bias,PC-Fairness: A Unified Framework for Measuring Causality-based Fairness,"A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.",[],[],"['Yongkai Wu', 'Lu Zhang', 'Xintao Wu', 'Hanghang Tong']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/73e0f7487b8e5297182c5a711d20bf26-Abstract.html,Fairness & Bias,The Fairness of Risk Scores Beyond Classification: Bipartite Ranking and the XAUC Metric,"Where machine-learned predictive risk scores inform high-stakes decisions, such as bail and sentencing in criminal justice, fairness has been a serious concern. Recent work has characterized the disparate impact that such risk scores can have when used for a binary classification task. This may not account, however, for the more diverse downstream uses of risk scores and their non-binary nature. To better account for this, in this paper, we investigate the fairness of predictive risk scores from the point of view of a bipartite ranking task, where one seeks to rank positive examples higher than negative ones. We introduce the xAUC disparity as a metric to assess the disparate impact of risk scores and define it as the difference in the probabilities of ranking a random positive example from one protected group above a negative one from another group and vice versa. We provide a decomposition of bipartite ranking loss into components that involve the discrepancy and components that involve pure predictive ability within each group. We use xAUC analysis to audit predictive risk scores for recidivism prediction, income prediction, and cardiac arrest prediction, where it describes disparities that are not evident from simply comparing within-group predictive performance.",[],[],"['Nathan Kallus', 'Angela Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html,Fairness & Bias,Characterizing Bias in Classifiers using Generative Models,"Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the ""blind spots"" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.",[],[],"['Daniel McDuff', 'Shuang Ma', 'Yale Song', 'Ashish Kapoor']","['Microsoft, Redmond, WA', 'Microsoft, Redmond, WA', 'Microsoft, Redmond, WA', 'SUNY Buffalo, Buffalo, NY']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/8d5e957f297893487bd98fa830fa6413-Abstract.html,Fairness & Bias,Noise-tolerant fair classification,"Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.",[],[],"['Alex Lamy', 'Ziyuan Zhong', 'Aditya K. Menon', 'Nakul Verma']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/b4189d9de0fb2b9cce090bd1a15e3420-Abstract.html,Fairness & Bias,Inherent Tradeoffs in Learning Fair Representations,"With the prevalence of machine learning in high-stakes applications, especially the ones regulated by anti-discrimination laws or societal norms, it is crucial to ensure that the predictive models do not propagate any existing bias or discrimination. Due to the ability of deep neural nets to learn rich representations, recent advances in algorithmic fairness have focused on learning fair representations with adversarial techniques to reduce bias in data while preserving utility simultaneously. In this paper, through the lens of information theory, we provide the first result that quantitatively characterizes the tradeoff between demographic parity and the joint utility across different population groups. Specifically, when the base rates differ between groups, we show that any method aiming to learn fair representations admits an information-theoretic lower bound on the joint error across these groups. To complement our negative results, we also prove that if the optimal decision functions across different groups are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, our theoretical findings are also confirmed empirically on real-world datasets.",[],[],"['Han Zhao', 'Geoff Gordon']","['Machine Learning Department, School of Computer Science, Carnegie Mellon University', 'Microsoft Research, Montreal, Machine Learning Department, Carnegie Mellon University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/bbc92a647199b832ec90d7cf57074e9e-Abstract.html,Fairness & Bias,Paradoxes in Fair Machine Learning,"Equalized odds is a statistical notion of fairness in machine learning that ensures that classification algorithms do not discriminate against protected groups. We extend equalized odds to the setting of cardinality-constrained fair classification, where we have a bounded amount of a resource to distribute. This setting coincides with classic fair division problems, which allows us to apply concepts from that literature in parallel to equalized odds. In particular, we consider the axioms of resource monotonicity, consistency, and population monotonicity, all three of which relate different allocation instances to prevent paradoxes. Using a geometric characterization of equalized odds, we examine the compatibility of equalized odds with these axioms. We empirically evaluate the cost of allocation rules that satisfy both equalized odds and axioms of fair division on a dataset of FICO credit scores.",[],[],"['Paul Goelz', 'Anson Kahng', 'Ariel D. Procaccia']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d0ac1ed0c5cb9ecbca3d2496ec1ad984-Abstract.html,Fairness & Bias,Re-examination of the Role of Latent Variables in Sequence Modeling,"With latent variables, stochastic recurrent models have achieved state-of-the-art performance in modeling sound-wave sequence.However, opposite results are also observed in other domains, where standard recurrent networks often outperform stochastic models.To better understand this discrepancy, we re-examine the roles of latent variables in stochastic recurrent models for speech density estimation.Our analysis reveals that under the restriction of fully factorized output distribution in previous evaluations, the stochastic variants were implicitly leveraging intra-step correlation but the deterministic recurrent baselines were prohibited to do so, resulting in an unfair comparison.To correct the unfairness, we remove such restriction in our re-examination, where all the models can explicitly leverage intra-step correlation with an auto-regressive structure.Over a diverse set of univariate and multivariate sequential data, including human speech, MIDI music, handwriting trajectory, and frame-permuted speech, our results show that stochastic recurrent models fail to deliver the performance advantage claimed in previous work. %exhibit any practical advantage despite the claimed theoretical superiority. In contrast, standard recurrent models equipped with an auto-regressive output distribution consistently perform better, dramatically advancing the state-of-the-art results on three speech datasets.",[],[],"['Guokun Lai', 'Zihang Dai', 'Yiming Yang', 'Shinjae Yoo']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Brookhaven National Laboratory']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d69768b3da745b77e82cdbddcc8bac98-Abstract.html,Fairness & Bias,Offline Contextual Bandits with High Probability Fairness Guarantees,"We present RobinHood, an ofﬂine contextual bandit algorithm designed to satisfy a broad family of fairness constraints. Our algorithm accepts multiple fairness deﬁnitions and allows users to construct their own unique fairness deﬁnitions for the problem at hand. We provide a theoretical analysis of RobinHood, which includes a proof that it will not return an unfair solution with probability greater than a user-speciﬁed threshold. We validate our algorithm on three applications: a tutoring system in which we conduct a user study and consider multiple unique fairness deﬁnitions; a loan approval setting (using the Statlog German credit data set) in which well-known fairness deﬁnitions are applied; and criminal recidivism (using data released by ProPublica). In each setting, our algorithm is able to produce fair policies that achieve performance competitive with other ofﬂine and online contextual bandit algorithms.",[],[],"['Blossom Metevier', 'Stephen Giguere', 'Sarah Brockman', 'Ari Kobren', 'Yuriy Brun', 'Emma Brunskill', 'Philip S. Thomas']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e00406144c1e7e35240afed70f34166a-Abstract.html,Fairness & Bias,Discrimination in Online Markets: Effects of Social Bias on Learning from Reviews and Policy Design,"The increasing popularity of online two-sided markets such as ride-sharing, accommodation and freelance labor platforms, goes hand in hand with new socioeconomic challenges.  One major issue remains the existence of bias and discrimination against certain social groups. We study this problem using  a two-sided large market model with  employers and workers mediated by a platform. Employers who seek to hire  workers face uncertainty about a candidate worker's  skill level. Therefore, they base their hiring decision  on learning from past reviews about an individual worker as well as on their (possibly misspecified)  prior beliefs about the ability level of the social group the worker belongs to. Drawing upon the  social learning literature with bounded rationality and limited information, uncertainty combined with social bias leads to  unequal hiring opportunities between workers of different social groups. Although the effect of social bias decreases as the number of reviews increases (consistent with empirical findings), minority workers still receive lower expected  payoffs. Finally, we  consider a simple directed matching policy (DM), which combines learning and matching to make better matching decisions for minority workers. Under this policy, there exists a steady-state equilibrium, in which  DM reduces the discrimination gap.",[],[],"['Faidra Georgia Monachou', 'Itai Ashlagi']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html,Fairness & Bias,Differential Privacy Has Disparate Impact on Model Accuracy,"Differential privacy (DP) is a popular mechanism for training machinelearning models with bounded leakage about the presence of specificpoints in the training data.  The cost of differential privacy is areduction in the model's accuracy.  We demonstrate that in the neuralnetworks trained using differentially private stochastic gradient descent(DP-SGD), this cost is not borne equally: accuracy of DP models dropsmuch more for the underrepresented classes and subgroups.For example, a gender classification model trained using DP-SGD exhibitsmuch lower accuracy for black faces than for white faces.  Critically,this gap is bigger in the DP model than in the non-DP model, i.e., ifthe original model is unfair, the unfairness becomes worse once DP isapplied.  We demonstrate this effect for a variety of tasks and models,including sentiment analysis of text and image classification.  We thenexplain why DP training mechanisms such as gradient clipping and noiseaddition have disproportionate effect on the underrepresented and morecomplex subgroups, resulting in a disparate reduction of model accuracy.",[],[],"['Eugene Bagdasaryan', 'Omid Poursaeed', 'Vitaly Shmatikov']","['Cornell Tech', 'Cornell Tech', 'Cornell Tech']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html,Privacy & Data Governance,Capacity Bounded Differential Privacy,"Differential privacy, a notion of algorithmic stability, is a gold standard for measuring the additional risk an algorithm's output poses to the privacy of asingle record in the dataset. Differential privacy is defined as the distancebetween the output distribution of an algorithm on neighboring datasets thatdiffer in one entry. In this work, we present a novel relaxation of differentialprivacy, capacity bounded differential privacy, where the adversarythat distinguishes output distributions is assumed to becapacity-bounded -- i.e. bounded not in computational power, but interms of the function class from which their attack algorithm is drawn. We modeladversaries in terms of restricted f-divergences between probabilitydistributions, and study properties of the definition and algorithms thatsatisfy them.",[],[],"['Kamalika Chaudhuri', 'Jacob Imola', 'Ashwin Machanavajjhala']","['UC San Diego', 'UC San Diego', 'Duke University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html,Privacy & Data Governance,MixMatch: A Holistic Approach to Semi-Supervised Learning,"Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets.In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, thatguesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using MixUp.MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example,on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10.We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy.Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.Code is attached.",[],[],"['David Berthelot', 'Nicholas Carlini', 'Ian Goodfellow', 'Nicolas Papernot', 'Avital Oliver', 'Colin A. Raffel']","['Google Research', 'Google Research', 'Work done at Google', 'Google Research', 'Google Research', 'Google Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/376c6b9ff3bedbbea56751a84fffc10c-Abstract.html,Privacy & Data Governance,An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors,"Differential privacy has emerged as the main definition for private data analysis and machine learning. The global model of differential privacy, which assumes that users trust the data collector, provides strong privacy guarantees and introduces small errors in the output. In contrast, applications of differential privacy in commercial systems by Apple, Google, and Microsoft, use the local model. Here, users do not trust the data collector, and hence randomize their data before sending it to the data collector. Unfortunately, local model is too strong for several important applications and hence is limited in its applicability. In this work, we propose a framework based on trusted processors and a new definition of differential privacy called Oblivious Differential Privacy, which combines the best of both local and global models. The algorithms we design in this framework show interesting interplay of ideas from the streaming algorithms, oblivious algorithms, and differential privacy.",[],[],"['Joshua Allen', 'Bolin Ding', 'Janardhan Kulkarni', 'Harsha Nori', 'Olga Ohrimenko', 'Sergey Yekhanin']","['Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/3c88c1db16b9523b4dcdcd572aa1e16a-Abstract.html,Privacy & Data Governance,Differentially Private Distributed Data Summarization under Covariate Shift,"We envision Artificial Intelligence marketplaces to be platforms where consumers, with very less data for a target task, can obtain a relevant model by accessing many private data sources with vast number of data samples.  One of the key challenges is to construct a training dataset that matches a target task without compromising on privacy of the data sources. To this end, we consider the following distributed data summarizataion problem. Given K private source datasets denoted by $[D_i]_{i\in [K]}$ and a small target validation set $D_v$, which may involve a considerable covariate shift with respect to the sources, compute a summary dataset $D_s\subseteq \bigcup_{i\in [K]} D_i$ such that its statistical distance from the validation dataset $D_v$ is minimized. We use the popular Maximum Mean Discrepancy as the measure of statistical distance. The non-private problem has received considerable attention in prior art, for example in prototype selection (Kim et al., NIPS 2016). Our work is the first to obtain strong differential privacy guarantees while ensuring the quality guarantees of the non-private version. We study this problem in a Parsimonious Curator Privacy Model, where a trusted curator coordinates the summarization process while minimizing the amount of private information accessed. Our central result is a novel protocol that (a) ensures the curator does not access more than $O(K^{\frac{1}{3}}|D_s| + |D_v|)$ points (b) has formal privacy guarantees on the leakage of information between the data owners and (c) closely matches the  best known non-private greedy algorithm. Our protocol uses two hash functions, one inspired by the Rahimi-Recht random features method and the second leverages state of the art differential privacy mechanisms. We introduce a novel ``noiseless'' differentially private auctioning protocol, which may be of independent interest.  Apart from theoretical guarantees, we demonstrate the efficacy of our protocol using real-world datasets.",[],[],"['Kanthi Sarpatwar', 'Karthikeyan Shanmugam', 'Venkata Sitaramagiridharganesh Ganapavarapu', 'Ashish Jagmohan', 'Roman Vaculin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/3ef815416f775098fe977004015c6193-Abstract.html,Privacy & Data Governance,Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation,"The simplest and most widely applied method for guaranteeing differential privacy is to add instance-independent noise to a statistic of interest that is scaled to its global sensitivity. However, global sensitivity is a worst-case notion that is often too conservative for realized dataset instances. We provide methods for scaling noise in an instance-dependent way and demonstrate that they provide greater accuracy under average-case distributional assumptions. Specifically, we consider the basic problem of privately estimating the mean of a real distribution from i.i.d. samples. The standard empirical mean estimator can have arbitrarily-high global sensitivity. We propose the trimmed mean estimator, which interpolates between the mean and the median, as a way of attaining much lower sensitivity on average while losing very little in terms of statistical accuracy. To privately estimate the trimmed mean, we revisit the smooth sensitivity framework of Nissim, Raskhodnikova, and Smith (STOC 2007), which provides a framework for using instance-dependent sensitivity. We propose three new additive noise distributions which provide concentrated differential privacy when scaled to smooth sensitivity. We provide theoretical and experimental evidence showing that our noise distributions compare favorably to others in the literature, in particular, when applied to the mean estimation problem.",[],[],"['Mark Bun', 'Thomas Steinke']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4158f6d19559955bae372bb00f6204e4-Abstract.html,Privacy & Data Governance,Differentially Private Covariance Estimation,"The covariance matrix of a dataset is a fundamental statistic that can be used for calculating optimum regression weights as well as in many other learning and data analysis settings. For datasets containing private user information, we often want to estimate the covariance matrix in a way that preserves differential privacy. While there are known methods for privately computing the covariance matrix, they all have one of two major shortcomings. Some, like the Gaussian mechanism, only guarantee (epsilon, delta)-differential privacy, leaving a non-trivial probability of privacy failure. Others give strong epsilon-differential privacy guarantees, but are impractical, requiring complicated sampling schemes, and tend to perform poorly on real data. In this work we propose a new epsilon-differentially private algorithm for computing the covariance matrix of a dataset that addresses both of these limitations. We show that it has lower error than existing state-of-the-art approaches, both analytically and empirically. In addition, the algorithm is significantly less complicated than other methods and can be efficiently implemented with rejection sampling.",[],[],"['Kareem Amin', 'Travis Dick', 'Alex Kulesza', 'Andres Munoz', 'Sergei Vassilvitskii']","['Google Research NY', 'Carnegie Mellon University', 'Google Research NY', 'Google Research NY', 'Google Research NY']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/41c576a3bac4220845f9427b002a2a9d-Abstract.html,Privacy & Data Governance,Facility Location Problem in Differential Privacy Model Revisited,"In this paper we study the facility location problem in the model of differential privacy (DP) with uniform facility cost. Specifically, we first show that under the hierarchically well-separated tree (HST) metrics and the super-set output setting that was introduced in Gupta et. al., there is an $\epsilon$-DP algorithm that achieves an $O(\frac{1}{\epsilon})$(expected multiplicative) approximation ratio; this implies an $O(\frac{\log n}{\epsilon})$ approximation ratio for the general metric case, where $n$ is the size of the input metric. These bounds improve the best-known results given by Gupta et. al.  In particular, our approximation ratio for HST-metrics is independent of $n$, and the ratio for general metrics is independent of the aspect ratio of the input metric. On the negative side, we show that the approximation ratio of any $\epsilon$-DP algorithm is lower bounded by $\Omega(\frac{1}{\sqrt{\epsilon}})$, even for instances on HST metrics with uniform facility cost, under the super-set output setting. The lower bound shows that the dependence of the approximation ratio for HST metrics on $\epsilon$ can not be removed or greatly improved. Our novel methods and techniques for both the upper and lower bound may find additional applications.",[],[],"['Yunus Esencayi', 'Marco Gaboardi', 'Shi Li', 'Di Wang']","['SUNY at Buffalo', 'Boston University', 'SUNY at Buffalo', 'SUNY at Buffalo']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/596f713f9a7376fe90a62abaaedecc2d-Abstract.html,Privacy & Data Governance,Knowledge Extraction with No Observable Data,"Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge, it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However, the data are not accessible in many cases due to the privacy or confidentiality issues in medical, industrial, and military domains. To the best of our knowledge, there has been no approach that distills the knowledge of a neural network when no data are observable. In this work, we propose KegNet (Knowledge Extraction with Generative Networks), a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KegNet outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.",[],[],"['Jaemin Yoo', 'Minyong Cho', 'Taebum Kim', 'U Kang']","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5dec707028b05bcbd3a1db5640f842c5-Abstract.html,Privacy & Data Governance,Differentially Private Bagging: Improved utility and cheaper privacy than subsample-and-aggregate,"Differential Privacy is a popular and well-studied notion of privacy. In the era ofbig data that we are in, privacy concerns are becoming ever more prevalent and thusdifferential privacy is being turned to as one such solution. A popular method forensuring differential privacy of a classifier is known as subsample-and-aggregate,in which the dataset is divided into distinct chunks and a model is learned on eachchunk, after which it is aggregated. This approach allows for easy analysis of themodel on the data and thus differential privacy can be easily applied. In this paper,we extend this approach by dividing the data several times (rather than just once)and learning models on each chunk within each division. The first benefit of thisapproach is the natural improvement of utility by aggregating models trained ona more diverse range of subsets of the data (as demonstrated by the well-knownbagging technique). The second benefit is that, through analysis that we provide inthe paper, we can derive tighter differential privacy guarantees when several queriesare made to this mechanism.  In order to derive these guarantees, we introducethe upwards and downwards moments accountants and derive bounds for thesemoments accountants in a data-driven fashion. We demonstrate the improvementsour model makes over standard subsample-and-aggregate in two datasets (HeartFailure (private) and UCI Adult (public)).",[],[],"['James Jordon', 'Jinsung Yoon', 'Mihaela van der Schaar']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6646b06b90bd13dabc11ddba01270d23-Abstract.html,Privacy & Data Governance,Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces,"We consider differentially private algorithms for reinforcement learning in continuous spaces, such that neighboring reward functions are indistinguishable. This protects the reward information from being exploited by methods such as inverse reinforcement learning. Existing studies that guarantee differential privacy are not extendable to infinite state spaces, as the noise level to ensure privacy will scale accordingly to infinity. Our aim is to protect the value function approximator, without regard to the number of states queried to the function. It is achieved by adding functional noise to the value function iteratively in the training. We show rigorous privacy guarantees by a series of analyses on the kernel of the noise space, the probabilistic bound of such noise samples, and the composition over the iterations. We gain insight into the utility analysis by proving the algorithm's approximate optimality when the state space is discrete. Experiments corroborate our theoretical findings and show improvement over existing approaches.",[],[],"['Baoxiang Wang', 'Nidhi Hegde']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6832a7b24bc06775d02b7406880b93fc-Abstract.html,Privacy & Data Governance,Uncoupled Regression from Pairwise Comparison Data,"Uncoupled regression is the problem to learn a model from unlabeled data and the set of target values while the correspondence between them is unknown. Such a situation arises in predicting anonymized targets that involve sensitive information, e.g., one's annual income. Since existing methods for uncoupled regression often require strong assumptions on the true target function, and thus, their range of applications is limited, we introduce a novel framework that does not require such assumptions in this paper. Our key idea is to utilize \emph{pairwise comparison data, which consists of pairs of unlabeled data that we know which one has a larger target value. Such pairwise comparison data is easy to collect, as typically discussed in the learning-to-rank scenario, and does not break the anonymity of data. We propose two practical methods for uncoupled regression from pairwise comparison data and show that the learned regression model converges to the optimal model with the optimal parametric convergence rate when the target variable distributes uniformly. Moreover, we empirically show that for linear models the proposed methods are comparable to ordinary supervised regression with labeled data.",[],[],"['Liyuan Xu', 'Junya Honda', 'Gang Niu', 'Masashi Sugiyama']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/68d30a9594728bc39aa24be94b319d21-Abstract.html,Privacy & Data Governance,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians,"Learning the parameters of Gaussian mixture models is a fundamental and widely studied problem with numerous applications. In this work, we give new algorithms for learning the parameters of a high-dimensional, well separated, Gaussian mixture model subject to the strong constraint of differential privacy. In particular, we give a differentially private analogue of the algorithm of Achlioptas and McSherry. Our algorithm has two key properties not achieved by prior work: (1) The algorithm’s sample complexity matches that of the corresponding non-private algorithm up to lower order terms in a wide range of parameters. (2) The algorithm requires very weak a priori bounds on the parameters of the mixture components.",[],[],"['Gautam Kamath', 'Or Sheffet', 'Vikrant Singhal', 'Jonathan Ullman']","['David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada', 'Department of Computer Science, Faculty of Exact Sciences, Bar-Ilan University, Ramat-Gan, Israel', 'Khoury College of Computer Sciences, Northeastern University, Boston, MA', 'Khoury College of Computer Sciences, Northeastern University, Boston, MA']","['Canada', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/700fdb2ba62d4554dc268c65add4b16e-Abstract.html,Privacy & Data Governance,Private Learning Implies Online Learning: An Efficient Reduction,"We study the relationship between the notions of differentially private learning and online learning. Several recent works have shown that differentially private learning implies online learning, but an open problem of Neel, Roth, and Wu \cite{NeelAaronRoth2018} asks whether this implication is {\it efficient}. Specifically, does an efficient differentially private learner imply an efficient online learner? In this paper we resolve this open question in the context of pure differential privacy.We derive an efficient black-box reduction from differentially private learning to online learning from expert advice.",[],[],"['Alon Gonen', 'Elad Hazan', 'Shay Moran']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/73231e53eeef362c814c8522f5257286-Abstract.html,Privacy & Data Governance,Oblivious Sampling Algorithms for Private Data Analysis,"We study secure and privacy-preserving data analysisbased on queries executed on samples from a dataset.Trusted execution environments (TEEs) can be used toprotect the content of the data during query computation,while supporting differential-private (DP) queries in TEEsprovides record privacy when query output is revealed.Support for sample-based queries is attractivedue to \emph{privacy amplification}since not all dataset is used to answer a query but only a small subset.However, extracting data samples with TEEswhile proving strong DP guarantees is nottrivial as secrecy of sample indices has to be preserved.To this end, we design efficient secure variants of common sampling algorithms.Experimentally we show that accuracy of modelstrained with shuffling and sampling is the same fordifferentially private models for MNIST and CIFAR-10,while sampling provides stronger privacy guarantees than shuffling.",[],[],"['Sajin Sasy', 'Olga Ohrimenko']","['University of Waterloo', 'Microsoft Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/7a674153c63cff1ad7f0e261c369ab2c-Abstract.html,Privacy & Data Governance,Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases,"Differential privacy has become a widely accepted notion of privacy, leading to the introduction and deployment of numerous privatization mechanisms. However, ensuring the privacy guarantee is an error-prone process, both in designing mechanisms and in implementing those mechanisms. Both types of errors will be greatly reduced, if we have a data-driven approach to verify privacy guarantees, from a black-box access to a mechanism. We pose it as a property estimation problem, and study the fundamental trade-offs involved in the accuracy in estimated privacy guarantees and the number of samples required. We introduce a novel estimator that uses polynomial approximation of a carefully chosen degree to optimally trade-off bias and variance. With n samples, we show that this estimator achieves performance of a straightforward plug-in estimator with n*log(n) samples, a phenomenon referred to as effective sample size amplification. The minimax optimality of the proposed estimator is proved by comparing it to a matching fundamental lower bound.",[],[],"['Xiyang Liu', 'Sewoong Oh']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8e036cc193d0af59aa9b22821248292b-Abstract.html,Privacy & Data Governance,Private Testing of Distributions via Sample Permutations,"Statistical tests are at the heart of many scientific tasks.To validate their hypothesis, researchers in medical and social sciences use individuals' data. The sensitivity of participants' data requires the design of statistical tests that ensure the privacy of the individuals in the most efficient way. In this paper, we use the framework of property testing to design algorithms to test the properties of the distribution that the data is drawn from with respect to differential privacy. In particular, we investigate testing two fundamental properties of distributions:  (1) testing the equivalence of two distributions when we have unequal numbers of samples from the two distributions. (2) Testing independence of two random variables. In both cases, we show that our testers achieve near optimal sample complexity (up to logarithmic factors). Moreover, our dependence on the privacy parameter is an additive term, which indicates that differential privacy can be obtained in most regimes of parameters for free.",[],[],"['Maryam Aliakbarpour', 'Ilias Diakonikolas', 'Daniel Kane', 'Ronitt Rubinfeld']","['CSAIL, MIT', 'University of Wisconsin, Madison', 'University of California, San Diego', 'CSAIL, MIT, TAU']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/955cb567b6e38f4c6b3f28cc857fc38c-Abstract.html,Privacy & Data Governance,Efficiently Estimating Erdos-Renyi Graphs with Node Differential Privacy,"We give a simple, computationally efficient, and node-differentially-private algorithm for estimating the parameter of an Erdos-Renyi graph---that is, estimating p in a G(n,p)---with near-optimal accuracy.  Our algorithm nearly matches the information-theoretically optimal exponential-time algorithm for the same problem due to Borgs et al. (FOCS 2018).  More generally, we give an optimal, computationally efficient, private algorithm for estimating the edge-density of any graph whose degree distribution is concentrated in a small interval.",[],[],"['Jonathan Ullman', 'Adam Sealfon']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html,Privacy & Data Governance,Private Hypothesis Selection,"We provide a differentially private algorithm for hypothesis selection.   Given samples from an unknown probability distribution $P$ and a set of $m$ probability distributions $\mathcal{H}$, the goal is to output, in a $\varepsilon$-differentially private manner, a distribution from $\mathcal{H}$ whose total variation distance to $P$ is comparable to that of the best such distribution (which we denote by $\alpha$).  The sample complexity of our basic algorithm is $O\left(\frac{\log m}{\alpha^2} + \frac{\log m}{\alpha \varepsilon}\right)$, representing a minimal cost for privacy when compared to the non-private algorithm. We also can handle infinite hypothesis classes $\mathcal{H}$ by relaxing to $(\varepsilon,\delta)$-differential privacy.  We apply our hypothesis selection algorithm to give learning algorithms for a number of natural distribution classes, including Gaussians, product distributions, sums of independent random variables, piecewise polynomials, and mixture classes.  Our hypothesis selection procedure allows us to generically convert a cover for a class to a learning algorithm, complementing known learning lower bounds which are in terms of the size of the packing number of the class.  As the covering and packing numbers are often closely  related, for constant $\alpha$, our algorithms achieve the optimal sample complexity for many classes of interest.  Finally, we describe an application to private distribution-free PAC learning.",[],[],"['Mark Bun', 'Gautam Kamath', 'Thomas Steinke', 'Steven Z. Wu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/9a6a1aaafe73c572b7374828b03a1881-Abstract.html,Privacy & Data Governance,Limits of Private Learning with Access to Public Data,"We consider learning problems where the training set consists of two types of examples: private and public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. This setting interpolates between private learning (where all examples are private) and classical learning (where all examples are public). We study the limits of learning in this setting in terms of private and public sample complexities. We show that any hypothesis class of VC-dimension $d$ can be agnostically learned up to an excess error of $\alpha$ using only (roughly) $d/\alpha$ public examples and $d/\alpha^2$ private labeled examples. This result holds even when the public examples are unlabeled. This gives a quadratic improvement over the standard $d/\alpha^2$ upper bound on the public sample complexity (where private examples can be ignored altogether if the public examples are labeled). Furthermore, we give a nearly matching lower bound, which we prove via a generic reduction from this setting to the one of private learning without public data.",[],[],"['Noga Alon', 'Raef Bassily', 'Shay Moran']","['Department of Mathematics, Princeton University', 'Department of Computer Science & Engineering, The Ohio State University', 'Google AI, Princeton']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/9d28de8ff9bb6a3fa41fddfdc28f3bc1-Abstract.html,Privacy & Data Governance,Partially Encrypted Deep Learning using Functional Encryption,"Machine learning on encrypted data has received a lot of attention thanks to recent breakthroughs in homomorphic encryption and secure multi-party computation. It allows outsourcing computation to untrusted servers without sacrificing privacy of sensitive data. We propose a practical framework to perform partially encrypted and privacy-preserving predictions which combines adversarial training and functional encryption. We first present a new functional encryption scheme to efficiently compute quadratic functions so that the data owner controls what can be computed but is not involved in the calculation: it provides a decryption key which allows one to learn a specific function evaluation of some encrypted data. We then show how to use it in machine learning to partially encrypt neural networks with quadratic activation functions at evaluation time and we provide a thorough analysis of the information leaks based on indistinguishability of data items of the same label. Last, since several encryption schemes cannot deal with the last thresholding operation used for classification, we propose a training method to prevent selected sensitive features from leaking which adversarially optimizes the network against an adversary trying to identify these features. This is of great interest for several existing works using partially encrypted machine learning as it comes with almost no cost on the model's accuracy and significantly improves data privacy.",[],[],"['Théo Ryffel', 'David Pointcheval', 'Francis Bach', 'Edouard Dufour-Sans', 'Romain Gay']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a501bebf79d570651ff601788ea9d16d-Abstract.html,Privacy & Data Governance,Privacy-Preserving Classification of Personal Text Messages with Secure Multi-Party Computation,"Classification of personal text messages has many useful applications in surveillance, e-commerce, and mental health care, to name a few. Giving applications access to personal texts can easily lead to (un)intentional privacy violations. We propose the first privacy-preserving solution for text classification that is provably secure. Our method, which is based on Secure Multiparty Computation (SMC), encompasses both feature extraction from texts, and subsequent classification with logistic regression and tree ensembles. We prove that when using our secure text classification method, the application does not learn anything about the text, and the author of the text does not learn anything about the text classification model used by the application beyond what is given by the classification result itself. We perform end-to-end experiments with an application for detecting hate speech against women and immigrants, demonstrating excellent runtime results without loss of accuracy.",[],[],"['Devin Reich', 'Ariel Todoki', 'Rafael Dowsley', 'Martine De Cock', 'anderson nascimento']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/a588a6199feff5ba48402883d9b72700-Abstract.html,Privacy & Data Governance,Locally Private Gaussian Estimation,"We study a basic private estimation problem: each of n users draws a single i.i.d. sample from an unknown Gaussian distribution N(\mu,\sigma^2), and the goal is to estimate \mu while guaranteeing local differential privacy for each user. As minimizing the number of rounds of interaction is important in the local setting, we provide adaptive two-round solutions and nonadaptive one-round solutions to this problem. We match these upper bounds with an information-theoretic lower bound showing that our accuracy guarantees are tight up to logarithmic factors for all sequentially interactive locally private protocols.",[],[],"['Matthew Joseph', 'Janardhan Kulkarni', 'Jieming Mao', 'Steven Z. Wu']","['University of Pennsylvania', 'Microsoft Research Redmond', 'Google Research New York', 'University of Minnesota']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/b139e104214a08ae3f2ebcce149cdf6e-Abstract.html,Privacy & Data Governance,Practical Differentially Private Top-k Selection with Pay-what-you-get Composition,"We study the problem of top-k selection over a large domain universe subject to user-level differential privacy.  Typically, the exponential mechanism or report noisy max are the algorithms used to solve this problem.  However, these algorithms require querying the database for the count of each domain element.  We focus on the setting where the data domain is unknown, which is different than the setting of frequent itemsets where an apriori type algorithm can help prune the space of domain elements to query.  We design algorithms that ensures (approximate) differential privacy and only needs access to the true top-k' elements from the data for any chosen k' ≥ k.  This is a highly desirable feature for making differential privacy practical, since the algorithms require no knowledge of the domain.  We consider both the setting where a user's data can modify an arbitrary number of counts by at most 1, i.e. unrestricted sensitivity, and the setting where a user's data can modify at most some small, fixed number of counts by at most 1, i.e. restricted sensitivity.  Additionally, we provide a pay-what-you-get privacy composition bound for our algorithms.  That is, our algorithms might return fewer than k elements when the top-k elements are queried, but the overall privacy budget only decreases by the size of the outcome set.",[],[],"['David Durfee', 'Ryan M. Rogers']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/b3dd760eb02d2e669c604f6b2f1e803f-Abstract.html,Privacy & Data Governance,Elliptical Perturbations for Differential Privacy,"We study elliptical distributions in locally convex vector spaces, and determine conditions when they can or cannot be used to satisfy differential privacy (DP). A requisite condition for a sanitized statistical summary to satisfy DP is that the corresponding privacy mechanism must induce equivalent probability measures for all possible input databases. We show that elliptical distributions with the same dispersion operator, $C$, are equivalent if the difference of their means lies in the Cameron-Martin space of $C$. In the case of releasing finite-dimensional summaries using elliptical perturbations, we show that the privacy parameter $\ep$ can be computed in terms of a one-dimensional maximization problem. We apply this result to consider multivariate Laplace, $t$, Gaussian, and $K$-norm noise. Surprisingly, we show that the multivariate Laplace noise does not achieve $\ep$-DP in any dimension greater than one. Finally, we show that when the dimension of the space is infinite, no elliptical distribution can be used to give $\ep$-DP; only $(\epsilon,\delta)$-DP is possible.",[],[],"['Matthew Reimherr', 'Jordan Awan']","['Department of Statistics, Pennsylvania State University, University Park, PA', 'Department of Statistics, Pennsylvania State University, University Park, PA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/c14a2a57ead18f3532a5a8949382c536-Abstract.html,Privacy & Data Governance,Learning Auctions with Robust Incentive Guarantees,"We study the problem of learning Bayesian-optimal revenue-maximizing auctions. The classical approach to maximizing revenue requires a known prior distribution on the demand of the bidders, although recent work has shown how to replace the knowledge of a prior distribution with a polynomial sample. However, in an online setting, when buyers can participate in multiple rounds, standard learning techniques are susceptible to \emph{strategic overfitting}: bidders can improve their long-term wellbeing by manipulating the trajectory of the learning algorithm in earlier rounds. For example, they may be able to strategically adjust their behavior in earlier rounds to achieve lower, more favorable future prices. Such non-truthful behavior can hinder learning and harm revenue.  In this paper, we combine tools from differential privacy, mechanism design, and sample complexity to give a repeated auction that (1) learns bidder demand from past data, (2) is approximately revenue-optimal, and (3) strategically robust, as it incentivizes bidders to behave truthfully.",[],[],"['Jacob D. Abernethy', 'Rachel Cummings', 'Bhuvesh Kumar', 'Sam Taggart', 'Jamie H. Morgenstern']","['Georgia Tech', 'Georgia Tech', 'Georgia Tech', 'Georgia Tech', 'Oberlin College']","['Georgia', 'Georgia', 'Georgia', 'Georgia']"
https://papers.nips.cc/paper_files/paper/2019/hash/c36b1132ac829ece87dda55d77ac06a4-Abstract.html,Privacy & Data Governance,Online Learning via the Differential Privacy Lens,"In this paper, we use differential privacy as a lens to examine online learning in both full and partial information settings. The differential privacy framework is, at heart, less about privacy and more about algorithmic stability, and thus has found application in domains well beyond those where information security is central. Here we develop an algorithmic property called one-step differential stability which facilitates a more refined regret analysis for online learning methods. We show that tools from the differential privacy literature can yield regret bounds for many interesting online learning problems including online convex optimization and online linear optimization. Our stability notion is particularly well-suited for deriving first-order regret bounds for follow-the-perturbed-leader algorithms, something that all previous analyses have struggled to achieve. We also generalize the standard max-divergence to obtain a broader class called Tsallis max-divergences. These define stronger notions of stability that are useful in deriving bounds in partial information settings such as multi-armed bandits and bandits with experts.",[],[],"['Jacob D. Abernethy', 'Young Hun Jung', 'Chansoo Lee', 'Audra McMillan', 'Ambuj Tewari']","['College of Computing, Georgia Institute of Technology', 'Department of Statistics, University of Michigan', 'Google Brain', 'Simons Inst. for the Theory of Computing, Department of Computer Science, Boston University, Khoury College of Computer Sciences, Northeastern University', 'Department of Statistics, Department of EECS, University of Michigan']",['Georgia']
https://papers.nips.cc/paper_files/paper/2019/hash/c4c42505a03f2e969b4c0a97ee9b34e7-Abstract.html,Privacy & Data Governance,Privacy Amplification by Mixing and Diffusion Mechanisms,"A fundamental result in differential privacy states that the privacy guarantees of a mechanism are preserved by any post-processing of its output. In this paper we investigate under what conditions stochastic post-processing can amplify the privacy of a mechanism. By interpreting post-processing as the application of a Markov operator, we first give a series of amplification results in terms of uniform mixing properties of the Markov process defined by said operator. Next we provide amplification bounds in terms of coupling arguments which can be applied in cases where uniform mixing is not available. Finally, we introduce a new family of mechanisms based on diffusion processes which are closed under post-processing, and analyze their privacy via a novel heat flow argument. On the applied side, we generalize the analysis of ""privacy amplification by iteration"" in Noisy SGD and show it admits an exponential improvement in the strongly convex case, and study a mechanism based on the Ornstein–Uhlenbeck diffusion process which contains the Gaussian mechanism with optimal post-processing on bounded inputs as a special case.",[],[],"['Borja Balle', 'Gilles Barthe', 'Marco Gaboardi', 'Joseph Geumlek']","['', 'MPI for Security and Privacy, IMDEA Software Institute', 'Boston University', 'University of California, San Diego']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d01c25576ff1c53de58e0e6970a2d510-Abstract.html,Privacy & Data Governance,Locally Private Learning without Interaction Requires Separation,"We consider learning under the constraint of local differential privacy (LDP). For many learning problems known efficient algorithms in this model require many rounds of communication between the server and the clients holding the data points. Yet multi-round protocols are prohibitively slow in practice due to network latency and, as a result, currently deployed large-scale systems are limited to a single round. Despite significant research interest, very little is known about which learning problems can be solved by such non-interactive systems. The only lower bound we are aware of is for PAC learning an artificial class of functions with respect to a uniform distribution (Kasiviswanathan et al., 2008).We show that the margin complexity of a class of Boolean functions is a lower bound on the complexity of any non-interactive LDP algorithm for distribution-independent PAC learning of the class. In particular, the classes of linear separators and decision lists require exponential number of samples to learn non-interactively even though they can be learned in polynomial time by an interactive LDP algorithm. This gives the first example of a natural problem that is significantly harder to solve without interaction and also resolves an open problem of Kasiviswanathan et al.~(2008). We complement this lower bound with a new efficient learning algorithm whose complexity is polynomial in the margin complexity of the class. Our algorithm is non-interactive on labeled samples but still needs interactive access to unlabeled samples. All of our results also apply to the statistical query model and any model in which the number of bits communicated about each data point is constrained.",[],[],"['Amit Daniely', 'Vitaly Feldman']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ddb4955263e6c08179393d1beaf18602-Abstract.html,Privacy & Data Governance,User-Specified Local Differential Privacy in Unconstrained Adaptive Online Learning,"Local differential privacy is a strong notion of privacy in which the provider of the data guarantees privacy by perturbing the data with random noise. In the standard application of local differential differential privacy the distribution of the noise is constant and known by the learner. In this paper we generalize this approach by allowing the provider of the data to choose the distribution of the noise without disclosing any parameters of the distribution to the learner, under the constraint that the distribution is symmetrical. We consider this problem in the unconstrained Online Convex Optimization setting with noisy feedback. In this setting the learner receives the subgradient of a loss function, perturbed by noise, and aims to achieve sublinear regret with respect to some competitor, without constraints on the norm of the competitor. We derive the first algorithms that have adaptive regret bounds in this setting, i.e. our algorithms adapt to the unknown competitor norm, unknown noise, and unknown sum of the norms of the subgradients, matching state of the art bounds in all cases.",[],[],['Dirk van der Hoeven'],[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e44e875c12109e4fa3716c05008048b2-Abstract.html,Privacy & Data Governance,On Differentially Private Graph Sparsification and Applications,"In this paper, we study private sparsification of graphs. In particular, we give an algorithm that given an input graph, returns a sparse graph which approximates the spectrum of the input graph while ensuring differential privacy. This allows one to solve many graph problems privately yet efficiently and accurately. This is exemplified with application of the proposed meta-algorithm to graph algorithms for privately answering cut-queries, as well as practical algorithms for computing {\scshape MAX-CUT} and {\scshape SPARSEST-CUT} with better accuracy than previously known. We also give the first efficient private algorithm to learn Laplacian eigenmap on a graph.",[],[],"['Raman Arora', 'Jalaj Upadhyay']","['Johns Hopkins University', 'Rutgers University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/faefec47428cf9a2f0875ba9c2042a81-Abstract.html,Privacy & Data Governance,KNG: The K-Norm Gradient Mechanism,"This paper presents a new mechanism for producing sanitized statistical summaries that achieve {\it differential privacy}, called the {\it K-Norm Gradient} Mechanism, or KNG. This new approach maintains the strong flexibility of the exponential mechanism, while achieving the powerful utility performance of objective perturbation. KNG starts with an inherent objective function (often an empirical risk), and promotes summaries that are close to minimizing the objective by weighting according to how far the gradient of the objective function is from zero.  Working with the gradient instead of the original objective function allows for additional flexibility as one can penalize using different norms.  We show that, unlike the exponential mechanism, the noise added by KNG is asymptotically negligible compared to the statistical error for many problems. In addition to theoretical guarantees on privacy and utility, we confirm the utility of KNG empirically in the settings of linear and quantile regression through simulations.",[],[],"['Matthew Reimherr', 'Jordan Awan']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html,Privacy & Data Governance,Differential Privacy Has Disparate Impact on Model Accuracy,"Differential privacy (DP) is a popular mechanism for training machinelearning models with bounded leakage about the presence of specificpoints in the training data.  The cost of differential privacy is areduction in the model's accuracy.  We demonstrate that in the neuralnetworks trained using differentially private stochastic gradient descent(DP-SGD), this cost is not borne equally: accuracy of DP models dropsmuch more for the underrepresented classes and subgroups.For example, a gender classification model trained using DP-SGD exhibitsmuch lower accuracy for black faces than for white faces.  Critically,this gap is bigger in the DP model than in the non-DP model, i.e., ifthe original model is unfair, the unfairness becomes worse once DP isapplied.  We demonstrate this effect for a variety of tasks and models,including sentiment analysis of text and image classification.  We thenexplain why DP training mechanisms such as gradient clipping and noiseaddition have disproportionate effect on the underrepresented and morecomplex subgroups, resulting in a disparate reduction of model accuracy.",[],[],"['Eugene Bagdasaryan', 'Omid Poursaeed', 'Vitaly Shmatikov']","['Cornell Tech', 'Cornell Tech', 'Cornell Tech']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/02bf86214e264535e3412283e817deaa-Abstract.html,Security,Lower Bounds on Adversarial Robustness from Optimal Transport,"While progress has been made in understanding the robustness of machine learning classifiers to test-time adversaries (evasion attacks), fundamental questions remain unresolved. In this paper, we use optimal transport to characterize the maximum achievable accuracy in an adversarial classification scenario. In this setting, an adversary receives a random labeled example from one of two classes, perturbs the example subject to a neighborhood constraint, and presents the modified example to the classifier. We define an appropriate cost function such that the minimum transportation cost between the distributions of the two classes determines the \emph{minimum $0-1$ loss for any classifier}. When the classifier comes from a restricted hypothesis class, the optimal transportation cost provides a lower bound. We apply our framework to the case of Gaussian data with norm-bounded adversaries and explicitly show matching bounds for the classification and transport problems and the optimality of linear classifiers. We also characterize the sample complexity of learning in this setting, deriving and extending previously known results as a special case. Finally, we use our framework to study the gap between the optimal classification performance possible and that currently achieved by state-of-the-art robustly trained neural networks for datasets of interest, namely, MNIST, Fashion MNIST and CIFAR-10.",[],[],"['Arjun Nitin Bhagoji', 'Daniel Cullina', 'Prateek Mittal']","['Department of Electrical Engineering, Princeton University', 'Department of Electrical Engineering, Pennsylvania State University', 'Department of Electrical Engineering, Princeton University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/093b60fd0557804c8ba0cbf1453da22f-Abstract.html,Security,Coda: An End-to-End Neural Program Decompiler,"Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda1, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into of two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior with performance compared to baseline approaches. We assess Coda’s performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70% program accuracy. Our work reveals the vulnerability of binary executables and imposes a new threat to the protection of Intellectual Property (IP) for software development.",[],[],"['Cheng Fu', 'Huili Chen', 'Haolan Liu', 'Xinyun Chen', 'Yuandong Tian', 'Farinaz Koushanfar', 'Jishen Zhao']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/09a8a8976abcdfdee15128b4cc02f33a-Abstract.html,Security,Linear Stochastic Bandits Under Safety Constraints,"Bandit algorithms have various application in safety-critical systems, where it is important to respect the system constraints that rely on the bandit's unknown parameters at every round. In this paper, we formulate a linear stochastic multi-armed bandit problem with safety constraints that depend (linearly) on an unknown parameter vector. As such, the learner is unable to identify all safe actions and must act conservatively in ensuring that her actions satisfy the safety constraint at all rounds (at least with high probability). For these bandits, we propose a new UCB-based algorithm called Safe-LUCB, which includes necessary modifications to respect safety constraints. The algorithm has two phases. During the pure exploration phase the learner chooses her actions at random from a restricted set of safe actions with the goal of learning a good approximation of the entire unknown safe set. Once this goal is achieved, the algorithm begins a safe exploration-exploitation phase where the learner gradually expands their estimate of the set of safe actions while controlling the growth of regret. We provide a general regret bound for the algorithm, as well as a problem dependent bound that is connected to the location of the optimal action within the safe set. We then propose a modified heuristic that exploits our problem dependent analysis to improve the regret.",[],[],"['Sanae Amani', 'Mahnoosh Alizadeh', 'Christos Thrampoulidis']","['University of California, Santa Barbara', 'University of California, Santa Barbara', 'University of California, Santa Barbara']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/0defd533d51ed0a10c5c9dbf93ee78a5-Abstract.html,Security,Adversarial Robustness through Local Linearization,"Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with L-infinity norm adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.",[],[],"['Chongli Qin', 'James Martens', 'Sven Gowal', 'Dilip Krishnan', 'Krishnamurthy Dvijotham', 'Alhussein Fawzi', 'Soham De', 'Robert Stanforth', 'Pushmeet Kohli']","['DeepMind', 'DeepMind', 'DeepMind', 'Google', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/107878346e1d8f8fe6af7a7a588aa807-Abstract.html,Security,On Robustness to Adversarial Examples and Polynomial Optimization,"We study the design of computationally efficient algorithms with provable guarantees, that are robust to adversarial (test time) perturbations. While there has been an explosion of recent work on this topic due to its connections to test time robustness of deep networks, there is limited theoretical understanding of several basic questions like (i) when and how can one design provably robust learning algorithms? (ii) what is the price of achieving robustness to adversarial examples in a computationally efficient manner?The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples, and a rich class of polynomial optimization problems, thereby making progress on the above questions. In particular, we leverage this connection to (a) design computationally efficient robust algorithms with provable guarantees for a large class of hypothesis, namely linear classifiers and degree-2 polynomial threshold functions~(PTFs), (b) give a precise characterization of the price of achieving robustness in a computationally efficient manner for these classes, (c) design efficient algorithms to certify robustness and generate adversarial attacks in a principled manner for 2-layer neural networks. We empirically demonstrate the effectiveness of these attacks on real data.",[],[],"['Pranjal Awasthi', 'Abhratanu Dutta', 'Aravindan Vijayaraghavan']","['Department of Computer Science, Rutgers University', 'Department of Computer Science, Northwestern University', 'Department of Computer Science, Northwestern University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/1091660f3dff84fd648efe31391c5524-Abstract.html,Security,In-Place Zero-Space Memory Protection for CNN,"Convolutional Neural Networks (CNN) are being actively explored for safety-critical applications such as autonomous vehicles and aerospace, where it is essential to ensure the reliability of inference results in the presence of possible memory faults. Traditional methods such as error correction codes (ECC) and Triple Modular Redundancy (TMR) are CNN-oblivious and incur substantial memory overhead and energy cost. This paper introduces in-place zero-space ECC assisted with a new training scheme weight distribution-oriented training. The new method provides the first known zero space cost memory protection for CNNs without compromising the reliability offered by traditional ECC.",[],[],"['Hui Guan', 'Lin Ning', 'Zhen Lin', 'Xipeng Shen', 'Huiyang Zhou', 'Seung-Hwan Lim']","['North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'North Carolina State University, Raleigh, NC', 'Oak Ridge National Laboratory, Oak Ridge, TN']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/16bda725ae44af3bb9316f416bd13b1b-Abstract.html,Security,Theoretical Analysis of Adversarial Learning: A Minimax Approach,"In this paper, we propose a general theoretical method for analyzing the risk bound in the presence of adversaries. Specifically, we try to fit the adversarial learning problem into the minimax framework. We first show that the original adversarial learning problem can be transformed into a minimax statistical learning problem by introducing a transport map between distributions. Then, we prove a new risk bound for this minimax problem in terms of covering numbers under a weak version of Lipschitz condition. Our method can be applied to multi-class classification and popular loss functions including the hinge loss and ramp loss. As some illustrative examples, we derive the adversarial risk bounds for SVMs and deep neural networks, and our bounds have two data-dependent terms, which can be optimized for achieving adversarial robustness.",[],[],"['Zhuozhuo Tu', 'Jingwei Zhang', 'Dacheng Tao']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html,Security,Calibration tests in multi-class classification: A unifying framework,"In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.",[],[],"['David Widmann', 'Fredrik Lindsten', 'Dave Zachariah']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/246a3c5544feb054f3ea718f61adfa16-Abstract.html,Security,A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks,"Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of neural network verification. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it.",[],[],"['Hadi Salman', 'Greg Yang', 'Huan Zhang', 'Cho-Jui Hsieh', 'Pengchuan Zhang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/2ca65f58e35d9ad45bf7f3ae5cfd08f1-Abstract.html,Security,Model Compression with Adversarial Robustness: A Unified Optimization Framework,"Deep model compression has been extensively studied, and state-of-the-art methods can now achieve high compression ratios with minimal accuracy loss. This paper studies model compression through a different lens: could we compress models without hurting their robustness to adversarial attacks, in addition to maintaining accuracy? Previous literature suggested that the goals of robustness and compactness might sometimes contradict. We propose a novel Adversarially Trained Model Compression (ATMC) framework. ATMC constructs a unified constrained optimization formulation, where existing compression means (pruning, factorization, quantization) are all integrated into the constraints. An efficient algorithm is then developed. An extensive group of experiments are presented, demonstrating that ATMC obtains remarkably more favorable trade-off among model size, accuracy and robustness, over currently available alternatives in various settings. The codes are publicly available at: https://github.com/shupenggui/ATMC.",[],[],"['Shupeng Gui', 'Haotao Wang', 'Haichuan Yang', 'Chen Yu', 'Zhangyang Wang', 'Ji Liu']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html,Security,Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks,"Unlike the white-box counterparts that are widely studied and readily accessible, adversarial examples in black-box settings are generally more Herculean on account of the difficulty of estimating gradients. Many methods achieve the task by issuing numerous queries to target classification systems, which makes the whole procedure costly and suspicious to the systems. In this paper, we aim at reducing the query complexity of black-box attacks in this category. We propose to exploit gradients of a few reference models which arguably span some promising search subspaces. Experimental results show that, in comparison with the state-of-the-arts, our method can gain up to 2x and 4x reductions in the requisite mean and medium numbers of queries with much lower failure rates even if the reference models are trained on a small and inadequate dataset disjoint to the one for training the victim model. Code and models for reproducing our results will be made publicly available.",[],[],"['Yiwen Guo', 'Ziang Yan', 'Changshui Zhang']","['Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University, Beijing, China and Intel Labs China', 'Bytedance AI Lab and Intel Labs China', 'Institute for Artificial Intelligence, Tsinghua University (THUAI), State Key Lab of Intelligent Technologies and Systems, Beijing National Research Center for Information Science and Technology (BNRist), Department of Automation, Tsinghua University, Beijing, China']","['China', 'China', 'China']"
https://papers.nips.cc/paper_files/paper/2019/hash/2d71b2ae158c7c5912cc0bbde2bb9d95-Abstract.html,Security,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning,"We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). The convergence of the proposed method to the asymptotically correct distribution is established under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks (CNN) and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.",[],[],"['Wei Deng', 'Xiao Zhang', 'Faming Liang', 'Guang Lin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/315f006f691ef2e689125614ea22cc61-Abstract.html,Security,Policy Poisoning in Batch Reinforcement Learning and Control,"We study a security threat to batch reinforcement learning and control where the attacker aims to poison the learned policy. The victim is a reinforcement learner / controller which first estimates the dynamics and the rewards from a batch data set, and then solves for the optimal policy with respect to the estimates. The attacker can modify the data set slightly before learning happens, and wants to force the learner into learning a target policy chosen by the attacker. We present a unified framework for solving batch policy poisoning attacks, and instantiate the attack on two standard victims: tabular certainty equivalence learner in reinforcement learning and linear quadratic regulator in control. We show that both instantiation result in a convex optimization problem on which global optimality is guaranteed, and provide analysis on attack feasibility and attack cost. Experiments show the effectiveness of policy poisoning attacks.",[],[],"['Yuzhe Ma', 'Xuezhou Zhang', 'Wen Sun', 'Jerry Zhu']","['University of Wisconsin–Madison', 'University of Wisconsin–Madison', 'Microsoft Research New York', 'University of Wisconsin–Madison']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html,Security,Improving Black-box Adversarial Attacks with a Transfer-based Prior,"We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.",[],[],"['Shuyu Cheng', 'Yinpeng Dong', 'Tianyu Pang', 'Hang Su', 'Jun Zhu']","['Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China', 'Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI, THBI Lab, Tsinghua University, Beijing, China']","['China', 'China', 'China', 'China', 'China']"
https://papers.nips.cc/paper_files/paper/2019/hash/32e0bd1497aa43e02a42f47d9d6515ad-Abstract.html,Security,Unlabeled Data Improves Adversarial Robustness,"We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning.  Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) $\ell_\infty$ robustness against several strong attacks via adversarial training and (ii) certified $\ell_2$ and $\ell_\infty$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.",[],[],"['Yair Carmon', 'Aditi Raghunathan', 'Ludwig Schmidt', 'John C. Duchi', 'Percy S. Liang']","['Stanford University', 'Stanford University', 'UC Berkeley', 'Stanford University', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html,Security,Certified Adversarial Robustness with Additive Noise,"The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defense models has been developed, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that our method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.",[],[],"['Bai Li', 'Changyou Chen', 'Wenlin Wang', 'Lawrence Carin']","['Department of Statistical Science, Duke University', 'Department of CSE, University at Buffalo, SUNY', 'Department of ECE, Duke University', 'Department of ECE, Duke University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/351869bde8b9d6ad1e3090bd173f600d-Abstract.html,Security,Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices,"In real-world machine learning applications, data subsets correspond to especially critical outcomes: vulnerable cyclist detections are safety-critical in an autonomous driving task, and ""question"" sentences might be important to a dialogue agent's language understanding for product purposes.  While machine learning models can achieve quality performance on coarse-grained metrics like F1-score and overall accuracy, they may underperform on these critical subsets---we define these as slices, the key abstraction in our approach. To address slice-level performance, practitioners often train separate ""expert"" models on slice subsets or use multi-task hard parameter sharing.  We propose Slice-based Learning, a new programming model in which the slicing function (SF), a programmer abstraction, is used to specify additional model capacity for each slice.  Any model can leverage SFs to learn slice-specific representations, which are combined with an attention mechanism to make slice-aware predictions.  We show that our approach improves over baselines in terms of computational complexity and slice-specific performance by up to 19.0 points, and overall performance by up to 4.6 F1 points on applications spanning natural language understanding and computer vision benchmarks as well as production-scale industrial systems.",[],[],"['Vincent Chen', 'Sen Wu', 'Alexander J. Ratner', 'Jen Weng', 'Christopher Ré']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/36ab62655fa81ce8735ce7cfdaf7c9e8-Abstract.html,Security,Theoretical evidence for adversarial robustness through randomization,"This paper investigates the theory of robustness against adversarial attacks. Itfocuses on the family of randomization techniques that consist in injecting noisein the network at inference time. These techniques have proven effective in manycontexts, but lack theoretical arguments. We close this gap by presenting a theo-retical analysis of these approaches, hence explaining why they perform well inpractice. More precisely, we make two new contributions. The first one relatesthe randomization rate to robustness to adversarial attacks. This result applies forthe general family of exponential distributions, and thus extends and unifies theprevious approaches. The second contribution consists in devising a new upperbound on the adversarial risk gap of randomized neural networks. We support ourtheoretical claims with a set of experiments.",[],[],"['Rafael Pinot', 'Laurent Meunier', 'Alexandre Araujo', 'Hisashi Kashima', 'Florian Yger', 'Cedric Gouy-Pailler', 'Jamal Atif']","['Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France and nstitut LIST, CEA, Université Paris-Saclay', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France and Facebook AI Research, Paris, France', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France and Wavestone, Paris, France', 'Kyoto University, Kyoto, Japan and RIKEN Center for AIP, Japan', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France', 'Institut LIST, CEA, Université Paris-Saclay', 'Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, Paris, France']","['France', 'France', 'France', 'Japan', 'France', 'France']"
https://papers.nips.cc/paper_files/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html,Security,Defending Against Neural Fake News,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",[],[],"['Rowan Zellers', 'Ari Holtzman', 'Hannah Rashkin', 'Yonatan Bisk', 'Ali Farhadi', 'Franziska Roesner', 'Yejin Choi']","['Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington and Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Paul G. Allen School of Computer Science & Engineering, University of Washington and Allen Institute for Artificial Intelligence']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/4206e38996fae4028a26d43b24f68d32-Abstract.html,Security,Provably robust boosted decision stumps and trees against adversarial attacks,"The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per input, where $T$ is the number of decision stumps and the optimal update step of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5\% for $\epsilon_\infty=0.3$), FMNIST (23.2\% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7\% for $\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at \url{http://github.com/max-andr/provably-robust-boosting}.",[],[],"['Maksym Andriushchenko', 'Matthias Hein']","['University of Tübingen', 'University of Tübingen']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/47d1e990583c9c67424d369f3414728e-Abstract.html,Security,Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection,"In this paper, we aim to understand the generalization properties of generative adversarial networks (GANs) from a new perspective of privacy protection. Theoretically, we prove that a differentially private learning algorithm used for training the GAN does not overfit to a certain degree, i.e., the generalization gap can be bounded. Moreover, some recent works, such as the Bayesian GAN, can be re-interpreted based on our theoretical insight from privacy protection. Quantitatively, to evaluate the information leakage of well-trained GAN models, we perform various membership attacks on these models. The results show that previous Lipschitz regularization techniques are effective in not only reducing the generalization gap but also alleviating the information leakage of the training dataset.",[],[],"['Bingzhe Wu', 'Shiwan Zhao', 'Chaochao Chen', 'Haoyang Xu', 'Li Wang', 'Xiaolu Zhang', 'Guangyu Sun', 'Jun Zhou']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html,Security,Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks,"Vulnerability identification is crucial to protect the software systems from attacksfor cyber security. It is especially important to localize the vulnerable functionsamong the source code to facilitate the fix. However, it is a challenging and tediousprocess, and also requires specialized security expertise. Inspired by the workon manually-defined patterns of vulnerabilities from various code representationgraphs and the recent advance on graph neural networks, we propose Devign, ageneral graph neural network based model for graph-level classification throughlearning on a rich set of code semantic representations. It includes a novel Convmodule to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.",[],[],"['Yaqin Zhou', 'Shangqing Liu', 'Jingkai Siow', 'Xiaoning Du', 'Yang Liu']","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/4f398cb9d6bc79ae567298335b51ba8a-Abstract.html,Security,Safe Exploration for Interactive Machine Learning,"In interactive machine learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on real-world systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.",[],[],"['Matteo Turchetta', 'Felix Berkenkamp', 'Andreas Krause']","['Dept. of Computer Science, ETH Zurich', 'Dept. of Computer Science, ETH Zurich', 'Dept. of Computer Science, ETH Zurich']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/4fc848051e4459b8a6afeb210c3664ec-Abstract.html,Security,Learning from Label Proportions with Generative Adversarial Networks,"In this paper, we leverage generative adversarial networks (GANs) to derive an effective algorithm LLP-GAN for learning from label proportions (LLP), where only the bag-level proportional information in labels is available. Endowed with end-to-end structure, LLP-GAN performs approximation in the light of an adversarial learning mechanism, without imposing restricted assumptions on distribution. Accordingly, we can directly induce the final instance-level classifier upon the discriminator. Under mild assumptions, we give the explicit generative representation and prove the global optimality for LLP-GAN. Additionally, compared with existing methods, our work empowers LLP solver with capable scalability inheriting from deep models. Several experiments on benchmark datasets demonstrate vivid advantages of the proposed approach.",[],[],"['Jiabin Liu', 'Bo Wang', 'Zhiquan Qi', 'YingJie Tian', 'Yong Shi']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/4fe5149039b52765bde64beb9f674940-Abstract.html,Security,Budgeted Reinforcement Learning in Continuous State Space,"A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of an upper bound on a constrains violation signal that -- importantly -- can be modified in real-time. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is the fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving.",[],[],"['Nicolas Carrara', 'Edouard Leurent', 'Romain Laroche', 'Tanguy Urvoy', 'Odalric-Ambrym Maillard', 'Olivier Pietquin']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/54e36c5ff5f6a1802925ca009f3ebb68-Abstract.html,Security,Real-Time Reinforcement Learning,"Markov Decision Processes (MDPs), the mathematical framework underlying most algorithms in Reinforcement Learning (RL), are often used in a way that wrongfully assumes that the state of an agent's environment does not change during action selection. As RL systems based on MDPs begin to find application in real-world safety critical situations, this mismatch between the assumptions underlying classical MDPs and the reality of real-time computation may lead to undesirable outcomes. In this paper, we introduce a new framework, in which states and actions evolve simultaneously and show how it is related to the classical MDP formulation. We analyze existing algorithms under the new real-time formulation and show why they are suboptimal when used in real-time. We then use those insights to create a new algorithm Real-Time Actor Critic (RTAC) that outperforms the existing state-of-the-art continuous control algorithm Soft Actor Critic both in real-time and non-real-time settings.",[],[],"['Simon Ramstedt', 'Chris Pal']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/55a988dfb00a914717b3000a3374694c-Abstract.html,Security,Disentangling Influence: Using disentangled representations to audit model predictions,"Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled influence audits, a procedure to audit the indirect influence of features. Specifically, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classifier being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature influence.",[],[],"['Charles Marx', 'Richard Phillips', 'Sorelle Friedler', 'Carlos Scheidegger', 'Suresh Venkatasubramanian']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/56a3107cad6611c8337ee36d178ca129-Abstract.html,Security,SHE: A Fast and Accurate Deep Neural Network for Encrypted Data,"Homomorphic Encryption (HE) is one of the most promising security solutions to emerging Machine Learning as a Service (MLaaS). Several Leveled-HE (LHE)-enabled Convolutional Neural Networks (LHECNNs) are proposed to implement MLaaS to avoid the large bootstrapping overhead. However, prior LHECNNs have to pay significant computational overhead but achieve only low inference accuracy, due to their polynomial approximation activations and poolings. Stacking many polynomial approximation activation layers in a network greatly reduces the inference accuracy, since the polynomial approximation activation errors lead to a low distortion of the output distribution of the next batch normalization layer. So the polynomial approximation activations and poolings have become the obstacle to a fast and accurate LHECNN model.In this paper, we propose a Shift-accumulation-based LHE-enabled deep neural network (SHE) for fast and accurate inferences on encrypted data. We use the binary-operation-friendly leveled-TFHE (LTFHE) encryption scheme to implement ReLU activations and max poolings. We also adopt the logarithmic quantization to accelerate inferences by replacing expensive LTFHE multiplications with cheap LTFHE shifts. We propose a mixed bitwidth accumulator to expedite accumulations. Since the LTFHE ReLU activations, max poolings, shifts and accumulations have small multiplicative depth, SHE can implement much deeper network architectures with more convolutional and activation layers. Our experimental results show SHE achieves the state-of-the-art inference accuracy and reduces the inference latency by 76.21% ~ 94.23% over prior LHECNNs on MNIST and CIFAR-10.",[],[],"['Qian Lou', 'Lei Jiang']","['Indiana University Bloomington', 'Indiana University Bloomington']","['India', 'India']"
https://papers.nips.cc/paper_files/paper/2019/hash/56bd37d3a2fda0f2f41925019c81011d-Abstract.html,Security,Non-Cooperative Inverse Reinforcement Learning,"Making decisions in the presence of a strategic opponent requires one to take into account the opponent’s ability to actively mask its intended objective. To describe such strategic situations, we introduce the non-cooperative inverse reinforcement learning (N-CIRL) formalism. The N-CIRL formalism consists of two agents with completely misaligned objectives, where only one of the agents knows the true objective function. Formally, we model the N-CIRL formalism as a zero-sum Markov game with one-sided incomplete information. Through interacting with the more informed player, the less informed player attempts to both infer and optimize the true objective function. As a result of the one-sided incomplete information, the multi-stage game can be decomposed into a sequence of single- stage games expressed by a recursive formula. Solving this recursive formula yields the value of the N-CIRL game and the more informed player’s equilibrium strategy. Another recursive formula, constructed by forming an auxiliary game, termed the dual game, yields the less informed player’s strategy. Building upon these two recursive formulas, we develop a computationally tractable algorithm to approximately solve for the equilibrium strategies. Finally, we demonstrate the benefits of our N-CIRL formalism over the existing multi-agent IRL formalism via extensive numerical simulation in a novel cyber security setting.",[],[],"['Xiangyuan Zhang', 'Kaiqing Zhang', 'Erik Miehling', 'Tamer Basar']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/576d026223582a390cd323bef4bad026-Abstract.html,Security,ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization,"The adaptive momentum method (AdaMM), which uses past gradients to update descent directions and learning rates simultaneously, has become one of the most popular first-order optimization methods for solving machine learning  problems. However,  AdaMM is not suited for solving black-box optimization problems, where explicit gradient forms are difficult or infeasible to obtain. In this paper, we propose a zeroth-order  AdaMM (ZO-AdaMM) algorithm, that generalizes AdaMM to the gradient-free regime. We show that the convergence rate of ZO-AdaMM for  both  convex and nonconvex optimization is roughly a factor of $O(\sqrt{d})$ worse than that of the first-order AdaMM algorithm, where $d$ is problem size. In particular, we provide a deep understanding on why  Mahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct, our analysis   makes the first step toward understanding adaptive learning rate methods for nonconvex constrained optimization.Furthermore, we demonstrate two applications, designing  per-image and universal adversarial attacks from black-box neural networks, respectively. We perform extensive experiments on ImageNet and empirically show that  ZO-AdaMM converges much faster to a solution of high accuracy compared with  $6$ state-of-the-art ZO optimization methods.",[],[],"['Xiangyi Chen', 'Sijia Liu', 'Kaidi Xu', 'Xingguo Li', 'Xue Lin', 'Mingyi Hong', 'David Cox']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/5cde6dedeb8892e3794f22db57ada073-Abstract.html,Security,A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning,"In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases --- poisoning regression tasks under $\ell_2$-norm constraint and classification tasks under $\ell_0$-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50\% error).",[],[],"['Xuanqing Liu', 'Si Si', 'Jerry Zhu', 'Yang Li', 'Cho-Jui Hsieh']","['Department of Computer Science, UCLA', 'Google Research', 'Department of Computer Science, University of Wisconsin-Madison', 'Google Research', 'Department of Computer Science, UCLA']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/5d4ae76f053f8f2516ad12961ef7fe97-Abstract.html,Security,Adversarial Training and Robustness for Multiple Perturbations,"Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small $\ell_\infty$-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model's vulnerability.Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types.We prove that a trade-off in robustness to different types of $\ell_p$-bounded and spatial perturbations must exist in a natural and simple statistical setting.We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. We propose new multi-perturbation adversarial training schemes, as well as an efficient attack for the $\ell_1$-norm, and use these to show that models trained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually. In particular, we find that adversarial training with first-order $\ell_\infty, \ell_1$ and $\ell_2$ attacks on MNIST achieves merely $50\%$ robust accuracy, partly because of gradient-masking.Finally, we propose affine attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models.",[],[],"['Florian Tramer', 'Dan Boneh']","['Stanford University', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html,Security,Deep Leakage from Gradients,"Passing gradient is a widely used scheme in  modern multi-node learning system (e.g, distributed training, collaborative learning). In a long time, people used to believe that gradients are safe to share: i.e, the training set will not be leaked by gradient sharing.  However, in this paper, we show that we can obtain the private training set from the publicly shared gradients.  The leaking only takes few gradient steps to process and can obtain the original training set instead of look-alike alternatives.  We name this leakage as \textit{deep leakage from gradient}  and practically validate the effectiveness of our algorithm on both computer vision and natural language processing tasks. We empirically show that our attack is much stronger than previous approaches and thereby and raise people's awareness to rethink the gradients' safety. We also discuss some possible strategies to defend this deep leakage.",[],[],"['Ligeng Zhu', 'Zhijian Liu', 'Song Han']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html,Security,Deliberative Explanations: visualizing network insecurities,"A new approach to explainable AI, denoted {\it deliberative explanations,\/}  is proposed. Deliberative explanations are a visualization technique  that aims to go beyond the simple visualization of the image regions  (or, more generally, input variables) responsible for a network  prediction. Instead, they aim to expose the deliberations carried  by the network to arrive at that prediction, by uncovering the  insecurities of the network about the latter. The  explanation consists of a list of insecurities, each composed of  1) an image region (more generally, a set of input variables), and 2)  an ambiguity formed by the pair of classes responsible for the network  uncertainty about the region. Since insecurity detection requires  quantifying the difficulty of network predictions, deliberative  explanations combine ideas from the literatures on visual explanations and  assessment of classification difficulty. More specifically,  the proposed implementation  combines attributions with respect to both class  predictions and a difficulty score.  An evaluation protocol that leverages object recognition (CUB200)  and scene classification (ADE20K) datasets that combine part and  attribute annotations is also introduced to evaluate the accuracy of  deliberative explanations. Finally, an experimental evaluation shows that  the most accurate explanations are achieved by combining non self-referential  difficulty scores and second-order attributions. The resulting  insecurities are shown to correlate with regions of attributes that  are shared by different classes. Since these regions are also ambiguous  for humans, deliberative explanations are intuitive, suggesting that  the deliberative process of modern networks correlates with human  reasoning.",[],[],"['Pei Wang', 'Nuno Nvasconcelos']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6a4d5952d4c018a1c1af9fa590a10dda-Abstract.html,Security,DM2C: Deep Mixed-Modal Clustering,"Data exhibited with multiple modalities are ubiquitous in real-world clustering tasks. Most existing methods, however, pose a strong assumption that the pairing information for modalities is available for all instances. In this paper, we consider a more challenging task where each instance is represented in only one modality, which we call mixed-modal data. Without any extra pairing supervision across modalities, it is difficult to find a universal semantic space for all of them. To tackle this problem, we present an adversarial learning framework for clustering with mixed-modal data. Instead of transforming all the samples into a joint modality-independent space, our framework learns the mappings across individual modal spaces by virtue of cycle-consistency. Through these mappings, we could easily unify all the samples into a single modal space and perform the clustering. Evaluations on several real-world mixed-modal datasets could demonstrate the superiority of our proposed framework.",[],[],"['Yangbangyan Jiang', 'Qianqian Xu', 'Zhiyong Yang', 'Xiaochun Cao', 'Qingming Huang']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/6e923226e43cd6fac7cfe1e13ad000ac-Abstract.html,Security,Functional Adversarial Attacks,"We propose functional adversarial attacks, a novel class of threat models for crafting adversarial examples to fool machine learning models. Unlike a standard lp-ball threat model, a functional adversarial threat model allows only a single function to be used to perturb input features to produce an adversarial example. For example, a functional adversarial attack applied on colors of an image can change all red pixels simultaneously to light red. Such global uniform changes in images can be less perceptible than perturbing pixels of the image individually. For simplicity, we refer to functional adversarial attacks on image colors as ReColorAdv, which is the main focus of our experiments. We show that functional threat models can be combined with existing additive (lp) threat models to generate stronger threat models that allow both small, individual perturbations and large, uniform changes to an input. Moreover, we prove that such combinations encompass perturbations that would not be allowed in either constituent threat model. In practice, ReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on CIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with other attacks leads to the strongest existing attack even after adversarial training.",[],[],"['Cassidy Laidlaw', 'Soheil Feizi']","['University of Maryland', 'University of Maryland']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/70117ee3c0b15a2950f1e82a215e812b-Abstract.html,Security,Learning from brains how to regularize machines,"Despite impressive performance on numerous visual tasks, Convolutional Neural Networks (CNNs) --- unlike brains --- are often highly sensitive to small perturbations of their input, e.g. adversarial noise leading to erroneous decisions. We propose to regularize CNNs using large-scale neuroscience data to learn more robust neural features in terms of representational similarity. We presented natural images to mice and measured the responses of thousands of neurons from cortical visual areas. Next, we denoised the notoriously variable neural activity using strong predictive models trained on this large corpus of responses from the mouse visual system, and calculated the representational similarity for millions of pairs of images from the model's predictions. We then used the neural representation similarity to regularize CNNs trained on image classification by penalizing intermediate representations that deviated from neural ones. This preserved performance of baseline models when classifying images under standard benchmarks, while maintaining substantially higher performance compared to baseline or control models when classifying noisy images. Moreover, the models regularized with cortical representations also improved model robustness in terms of adversarial attacks. This demonstrates that regularizing with neural data can be an effective tool to create an inductive bias towards more robust inference.",[],[],"['Zhe Li', 'Wieland Brendel', 'Edgar Walker', 'Erick Cobos', 'Taliah Muhammad', 'Jacob Reimer', 'Matthias Bethge', 'Fabian Sinz', 'Zachary Pitkow', 'Andreas Tolias']","['Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Centre for Integrative Neuroscience, University of Tübingen and Bernstein Center for Computational Neuroscience, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Institute Bioinformatics and Medical Informatics, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine', 'Centre for Integrative Neuroscience, University of Tübingen and Bernstein Center for Computational Neuroscience, University of Tübingen and Institute for Theoretical Physics, University of Tübingen', 'Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Bernstein Center for Computational Neuroscience, University of Tübingen and Institute Bioinformatics and Medical Informatics, University of Tübingen', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Department of Electrical and Computer Engineering, Rice University', 'Department of Neuroscience, Baylor College of Medicine and Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine and Department of Electrical and Computer Engineering, Rice University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/7503cfacd12053d309b6bed5c89de212-Abstract.html,Security,Adversarial training for free!,"Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our ""free"" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks.",[],[],"['Ali Shafahi', 'Mahyar Najibi', 'Mohammad Amin Ghiasi', 'Zheng Xu', 'John Dickerson', 'Christoph Studer', 'Larry S. Davis', 'Gavin Taylor', 'Tom Goldstein']","['University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'Cornell University', 'University of Maryland', 'United States Naval Academy', 'University of Maryland']",['United States']
https://papers.nips.cc/paper_files/paper/2019/hash/75455e062929d32a333868084286bb68-Abstract.html,Security,Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks,"With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR",[],[],"['Lixin Fan', 'Kam Woh Ng', 'Chee Seng Chan']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/78211247db84d96acf4e00092a7fba80-Abstract.html,Security,Defending Neural Backdoors via Generative Distribution Modeling,"Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key of effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA) for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",[],[],"['Ximing Qiao', 'Yukun Yang', 'Hai Li']","['ECE Department, Duke University, Durham, NC', 'ECE Department, Duke University, Durham, NC', 'ECE Department, Duke University, Durham, NC']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/7dd2ae7db7d18ee7c9425e38df1af5e2-Abstract.html,Security,Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness,"Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently emulate an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the reverse KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful adaptive whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.",[],[],"['Andrey Malinin', 'Mark Gales']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8133415ea4647b6345849fb38311cf32-Abstract.html,Security,On the Hardness of Robust Classification,"It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks.  In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity.  In particular, our definition of robust learnability requires polynomial sample complexity.  We start with two negative results.  We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit.  We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb $\omega(\log n)$ input bits. However if the adversary is restricted to perturbing $O(\log n)$ bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework.",[],[],"['Pascale Gourdeau', 'Varun Kanade', 'Marta Kwiatkowska', 'James Worrell']","['Department of Computer Science, University of Oxford, Oxford, UK', 'Department of Computer Science, University of Oxford, Oxford, UK', 'Department of Computer Science, University of Oxford, Oxford, UK', 'Department of Computer Science, University of Oxford, Oxford, UK']","['UK', 'UK', 'UK', 'UK']"
https://papers.nips.cc/paper_files/paper/2019/hash/8420d359404024567b5aefda1231af24-Abstract.html,Security,On Single Source Robustness in Deep Fusion Models,"Algorithms that fuse multiple input sources benefit from both complementary and shared information. Shared information may provide robustness against faulty or noisy inputs, which is indispensable for safety-critical applications like self-driving cars. We investigate learning fusion algorithms that are robust against noise added to a single source. We first demonstrate that robustness against single source noise is not guaranteed in a linear fusion model. Motivated by this discovery, two possible approaches are proposed to increase robustness: a carefully designed loss with corresponding training algorithms for deep fusion models, and a simple convolutional fusion layer that has a structural advantage in dealing with noise. Experimental results show that both training algorithms and our fusion layer make a deep fusion-based 3D object detector robust against noise applied to a single source, while preserving the original performance on clean data.",[],[],"['Taewan Kim', 'Joydeep Ghosh']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/873be0705c80679f2c71fbf4d872df59-Abstract.html,Security,Reinforcement Learning with Convex Constraints,"In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks: specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms do not incorporate, such as diversity.",[],[],"['Sobhan Miryoosefi', 'Kianté Brantley', 'Hal Daume III', 'Miro Dudik', 'Robert E. Schapire']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/885fe656777008c335ac96072a45be15-Abstract.html,Security,"Accurate, reliable and fast robustness evaluation","Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and Linf in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.",[],[],"['Wieland Brendel', 'Jonas Rauber', 'Matthias Kümmerer', 'Ivan Ustyuzhaninov', 'Matthias Bethge']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/8ba6c657b03fc7c8dd4dff8e45defcd2-Abstract.html,Security,Sequential Experimental Design for Transductive Linear Bandits,"In this paper we introduce the pure exploration transductive linear bandit problem: given a set of measurement vectors $\mathcal{X}\subset \mathbb{R}^d$, a set of items $\mathcal{Z}\subset \mathbb{R}^d$, a fixed confidence $\delta$, and an unknown vector $\theta^{\ast}\in \mathbb{R}^d$, the goal is to infer $\arg\max_{z\in \mathcal{Z}} z^\top\theta^\ast$ with probability $1-\delta$ by making as few sequentially chosen noisy measurements of the form $x^\top\theta^{\ast}$ as possible. When $\mathcal{X}=\mathcal{Z}$, this setting generalizes linear bandits, and when $\mathcal{X}$ is the standard basis vectors and $\mathcal{Z}\subset \{0,1\}^d$, combinatorial bandits. The transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages $\mathcal{X}$ a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages $\mathcal{Z}$ that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books $\mathcal{X}$ a user is queried about may be restricted to known best-sellers even though the goal might be to recommend more esoteric titles $\mathcal{Z}$. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we present the first non-asymptotic algorithm for linear bandits that nearly achieves the information-theoretic lower bound.",[],[],"['Tanner Fiez', 'Lalit Jain', 'Kevin G. Jamieson', 'Lillian Ratliff']","['Electrical & Computer Engineering, University of Washington', 'Allen School of Computer Science & Engineering, University of Washington', 'Allen School of Computer Science & Engineering, University of Washington', 'Electrical & Computer Engineering, University of Washington']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/99cd3843754d20ec3c5885d805db8a32-Abstract.html,Security,Cross-Domain Transferability of Adversarial Perturbations,"Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as $\sim$99\% ($\ell_{\infty} \le 10$). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods.",[],[],"['Muhammad Muzammal Naseer', 'Salman H. Khan', 'Muhammad Haris Khan', 'Fahad Shahbaz Khan', 'Fatih Porikli']","['Australian National University, Canberra, Australia and Inception Institute of Artificial Intelligence, Abu Dhabi, UAE', 'Inception Institute of Artificial Intelligence, Abu Dhabi, UAE and Australian National University, Canberra, Australia', 'Inception Institute of Artificial Intelligence, Abu Dhabi, UAE', 'Inception Institute of Artificial Intelligence, Abu Dhabi, UAE and CVL, Department of Electrical Engineering, Linköping University, Sweden', 'Australian National University, Canberra, Australia']","['Australia', 'Australia', 'Sweden', 'Australia']"
https://papers.nips.cc/paper_files/paper/2019/hash/b20bb95ab626d93fd976af958fbc61ba-Abstract.html,Security,Controlling Neural Level Sets,"The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning.In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest.We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.",[],[],"['Matan Atzmon', 'Niv Haim', 'Lior Yariv', 'Ofer Israelov', 'Haggai Maron', 'Yaron Lipman']","['Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel', 'Weizmann Institute of Science, Rehovot, Israel']","['Israel', 'Israel', 'Israel', 'Israel', 'Israel', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/b83aac23b9528732c23cc7352950e880-Abstract.html,Security,Adversarial Self-Defense for Cycle-Consistent GANs,"The goal of unsupervised image-to-image translation is to  map images from one domain to another without the ground truth correspondence between the two domains. State-of-art methods  learn the correspondence using large numbers of unpaired examples from both domains and are based on generative adversarial networks. In order to preserve the semantics of the input image, the adversarial objective is usually combined with a cycle-consistency loss that penalizes incorrect reconstruction of the input image from the translated one. However, if the target mapping is many-to-one, e.g. aerial photos to maps, such a restriction forces the generator to hide information in low-amplitude structured noise that is undetectable by human eye or by the discriminator. In this paper, we show how such self-attacking behavior of unsupervised translation methods affects their performance and provide two defense techniques. We perform a quantitative evaluation of the proposed techniques and show that making the translation model more robust to the self-adversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to low-amplitude perturbations. Our project page can be found at ai.bu.edu/selfadv.",[],[],"['Dina Bashkirova', 'Ben Usman', 'Kate Saenko']","['Boston University', 'Boston University', 'Boston University and MIT-IBM Watson AI Lab']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/b994697479c5716eda77e8e9713e5f0f-Abstract.html,Security,Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs,"Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.",[],[],"['Marek Petrik', 'Reazul Hasan Russel']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/bc1ad6e8f86c42a371aff945535baebb-Abstract.html,Security,Attribution-Based Confidence Metric For Deep Neural Networks,"We propose a novel confidence metric, namely, attribution-based confidence (ABC) for deep neural networks (DNNs).  ABC metric characterizes whether the output of a DNN on an input can be trusted. DNNs are known to be brittle on inputs outside the training distribution and are, hence, susceptible to adversarial attacks. This fragility is compounded by a lack of effectively computable measures of model confidence that correlate well with the accuracy of DNNs. These factors have impeded the adoption of DNNs in high-assurance systems. The proposed ABC metric addresses these challenges. It does not require access to the training data, the use of ensembles, or the need to train a  calibration model on a held-out validation set. Hence, the new metric is usable even when only a trained model is available for inference. We mathematically motivate the proposed metric and evaluate its effectiveness with two sets of experiments.  First, we study the change in accuracy and the associated confidence over out-of-distribution inputs.  Second, we consider several digital and physically realizable attacks such as FGSM, CW, DeepFool, PGD, and adversarial patch generation methods. The ABC metric is low on out-of-distribution data and adversarial examples, where the accuracy of the model is also low. These experiments demonstrate the effectiveness of the ABC  metric to make DNNs more trustworthy and resilient.",[],[],"['Susmit Jha', 'Sunny Raj', 'Steven Fernandes', 'Sumit K. Jha', 'Somesh Jha', 'Brian Jalaian', 'Gunjan Verma', 'Ananthram Swami']","['Computer Science Laboratory, SRI International', 'Computer Science Department, University of Central Florida, Orlando', 'Computer Science Department, University of Central Florida, Orlando', 'Computer Science Department, University of Central Florida, Orlando', 'University of Wisconsin-Madison and Xaipient', 'US Army Research Laboratory, Adelphi', 'US Army Research Laboratory, Adelphi', 'US Army Research Laboratory, Adelphi']","['US', 'US', 'US']"
https://papers.nips.cc/paper_files/paper/2019/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,Security,Metric Learning for Adversarial Robustness,"Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and find that the attack causes the internal representation to shift closer to the ``false'' class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classifiers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4% and detection efficiency by up to 6% according to Area Under Curve score over prior work. The code of our work is available at https://github.com/columbia/MetricLearningAdversarial_Robustness.",[],[],"['Chengzhi Mao', 'Ziyuan Zhong', 'Junfeng Yang', 'Carl Vondrick', 'Baishakhi Ray']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c36b1132ac829ece87dda55d77ac06a4-Abstract.html,Security,Online Learning via the Differential Privacy Lens,"In this paper, we use differential privacy as a lens to examine online learning in both full and partial information settings. The differential privacy framework is, at heart, less about privacy and more about algorithmic stability, and thus has found application in domains well beyond those where information security is central. Here we develop an algorithmic property called one-step differential stability which facilitates a more refined regret analysis for online learning methods. We show that tools from the differential privacy literature can yield regret bounds for many interesting online learning problems including online convex optimization and online linear optimization. Our stability notion is particularly well-suited for deriving first-order regret bounds for follow-the-perturbed-leader algorithms, something that all previous analyses have struggled to achieve. We also generalize the standard max-divergence to obtain a broader class called Tsallis max-divergences. These define stronger notions of stability that are useful in deriving bounds in partial information settings such as multi-armed bandits and bandits with experts.",[],[],"['Jacob D. Abernethy', 'Young Hun Jung', 'Chansoo Lee', 'Audra McMillan', 'Ambuj Tewari']","['College of Computing, Georgia Institute of Technology', 'Department of Statistics, University of Michigan', 'Google Brain', 'Simons Inst. for the Theory of Computing, Department of Computer Science, Boston University, Khoury College of Computer Sciences, Northeastern University', 'Department of Statistics, Department of EECS, University of Michigan']",['Georgia']
https://papers.nips.cc/paper_files/paper/2019/hash/c46482dd5d39742f0bfd417b492d0e8e-Abstract.html,Security,Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning,"Generalized zero-shot learning (GZSL) is a challenging class of vision and knowledge transfer problems in which both seen and unseen classes appear during testing. Existing GZSL approaches either suffer from semantic loss and discard discriminative information at the embedding stage, or cannot guarantee the visual-semantic interactions. To address these limitations, we propose a Dual Adversarial Semantics-Consistent Network (referred to as DASCN), which learns both primal and dual Generative Adversarial Networks (GANs) in a unified framework for GZSL. In DASCN, the primal GAN learns to synthesize inter-class discriminative and semantics-preserving visual features from both the semantic representations of seen/unseen classes and the ones reconstructed by the dual GAN. The dual GAN enforces the synthetic visual features to represent prior semantic knowledge well via semantics-consistent adversarial learning. To the best of our knowledge, this is the first work that employs a novel dual-GAN mechanism for GZSL. Extensive experiments show that our approach achieves significant improvements over the state-of-the-art approaches.",[],[],"['Jian Ni', 'Shanghang Zhang', 'Haiyong Xie']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/c4819d06b0ca810d38506453cfaae9d8-Abstract.html,Security,Manipulating a Learning Defender and Ways to Counteract,"In Stackelberg security games when information about the attacker's payoffs is uncertain, algorithms have been proposed to learn the optimal defender commitment by interacting with the attacker and observing their best responses. In this paper, we show that, however, these algorithms can be easily manipulated if the attacker responds untruthfully. As a key finding, attacker manipulation normally leads to the defender learning a maximin strategy, which effectively renders the learning attempt meaningless as to compute a maximin strategy requires no additional information about the other player at all. We then apply a game-theoretic framework at a higher level to counteract such manipulation, in which the defender commits to a policy that specifies her strategy commitment according to the learned information. We provide a polynomial-time algorithm to compute the optimal such policy, and in addition, a heuristic approach that applies even when the attacker's payoff space is infinite or completely unknown. Empirical evaluation shows that our approaches can improve the defender's utility significantly as compared to the situation when attacker manipulation is ignored.",[],[],"['Jiarui Gan', 'Qingyu Guo', 'Long Tran-Thanh', 'Bo An', 'Michael Wooldridge']","['University of Oxford, Oxford, UK', 'Nanyang Technological University, Singapore', 'University of Southampton, Southampton, UK', 'Nanyang Technological University, Singapore', 'University of Oxford, Oxford, UK']","['UK', 'Singapore', 'UK', 'Singapore', 'UK']"
https://papers.nips.cc/paper_files/paper/2019/hash/cbb6a3b884f4f88b3a8e3d44c636cbd8-Abstract.html,Security,A New Defense Against Adversarial Images: Turning a Weakness into a Strength,"Natural images are virtually surrounded by low-density misclassified regions that can be efficiently discovered by gradient-guided search --- enabling the generation of adversarial images. While many techniques for detecting these attacks have been proposed, they are easily bypassed when the adversary has full knowledge of the detection mechanism and adapts the attack strategy accordingly. In this paper, we adopt a novel perspective and regard the omnipresence of adversarial perturbations as a strength rather than a weakness. We postulate that if an image has been tampered with, these adversarial directions either become harder to find with gradient methods or have substantially higher density than for natural images. We develop a practical test for this signature characteristic to successfully detect adversarial attacks, achieving unprecedented accuracy under the white-box setting where the adversary is given full knowledge of our detection mechanism.",[],[],"['Shengyuan Hu', 'Tao Yu', 'Chuan Guo', 'Wei-Lun Chao', 'Kilian Q. Weinberger']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/cd61a580392a70389e27b0bc2b439f49-Abstract.html,Security,Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks,"Modern machine learning systems are susceptible to adversarial examples; inputswhich clearly preserve the characteristic semantics of a given class, but whoseclassification is (usually confidently) incorrect. Existing approaches to adversarialdefense generally rely on modifying the input, e.g. quantization, or the learnedmodel parameters, e.g. via adversarial training. However, recent research hasshown that most such approaches succumb to adversarial examples when different norms or more sophisticated adaptive attacks are considered. In this paper, we propose a fundamentally different approach which instead changes the way the output is represented and decoded. This simple approach achieves state-of-the-art robustness to adversarial examples for L 2 and L ∞ based adversarial perturbations on MNIST and CIFAR10. In addition, even under strong white-box attacks, we find that our model often assigns adversarial examples a low probability; those with high probability are usually interpretable, i.e. perturbed towards the perceptual boundary between the original and adversarial class. Our approach has several advantages: it yields more meaningful probability estimates, is extremely fast during training and testing, requires essentially no architectural changes to existing discriminative learning pipelines, is wholly complementary to other defense approaches including adversarial training, and does not sacrifice benign test set performance",[],[],"['Gunjan Verma', 'Ananthram Swami']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html,Security,Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks,"Predicting the future trajectories of multiple interacting pedestrians in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory which is noticeably influenced by the intricate social interactions. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions for multiple pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.",[],[],"['Vineet Kosaraju', 'Amir Sadeghian', 'Roberto Martín-Martín', 'Ian Reid', 'Hamid Rezatofighi', 'Silvio Savarese']","['Stanford University', 'Stanford University and Aibee Inc', 'Stanford University', 'University of Adelaide', 'Stanford University and University of Adelaide', 'Stanford University']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/d18f655c3fce66ca401d5f38b48c89af-Abstract.html,Security,"Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge","Text-to-image generation, i.e. generating an image given a text description, is a very challenging task due to the significant semantic gap between the two domains. Humans, however, tackle this problem intelligently. We learn from diverse objects to form a solid prior about semantics, textures, colors, shapes, and layouts. Given a text description, we immediately imagine an overall visual impression using this prior and, based on this, we draw a picture by progressively adding more and more details. In this paper, and inspired by this process, we propose a novel text-to-image method called LeicaGAN to combine the above three phases in a unified framework. First, we formulate the multiple priors learning phase as a textual-visual co-embedding (TVE) comprising a text-image encoder for learning semantic, texture, and color priors and a text-mask encoder for learning shape and layout priors. Then, we formulate the imagination phase as multiple priors aggregation (MPA) by combining these complementary priors and adding noise for diversity. Lastly, we formulate the creation phase by using a cascaded attentive generator (CAG) to progressively draw a picture from coarse to fine. We leverage adversarial learning for LeicaGAN to enforce semantic consistency and visual realism. Thorough experiments on two public benchmark datasets demonstrate LeicaGAN's superiority over the baseline method. Code has been made available at https://github.com/qiaott/LeicaGAN.",[],[],"['Tingting Qiao', 'Jing Zhang', 'Duanqing Xu', 'Dacheng Tao']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/d384dec9f5f7a64a36b5c8f03b8a6d92-Abstract.html,Security,Cross-Modal Learning with Adversarial Samples,"With the rapid developments of deep neural networks, numerous deep cross-modal analysis methods have been presented and are being applied in widespread real-world applications, including healthcare and safety-critical environments. However, the recent studies on robustness and stability of deep neural networks show that a microscopic modification, known as adversarial sample, which is even imperceptible to humans, can easily fool a well-performed deep neural network and brings a new obstacle to deep cross-modal correlation exploring. In this paper, we propose a novel Cross-Modal correlation Learning with Adversarial samples, namely CMLA, which for the first time presents the existence of adversarial samples in cross-modal data. Moreover, we provide a simple yet effective adversarial sample learning method, where inter- and intra- modality similarity regularizations across different modalities are simultaneously integrated into the learning of adversarial samples. Finally, our proposed CMLA is demonstrated to be highly effective in cross-modal hashing based retrieval. Extensive experiments on two cross-modal benchmark datasets show that the adversarial examples produced by our CMLA are efficient in fooling a target deep cross-modal hashing network. On the other hand, such adversarial examples can significantly strengthen the robustness of the target network by conducting an adversarial training.",[],[],"['CHAO LI', 'Shangqian Gao', 'Cheng Deng', 'De Xie', 'Wei Liu']","[""School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China and Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA"", ""School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China"", 'Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, PA', ""School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China"", 'Tencent AI Lab, China']","['China', 'China', 'China', 'China']"
https://papers.nips.cc/paper_files/paper/2019/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html,Security,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,"We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks.Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works.Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach  through extensively experiments on different datasets compared with state-of-the-art approaches.",[],[],"['Haichao Zhang', 'Jianyu Wang']","['Horizon Robotics', 'Baidu Research']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/db29450c3f5e97f97846693611f98c15-Abstract.html,Security,Convergent Policy Optimization for Safe Reinforcement Learning,"We study the safe reinforcement learning problem with nonlinear function approximation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators.  We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore, to extend our theoretical results, we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.",[],[],"['Ming Yu', 'Zhuoran Yang', 'Mladen Kolar', 'Zhaoran Wang']","['The University of Chicago Booth School of Business, Chicago, IL', 'Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ', 'The University of Chicago Booth School of Business, Chicago, IL', 'Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL']",[]
https://papers.nips.cc/paper_files/paper/2019/hash/dca5672ff3444c7e997aa9a2c4eb2094-Abstract.html,Security,Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components,"Abstract Neural networks are state-of-the-art classification approaches but are generally difficult to interpret. This issue can be partly alleviated by constructing a precise decision process within the neural network. In this work, a network architecture, denoted as Classification-By-Components network (CBC), is proposed. It is restricted to follow an intuitive reasoning based decision process inspired by Biederman's recognition-by-components theory from cognitive psychology. The network is trained to learn and detect generic components that characterize objects. In parallel, a class-wise reasoning strategy based on these components is learned to solve the classification problem. In contrast to other work on reasoning, we propose three different types of reasoning: positive, negative, and indefinite. These three types together form a probability space to provide a probabilistic classifier. The decomposition of objects into generic components combined with the probabilistic reasoning provides by design a clear interpretation of the classification decision process. The evaluation of the approach on MNIST shows that CBCs are viable classifiers. Additionally, we demonstrate that the inherent interpretability offers a profound understanding of the classification behavior such that we can explain the success of an adversarial attack. The method's scalability is successfully tested using the ImageNet dataset.",[],[],"['Sascha Saralajew', 'Lars Holdijk', 'Maike Rees', 'Ebubekir Asan', 'Thomas Villmann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e2f374c3418c50bc30d67d5f7454a5b4-Abstract.html,Security,Certifiable Robustness to Graph Perturbations,"Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.",[],[],"['Aleksandar Bojchevski', 'Stephan Günnemann']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/e46bc064f8e92ac2c404b9871b2a4ef2-Abstract.html,Security,Self-Routing Capsule Networks,"Capsule networks have recently gained a great deal of interest as a new architecture of neural networks that can be more robust to input perturbations than similar-sized CNNs. Capsule networks have two major distinctions from the conventional CNNs: (i) each layer consists of a set of capsules that specialize in disjoint regions of the feature space and (ii) the routing-by-agreement coordinates connections between adjacent capsule layers. Although the routing-by-agreement is capable of filtering out noisy predictions of capsules by dynamically adjusting their influences, its unsupervised clustering nature causes two weaknesses: (i) high computational complexity and (ii) cluster assumption that may not hold in presence of heavy input noise. In this work, we propose a novel and surprisingly simple routing strategy called self-routing where each capsule is routed independently by its subordinate routing network. Therefore, the agreement between capsules is not required anymore but both poses and activations of upper-level capsules are obtained in a way similar to Mixture-of-Experts. Our experiments on CIFAR-10, SVHN and SmallNORB show that the self-routing performs more robustly against white-box adversarial attacks and affine transformations, requiring less computation.",[],[],"['Taeyoung Hahn', 'Myeongjang Pyeon', 'Gunhee Kim']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ebbdfea212e3a756a1fded7b35578525-Abstract.html,Security,Adversarial Music: Real world Audio Adversary against Wake-word Detection System,"Voice Assistants (VAs) such as Amazon Alexa or Google Assistant rely on wake-word detection to respond to people's commands, which could potentially be vulnerable to audio adversarial examples. In this work, we target our attack on the wake-word detection system. Our goal is to jam the model with some inconspicuous background music to deactivate the VAs while our audio adversary is present. We implemented an emulated wake-word detection system of Amazon Alexa based on recent publications. We validated our models against the real Alexa in terms of wake-word detection accuracy. Then we computed our audio adversaries with consideration of expectation over transform and we implemented our audio adversary with a differentiable synthesizer. Next we verified our audio adversaries digitally on hundreds of samples of utterances collected from the real world. Our experiments show that we can effectively reduce the recognition F1 score of our emulated model from 93.4% to 11.0%. Finally, we tested our audio adversary over the air, and verified it works effectively against Alexa, reducing its F1 score from 92.5% to 11.0%. To the best of our knowledge, this is the first real-world adversarial attack against a commercial grade VA wake-word detection system. Our demo video is included in the supplementary material.",[],[],"['Juncheng Li', 'Shuhui Qu', 'Xinjian Li', 'Joseph Szurley', 'J. Zico Kolter', 'Florian Metze']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html,Security,A Little Is Enough: Circumventing Defenses For Distributed Learning,"Distributed learning is central for large-scale training of deep-learning models. However, it is exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models assume that the rogue participants (a) are omniscient (know the data of all other participants), and (b) introduce large changes to the parameters. Accordingly, most defense mechanisms make a similar assumption and attempt to use statistically robust methods to identify and discard values whose reported gradients are far from the population mean. We observe that if the empirical variance between the gradients of workers is high enough, an attacker could take advantage of this and launch a non-omniscient attack that operates within the population variance. We show that the variance is indeed high enough even for simple datasets such as MNIST, allowing an attack that is not only undetected by existing defenses, but also uses their power against them, causing those defense mechanisms to consistently select the byzantine workers while discarding legitimate ones. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (``backdooring''). We show that less than 25\% of colluding workers are sufficient to degrade the accuracy of  models trained on MNIST, CIFAR10 and CIFAR100 by 50\%, as well as to introduce backdoors without hurting the accuracy for MNIST and CIFAR10 datasets, but with a degradation for CIFAR100.",[],[],"['Gilad Baruch', 'Moran Baruch', 'Yoav Goldberg']","['Dept. of Computer Science, Bar Ilan University, Israel', 'Dept. of Computer Science, Bar Ilan University, Israel', 'Dept. of Computer Science, Bar Ilan University, Israel and The Allen Institute for Artificial Intelligence']","['Israel', 'Israel', 'Israel']"
https://papers.nips.cc/paper_files/paper/2019/hash/f29e2360ef277f77595dfae0aab78138-Abstract.html,Security,Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning,"We investigate the theoretical limits of pipeline parallel learning of deep learning architectures, a distributed setup in which the computation is distributed per layer instead of per example. For smooth convex and non-convex objective functions, we provide matching lower and upper complexity bounds and show that a naive pipeline parallelization of Nesterov's accelerated gradient descent is optimal. For non-smooth convex functions, we provide a novel algorithm coined Pipeline Parallel Random Smoothing (PPRS) that is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate, where $d$ is the underlying dimension. While the convergence rate still obeys a slow $\varepsilon^{-2}$ convergence rate, the depth-dependent part is accelerated, resulting in a near-linear speed-up and convergence time that only slightly depends on the depth of the deep learning architecture. Finally, we perform an empirical analysis of the non-smooth non-convex case and show that, for difficult and highly non-smooth problems, PPRS outperforms more traditional optimization algorithms such as gradient descent and Nesterov's accelerated gradient descent for problems where the sample size is limited, such as few-shot or adversarial learning.",[],[],"['Igor Colin', 'Ludovic DOS SANTOS', 'Kevin Scaman']","[""Huawei Noah's Ark Lab"", ""Huawei Noah's Ark Lab"", ""Huawei Noah's Ark Lab""]",[]
https://papers.nips.cc/paper_files/paper/2019/hash/f5aa4bd09c07d8b2f65bad6c7cd3358f-Abstract.html,Security,A Composable Specification Language for Reinforcement Learning Tasks,"Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL, and show that it outperforms several state-of-the-art baselines.",[],[],"['Kishor Jothimurugan', 'Rajeev Alur', 'Osbert Bastani']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html,Security,Certifying Geometric Robustness of Neural Networks,"The use of neural networks in safety-critical computer vision systems calls for theirrobustness certification against natural geometric transformations (e.g., rotation,scaling). However, current certification methods target mostly norm-based pixelperturbations and cannot certify robustness against geometric transformations. Inthis work, we propose a new method to compute sound and asymptotically optimallinear relaxations for any composition of transformations. Our method is based ona novel combination of sampling and optimization. We implemented the methodin a system called DeepG and demonstrated that it certifies significantly morecomplex geometric transformations than existing methods on both defended andundefended networks while scaling to large architectures.",[],[],"['Mislav Balunovic', 'Maximilian Baader', 'Gagandeep Singh', 'Timon Gehr', 'Martin Vechev']",[],[]
https://papers.nips.cc/paper_files/paper/2019/hash/fe73f687e5bc5280214e0486b273a5f9-Abstract.html,Security,Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control,"Data-driven models are subject to model errors due to limited and noisy training data. Key to the application of such models in safety-critical domains is the quantification of their model error. Gaussian processes provide such a measure and uniform error bounds have been derived, which allow safe control based on these models. However, existing error bounds require restrictive assumptions. In this paper, we employ the Gaussian process distribution and continuity arguments to derive a novel uniform error bound under weaker assumptions. Furthermore, we demonstrate how this distribution can be used to derive probabilistic Lipschitz constants and analyze the asymptotic behavior of our bound. Finally, we derive safety conditions for the control of unknown dynamical systems based on Gaussian process models and evaluate them in simulations of a robotic manipulator.",[],[],"['Armin Lederer', 'Jonas Umlauft', 'Sandra Hirche']","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich']",[]