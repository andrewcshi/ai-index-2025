link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://dl.acm.org/authorize?N675479,Transparency & Explainability,Explaining Explanations in AI,"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.","['Interpretability', 'Explanations', 'Accountability', 'Philosophy of Science']","['Computing methodologies _ Artificial intelligence', 'Cognitive science', 'Machine learning', 'Human-centered computing _ HCI theory', 'concepts and models']","['Brent Mittelstadt', 'Chris Russell', 'Sandra Wachter']","['University of Oxford, The Alan Turing Institute', 'University of Surrey, The Alan Turing Institute', 'University of Oxford, The Alan Turing Institute']","[None, None, None]"
https://dl.acm.org/authorize?N675488,Transparency & Explainability,Deep Weighted Averaging Classifiers,"Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.","['interpretability', 'credibility', 'conformal methods']",['Computing methodologies _ Supervised learning'],"['Dallas Card', 'Michael Zhang', 'Noah A. Smith']","['Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, Washington', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, Washington and Allen Institute for Artificial, Intelligence, Seattle, Washington']","[None, None, None]"
https://dl.acm.org/authorize?N675460,Transparency & Explainability,Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,"Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.","['data ethics', 'data governance', 'privacy', 'algorithmic bias', 'data sharing']","['Social and professional topics _ Socio-technical systems', 'Applied computing _ IT governance']","['Meg Young', 'Luke Rodriguez', 'Emily Keller', 'Feiyang Sun', 'Boyang Sa', 'Jan Whittington', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']","[None, None, None, None, None, None, None]"
https://dl.acm.org/authorize?N675349,Transparency & Explainability,Actionable Recourse in Linear Classification,"Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.","['recourse', 'classification', 'accountability', 'integer programming', 'audit', 'credit scoring']","['Human-centered computing _ Social recommendation', 'Theory of computation _ Integer programming', 'Computing methodologies_Philosophical/theoretical foundations of artificial intelligence', 'Machine learning']","['Berk Ustun', 'Alexander Spangher', 'Yang Liu']","['Harvard University, Cambridge, MA', 'Carnegie Mellon University, Pittsburgh, PA', 'UC Santa Cruz CSE, Santa Cruz, CA']","[None, None, None]"
https://dl.acm.org/authorize?N675468,Transparency & Explainability,Fair Allocation through Competitive Equilibrium from Generic Incomes,"Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a ""fair"" allocation of the items among them? Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to ""purchase"" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency -- prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible. We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.","['Market equilibrium', 'fairness', 'unequal entitlements', 'additive preferences', 'Fisher markets']",['Theory of computation _ Algorithmic game theory and mechanism design'],"['Moshe Babaioff', 'Noam Nisan', 'Inbal Talgam-Cohen']","['Microsoft Research', 'Hebrew University of Jerusalem', 'Technion']","[None, None, None]"
https://dl.acm.org/authorize?N675455,Transparency & Explainability,"Clear Sanctions, Vague Rewards: How China’s Social Credit System Defines “Good” and “Bad” Behavior","China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, ""good"" behavior can result in material rewards and reputational gain while ""bad"" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines ""good"" and ""bad"" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform ""Credit China"", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on ""good"" behavior) and the Blacklist (information on ""bad"" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.","['Social Credit System', 'Socio-Technical Systems', 'Transparency', 'Behavioral Engineering']","['Social and professional topics _ Government technology policy', 'Information systems _ Decision support systems', 'Security and privacy _ Social aspects of security and privacy', 'Applied computing _ Anthropology']","['Severin Engelmann', 'Mo Chen', 'Felix Fischer', 'Ching-yu Kao', 'Jens Grossklags']","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich and IF', 'Fraunhofer Institute for Applied and Integrated Security', 'Technical University of Munich']","[None, None, 'London', None, None]"
https://dl.acm.org/authorize?N675461,Transparency & Explainability,Who's the Guinea Pig? Investigating Online A/B/n Tests In-The-Wild,"A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.","['online controlled experiments', 'A/B/n testing', 'personalization']","['Security and privacy_Privacy protections', 'Human-centered computing _User studies', 'Empirical studies in HCI', 'Social and professional topics _ Codes of ethics']","['Shan Jiang', 'John Martin', 'Christo Wilson']","['Northeastern University', 'Northeastern University', 'Northeastern University']","[None, None, None]"
https://dl.acm.org/authorize?N675454,Transparency & Explainability,On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook,"Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are ""divisive"": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.","['advertisements', 'targeting', 'social divisiveness', 'news media', 'social media', 'perception bias']","['General and reference _ Empirical studies', 'Measurement', 'Human-centered computing _ Social networking sites', 'Empirical studies in collaborative and social computing']","['Filipe N. Ribeiro', 'Koustuv Saha', 'Mahmoudreza Babaei', 'Lucas Henrique', 'Johnnatan Messias', 'Fabricio Benevenuto', 'Oana Goga', 'Krishna P. Gummadi', 'Elissa M. Redmiles']","['UFOP/UFMG', 'Georgia Tech', 'MPI-SWS', 'UFMG', 'MPI-SWS', 'UFMG', 'Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG', 'MPI-SWS', 'University of Maryland']","['Brazil', 'US', 'Germany', 'Brazil', 'Germany', 'Brazil', 'France', 'Germany', 'US']"
https://dl.acm.org/authorize?N675463,Transparency & Explainability,Model Cards for Model Reporting,"Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.","['datasheets', 'model cards', 'documentation', 'disaggregated evaluation', 'fairness evaluation', 'ML model evaluation', 'ethical considerations']","['General and reference _ Evaluation', 'Social and professional topics _ User characteristics', 'Software and its engineering_Use cases', 'Documentation', 'Software evolution', 'Humancentered computing _ Walkthrough evaluations']","['Margaret Mitchell', 'Simone Wu', 'Andrew Zaldivar', 'Parker Barnes', 'Lucy Vasserman', 'Ben Hutchinson', 'Elena Spitzer', 'Inioluwa Deborah Raji', 'Timnit Gebru']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/authorize?N675465,Transparency & Explainability,SIREN: A Simulation Framework for Understanding the Effects of Recommender Systems in Online News Environments,"The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as ""Matthew effects"", ""filter bubbles"", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.","['recommender systems', 'diversity', 'news media', 'simulation']","['Information systems _ Recommender systems', 'Applied computing _ Publishing', 'Computing methodologies _ Simulation tools']","['Dimitrios Bountouridis', 'Jaron Harambam', 'Mykola Makhortykh', 'Mónica Marrero', 'Nava Tintarev', 'Claudia Hauff']","['Delft University of Technology', 'Institute for Information Law, University of Amsterdam', 'Amsterdam School of Communication Research, University of Amsterdam', 'Delft University of Technology', 'Delft University of Technology', 'Delft University of Technology']","['The Netherlands', 'The Netherlands', 'The Netherlands', 'The Netherlands', 'The Netherlands', 'The Netherlands']"
https://dl.acm.org/authorize?N675344,Fairness & Bias,Fairness and Abstraction in Sociotechnical Systems,"A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.","['Fairness-aware Machine Learning', 'Sociotechnical Systems', 'Interdisciplinary']","['Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Machine learning']","['Andrew D. Selbst', 'Danah Boyd', 'Sorelle A. Friedler', 'Suresh Venkatasubramanian', 'Janet Vertesi']","['Data & Society Research Institute, New York, NY', 'Microsoft Research and Data & Society Research Institute, New York, NY', 'Haverford College, Haverford, PA', 'University of Utah, Salt Lake City, UT', 'Princeton University, Princeton, NJ']","[None, None, None, None, None]"
https://dl.acm.org/authorize?N675343,Fairness & Bias,50 Years of Test (Un)fairness: Lessons for Machine Learning,"Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.","['history', 'fairness', 'ML fairness', 'test fairness', 'psychometrics']","['General and reference _ Surveys and overviews', 'Metrics', 'Social and professional topics _ History of computing', 'Historical people', 'History of computing theory', 'Socio-technical systems', 'User characteristics', 'Mathematics of computing _ Probability and statistics', 'Probabilistic algorithms', 'Theory of computation _ Probabilistic computation', 'Computing methodologies _ Algebraic algorithms', 'Software and its engineering _ Model checking']","['Ben Hutchinson', 'Margaret Mitchell']","[None, None]","[None, None]"
https://dl.acm.org/authorize?N675460,Fairness & Bias,Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,"Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.","['data ethics', 'data governance', 'privacy', 'algorithmic bias', 'data sharing']","['Social and professional topics _ Socio-technical systems', 'Applied computing _ IT governance']","['Meg Young', 'Luke Rodriguez', 'Emily Keller', 'Feiyang Sun', 'Boyang Sa', 'Jan Whittington', 'Bill Howe']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']","[None, None, None, None, None, None, None]"
https://dl.acm.org/authorize?N675453,Fairness & Bias,Analyzing Biases in Perception of Truth in News Stories and their Implications for Fact Checking,"Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check. However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories -- (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users. The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion. On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking. In summary, we make the following contributions in this work. 1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story. 2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions. 3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth. Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO","['Truth Perception Bias', 'False News', 'Fact Checking']",['Human-centered computing Social networking sites'],"['Mahmoudreza Babaei', 'Abhijnan Chakraborty', 'Juhi Kulshrestha', 'Elissa M. Redmiles', 'Meeyoung Cha', 'Krishna P. Gummadi']","['MPI-SWS', 'MPI-SWS', 'GESIS', 'University of Maryland', 'KAIST', 'MPI-SWS']","['Germany', 'Germany', 'Germany', 'US', 'South Korea', 'Germany']"
https://dl.acm.org/authorize?N675458,Fairness & Bias,Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments,"Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with ""disparate interactions,"" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new ""algorithm-in-the-loop"" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.","['fairness', 'risk assessment', 'behavioral experiment', 'Mechanical Turk']","['Human-centered computing _ Human computer interaction (HCI)', 'Applied computing _ Law']","['Ben Green', 'Yiling Chen']","['Harvard University', 'Harvard University']","[None, None]"
https://dl.acm.org/authorize?N675485,Fairness & Bias,Fairness under unawareness: assessing disparity when protected class is unobserved,"Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.","['fair lending', 'disparate impact', 'protected class', 'racial discrimination', 'race imputation', 'probablistic proxy model', 'Bayesian Improved Surname Geocoding']","['Social and professional topics _ Race and ethnicity', 'Geographic characteristics', 'Applied computing_IT governance', 'Law']","['Jiahao Chen', 'Nathan Kallus', 'Xiaojie Mao', 'Geoffry Svacha', 'Madeleine Udell']","[None, 'Cornell Tech, New York, New York', 'Cornell Tech, New York, New York', None, 'Cornell University, Ithaca, New York']","[None, 'USA', 'USA', None, 'USA']"
https://dl.acm.org/authorize?N675456,Fairness & Bias,A Taxonomy of Ethical Tensions in Inferring Mental Health States from Social Media,"Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.","['mental health', 'ethics', 'machine learning', 'algorithms', 'social media']","['Human-centered computing _ Collaborative and social computing', 'Social media', 'Applied computing _ Psychology']","['Stevie Chancellor', 'Michael L. Birnbaum', 'Eric D. Caine', 'Vincent M. B. Silenzio', 'Munmun De Choudhury']","['Georgia Tech, Atlanta, GA', 'Northwell Health, Glen Oaks, NY', 'University of Rochester, Rochester, NY', 'University of Rochester, Rochester, NY', 'Georgia Tech, Atlanta, GA']","['US', 'US', 'US', 'US', 'US']"
https://dl.acm.org/authorize?N675466,Fairness & Bias,An Algorithmic Framework to Control Polarization in Personalization,"Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but -- subject to these constraints -- will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.","['Personalization', 'recommender systems', 'polarization', 'bandit optimization', 'group fairness', 'diversifcation']","['Information systems _ Personalization', 'Theory of computation _ Online learning algorithms']","['L. Elisa Celis', 'Sayash Kapoor', 'Farnood Salehi', 'Nisheeth Vishnoi']","['Yale University', 'IIT Kanpur', 'École Polytechnique Fédérale de Lausanne (EPFL)', 'Yale University']","[None, None, None, None]"
https://dl.acm.org/authorize?N675470,Fairness & Bias,Racial categories in machine learning,"Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled ""Black"" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.","['fairness', 'machine learning', 'racial classification', 'segregation']","['Social and professional topics _ Race and ethnicity', 'Systems analysis and design', 'Applied computing _ Sociology', 'Computing methodologies _ Dimensionality reduction and manifold learning']","['Sebastian Benthall', 'Bruce D. Haynes']","['New York University', 'University of California, Davis']","[None, None]"
https://dl.acm.org/authorize?N675486,Fairness & Bias,Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data,"How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.","['causal inference', 'variational inference', 'fairness in machine learning']","['Mathematics of computing _ Causal networks', 'Computing methodologies_Latent variable models', 'Neural networks']","['David Madras', 'Elliot Creager', 'Toniann Pitassi', 'Richard Zemel']","['University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute']","[None, None, None, None]"
https://dl.acm.org/authorize?N675459,Fairness & Bias,An Empirical Study of Rich Subgroup Fairness for Machine Learning,"Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.","['Algorithmic Bias', 'Subgroup Fairness', 'Fairness Auditing', 'Fair Clas-sification']",['Computing methodologies _ Machine learning'],"['Michael Kearns', 'Seth Neel', 'Aaron Roth', 'Zhiwei Steven Wu']","['Department of Computer and Information Sciences, University of Pennsylvania', 'Department of Statistics, University of Pennsylvania', 'Department of Computer and Information Sciences, University of Pennsylvania', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None]"
https://dl.acm.org/authorize?N675469,Fairness & Bias,A Moral Framework for Understanding Fair ML through Economic Models of Equality of Opportunity,"We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)---an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.","['Equality of Opportunity (EOP)', 'Fairness for Machine Learning', 'Rawlsian and Luck Egalitarian EOP', 'Statistical Parity', 'Equality of Odds', 'Predictive Value Parity']","['Computing methodologies _ Supervised learning', 'Batch learning', 'Applied computing _ Economics', 'Sociology']","['Hoda Heidari', 'Michele Loi', 'Krishna P. Gummadi', 'Andreas Krause']","['ETH Zürich', 'University of Zürich', 'MPI-SWS', 'ETH Zürich']","[None, None, None, None]"
https://dl.acm.org/authorize?N675478,Fairness & Bias,Measuring the Biases that Matter: The Ethical and Causal Foundations for Measures of Fairness in Algorithms,"Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of ""procedural bias"" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of ""outcome bias"" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of ""behavior-relative error bias"" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of ""score-relative error bias"" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized. In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.","['Algorithmic decision-making', 'fairness', 'casual inference', 'discrimination']","['Social and professional topics ~ Computing / technology policy', 'Theory of computation ~ Design and analysis of algorithm']","['Bruce Glymour', 'Jonathan Herington']","['Department of Philosophy, Kansas State University, Manhattan, KS', 'Department of Philosophy, Kansas State University, Manhattan, KS']","['USA', 'USA']"
https://dl.acm.org/authorize?N675462,Fairness & Bias,Fairness-Aware Programming,"Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness. We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested. We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.","['Probabilistic specifications', 'Fairness', 'Assertion languages', 'Runtime monitoring', 'Runtime verification']","['Theory of computation _ Program specifications', 'Software and its engineering _ Specification languages']","['Aws Albarghouthi', 'Samuel Vinitsky']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison']","[None, None]"
https://dl.acm.org/authorize?N675451,Fairness & Bias,Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting,"We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are ""scrubbed,"" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.","['Supervised learning', 'algorithmic fairness', 'gender bias', 'online re-cruiting', 'automated hiring', 'compounding injustices']","['Computing methodologies _ Machine learning', 'Applied computing _ Document management and text processing']","['Maria De-Arteaga', 'Alexey Romanov', 'Hanna Wallach', 'Jennifer Chayes', 'Christian Borgs', 'Alexandra Chouldechova', 'Sahin Geyik', 'Krishnaram Kenthapadi', 'Adam Tauman Kalai']","['Carnegie Mellon University', 'University of Massachusetts Lowell', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Carnegie Mellon University', 'LinkedIn', 'LinkedIn', 'Microsoft Research']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/authorize?N675461,Fairness & Bias,Who's the Guinea Pig? Investigating Online A/B/n Tests In-The-Wild,"A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.","['online controlled experiments', 'A/B/n testing', 'personalization']","['Security and privacy_Privacy protections', 'Human-centered computing _User studies', 'Empirical studies in HCI', 'Social and professional topics _ Codes of ethics']","['Shan Jiang', 'John Martin', 'Christo Wilson']","['Northeastern University', 'Northeastern University', 'Northeastern University']","[None, None, None]"
https://dl.acm.org/authorize?N675457,Fairness & Bias,Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 million people,"A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care. To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs). We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate. An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t--1 (features), fine-grained care utilization data in year t -- 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks. The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health -- not just costs -- also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost -- for example, race. We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health. The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on -- cost -- is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care -- incentives that induce health systems to focus on dollars rather than health -- also has consequences for the way algorithms are built and monitored.","['bias', 'algorithms', 'racial disparities', 'health policy', 'medicine']",['Social and professional topics _ Computing / technology policy'],"['Ziad Obermeyer', 'Sendhil Mullainathan']","['UC Berkeley, Berkeley, CA', 'University of Chicago, Chicago, IL']","[None, None]"
https://dl.acm.org/authorize?N675348,Security,Model Reconstruction from Model Explanations,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.","['Explanations', 'machine learning', 'security', 'privacy']","['Computing methodologies _ Machine learning', 'Security and privacy']","['Smitha Milli', 'Ludwig Schmidt', 'Anca D. Dragan', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","[None, None, None, None]"