link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/1802.07810,Transparency & Explainability,Guiding Prosecutorial Decisions with an Interpretable Statistical Model,"With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.",[],[],"['Forough Poursabzi-Sangdeh', 'Daniel G. Goldstein', 'Jake M. Hofman', 'Jennifer Wortman Vaughan', 'Hanna Wallach']",[],[]
https://arxiv.org/abs/2001.01891,Transparency & Explainability,IMLI: An Incremental Framework for MaxSAT-Based Learning of Interpretable Classification Rules,"The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.",[],[],"['Bishwamittra Ghosh', 'Kuldeep S. Meel']",[],[]
https://arxiv.org/abs/1805.12317,Transparency & Explainability,Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,"Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.",[],[],"['Michael P. Kim', 'Amirata Ghorbani', 'James Zou']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1902.02384,Transparency & Explainability,Global Explanations of Neural Networks: Mapping the Landscape of Predictions,"A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.",[],[],"['Mark Ibrahim', 'Melissa Louie', 'Ceena Modarres', 'John Paisley']","['Center for Machine Learning, Capital One, New York, NY, USA', 'Center for Machine Learning, Capital One, New York, NY, USA', 'Center for Machine Learning, Capital One, New York, NY, USA', 'Columbia University, New York, NY, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2001.05207,Transparency & Explainability,A Formal Approach to Explainability,"We regard explanations as a blending of the input sample and the model's output and offer a few definitions that capture various desired properties of the function that generates these explanations. We study the links between these properties and between explanation-generating functions and intermediate representations of learned models and are able to show, for example, that if the activations of a given layer are consistent with an explanation, then so do all other subsequent layers. In addition, we study the intersection and union of explanations as a way to construct new explanations.",[],[],"['Lior Wolf', 'Tomer Galanti', 'Tamir Hazan']","['Facebook AI Research & Tel Aviv University, Tel Aviv, Israel', 'Tel Aviv University, Tel Aviv, Israel', 'Technion, Haifa, Israel']","['Israel', 'Israel', 'Israel']"
https://arxiv.org/abs/1810.00031,Fairness & Bias,Active Fairness in Algorithmic Decision Making,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternative active framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g., equal opportunity); and 2) parity in both false positive and false negative rates (i.e., equal odds). Moreover, we show that by leveraging their additional degree of freedom, active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",[],[],"['Alejandro Noriega-Campero', 'Michiel A. Bakker', 'Bernardo Garcia-Bulle', 'Alex Pentland']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Mexican Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/1901.04562,Fairness & Bias,"Putting Fairness Principles into Practice: Challenges, Metrics, and Improvements","As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product",[],[],"['Alex Beutel', 'Jilin Chen', 'Tulsee Doshi', 'Hai Qian', 'Allison Woodruff', 'Christine Luu', 'Pierre Kreitmann', 'Jonathan Bischof', 'Ed H. Chi']","['Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, San Bruno, CA, USA', 'Google, Seattle, WA, USA', 'Google, Mountain View, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1811.03654,Fairness & Bias,How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness,"What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more preferred than the others, and the results also provide support for the principle of affirmative action.",[],[],"['Nripsuta Saxena', 'Karen Huang', 'Evan DeFilippis', 'Goran Radanovic', 'David Parkes', 'Yang Liu']",[],[]
https://arxiv.org/abs/1809.10610,Fairness & Bias,Counterfactual Fairness in Text Classification through Robustness,"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that ""Some people are gay"" is toxic while ""Some people are straight"" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.",[],[],"['Sahaj Garg', 'Vincent Perot', 'Nicole Limtiaco', 'Ankur Taly', 'Ed H. Chi', 'Alex Beutel']","['Stanford University, Stanford, CA, USA', 'Google AI, New York, NY, USA', 'Google AI, New York, NY, USA', 'Google AI, Mountain View, CA, USA', 'Google AI, Mountain View, CA, USA', 'Google AI, New York, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1810.08683,Fairness & Bias,Taking Advantage of Multitask Learning for Fair Classification,"A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information (e.g., gender or ethnic group) as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.",[],[],"['Luca Oneto', 'Michele Donini', 'Amon Elders', 'Massimiliano Pontil']","['DIBRIS - University of Genoa, Genova, Italy', 'Istituto Italiano di Tecnologia, Genova, Italy', 'Istituto Italiano di Tecnologia, Genova, Italy', 'Istituto Italiano di Tecnologia & University College London, Genova, Italy']","['Italy', 'Italy', 'Italy', 'Italy']"
https://arxiv.org/abs/1805.12317,Fairness & Bias,Multiaccuracy: Black-Box Post-Processing for Fairness in Classification,"Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for ""black women"") even when the sensitive features (e.g. ""race"", ""gender"") are not given to the algorithm explicitly.",[],[],"['Michael P. Kim', 'Amirata Ghorbani', 'James Zou']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.04273,Fairness & Bias,Loss-Aversively Fair Classification,"The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers. Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.",[],[],"['Junaid Ali', 'Muhammad Bilal Zafar', 'Adish Singla', 'Krishna P. Gummadi']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","['Germany', 'Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/1812.08769,Fairness & Bias,What are the biases in my word embedding?,"This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly ""debiased"" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination--such as racial discrimination--are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.",[],[],"['Nathaniel Swinger', 'Maria De-Arteaga', 'Neil Thomas Heffernan IV', 'Mark DM Leiserson', 'Adam Tauman Kalai']","['Lexington High School, Lexington, MA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Shrewsbury High School, Shrewsbury, MA, USA', 'University of Maryland, Flibbertigibbet, MD, USA', 'Microsoft Research, Cambridge, MA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1809.04663,Security,Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk,"Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a ""fair"" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.",[],[],"['Stephen Pfohl', 'Ben Marafino', 'Adrien Coulet', 'Fatima Rodriguez', 'Latha Palaniappan', 'Nigam H. Shah']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University & Université de Lorraine, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"