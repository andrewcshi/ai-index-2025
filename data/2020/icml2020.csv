link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://dl.acm.org/doi/10.5555/3524938.3525082,Transparency & Explainability,Graph Optimal Transport for Cross-Domain Alignment,"Cross-domain alignment between two sets of entities (e.g., objects in an image, words in a sentence) is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, where no training signals are provided to explicitly encourage alignment. Plus, the learned attention matrices are often dense and difficult to interpret. We propose Graph Optimal Transport (GOT), a principled framework that builds upon recent advances in Optimal Transport (OT). In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities as a dynamically-constructed graph. Two types of OT distances are considered: (i) Wasserstein distance (WD) for node (entity) matching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure) matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer.The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization.",[],[],"['Liqun Chen', 'Zhe Gan', 'Yu Cheng', 'Linjie Li', 'Lawrence Carin', 'Jingjing Liu']","['Duke University', 'Microsoft Dynamics 365 AI Research', 'Microsoft Dynamics 365 AI Research', 'Microsoft Dynamics 365 AI Research', 'Duke University', 'Microsoft Dynamics 365 AI Research']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525073,Transparency & Explainability,Invariant Rationalization,"Selective rationalization improves neural network interpretability by identifying a small subset of input features — the rationale — that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output.  Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations and generalize better to different test scenarios. The resulting explanations also align better with human judgments. Our implementations are publicly available at https://github.com/code-terminator/invariant_rationalization.",[],[],"['Shiyu Chang', 'Yang Zhang', 'Mo Yu', 'Tommi S. Jaakkola']","['MIT-IBM Watson AI Lab', 'MIT-IBM Watson AI Lab', 'IBM Research', 'CSAIL MIT']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525338,Transparency & Explainability,Parameterized Rate-Distortion Stochastic Encoder,"We propose a novel gradient-based tractable approach for the Blahut-Arimoto (BA) algorithm to compute the rate-distortion function where the BA algorithm is fully parameterized. This results in a rich and flexible framework to learn a new class of stochastic encoders, termed PArameterized RAte-DIstortion Stochastic Encoder (PARADISE). The framework can be applied to a wide range of settings from semi-supervised, multi-task to supervised and robust learning. We show that the training objective of PARADISE can be seen as a form of regularization that helps improve generalization. With an emphasis on robust learning we further develop a novel posterior matching objective to encourage smoothness on the loss function and show that PARADISE can significantly improve interpretability as well as robustness to adversarial attacks on the CIFAR-10 and ImageNet datasets. In particular, on the CIFAR-10 dataset, our model reduces standard and adversarial error rates in comparison to the state-of-the-art by 50% and 41%, respectively without the expensive computational cost of adversarial training.",[],[],"['Quan Hoang', 'Trung Le', 'Dinh Phung']","['Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University']","['Australia', 'Australia', 'Australia']"
https://dl.acm.org/doi/10.5555/3524938.3525206,Transparency & Explainability,Decision Trees for Decision-Making under the Predict-then-Optimize Framework,"We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.",[],[],"['Adam N. Elmachtoub', 'Jason Cheuk Nam Liang', 'Ryan McNellis']","['Department of Industrial Engineering and Operations Research and Data Science Institute, Columbia University, NY', 'Operations Research Center, Massachusetts Institute of Technology, MA', 'Department of Industrial Engineering and Operations Research and Data Science Institute, Columbia University, NY and Amazon, NY']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525084,Transparency & Explainability,Mapping natural-language problems to formal-language solutions using structured neural representations,"Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, to solve problems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for mapping Natural-language problems to Formal-language solutions, called TPN2F. The encoder of TP-N2F employs TPR ‘binding’ to encode natural-language symbolic structure in vector space and the decoder uses TPR ‘unbinding’ to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.",[],[],"['Kezhen Chen', 'Qiuyuan Huang', 'Hamid Palangi', 'Paul Smolensky', 'Kenneth D. Forbus', 'Jianfeng Gao']","['Microsoft Research, Redmond and Department of Computer Science, Northwestern University, Evanston', 'Microsoft Research, Redmond', 'Microsoft Research, Redmond', 'Microsoft Research, Redmond and Department of Cognitive Science, Johns Hopkins University, Baltimore', 'Department of Computer Science, Northwestern University, Evanston', 'Microsoft Research, Redmond']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525670,Transparency & Explainability,Transparency Promotion with Model-Agnostic Linear Competitors,"We propose a novel type of hybrid model for multi-class classification, which utilizes competing linear models to collaborate with an existing black-box model, promoting transparency in the decision-making process. Our proposed hybrid model, Model-Agnostic Linear Competitors (MALC), brings together the interpretable power of linear models and the good predictive performance of the state-of-the-art black-box models. We formulate the training of a MALC model as a convex optimization problem, optimizing the predictive accuracy and transparency (defined as the percentage of data captured by the linear models) in the objective function. Experiments show that MALC offers more model flexibility for users to balance transparency and accuracy, in contrast to the currently available choice of either a pure black-box model or a pure interpretable model. The human evaluation also shows that more users are likely to choose MALC for this model flexibility compared with interpretable models and black-box models.",[],[],"['Hassan Rafique', 'Tong Wang', 'Qihang Lin', 'Arshia Sighani']","['Program in Applied Mathematical and Computational Sciences, The University of Iowa, Iowa City, Iowa', 'Department of Business Analytics, The University of Iowa, Iowa City, Iowa', 'Department of Business Analytics, The University of Iowa, Iowa City, Iowa', 'BASIS Independent Silicon Valley, San Jose, California']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525519,Transparency & Explainability,Hallucinative Topological Memory for Zero-Shot Visual Planning,"In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. Most previous works on VP approached the problem by planning in a learned latent space, resulting in low-quality visual plans, and difficult training algorithms. Here, instead, we propose a simple VP method that plans directly in image space and displays competitive performance. We build on the semi-parametric topological memory (SPTM) method: image samples are treated as nodes in a graph, the graph connectivity is learned from image sequence data, and planning can be performed using conventional graph search methods. We propose two modifications on SPTM. First, we train an energy-based graph connectivity function using contrastive predictive coding that admits stable training. Second, to allow zero-shot planning in new domains, we learn a conditional VAE model that generates images given a context describing the domain, and use these hallucinated samples for building the connectivity graph and planning. We show that this simple approach significantly outperform the SOTA VP methods, in terms of both plan interpretability and success rate when using the plan to guide a trajectory-following controller. Interestingly, our method can pick up non-trivial visual properties of objects, such as their geometry, and account for it in the plans.",[],[],"['Kara Liu', 'Thanard Kurutach', 'Christine Tung', 'Pieter Abbeel', 'Aviv Tamar']","['Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley', 'Berkeley AI Research, University of California, Berkeley and Technion']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525556,Transparency & Explainability,Estimation of Bounds on Potential Outcomes For Decision Making,"Estimation of individual treatment effects is commonly used as the basis for contextual decision making in fields such as healthcare, education, and economics. However, it is often sufficient for the decision maker to have estimates of upper and lower bounds on the potential outcomes of decision alternatives to assess risks and benefits. We show that, in such cases, we can improve sample efficiency by estimating simple functions that bound these outcomes instead of estimating their conditional expectations, which may be complex and hard to estimate. Our analysis highlights a trade-off between the complexity of the learning task and the confidence with which the learned bounds hold. Guided by these findings, we develop an algorithm for learning upper and lower bounds on potential outcomes which optimize an objective function defined by the decision maker, subject to the probability that bounds are violated being small. Using a clinical dataset and a well-known causality benchmark, we demonstrate that our algorithm outperforms baselines, providing tighter, more reliable bounds.",[],[],"['Maggie Makar', 'Fredrik Johansson', 'John Guttag', 'David Sontag']","['CSAIL, MIT', 'Chalmers University of Technology', 'CSAIL, MIT', 'CSAIL, MIT']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.5555/3524938.3525306,Transparency & Explainability,Retrieval Augmented Language Model Pre-Training,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",[],[],"['Kelvin Guu', 'Kenton Lee', 'Zora Tung', 'Panupong Pasupat', 'Ming-Wei Chang']","['Google Research', 'Google Research', 'Google Research', 'Google Research', 'Google Research']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525592,Transparency & Explainability,Explainable k-Means and k-Medians Clustering,"Many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the k-means and k-medians objectives. In terms of negative results, we show that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and any clustering based on a tree with k leaves must incur an Omega(log k) approximation factor compared to the optimal clustering. On the positive side, for two means/medians, we show that a single threshold cut can achieve a constant factor approximation, and we give nearly-matching lower bounds; for general k > 2, we design an efficient algorithm that leads to an O(k) approximation to the optimal k-medians and an O(k^2) approximation to the optimal k-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size.",[],[],"['Sanjoy Dasgupta', 'Nave Frost', 'Michal Moshkovitz', 'Cyrus Rashtchian']","['University of California, San Diego', 'Tel Aviv University', 'University of California, San Diego', 'University of California, San Diego']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.5555/3524938.3525447,Transparency & Explainability,Problems with Shapley-value-based explanations as feature importance measures,"Game-theoretic formulations of feature importance have become popular as a way to ""explain"" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.",[],[],"['I. Elizabeth Kumar', 'Suresh Venkatasubramanian', 'Carlos Scheidegger', 'Sorelle A. Friedler']","['School of Computing, University of Utah, Salt Lake City, UT', 'School of Computing, University of Utah, Salt Lake City, UT', 'Department of Computer Science, University of Arizona, Tucson, AZ,', 'Department of Computer Science, Haverford College, Haverford, PA']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525367,Transparency & Explainability,Semi-Supervised Learning with Normalizing Flows,"Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.",[],[],"['Pavel Izmailov', 'Polina Kirichenko', 'Marc Finzi', 'Andrew Gordon Wilson']","['New York University', 'New York University', 'New York University', 'New York University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525668,Transparency & Explainability,DeepCoDA: personalized interpretability for compositional health data,"Abstract Interpretability allows the domain-expert to directly evaluate the model's relevance and reliability, a practice that offers assurance and builds trust. In the healthcare setting, interpretable models should implicate relevant biological mechanisms independent of technical factors like data pre-processing. We define personalized interpretability as a measure of sample-specific feature attribution, and view it as a minimum requirement for a precision health model to justify its conclusions. Some health data, especially those generated by high-throughput sequencing experiments, have nuances that compromise precision health models and their interpretation. These data are compositional, meaning that each feature is conditionally dependent on all other features. We propose the Deep Compositional Data Analysis (DeepCoDA) framework to extend precision health modelling to high-dimensional compositional data, and to provide personalized interpretability through patient-specific weights. Our architecture maintains state-of-the-art performance across 25 real-world data sets, all while producing interpretations that are both personalized and fully coherent for compositional data.",[],[],"['Thomas P. Quinn', 'Dang Nguyen', 'Santu Rana', 'Sunil Gupta', 'Svetha Venkatesh']","['Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong', 'Applied Artificial Intelligence Institute, Deakin University, Geelong']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://dl.acm.org/doi/10.5555/3524938.3525443,Transparency & Explainability,A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition,"An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching approach to learn sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled and/or noisy labeled data,  and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this method via technical results and on Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9% improvement in performance. A comprehensive evaluation also shows that the method leads to improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks.",[],[],"['Anurag Kumar', 'Vamsi Krishna Ithapu']","['Facebook Reality Labs, Redmond', 'Facebook Reality Labs, Redmond']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525832,Transparency & Explainability,Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network,"Recent work has shown that topological enhancements to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learning non-linear functions, and bidirectional processing, which exploits acausal information in a sequence. In this work, we explore the delayed-RNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayed-RNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic experiments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in others. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs -- but with equivalent or faster runtimes for the delayed-RNNs.",[],[],"['Javier S. Turek', 'Shailee Jain', 'Vy A. Vo', 'Mihai Capotă', 'Alexander G. Huth', 'Theodore L. Willke']","['Intel Labs, Hillsboro, Oregon', 'Department of Computer Science, The University of Texas at Austin, Austin, Texas', 'Intel Labs, Hillsboro, Oregon', 'Intel Labs, Hillsboro, Oregon', 'Department of Computer Science and Department of Neuroscience, The University of Texas at Austin, Austin, Texas', 'Intel Labs, Hillsboro, Oregon']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525841,Transparency & Explainability,Born-again Tree Ensembles,"The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles, in particular, offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.",[],[],"['Thibaut Vidal', 'Maximilian Schiffer']","['Department of Computer Science, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro', 'TUM School of Management, Technical University of Munich, Munich']","['Brazil', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3525696,Transparency & Explainability,Attentive Group Equivariant Convolutional Networks,"Although group convolutional networks are able to learn powerful representations based on symmetry patterns, they lack explicit means to learn meaningful relationships among them (e.g., relative positions and poses). In this paper, we present attentive group equivariant convolutions, a generalization of the group convolution, in which attention is applied during the course of convolution to accentuate meaningful symmetry combinations and suppress non-plausible, misleading ones. We indicate that prior work on visual attention can be described as special cases of our proposed framework and show empirically that our attentive group equivariant convolutional networks consistently outperform conventional group convolutional networks on benchmark image datasets. Simultaneously, we provide interpretability to the learned concepts through the visualization of equivariant attention maps.",[],[],"['David W. Romero', 'Erik J. Bekkers', 'Jakub M. Tomczak', 'Mark Hoogendoorn']","['Vrije Universiteit Amsterdam', 'University of Amsterdam, Th', 'Vrije Universiteit Amsterdam', 'Vrije Universiteit Amsterdam']","[None, 'Netherlands', None, None]"
https://dl.acm.org/doi/10.5555/3491440.3491780,Transparency & Explainability,Scalable Exact Inference in Multi-Output Gaussian Processes,"Multi-output Gaussian processes (MOGPs) leverage the flexibility and interpretability of GPs while capturing structure across outputs, which is desirable, for example, in spatio-temporal modelling. The key problem with MOGPs is their computational scaling $O(n^3 p^3)$, which is cubic in the number of both inputs $n$ (e.g., time points or locations) and outputs $p$. For this reason, a popular class of MOGPs assumes that the data live around a low-dimensional linear subspace, reducing the complexity to $O(n^3 m^3)$. However, this cost is still cubic in the dimensionality of the subspace $m$, which is still prohibitively expensive for many applications. We propose the use of a sufficient statistic of the data to accelerate inference and learning in MOGPs with orthogonal bases. The method achieves linear scaling in $m$ in practice, allowing these models to scale to large $m$ without sacrificing significant expressivity or requiring approximation. This advance opens up a wide range of real-world tasks and can be combined with existing GP approximations in a plug-and-play way. We demonstrate the efficacy of the method on various synthetic and real-world data sets.",[],[],"['Shibo Li', 'Wei Xing', 'Robert M. Kirby', 'Shandian Zhe']","['School of Computing, University of Utah', 'Scientific Computing and Imaging Institute, University of Utah', 'School of Computing, University of Utah and Scientific Computing and Imaging Institute', 'School of Computing, University of Utah']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525365,Transparency & Explainability,Fast Deterministic CUR Matrix Decomposition with Accuracy Assurance,"The deterministic CUR matrix decomposition is a low-rank approximation method to analyze a data matrix. It has attracted considerable attention due to its high interpretability, which results from the fact that the decomposed matrices consist of subsets of the original columns and rows of the data matrix. The subset is obtained by optimizing an objective function with sparsity-inducing norms via coordinate descent. However, the existing algorithms for optimization incur high computation costs. This is because coordinate descent iteratively updates all the parameters in the objective until convergence. This paper proposes a fast deterministic CUR matrix decomposition. Our algorithm safely skips unnecessary updates by efficiently evaluating the optimality conditions for the parameters to be zeros. In addition, we preferentially update the parameters that must be nonzeros. Theoretically, our approach guarantees the same result as the original approach. Experiments demonstrate that our algorithm speeds up the deterministic CUR while achieving the same accuracy.",[],[],"['Yasutoshi Ida', 'Sekitoshi Kanai', 'Yasuhiro Fujiwara', 'Tomoharu Iwata', 'Koh Takeuchi', 'Hisashi Kashima']","['NTT Software Innovation Center, Tokyo,  and Department of Intelligence Science and Technology, Kyoto University, Kyoto', 'NTT Software Innovation Center, Tokyo', 'NTT Communication Science Laboratories, Kyoto', 'NTT Communication Science Laboratories, Kyoto', 'Department of Intelligence Science and Technology, Kyoto University, Kyoto,  and RIKEN Center for Advanced Intelligence Project, Tokyo', 'Department of Intelligence Science and Technology, Kyoto University, Kyoto,  and RIKEN Center for Advanced Intelligence Project, Tokyo']","['Japan', 'Japan', 'Japan', 'Japan', 'Japan', 'Japan']"
https://dl.acm.org/doi/10.5555/3524938.3525174,Transparency & Explainability,Enhancing Simple Models by Exploiting What They Already Know,"There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.",[],[],"['Amit Dhurandhar', 'Karthikeyan Shanmugam', 'Ronny Luss']","['IBM Research, Yorktown Heights, NY', 'IBM Research, Yorktown Heights, NY', 'IBM Research, Yorktown Heights, NY']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525642,Transparency & Explainability,Performative Prediction,"When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.",[],[],"['Juan C. Perdomo', 'Tijana Zrnic', 'Celestine Mendler-Dünner', 'Moritz Hardt']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525064,Fairness & Bias,Data preprocessing to mitigate bias: A maximum entropy based approach,"Data containing human or social attributes may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. This paper presents an algorithmic framework that can be used as a data preprocessing method towards mitigating such bias. Unlike prior work, it can efficiently learn distributions over large domains, controllably adjust the representation rates of protected groups and achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach leverages the principle of maximum entropy – amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main contribution is an instantiation of this framework for our set of constraints and priors, which encode our bias mitigation goals, and that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.",[],[],"['L. Elisa Celis', 'Vijay Keswani', 'Nisheeth K. Vishnoi']","['Department of Statistics and Data Science, Yale University', 'Department of Statistics and Data Science, Yale University', 'Department of Computer Science, Yale University']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525596,Fairness & Bias,Two Simple Ways to Learn Individual Fairness Metrics from Data,"Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches.",[],[],"['Debarghya Mukherjee', 'Mikhail Yurochkin', 'Moulinath Banerjee', 'Yuekai Sun']","['Department of Statistics, University of Michigan', 'IBM Research, MIT-IBM Watson AI Lab.', 'Department of Statistics, University of Michigan', 'Department of Statistics, University of Michigan']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525708,Fairness & Bias,Bounding the fairness and accuracy of classifiers from population statistics,"We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations.  We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds.  We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.",[],[],"['Sivan Sabato', 'Elad Yom-Tov']","['Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva,  and Microsoft Research, Herzelia', 'Microsoft Research, Herzelia']","['Israel', 'Israel']"
https://dl.acm.org/doi/10.5555/3524938.3525680,Fairness & Bias,AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.",[],[],"['Esteban Real', 'Chen Liang', 'David R. So', 'Quoc V. Le']","['Google Brain/Google Research, Mountain View, CA', 'Google Brain/Google Research, Mountain View, CA', 'Google Brain/Google Research, Mountain View, CA', 'Google Brain/Google Research, Mountain View, CA']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525714,Fairness & Bias,Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,"Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions--demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.",[],[],"['Debjani Saha', 'Candice Schumann', 'Duncan C. McElfresh', 'John P. Dickerson', 'Michelle L. Mazurek', 'Michael Carl Tschantz']","['University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'University of Maryland, College Park, MD', 'ICSI, Berkeley, CA']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525048,Fairness & Bias,A Pairwise Fair and Community-preserving Approach to k-Center Clustering,"Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing $k$-center algorithms to satisfy these fairness constraints. Analysis of this approach proves that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical $k$-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.",[],[],"['Brian Brubach', 'Darshan Chakrabarti', 'John P. Dickerson', 'Samir Khuller', 'Aravind Srinivasan', 'Leonidas Tsepenekas']","['Department of Computer Science, University of Maryland, College Park, Maryland and Computer Science Department, Wellesley College, Wellesley, MA', 'School of Computer Science, Carnegie Mellon University, Pittsburgh, PA', 'Department of Computer Science, University of Maryland, College Park, Maryland', 'Department of Computer Science, Northwestern University, Evanston, IL', 'Department of Computer Science, University of Maryland, College Park, Maryland', 'Department of Computer Science, University of Maryland, College Park, Maryland']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525549,Fairness & Bias,Individual Fairness for k-Clustering,"We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\ell_p$ norm cost function) from the perspective of individual fairness. More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. An individually fair clustering provides such a guarantee for every point $x\in P$. This notion of fairness was introduced in [Jung et al., 2019] where they showed how to get an approximately feasible $k$-clustering with respect to this fairness condition. In this work, we show how to get an approximately \emph{optimal} such fair $k$-clustering. The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor).",[],[],"['Sepideh Mahabadi', 'Ali Vakilian']","['TTIC, Chicago, Illinois', 'University of Wisconsin, Madison, Wisconsin']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525397,Fairness & Bias,Fair k-Centers via Maximum Matching,"The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best of each algorithm by presenting a linear-time algorithm with a guaranteed 3-approximation factor and provides empirical evidence of both the algorithm's runtime and effectiveness.",[],[],"['Matthew Jones', 'Huy Lê Nguyê˜n', 'Thy Nguyen']","['Khoury College of Computer Science, Northeastern University, Boston, Massachusetts', 'Khoury College of Computer Science, Northeastern University, Boston, Massachusetts', 'Khoury College of Computer Science, Northeastern University, Boston, Massachusetts']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525994,Fairness & Bias,Individual Calibration with Randomized Forecasting,"Machine learning applications often require calibrated predictions, e.g. a 90\% credible interval should contain the true outcome 90\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.",[],[],"['Shengjia Zhao', 'Tengyu Ma', 'Stefano Ermon']","['Computer Science Department, Stanford University', 'Computer Science Department, Stanford University', 'Computer Science Department, Stanford University']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525201,Fairness & Bias,Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing,"A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.",[],[],"['Sanghamitra Dutta', 'Dennis Wei', 'Hazar Yueksel', 'Pin-Yu Chen', 'Sijia Liu', 'Kush R. Varshney']","['IBM Research and Department of Electrical and Computer Engineering, Carnegie Mellon University', 'IBM Research', 'IBM Research', 'IBM Research', 'IBM Research', 'IBM Research']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525052,Fairness & Bias,DeBayes: a Bayesian Method for Debiasing Network Embeddings,"As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.",[],[],"['Maarten Buyl', 'Tijl De Bie']","['Department of Electronics and Information Systems, IDLab, Ghent University, Ghent', 'Department of Electronics and Information Systems, IDLab, Ghent University, Ghent']","['Belgium', 'Belgium']"
https://dl.acm.org/doi/10.5555/3524938.3525528,Fairness & Bias,Too Relaxed to Be Fair,"We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.",[],[],"['Michael Lohaus', 'Michaël Perrot', 'Ulrike Von Luxburg']","['University of Tübingen,  and Max Planck Institute for Intelligent Systems, Tübingen', 'Max Planck Institute for Intelligent Systems, Tübingen,  and Univ Lyon, UJM-Saint-Etienne, CNRS, IOGS, LabHC UMR 5516, SAINT-ETIENNE, Franc', 'University of Tübingen,  and Max Planck Institute for Intelligent Systems, Tübingen']","['Germany', 'Germany', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3524988,Fairness & Bias,Learning De-biased Representations with Biased Representations,"Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. We demonstrate the efficacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at https://github.com/clovaai/rebias.",[],[],"['Hyojin Bahng', 'Sanghyuk Chun', 'Sangdoo Yun', 'Jaegul Choo', 'Seong Joon Oh']","['Korea University', 'Clova AI Research, NAVER Corp.', 'Clova AI Research, NAVER Corp.', 'Graduate School of AI, KAIST', 'Clova AI Research, NAVER Corp.']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525669,Privacy & Data Governance,Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraints,"The problem of maximizing nonnegative monotone submodular functions under a certain constraint has been intensively studied in the last decade, and a wide range of efficient approximation algorithms have been developed for this problem. Many machine learning problems, including data summarization and influence maximization, can be naturally modeled as the problem of maximizing monotone submodular functions. However, when such applications involve sensitive data about individuals, their privacy concerns should be addressed. In this paper, we study the problem of maximizing monotone submodular functions subject to matroid constraints in the framework of differential privacy. We provide $(1-\frac{1}{\mathrm{e}})$-approximation algorithm which improves upon the previous results in terms of approximation guarantee. This is done with an almost cubic number of function evaluations in our algorithm. Moreover, we study $k$-submodularity, a natural generalization of submodularity. We give the first $\frac{1}{2}$-approximation algorithm that preserves differential privacy for maximizing monotone $k$-submodular functions subject to matroid constraints. The approximation ratio is asymptotically tight and is obtained with an almost linear number of function evaluations.",[],[],"['Akbar Rafiey', 'Yuichi Yoshida']","['Department of Computing Science, Simon Fraser University, Burnaby', 'National Institute of Informatics, Tokyo']","['Canada', 'Japan']"
https://dl.acm.org/doi/10.5555/3524938.3525003,Privacy & Data Governance,Private Query Release Assisted by Public Data,"We study the problem of differentially private query release assisted by access to public data. In this problem, the goal is to answer a large class $\mathcal{H}$ of statistical queries with error no more than $\alpha$ using a combination of public and private samples. The algorithm is required to satisfy differential privacy only with respect to the private samples. We study the limits of this task in terms of the private and public sample complexities. Our upper and lower bounds on the private sample complexity have matching dependence on the dual VC-dimension of $\mathcal{H}$. For a large category of query classes, our bounds on the public sample complexity have matching dependence on $\alpha$.",[],[],"['Raef Bassily', 'Albert Cheu', 'Shay Moran', 'Aleksandar Nikolov', 'Jonathan Ullman', 'Zhiwei Steven Wu']","['Department of Computer Science & Engineering, The Ohio State University', 'Khoury College of Computer Sciences, Northeastern University', 'Google AI Princeton', 'Department of Computer Science, University of Toronto', 'Khoury College of Computer Sciences, Northeastern University', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525997,Privacy & Data Governance,Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion,"Datasets containing sensitive information are often sequentially analyzed by many algorithms and, accordingly, a fundamental question in differential privacy is concerned with how the overall privacy bound degrades under composition. To address this question, we introduce a family of analytical and sharp privacy bounds under composition using the Edgeworth expansion in the framework of the recently proposed $f$-differential privacy. In short, whereas the existing composition theorem, for example, relies on the central limit theorem, our new privacy bounds under composition gain improved tightness by leveraging the refined approximation accuracy of the Edgeworth expansion. Our approach is easy to implement and computationally efficient for any number of compositions. The superiority of these new bounds is confirmed by an asymptotic error analysis and an application to quantifying the overall privacy guarantees of noisy stochastic gradient descent used in training private deep neural networks.",[],[],"['Qinqing Zheng', 'Jinshuo Dong', 'Qi Long', 'Weijie Su']","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525181,Privacy & Data Governance,Optimal Differential Privacy Composition for Exponential Mechanisms,"Composition is one of the most important properties of differential privacy (DP), as it allows algorithm designers to build complex private algorithms from DP primitives. We consider precise composition bounds of the overall privacy loss for exponential mechanisms, one of the fundamental classes of mechanisms in DP. Exponential mechanism has also become a fundamental building block in private machine learning, e.g. private PCA and hyper-parameter selection. We give explicit formulations of the optimal privacy loss for both the adaptive and non-adaptive composition of exponential mechanism. For the non-adaptive setting in which each mechanism has the same privacy parameter, we give an efficiently computable formulation of the optimal privacy loss. In the adaptive case, we derive a recursive formula and an efficiently computable upper bound. These precise understandings about the problem lead to a 40\% saving of the privacy budget in a practical application. Furthermore, the algorithm-specific analysis shows a difference in privacy parameters of adaptive and non-adaptive composition, which was widely believed to not exist based on the evidence from general analysis.",[],[],"['Jinshuo Dong', 'David Durfee', 'Ryan Rogers']","['Applied Mathematics and Computational Sciences, University of Pennsylvania and Data Science Applied Research, LinkedIn', 'Data Science Applied Research, LinkedIn', 'Data Science Applied Research, LinkedIn']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525278,Privacy & Data Governance,Differentially Private Set Union,"We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe $U$ of items, possibly of infinite size, and a database $D$ of users. Each user $i$ contributes a subset $W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentially private Algorithm which outputs a subset $S \subset \cup_i W_i$ such that the size of $S$ is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, $n$-grams etc., from private text data belonging to users is an instance of the set union problem. In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.",[],[],"['Sivakanth Gopi', 'Pankaj Gulhane', 'Janardhan Kulkarni', 'Judy Hanwen Shen', 'Milad Shokouhi', 'Sergey Yekhanin']","['Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft', 'Microsoft']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525357,Privacy & Data Governance,InstaHide: Instance-hiding Schemes for Private Distributed Learning,"How can multiple distributed entities train a shared deep net on their private data while protecting data privacy? This paper introduces InstaHide, a simple encryption of training images. Encrypted images can be used in standard deep learning pipelines (PyTorch, Federated Learning etc.) with no additional setup or infrastructure. The encryption has a minor effect on test accuracy (unlike differential privacy).Encryption consists of mixing the image with a set of other images (in the sense of Mixup data augmentation technique (Zhang et al., 2018)) followed by applying a random pixel-wise mask on the mixed image. Other contributions of this paper are: (a) Use of large public dataset of images (e.g. ImageNet) for mixing during encryption; this improves security. (b) Experiments demonstrating effectiveness in protecting privacy against known attacks while preserving model accuracy. (c) Theoretical analysis showing that successfully attacking privacy requires attackers to solve a difficult computational problem. (d) Demonstration that Mixup alone is insecure as (contrary to recent proposals), by showing some efficient attacks. (e) Release of a challenge dataset to allow design of new attacks.",[],[],"['Yangsibo Huang', 'Zhao Song', 'Kai Li', 'Sanjeev Arora']","['Princeton University', 'Princeton University and Institute for Advanced Study', 'Princeton University', 'Princeton University and Institute for Advanced Study']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525826,Privacy & Data Governance,Bayesian Differential Privacy for Machine Learning,"Traditional differential privacy is independent of the data distribution. However, this is not well-matched with the modern machine learning context, where models are trained on specific data. As a result, achieving meaningful privacy guarantees in ML often excessively reduces accuracy. We propose Bayesian differential privacy (BDP), which takes into account the data distribution to provide more practical privacy guarantees. We also derive a general privacy accounting method under BDP, building upon the well-known moments accountant. Our experiments demonstrate that in-distribution samples in classic machine learning datasets, such as MNIST and CIFAR-10, enjoy significantly stronger privacy guarantees than postulated by DP, while models maintain high classification accuracy.",[],[],"['Aleksei Triastcyn', 'Boi Faltings']","['Artificial Intelligence Lab, Ecole Polytechnique Fedérale de Lausanne, Lausanne', 'Artificial Intelligence Lab, Ecole Polytechnique Fedérale de Lausanne, Lausanne']","['Switzerland', 'Switzerland']"
https://dl.acm.org/doi/10.5555/3524938.3525164,Privacy & Data Governance,An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm,"We provide an end-to-end differentially private spectral algorithm for learning LDA, based on matrix/tensor decompositions, and establish theoretical guarantees on utility/consistency of the estimated model parameters. We represent the spectral algorithm as a computational graph. Noise can be injected along the edges of this graph to obtain differential privacy. We identify subsets of edges, named ``configurations'', such that adding noise to all edges in such a subset guarantees differential privacy of the end-to-end spectral algorithm. We characterize the sensitivity of the edges with respect to the input and thus estimate the amount of noise to be added to each edge for any required privacy level. We then characterize the utility loss  for each configuration as a function of injected noise.  Overall, by combining the sensitivity and utility characterization, we obtain an end-to-end differentially private spectral algorithm for LDA and identify which configurations outperform others under specific regimes. We are the first to achieve utility guarantees under a required level of differential privacy for learning in LDA. We additionally show that our method systematically outperforms differentially private variational inference.",[],[],"['Christopher DeCarolis', 'Mukul Ram', 'Seyed Esmaeili', 'Yu-Xiang Wang', 'Furong Huang']","['Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, University of Maryland', 'Department of Computer Science, UC Santa Barbara', 'Department of Computer Science, University of Maryland']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525842,Privacy & Data Governance,Private Reinforcement Learning with PAC and Regret Guarantees,"Motivated by high-stakes decision-making domains like personalized medicine where user information is inherently sensitive, we design privacy preserving exploration policies for episodic reinforcement learning (RL). We first provide a meaningful privacy formulation using the notion of joint differential privacy (JDP)--a strong variant of differential privacy for settings where each user receives their own sets of output (e.g., policy recommendations). We then develop a private optimism-based learning algorithm that simultaneously achieves strong PAC and regret bounds, and enjoys a JDP guarantee. Our algorithm only pays for a moderate privacy cost on exploration: in comparison to the non-private bounds, the privacy parameter only appears in lower-order terms.  Finally, we present lower bounds on sample complexity and regret for reinforcement learning subject to JDP.",[],[],"['Giuseppe Vietri', 'Borja Balle', 'Akshay Krishnamurthy', 'Zhiwei Steven Wu']","['Department of Computer Science and Engineering, University of Minnesota', 'Deepmind', 'Microsoft Research', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3524944,Privacy & Data Governance,Context Aware Local Differential Privacy,"Local differential privacy (LDP) is a strong notion of privacy that often leads to a significant drop in utility. The original definition of LDP assumes that all the elements in the data domain are equally sensitive. However, in many real-life applications, some elements are more sensitive than others. We propose a context-aware framework for LDP that allows the privacy level to vary across the data domain, enabling system designers to place privacy constraints where they matter without paying the cost where they do not. For binary data domains, we provide a universally optimal privatization scheme and highlight its connections to Warner’s randomized response and Mangat’s improved response. Motivated by geo-location and web search applications, for k-ary data domains, we consider two special cases of context-aware LDP: block-structured LDP and high-low LDP. We study minimax discrete distribution estimation under both cases and provide communication-efficient, sample-optimal schemes, and information-theoretic lower bounds. We show, using worst-case analyses and experiments on Gowalla’s 3.6 million check-ins to 43,750 locations, that context-aware LDP achieves a far better accuracy under the same number of samples.",[],[],"['Jayadev Acharya', 'K. A. Bonawitz', 'Peter Kairouz', 'Daniel Ramage', 'Ziteng Sun']","['ECE, Cornell University, Ithaca, New York', 'Google, Seattle', 'Google, Seattle', 'Google, Seattle', 'ECE, Cornell University, Ithaca, New York']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525970,Privacy & Data Governance,Privately Learning Markov Random Fields,"We consider the problem of learning Markov Random Fields (including the prototypical example, the Ising model) under the constraint of differential privacy.  Our learning goals include both \emph{structure learning}, where we try to estimate the underlying graph structure of the model, as well as the harder goal of \emph{parameter learning}, in which we additionally estimate the parameter on each edge.  We provide algorithms and lower bounds for both problems under a variety of privacy constraints -- namely pure, concentrated, and approximate differential privacy. While non-privately, both learning goals enjoy roughly the same complexity, we show that this is not the case under differential privacy.  In particular, only structure learning under approximate differential privacy maintains the non-private logarithmic dependence on the dimensionality of the data, while a change in either the learning goal or the privacy notion would necessitate a polynomial dependence. As a result, we show that the privacy constraint imposes a strong separation between these two learning problems in the high-dimensional data regime.",[],[],"['Huanyu Zhang', 'Gautam Kamath', 'Janardhan Kulkarni', 'Zhiwei Steven Wu']","['School of Electrical and Computer Engineering, Cornell University', 'Cheriton School of Computer Science, University of Waterloo', 'Microsoft Research Redmond', 'Computer Science & Engineering, University of Minnesota']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525843,Privacy & Data Governance,New Oracle-Efficient Algorithms for Private Synthetic Data Release,"We present three new algorithms for constructing differentially private synthetic data---a sanitized version of a sensitive dataset that approximately preserves the answers to a large collection of statistical queries. All three algorithms are \emph{oracle-efficient} in the sense that they are computationally efficient when given access to an optimization oracle. Such an oracle can be implemented using many existing (non-private) optimization tools such as sophisticated integer program solvers. While the accuracy of the synthetic data is contingent on the oracle's optimization performance, the algorithms satisfy differential privacy even in the worst case. For all three algorithms, we provide theoretical guarantees for both accuracy and privacy. Through empirical evaluation, we demonstrate that our methods scale well with both the dimensionality of the data and the number of queries. Compared to the state-of-the-art method High-Dimensional Matrix Mechanism (McKenna et al.~VLDB 2018), our algorithms provide better accuracy in the large workload and high privacy regime (corresponding to low privacy loss $\eps$).",[],[],"['Giuseppe Vietri', 'Grace Tian', 'Mark Bun', 'Thomas Steinke', 'Zhiwei Steven Wu']","['Department of Computer Science and Engineering, University of Minnesota', 'Harvard University', 'Boston University', 'IBM Research, Almaden', 'Department of Computer Science and Engineering, University of Minnesota']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525748,Security,Learning for Dose Allocation in Adaptive Clinical Trials with Safety Constraints,"Phase I dose-finding trials are increasingly challenging as the relationship between efficacy and toxicity of new compounds (or combination of them) becomes more complex. Despite this, most commonly used methods in practice focus on identifying a Maximum Tolerated Dose (MTD) by learning only from toxicity events. We present a novel adaptive clinical trial methodology, called Safe Efficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the cumulative efficacies while satisfying the toxicity safety constraint with high probability. We evaluate performance objectives that have operational meanings in practical clinical trials, including cumulative efficacy, recommendation/allocation success probabilities, toxicity violation probability, and sample efficiency. An extended SEEDA-Plateau algorithm that is tailored for the increase-then-plateau efficacy behavior of molecularly targeted agents (MTA) is also presented. Through numerical experiments using both synthetic and real-world datasets, we show that SEEDA outperforms state-of-the-art clinical trial designs by finding the optimal dose with higher success rate and fewer patients.",[],[],"['Cong Shen', 'Zhiyang Wang', 'Sofía S. Villar', 'Mihaela Van Der Schaar']","['University of Virginia', 'University of Pennsylvania', 'University of Cambridge', 'University of Cambridge,  and University of California, Los Angele']","[None, None, 'UK', 'UK']"
https://dl.acm.org/doi/10.5555/3524938.3525726,Security,Constrained Markov Decision Processes via Backward Value Functions,"Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.",[],[],"['Harsh Satija', 'Philip Amortila', 'Joelle Pineau']","['Department of Computer Science, McGill University, Montreal,  and Mila Québec AI Institute and Facebook AI Research, Montrea', 'Department of Computer Science, McGill University, Montreal,  and Mila Québec AI Institut', 'Department of Computer Science, McGill University, Montreal,  and Mila Québec AI Institute and Facebook AI Research, Montrea']","['Canada', 'Canada', 'Canada']"
https://dl.acm.org/doi/10.5555/3524938.3525785,Security,Responsive Safety in Reinforcement Learning by PID Lagrangian Methods,"Lagrangian methods are widely used algorithms for constrained optimization problems, but their learning dynamics exhibit oscillations and overshoot which, when applied to safe reinforcement learning, leads to constraint-violating behavior during agent training.  We address this shortcoming by proposing a novel Lagrange multiplier update method that utilizes derivatives of the constraint function.  We take a controls perspective, wherein the traditional Lagrange multiplier update behaves as \emph{integral} control; our terms introduce \emph{proportional} and \emph{derivative} control, achieving favorable learning dynamics through damping and predictive measures.  We apply our PID Lagrangian methods in deep RL, setting a new state of the art in Safety Gym, a safe RL benchmark.  Lastly, we introduce a new method to ease controller tuning by providing invariance to the relative numerical scales of reward and cost.  Our extensive experiments demonstrate improved performance and hyperparameter robustness, while our algorithms remain nearly as simple to derive and implement as the traditional Lagrangian approach.",[],[],"['Adam Stooke', 'Joshua Achiam', 'Pieter Abbeel']","['University of California, Berkeley and OpenAI', 'University of California, Berkeley and OpenAI', 'University of California, Berkeley']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525590,Security,Confidence-Aware Learning for Deep Neural Networks,"Despite the power of deep neural networks for a wide range of tasks, an overconfident prediction issue has limited their practical use in many safety-critical applications. Many recent works have been proposed to mitigate this issue, but most of them require either additional computational costs in training and/or inference phases or customized architectures to output confidence estimates separately. In this paper, we propose a method of training deep neural networks with a novel loss function, named Correctness Ranking Loss, which regularizes class probabilities explicitly to be better confidence estimates in terms of ordinal ranking according to confidence. The proposed method is easy to implement and can be applied to the existing architectures without any modification. Also, it has almost the same computational costs for training as conventional deep classifiers and outputs reliable predictions by a single inference. Extensive experimental results on classification benchmark datasets indicate that the proposed method helps networks to produce well-ranked confidence estimates. We also demonstrate that it is effective for the tasks closely related to confidence estimation, out-of-distribution detection and active learning.",[],[],"['Jooyoung Moon', 'Jihyo Kim', 'Younghak Shin', 'Sangheum Hwang']","['Department of Data Science, Seoul National University of Science and Technology, Seoul, Republic of Korea', 'Department of Data Science, Seoul National University of Science and Technology, Seoul, Republic of Korea', 'LG CNS, Seoul, Republic of Korea', 'Department of Data Science, Seoul National University of Science and Technology, Seoul, Republic of Korea and Department of Industrial & Information Systems Engineering, Seoul National University of Science and Technology, Seoul, Republic of Korea']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525358,Security,Accelerated Stochastic Gradient-free and Projection-free Methods,"In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank-Wolfe) methods to solve the constrained stochastic and finite-sum nonconvex optimization. Specifically, we propose an accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW) method based on the variance reduced technique of SPIDER/SpiderBoost and a novel momentum accelerated technique. Moreover, under some mild conditions, we prove that the Acc-SZOFW has the function query complexity of $O(d\sqrt{n}\epsilon^{-2})$ for finding an $\epsilon$-stationary point in the finite-sum problem, which improves the exiting best result by a factor of $O(\sqrt{n}\epsilon^{-2})$, and has the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem, which improves the exiting best result by a factor of $O(\epsilon^{-1})$. To relax the large batches required in the Acc-SZOFW, we further propose a novel accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW*) based on a new variance reduced technique of STORM, which still reaches the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem without relying on any large batches. In particular, we present an accelerated framework of the Frank-Wolfe methods based on the proposed momentum accelerated technique. The extensive experimental results on black-box adversarial attack and robust black-box classification demonstrate the efficiency of our algorithms.",[],[],"['Feihu Huang', 'Lue Tao', 'Songcan Chen']","['College of Computer Science & Technology, Nanjing University of Aeronautics and Astronautics, Nanjing,  and MIIT Key Laboratory of Pattern Analysis & Machine Intelligenc', 'College of Computer Science & Technology, Nanjing University of Aeronautics and Astronautics, Nanjing,  and MIIT Key Laboratory of Pattern Analysis & Machine Intelligenc', 'College of Computer Science & Technology, Nanjing University of Aeronautics and Astronautics, Nanjing,  and MIIT Key Laboratory of Pattern Analysis & Machine Intelligenc']","['China', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3524938.3525929,Security,Randomized Smoothing of All Shapes and Sizes,"Randomized smoothing is the current state-of-the-art defense with provable robustness against $\ell_2$ adversarial attacks. Many works have devised new randomized smoothing schemes for other metrics, such as $\ell_1$ or $\ell_\infty$; however, substantial effort was needed to derive such new guarantees. This begs the question: can we find a general theory for randomized smoothing? We propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are: (1) we show that for an appropriate notion of ""optimal"", the optimal smoothing distributions for any ""nice"" norms have level sets given by the norm's *Wulff Crystal*; (2) we propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution; and, (3) we show fundamental limits to current randomized smoothing techniques via the theory of *Banach space cotypes*. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in $\ell_1$ on standard datasets. Meanwhile, we show using (3) that with only label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of $\ell_p$-norm $\Omega(\min(1, d^{\frac{1}{p} - \frac{1}{2}}))$, when the input dimension $d$ is large. We provide code in github.com/tonyduan/rs4a.",[],[],"['Greg Yang', 'Tony Duan', 'J. Edward Hu', 'Hadi Salman', 'Ilya Razenshteyn', 'Jerry Li']","['Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI', 'Microsoft Research AI']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525771,Security,Second-Order Provable Defenses against Adversarial Attacks,"A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for neural networks is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for neural networks with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded (globally or locally), we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness. Putting these results together leads to our proposed {\bf C}urvature-based {\bf R}obustness {\bf C}ertificate (CRC) and {\bf C}urvature-based {\bf R}obust {\bf T}raining (CRT). Our numerical results show that CRT leads to significantly higher certified robust accuracy compared to interval-bound propagation based training.",[],[],"['Sahil Singla', 'Soheil Feizi']","['Department of Computer Science, University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park']","[None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525338,Security,Parameterized Rate-Distortion Stochastic Encoder,"We propose a novel gradient-based tractable approach for the Blahut-Arimoto (BA) algorithm to compute the rate-distortion function where the BA algorithm is fully parameterized. This results in a rich and flexible framework to learn a new class of stochastic encoders, termed PArameterized RAte-DIstortion Stochastic Encoder (PARADISE). The framework can be applied to a wide range of settings from semi-supervised, multi-task to supervised and robust learning. We show that the training objective of PARADISE can be seen as a form of regularization that helps improve generalization. With an emphasis on robust learning we further develop a novel posterior matching objective to encourage smoothness on the loss function and show that PARADISE can significantly improve interpretability as well as robustness to adversarial attacks on the CIFAR-10 and ImageNet datasets. In particular, on the CIFAR-10 dataset, our model reduces standard and adversarial error rates in comparison to the state-of-the-art by 50% and 41%, respectively without the expensive computational cost of adversarial training.",[],[],"['Quan Hoang', 'Trung Le', 'Dinh Phung']","['Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University', 'Department of DSAI, Faculty of Information Technology, Monash University']","['Australia', 'Australia', 'Australia']"
https://dl.acm.org/doi/10.5555/3524938.3525933,Security,Interpolation between Residual and Non-Residual Networks,"Although ordinary differential equations (ODEs) provide insights for designing network architectures, its relationship with the non-residual convolutional neural networks (CNNs) is still unclear. In this paper, we present a novel ODE model by adding a damping term. It can be shown that the proposed model can recover both a ResNet and a CNN by adjusting an interpolation coefficient. Therefore, the damped ODE model provides a unified framework for the interpretation of residual and non-residual networks. The Lyapunov analysis reveals better stability of the proposed model, and thus yields robustness improvement of the learned networks. Experiments on a number of image classification benchmarks show that the proposed model substantially improves the accuracy of ResNet and ResNeXt over the perturbed inputs from both stochastic noise and adversarial attack methods. Moreover, the loss landscape analysis demonstrates the improved robustness of our method along the attack direction.",[],[],"['Zonghan Yang', 'Yang Liu', 'Chenglong Bao', 'Zuoqiang Shi']","['Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University', 'Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology, Department of Computer Science and Technology, Tsinghua University', 'Yau Mathematical Sciences Center, Tsinghua University', 'Department of Mathematical Sciences, Tsinghua University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525158,Security,Adversarial Attacks on Probabilistic Autoregressive Forecasting Models,"We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values. This setting includes the recently proposed deep probabilistic autoregressive forecasting models that estimate the probability distribution of a time series given its past and achieve state-of-the-art results in a diverse set of application domains. The key technical challenge we address is how to effectively differentiate through the Monte-Carlo estimation of statistics of the output sequence joint distribution. Additionally, we extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations. We demonstrate that our approach can successfully generate attacks with small input perturbations in two challenging tasks where robust decision making is crucial -- stock market trading and prediction of electricity consumption.",[],[],"['Raphaël Dang-Nhu', 'Gagandeep Singh', 'Pavol Bielik', 'Martin Vechev']","['Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich']","['Switzerland', 'Switzerland', 'Switzerland', 'Switzerland']"
https://dl.acm.org/doi/10.5555/3524938.3526000,Security,Robust Graph Representation Learning via Neural Sparsification,"Graph representation learning serves as the core of important prediction tasks, ranging from product recommendation to fraud detection. Real-life graphs usually have complex information in the local neighborhood, where each node is described by a rich set of features and connects to dozens or even hundreds of neighbors. Despite the success of neighborhood aggregation in graph neural networks, task-irrelevant information is mixed into nodes' neighborhood, making learned models suffer from sub-optimal generalization performance. In this paper, we present NeuralSparse, a supervised graph sparsification technique that improves generalization power by learning to remove potentially task-irrelevant edges from input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize sparsification processes, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance. Experimental results on both benchmark and private datasets show that NeuralSparse can yield up to 7.2% improvement in testing accuracy when working with existing graph neural networks on node classification tasks.",[],[],"['Cheng Zheng', 'Bo Zong', 'Wei Cheng', 'Dongjin Song', 'Jingchao Ni', 'Wenchao Yu', 'Haifeng Chen', 'Wei Wang']","['Department of Computer Science, University of California, Los Angeles, CA', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'NEC Laboratories America, Princeton, NJ', 'Department of Computer Science, University of California, Los Angeles, CA']","[None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525700,Security,Certified Robustness to Label-Flipping Attacks via Randomized Smoothing,"Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. In this work, we propose a strategy for building linear classifiers that are certifiably robust against a strong variant of label flipping, where each test example is targeted independently. In other words, for each test point, our classifier includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier. We derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with minimal additional runtime complexity over standard classification and no assumptions on the train or test distributions. We generalize our results to the multi-class case, providing the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.",[],[],"['Elan Rosenfeld', 'Ezra Winston', 'Pradeep Ravikumar', 'J. Zico Kolter']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University and Bosch Center for AI']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525984,Security,Attacks Which Do Not Kill Training Make Adversarial Learning Stronger,"Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question—do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel formulation of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively—adversarial robustness can indeed be achieved without compromising the natural generalization.",[],[],"['Jingfeng Zhang', 'Xilie Xu', 'Bo Han', 'Gang Niu', 'Lizhen Cui', 'Masashi Sugiyama', 'Mohan Kankanhalli']","['School of Computing, National University of ', 'Taishan College, Shandong University, Jinan', 'Department of Computer Science, Hong Kong Baptist University, Hong Kong,  and RIKEN Center for Advanced Intelligence Project, Tokyo, Japa', 'RIKEN Center for Advanced Intelligence Project, Tokyo', 'School of Software & Joint SDU-NTU Centre for Artificial Intelligence Research, Shandong University, Jinan', 'RIKEN Center for Advanced Intelligence Project, Tokyo,  and Graduate School of Frontier Sciences, The University of Tokyo, Tokyo', 'School of Computing, National University of ']","['Singapore', 'China', 'China', 'Japan', 'China', 'Japan', 'Singapore']"
https://dl.acm.org/doi/10.5555/3524938.3525901,Security,Adversarial Robustness via Runtime Masking and Cleansing,"Deep neural networks are shown to be vulnerable to adversarial attacks. This motivates robust learning techniques, such as the adversarial training, whose goal is to learn a network that is robust against adversarial attacks.  However, the sample complexity of robust learning can be significantly larger than that of “standard” learning. In this paper, we propose improving the adversarial robustness of a network by leveraging the potentially large test data seen at runtime. We devise a new defense method, called runtime masking and cleansing (RMC), that adapts the network at runtime before making a prediction to dynamically mask network gradients and cleanse the model of the non-robust features inevitably learned during the training process due to the size limit of the training set. We conduct experiments on real-world datasets and the results demonstrate the effectiveness of RMC empirically.",[],[],"['Yi-Hsuan Wu', 'Chia-Hung Yuan', 'Shan-Hung Wu']","['Department of Computer Science, National Tsing Hua University, Taiwan', 'Department of Computer Science, National Tsing Hua University, Taiwan', 'Department of Computer Science, National Tsing Hua University, Taiwan']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525100,Security,On Breaking Deep Generative Model-based Defenses and Beyond,"Deep neural networks have been proven to be vulnerable to the so-called adversarial attacks. Recently there have been efforts to defend such attacks with deep generative models. These defense often predict by inverting the deep generative models rather than simple feedforward propagation. Such defenses are difficult to attack due to obfuscated gradient. In this work, we develop a new gradient approximation attack to break these defenses. The idea is to view the inversion phase as a dynamical system, through which we extract the gradient w.r.t the input by tracing its recent trajectory. An amortized strategy is further developed to accelerate the attack. Experiments show that our attack breaks state-of-the-art defenses (e.g DefenseGAN, ABS) much more effectively than other attacks. Additionally, our empirical results provide insights for understanding the weaknesses of deep generative model-based defenses.",[],[],"['Yanzhi Chen', 'Renjie Xie', 'Zhanxing Zhu']","['School of Informatics, The University of Edinburgh', 'School of Information Engineering, Southeast University', 'School of Mathematical Sciences, Peking University']","['UK', 'China', 'China']"
https://dl.acm.org/doi/10.5555/3524938.3525707,Security,Adversarial Attacks on Copyright Detection Systems,"It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net. We then attack this system using simple gradient methods and show that it is easily broken with white-box attacks. By scaling these perturbations up, we can create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube's Content ID system, using perturbations that are audible but significantly smaller than a random baseline. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.",[],[],"['Parsa Saadatpanah', 'Ali Shafahi', 'Tom Goldstein']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525539,Security,Adversarial Nonnegative Matrix Factorization,"Nonnegative Matrix Factorization (NMF) has become an increasingly important research topic in machine learning. Despite all the practical success, most of existing NMF models are still vulnerable to adversarial attacks. To overcome this limitation, we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process. Different from the traditional NMF models which focus on  either the regular input or certain types of noise, our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations). We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis. Theoretically, the robustness analysis of ANMF is established under mild conditions dedicating asymptotically unbiased prediction. Extensive experiments verify that ANMF is robust to a broad categories of perturbations, and achieves state-of-the-art performances on distinct real-world benchmark datasets.",[],[],"['Lei Luo', 'Yanfu Zhang', 'Heng Huang']","['JD Finance America Corporation, Mountain View, CA and Department of Electrical and Computer Engineering, University of Pittsburgh, PA', 'Department of Electrical and Computer Engineering, University of Pittsburgh, PA', 'JD Finance America Corporation, Mountain View, CA and Department of Electrical and Computer Engineering, University of Pittsburgh, PA']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525979,Security,Adaptive Reward-Poisoning Attacks against Reinforcement Learning,"In reward-poisoning attacks against reinforcement learning (RL), an attacker can perturb the environment reward $r_t$ into $r_t+\delta_t$ at each step, with the goal of forcing the RL agent to learn a nefarious policy. We categorize such attacks by the infinity-norm constraint on $\delta_t$: We provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe; we provide a corresponding upper threshold above which the attack is feasible. Feasible attacks can be further categorized as non-adaptive where $\delta_t$ depends only on $(s_t,a_t, s_{t+1})$, or adaptive where $\delta_t$ depends further on the RL agent's learning process at time $t$. Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$, whereas non-adaptive attacks require exponential steps. We provide a constructive proof that a Fast Adaptive Attack strategy achieves the polynomial rate. Finally, we show that empirically an attacker can find effective reward-poisoning attacks using state-of-the-art deep RL techniques.",[],[],"['Xuezhou Zhang', 'Yuzhe Ma', 'Adish Singla', 'Xiaojin Zhu']","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'Max Planck Institute for Software Systems', 'University of Wisconsin-Madison']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525548,Security,Adversarial Neural Pruning with Latent Vulnerability Suppression,"Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.",[],[],"['Divyam Madaan', 'Jinwoo Shin', 'Sung Ju Hwang']","['School of Computing, KAIST, South Korea', 'School of Electrical Engineering, KAIST, South Korea and Graduate School of AI, KAIST, South Korea', 'School of Computing, KAIST, South Korea and Graduate School of AI, KAIST, South Korea and AITRICS, South Korea']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525554,Security,Adversarial Robustness Against the Union of Multiple Perturbation Models,"Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against $\ell_\infty$, $\ell_2$, and $\ell_1$ attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 46.1% against the union of ($\ell_\infty$, $\ell_2$, $\ell_1$) perturbations with radius  = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6% accuracy.",[],[],"['Pratyush Maini', 'Eric Wong', 'J. Zico Kolter']","['Department of Computer Science and Engineering, IIT Delhi', 'Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania', 'Computer Science Department, Carnegie Mellon University, Pittsburgh, Pennsylvania and Bosch Center for Artificial Intelligence, Pittsburgh, Pennsylvania']","['India', None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525094,Security,More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models,"Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under $\ell_\infty$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.",[],[],"['Lin Chen', 'Yifei Min', 'Mingrui Zhang', 'Amin Karbasi']","['Department of Electrical Engineering, Yale University', 'Department of Statistics and Data Science, Yale University', 'Department of Statistics and Data Science, Yale University', 'Department of Electrical Engineering, Yale University']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3524979,Security,Adversarial Learning Guarantees for Linear Hypotheses and Neural Networks,"Adversarial or test time robustness measures the susceptibility of a classifier to perturbations to the test input. While there has been a flurry of recent work on designing defenses against such perturbations, the theory of adversarial robustness is not well understood. In order to make progress on this, we focus on the problem of understanding generalization in adversarial settings, via the lens of Rademacher complexity. We give upper and lower bounds for the adversarial empirical Rademacher complexity of linear hypotheses with adversarial perturbations measured in $l_r$-norm for an arbitrary $r \geq 1$. We then extend our analysis to provide Rademacher complexity lower and upper bounds for a single ReLU unit. Finally, we give adversarial Rademacher complexity bounds for feed-forward neural networks with one hidden layer.",[],[],"['Pranjal Awasthi', 'Natalie S. Frank', 'Mehryar Mohri']","['Google Research and Rutgers University', 'Courant Institute of Math. Sciences', 'Google Research and Courant Institute of Math. Sciences']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525846,Security,Safe Reinforcement Learning in Constrained Markov Decision Processes,"Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a step-wise approach for optimizing safety and cumulative reward.  In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-Safety-Gym, and the other simulates Mars surface exploration by using real observation data.",[],[],"['Akifumi Wachi', 'Yanan Sui']","['IBM Research AI, Tokyo', 'Tsinghua University, Beijing']","['Japan', 'China']"
https://dl.acm.org/doi/10.5555/3524938.3525047,Security,Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences,"Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.",[],[],"['Daniel S. Brown', 'Russell Coleman', 'Ravi Srinivasan', 'Scott Niekum']","['Computer Science Department, The University of Texas at Austin', 'Computer Science Department, The University of Texas at Austin and Applied Research Laboratories, The University of Texas at Austin', 'Applied Research Laboratories, The University of Texas at Austin', 'Computer Science Department, The University of Texas at Austin']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525385,Security,Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation,"We present an approach for unsupervised domain adaptation—with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift—from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels. Instead, we present a sampling-based implicit alignment approach, where the sample selection is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift.",[],[],"['Xiang Jiang', 'Qicheng Lao', 'Stan Matwin', 'Mohammad Havaei']","['Imagia,  and Dalhousie University', 'Imagia,  and Mila, Université de Montréal', 'Dalhousie University,  and Polish Academy of Sciences, Polan', 'Imagia']","['Canada', 'Canada', 'Canada', 'Canada']"
https://dl.acm.org/doi/10.5555/3524938.3525787,Security,Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks,"Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.",[],[],"['David Stutz', 'Matthias Hein', 'Bernt Schiele']","['Max Planck Institute for Informatics, Saarbrücken', 'University of Tübingen, Tübingen', 'Max Planck Institute for Informatics, Saarbrücken']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525357,Security,InstaHide: Instance-hiding Schemes for Private Distributed Learning,"How can multiple distributed entities train a shared deep net on their private data while protecting data privacy? This paper introduces InstaHide, a simple encryption of training images. Encrypted images can be used in standard deep learning pipelines (PyTorch, Federated Learning etc.) with no additional setup or infrastructure. The encryption has a minor effect on test accuracy (unlike differential privacy).Encryption consists of mixing the image with a set of other images (in the sense of Mixup data augmentation technique (Zhang et al., 2018)) followed by applying a random pixel-wise mask on the mixed image. Other contributions of this paper are: (a) Use of large public dataset of images (e.g. ImageNet) for mixing during encryption; this improves security. (b) Experiments demonstrating effectiveness in protecting privacy against known attacks while preserving model accuracy. (c) Theoretical analysis showing that successfully attacking privacy requires attackers to solve a difficult computational problem. (d) Demonstration that Mixup alone is insecure as (contrary to recent proposals), by showing some efficient attacks. (e) Release of a challenge dataset to allow design of new attacks.",[],[],"['Yangsibo Huang', 'Zhao Song', 'Kai Li', 'Sanjeev Arora']","['Princeton University', 'Princeton University and Institute for Advanced Study', 'Princeton University', 'Princeton University and Institute for Advanced Study']","[None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525143,Security,Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,"The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.",[],[],"['Francesco Croce', 'Matthias Hein']","['University of Tübingen', 'University of Tübingen']","['Germany', 'Germany']"
https://dl.acm.org/doi/10.5555/3524938.3525173,Security,Margin-aware Adversarial Domain Adaptation with Optimal Transport,"In this paper, we propose a new theoretical analysis of unsupervised domain adaptation that relates notions of large margin separation, adversarial learning and optimal transport. This analysis generalizes previous work on the subject by providing a bound on the target margin violation rate, thus reflecting a better control of the quality of separation between classes in the target domain than bounding the misclassification rate. The bound also highlights the benefit of a large margin separation on the source domain for adaptation and introduces an optimal transport (OT) based distance between domains that has the virtue of being task-dependent, contrary to other approaches. From the obtained theoretical results, we derive a novel algorithmic solution for domain adaptation that introduces a novel shallow OT-based adversarial approach and outperforms other OT-based DA baselines on several simulated and real-world classification tasks.",[],[],"['Sofien Dhouib', 'Ievgen Redko', 'Carole Lartizien']","['Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, Inserm, CREATIS UMR 5220, U1206, LYON', 'Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d Optique Graduate School Laboratoire Hubert Curien UMR 5516, Saint-Etienne', 'Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, Inserm, CREATIS UMR 5220, U1206, LYON']","['France', 'France', 'France']"
https://dl.acm.org/doi/10.5555/3524938.3525673,Security,Improving Robustness of Deep-Learning-Based Image Reconstruction,"Deep-learning-based methods for various applications have been shown vulnerable to adversarial examples.  Here we address the use of deep-learning networks as inverse problem solvers, which has generated much excitement and even adoption efforts by the main equipment vendors for medical imaging including computed tomography (CT) and MRI. However, the recent demonstration that such networks suffer from a similar vulnerability to adversarial attacks potentially undermines their future.  We propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. To this end, we introduce an auxiliary net-work to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we argue that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of in the signal-space used in previous work. We show for a linear reconstruction scheme that our min-max formulation results in a singular-value filter regularized solution, which suppresses the effect of adversarial examples.  Numerical experiments using the proposed min-max scheme confirm convergence to this solution.  We complement the theory by experiments on non-linear Compressive Sensing(CS) reconstruction by a deep neural network on two standard datasets, and, using anonymized clinical data, on a state-of-the-art published algorithm for low-dose x-ray CT reconstruction. We show a significant improvement in robustness over other methods for deep network-based reconstruction, by using the proposed approach.",[],[],"['Ankit Raj', 'Yoram Bresler', 'Bo Li']","['Coordinated Science Laboratory and Department of Electrical and Computer Engineering, University of Illinois at Urbana- Champaign', 'Coordinated Science Laboratory and Department of Electrical and Computer Engineering, University of Illinois at Urbana- Champaign', 'Department of Computer Science, UIUC']","[None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525022,Security,Adversarial Robustness for Code,"Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code including -- finding and fixing bugs, code completion, decompilation, malware detection, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) developing a set of novel techniques that enable training robust and accurate models of code.",[],[],"['Pavol Bielik', 'Martin Vechev']","['Department of Computer Science, ETH Zürich', 'Department of Computer Science, ETH Zürich']","['Switzerland', 'Switzerland']"
https://dl.acm.org/doi/10.5555/3524938.3525772,Security,FormulaZero: Distributionally Robust Online Adaptation via Offline Population Synthesis,"Balancing performance and safety is crucial to deploying autonomous vehicles in multi-agent environments. In particular, autonomous racing is a domain that penalizes safe but conservative policies, highlighting the need for robust, adaptive strategies. Current approaches either make simplifying assumptions about other agents or lack robust mechanisms for online adaptation. This work makes algorithmic contributions to both challenges. First, to generate a realistic, diverse set of opponents, we develop a novel method for self-play based on replica-exchange Markov chain Monte Carlo. Second, we propose a distributionally robust bandit optimization procedure that adaptively adjusts risk aversion relative to uncertainty in beliefs about opponents’ behaviors. We rigorously quantify the tradeoffs in performance and robustness when approximating these computations in real-time motion-planning, and we demonstrate our methods experimentally on autonomous vehicles that achieve scaled speeds comparable to Formula One racecars.",[],[],"['Aman Sinha', ""Matthew O'Kelly"", 'Hongrui Zheng', 'Rahul Mangharam', 'John Duchi', 'Russ Tedrake']","['Stanford University, Stanford, CA', 'University of Pennsylvania, Philadelphia, PA', 'University of Pennsylvania, Philadelphia, PA', 'University of Pennsylvania, Philadelphia, PA', 'Stanford University, Stanford, CA', 'Massachusetts Institute of Technology, Cambridge, Massachusetts']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.5555/3524938.3525422,Security,Entropy Minimization In Emergent Languages,"There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel.  We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.",[],[],"['Eugene Kharitonov', 'Rahma Chaabouni', 'Diane Bouchacourt', 'Marco Baroni']","['Facebook AI Research, Paris', 'Facebook AI Research, Paris,  and Cognitive Machine Learning (ENS, EHESS, SL - CNRS, INRIA', 'Facebook AI Research, Paris', 'Facebook AI Research, Paris, France and Catalan Institute for Research and Advanced Studies, Barcelona']","['France', 'France', 'France', 'Spain']"