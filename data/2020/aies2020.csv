link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/1905.07857,Transparency & Explainability,CERTIFAI: A common framework to provide explanations and analyse the fairness and robustness of black-box models,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.",[],[],"['Shubham Sharma', 'Jette Henderson', 'Joydeep Ghosh']","['University of Texas at Austin, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA', 'CognitiveScale, Austin, TX, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1912.07820,Transparency & Explainability,Balancing the Tradeoff Between Clustering Value and Interpretability,"Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a $\beta$-interpretable clustering algorithm that ensures that at least $\beta$ fraction of nodes in each cluster share the same feature value. The tunable parameter $\beta$ is user-specified. We also present a more efficient algorithm for scenarios with $\beta\!=\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.",[],[],"['Sandhya Saisubramanian', 'Sainyam Galhotra', 'Shlomo Zilberstein']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1912.09318,Transparency & Explainability,AI and Holistic Review: Informing Human Reading in College Admissions,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",[],[],"['AJ Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2001.03071,Transparency & Explainability,Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification,"Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.",[],[],"['Chris Dulhanty', 'Alexander Wong']","['University of Waterloo, Waterloo, ON, Canada', 'University of Waterloo, Waterloo, ON, Canada']","['Canada', 'Canada']"
https://arxiv.org/abs/1912.09140,Transparency & Explainability,Meta Decision Trees for Explainable Recommendation Systems,"We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature.",[],[],"['Eyal Shulman', 'Lior Wolf']","['Tel Aviv University, Tel Aviv-Yafo, Israel', 'Tel Aviv University, Tel Aviv-Yafo, Israel']","['Israel', 'Israel']"
https://arxiv.org/abs/2001.09773,Fairness & Bias,Algorithmic Fairness from a Non-ideal Perspective,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.",[],[],"['Sina Fazelpour', 'Zachary C. Lipton']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US']"
https://arxiv.org/abs/1912.08388,Fairness & Bias,Balancing the Tradeoff between Profit and Fairness in Rideshare Platforms during High-Demand Hours,"Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g. from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, \lpalg, that allows the platform designer to control the profit and fairness of the system via parameters $\alpha$ and $\beta$ respectively. We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use \lpalg to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using \lpalg, the competitive ratios for profit and fairness measures would be no worse than $\alpha/e$ and $\beta/e$ respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that $\lpalg$ under some choice of $(\alpha, \beta)$ can beat two natural heuristics, Greedy and Uniform, on \emph{both} fairness and profit.",[],[],"['Vedant Nanda', 'Pan Xu', 'Karthik Abinav Sankararaman', 'John P. Dickerson', 'Aravind Srinivasan']",[],[]
https://arxiv.org/abs/2002.01621,Fairness & Bias,Joint Optimization of AI Fairness and Utility: A Human-Centered Approach,"Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.",[],[],"['Yunfeng Zhang', 'Rachel K. E. Bellamy', 'Kush R. Varshney']","['IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA', 'IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1912.09318,Fairness & Bias,AI and Holistic Review: Informing Human Reading in College Admissions,"College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",[],[],"['AJ Alvero', 'Noah Arthurs', 'anthony lising antonio', 'Benjamin W. Domingue', 'Ben Gebre-Medhin', 'Sonia Giebel', 'Mitchell L. Stevens']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1911.09005,Security,Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",[],[],"['Roel Dobbe', 'Thomas Krendl Gilbert', 'Yonatan Mintz']",[],[]
https://arxiv.org/abs/2002.05672,Security,Steps Towards Value-Aligned Systems,"Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are indispensable tools for managing the flood of information needed to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the literature on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.",[],[],"['Osonde A. Osoba', 'Benjamin Boudreaux', 'Douglas Yeung']","['RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA', 'RAND Corporation, Santa Monica, CA, USA']","['US', 'US', 'US']"