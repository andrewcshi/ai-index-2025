link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://dl.acm.org/doi/abs/10.1145/3351095.3372873,Transparency & Explainability,Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing,"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.","['Algorithmic audits', 'machine learning', 'accountability', 'responsible innovation']","['Social and professional topics_System management', 'Technology audits', 'Software and its engineering _ Software development process management']","['Inioluwa Deborah Raji', 'Andrew Smart', 'Rebecca N. White', 'Margaret Mitchell', 'Timnit Gebru', 'Ben Hutchinson', 'Jamila Smith-Loud', 'Daniel Theron', 'Parker Barnes']","['Partnership on AI', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372874,Transparency & Explainability,Toward Situated Interventions for Algorithmic Equity: Lessons from the Field,"Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.",[],[],"['Michael Katell', 'Meg Young', 'Dharma Dailey', 'Bernease Herman', 'Vivian Guetler', 'Aaron Tam', 'Corinne Bintz', 'Daniella Raz', 'P. M. Krafft']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'West Virginia University', 'University of Washington', 'Middlebury College', 'University of Michigan', 'University of Oxford and University of Washington']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372870,Transparency & Explainability,Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","['Explainability', 'Interpretability', 'Transparency', 'Fact Sheet', 'Work Sheet', 'Desiderata', 'Taxonomy', 'AI', 'ML']","['General and reference _ Evaluation', 'Computing methodologies _ Artificial intelligence', 'Machine learning']","['Kacper Sokol', 'Peter Flach']","['University of Bristol, Bristol', 'University of Bristol, Bristol']","['United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372875,Transparency & Explainability,Multi-layered Explanation from Algorithmic Impact Assessments in the GDPR,"Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability---individual rights and systemic governance--- and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.","['Law', 'General Data Protection Regulation', 'Impact Assessments']","['Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Machine learning']","['Margot E. Kaminski', 'Gianclaudio Malgieri']","['University of Colorado', 'Vrije Universiteit Brussels, Brussels']","[None, 'Belgium']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372827,Transparency & Explainability,The Human Body is a Black Box: Supporting Clinical Decision-Making with Deep Learning,"Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.","['Deep learning', 'Interpretability', 'Medicine', 'Trust', 'Expertise']","['Computing methodologies _ Machine learning', 'Human-centered computing _ Field study', 'Social and professional topics _ Government technology policy']","['Mark Sendak', 'Madeleine Clare Elish', 'Michael Gao', 'Joseph Futoma', 'William Ratliff', 'Marshall Nichols', 'Armando Bedoya', 'Suresh Balu', ""Cara O'Brien""]","['Duke Institute for Health Innovation', 'Data & Society Research Institute', 'Duke Institute for Health Innovation', 'Harvard University and Duke University', 'Duke Institute for Health Innovation', 'Duke Institute for Health Innovation', 'Duke School of Medicine', 'Duke Institute for Health Innovation', 'Duke School of Medicine']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372845,Transparency & Explainability,FlipTest: Fairness Testing via Optimal Transport,"We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.","['fairness', 'machine learning', 'optimal transport', 'disparate impact']","['Computing methodologies _ Machine learning', 'Human centered computing']","['Emily Black', 'Samuel Yeom', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372879,Transparency & Explainability,Auditing Radicalization Pathways on YouTube,"Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.","['Radicalization', 'hate speech', 'extremism', 'algorithmic auditing']",['Human-centered computing_Empirical studies in collaborative and social computing'],"['Manoel Horta Ribeiro', 'Raphael Ottoni', 'Robert West', 'Virgílio A. F. Almeida', 'Wagner Meira']","['EPFL', 'UFMG', 'EPFL', 'UFMG', 'UFMG']","[None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372868,Transparency & Explainability,The concept of fairness in the GDPR: a linguistic and contextual interpretation,"There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.","['Fairness', 'Data Protection', 'GDPR', 'Linguistic Comparison']","['Applied computing _ Law', 'social and behavioral sciences']",['Gianclaudio Malgieri'],"['Vrije Universiteit Brussel, Brissels']",['Belgium']
https://dl.acm.org/doi/abs/10.1145/3351095.3372856,Transparency & Explainability,"Regulating Transparency? Facebook, Twitter and the German Network Enforcement Act","Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns --- design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG. This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.",[],[],"['Ben Wagner', 'Krisztina Rozgonyi', 'Marie-Therese Sekwenz', 'Jennifer Cobbe', 'Jatinder Singh']","['Vienna University of Economics and Business', 'University of Vienna', 'Vienna University of Economics and Business', 'University of Cambridge', 'University of Cambridge']","[None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372852,Transparency & Explainability,Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.","['decision support', 'trust', 'conidence', 'explainable AI']",[],"['Yunfeng Zhang', 'Q. Vera Liao', 'Rachel K. E. Bellamy']","['IBM Research AI', 'IBM Research AI', 'IBM Research AI']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372829,Transparency & Explainability,Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning,"A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.","['datasets', 'machine learning', 'ML fairness', 'data collection', 'sociocultural data', 'archives']",['Computing methodologies _ Machine learning'],"['Eun Seo Jo', 'Timnit Gebru']","['Stanford University', 'Google']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372832,Transparency & Explainability,Integrating FATE/Critical Data Studies into Data Science curricula: where are we going and how do we get there?,"There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.","['data science', 'FATE', 'critical data studies', 'higher education']",['Social and professional topics _ Computing education'],"['Jo Bates', 'David Cameron', 'Alessandro Checco', 'Paul Clough', 'Frank Hopfgartner', 'Suvodeep Mazumdar', 'Laura Sbaffi', 'Peter Stordy', 'Antonio de la Vega de León']","['University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield,  and Peak Indicators, Chesterfield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield']","['UK', 'UK', 'UK', 'UK', 'UK', 'UK', 'UK', 'UK', 'UK']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372866,Transparency & Explainability,Recommendations and User Agency: The Reachability of Collaboratively-Filtered Information,"Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.",[],[],"['Sarah Dean', 'Sarah Rich', 'Benjamin Recht']","['UC Berkeley', 'Canopy Crest', 'UC Berkeley']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372849,Transparency & Explainability,"What does it mean to ‘solve’ the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems","Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.","['Socio-technical systems', 'automated hiring', 'algorithmic decisionmaking', 'fairness', 'discrimination', 'GDPR', 'social justice']","['Social and professional topics _ Socio-technical systems', 'Systems analysis and design', 'Applied computing _ Law', 'Sociology']","['Javier Sánchez-Monedero', 'Lina Dencik', 'Lilian Edwards']","['Cardiff University, Cardiff, Wales', 'Cardiff University, Cardiff, Wales', 'University of Newcastle, Newcastle upon Tyne, England']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372835,Transparency & Explainability,The Case for Voter-Centered Audits of Search Engines during Political Elections,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['algorithm audits', 'search engines', 'Google', 'voters', 'elections', 'bias']",['Information systems _ Web search engines'],"['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372841,Transparency & Explainability,Whose Tweets are Surveilled for the Police: An Audit of a Social-Media Monitoring Tool via Log Files,"Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement. As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.","['social media monitoring', 'surveillance', 'police', 'demographics', 'keywords', 'audit']","['Social and professional topics _ Governmental surveillance', 'Corporate surveillance', 'Race and ethnicity', 'General and reference _ Empirical studies']","['Glencora Borradaile', 'Brett Burkhardt', 'Alexandria LeClerc']","['Oregon State University', 'Oregon State University', 'Oregon State University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372825,Transparency & Explainability,Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability,"Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community. In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps: Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*) ~ Dir(α). Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y. Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i ~ Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels. Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction. Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality. Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.","['auditability', 'accuracy', 'uncertainty', 'black-box models', 'machine learning']",[],"['José Mena Roldán', 'Oriol Pujol Vila', 'Jordi Vitrià Marca']","['Universitat de Barcelona', 'Universitat de Barcelona', 'Universitat de Barcelona']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375234,Transparency & Explainability,Model Agnostic Interpretability of Text Rankers via Intent Modelling,"A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models. We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.",[],[],"['Jaspreet Singh', 'Avishek Anand']","['L3S Research Center, Hannover', 'L3S Research Center, Hannover']","['Germany', 'Germany']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372855,Transparency & Explainability,Doctor XAI: An ontology-based approach to black-box sequential data classification explanations,"Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.","['explainable artificial intelligence', 'machine learning', 'healthcare data']","['Computing methodologies _ Artificial intelligence', 'Machine learning', 'Applied computing _ Health care information systems']","['Cecilia Panigutti', 'Alan Perotti', 'Dino Pedreschi']","['Scuola Normale Superiore', 'ISI foundation', 'University of Pisa']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375624,Transparency & Explainability,Explainable Machine Learning in Deployment,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.",[],"['Human-centered computing', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning']","['Umang Bhatt', 'Alice Xiang', 'Shubham Sharma', 'Adrian Weller', 'Ankur Taly', 'Yunhan Jia', 'Joydeep Ghosh', 'Ruchir Puri', 'José M. F. Moura', 'Peter Eckersley']","['Carnegie Mellon University and Partnership on AI and University of Cambridge and Leverhulme CFI', 'Partnership on AI', 'University of Texas at Austin', 'University of Cambridge and Leverhulme CFI and The Alan Turing Institute', 'Fiddler Labs', 'Baidu', 'University of Texas at Austin and CognitiveScale', 'IBM Research', 'Carnegie Mellon University', 'Partnership on AI']","[None, None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372874,Fairness & Bias,Toward Situated Interventions for Algorithmic Equity: Lessons from the Field,"Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is ""scalable"" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.",[],[],"['Michael Katell', 'Meg Young', 'Dharma Dailey', 'Bernease Herman', 'Vivian Guetler', 'Aaron Tam', 'Corinne Bintz', 'Daniella Raz', 'P. M. Krafft']","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington', 'West Virginia University', 'University of Washington', 'Middlebury College', 'University of Michigan', 'University of Oxford and University of Washington']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372827,Fairness & Bias,The Human Body is a Black Box: Supporting Clinical Decision-Making with Deep Learning,"Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.","['Deep learning', 'Interpretability', 'Medicine', 'Trust', 'Expertise']","['Computing methodologies _ Machine learning', 'Human-centered computing _ Field study', 'Social and professional topics _ Government technology policy']","['Mark Sendak', 'Madeleine Clare Elish', 'Michael Gao', 'Joseph Futoma', 'William Ratliff', 'Marshall Nichols', 'Armando Bedoya', 'Suresh Balu', ""Cara O'Brien""]","['Duke Institute for Health Innovation', 'Data & Society Research Institute', 'Duke Institute for Health Innovation', 'Harvard University and Duke University', 'Duke Institute for Health Innovation', 'Duke Institute for Health Innovation', 'Duke School of Medicine', 'Duke Institute for Health Innovation', 'Duke School of Medicine']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3373154,Fairness & Bias,Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination,"The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.",[],[],"['Nathan Kallus', 'Xiaojie Mao', 'Angela Zhou']","['Cornell University', 'Cornell University', 'Cornell University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372845,Fairness & Bias,FlipTest: Fairness Testing via Optimal Transport,"We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.","['fairness', 'machine learning', 'optimal transport', 'disparate impact']","['Computing methodologies _ Machine learning', 'Human centered computing']","['Emily Black', 'Samuel Yeom', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372867,Fairness & Bias,"Implications of AI (Un-)Fairness in Higher Education Admissions: The Effects of Perceived AI (Un-)Fairness on Exit, Voice and Organizational Reputation","Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.","['Distributive Fairness', 'Procedural Fairness', 'Artificial Intelligence', 'Algorithmic Decision Making', 'Higher Education Systems', 'Reputation', 'Voice', 'Exit']","['Human-centered computing _ Empirical studies in HCI', 'Applied Computing _ Sociology', 'Applied Computing _ Education']","['Frank Marcinkowski', 'Kimon Kieslich', 'Christopher Starke', 'Marco Lünich']","['University of Düsseldorf', 'University of Düsseldorf', 'University of Düsseldorf', 'University of Düsseldorf']","['Germany', 'Germany', 'Germany', 'Germany']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372863,Fairness & Bias,Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions,"The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.",[],"['Applied computing _ Computing in government', 'Law', 'social and behavioral sciences', 'Social and professional topics _ Computing / technology policy', 'Human-centered computing _ Interaction design', 'Computing methodologies _ Machine learning']","['Kit T. Rodolfa', 'Erika Salomon', 'Lauren Haynes', 'Iván Higuera Mendieta', 'Jamie Larson', 'Rayid Ghani']","['Carnegie Mellon University', 'University of Chicago', 'University of Chicago', 'University of Chicago', ""Los Angeles City Attorney's Office"", 'Carnegie Mellon University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372868,Fairness & Bias,The concept of fairness in the GDPR: a linguistic and contextual interpretation,"There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.","['Fairness', 'Data Protection', 'GDPR', 'Linguistic Comparison']","['Applied computing _ Law', 'social and behavioral sciences']",['Gianclaudio Malgieri'],"['Vrije Universiteit Brussel, Brissels']",['Belgium']
https://dl.acm.org/doi/abs/10.1145/3351095.3372859,Fairness & Bias,Studying Up: Reorienting the study of algorithmic fairness around issues of power,"Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.",[],[],"['Chelsea Barabas', 'Colin Doyle', 'JB Rubinovitz', 'Karthik Dinakar']","['Massachusetts Institute of Technology', 'Harvard Law School', 'Massachusetts Institute of Technology', 'Harvard Law School']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372853,Fairness & Bias,POTs: Protective Optimization Technologies,"Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.","['Fairness and Accountability', 'Protective Optimization Technologies']",['Social and professional topics _ Socio-technical systems'],"['Bogdan Kulynych', 'Rebekah Overdorf', 'Carmela Troncoso', 'Seda Gürses']","['EPFL', 'EPFL', 'EPFL', 'TU Delft / KU Leuven']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372872,Fairness & Bias,Fair Decision Making using Privacy-Protected Data,"Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem. Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.",[],[],"['David Pujol', 'Ryan McKenna', 'Satya Kuppam', 'Michael Hay', 'Ashwin Machanavajjhala', 'Gerome Miklau']","['Duke University', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Colgate University', 'Duke University', 'University of Massachusetts, Amherst']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372839,Fairness & Bias,Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data,"Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.","['machine learning', 'fairness', 'meta-learning', 'covariate shift']",['Computing methodologies _ Machine learning'],"['Dylan Slack', 'Sorelle A. Friedler', 'Emile Givental']","['University of California, Irvine', 'Haverford College', 'Haverford College']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372871,Fairness & Bias,Roles for Computing in Social Change,"A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.","['social change', 'inequality', 'discrimination', 'societal implications of AI']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Computers in other domains']","['Rediet Abebe', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy', 'Manish Raghavan', 'David G. Robinson']","['Harvard University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Cornell University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372865,Fairness & Bias,Data in New Delhi's predictive policing system,"In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.","['Fairness-Aware Machine Learning', 'Predictive Policing', 'Interdisciplinary', 'Sociotechnical systems']","['Social and professional topics _ Governmental regulations', 'Race and ethnicity', 'Cultural characteristics', 'Computing methodologies _ Reasoning about belief and knowledge']","['Vidushi Marda', 'Shivangi Narayan']","['Article 19', 'Jawaharlal Nehru University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375783,Fairness & Bias,Bidding Strategies with Gender Nondiscrimination Constraints for Online Ad Auctions,"Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.","['targeted advertising', 'online auctions', 'fairness constraints', 'MDPs']",['Information systems _ Online advertising'],"['Milad Nasr', 'Michael Carl Tschantz']","['University of Massachusetts Amherst', 'International Computer Science Institute']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372848,Fairness & Bias,Multi-category Fairness in Sponsored Search Auctions,"Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the ""platform utility"" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.","['algorithmic fairness', 'advertisement auctions', 'utility', 'individual fairness', 'envy-freeness']",['Theory of Computation _ Algorithmic Mechanism Design'],"['Christina Ilvento', 'Meena Jagadeesan', 'Shuchi Chawla']","['Harvard University', 'Harvard University', 'University of Wisconsin-Madison']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372837,Fairness & Bias,Reducing Sentiment Polarity for Demographic Attributes in Word Embeddings using Adversarial Learning,"The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.","['embeddings', 'fairness', 'NLP']",['Computing methodologies _ Natural language processing'],"['Chris Sweeney', 'Maryam Najafian']","['MIT', 'MIT']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372858,Fairness & Bias,Interventions for Ranking in the Presence of Implicit Bias,"Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.","['Implicit bias', 'ranking', 'algorithmic fairness', 'interventions']","['Information systems _ Content ranking', 'Mathematics of computing _ Probability and statistics']","['L. Elisa Celis', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'IIT Kanpur', 'Yale University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372831,Fairness & Bias,"An Empirical Study on the Perceived Fairness of Realistic, Imperfect Machine Learning Models","There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model ""unbiased"" and considering it ""fair."" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.","['Fairness', 'Accountability', 'Machine Learning', 'Survey', 'Data Science']","['Human-centered computing _ Empirical studies in interaction design', 'Computing methodologies _ Machine learning']","['Galen Harrison', 'Julia Hanson', 'Christine Jacinto', 'Julio Ramirez', 'Blase Ur']","['University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago']","[None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375623,Fairness & Bias,Artificial mental phenomena: Psychophysics as a framework to detect perception biases in AI models,"Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.","['Biases in word embeddings', 'Biases in Sentiment Analysis', 'Artificial Psychophysics', 'Two-alternative forced choice task']",[],"['Lizhen Liang', 'Daniel E. Acuna']","['Syracuse University', 'Syracuse University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3373156,Fairness & Bias,The Social Lives of Generative Adversarial Networks,"Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled. Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus---a ""durably installed generative principle of regulated improvisations""---that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill ""deeply interiorized master patterns"" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development. In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because ""sometimes we don't follow the rules... language is full of exceptions to the rules""; and in the case of Bourdieu, the habitus was an answer to a long-standing question: ""how can behaviour be regulated without being the product of obedience to rules?"" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency. Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations---or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives. Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a ""two-player minimax game with value function V(G,D)"", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, ""the degree zero of sociology"", by which he means an isolated, inert, and amodal---and therefore not particularly sociological---starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and ""selling out"" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the ""value functions"" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.","['generative adversarial networks', 'sociological theory', 'habitus', 'bias', 'game theory']","['Applied computing _ Sociology', 'Computer systems organization _ Neural networks']",['Michael Castelle'],['University of Warwick'],[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3372832,Fairness & Bias,Integrating FATE/Critical Data Studies into Data Science curricula: where are we going and how do we get there?,"There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.","['data science', 'FATE', 'critical data studies', 'higher education']",['Social and professional topics _ Computing education'],"['Jo Bates', 'David Cameron', 'Alessandro Checco', 'Paul Clough', 'Frank Hopfgartner', 'Suvodeep Mazumdar', 'Laura Sbaffi', 'Peter Stordy', 'Antonio de la Vega de León']","['University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield,  and Peak Indicators, Chesterfield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield', 'University of Sheffield, Sheffield']","['UK', 'UK', 'UK', 'UK', 'UK', 'UK', 'UK', 'UK', 'UK']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372843,Fairness & Bias,Bias in word embeddings,"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.","['word embeddings', 'bias', 'detection', 'diffusion', 'mitigation', 'fairness', 'sexism', 'racism', 'homophobia']","['Human-centered computing _ HCI design and evaluation methods', 'Computing methodologies _ Machine learning', 'Information systems _ Data mining']","['Orestis Papakyriakopoulos', 'Simon Hegelich', 'Juan Carlos Medina Serrano', 'Fabienne Marco']","['Technical University of Munich, Munich', 'Technical University of Munich, Munich', 'Technical University of Munich, Munich', 'Technical University of Munich, Munich']","['Germany', 'Germany', 'Germany', 'Germany']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372849,Fairness & Bias,"What does it mean to ‘solve’ the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems","Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.","['Socio-technical systems', 'automated hiring', 'algorithmic decisionmaking', 'fairness', 'discrimination', 'GDPR', 'social justice']","['Social and professional topics _ Socio-technical systems', 'Systems analysis and design', 'Applied computing _ Law', 'Sociology']","['Javier Sánchez-Monedero', 'Lina Dencik', 'Lilian Edwards']","['Cardiff University, Cardiff, Wales', 'Cardiff University, Cardiff, Wales', 'University of Newcastle, Newcastle upon Tyne, England']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372828,Fairness & Bias,Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices,"There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.","['algorithmic hiring', 'discrimination law', 'algorithmic bias']","['Social and professional topics_Employment issues',� Computing methodologies _ Machine learning', 'Applied computing _ Law']","['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy']","['Cornell University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372877,Fairness & Bias,Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination,"Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted. This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities. This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.",[],[],"['Miranda Bogen', 'Aaron Rieke', 'Shazeda Ahmed']","['Upturn', 'Upturn', 'University of California']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372826,Fairness & Bias,Towards a Critical Race Methodology in Algorithmic Fairness,"We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.","['algorithmic fairness', 'critical race theory', 'race and ethnicity']","['Applied computing _ Sociology', 'Social and professional topics _ Race and ethnicity']","['Alex Hanna', 'Emily Denton', 'Andrew Smart', 'Jamila Smith-Loud']","['', '', '', '']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375674,Fairness & Bias,What’s Sex Got to Do With Fair Machine Learning?,"The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world. We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature. Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-à-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.","['machine learning', 'algorithmic fairness', 'causal inference', 'discrimination', 'law', 'social philosophy']","['Theory of computation_Design and analysis of algorithms', 'Applied computing _ Law', 'social and behavioral sciences']","['Lily Hu', 'Issa Kohler-Hausmann']","['Harvard University', 'Yale University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372864,Fairness & Bias,On the Apparent Conflict Between Individual and Group Fairness,"A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.","['fairness', 'individual fairness', 'justice', 'machine learning', 'discrimination', 'statistical parity']","['Social and professional topics _ Computing / technology policy', 'Codes of ethics', 'Computing methodologies _ Machine learning', 'Applied computing _ Law', 'Sociology']",['Reuben Binns'],['University of Oxford'],[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3372857,Fairness & Bias,Fair Classification and Social Welfare,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.",[],[],"['Lily Hu', 'Yiling Chen']","['Harvard University', 'Harvard University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3373155,Fairness & Bias,Preference-Informed Fairness,"In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome. We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].",[],[],"['Michael P. Kim', 'Aleksandra Korolova', 'Guy N. Rothblum', 'Gal Yona']","['Stanford University', 'University of Southern California', 'Weizmann Institute of Science', 'Weizmann Institute of Science']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372835,Fairness & Bias,The Case for Voter-Centered Audits of Search Engines during Political Elections,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['algorithm audits', 'search engines', 'Google', 'voters', 'elections', 'bias']",['Information systems _ Web search engines'],"['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372841,Fairness & Bias,Whose Tweets are Surveilled for the Police: An Audit of a Social-Media Monitoring Tool via Log Files,"Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement. As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.","['social media monitoring', 'surveillance', 'police', 'demographics', 'keywords', 'audit']","['Social and professional topics _ Governmental surveillance', 'Corporate surveillance', 'Race and ethnicity', 'General and reference _ Empirical studies']","['Glencora Borradaile', 'Brett Burkhardt', 'Alexandria LeClerc']","['Oregon State University', 'Oregon State University', 'Oregon State University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372851,Fairness & Bias,"Counterfactual Risk Assessments, Evaluation, and Fairness","Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.",[],[],"['Amanda Coston', 'Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372869,Fairness & Bias,The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness,"Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an ""epistemic reform,"" the path forward for criminal justice reform. I reinterpret recent results regarding the ""impossibility of fairness"" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how ""fair"" algorithms can reinforce discrimination.","['risk assessment', 'criminal justice system', 'fairness', 'social justice']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Law']",['Ben Green'],['Harvard University'],[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3372842,Fairness & Bias,The Effects of Competition and Regulation on Error Inequality in Data-Driven Markets,"Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.","['learning theory', 'algorithmic fairness', 'data markets', 'game theory', 'industrial organization', 'economics']","['Theory of computation _ Market equilibria', 'Machine learning theory', 'Sample complexity and generalization bounds', 'Quality of equilibria', 'Applied computing _ Economics']","['Hadi Elzayn', 'Benjamin Fish']","['University of Pennsylvania', 'Microsoft Research']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372838,Fairness & Bias,Measuring Justice in Machine Learning,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",[],[],['Alan Lundgard'],"['Massachusetts Institute of Technology Cambridge, Massachusetts']",[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3373153,Privacy & Data Governance,Algorithmic Accountability in Public Administration: The GDPR Paradox,"The EU General Data Protection Regulation (""GDPR"") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders. With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration. Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation. The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.","['General Data Protection Regulation (GDPR)', 'Algorithmic Accountability', 'Automated Decisions', 'Predictive Enforcement Tools', 'Due Process']","['Security and privacy', 'Human-centered computing', 'Social and professional topics']",['Sunny Seon Kang'],['Inpher'],[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3372868,Privacy & Data Governance,The concept of fairness in the GDPR: a linguistic and contextual interpretation,"There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal. This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation. In terms of linguistic comparison, the paper analyses all translations of the world ""fair"" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law. The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive). In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter) In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version). The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of ""Treu und Glaube"") and equitability (French, Spanish and Portuguese). Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of ""bona fide"". Taking into account both the value of ""bona fide"" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects. The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of ""vulnerability"". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR. In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.","['Fairness', 'Data Protection', 'GDPR', 'Linguistic Comparison']","['Applied computing _ Law', 'social and behavioral sciences']",['Gianclaudio Malgieri'],"['Vrije Universiteit Brussel, Brissels']",['Belgium']
https://dl.acm.org/doi/abs/10.1145/3351095.3372872,Privacy & Data Governance,Fair Decision Making using Privacy-Protected Data,"Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem. Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.",[],[],"['David Pujol', 'Ryan McKenna', 'Satya Kuppam', 'Michael Hay', 'Ashwin Machanavajjhala', 'Gerome Miklau']","['Duke University', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Colgate University', 'Duke University', 'University of Massachusetts, Amherst']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372844,Privacy & Data Governance,"Whose Side are Ethics Codes On? Power, Responsibility and the Social Good","The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the ""social good"", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..","['ethics codes', 'social movements', 'digital differential vulnerability', 'digital vulnerability', 'data science', 'public interest technology']","['codes of ethics', 'computing profession', 'machine learning']","['Anne L. Washington', 'Rachel Kuo']","['New York University', 'New York University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372849,Privacy & Data Governance,"What does it mean to ‘solve’ the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems","Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.","['Socio-technical systems', 'automated hiring', 'algorithmic decisionmaking', 'fairness', 'discrimination', 'GDPR', 'social justice']","['Social and professional topics _ Socio-technical systems', 'Systems analysis and design', 'Applied computing _ Law', 'Sociology']","['Javier Sánchez-Monedero', 'Lina Dencik', 'Lilian Edwards']","['Cardiff University, Cardiff, Wales', 'Cardiff University, Cardiff, Wales', 'University of Newcastle, Newcastle upon Tyne, England']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372870,Security,Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","['Explainability', 'Interpretability', 'Transparency', 'Fact Sheet', 'Work Sheet', 'Desiderata', 'Taxonomy', 'AI', 'ML']","['General and reference _ Evaluation', 'Computing methodologies _ Artificial intelligence', 'Machine learning']","['Kacper Sokol', 'Peter Flach']","['University of Bristol, Bristol', 'University of Bristol, Bristol']","['United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372837,Security,Reducing Sentiment Polarity for Demographic Attributes in Word Embeddings using Adversarial Learning,"The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.","['embeddings', 'fairness', 'NLP']",['Computing methodologies _ Natural language processing'],"['Chris Sweeney', 'Maryam Najafian']","['MIT', 'MIT']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372835,Security,The Case for Voter-Centered Audits of Search Engines during Political Elections,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['algorithm audits', 'search engines', 'Google', 'voters', 'elections', 'bias']",['Information systems _ Web search engines'],"['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","[None, None, None]"