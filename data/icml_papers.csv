link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://openreview.net/forum?id=gE7qZurGH3,Fairness & Bias,Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching,"Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward *lossless graph condensation* by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching. Moreover, we design a loss function to further extract knowledge from the expert trajectories. Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets. Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM.",[],,"['Yuchen Zhang', 'Tianle Zhang', 'Kai Wang', 'Ziyao Guo', 'Yuxuan Liang', 'Xavier Bresson', 'Wei Jin', 'Yang You']","['Shanghai Artificial Intelligence Laboratory', 'NTU, Nanyang Technological University', 'soc, national university of singaore, National University of Singapore', 'School of Computing, national university of singaore, National University of Singapore', 'INTR & DSA, The Hong Kong University of Science and Technology (Guangzhou)', 'National University of Singapore', 'Emory University', 'Computer Science, National University of Singapore']",
https://openreview.net/forum?id=Ln3moCobjO,Fairness & Bias,Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering,"Recommender system aims to recommend items or information that may interest users based on their behaviors and preferences. However, there may be sampling selection bias in the data collection process, i.e., the collected data is not a representative of the target population. Many debiasing methods are developed based on pseudo-labelings. Nevertheless, the validity of these methods relies heavily on accurate pseudo-labelings (i.e., the imputed labels), which is difficult to satisfy in practice. In this paper, we theoretically propose several novel doubly robust estimators that are unbiased when either (a) the pseudo-labelings deviate from the true labels with an arbitrary user-specific inductive bias, item-specific inductive bias, or a combination of both, or (b) the learned propensities are accurate. We further propose a propensity reconstruction learning approach that adaptively updates the constraint weights using an attention mechanism and effectively controls the variance. Extensive experiments show that our approach outperforms the state-of-the-art on one semi-synthetic and three real-world datasets.",[],,"['Haoxuan Li', 'Chunyuan Zheng', 'Shuyi Wang', 'Kunhan Wu', 'Eric Wang', 'Peng Wu', 'Zhi Geng', 'Xu Chen', 'Xiao-Hua Zhou']","['Center for Data Science, Peking University', 'meituan', 'University of Pennsylvania, University of Pennsylvania', 'Carnegie Mellon University', '', 'Beijing Technology and Business University', 'School of Matematics asn Statistics, Beijing Technology and Business University', 'AI, Renmin University of China', 'School of mathematical Science, Peking University, Peking University']",
https://openreview.net/forum?id=2dlmcTXfcY,Fairness & Bias,Kernel-Based Evaluation of Conditional Biological Sequence Models,"We propose a set of kernel-based tools to evaluate the designs and tune the hyperparameters of conditional sequence models, with a focus on problems in computational biology. The backbone of our tools is a new measure of discrepancy between the true conditional distribution and the model's estimate, called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided that the model can be sampled from, the ACMMD can be estimated unbiasedly from data to quantify absolute model fit, integrated within hypothesis tests, and used to evaluate model reliability. We demonstrate the utility of our approach by analyzing a popular protein design model, ProteinMPNN. We are able to reject the hypothesis that ProteinMPNN fits its data for various protein families, and tune the model's temperature hyperparameter to achieve a better fit.",[],,"['Pierre Glaser', 'Steffanie Paul', 'Alissa M Hummer', 'Charlotte Deane', 'Debora Susan Marks', 'Alan Nawzad Amin']","['Gatsby Computational Neuroscience Unit, University College London', 'Harvard University', 'University of Oxford', 'Department of Statistics, University of Oxford', 'Harvard University', 'New York University']",
https://openreview.net/forum?id=gPStP3FSY9,Fairness & Bias,Discovering Environments with XRM,"Environment annotations are essential for the success of many out-of-distribution (OOD) generalization methods. Unfortunately, these are costly to obtain and often limited by human annotators' biases. To achieve robust generalization, it is essential to develop algorithms for automatic environment discovery within datasets. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods introduce hyper-parameters and early-stopping criteria, which require a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk Minimization (XRM) to address this issue. XRM trains twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Algorithms built on top of XRM environments achieve oracle worst-group-accuracy, addressing a long-standing challenge in OOD generalization.",[],,"['Mohammad Pezeshki', 'Diane Bouchacourt', 'Mark Ibrahim', 'Nicolas Ballas', 'Pascal Vincent', 'David Lopez-Paz']","['Facebook', 'Facebook AI Research', 'Facebook AI Research (FAIR) Meta', 'Meta', 'University of Montreal', 'Facebook']",
https://openreview.net/forum?id=GHZVjmaGQM,Transparency & Explainability,Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response,"Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: *ranking* of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a *causal loss* that we combine with the standard predictive loss to arrive at a *hybrid loss* that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance *and* causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes.",[],,"['Bob Junyi Zou', 'Matthew E Levine', 'Dessi P. Zaharieva', 'Ramesh Johari', 'Emily Fox']","['ICME, Stanford University', 'California Institute of Technology', 'Pediatrics, Stanford University', 'Management Science and Engineering, Stanford University', 'Inistro Inc.']",
https://openreview.net/forum?id=XsDWw1Mn2p,Transparency & Explainability,How Learning by Reconstruction Produces Uninformative Features For Perception,"Input space reconstruction is an attractive representation learning paradigm. Despite interpretability benefit of reconstruction and generation, we identify a misalignment between learning to reconstruct, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90% of the pixel variance can be solved with 45% test accuracy. Using the bottom subspace instead, accounting for only 20% of the pixel variance, reaches 55% test accuracy. Learning by reconstruction is also wasteful as the features for perception are learned last, pushing the need for long training schedules. We finally prove that learning by denoising can alleviate that misalignment for some noise strategies, e.g., masking. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide a solution to detect if a noise strategy is never beneficial regardless of the perception task, e.g., additive Gaussian noise.",[],,"['Randall Balestriero', 'Yann LeCun']","['Computer Science, Brown University', 'Facebook']",
https://openreview.net/forum?id=jTn4AIOgpM,Fairness & Bias,Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference,"Deep Gaussian processes (DGPs) provide a robust paradigm in Bayesian deep learning. In DGPs, a set of sparse integration locations called inducing points are selected to approximate the posterior distribution of the model. This is done to reduce computational complexity and improve model efficiency. However, inferring the posterior distribution of inducing points is not straightforward. Traditional variational inference techniques methods to approximate the posterior often leads to significant bias. To address this issue, we propose an alternative named Denoising Diffusion Variational Inference (DDVI) that utilizes a denoising diffusion stochastic differential equation (SDE) for generating posterior samples of inducing variables. We refer to the score matching method in the denoising diffusion model to approximate challenging score functions using a neural network. Furthermore, by combining classical mathematical theory of SDE with the minimization of KL divergence between the approximate and true processes, we propose a novel explicit variational lower bound for the marginal likelihood function of DGP. Through extensive experiments on various datasets and comparisons with baseline methods, we empirically demonstrate the effectiveness of the DDVI method in posterior inference of inducing points for DGP models.",[],,"['JIAN XU', 'Delu Zeng', 'John Paisley']","['South China University of Technology', 'School  of Electronics and Information Processing, South China University of Technology', 'Columbia University']",
https://openreview.net/forum?id=s4EYBJ30WY,Fairness & Bias,"Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance","Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pre-trained encoders, and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation strategies to mitigate the issue. Our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pre-trained features may have unknown biases that can be diagnosed through their spectra.",[],,"['Chiraag Kaushik', 'Ran Liu', 'Chi-Heng Lin', 'Amrit Khera', 'Matthew Y Jin', 'Wenrui Ma', 'Vidya Muthukumar', 'Eva L Dyer']","['Georgia Institute of Technology', 'Apple', 'Samsung Research America', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Machine Learning, Georgia Institute of Technology']",
https://openreview.net/forum?id=jU6iPouOZ6,Transparency & Explainability,From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation,"Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.",[],,"['Kun Su', 'Xiulong Liu', 'Eli Shlizerman']","['Google', 'University of Washington, Seattle', 'Electrical and computer engineering , University of Washington']",
https://openreview.net/forum?id=upO8FUwf92,Transparency & Explainability,Towards Compositionality in Concept Learning,"Concept-based interpretability methods offer a lens into the internals of foundation models by decomposing their embeddings into high-level concepts. These concept representations are most useful when they are *compositional*, meaning that the individual concepts compose to explain the full sample. We show that existing unsupervised concept extraction methods find concepts which are not compositional. To automatically discover compositional concept representations, we identify two salient properties of such representations, and propose Compositional Concept Extraction (CCE) for finding concepts which obey these properties. We evaluate CCE on five different datasets over image and text data. Our evaluation shows that CCE finds more compositional concept representations than baselines and yields better accuracy on four downstream classification tasks.",[],,"['Adam Stein', 'Aaditya Naik', 'Yinjun Wu', 'Mayur Naik', 'Eric Wong']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'Department of Computer and information science, Peking University', 'University of Pennsylvania', 'University of Pennsylvania']",
https://openreview.net/forum?id=8ho1l6RZNB,Security,Image Hijacks: Adversarial Images can Control Generative Models at Runtime,"Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour matching to craft hijacks for four types of attack: forcing VLMs to generate outputs of the adversary’s choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",[],,"['Luke Bailey', 'Euan Ong', 'Stuart Russell', 'Scott Emmons']","['Computer Science, Stanford University', 'University of Cambridge', 'EECS, University of California Berkeley', 'Google DeepMind']",
https://openreview.net/forum?id=GLGYYqPwjy,Fairness & Bias,QuRating: Selecting High-Quality Data for Training Language Models,"Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",[],,"['Alexander Wettig', 'Aatmik Gupta', 'Saumya Malik', 'Danqi Chen']","['Department of Computer Science, Princeton University', 'Computer Science, Princeton University', 'Computer Science, Princeton University', 'Department of Computer Science, Princeton University']",
https://openreview.net/forum?id=5cm2jGct2W,Fairness & Bias,On the Maximal Local Disparity of Fairness-Aware Classifiers,"Fairness has become a crucial aspect in the development of trustworthy machine learning algorithms. Current fairness metrics to measure the violation of demographic parity have the following drawbacks: (i) the *average difference* of model predictions on two groups cannot reflect their *distribution disparity*, and (ii) the *overall* calculation along all possible predictions conceals the *extreme local disparity* at or around certain predictions. In this work, we propose a novel fairness metric called **M**aximal **C**umulative ratio **D**isparity along varying **P**redictions' neighborhood (MCDP), for measuring the maximal local disparity of the fairness-aware classifiers. To accurately and efficiently calculate the MCDP, we develop a provably exact and an approximate calculation algorithm that greatly reduces the computational complexity with low estimation error. We further propose a bi-level optimization algorithm using a differentiable approximation of the MCDP for improving the algorithmic fairness. Extensive experiments on both tabular and image datasets validate that our fair training algorithm can achieve superior fairness-accuracy trade-offs.",[],,"['Jinqiu Jin', 'Haoxuan Li', 'Fuli Feng']","['University of Science and Technology of China', 'Center for Data Science, Peking University', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China']",
https://openreview.net/forum?id=DyvhD8J3Wl,Security,Benign Overfitting in Adversarial Training of Neural Networks,"Benign overfitting is the phenomenon wherein none of the predictors in the hypothesis class can achieve perfect accuracy (i.e., non-realizable or noisy setting), but a model that interpolates the training data still achieves good generalization. A series of recent works aim to understand this phenomenon for regression and classification tasks using linear predictors as well as two-layer neural networks. In this paper, we study such a benign overfitting phenomenon in an adversarial setting. We show that under a distributional assumption, interpolating neural networks found using adversarial training generalize well despite inference-time attacks. Specifically, we provide convergence and generalization guarantees for adversarial training of two-layer networks (with smooth as well as non-smooth activation functions) showing that under moderate $\ell_2$ norm perturbation budget, the trained model has near-zero robust training loss and near-optimal robust generalization error. We support our theoretical findings with an empirical study on synthetic and real-world data.",[],,"['Yunjuan Wang', 'Kaibo Zhang', 'Raman Arora']","['Computer Science Department, Johns Hopkins University', 'Department of Computer Science, Whiting School of Engineering', 'Computer Science, Johns Hopkins University']",
https://openreview.net/forum?id=6jmdOTRMIO,Security,Scalable AI Safety via Doubly-Efficient Debate,"The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al (2018). proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate *deterministic* AI systems for an *exponential* number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a *polynomial* number of steps, whilst being able to verify the alignment of *stochastic* AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.",[],,"['Jonah Brown-Cohen', 'Geoffrey Irving', 'Georgios Piliouras']","['Google DeepMind', 'UK AI Safety Institute', 'Google DeepMind']",
https://openreview.net/forum?id=T0zR4mdSce,Fairness & Bias,PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling,"Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested on both standard benchmark problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials. We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes.",[],,"['Phong C.H. Nguyen', 'Xinlun Cheng', 'Shahab Azarfar', 'Pradeep Seshadri', 'Yen T. Nguyen', 'Munho Kim', 'Sanghun Choi', 'H.S. Udaykumar', 'Stephen Baek']","['School of Data Science, University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']",
https://openreview.net/forum?id=lQzmDFlsHX,Fairness & Bias,Unsupervised Concept Discovery Mitigates Spurious Correlations,"Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring subgroups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the benchmark datasets for sub-population shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation. Code is available at https://github.com/rarefin/CoBalT",[],,"['Md Rifat Arefin', 'Yan Zhang', 'Aristide Baratin', 'Francesco Locatello', 'Irina Rish', 'Dianbo Liu', 'Kenji Kawaguchi']","['Université de Montréal', 'Samsung - SAIT AI Lab, Montreal', '', 'Institute of Science and Technology', 'DIRO, University of Montreal', 'National University of Singapore', 'National University of Singapore']",
https://openreview.net/forum?id=AocOA4h3bu,Fairness & Bias,Sequential Disentanglement by Extracting Static Information From A Single Sequence Element,"One of the fundamental representation learning tasks is unsupervised sequential disentanglement, where latent codes of inputs are decomposed to a single static factor and a sequence of dynamic factors. To extract this latent information, existing methods condition the static and dynamic codes on the entire input sequence. Unfortunately, these models often suffer from information leakage, i.e., the dynamic vectors encode both static and dynamic information, or vice versa, leading to a non-disentangled representation. Attempts to alleviate this problem via reducing the dynamic dimension and auxiliary loss terms gain only partial success. Instead, we propose a novel and simple architecture that mitigates information leakage by offering a simple and effective subtraction inductive bias while conditioning on a single sample. Remarkably, the resulting variational framework is simpler in terms of required loss terms, hyper-parameters, and data augmentation. We evaluate our method on multiple data-modality benchmarks including general time series, video, and audio, and we show beyond state-of-the-art results on generation and prediction tasks in comparison to several strong baselines.",[],,"['Nimrod Berman', 'Ilan Naiman', 'Idan Arbiv', 'Gal Fadlon', 'Omri Azencot']","['Ben-Gurion University of the Negev', 'Amazon', 'Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev']",
https://openreview.net/forum?id=sT7UJh5CTc,Privacy & Data Governance,Low-Cost High-Power Membership Inference Attacks,"Membership inference attacks aim to detect if a particular data point was used in training a model. We design a novel statistical test to perform robust membership inference attacks (RMIA) with low computational overhead. We achieve this by a fine-grained modeling of the null hypothesis in our likelihood ratio tests, and effectively leveraging both reference models and reference population data samples. RMIA has superior test power compared with prior methods, throughout the TPR-FPR curve (even at extremely low FPR, as low as 0). Under computational constraints, where only a limited number of pre-trained reference models (as few as 1) are available, and also when we vary other elements of the attack (e.g., data distribution), our method performs exceptionally well, unlike prior attacks that approach random guessing. RMIA lays the groundwork for practical yet accurate data privacy risk assessment in machine learning.",[],,"['Sajjad Zarifzadeh', 'Philippe Liu', 'Reza Shokri']","['Yazd University', 'national university of singaore, National University of Singapore', '']",
https://openreview.net/forum?id=LyJ85kgHFe,Security,$\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts,"Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose $\texttt{MoE-RBench}$, the first comprehensive assessment of SMoE reliability from three aspects: $\textit{(i)}$ safety and hallucination, $\textit{(ii)}$ resilience to adversarial attacks, and $\textit{(iii)}$ out-of-distribution robustness. Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability. Codes are available at https://github.com/UNITES-Lab/MoE-RBench.",[],,"['Guanjie Chen', 'Xinyu Zhao', 'Tianlong Chen', 'Yu Cheng']","['Shanghai Jiaotong University', 'University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'The Chinese University of Hong Kong']",
https://openreview.net/forum?id=pgI9inG2Ny,Security,Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error,"Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of Bellman Infinity-error. The top-tier performance of CAR-DQN across various benchmarks validates its practical effectiveness and reinforces the soundness of our theoretical analysis.",[],,"['Haoran Li', 'Zicheng Zhang', 'Wang Luo', 'Congying Han', 'Yudong Hu', 'Tiande Guo', 'Shichen Liao']","['School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'JD.com', 'School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'Scool of Mathematical Sciences, University of the Chinese Academy of Sciences', 'Mathematics, Beijing Normal University']",
https://openreview.net/forum?id=wuQ2DRPAuy,Privacy & Data Governance,Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning,"High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients' privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients' model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients' aggregation weights proportional to their privacy parameters ($\epsilon$) will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients' model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter $\epsilon$ to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here: https://github.com/Saber-mm/HDPFL.git",[],,"['Saber Malekmohammadi', 'Yaoliang Yu', 'YANG CAO']","['University of Waterloo', 'University of Waterloo', 'Tokyo Institute of Technology']",
https://openreview.net/forum?id=0tPBk24xNj,Security,Adversarial Attacks on Combinatorial Multi-Armed Bandits,"We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, a notion to capture the vulnerability and robustness of CMAB. The attackability condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path.",[],,"['Rishab Balasubramanian', 'Jiawei Li', 'Prasad Tadepalli', 'Huazheng Wang', 'Qingyun Wu', 'Haoyu Zhao']","['Oregon State University', 'IIIS, Tsinghua University', 'Oregon State University', 'Oregon State University', 'Pennsylvania State University', 'Princeton University']",
https://openreview.net/forum?id=ZUXvpIrz5l,Security,Safe Exploration in Dose Finding Clinical Trials with Heterogeneous Participants,"In drug development, early phase dose-finding clinical trials are carried out to identify an optimal dose to administer to patients in larger confirmatory clinical trials. Standard trial procedures do not optimize for participant benefit and do not consider participant heterogeneity, despite consequences to participants' health and downstream impacts to under-represented population subgroups. Many novel drugs also do not obey parametric modelling assumptions made in common dose-finding procedures. We present Safe Allocation for Exploration of Treatments SAFE-T, a procedure for adaptive dose-finding that adheres to safety constraints, improves utility for heterogeneous participants, and works well with small sample sizes. SAFE-T flexibly learns non-parametric multi-output Gaussian process models for dose toxicity and efficacy, using Bayesian optimization, and provides accurate final dose recommendations. We provide theoretical guarantees for the satisfaction of safety constraints. Using a comprehensive set of realistic synthetic scenarios, we demonstrate empirically that SAFE-T generally outperforms comparable methods and maintains performance across variations in sample size and subgroup distribution. Finally, we extend SAFE-T to a new adaptive setting, demonstrating its potential to improve traditional clinical trial procedures.",[],,"['Isabel Chien', 'Wessel P Bruinsma', 'Javier Gonzalez', 'Richard E. Turner']","['University of Cambridge', 'Microsoft Research', 'Microsoft', 'Alan Turing Institute']",
https://openreview.net/forum?id=dBMLtuKH01,Transparency & Explainability,Position: The Causal Revolution Needs Scientific Pragmatism,"Causal models and methods have great promise, but their progress has been stalled. Proposals using causality get squeezed between two opposing worldviews. Scientific perfectionism--an insistence on only using ``correct'' models--slows the adoption of causal methods in knowledge generating applications. Pushing in the opposite direction, the academic discipline of computer science prefers algorithms with no or few assumptions, and technologies based on automation and scalability are often selected for economic and business applications. We argue that these system-centric inductive biases should be replaced with a human-centric philosophy we refer to as scientific pragmatism. The machine learning community must strike the right balance to make space for the causal revolution to prosper.",[],,['Joshua R. Loftus'],['London School of Economics'],
https://openreview.net/forum?id=6VZOONPn8S,Privacy & Data Governance,Disparate Impact on Group Accuracy of Linearization for Private Inference,"Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the finetuning step for linearized models can serve as an effective mitigation strategy.",[],,"['Saswat Das', 'Marco Romanelli', 'Ferdinando Fioretto']","['Department of Computer Science, University of Virginia, Charlottesville', '', 'University of Virginia, Charlottesville']",
https://openreview.net/forum?id=bgP8Rxv2eB,Fairness & Bias,Unbiased Multi-Label Learning from Crowdsourced Annotations,"This work studies the novel Crowdsourced Multi-Label Learning (CMLL) problem, where each instance is related to multiple true labels but the model only receives unreliable labels from different annotators. Although a few Crowdsourced Multi-Label Inference (CMLI) methods have been developed, they require both the training and testing sets to be assigned crowdsourced labels and focus on true label inferring rather than prediction, making them less practical. In this paper, by excavating the generation process of crowdsourced labels, we establish the first **unbiased risk estimator** for CMLL based on the crowdsourced transition matrices. To facilitate transition matrix estimation, we upgrade our unbiased risk estimator by aggregating crowdsourced labels and transition matrices from all annotators while guaranteeing its theoretical characteristics. Integrating with the unbiased risk estimator, we further propose a decoupled autoencoder framework to exploit label correlations and boost performance. We also provide a generalization error bound to ensure the convergence of the empirical risk estimator. Experiments on various CMLL scenarios demonstrate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CLEAR.",[],,"['Mingxuan Xia', 'Zenan Huang', 'Runze Wu', 'Gengyu Lyu', 'Junbo Zhao', 'Gang Chen', 'Haobo Wang']","['Zhejiang University', 'Zhejiang University', 'NetEase Corp', 'Beijing University of Technology', 'Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Zhejiang University']",
https://openreview.net/forum?id=VUTyzH63Xa,Fairness & Bias,Simplicity Bias via Global Convergence of Sharpness Minimization,"The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features. Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) toward flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low-rank) is not well understood. In this work, we take a step toward bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property --- a local geodesic convexity --- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest.",[],,"['Khashayar Gatmiry', 'Zhiyuan Li', 'Sashank J. Reddi', 'Stefanie Jegelka']","['Massachusetts Institute of Technology', 'Toyota Technological Institute at Chicago', 'Google', 'Computer Science, Technische Universität München']",
https://openreview.net/forum?id=8VEGkphQaK,Fairness & Bias,Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model,"Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. To unravel the underlying mechanisms of stepwise inference we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. We find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy trade-off in model generations as sampling temperature varies; (iii) a simplicity bias in the model’s output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.",[],,"['Mikail Khona', 'Maya Okawa', 'Jan Hula', 'Rahul Ramesh', 'Kento Nishi', 'Robert P. Dick', 'Ekdeep Singh Lubana', 'Hidenori Tanaka']","['Massachusetts Institute of Technology', '', 'CIIRC, Czech Technical University, Czech Technical University of Prague', 'University of Pennsylvania', 'Harvard University', 'Electrical Engineering and Computer Science, University of Michigan', 'Center for Brain Science, Harvard University, Harvard University', 'Harvard University, Harvard University']",
https://openreview.net/forum?id=0pSTzCnEmi,Transparency & Explainability,Improving Neural Additive Models with Bayesian Principles,"Neural additive models (NAMs) enhance the transparency of deep neural networks by handling input features in separate additive sub-networks. However, they lack inherent mechanisms that provide calibrated uncertainties and enable selection of relevant features and interactions. Approaching NAMs from a Bayesian perspective, we augment them in three primary ways, namely by a) providing credible intervals for the individual additive sub-networks; b) estimating the marginal likelihood to perform an implicit selection of features via an empirical Bayes procedure; and c) facilitating the ranking of feature pairs as candidates for second-order interaction in fine-tuned models. In particular, we develop Laplace-approximated NAMs (LA-NAMs), which show improved empirical performance on tabular datasets and challenging real-world medical tasks.",[],,"['Kouroche Bouchiat', 'Alexander Immer', 'Hugo Yèche', 'Gunnar Ratsch', 'Vincent Fortuin']","['Department of Computer Science, ETH Zürich', '', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'Technical University of Munich']",
https://openreview.net/forum?id=IC9UZ8lm25,Transparency & Explainability,MultiMax: Sparse and Multi-Modal Attention Learning,"SoftMax is a ubiquitous ingredient of modern machine learning algorithms. It maps an input vector onto a probability simplex and reweights the input by concentrating the probability mass at large entries. Yet, as a smooth approximation to the Argmax function, a significant amount of probability mass is distributed to other, residual entries, leading to poor interpretability and noise. Although sparsity can be achieved by a family of SoftMax variants, they often require an alternative loss function and do not preserve multimodality. We show that this trade-off between multi-modality and sparsity limits the expressivity of SoftMax as well as its variants. We provide a solution to this tension between objectives by proposing a piece-wise differentiable function, termed MultiMax, which adaptively modulates the output distribution according to input entry range. Through comprehensive analysis and evaluation, we show that MultiMax successfully produces a distribution that supresses irrelevant entries while preserving multi-modality, with benefits in image classification, language modeling and machine translation.",[],,"['Yuxuan Zhou', 'Mario Fritz', 'Margret Keuper']","['Universität Mannheim', 'CISPA Helmholtz Center for Information Security', 'Universität Mannheim']",
https://openreview.net/forum?id=JsPvL6ExK8,Security,Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE,"Fluid dynamics modeling has received extensive attention in the machine learning community. Although numerous graph neural network (GNN) approaches have been proposed for this problem, the problem of out-of-distribution (OOD) generalization remains underexplored. In this work, we propose a new large-scale dataset Prometheus which simulates tunnel and pool fires across various environmental conditions and builds an extensive benchmark of 12 baselines, which demonstrates that the OOD generalization performance is far from satisfactory. To tackle this, this paper introduces a new approach named Disentangled Graph ODE (DGODE), which learns disentangled representations for continuous interacting dynamics modeling. In particular, we utilize a temporal GNN and a frequency network to extract semantics from historical trajectories into node representations and environment representations respectively. To mitigate the potential distribution shift, we minimize the mutual information between invariant node representations and the discretized environment features using adversarial learning. Then, they are fed into a coupled graph ODE framework, which models the evolution using neighboring nodes and dynamical environmental context. In addition, we enhance the stability of the framework by perturbing the environment features to enhance robustness. Extensive experiments validate the effectiveness of DGODE compared with state-of-the-art approaches.",[],,"['Hao Wu', 'Huiyuan Wang', 'Kun Wang', 'Weiyan Wang', 'ChanganYe', 'Yangyu Tao', 'Chong Chen', 'Xian-Sheng Hua', 'Xiao Luo']","['', 'Department of Biostatistics, Epidemiology and Informatics, University of Pennsylvania, University of Pennsylvania', 'Nanyang Technological University', 'TEG, Tencent', 'Tencent MLPD', 'University of Science and Technology of China, Tsinghua University', 'Terminus Group', 'Terminus Group', 'Department of Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=cMige5MK1N,Fairness & Bias,Accelerating Heterogeneous Federated Learning with Closed-form Classifiers,"Federated Learning (FL) methods often struggle in highly statistically heterogeneous settings. Indeed, non-IID data distributions cause client drift and biased local solutions, particularly pronounced in the final classification layer, negatively impacting convergence speed and accuracy. To address this issue, we introduce *Federated Recursive Ridge Regression* (Fed3R). Our method fits a Ridge Regression classifier computed in closed form leveraging pre-trained features. Fed3R is immune to statistical heterogeneity and is invariant to the sampling order of the clients. Therefore, it proves particularly effective in cross-device scenarios. Furthermore, it is fast and efficient in terms of communication and computation costs, requiring up to two orders of magnitude fewer resources than the competitors. Finally, we propose to leverage the Fed3R parameters as an initialization for a softmax classifier and subsequently fine-tune the model using any FL algorithm (Fed3R with Fine-Tuning, Fed3R+FT). Our findings also indicate that maintaining a fixed classifier aids in stabilizing the training and learning more discriminative features in cross-device settings. Official website: https://fed-3r.github.io/.",[],,"['Eros Fanì', 'Raffaello Camoriano', 'Barbara Caputo', 'Marco Ciccone']","['Polytechnic Institute of Turin', 'DAUIN, Polytechnic Institute of Turin', 'Politecnico di Torino', 'Vector Institute']",
https://openreview.net/forum?id=xS2YKQlBIZ,Fairness & Bias,Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling,"In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an *exponential rate*. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow *polynomial rate*. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) *provably fail* in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when applied to linearly non-separable datasets and deep neural networks.",[],,"['Mingze Wang', 'Zeping Min', 'Lei Wu']","['Peking University', '', 'Peking University']",
https://openreview.net/forum?id=Wp054bnPq9,Security,Watermark Stealing in Large Language Models,"LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying *watermark stealing* (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical *spoofing attacks*, as hypothesized in prior work, but also greatly boosts *scrubbing* attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80\%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",[],,"['Nikola Jovanović', 'Robin Staab', 'Martin Vechev']","['ETHZ - ETH Zurich', 'Computer Science, ETHZ - ETH Zurich', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=emtXYlBrNF,Fairness & Bias,AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers,"Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers.",[],,"['Reduan Achtibat', 'Sayed Mohammad Vakilzadeh Hatefi', 'Maximilian Dreyer', 'Aakriti Jain', 'Thomas Wiegand', 'Sebastian Lapuschkin', 'Wojciech Samek']","['Fraunhofer HHI', 'AI Department, XAI Group, Fraunhofer HHI, Fraunhofer IAIS', 'Fraunhofer HHI', 'Fraunhofer HHI, Fraunhofer IAIS', 'Fraunhofer HHI', 'Fraunhofer HHI', 'Department of Electrical Engineering and Computer Science, TU Berlin']",
