link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://openreview.net/forum?id=gE7qZurGH3,Fairness & Bias,Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching,"Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward *lossless graph condensation* by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert trajectories with more diverse supervision signals from the original graph, and then effectively transfer the information into the condensed graph with expanding window matching. Moreover, we design a loss function to further extract knowledge from the expert trajectories. Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets. Code is released at https://github.com/NUS-HPC-AI-Lab/GEOM.",[],,"['Yuchen Zhang', 'Tianle Zhang', 'Kai Wang', 'Ziyao Guo', 'Yuxuan Liang', 'Xavier Bresson', 'Wei Jin', 'Yang You']","['Shanghai Artificial Intelligence Laboratory', 'NTU, Nanyang Technological University', 'soc, national university of singaore, National University of Singapore', 'School of Computing, national university of singaore, National University of Singapore', 'INTR & DSA, The Hong Kong University of Science and Technology (Guangzhou)', 'National University of Singapore', 'Emory University', 'Computer Science, National University of Singapore']",
https://openreview.net/forum?id=Ln3moCobjO,Fairness & Bias,Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering,"Recommender system aims to recommend items or information that may interest users based on their behaviors and preferences. However, there may be sampling selection bias in the data collection process, i.e., the collected data is not a representative of the target population. Many debiasing methods are developed based on pseudo-labelings. Nevertheless, the validity of these methods relies heavily on accurate pseudo-labelings (i.e., the imputed labels), which is difficult to satisfy in practice. In this paper, we theoretically propose several novel doubly robust estimators that are unbiased when either (a) the pseudo-labelings deviate from the true labels with an arbitrary user-specific inductive bias, item-specific inductive bias, or a combination of both, or (b) the learned propensities are accurate. We further propose a propensity reconstruction learning approach that adaptively updates the constraint weights using an attention mechanism and effectively controls the variance. Extensive experiments show that our approach outperforms the state-of-the-art on one semi-synthetic and three real-world datasets.",[],,"['Haoxuan Li', 'Chunyuan Zheng', 'Shuyi Wang', 'Kunhan Wu', 'Eric Wang', 'Peng Wu', 'Zhi Geng', 'Xu Chen', 'Xiao-Hua Zhou']","['Center for Data Science, Peking University', 'meituan', 'University of Pennsylvania, University of Pennsylvania', 'Carnegie Mellon University', '', 'Beijing Technology and Business University', 'School of Matematics asn Statistics, Beijing Technology and Business University', 'AI, Renmin University of China', 'School of mathematical Science, Peking University, Peking University']",
https://openreview.net/forum?id=2dlmcTXfcY,Fairness & Bias,Kernel-Based Evaluation of Conditional Biological Sequence Models,"We propose a set of kernel-based tools to evaluate the designs and tune the hyperparameters of conditional sequence models, with a focus on problems in computational biology. The backbone of our tools is a new measure of discrepancy between the true conditional distribution and the model's estimate, called the Augmented Conditional Maximum Mean Discrepancy (ACMMD). Provided that the model can be sampled from, the ACMMD can be estimated unbiasedly from data to quantify absolute model fit, integrated within hypothesis tests, and used to evaluate model reliability. We demonstrate the utility of our approach by analyzing a popular protein design model, ProteinMPNN. We are able to reject the hypothesis that ProteinMPNN fits its data for various protein families, and tune the model's temperature hyperparameter to achieve a better fit.",[],,"['Pierre Glaser', 'Steffanie Paul', 'Alissa M Hummer', 'Charlotte Deane', 'Debora Susan Marks', 'Alan Nawzad Amin']","['Gatsby Computational Neuroscience Unit, University College London', 'Harvard University', 'University of Oxford', 'Department of Statistics, University of Oxford', 'Harvard University', 'New York University']",
https://openreview.net/forum?id=gPStP3FSY9,Fairness & Bias,Discovering Environments with XRM,"Environment annotations are essential for the success of many out-of-distribution (OOD) generalization methods. Unfortunately, these are costly to obtain and often limited by human annotators' biases. To achieve robust generalization, it is essential to develop algorithms for automatic environment discovery within datasets. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods introduce hyper-parameters and early-stopping criteria, which require a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk Minimization (XRM) to address this issue. XRM trains twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Algorithms built on top of XRM environments achieve oracle worst-group-accuracy, addressing a long-standing challenge in OOD generalization.",[],,"['Mohammad Pezeshki', 'Diane Bouchacourt', 'Mark Ibrahim', 'Nicolas Ballas', 'Pascal Vincent', 'David Lopez-Paz']","['Facebook', 'Facebook AI Research', 'Facebook AI Research (FAIR) Meta', 'Meta', 'University of Montreal', 'Facebook']",
https://openreview.net/forum?id=GHZVjmaGQM,Transparency & Explainability,Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response,"Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: *ranking* of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a *causal loss* that we combine with the standard predictive loss to arrive at a *hybrid loss* that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance *and* causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes.",[],,"['Bob Junyi Zou', 'Matthew E Levine', 'Dessi P. Zaharieva', 'Ramesh Johari', 'Emily Fox']","['ICME, Stanford University', 'California Institute of Technology', 'Pediatrics, Stanford University', 'Management Science and Engineering, Stanford University', 'Inistro Inc.']",
https://openreview.net/forum?id=XsDWw1Mn2p,Transparency & Explainability,How Learning by Reconstruction Produces Uninformative Features For Perception,"Input space reconstruction is an attractive representation learning paradigm. Despite interpretability benefit of reconstruction and generation, we identify a misalignment between learning to reconstruct, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90% of the pixel variance can be solved with 45% test accuracy. Using the bottom subspace instead, accounting for only 20% of the pixel variance, reaches 55% test accuracy. Learning by reconstruction is also wasteful as the features for perception are learned last, pushing the need for long training schedules. We finally prove that learning by denoising can alleviate that misalignment for some noise strategies, e.g., masking. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide a solution to detect if a noise strategy is never beneficial regardless of the perception task, e.g., additive Gaussian noise.",[],,"['Randall Balestriero', 'Yann LeCun']","['Computer Science, Brown University', 'Facebook']",
https://openreview.net/forum?id=jTn4AIOgpM,Fairness & Bias,Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference,"Deep Gaussian processes (DGPs) provide a robust paradigm in Bayesian deep learning. In DGPs, a set of sparse integration locations called inducing points are selected to approximate the posterior distribution of the model. This is done to reduce computational complexity and improve model efficiency. However, inferring the posterior distribution of inducing points is not straightforward. Traditional variational inference techniques methods to approximate the posterior often leads to significant bias. To address this issue, we propose an alternative named Denoising Diffusion Variational Inference (DDVI) that utilizes a denoising diffusion stochastic differential equation (SDE) for generating posterior samples of inducing variables. We refer to the score matching method in the denoising diffusion model to approximate challenging score functions using a neural network. Furthermore, by combining classical mathematical theory of SDE with the minimization of KL divergence between the approximate and true processes, we propose a novel explicit variational lower bound for the marginal likelihood function of DGP. Through extensive experiments on various datasets and comparisons with baseline methods, we empirically demonstrate the effectiveness of the DDVI method in posterior inference of inducing points for DGP models.",[],,"['JIAN XU', 'Delu Zeng', 'John Paisley']","['South China University of Technology', 'School  of Electronics and Information Processing, South China University of Technology', 'Columbia University']",
https://openreview.net/forum?id=s4EYBJ30WY,Fairness & Bias,"Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance","Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pre-trained encoders, and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation strategies to mitigate the issue. Our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pre-trained features may have unknown biases that can be diagnosed through their spectra.",[],,"['Chiraag Kaushik', 'Ran Liu', 'Chi-Heng Lin', 'Amrit Khera', 'Matthew Y Jin', 'Wenrui Ma', 'Vidya Muthukumar', 'Eva L Dyer']","['Georgia Institute of Technology', 'Apple', 'Samsung Research America', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Machine Learning, Georgia Institute of Technology']",
https://openreview.net/forum?id=jU6iPouOZ6,Transparency & Explainability,From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation,"Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.",[],,"['Kun Su', 'Xiulong Liu', 'Eli Shlizerman']","['Google', 'University of Washington, Seattle', 'Electrical and computer engineering , University of Washington']",
https://openreview.net/forum?id=upO8FUwf92,Transparency & Explainability,Towards Compositionality in Concept Learning,"Concept-based interpretability methods offer a lens into the internals of foundation models by decomposing their embeddings into high-level concepts. These concept representations are most useful when they are *compositional*, meaning that the individual concepts compose to explain the full sample. We show that existing unsupervised concept extraction methods find concepts which are not compositional. To automatically discover compositional concept representations, we identify two salient properties of such representations, and propose Compositional Concept Extraction (CCE) for finding concepts which obey these properties. We evaluate CCE on five different datasets over image and text data. Our evaluation shows that CCE finds more compositional concept representations than baselines and yields better accuracy on four downstream classification tasks.",[],,"['Adam Stein', 'Aaditya Naik', 'Yinjun Wu', 'Mayur Naik', 'Eric Wong']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'Department of Computer and information science, Peking University', 'University of Pennsylvania', 'University of Pennsylvania']",
https://openreview.net/forum?id=8ho1l6RZNB,Security,Image Hijacks: Adversarial Images can Control Generative Models at Runtime,"Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour matching to craft hijacks for four types of attack: forcing VLMs to generate outputs of the adversary’s choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",[],,"['Luke Bailey', 'Euan Ong', 'Stuart Russell', 'Scott Emmons']","['Computer Science, Stanford University', 'University of Cambridge', 'EECS, University of California Berkeley', 'Google DeepMind']",
https://openreview.net/forum?id=GLGYYqPwjy,Fairness & Bias,QuRating: Selecting High-Quality Data for Training Language Models,"Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",[],,"['Alexander Wettig', 'Aatmik Gupta', 'Saumya Malik', 'Danqi Chen']","['Department of Computer Science, Princeton University', 'Computer Science, Princeton University', 'Computer Science, Princeton University', 'Department of Computer Science, Princeton University']",
https://openreview.net/forum?id=5cm2jGct2W,Fairness & Bias,On the Maximal Local Disparity of Fairness-Aware Classifiers,"Fairness has become a crucial aspect in the development of trustworthy machine learning algorithms. Current fairness metrics to measure the violation of demographic parity have the following drawbacks: (i) the *average difference* of model predictions on two groups cannot reflect their *distribution disparity*, and (ii) the *overall* calculation along all possible predictions conceals the *extreme local disparity* at or around certain predictions. In this work, we propose a novel fairness metric called **M**aximal **C**umulative ratio **D**isparity along varying **P**redictions' neighborhood (MCDP), for measuring the maximal local disparity of the fairness-aware classifiers. To accurately and efficiently calculate the MCDP, we develop a provably exact and an approximate calculation algorithm that greatly reduces the computational complexity with low estimation error. We further propose a bi-level optimization algorithm using a differentiable approximation of the MCDP for improving the algorithmic fairness. Extensive experiments on both tabular and image datasets validate that our fair training algorithm can achieve superior fairness-accuracy trade-offs.",[],,"['Jinqiu Jin', 'Haoxuan Li', 'Fuli Feng']","['University of Science and Technology of China', 'Center for Data Science, Peking University', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China']",
https://openreview.net/forum?id=DyvhD8J3Wl,Security,Benign Overfitting in Adversarial Training of Neural Networks,"Benign overfitting is the phenomenon wherein none of the predictors in the hypothesis class can achieve perfect accuracy (i.e., non-realizable or noisy setting), but a model that interpolates the training data still achieves good generalization. A series of recent works aim to understand this phenomenon for regression and classification tasks using linear predictors as well as two-layer neural networks. In this paper, we study such a benign overfitting phenomenon in an adversarial setting. We show that under a distributional assumption, interpolating neural networks found using adversarial training generalize well despite inference-time attacks. Specifically, we provide convergence and generalization guarantees for adversarial training of two-layer networks (with smooth as well as non-smooth activation functions) showing that under moderate $\ell_2$ norm perturbation budget, the trained model has near-zero robust training loss and near-optimal robust generalization error. We support our theoretical findings with an empirical study on synthetic and real-world data.",[],,"['Yunjuan Wang', 'Kaibo Zhang', 'Raman Arora']","['Computer Science Department, Johns Hopkins University', 'Department of Computer Science, Whiting School of Engineering', 'Computer Science, Johns Hopkins University']",
https://openreview.net/forum?id=6jmdOTRMIO,Security,Scalable AI Safety via Doubly-Efficient Debate,"The emergence of pre-trained AI systems with powerful capabilities across a diverse and ever-increasing set of complex domains has raised a critical challenge for AI safety as tasks can become too complicated for humans to judge directly. Irving et al (2018). proposed a debate method in this direction with the goal of pitting the power of such AI models against each other until the problem of identifying (mis)-alignment is broken down into a manageable subtask. While the promise of this approach is clear, the original framework was based on the assumption that the honest strategy is able to simulate *deterministic* AI systems for an *exponential* number of steps, limiting its applicability. In this paper, we show how to address these challenges by designing a new set of debate protocols where the honest strategy can always succeed using a simulation of a *polynomial* number of steps, whilst being able to verify the alignment of *stochastic* AI systems, even when the dishonest strategy is allowed to use exponentially many simulation steps.",[],,"['Jonah Brown-Cohen', 'Geoffrey Irving', 'Georgios Piliouras']","['Google DeepMind', 'UK AI Safety Institute', 'Google DeepMind']",
https://openreview.net/forum?id=T0zR4mdSce,Fairness & Bias,PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling,"Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested on both standard benchmark problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials. We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes.",[],,"['Phong C.H. Nguyen', 'Xinlun Cheng', 'Shahab Azarfar', 'Pradeep Seshadri', 'Yen T. Nguyen', 'Munho Kim', 'Sanghun Choi', 'H.S. Udaykumar', 'Stephen Baek']","['School of Data Science, University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']",
https://openreview.net/forum?id=lQzmDFlsHX,Fairness & Bias,Unsupervised Concept Discovery Mitigates Spurious Correlations,"Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring subgroups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the benchmark datasets for sub-population shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation. Code is available at https://github.com/rarefin/CoBalT",[],,"['Md Rifat Arefin', 'Yan Zhang', 'Aristide Baratin', 'Francesco Locatello', 'Irina Rish', 'Dianbo Liu', 'Kenji Kawaguchi']","['Université de Montréal', 'Samsung - SAIT AI Lab, Montreal', '', 'Institute of Science and Technology', 'DIRO, University of Montreal', 'National University of Singapore', 'National University of Singapore']",
https://openreview.net/forum?id=AocOA4h3bu,Fairness & Bias,Sequential Disentanglement by Extracting Static Information From A Single Sequence Element,"One of the fundamental representation learning tasks is unsupervised sequential disentanglement, where latent codes of inputs are decomposed to a single static factor and a sequence of dynamic factors. To extract this latent information, existing methods condition the static and dynamic codes on the entire input sequence. Unfortunately, these models often suffer from information leakage, i.e., the dynamic vectors encode both static and dynamic information, or vice versa, leading to a non-disentangled representation. Attempts to alleviate this problem via reducing the dynamic dimension and auxiliary loss terms gain only partial success. Instead, we propose a novel and simple architecture that mitigates information leakage by offering a simple and effective subtraction inductive bias while conditioning on a single sample. Remarkably, the resulting variational framework is simpler in terms of required loss terms, hyper-parameters, and data augmentation. We evaluate our method on multiple data-modality benchmarks including general time series, video, and audio, and we show beyond state-of-the-art results on generation and prediction tasks in comparison to several strong baselines.",[],,"['Nimrod Berman', 'Ilan Naiman', 'Idan Arbiv', 'Gal Fadlon', 'Omri Azencot']","['Ben-Gurion University of the Negev', 'Amazon', 'Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev', 'Ben-Gurion University of the Negev']",
https://openreview.net/forum?id=sT7UJh5CTc,Privacy & Data Governance,Low-Cost High-Power Membership Inference Attacks,"Membership inference attacks aim to detect if a particular data point was used in training a model. We design a novel statistical test to perform robust membership inference attacks (RMIA) with low computational overhead. We achieve this by a fine-grained modeling of the null hypothesis in our likelihood ratio tests, and effectively leveraging both reference models and reference population data samples. RMIA has superior test power compared with prior methods, throughout the TPR-FPR curve (even at extremely low FPR, as low as 0). Under computational constraints, where only a limited number of pre-trained reference models (as few as 1) are available, and also when we vary other elements of the attack (e.g., data distribution), our method performs exceptionally well, unlike prior attacks that approach random guessing. RMIA lays the groundwork for practical yet accurate data privacy risk assessment in machine learning.",[],,"['Sajjad Zarifzadeh', 'Philippe Liu', 'Reza Shokri']","['Yazd University', 'national university of singaore, National University of Singapore', '']",
https://openreview.net/forum?id=LyJ85kgHFe,Security,$\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts,"Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose $\texttt{MoE-RBench}$, the first comprehensive assessment of SMoE reliability from three aspects: $\textit{(i)}$ safety and hallucination, $\textit{(ii)}$ resilience to adversarial attacks, and $\textit{(iii)}$ out-of-distribution robustness. Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability. Codes are available at https://github.com/UNITES-Lab/MoE-RBench.",[],,"['Guanjie Chen', 'Xinyu Zhao', 'Tianlong Chen', 'Yu Cheng']","['Shanghai Jiaotong University', 'University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'The Chinese University of Hong Kong']",
https://openreview.net/forum?id=pgI9inG2Ny,Security,Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error,"Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of Bellman Infinity-error. The top-tier performance of CAR-DQN across various benchmarks validates its practical effectiveness and reinforces the soundness of our theoretical analysis.",[],,"['Haoran Li', 'Zicheng Zhang', 'Wang Luo', 'Congying Han', 'Yudong Hu', 'Tiande Guo', 'Shichen Liao']","['School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'JD.com', 'School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'School of Mathematical Sciences, University of the Chinese Academy of Sciences', 'Scool of Mathematical Sciences, University of the Chinese Academy of Sciences', 'Mathematics, Beijing Normal University']",
https://openreview.net/forum?id=wuQ2DRPAuy,Privacy & Data Governance,Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning,"High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients' privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients' model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients' aggregation weights proportional to their privacy parameters ($\epsilon$) will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients' model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter $\epsilon$ to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here: https://github.com/Saber-mm/HDPFL.git",[],,"['Saber Malekmohammadi', 'Yaoliang Yu', 'YANG CAO']","['University of Waterloo', 'University of Waterloo', 'Tokyo Institute of Technology']",
https://openreview.net/forum?id=0tPBk24xNj,Security,Adversarial Attacks on Combinatorial Multi-Armed Bandits,"We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, a notion to capture the vulnerability and robustness of CMAB. The attackability condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path.",[],,"['Rishab Balasubramanian', 'Jiawei Li', 'Prasad Tadepalli', 'Huazheng Wang', 'Qingyun Wu', 'Haoyu Zhao']","['Oregon State University', 'IIIS, Tsinghua University', 'Oregon State University', 'Oregon State University', 'Pennsylvania State University', 'Princeton University']",
https://openreview.net/forum?id=ZUXvpIrz5l,Security,Safe Exploration in Dose Finding Clinical Trials with Heterogeneous Participants,"In drug development, early phase dose-finding clinical trials are carried out to identify an optimal dose to administer to patients in larger confirmatory clinical trials. Standard trial procedures do not optimize for participant benefit and do not consider participant heterogeneity, despite consequences to participants' health and downstream impacts to under-represented population subgroups. Many novel drugs also do not obey parametric modelling assumptions made in common dose-finding procedures. We present Safe Allocation for Exploration of Treatments SAFE-T, a procedure for adaptive dose-finding that adheres to safety constraints, improves utility for heterogeneous participants, and works well with small sample sizes. SAFE-T flexibly learns non-parametric multi-output Gaussian process models for dose toxicity and efficacy, using Bayesian optimization, and provides accurate final dose recommendations. We provide theoretical guarantees for the satisfaction of safety constraints. Using a comprehensive set of realistic synthetic scenarios, we demonstrate empirically that SAFE-T generally outperforms comparable methods and maintains performance across variations in sample size and subgroup distribution. Finally, we extend SAFE-T to a new adaptive setting, demonstrating its potential to improve traditional clinical trial procedures.",[],,"['Isabel Chien', 'Wessel P Bruinsma', 'Javier Gonzalez', 'Richard E. Turner']","['University of Cambridge', 'Microsoft Research', 'Microsoft', 'Alan Turing Institute']",
https://openreview.net/forum?id=dBMLtuKH01,Transparency & Explainability,Position: The Causal Revolution Needs Scientific Pragmatism,"Causal models and methods have great promise, but their progress has been stalled. Proposals using causality get squeezed between two opposing worldviews. Scientific perfectionism--an insistence on only using ``correct'' models--slows the adoption of causal methods in knowledge generating applications. Pushing in the opposite direction, the academic discipline of computer science prefers algorithms with no or few assumptions, and technologies based on automation and scalability are often selected for economic and business applications. We argue that these system-centric inductive biases should be replaced with a human-centric philosophy we refer to as scientific pragmatism. The machine learning community must strike the right balance to make space for the causal revolution to prosper.",[],,['Joshua R. Loftus'],['London School of Economics'],
https://openreview.net/forum?id=6VZOONPn8S,Privacy & Data Governance,Disparate Impact on Group Accuracy of Linearization for Private Inference,"Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the finetuning step for linearized models can serve as an effective mitigation strategy.",[],,"['Saswat Das', 'Marco Romanelli', 'Ferdinando Fioretto']","['Department of Computer Science, University of Virginia, Charlottesville', '', 'University of Virginia, Charlottesville']",
https://openreview.net/forum?id=bgP8Rxv2eB,Fairness & Bias,Unbiased Multi-Label Learning from Crowdsourced Annotations,"This work studies the novel Crowdsourced Multi-Label Learning (CMLL) problem, where each instance is related to multiple true labels but the model only receives unreliable labels from different annotators. Although a few Crowdsourced Multi-Label Inference (CMLI) methods have been developed, they require both the training and testing sets to be assigned crowdsourced labels and focus on true label inferring rather than prediction, making them less practical. In this paper, by excavating the generation process of crowdsourced labels, we establish the first **unbiased risk estimator** for CMLL based on the crowdsourced transition matrices. To facilitate transition matrix estimation, we upgrade our unbiased risk estimator by aggregating crowdsourced labels and transition matrices from all annotators while guaranteeing its theoretical characteristics. Integrating with the unbiased risk estimator, we further propose a decoupled autoencoder framework to exploit label correlations and boost performance. We also provide a generalization error bound to ensure the convergence of the empirical risk estimator. Experiments on various CMLL scenarios demonstrate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CLEAR.",[],,"['Mingxuan Xia', 'Zenan Huang', 'Runze Wu', 'Gengyu Lyu', 'Junbo Zhao', 'Gang Chen', 'Haobo Wang']","['Zhejiang University', 'Zhejiang University', 'NetEase Corp', 'Beijing University of Technology', 'Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Zhejiang University']",
https://openreview.net/forum?id=VUTyzH63Xa,Fairness & Bias,Simplicity Bias via Global Convergence of Sharpness Minimization,"The remarkable generalization ability of neural networks is usually attributed to the implicit bias of SGD, which often yields models with lower complexity using simpler (e.g. linear) and low-rank features. Recent works have provided empirical and theoretical evidence for the bias of particular variants of SGD (such as label noise SGD) toward flatter regions of the loss landscape. Despite the folklore intuition that flat solutions are 'simple', the connection with the simplicity of the final trained model (e.g. low-rank) is not well understood. In this work, we take a step toward bridging this gap by studying the simplicity structure that arises from minimizers of the sharpness for a class of two-layer neural networks. We show that, for any high dimensional training data and certain activations, with small enough step size, label noise SGD always converges to a network that replicates a single linear feature across all neurons; thereby implying a simple rank one feature matrix. To obtain this result, our main technical contribution is to show that label noise SGD always minimizes the sharpness on the manifold of models with zero loss for two-layer networks. Along the way, we discover a novel property --- a local geodesic convexity --- of the trace of Hessian of the loss at approximate stationary points on the manifold of zero loss, which links sharpness to the geometry of the manifold. This tool may be of independent interest.",[],,"['Khashayar Gatmiry', 'Zhiyuan Li', 'Sashank J. Reddi', 'Stefanie Jegelka']","['Massachusetts Institute of Technology', 'Toyota Technological Institute at Chicago', 'Google', 'Computer Science, Technische Universität München']",
https://openreview.net/forum?id=8VEGkphQaK,Fairness & Bias,Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model,"Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. To unravel the underlying mechanisms of stepwise inference we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. We find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy trade-off in model generations as sampling temperature varies; (iii) a simplicity bias in the model’s output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.",[],,"['Mikail Khona', 'Maya Okawa', 'Jan Hula', 'Rahul Ramesh', 'Kento Nishi', 'Robert P. Dick', 'Ekdeep Singh Lubana', 'Hidenori Tanaka']","['Massachusetts Institute of Technology', '', 'CIIRC, Czech Technical University, Czech Technical University of Prague', 'University of Pennsylvania', 'Harvard University', 'Electrical Engineering and Computer Science, University of Michigan', 'Center for Brain Science, Harvard University, Harvard University', 'Harvard University, Harvard University']",
https://openreview.net/forum?id=0pSTzCnEmi,Transparency & Explainability,Improving Neural Additive Models with Bayesian Principles,"Neural additive models (NAMs) enhance the transparency of deep neural networks by handling input features in separate additive sub-networks. However, they lack inherent mechanisms that provide calibrated uncertainties and enable selection of relevant features and interactions. Approaching NAMs from a Bayesian perspective, we augment them in three primary ways, namely by a) providing credible intervals for the individual additive sub-networks; b) estimating the marginal likelihood to perform an implicit selection of features via an empirical Bayes procedure; and c) facilitating the ranking of feature pairs as candidates for second-order interaction in fine-tuned models. In particular, we develop Laplace-approximated NAMs (LA-NAMs), which show improved empirical performance on tabular datasets and challenging real-world medical tasks.",[],,"['Kouroche Bouchiat', 'Alexander Immer', 'Hugo Yèche', 'Gunnar Ratsch', 'Vincent Fortuin']","['Department of Computer Science, ETH Zürich', '', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'Technical University of Munich']",
https://openreview.net/forum?id=IC9UZ8lm25,Transparency & Explainability,MultiMax: Sparse and Multi-Modal Attention Learning,"SoftMax is a ubiquitous ingredient of modern machine learning algorithms. It maps an input vector onto a probability simplex and reweights the input by concentrating the probability mass at large entries. Yet, as a smooth approximation to the Argmax function, a significant amount of probability mass is distributed to other, residual entries, leading to poor interpretability and noise. Although sparsity can be achieved by a family of SoftMax variants, they often require an alternative loss function and do not preserve multimodality. We show that this trade-off between multi-modality and sparsity limits the expressivity of SoftMax as well as its variants. We provide a solution to this tension between objectives by proposing a piece-wise differentiable function, termed MultiMax, which adaptively modulates the output distribution according to input entry range. Through comprehensive analysis and evaluation, we show that MultiMax successfully produces a distribution that supresses irrelevant entries while preserving multi-modality, with benefits in image classification, language modeling and machine translation.",[],,"['Yuxuan Zhou', 'Mario Fritz', 'Margret Keuper']","['Universität Mannheim', 'CISPA Helmholtz Center for Information Security', 'Universität Mannheim']",
https://openreview.net/forum?id=JsPvL6ExK8,Security,Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE,"Fluid dynamics modeling has received extensive attention in the machine learning community. Although numerous graph neural network (GNN) approaches have been proposed for this problem, the problem of out-of-distribution (OOD) generalization remains underexplored. In this work, we propose a new large-scale dataset Prometheus which simulates tunnel and pool fires across various environmental conditions and builds an extensive benchmark of 12 baselines, which demonstrates that the OOD generalization performance is far from satisfactory. To tackle this, this paper introduces a new approach named Disentangled Graph ODE (DGODE), which learns disentangled representations for continuous interacting dynamics modeling. In particular, we utilize a temporal GNN and a frequency network to extract semantics from historical trajectories into node representations and environment representations respectively. To mitigate the potential distribution shift, we minimize the mutual information between invariant node representations and the discretized environment features using adversarial learning. Then, they are fed into a coupled graph ODE framework, which models the evolution using neighboring nodes and dynamical environmental context. In addition, we enhance the stability of the framework by perturbing the environment features to enhance robustness. Extensive experiments validate the effectiveness of DGODE compared with state-of-the-art approaches.",[],,"['Hao Wu', 'Huiyuan Wang', 'Kun Wang', 'Weiyan Wang', 'ChanganYe', 'Yangyu Tao', 'Chong Chen', 'Xian-Sheng Hua', 'Xiao Luo']","['', 'Department of Biostatistics, Epidemiology and Informatics, University of Pennsylvania, University of Pennsylvania', 'Nanyang Technological University', 'TEG, Tencent', 'Tencent MLPD', 'University of Science and Technology of China, Tsinghua University', 'Terminus Group', 'Terminus Group', 'Department of Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=cMige5MK1N,Fairness & Bias,Accelerating Heterogeneous Federated Learning with Closed-form Classifiers,"Federated Learning (FL) methods often struggle in highly statistically heterogeneous settings. Indeed, non-IID data distributions cause client drift and biased local solutions, particularly pronounced in the final classification layer, negatively impacting convergence speed and accuracy. To address this issue, we introduce *Federated Recursive Ridge Regression* (Fed3R). Our method fits a Ridge Regression classifier computed in closed form leveraging pre-trained features. Fed3R is immune to statistical heterogeneity and is invariant to the sampling order of the clients. Therefore, it proves particularly effective in cross-device scenarios. Furthermore, it is fast and efficient in terms of communication and computation costs, requiring up to two orders of magnitude fewer resources than the competitors. Finally, we propose to leverage the Fed3R parameters as an initialization for a softmax classifier and subsequently fine-tune the model using any FL algorithm (Fed3R with Fine-Tuning, Fed3R+FT). Our findings also indicate that maintaining a fixed classifier aids in stabilizing the training and learning more discriminative features in cross-device settings. Official website: https://fed-3r.github.io/.",[],,"['Eros Fanì', 'Raffaello Camoriano', 'Barbara Caputo', 'Marco Ciccone']","['Polytechnic Institute of Turin', 'DAUIN, Polytechnic Institute of Turin', 'Politecnico di Torino', 'Vector Institute']",
https://openreview.net/forum?id=xS2YKQlBIZ,Fairness & Bias,Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling,"In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an *exponential rate*. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow *polynomial rate*. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) *provably fail* in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when applied to linearly non-separable datasets and deep neural networks.",[],,"['Mingze Wang', 'Zeping Min', 'Lei Wu']","['Peking University', '', 'Peking University']",
https://openreview.net/forum?id=Wp054bnPq9,Security,Watermark Stealing in Large Language Models,"LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying *watermark stealing* (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical *spoofing attacks*, as hypothesized in prior work, but also greatly boosts *scrubbing* attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80\%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",[],,"['Nikola Jovanović', 'Robin Staab', 'Martin Vechev']","['ETHZ - ETH Zurich', 'Computer Science, ETHZ - ETH Zurich', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=emtXYlBrNF,Fairness & Bias,AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers,"Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a single backward pass. Through extensive evaluations against existing methods on LLaMa 2, Mixtral 8x7b, Flan-T5 and vision transformer architectures, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an LRP library at https://github.com/rachtibat/LRP-eXplains-Transformers.",[],,"['Reduan Achtibat', 'Sayed Mohammad Vakilzadeh Hatefi', 'Maximilian Dreyer', 'Aakriti Jain', 'Thomas Wiegand', 'Sebastian Lapuschkin', 'Wojciech Samek']","['Fraunhofer HHI', 'AI Department, XAI Group, Fraunhofer HHI, Fraunhofer IAIS', 'Fraunhofer HHI', 'Fraunhofer HHI, Fraunhofer IAIS', 'Fraunhofer HHI', 'Fraunhofer HHI', 'Department of Electrical Engineering and Computer Science, TU Berlin']",
https://openreview.net/forum?id=afnyJfQddk,Fairness & Bias,Gaussian Processes on Cellular Complexes,"In recent years, there has been considerable interest in developing machine learning models on graphs to account for topological inductive biases. In particular, recent attention has been given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Matérn kernel and one that additionally mixes information of different cell types.",[],,"['Mathieu Alain', 'So Takao', 'Brooks Paige', 'Marc Peter Deisenroth']","['University College London', 'Computating and Mathematical Sciences, California Institute of Technology', 'University College London', 'Fundamental Research, Google']",
https://openreview.net/forum?id=5JrlywYHRi,Privacy & Data Governance,The Privacy Power of Correlated Noise in Decentralized Learning,"Decentralized learning is appealing as it enables the scalable usage of large amounts of distributed data and resources without resorting to any central entity, while promoting privacy since every user minimizes the direct exposure of their data. Yet, without additional precautions, curious users can still leverage models obtained from their peers to violate privacy. In this paper, we propose Decor, a variant of decentralized SGD with differential privacy (DP) guarantees. Essentially, in Decor, users securely exchange randomness seeds in one communication round to generate pairwise-canceling correlated Gaussian noises, which are injected to protect local models at every communication round. We theoretically and empirically show that, for arbitrary connected graphs, Decor matches the central DP optimal privacy-utility trade-off. We do so under SecLDP, our new relaxation of local DP, which protects all user communications against an external eavesdropper and curious users, assuming that every pair of connected users shares a secret, i.e., an information hidden to all others. The main theoretical challenge is to control the accumulation of non-canceling correlated noise due to network sparsity. We also propose a companion SecLDP privacy accountant for public use.",[],,"['Youssef Allouah', 'Anastasia Koloskova', 'Aymane El Firdoussi', 'Martin Jaggi', 'Rachid Guerraoui']","['EPFL - EPF Lausanne', 'Stanford University', 'Télécom ParisTech', 'EPFL', 'Swiss Federal Institute of Technology Lausanne']",
https://openreview.net/forum?id=iOEReiiTit,Security,Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing,"Certification for machine learning is proving that no adversarial sample can evade a model within a range under certain conditions, a necessity for safety-critical domains. Common certification methods for segmentation use a flat set of fine-grained classes, leading to high abstain rates due to model uncertainty across many classes. We propose a novel, more practical setting, which certifies pixels within a multi-level hierarchy, and adaptively relaxes the certification to a coarser level for unstable components classic methods would abstain from, effectively lowering the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup, introduce an adaptive hierarchical certification algorithm and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account for coarser classes, we introduce the Certified Information Gain ($\mathrm{CIG}$) metric, which is proportional to the class granularity level. Our extensive experiments on the datasets Cityscapes, PASCAL-Context, ACDC and COCO-Stuff demonstrate that our adaptive algorithm achieves a higher $\mathrm{CIG}$ and lower abstain rate compared to the current state-of-the-art certification method. Our code can be found here: [https://github.com/AlaaAnani/adaptive-certify](https://github.com/AlaaAnani/adaptive-certify).",[],,"['Alaa Anani', 'Tobias Lorenz', 'Bernt Schiele', 'Mario Fritz']","['Computer Vision and Machine Learning, Saarland Informatics Campus, Max-Planck Institute', 'Department of Computer Science, University of Oxford', 'Max Planck Institute for Informatics, Saarland Informatics Campus', 'CISPA Helmholtz Center for Information Security']",
https://openreview.net/forum?id=PTGJOUlQ68,Fairness & Bias,Private Vector Mean Estimation in the Shuffle Model: Optimal Rates Require Many Messages,"We study the problem of private vector mean estimation in the shuffle model of privacy where $n$ users each have a unit vector $v^{(i)} \in \mathbb{R}^d$. We propose a new multi-message protocol that achieves the optimal error using $O(\min(n\varepsilon^2,d))$ messages per user. Moreover, we show that any (unbiased) protocol that achieves optimal error must require each user to send $\Omega(\min(n\varepsilon^2,d)/\log(n))$ messages, demonstrating the optimality of our message complexity up to logarithmic factors. Additionally, we study the single-message setting and design a protocol that achieves mean squared error $O(dn^{d/(d+2)}\varepsilon^{-4/(d+2)})$. Moreover, we show that *any* single-message protocol must incur mean squared error $\Omega(dn^{d/(d+2)})$, showing that our protocol is optimal in the standard setting where $\varepsilon = \Theta(1)$. Finally, we study robustness to malicious users and show that malicious users can incur large additive error with a single shuffler.",[],,"['Hilal Asi', 'Vitaly Feldman', 'Jelani Nelson', 'Huy Nguyen', 'Kunal Talwar', 'Samson Zhou']","['Apple', 'Apple AI Research', 'University of California Berkeley', 'Northeastern University', 'Apple', 'Texas A&M University - College Station']",
https://openreview.net/forum?id=B5g6y7JlMw,Fairness & Bias,Random features models: a way to study the success of naive imputation,"Constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data. Yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data. However, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (MCAR). This paper completes the picture for linear predictors by confirming the intuition that the bias is negligible and that surprisingly naive imputation also remains relevant in very low dimension. To this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies. Building on these theoretical results, we establish finite-sample bounds on stochastic gradient (SGD) predictors applied to zero-imputed data, a strategy particularly well suited for large-scale learning. If the MCAR assumption appears to be strong, we show that similar favorable behaviors occur for more complex missing data scenarios.",[],,"['Alexis Ayme', 'Claire Boyer', 'Aymeric Dieuleveut', 'Erwan Scornet']","['LPSM', 'Université Paris-Saclay', 'Applied Mathematics, École Polytechnique', 'LPSM, Sorbonne Université - Faculté des Sciences (Paris VI)']",
https://openreview.net/forum?id=2NUGeV64y2,Security,Diffusion Models Demand Contrastive Guidance for Adversarial Purification to Advance,"In adversarial defense, adversarial purification can be viewed as a special generation task with the purpose to remove adversarial attacks and diffusion models excel in adversarial purification for their strong generative power. With different predetermined generation requirements, various types of guidance have been proposed, but few of them focuses on adversarial purification. In this work, we propose to guide diffusion models for adversarial purification using contrastive guidance. We theoretically derive the proper noise level added in the forward process diffusion models for adversarial purification from a feature learning perspective. For the reverse process, it is implied that the role of contrastive loss guidance is to facilitate the evolution towards the signal direction. From the theoretical findings and implications, we design the forward process with the proper amount of Gaussian noise added and the reverse process with the gradient of contrastive loss as the guidance of diffusion models for adversarial purification. Empirically, extensive experiments on CIFAR-10, CIFAR-100, the German Traffic Sign Recognition Benchmark and ImageNet datasets with ResNet and WideResNet classifiers show that our method outperforms most of current adversarial training and adversarial purification methods by a large improvement.",[],,"['Mingyuan Bai', 'Wei Huang', 'Tenghui Li', 'Andong Wang', 'Junbin Gao', 'Cesar F Caiafa', 'Qibin Zhao']","['RIKEN', 'RIKEN AIP', 'RIKEN AIP', 'Discipline of Business Analytics, University of Sydney', 'CONICET', 'AIP, RIKEN']",
https://openreview.net/forum?id=Eew3yUQQtE,Transparency & Explainability,On the Identifiability of Switching Dynamical Systems,"The identifiability of latent variable models has received increasing attention due to its relevance in interpretability and out-of-distribution generalisation. In this work, we study the identifiability of Switching Dynamical Systems, taking an initial step toward extending identifiability analysis to sequential latent variable models. We first prove the identifiability of Markov Switching Models, which commonly serve as the prior distribution for the continuous latent variables in Switching Dynamical Systems. We present identification conditions for first-order Markov dependency structures, whose transition distribution is parametrised via non-linear Gaussians. We then establish the identifiability of the latent variables and non-linear mappings in Switching Dynamical Systems up to affine transformations, by leveraging identifiability analysis techniques from identifiable deep latent variable models. We finally develop estimation algorithms for identifiable Switching Dynamical Systems. Throughout empirical studies, we demonstrate the practicality of identifiable Switching Dynamical Systems for segmenting high-dimensional time series such as videos, and showcase the use of identifiable Markov Switching Models for regime-dependent causal discovery in climate data.",[],,"['Carles Balsells-Rodas', 'Yixin Wang', 'Yingzhen Li']","['Imperial College London, Imperial College London', 'Statistics, University of Michigan - Ann Arbor', 'Imperial College London']",
https://openreview.net/forum?id=gUFufRkzjV,Security,VNN: Verification-Friendly Neural Networks with Hard Robustness Guarantees,"Machine learning techniques often lack formal correctness guarantees, evidenced by the widespread adversarial examples that plague most deep-learning applications. This lack of formal guarantees resulted in several research efforts that aim at verifying Deep Neural Networks (DNNs), with a particular focus on safety-critical applications. However, formal verification techniques still face major scalability and precision challenges. The over-approximation introduced during the formal verification process to tackle the scalability challenge often results in inconclusive analysis. To address this challenge, we propose a novel framework to generate Verification-Friendly Neural Networks (VNNs). We present a post-training optimization framework to achieve a balance between preserving prediction performance and verification-friendliness. Our proposed framework results in VNNs that are comparable to the original DNNs in terms of prediction performance, while amenable to formal verification techniques. This essentially enables us to establish robustness for more VNNs than their DNN counterparts, in a time-efficient manner.",[],,"['Anahita Baninajjar', 'Ahmed Rezine', 'Amir Aminifar']","['', '']",
https://openreview.net/forum?id=PY3bKuorBI,Fairness & Bias,Generalization in Kernel Regression Under Realistic Assumptions,"It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. When applied to common kernels, our results imply benign overfitting in high input dimensions, nearly tempered overfitting in fixed dimensions, and explicit convergence rates for regularized regression. As a by-product, we obtain time-dependent bounds for neural networks trained in the kernel regime. Our results rely on new relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the kernel implicitly leads to good generalization.",[],,"['Daniel Barzilai', 'Ohad Shamir']","['Weizmann Institute of Science', 'Weizmann Institute']",
https://openreview.net/forum?id=HQtTg1try7,Security,Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies,"This paper revisits the simple, long-studied, yet still unsolved problem of making image classifiers robust to imperceptible perturbations. Taking CIFAR10 as an example, SOTA clean accuracy is about $100$%, but SOTA robustness to $\ell_{\infty}$-norm bounded perturbations barely exceeds $70$%. To understand this gap, we analyze how model size, dataset size, and synthetic data quality affect robustness by developing the first scaling laws for adversarial training. Our scaling laws reveal inefficiencies in prior art and provide actionable feedback to advance the field. For instance, we discovered that SOTA methods diverge notably from compute-optimal setups, using excess compute for their level of robustness. Leveraging a compute-efficient setup, we surpass the prior SOTA with $20$% ($70$%) fewer training (inference) FLOPs. We trained various compute-efficient models, with our best achieving $74$% AutoAttack accuracy ($+3$% gain). However, our scaling laws also predict robustness slowly grows then plateaus at $90$%: dwarfing our new SOTA by scaling is impractical, and perfect robustness is impossible. To better understand this predicted limit, we carry out a small-scale human evaluation on the AutoAttack data that fools our top-performing model. Concerningly, we estimate that human performance also plateaus near $90$%, which we show to be attributable to $\ell_{\infty}$-constrained attacks' generation of invalid images not consistent with their original labels. Having characterized limiting roadblocks, we outline promising paths for future research.",[],,"['Brian R. Bartoldson', 'James Diffenderfer', 'Konstantinos Parasyris', 'Bhavya Kailkhura']","['Lawrence Livermore National Labs', 'Lawrence Livermore National Labs', 'Lawrence Livermore National Labs', 'CASC, Lawrence Livermore National Laboratory']",
https://openreview.net/forum?id=6EF0bxcZvT,Transparency & Explainability,Monotone Individual Fairness,"We revisit the problem of online learning with individual fairness, where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding fairness violations, to allow for auditing schemes that can aggregate feedback from any number of auditors, using a rich class we term monotone aggregation functions, for which we also prove a useful characterization. Using our generalized framework, we present an oracle-efficient algorithm guaranteeing a bound of $\mathcal{O}(T^\frac{3}{4})$ simultaneously for regret and number of fairness violations. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an algorithm guaranteeing a bound of $\mathcal{O}(T^\frac{5}{6})$ simultaneously for regret and number of fairness violations. In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms. Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle, as opposed to previous algorithms which required $T$ such calls every round.",[],,['Yahav Bechavod'],['University of Pennsylvania'],
https://openreview.net/forum?id=IGdpKP0N6w,Fairness & Bias,Neural Networks Learn Statistics of Increasing Complexity,"The _distributional simplicity bias_ (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.",[],,"['Nora Belrose', 'Quintin Pope', 'Lucia Quirke', 'Alex Troy Mallen', 'Xiaoli Fern']","['EleutherAI', 'Electrical Engineering and Computer Science, Oregon State University', 'University of Auckland', 'University of Washington, Seattle', 'Oregon State University']",
https://openreview.net/forum?id=CvRu2inbGV,Fairness & Bias,Standardized Interpretable Fairness Measures for Continuous Risk Scores,"We propose a standardized version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed standardized fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-based fairness measures miss.",[],,"['Ann-Kristin Becker', 'Oana Dumitrasc', 'Klaus Broelemann']","['SCHUFA Holding AG', 'Schufa Holding AG', 'SCHUFA']",
https://openreview.net/forum?id=Ez3Lckpe4l,Fairness & Bias,The Role of Learning Algorithms in Collective Action,"Collective action in machine learning is the study of the control that a coordinated group can have over machine learning algorithms. While previous research has concentrated on assessing the impact of collectives against Bayes (sub-)optimal classifiers, this perspective is limited in that it does not account for the choice of learning algorithm. Since classifiers seldom behave like Bayes classifiers and are influenced by the choice of learning algorithms along with their inherent biases, in this work we initiate the study of how the choice of the learning algorithm plays a role in the success of a collective in practical settings. Specifically, we focus on distributionally robust optimization (DRO), popular for improving a worst group error, and on the ubiquitous stochastic gradient descent (SGD), due to its inductive bias for ""simpler"" functions. Our empirical results, supported by a theoretical foundation, show that the effective size and success of the collective are highly dependent on properties of the learning algorithm. This highlights the necessity of taking the learning algorithm into account when studying the impact of collective action in machine learning.",[],,"['Omri Ben-Dov', 'Jake Fawkes', 'Samira Samadi', 'Amartya Sanyal']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'University of Oxford', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Computer Science, Copenhagen University']",
https://openreview.net/forum?id=ooikIHLHCs,Transparency & Explainability,Position: Explain to Question not to Justify,"Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. We separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). This position paper argues that the area of RED XAI is currently under-explored, i.e., more methods for explainability are desperately needed to question models (e.g., extract knowledge from well-performing models as well as spotting and fixing bugs in faulty models), and the area of RED XAI hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.",[],,"['Przemyslaw Biecek', 'Wojciech Samek']","['Warsaw University of Technology', 'Department of Electrical Engineering and Computer Science, TU Berlin']",
https://openreview.net/forum?id=KCVCFsPkrm,Privacy & Data Governance,Shifted Interpolation for Differential Privacy,"Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the “privacy amplification by iteration” phenomenon in the unifying framework of $f$-differential privacy---which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and Rényi DP. Our key technical insight is the construction of *shifted interpolated processes* that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first *exact* privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings.",[],,"['Jinho Bok', 'Weijie J Su', 'Jason Altschuler']","['Statistics and Data Science, The Wharton School, University of Pennsylvania', 'Statistics and Data Science, University of Pennsylvania, University of Pennsylvania', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=fqeANcjBMT,Fairness & Bias,Differentially Private Bias-Term Fine-tuning of Foundation Models,"We study the problem of differentially private (DP) fine-tuning of large pre-trained models — a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture. We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2 - 30X faster and uses 2 - 8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods.",[],,"['Zhiqi Bu', 'Yu-Xiang Wang', 'Sheng Zha', 'George Karypis']","['Amazon', 'University of California, San Diego', 'Amazon', 'University of Minnesota, Minneapolis']",
https://openreview.net/forum?id=xgoilgLPGD,Security,Langevin Policy for Safe Reinforcement Learning,"Optimization and sampling based algorithms are two branches of methods in machine learning, while existing safe reinforcement learning (RL) algorithms are mainly based on optimization, it is still unclear whether sampling based methods can lead to desirable performance with safe policy. This paper formulates the Langevin policy for safe RL, and proposes Langevin Actor-Critic (LAC) to accelerate the process of policy inference. Concretely, instead of parametric policy, the proposed Langevin policy provides a stochastic process that directly infers actions, which is the numerical solver to the Langevin dynamic of actions on the continuous time. Furthermore, to make Langevin policy practical on RL tasks, the proposed LAC accumulates the transitions induced by Langevin policy and reproduces them with a generator. Finally, extensive empirical results show the effectiveness and superiority of LAC on the MuJoCo-based and Safety Gym tasks.",[],,"['Fenghao Lei', 'Long Yang', 'Shiting Wen', 'Zhixiong Huang', 'Zhiwang Zhang', 'Chaoyi Pang']","['Zhejiang University', 'Peking University', 'School of Computer and Data Engineering, NingboTech University', 'Zhejiang University', '', 'Zhejiang University NIT']",
https://openreview.net/forum?id=LJcIIhqGDN,Security,Successor Features for Efficient Multi-Subject Controlled Text Generation,"While large language models (LLMs) have achieved impressive performance in generating fluent and realistic text, controlling the generated text so that it exhibits properties such as safety, factuality, and non-toxicity remains challenging. Existing decoding-based controllable text generation methods are static in terms of the dimension of control; if the target subject is changed, they require new training. Moreover, it can quickly become prohibitive to concurrently control multiple subjects. To address these challenges, we first show that existing methods can be framed as a reinforcement learning problem, where an action-value function estimates the likelihood of a desired attribute appearing in the generated text. Then, we introduce a novel approach named SF-Gen, which leverages the concept of successor features to decouple the dynamics of LLMs from task-specific rewards. By employing successor features, our method proves to be memory-efficient and computationally efficient for both training and decoding, especially when dealing with multiple target subjects. To the best of our knowledge, our research represents the first application of successor features in text generation. In addition to its computational efficiency, the resultant language produced by our method is comparable to the SOTA (and outperforms baselines) in both control measures as well as language quality, which we demonstrate through a series of experiments in various controllable text generation tasks.",[],,"['Meng Cao', 'Mehdi Fatemi', 'Jackie CK Cheung', 'Samira Shabanian']","['McGill University', 'Microsoft', 'McGill University', 'Microsoft']",
https://openreview.net/forum?id=3eHNvPHL9Z,Fairness & Bias,How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers,"A main theoretical puzzle is why over-parameterized Neural Networks (NNs) generalize well when trained to zero loss (i.e., so they interpolate the data). Usually, the NN is trained with Stochastic Gradient Descent (SGD) or one of its variants. However, recent empirical work examined the generalization of a random NN that interpolates the data: the NN was sampled from a seemingly uniform prior over the parameters, conditioned on that the NN perfectly classifying the training set. Interestingly, such a NN sample typically generalized as well as SGD-trained NNs. We prove that such a random NN interpolator typically generalizes well if there exists an underlying narrow ``teacher NN"" that agrees with the labels. Specifically, we show that such a `flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure. In particular, this creates a bias towards simpler functions, which require less relevant parameters to represent --- enabling learning with a sample complexity approximately proportional to the complexity of the teacher (roughly, the number of non-redundant parameters), rather than the student's.",[],,"['Gon Buzaglo', 'Itamar Harel', 'Mor Shpigel Nacson', 'Alon Brutzkus', 'Nathan Srebro', 'Daniel Soudry']","['Electrical & Copmuter Engineering , Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'The Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'Technion, Technion', 'GSK plc', 'Toyota Technological Institute at Chicago', 'Electrical and Computer Engineering, Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=y8YovS0lOg,Fairness & Bias,On the Implicit Bias of Adam,"In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different ""norm"" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.",[],,"['Matias D. Cattaneo', 'Jason Matthew Klusowski', 'Boris Shigida']","['Princeton University', 'Princeton University', 'ORFE, Princeton University']",
https://openreview.net/forum?id=fywWm06IGn,Fairness & Bias,Feature Importance Disparities for Data Bias Investigations,"It is widely held that one cause of downstream bias in classifiers is bias present in the training data. Rectifying such biases may involve context-dependent interventions such as training separate models on subgroups, removing features with bias in the collection process, or even conducting real-world experiments to ascertain sources of bias. Despite the need for such data bias investigations, few automated methods exist to assist practitioners in these efforts. In this paper, we present one such method that given a dataset $X$ consisting of protected and unprotected features, outcomes $y$, and a regressor $h$ that predicts $y$ given $X$, outputs a tuple $(f_j, g)$, with the following property: $g$ corresponds to a subset of the training dataset $(X, y)$, such that the $j^{th}$ feature $f_j$ has much larger (or smaller) *influence* in the subgroup $g$, than on the dataset overall, which we call *feature importance disparity* (FID). We show across $4$ datasets and $4$ common feature importance methods of broad interest to the machine learning community that we can efficiently find subgroups with large FID values even over exponentially large subgroup classes and in practice these groups correspond to subgroups with potentially serious bias issues as measured by standard fairness metrics.",[],,"['Peter W Chang', 'Leor Fishman', 'Seth Neel']","['Harvard University', 'Harvard University', 'Harvard University']",
https://openreview.net/forum?id=FVmqX0sYz9,Transparency & Explainability,Auditing Private Prediction,"Differential privacy (DP) offers a theoretical upper bound on the potential privacy leakage of an algorithm, while empirical auditing establishes a practical lower bound. Auditing techniques exist for DP training algorithms. However machine learning can also be made private at inference. We propose the first framework for auditing private prediction where we instantiate adversaries with varying poisoning and query capabilities. This enables us to study the privacy leakage of four private prediction algorithms: PATE (Papernot et al., 2016), CaPC (Choquette-Choo et al., 2020), PromptPATE (Duan et al., 2023), and Private-kNN (Zhu et al., 2020). To conduct our audit, we introduce novel techniques to empirically evaluate privacy leakage in terms of Renyi DP. Our experiments show that (i) the privacy analysis of private prediction can be improved, (ii) algorithms which are easier to poison lead to much higher privacy leakage, and (iii) the privacy leakage is significantly lower for adversaries without query control than those with full control.",[],,"['Karan Chadha', 'Matthew Jagielski', 'Nicolas Papernot', 'Christopher A. Choquette-Choo', 'Milad Nasr']","['Stanford University', 'Google', 'University of Toronto', 'Google DeepMind', 'Google']",
https://openreview.net/forum?id=JNHK11bAGl,Security,Feasibility Consistent Representation Learning for Safe Reinforcement Learning,"In the field of safe reinforcement learning (RL), finding a balance between satisfying safety constraints and optimizing reward performance presents a significant challenge. A key obstacle in this endeavor is the estimation of safety constraints, which is typically more difficult than estimating a reward metric due to the sparse nature of the constraint signals. To address this issue, we introduce a novel framework named Feasibility Consistent Safe Reinforcement Learning (FCSRL). This framework combines representation learning with feasibility-oriented objectives to identify and extract safety-related information from the raw state for safe RL. Leveraging self-supervised learning techniques and a more learnable safety metric, our approach enhances the policy learning and constraint estimation. Empirical evaluations across a range of vector-state and image-based tasks demonstrate that our method is capable of learning a better safety-aware embedding and achieving superior performance than previous representation learning baselines.",[],,"['Zhepeng Cen', 'Yihang Yao', 'Zuxin Liu', 'Ding Zhao']","['CMU, Carnegie Mellon University', 'Carnegie Mellon University', 'Salesforce AI Research', 'Carnegie Mellon University']",
https://openreview.net/forum?id=F3G2udCF3Q,Transparency & Explainability,How Interpretable Are Interpretable Graph Neural Networks?,"Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.",[],,"['Yongqiang Chen', 'Yatao Bian', 'Bo Han', 'James Cheng']","['Mohamed bin Zayed University of Artificial Intelligence', 'Tencent AI Lab', 'Department of Computer Science, HKBU', 'The Chinese University of Hong Kong']",
https://openreview.net/forum?id=xaSpuvNYwS,Security,Robust Classification via a Single Diffusion Model,"Diffusion models have been applied to improve adversarial robustness of image classifiers by purifying the adversarial noises or generating realistic data for adversarial training. However, diffusion-based purification can be evaded by stronger adaptive attacks while adversarial training does not perform well under unseen threats, exhibiting inevitable limitations of these methods. To better harness the expressive power of diffusion models, this paper proposes Robust Diffusion Classifier (RDC), a generative classifier that is constructed from a pre-trained diffusion model to be adversarially robust. RDC first maximizes the data likelihood of a given input and then predicts the class probabilities of the optimized input using the conditional likelihood estimated by the diffusion model through Bayes' theorem. To further reduce the computational cost, we propose a new diffusion backbone called multi-head diffusion and develop efficient sampling strategies. As RDC does not require training on particular adversarial attacks, we demonstrate that it is more generalizable to defend against multiple unseen threats. In particular, RDC achieves $75.67\%$ robust accuracy against various $\ell_\infty$ norm-bounded adaptive attacks with $\epsilon_\infty=8/255$ on CIFAR-10, surpassing the previous state-of-the-art adversarial training models by $+4.77\%$. The results highlight the potential of generative classifiers by employing pre-trained diffusion models for adversarial robustness compared with the commonly studied discriminative classifiers.",[],,"['Huanran Chen', 'Yinpeng Dong', 'Zhengyi Wang', 'Xiao Yang', 'Chengqi Duan', 'Hang Su', 'Jun Zhu']","['AI, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University, Tsinghua University', 'University of Hong Kong', 'Computer Science, Tsinghua University', 'Computer Science, Tsinghua University']",
https://openreview.net/forum?id=5lI9wm4dws,Fairness & Bias,Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning,"Causal effect estimation under networked interference is an important but challenging problem. Available parametric methods are limited in their model space, while previous semiparametric methods, e.g., leveraging neural networks to fit only one single nuisance function, may still encounter misspecification problems under networked interference without appropriate assumptions on the data generation process. To mitigate bias stemming from misspecification, we propose a novel doubly robust causal effect estimator under networked interference, by adapting the targeted learning technique to the training of neural networks. Specifically, we generalize the targeted learning technique into the networked interference setting and establish the condition under which an estimator achieves double robustness. Based on the condition, we devise an end-to-end causal effect estimator by transforming the identified theoretical condition into a targeted loss. Moreover, we provide a theoretical analysis of our designed estimator, revealing a faster convergence rate compared to a single nuisance model. Extensive experimental results on two real-world networks with semisynthetic data demonstrate the effectiveness of our proposed estimators.",[],,"['Weilin Chen', 'Ruichu Cai', 'Zeqin Yang', 'Jie Qiao', 'Yuguang Yan', 'Zijian Li', 'Zhifeng Hao']","['Guangdong University of Technology', 'School of Computer Science, Guangdong University of Technology', 'School of Computer Science and Technology, Guangdong University of Technology', 'Guangdong University of Technology', 'Guangdong University of Technology', 'Mohamed bin Zayed University of Artificial Intelligence', 'Shantou University']",
https://openreview.net/forum?id=QAGRPiC3FS,Fairness & Bias,RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content,"Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats.",[],,"['Zhuowen Yuan', 'Zidi Xiong', 'Yi Zeng', 'Ning Yu', 'Ruoxi Jia', 'Dawn Song', 'Bo Li']","['University of Illinois Urbana-Champaign', 'Harvard University', 'Virginia Tech', '', 'Virginia Tech', 'University of California Berkeley', 'CS, University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=J6prHJsIlf,Transparency & Explainability,Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation,"We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs' discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via https://github.com/DMIRLAB-Group/FANS.",[],,"['Xuexin Chen', 'Ruichu Cai', 'ZhengTingHuang', 'Yuxuan Zhu', 'Julien Horwood', 'Zhifeng Hao', 'Zijian Li', 'José Miguel Hernández-Lobato']","['Guangdong University of Technology', 'School of Computer Science, Guangdong University of Technology', 'Guangdong University of Technology', 'S&R&A, Shopee Pte. Ltd', 'University of Cambridge', 'Shantou University', 'Mohamed bin Zayed University of Artificial Intelligence', 'University of Cambridge']",
https://openreview.net/forum?id=61RlaY9EIn,Privacy & Data Governance,MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective,"The growing richness of large-scale datasets has been crucial in driving the rapid advancement and wide adoption of machine learning technologies. The massive collection and usage of data, however, pose an increasing risk for people's private and sensitive information due to either inadvertent mishandling or malicious exploitation. Besides legislative solutions, many technical approaches have been proposed towards data privacy protection. However, they bear various limitations such as leading to degraded data availability and utility, or relying on heuristics and lacking solid theoretical bases. To overcome these limitations, we propose a formal information-theoretic definition for this utility-preserving privacy protection problem, and design a data-driven learnable data transformation framework that is capable of selectively suppressing sensitive attributes from target datasets while preserving the other useful attributes, regardless of whether or not they are known in advance or explicitly annotated for preservation. We provide rigorous theoretical analyses on the operational bounds for our framework, and carry out comprehensive experimental evaluations using datasets of a variety of modalities, including facial images, voice audio clips, and human activity motion sensor signals. Results demonstrate the effectiveness and generalizability of our method under various configurations on a multitude of tasks. Our source code is available at this [URL](https://arxiv.org/abs/2405.14981).",[],,"['Yizhuo Chen', 'Chun-Fu Chen', 'Hsiang Hsu', 'Shaohan Hu', 'Marco Pistoia', 'Tarek F. Abdelzaher']","['University of Illinois', 'JPMorganChase, GTAR', 'JP Morgan & Chase Bank', 'J.P. Morgan Chase', 'J.P. Morgan Chase', 'University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=UQYXZdca92,Fairness & Bias,Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes,"We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be learned efficiently by square loss regression over the time-series data. We show that the drift and the diffusion coefficients of this SDE can be adjusted after training, and that a specific choice that minimizes the impact of the estimation error gives a Föllmer process. We highlight the utility of our approach on several complex, high-dimensional forecasting problems, including stochastically forced Navier-Stokes and video prediction on the KTH and CLEVRER datasets. The code is available at https://github.com/interpolants/forecasting.",[],,"['Yifan Chen', 'Mark Goldstein', 'Mengjian Hua', 'Michael Samuel Albergo', 'Nicholas Matthew Boffi', 'Eric Vanden-Eijnden']","['New York University', 'New York University', 'Courant Institute of Mathematical Sciences, New York University', 'Society of Fellows, Harvard University', 'NYU, New York University', 'New York University']",
https://openreview.net/forum?id=bBzlapzeR1,Fairness & Bias,High-Dimensional Kernel Methods under Covariate Shift: Data-Dependent Implicit Regularization,"This paper studies kernel ridge regression in high dimensions under covariate shifts and analyzes the role of importance re-weighting. We first derive the asymptotic expansion of high dimensional kernels under covariate shifts. By a bias-variance decomposition, we theoretically demonstrate that the re-weighting strategy allows for decreasing the variance. For bias, we analyze the regularization of the arbitrary or well-chosen scale, showing that the bias can behave very differently under different regularization scales. In our analysis, the bias and variance can be characterized by the spectral decay of a data-dependent regularized kernel: the original kernel matrix associated with an additional re-weighting matrix, and thus the re-weighting strategy can be regarded as a data-dependent regularization for better understanding. Besides, our analysis provides asymptotic expansion of kernel functions/vectors under covariate shift, which has its own interest.",[],,"['Yihang Chen', 'Fanghui Liu', 'Taiji Suzuki', 'Volkan Cevher']","['University of California, Los Angeles', 'Department of Computer Science, University of Warwick', 'The University of Tokyo', 'EPFL - EPF Lausanne']",
https://openreview.net/forum?id=dqpg8jdA2w,Fairness & Bias,Offline Transition Modeling via Contrastive Energy Learning,"Learning a high-quality transition model is of great importance for sequential decision-making tasks, especially in offline settings. Nevertheless, the complex behaviors of transition dynamics in real-world environments pose challenges for the standard forward models because of their inductive bias towards smooth regressors, conflicting with the inherent nature of transitions such as discontinuity or large curvature. In this work, we propose to model the transition probability implicitly through a scalar-value energy function, which enables not only flexible distribution prediction but also capturing complex transition behaviors. The Energy-based Transition Models (ETM) are shown to accurately fit the discontinuous transition functions and better generalize to out-of-distribution transition data. Furthermore, we demonstrate that energy-based transition models improve the evaluation accuracy and significantly outperform other off-policy evaluation methods in DOPE benchmark. Finally, we show that energy-based transition models also benefit reinforcement learning and outperform prior offline RL algorithms in D4RL Gym-Mujoco tasks.",[],,"['Ruifeng Chen', 'Chengxing Jia', 'Zefang Huang', 'Tian-Shuo Liu', 'Xu-Hui Liu', 'Yang Yu']","['Nanjing University', 'Nanjing University', 'Computer Science and Technology, Nanjing University', 'Nanjing university', 'Nanjing University', 'School of Artificial Intelligence, Nanjing University']",
https://openreview.net/forum?id=x1G7ieRgRd,Privacy & Data Governance,Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under Streaming Differential Privacy,"We study $L_2$ mean estimation under central differential privacy and communication constraints, and address two key challenges: firstly, existing mean estimation schemes that simultaneously handle both constraints are usually optimized for $L_\infty$ geometry and rely on random rotation or Kashin's representation to adapt to $L_2$ geometry, resulting in suboptimal leading constants in mean square errors (MSEs); secondly, schemes achieving order-optimal communication-privacy trade-offs do not extend seamlessly to streaming differential privacy (DP) settings (e.g., tree aggregation or matrix factorization), rendering them incompatible with DP-FTRL type optimizers. In this work, we tackle these issues by introducing a novel privacy accounting method for the sparsified Gaussian mechanism that incorporates the randomness inherent in sparsification into the DP noise. Unlike previous approaches, our accounting algorithm directly operates in $L_2$ geometry, yielding MSEs that fast converge to those of the uncompressed Gaussian mechanism. Additionally, we extend the sparsification scheme to the matrix factorization framework under streaming DP and provide a precise accountant tailored for DP-FTRL type optimizers. Empirically, our method demonstrates at least a 100x improvement of compression for DP-SGD across various FL tasks.",[],,"['Wei-Ning Chen', 'Berivan Isik', 'Peter Kairouz', 'Albert No', 'Sewoong Oh', 'Zheng Xu']","['Stanford University', 'Google', 'Google', 'Yonsei University', 'Allen school, University of Washington', 'Google']",
https://openreview.net/forum?id=JU3xHh1vWw,Fairness & Bias,Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank,"Unbiased Learning to Rank (ULTR) aims to train unbiased ranking models from biased click logs, by explicitly modeling a generation process for user behavior and fitting click data based on examination hypothesis. Previous research found empirically that the true latent relevance is mostly recoverable through click fitting. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. This research investigates the conditions under which relevance can be recovered from click data in the first principle. We initially characterize a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, a criterion sufficient for the pairwise ranking objective. Subsequently, we investigate an equivalent condition for identifiability, articulated as a graph connectivity test problem: the recovery of relevance is feasible if and only if the identifiability graph (IG), derived from the underlying structure of the dataset, is connected. The presence of a disconnected IG may lead to degenerate cases and suboptimal ranking performance. To tackle this challenge, we introduce two methods, namely node intervention and node merging, designed to modify the dataset and restore the connectivity of the IG. Empirical results derived from a simulated dataset and two real-world LTR benchmark datasets not only validate our proposed theory, but also demonstrate the effectiveness of our methods in alleviating data bias when the relevance model is unidentifiable.",[],,"['Mouxiang Chen', 'Chenghao Liu', 'Zemin Liu', 'Zhuo Li', 'Jianling Sun']","['Zhejiang University', 'SalesForce.com', 'College of Computer Science and Technology, Zhejiang University', 'Zhejiang University', 'Zhejiang University']",
https://openreview.net/forum?id=RuH78kOcDi,Privacy & Data Governance,Locally Differentially Private Decentralized Stochastic Bilevel Optimization with Guaranteed Convergence Accuracy,"Decentralized bilevel optimization based machine learning techniques are achieving remarkable success in a wide variety of domains. However, the intensive exchange of information (involving nested-loops of consensus or communication iterations) in existing decentralized bilevel optimization algorithms leads to a great challenge to ensure rigorous differential privacy, which, however, is necessary to bring the benefits of machine learning to domains where involved data are sensitive. By proposing a new decentralized stochastic bilevel-optimization algorithm which avoids nested-loops of information-exchange iterations, we achieve, for the first time, both differential privacy and accurate convergence in decentralized bilevel optimization. This is significant since even for single-level decentralized optimization and learning, existing differential-privacy solutions have to sacrifice convergence accuracy for privacy. Besides characterizing the convergence rate under nonconvex/convex/strongly convex conditions, we also rigorously quantify the price of differential privacy in the convergence rate. Experimental results on machine learning models confirm the efficacy of our algorithm.",[],,"['Ziqin Chen', 'Yongqiang Wang']","['Clemson University', 'Clemson University']",
https://openreview.net/forum?id=E41gvBG4s6,Security,Recovering Labels from Local Updates in Federated Learning,"Gradient inversion (GI) attacks present a threat to the privacy of clients in federated learning (FL) by aiming to enable reconstruction of the clients' data from communicated model updates. A number of such techniques attempts to accelerate data recovery by first reconstructing labels of the samples used in local training. However, existing label extraction methods make strong assumptions that typically do not hold in realistic FL settings. In this paper we present a novel label recovery scheme, Recovering Labels from Local Updates (RLU), which provides near-perfect accuracy when attacking untrained (most vulnerable) models. More significantly, RLU achieves high performance even in realistic real-world settings where the clients in an FL system run multiple local epochs, train on heterogeneous data, and deploy various optimizers to minimize different objective functions. Specifically, RLU estimates labels by solving a least-square problem that emerges from the analysis of the correlation between labels of the data points used in a training round and the resulting update of the output layer. The experimental results on several datasets, architectures, and data heterogeneity scenarios demonstrate that the proposed method consistently outperforms existing baselines, and helps improve quality of the reconstructed images in GI attacks in terms of both PSNR and LPIPS.",[],,"['Huancheng Chen', 'Haris Vikalo']","['Department of Electrical and Computer Engineering, University of Texas, Austin', '']",
https://openreview.net/forum?id=VOcsmIBiXE,Transparency & Explainability,Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning,"Operator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism---a powerful tool originally designed for natural language processing---have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: *Is there a more efficient attention mechanism for Transformer-based operator learning?* This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Position-attention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.",[],,"['Junfeng CHEN', 'Kailiang Wu']","['Department of Mathematics, Southern University of Science and Technology', 'Southern University of Science and Technology']",
https://openreview.net/forum?id=B48Pzc4oKi,Transparency & Explainability,LLaGA: Large Language and Graph Assistant,"Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the the **L**arge **L**anguage **a**nd **G**raph **A**ssistant (**LLaGA**), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for graphs. Our extensive experiments across popular graph benchmarks show that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art graph models in both supervised and zero-shot scenarios.",[],,"['Runjin Chen', 'Tong Zhao', 'AJAY KUMAR JAISWAL', 'Neil Shah', 'Zhangyang Wang']","['', 'Snap Inc.', 'Apple', 'Snap Inc.', 'University of Texas at Austin']",
https://openreview.net/forum?id=0xmfExPqFf,Security,Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation,"In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: RS-DisRL-M, a model-based strategy for model-based function approximation, and RS-DisRL-V, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Markov Decision Process (MDP), we derive the first $\widetilde{\mathcal{O}}(\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain.",[],,"['Yu Chen', 'XiangCheng Zhang', 'Siwei Wang', 'Longbo Huang']","['Institution for Interdisciplinary Information Sciences (IIIS), Tsinghua University', 'Tsinghua University, Tsinghua University', 'Microsoft', 'IIIS, Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=w5oUo0LhO1,Fairness & Bias,Kernel Semi-Implicit Variational Inference,"Semi-implicit variational inference (SIVI) extends traditional variational families with semi-implicit distributions defined in a hierarchical manner. Due to the intractable densities of semi-implicit distributions, classical SIVI often resorts to surrogates of evidence lower bound (ELBO) that would introduce biases for training. A recent advancement in SIVI, named SIVI-SM, utilizes an alternative score matching objective made tractable via a minimax formulation, albeit requiring an additional lower-level optimization. In this paper, we propose kernel SIVI (KSIVI), a variant of SIVI-SM that eliminates the need for the lower-level optimization through kernel tricks. Specifically, we show that when optimizing over a reproducing kernel Hilbert space (RKHS), the lower-level problem has an explicit solution. This way, the upper-level objective becomes the kernel Stein discrepancy (KSD), which is readily computable for stochastic gradient descent due to the hierarchical structure of semi-implicit variational distributions. An upper bound for the variance of the Monte Carlo gradient estimators of the KSD objective is derived, which allows us to establish novel convergence guarantees of KSIVI. We demonstrate the effectiveness and efficiency of KSIVI on both synthetic distributions and a variety of real data Bayesian inference tasks.",[],,"['Ziheng Cheng', 'Longlin Yu', 'Tianyu Xie', 'Shiyue Zhang', 'Cheng Zhang']","['IEOR, University of California, Berkeley', 'Peking University', 'School of Mathematical Sciences, Peking University', 'Peking University', 'School of Mathematical Sciences, Peking University']",
https://openreview.net/forum?id=haUOhXo70o,Fairness & Bias,Hard Tasks First: Multi-Task Reinforcement Learning Through Task Scheduling,"Multi-task reinforcement learning (RL) faces the significant challenge of varying task difficulties, often leading to negative transfer when simpler tasks overshadow the learning of more complex ones. To overcome this challenge, we propose a novel algorithm, Scheduled Multi-Task Training (SMT), that strategically prioritizes more challenging tasks, thereby enhancing overall learning efficiency. SMT introduces a dynamic task prioritization strategy, underpinned by an effective metric for assessing task difficulty. This metric ensures an efficient and targeted allocation of training resources, significantly improving learning outcomes. Additionally, SMT incorporates a reset mechanism that periodically reinitializes key network parameters to mitigate the simplicity bias, further enhancing the adaptability and robustness of the learning process across diverse tasks. The efficacy of SMT's scheduling method is validated by significantly improving performance on challenging Meta-World benchmarks.",[],,"['Myungsik Cho', 'Jongeui Park', 'Suyoung Lee', 'Youngchul Sung']","['Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Language Intelligence Team, Samsung Research', 'School of Electrical Engineering, Korea Advanced Institute of Science and Technology']",
https://openreview.net/forum?id=vq7ITv8a49,Fairness & Bias,"Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters","When estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce ""plug-in bias."" Traditional methods addressing this suboptimal bias-variance trade-off rely on the influence function (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, which poses analytical and computational challenges. In this work, we leverage the targeted maximum likelihood estimation (TMLE) framework to propose a novel method named kernel debiased plug-in estimation (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on reproducing kernel Hilbert spaces. We show that KDPE: (i) simultaneously debiases all pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results.",[],,"['Brian M Cho', 'Yaroslav Mukhin', 'Kyra Gan', 'Ivana Malenica']","['ORIE, Cornell University', '', '', 'Harvard University']",
https://openreview.net/forum?id=uGoi3nY62g,Security,BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks,"Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and optical flow estimation (OFE)) have been widely involved in our daily life in applications like autonomous driving, augmented reality and video composition. Although certain applications are security-critical or bear societal significance, the adversarial robustness of such models are not sufficiently studied, especially in the black-box scenario. In this work, we introduce the first unified black-box adversarial patch attack framework against pixel-wise regression tasks, aiming to identify the vulnerabilities of these models under query-based black-box attacks. We propose a novel square-based adversarial patch optimization framework and employ probabilistic square sampling and score-based gradient estimation techniques to generate the patch effectively and efficiently, overcoming the scalability problem of previous black-box patch attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in terms of both attack performance and efficiency. We also apply BadPart on the Google online service for portrait depth estimation, causing 43.5% relative distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot defend our attack effectively.",[],,"['Zhiyuan Cheng', 'Zhaoyi Liu', 'Tengda Guo', 'Shiwei Feng', 'Dongfang Liu', 'Mingjie Tang', 'Xiangyu Zhang']","['Dept. of Computer Science, Purdue University', 'Computer Science, University of Illinois at Urbana-Champaign', 'Sichuan University', 'Purdue University', 'Rochester Institute of Technology', 'CS, Purdue University', 'Computer Science, Purdue University']",
https://openreview.net/forum?id=WvIHbQhrTq,Fairness & Bias,Leveraging (Biased) Information: Multi-armed Bandits with Offline Data,"We leverage offline data to facilitate online learning in stochastic multi-armed bandits. The probability distributions that govern the offline data and the online rewards can be different. Without any non-trival upper bound on their difference, we show that no non-anticipatory policy can out-perform the UCB policy by (Auer et al. 2002), even in the presence of offline data. In complement, we propose an online policy MIN-UCB, which outperforms UCB when a non-trivial upper bound is given. MIN-UCB adaptively chooses to utilize the offline data when they are deemed informative, and to ignore them otherwise. MIN-UCB is shown to be tight in terms of both instance indepedent and dependent regret bounds. Finally, we corroborate the theoretical results with numerical experiments.",[],,"['Wang Chi Cheung', 'Lixing Lyu']","['Industrial Systems Engineering and Management, National University of Singapore', 'national university of singaore, National University of Singapore']",
https://openreview.net/forum?id=CiZN2OATRp,Fairness & Bias,Statistical Inference Under Constrained Selection Bias,"Large-scale datasets are increasingly being used to inform decision making. While this effort aims to ground policy in real-world evidence, challenges have arisen as selection bias and other forms of distribution shifts often plague observational data. Previous attempts to provide robust inference have given guarantees depending on a user-specified amount of possible distribution shift (e.g., the maximum KL divergence between the observed and target distributions). However, decision makers will often have additional knowledge about the target distribution which constrains the kind of possible shifts. To leverage such information, we propose a framework that enables statistical inference in the presence of selection bias which obeys user-specified constraints in the form of functions whose expectation is known under the target distribution. The output is high-probability bounds on the value of an estimand for the target distribution. Hence, our method leverages domain knowledge in order to partially identify a wide class of estimands. We analyze the computational and statistical properties of methods to estimate these bounds and show that our method can produce informative bounds on a variety of simulated and semisynthetic tasks, as well as in a real-world use case.",[],,"['Santiago Cortes-Gomez', 'Mateo Dulce Rubio', 'Carlos Miguel Patiño', 'Bryan Wilder']","['School of Computer Science, Carnegie Mellon University', 'CMU, Carnegie Mellon University', 'Graduate School of Informatics, University of Amsterdam', 'Carnegie Mellon University']",
https://openreview.net/forum?id=BOorDpKHiJ,Fairness & Bias,ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback,"Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality AI feedback automatically for a scalable alternative. Specifically, we identify scale and diversity as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present UltraFeedback, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon UltraFeedback, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research.",[],,"['Ganqu Cui', 'Lifan Yuan', 'Ning Ding', 'Guanming Yao', 'Bingxiang He', 'Wei Zhu', 'Yuan Ni', 'Guotong Xie', 'Ruobing Xie', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun']","['Tsinghua University, Tsinghua University', '', 'Tsinghua University, Tsinghua University', '', 'Computer Science and Technology, Tsinghua University, Tsinghua University', 'University of Hong Kong', 'Pingan Technology', 'Pingan Technology', 'Tencent', 'Renmin University of China', 'Department of Computer Science and Technology, Tsinghua University', 'Department of Computer Science and Technology, Tsinghua University']",
https://openreview.net/forum?id=BiENLaUwlK,Security,Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation,"A key aspect of Safe Reinforcement Learning (Safe RL) involves estimating the constraint condition for the next policy, which is crucial for guiding the optimization of safe policy updates. However, the existing *Advantage-based Estimation* (ABE) method relies on the infinite-horizon discounted advantage function. This dependence leads to catastrophic errors in finite-horizon scenarios with non-discounted constraints, resulting in safety-violation updates. In response, we propose the first estimation method for finite-horizon non-discounted constraints in deep Safe RL, termed *Gradient-based Estimation* (GBE), which relies on the analytic gradient derived along trajectories. Our theoretical and empirical analyses demonstrate that GBE can effectively estimate constraint changes over a finite horizon. Constructing a surrogate optimization problem with GBE, we developed a novel Safe RL algorithm called *Constrained Gradient-based Policy Optimization* (CGPO). CGPO identifies feasible optimal policies by iteratively resolving sub-problems within trust regions. Our empirical results reveal that CGPO, unlike baseline algorithms, successfully estimates the constraint functions of subsequent policies, thereby ensuring the efficiency and feasibility of each update.",[],,"['Juntao Dai', 'Yaodong Yang', 'Qian Zheng', 'Gang Pan']","['Computer Science and Technology Department, Zhejiang University', 'Peking University', 'Computer Science and Technology, Zhejiang University', '']",
https://openreview.net/forum?id=F3RdeyiR5H,Transparency & Explainability,Trust Regions for Explanations via Black-Box Probabilistic Certification,"Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a *trust region* has multiple benefits: i) insight into model behavior in a *region*, with a *guarantee*; ii) ascertained *stability* of the explanation; iii) *explanation reuse*, which can save time, energy and money by not having to find explanations for every example; and iv) a possible *meta-metric* to compare explanation methods. Our contributions include formalizing this problem, proposing solutions, providing theoretical guarantees for these solutions that are computable, and experimentally showing their efficacy on synthetic and real data.",[],,"['Amit Dhurandhar', 'Swagatam Haldar', 'Dennis Wei', 'Karthikeyan Natesan Ramamurthy']","['Trustworthy AI, International Business Machines', 'Eberhard-Karls-Universität Tübingen', 'International Business Machines', 'Trustworthy AI, International Business Machines']",
https://openreview.net/forum?id=XQz7ytgETQ,Fairness & Bias,Network Tight Community Detection,"Conventional community detection methods often categorize all nodes into clusters. However, the presumed community structure of interest may only be valid for a subset of nodes (named as `tight nodes'), while the rest of the network may consist of noninformative ``scattered nodes''. For example, a protein-protein network often contains proteins that do not belong to specific biological functional modules but are involved in more general processes, or act as bridges between different functional modules. Forcing each of these proteins into a single cluster introduces unwanted biases and obscures the underlying biological implication. To address this issue, we propose a tight community detection (TCD) method to identify tight communities excluding scattered nodes. The algorithm enjoys a strong theoretical guarantee of tight node identification accuracy and is scalable for large networks. The superiority of the proposed method is demonstrated by various synthetic and real experiments.",[],,"['Jiayi Deng', 'Xiaodong Yang', 'Jun Yu', 'Jun Liu', 'Zhaiming Shen', 'Danyang Huang', 'Huimin Cheng']","['Renmin University of China', 'Harvard University, Harvard University', 'Beijing Institute of Technology', 'Harvard University', 'Georgia Institute of Technology', 'Boston University, Boston University']",
https://openreview.net/forum?id=YEQM0asWCH,Transparency & Explainability,Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning,"Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models force a tradeoff between accuracy and interpretability, limiting data-driven interpretations of human decision-making processes. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically under different contexts. Thus, we develop Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem, where each context poses a unique task and complex decision policies can be constructed piece-wise from many simple context-specific policies. CPR models each context-specific policy as a linear map, and generates new policy models _on-demand_ as contexts are updated with new observations. We provide two flavors of the CPR framework: one focusing on exact local interpretability, and one retaining full global interpretability. We assess CPR through studies on simulated and real data, achieving state-of-the-art performance on predicting antibiotic prescription in intensive care units ($+22$% AUROC vs. previous SOTA) and predicting MRI prescription for Alzheimer's patients ($+7.7$% AUROC vs. previous SOTA). With this improvement, CPR closes the accuracy gap between interpretable and black-box methods, allowing high-resolution exploration and analysis of context-specific decision models.",[],,"['Jannik Deuschel', 'Caleb Ellington', 'Yingtao Luo', 'Ben Lengerich', 'Pascal Friederich', 'Eric P. Xing']","['CMU, Carnegie Mellon University', '', 'Machine Learning Department, School of Computer Science, Carnegie Mellon University', 'University of Wisconsin - Madison', 'Karlsruher Institut für Technologie', 'Mohamed bin Zayed Univeristy of AI']",
https://openreview.net/forum?id=MV2b44zDd3,Security,Consistent Adversarially Robust Linear Classification: Non-Parametric Setting,"For binary classification in $d$ dimensions, it is known that with a sample size of $n$, an excess adversarial risk of $O(d/n)$ is achievable under strong parametric assumptions about the underlying data distribution (e.g., assuming a Gaussian mixture model). In the case of well-separated distributions, this rate can be further refined to $O(1/n)$. Our work studies the non-parametric setting, where very little is known. With only mild regularity conditions on the conditional distribution of the features, we examine adversarial attacks with respect to arbitrary norms and introduce a straightforward yet effective estimator with provable consistency w.r.t adversarial risk. Our estimator is given by minimizing a series of smoothed versions of the robust 0/1 loss, with a smoothing bandwidth that adapts to both $n$ and $d$. Furthermore, we demonstrate that our estimator can achieve the minimax excess adversarial risk of $\widetilde O(\sqrt{d/n})$ for linear classifiers, at the cost of solving possibly rougher optimization problems.",[],,['Elvis Dohmatob'],['Facebook'],
https://openreview.net/forum?id=btYeH65fI3,Security,Precise Accuracy / Robustness Tradeoffs in Regression: Case of General Norms,"In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work covers feature covariance matrices and attack norms of any nature, extending previous works in this area.",[],,"['Elvis Dohmatob', 'Meyer Scetbon']","['Facebook', 'Microsoft']",
https://openreview.net/forum?id=lIYtJtpJR0,Security,Robust Stable Spiking Neural Networks,"Spiking neural networks (SNNs) are gaining popularity in deep learning due to their low energy budget on neuromorphic hardware. However, they still face challenges in lacking sufficient robustness to guard safety-critical applications such as autonomous driving. Many studies have been conducted to defend SNNs from the threat of adversarial attacks. This paper aims to uncover the robustness of SNN through the lens of the stability of nonlinear systems. We are inspired by the fact that searching for parameters altering the leaky integrate-and-fire dynamics can enhance their robustness. Thus, we dive into the dynamics of membrane potential perturbation and simplify the formulation of the dynamics. We present that membrane potential perturbation dynamics can reliably convey the intensity of perturbation. Our theoretical analyses imply that the simplified perturbation dynamics satisfy input-output stability. Thus, we propose a training framework with modified SNN neurons and to reduce the mean square of membrane potential perturbation aiming at enhancing the robustness of SNN. Finally, we experimentally verify the effectiveness of the framework in the setting of Gaussian noise training and adversarial training on the image classification task. Please refer to https://github.com/DingJianhao/stable-snn for our code implementation.",[],,"['Jianhao Ding', 'Zhiyu Pan', 'Yujia Liu', 'Zhaofei Yu', 'Tiejun Huang']","['Peking University', 'Peking University', 'Peking University', 'Peking University', 'School of Computer Science, Peking University']",
https://openreview.net/forum?id=4zN9tvZfns,Privacy & Data Governance,Privacy-Preserving Data Release Leveraging Optimal Transport and Particle Gradient Descent,"We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.",[],,"['Konstantin Donhauser', 'Javier Abad', 'Neha Hulkund', 'Fanny Yang']","['CS, Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'EECS, Massachusetts Institute of Technology', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=AwLLSlJAeJ,Fairness & Bias,Principled Gradient-Based MCMC for Conditional Sampling of Text,"We consider the problem of sampling text from an energy-based model. This arises, for example, when sampling text from a neural language model subject to soft constraints. Although the target distribution is discrete, the internal computations of the energy function (given by the language model) are differentiable, so one would like to exploit gradient information within a method such as MCMC. Alas, all previous attempts to generalize gradient-based MCMC to text sampling fail to sample correctly from the target distribution. We propose a solution, along with variants, and study its theoretical properties. Through experiments on various forms of text generation, we demonstrate that our unbiased samplers are able to generate more fluent text while better adhering to the control objectives. The same methods could be used to sample from discrete energy-based models unrelated to text.",[],,"['Li Du', 'Afra Amini', 'Lucas Torroba Hennigen', 'Xinyan Velocity Yu', 'Holden Lee', 'Jason Eisner', 'Ryan Cotterell']","['Johns Hopkins University', 'ETHZ - ETH Zurich', 'Massachusetts Institute of Technology', 'University of Southern California', 'Johns Hopkins University', 'Microsoft', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=c3ls5AVOw7,Fairness & Bias,Position: Insights from Survey Methodology can Improve Training Data,"Whether future AI models are fair, trustworthy, and aligned with the public's interests rests in part on our ability to collect accurate data about what we want the models to do. However, collecting high-quality data is difficult, and few AI/ML researchers are trained in data collection methods. Recent research in data-centric AI has show that higher quality training data leads to better performing models, making this the right moment to introduce AI/ML researchers to the field of survey methodology, the science of data collection. We summarize insights from the survey methodology literature and discuss how they can improve the quality of training and feedback data. We also suggest collaborative research ideas into how biases in data collection can be mitigated, making models more accurate and human-centric.",[],,"['Stephanie Eckman', 'Barbara Plank', 'Frauke Kreuter']","['', 'Ludwig-Maximilians-Universität München', 'University of Maryland, College Park']",
https://openreview.net/forum?id=3ajK5xplDL,Privacy & Data Governance,Making Old Things New: A Unified Algorithm for Differentially Private Clustering,"As a staple of data analysis and unsupervised learning, the problem of private clustering has been widely studied, under various privacy models. Centralized differential privacy is the first of them, and the problem has also been studied for the local and the shuffle variation. In each case, the goal is to design an algorithm that computes privately a clustering, with the smallest possible error. The study of each variation gave rise to new algorithm: the landscape of private clustering algorithm is therefore quite intricate. In this paper, we show that a 20 year-old algorithm can be slightly modified to work for any of those models. This provides a unified picture: while matching almost all previously known results, it allows us to improve some of them, and extend to a new privacy model, the continual observation setting, where the input is changing over time and the algorithm must output a new solution at each time step.",[],,"['Max Dupre la Tour', 'Monika Henzinger', 'David Saulpic']","['McGill University', 'Institute of Science and Technology', 'IRIF, CNRS']",
https://openreview.net/forum?id=yrFUJzcTsk,Fairness & Bias,Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning,"Second-order information is valuable for many applications but challenging to compute. Several works focus on computing or approximating Hessian diagonals, but even this simplification introduces significant additional costs compared to computing a gradient. In the absence of efficient exact computation schemes for Hessian diagonals, we revisit an early approximation scheme proposed by Becker and LeCun (1989, BL89), which has a cost similar to gradients and appears to have been overlooked by the community. We introduce HesScale, an improvement over BL89, which adds negligible extra computation. On small networks, we find that this improvement is of higher quality than all alternatives, even those with theoretical guarantees, such as unbiasedness, while being much cheaper to compute. We use this insight in reinforcement learning problems where small networks are used and demonstrate HesScale in second-order optimization and scaling the step-size parameter. In our experiments, HesScale optimizes faster than existing methods and improves stability through step-size scaling. These findings are promising for scaling second-order methods in larger models in the future.",[],,"['Mohamed Elsayed', 'Homayoon Farrahi', 'Felix Dangel', 'A. Rupam Mahmood']","['University of Alberta', 'University of Alberta', 'Vector Institute, Toronto', 'University of Alberta']",
https://openreview.net/forum?id=iUwHnoENnl,Fairness & Bias,Model Alignment as Prospect Theoretic Optimization,"Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases---the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",[],,"['Kawin Ethayarajh', 'Winnie Xu', 'Niklas Muennighoff', 'Dan Jurafsky', 'Douwe Kiela']","['Computer Science, Princeton University', 'University of Toronto', 'Stanford University', 'Computer Science, Stanford University', 'Stanford University']",
https://openreview.net/forum?id=RtnGLJNtEG,Security,Compositional Curvature Bounds for Deep Neural Networks,"A key challenge that threatens the widespread use of neural networks in safety-critical applications is their vulnerability to adversarial attacks. In this paper, we study the second-order behavior of continuously differentiable deep neural networks, focusing on robustness against adversarial perturbations. First, we provide a theoretical analysis of robustness and attack certificates for deep classifiers by leveraging local gradients and upper bounds on the second derivative (curvature constant). Next, we introduce a novel algorithm to analytically compute provable upper bounds on the second derivative of neural networks. This algorithm leverages the compositional structure of the model to propagate the curvature bound layer-by-layer, giving rise to a scalable and modular approach. The proposed bound can serve as a differentiable regularizer to control the curvature of neural networks during training, thereby enhancing robustness. Finally, we demonstrate the efficacy of our method on classification tasks using the MNIST and CIFAR-10 datasets.",[],,"['Taha Entesari', 'Sina Sharifi', 'Mahyar Fazlyab']","['Whiting School of Engineering', 'Electrical and Computer, Johns Hopkins University', 'Electrical and Computer Engineering, Johns Hopkins University']",
https://openreview.net/forum?id=zS8zUuAU8T,Fairness & Bias,DSD-DA: Distillation-based Source Debiasing for Domain Adaptive Object Detection,"Though feature-alignment based Domain Adaptive Object Detection (DAOD) methods have achieved remarkable progress, they ignore the source bias issue, i.e., the detector tends to acquire more source-specific knowledge, impeding its generalization capabilities in the target domain. Furthermore, these methods face a more formidable challenge in achieving consistent classification and localization in the target domain compared to the source domain. To overcome these challenges, we propose a novel Distillation-based Source Debiasing (DSD) framework for DAOD, which can distill domain-agnostic knowledge from a pre-trained teacher model, improving the detector's performance on both domains. In addition, we design a Target-Relevant Object Localization Network (TROLN), which can mine target-related localization information from source and target-style mixed data. Accordingly, we present a Domain-aware Consistency Enhancing (DCE) strategy, in which these information are formulated into a new localization representation to further refine classification scores in the testing stage, achieving a harmonization between classification and localization. Extensive experiments have been conducted to manifest the effectiveness of this method, which consistently improves the strong baseline by large margins, outperforming existing alignment-based works.",[],,"['Yongchao Feng', 'Shiwei Li', 'Yingjie Gao', 'Ziyue Huang', 'Yanan Zhang', 'Qingjie Liu', 'Yunhong Wang']","['Beihang University', 'Beijing University of Aeronautics and Astronautics', 'Beihang University', 'Beijing University', 'School of Computer Science and Information Engineering, Hefei University of Technology', 'School of Computer Science and Engineering, Beihang University', '']",
https://openreview.net/forum?id=4DAl3IsvlU,Transparency & Explainability,From Geometry to Causality- Ricci Curvature and the Reliability of Causal Inference on Networks,"Causal inference on networks faces challenges posed in part by violations of standard identification assumptions due to dependencies between treatment units. Although graph geometry fundamentally influences such dependencies, the potential of geometric tools for causal inference on networked treatment units is yet to be unlocked. Moreover, despite significant progress utilizing graph neural networks (GNNs) for causal inference on networks, methods for evaluating their achievable reliability without ground truth are lacking. In this work we establish for the first time a theoretical link between network geometry, the graph Ricci curvature in particular, and causal inference, formalizing the intrinsic challenges that negative curvature poses to estimating causal parameters. The Ricci curvature can then be used to assess the reliability of causal estimates in structured data, as we empirically demonstrate. Informed by this finding, we propose a method using the geometric Ricci flow to reduce causal effect estimation error in networked data, showcasing how this newfound connection between graph geometry and causal inference could improve GNN-based causal inference. Bridging graph geometry and causal inference, this paper opens the door to geometric techniques for improving causal estimation on networks.",[],,"['Amirhossein Farzam', 'Allen Tannenbaum', 'Guillermo Sapiro']","['', 'State University of New York at Stony Brook', 'Apple']",
https://openreview.net/forum?id=7yixJXmzb8,Privacy & Data Governance,Privacy Backdoors: Stealing Data with Corrupted Pretrained Models,"Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model’s weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",[],,"['Shanglun Feng', 'Florian Tramèr']","['INFK, ETHZ - ETH Zurich', 'ETHZ - ETH Zurich']",
https://openreview.net/forum?id=TUKOklS3gg,Fairness & Bias,Inverse-Variance Weighting for Estimation of Heterogeneous Treatment Effects,"Many methods for estimating conditional average treatment effects (CATEs) can be expressed as weighted pseudo-outcome regressions (PORs). Previous comparisons of POR techniques have paid careful attention to the choice of pseudo-outcome transformation. However, we argue that the dominant driver of performance is actually the choice of weights. For example, we point out that R-Learning implicitly performs a POR with inverse-variance weights (IVWs). In the CATE setting, IVWs mitigate the instability associated with inverse-propensity weights, and lead to convenient simplifications of bias terms. We demonstrate the superior performance of IVWs in simulations, and derive convergence rates for IVWs that are, to our knowledge, the fastest yet shown without assuming knowledge of the covariate distribution.",[],,['Aaron Fisher'],"['Decision Sciences, Foundation Medicine Inc']",
https://openreview.net/forum?id=6OkvBGqW62,Transparency & Explainability,Hyperbolic Geometric Latent Diffusion Model for Graph Generation,"Diffusion models have made significant contributions to computer vision, sparking a growing interest in the community recently regarding the application of it to graph generation. The existing discrete graph diffusion models exhibit heightened computational complexity and diminished training efficiency. A preferable and natural way is to directly diffuse the graph within the latent space. However, due to the non-Euclidean structure of graphs is not isotropic in the latent space, the existing latent diffusion models effectively make it difficult to capture and preserve the topological information of graphs. To address the above challenges, we propose a novel geometrically latent diffusion framework HypDiff. Specifically, we first establish a geometrically latent space with interpretability measures based on hyperbolic geometry, to define anisotropic latent diffusion processes for graphs. Then, we propose a geometrically latent diffusion process that is constrained by both radial and angular geometric properties, thereby ensuring the preservation of the original topological properties in the generative graphs. Extensive experimental results demonstrate the superior effectiveness of HypDiff for graph generation with various topologies.",[],,"['Xingcheng Fu', 'Yisen Gao', 'Yuecen Wei', 'Qingyun Sun', 'Hao Peng', 'Jianxin Li', 'Xianxian LI']","['Guangxi Normal University', 'Beihang University, Beijing University of Aeronautics and Astronautics', 'School of Software, Beijing University of Aeronautics and Astronautics', 'School of Computer Science and Engineering, Beihang University', 'Beihang University', 'School of Computer Science and Engineering, Beihang University', 'Guangxi Normal University']",
https://openreview.net/forum?id=YJWlUMW6YP,Transparency & Explainability,Interpretability Illusions in the Generalization of Simplified Models,"A common method to study deep learning systems is to use simplified model representations—for example, using singular value decomposition to visualize the model’s hidden states in a lower dimensional space. This approach assumes that the results of these simplifications are faithful to the original model. Here, we illustrate an important caveat to this assumption: even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model’s behavior out of distribution. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits, including the Dyck balanced-parenthesis languages and a code completion task. We simplify these models using tools like dimensionality reduction and clustering, and then explicitly test how these simplified proxies match the behavior of the original model. We find consistent generalization gaps: cases in which the simplified proxies are more faithful to the original model on the in-distribution evaluations and less faithful on various tests of systematic generalization. This includes cases where the original model generalizes systematically but the simplified proxies fail, and cases where the simplified proxies generalize better. Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations.",[],,"['Dan Friedman', 'Andrew Kyle Lampinen', 'Lucas Dixon', 'Danqi Chen', 'Asma Ghandeharioun']","['Princeton University', 'Google DeepMind', 'Research, Google', 'Department of Computer Science, Princeton University', 'Google']",
https://openreview.net/forum?id=zkjGpZrIX3,Security,Trustworthy Actionable Perturbations,"*Counterfactuals*, or modified inputs that lead to a different outcome, are an important tool for understanding the logic used by machine learning classifiers and how to change an undesirable classification. Even if a counterfactual changes a classifier's decision, however, it may not affect the true underlying class probabilities, i.e. the counterfactual may act like an adversarial attack and ``fool'' the classifier. We propose a new framework for creating modified inputs that change the true underlying probabilities in a beneficial way which we call *Trustworthy Actionable Perturbations* (TAP). This includes a novel verification procedure to ensure that TAP change the true class probabilities instead of acting adversarially. Our framework also includes new cost, reward, and goal definitions that are better suited to effectuating change in the real world. We present PAC-learnability results for our verification procedure and theoretically analyze our new method for measuring reward. We also develop a methodology for creating TAP and compare our results to those achieved by previous counterfactual methods.",[],,"['Jesse Friedbaum', 'Sudarshan Adiga', 'Ravi Tandon']","['University of Arizona', 'Electrical and Computer Engineering, University of Arizona', 'University of Arizona']",
https://openreview.net/forum?id=f47ZK6gy3I,Fairness & Bias,Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning,"Semi-supervised learning (SSL) has witnessed remarkable progress, resulting in the emergence of numerous method variations. However, practitioners often encounter challenges when attempting to deploy these methods due to their subpar performance. In this paper, we present a novel SSL approach named FineSSL that significantly addresses this limitation by adapting pre-trained foundation models. We identify the aggregated biases and cognitive deviation problems inherent in foundation models, and propose a simple yet effective solution by imposing balanced margin softmax and decoupled label smoothing. Through extensive experiments, we demonstrate that FineSSL sets a new state of the art for SSL on multiple benchmark datasets, reduces the training cost by over six times, and can seamlessly integrate various fine-tuning and modern SSL algorithms. The source code is available at https://github.com/Gank0078/FineSSL.",[],,"['Kai Gan', 'Tong Wei']","['School of Computer Science and Engineering, Southeast University', 'Southeast University']",
https://openreview.net/forum?id=lcX5GbDIi8,Fairness & Bias,DMTG: One-Shot Differentiable Multi-Task Grouping,"We aim to address Multi-Task Learning (MTL) with a large number of tasks by Multi-Task Grouping (MTG). Given $N$ tasks, we propose to **simultaneously identify the best task groups from $2^N$ candidates and train the model weights simultaneously in one-shot**, with **the high-order task-affinity fully exploited**. This is distinct from the pioneering methods which sequentially identify the groups and train the model weights, where the group identification often relies on heuristics. As a result, our method not only improves the training efficiency, but also mitigates the objective bias introduced by the sequential procedures that potentially leads to a suboptimal solution. Specifically, **we formulate MTG as a fully differentiable pruning problem on an adaptive network architecture determined by an unknown Categorical distribution**. To categorize $N$ tasks into $K$ groups (represented by $K$ encoder branches), we initially set up $KN$ task heads, where each branch connects to all $N$ task heads to exploit the high-order task-affinity. Then, we gradually prune the $KN$ heads down to $N$ by learning a relaxed differentiable Categorical distribution, ensuring that each task is exclusively and uniquely categorized into only one branch. Extensive experiments on CelebA and Taskonomy datasets with detailed ablations show the promising performance and efficiency of our method. The codes are available at https://github.com/ethanygao/DMTG.",[],,"['Yuan Gao', 'Shuguo Jiang', 'Moran Li', 'Jin-Gang Yu', 'Gui-Song Xia']","['', 'School of Computer Science, Wuhan University', 'Tencent Youtu Lab', 'South China University of Technology', 'School of Computer Science , Wuhan University']",
https://openreview.net/forum?id=s0Jvdolv2I,Transparency & Explainability,Don't trust your eyes: on the (un)reliability of feature visualizations,"How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to ""explain"" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations.",[],,"['Robert Geirhos', 'Roland S. Zimmermann', 'Blair Bilodeau', 'Wieland Brendel', 'Been Kim']","['Google DeepMind', 'Google DeepMind', 'University of Toronto', 'ELLIS Institute Tübingen', 'Google DeepMind']",
https://openreview.net/forum?id=5uwBzcn885,Transparency & Explainability,Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models,"Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up *new* possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.",[],,"['Asma Ghandeharioun', 'Avi Caciularu', 'Adam Pearce', 'Lucas Dixon', 'Mor Geva']","['Google', 'Google', 'Google', 'Research, Google', 'Tel Aviv University']",
https://openreview.net/forum?id=EaJ7nqJ2Fa,Fairness & Bias,"Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning","No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.",[],,"['Micah Goldblum', 'Marc Anton Finzi', 'Keefer Rowan', 'Andrew Gordon Wilson']","['Electrical Engineering, Columbia University', 'Carnegie Mellon University', 'NYU, New York University', 'New York University']",
https://openreview.net/forum?id=JV84NVo1em,Security,Safe and Robust Subgame Exploitation in Imperfect Information Games,"Opponent exploitation is an important task for players to exploit the weaknesses of others in games. Existing approaches mainly focus on balancing between exploitation and exploitability but are often vulnerable to modeling errors and deceptive adversaries. To address this problem, our paper offers a novel perspective on the safety of opponent exploitation, named Adaptation Safety. This concept leverages the insight that strategies, even those not explicitly aimed at opponent exploitation, may inherently be exploitable due to computational complexities, rendering traditional safety overly rigorous. In contrast, adaptation safety requires that the strategy should not be more exploitable than it would be in scenarios where opponent exploitation is not considered. Building on such adaptation safety, we further propose an Opponent eXploitation Search (OX-Search) framework by incorporating real-time search techniques for efficient online opponent exploitation. Moreover, we provide theoretical analyses to show the adaptation safety and robust exploitation of OX-Search, even with inaccurate opponent models. Empirical evaluations in popular poker games demonstrate OX-Search's superiority in both exploitability and exploitation compared to previous methods.",[],,"['Zhenxing Ge', 'Zheng Xu', 'Tianyu Ding', 'Linjian Meng', 'Bo An', 'Wenbin Li', 'Yang Gao']","['Nanjing University', 'NetEase, Inc.', 'Microsoft', 'Nanjing University', 'School of Computer Science and Engineering, Nanyang Technological University', 'School of Intelligence Science and Technology, Nanjing University', 'School of Intelligence Science and Technology, Nanjing University']",
https://openreview.net/forum?id=SlRcJvf1yd,Fairness & Bias,Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates,"We study the problem of efficiently computing the derivative of the fixed-point of a parametric nondifferentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. We build upon the work by Bolte et al. (2022), who prove linear convergence of nonsmooth ITD under a piecewise Lipschitz smooth assumption. In the deterministic case, we provide a linear rate for AID and an improved linear rate for ITD which closely match the ones for the smooth setting. We further introduce NSID, a new stochastic method to compute the implicit derivative when the contraction map is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of NSID, encompassing the best available rates in the smooth setting. We also present illustrative experiments confirming our analysis.",[],,"['Riccardo Grazzi', 'massimiliano pontil', 'Saverio Salzo']","['Istituto Italiano di Tecnologia', 'Istituto Italiano di Tecnologia', 'DIAG, University of Roma ""La Sapienza""']",
https://openreview.net/forum?id=n9pru4bJU9,Fairness & Bias,Scaling Down Deep Learning with MNIST-1D,"Although deep learning models have taken on commercial and political relevance, key aspects of their training and operation remain poorly understood. This has sparked interest in science of deep learning projects, many of which require large amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, procedurally generated, low-memory, and low-compute alternative to classic deep learning benchmarks. Although the dimensionality of MNIST-1D is only 40 and its default training set size only 4000, MNIST-1D can be used to study inductive biases of different deep architectures, find lottery tickets, observe deep double descent, metalearn an activation function, and demonstrate guillotine regularization in self-supervised learning. All these experiments can be conducted on a GPU or often even on a CPU within minutes, allowing for fast prototyping, educational use cases, and cutting-edge research on a low budget.",[],,"['Samuel James Greydanus', 'Dmitry Kobak']","['Oregon State University', 'Eberhard-Karls-Universität Tübingen']",
https://openreview.net/forum?id=QwgSOwynxD,Fairness & Bias,A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models,"Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc and task-dependent manner. For example, natural language approaches cannot be transferred to image generation. In this paper, we introduce the first bias-variance-covariance decomposition for kernel scores. This decomposition represents a theoretical framework from which we derive a kernel-based variance and entropy for uncertainty estimation. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. Based on the wide applicability of kernels, we demonstrate our framework via generalization and uncertainty experiments for image, audio, and language generation. Specifically, kernel entropy for uncertainty estimation is more predictive of performance on CoQA and TriviaQA question answering datasets than existing baselines and can also be applied to closed-source models.",[],,"['Sebastian Gregor Gruber', 'Florian Buettner']","['Johann Wolfgang Goethe Universität Frankfurt am Main', 'Deutsches Krebsforschungszentrum']",
https://openreview.net/forum?id=KviM5k8pcP,Security,AI Control: Improving Safety Despite Intentional Subversion,"As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. To do so, safety measures either aim at making LLMs try to avoid harmful outcomes or aim at preventing LLMs from causing harmful outcomes, even if they try to cause them. In this paper, we focus on this second layer of defense. We develop and evaluate pipelines of safety techniques (protocols) that try to ensure safety despite intentional subversion - an approach we call AI control. We investigate a setting in which we want to solve a sequence of programming problems without ever submitting subtly wrong code, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate a range of protocols and red-team them by exploring strategies that the untrusted model could use to subvert them. We find that using the trusted model to edit untrusted-model code or using the untrusted model as a monitor substantially improves on simple baselines.",[],,"['Ryan Greenblatt', 'Buck Shlegeris', 'Kshitij Sachan', 'Fabien Roger']","['Brown University', 'Redwood Research', 'Brown University', 'Anthropic']",
https://openreview.net/forum?id=O1hmwi51pp,Fairness & Bias,Automated Loss function Search for Class-imbalanced Node Classification,"Class-imbalanced node classification tasks are prevalent in real-world scenarios. Due to the uneven distribution of nodes across different classes, learning high-quality node representations remains a challenging endeavor. The engineering of loss functions has shown promising potential in addressing this issue. It involves the meticulous design of loss functions, utilizing information about the quantities of nodes in different categories and the network's topology to learn unbiased node representations. However, the design of these loss functions heavily relies on human expert knowledge and exhibits limited adaptability to specific target tasks. In this paper, we introduce a high-performance, flexible, and generalizable automated loss function search framework to tackle this challenge. Across 15 combinations of graph neural networks and datasets, our framework achieves a significant improvement in performance compared to state-of-the-art methods. Additionally, we observe that homophily in graph-structured data significantly contributes to the transferability of the proposed framework.",[],,"['Xinyu Guo', 'Kai Wu', 'Xiaoyu Zhang', 'Jing Liu']","['Xidian University', '', 'School of Cyber Engineering, Xidian University', 'Guangzhou Institute of Technology, Xidian University, China']",
https://openreview.net/forum?id=JOrLz5d7OW,Transparency & Explainability,Prototypical Transformer As Unified Motion Learners,"In this work, we introduce the Prototypical Transformer (ProtoFormer), a general and unified framework that approaches various motion tasks from a prototype perspective. ProtoFormer seamlessly integrates prototype learning with Transformer by thoughtfully considering motion dynamics, introducing two innovative designs. First, Cross-Attention Prototyping discovers prototypes based on signature motion patterns, providing transparency in understanding motion scenes. Second, Latent Synchronization guides feature representation learning via prototypes, effectively mitigating the problem of motion uncertainty. Empirical results demonstrate that our approach achieves competitive performance on popular motion tasks such as optical flow and scene depth. Furthermore, it exhibits generality across various downstream tasks, including object tracking and video stabilization.",[],,"['Cheng Han', 'Yawen Lu', 'Guohao Sun', 'James Chenhao Liang', 'Zhiwen Cao', 'Qifan Wang', 'Qiang Guan', 'Sohail Dianat', 'Raghuveer Rao', 'Tong Geng', 'ZHIQIANG TAO', 'Dongfang Liu']","['Computer Science, University of Missouri - Kansas City', 'Purdue University', 'GCCIS, Rochester Institute of Technology', 'U. S. Naval Research Laboratory', 'Adobe Systems', 'Meta AI', 'Kent State University', 'Electrical and Microelectronic Engineering, Rochester Institute of Technology', 'Intelligent Perception , DEVCOM Army Research Laboratory', 'University of Rochester', 'Rochester Institute of Technology', 'Rochester Institute of Technology']",
https://openreview.net/forum?id=Su0qe33cWA,Transparency & Explainability,Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformer,"Optimal transport (OT) and the related Wasserstein metric ($W$) are powerful and ubiquitous tools for comparing distributions. However, computing pairwise Wasserstein distances rapidly becomes intractable as cohort size grows. An attractive alternative would be to find an embedding space in which pairwise Euclidean distances map to OT distances, akin to standard multidimensional scaling (MDS). We present Wasserstein Wormhole, a transformer-based autoencoder that embeds empirical distributions into a latent space wherein Euclidean distances approximate OT distances. Extending MDS theory, we show that our objective function implies a bound on the error incurred when embedding non-Euclidean distances. Empirically, distances between Wormhole embeddings closely match Wasserstein distances, enabling linear time computation of OT distances. Along with an encoder that maps distributions to embeddings, Wasserstein Wormhole includes a decoder that maps embeddings back to distributions, allowing for operations in the embedding space to generalize to OT spaces, such as Wasserstein barycenter estimation and OT interpolation. By lending scalability and interpretability to OT approaches, Wasserstein Wormhole unlocks new avenues for data analysis in the fields of computational geometry and single-cell biology.",[],,"['Doron Haviv', 'Russell Zhang Kunes', 'Thomas Dougherty', 'Cassandra Burdziak', 'Tal Nawy', 'Anna Gilbert', ""Dana Pe'er""]","['Cornell University', 'California Institute of Technology', 'Yale University', '']",
https://openreview.net/forum?id=S4LqI6CcJ3,Security,Be Your Own Neighborhood: Detecting Adversarial Examples by the Neighborhood Relations Built on Self-Supervised Learning,"Deep Neural Networks (DNNs) are vulnerable to Adversarial Examples (AEs), hindering their use in safety-critical systems. In this paper, we present **BEYOND**, an innovative AE detection framework designed for reliable predictions. BEYOND identifies AEs by distinguishing the AE’s abnormal relation with its augmented versions, i.e. neighbors, from two prospects: representation similarity and label consistency. An off-the-shelf Self-Supervised Learning (SSL) model is used to extract the representation and predict the label for its highly informative representation capacity compared to supervised learning models. We found clean samples maintain a high degree of representation similarity and label consistency relative to their neighbors, in contrast to AEs which exhibit significant discrepancies. We explain this observation and show that leveraging this discrepancy BEYOND can accurately detect AEs. Additionally, we develop a rigorous justification for the effectiveness of BEYOND. Furthermore, as a plug-and-play model, BEYOND can easily cooperate with the Adversarial Trained Classifier (ATC), achieving state-of-the-art (SOTA) robustness accuracy. Experimental results show that BEYOND outperforms baselines by a large margin, especially under adaptive attacks. Empowered by the robust relationship built on SSL, we found that BEYOND outperforms baselines in terms of both detection ability and speed. Project page: https://huggingface.co/spaces/allenhzy/Be-Your-Own-Neighborhood.",[],,"['Zhiyuan He', 'Yijun Yang', 'Pin-Yu Chen', 'Qiang Xu', 'Tsung-Yi Ho']","['', 'CS, Tsinghua University', 'International Business Machines', 'The Chinese University of Hong Kong', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong']",
https://openreview.net/forum?id=nAoiUlz4Bf,Security,Verifying message-passing neural networks via topology-based bounds tightening,"Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classification problems and consider topological attacks that both add and remove edges.",[],,"['Christopher Hojny', 'Shiqiang Zhang', 'Juan S Campos', 'Ruth Misener']","['', 'Department of Computing, Imperial College London, Imperial College London', 'Imperial College London', 'Imperial College London']",
https://openreview.net/forum?id=e3Dpq3WdMv,Security,Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression,"Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of **three (3) leading LLMs** using **five (5) SoTA compression techniques** across **eight (8) trustworthiness dimensions**. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% sparsity. Moreover, employing quantization within a moderate bit range could unexpectedly improve certain trustworthiness dimensions such as ethics and fairness. Conversely, extreme quantization to very low bit levels (3 bits) tends to reduce trustworthiness significantly. This increased risk cannot be uncovered by looking at benign performance alone, in turn, mandating comprehensive trustworthiness evaluation in practice. These findings culminate in practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs. Code and models are available at https://decoding-comp-trust.github.io.",[],,"['Junyuan Hong', 'Jinhao Duan', 'Chenhui Zhang', 'Zhangheng LI', 'Chulin Xie', 'Kelsey Lieberman', 'James Diffenderfer', 'Brian R. Bartoldson', 'AJAY KUMAR JAISWAL', 'Kaidi Xu', 'Bhavya Kailkhura', 'Dan Hendrycks', 'Dawn Song', 'Zhangyang Wang', 'Bo Li']","['University of Texas at Austin', 'CS, Drexel University', 'Institute for Data, Systems, and Society, Massachusetts Institute of Technology', 'ECE, University of Texas at Austin', 'University of Illinois, Urbana Champaign', 'Computer Science, Duke University', 'Lawrence Livermore National Labs', 'Lawrence Livermore National Labs', 'Apple', 'CS, Drexel University', 'CASC, Lawrence Livermore National Laboratory', 'UC Berkeley', 'University of California Berkeley', 'University of Texas at Austin', 'CS, University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=YCzbfs2few,Security,IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency,"Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries can maliciously trigger model misclassifications by implanting a hidden backdoor during model training. This paper proposes a simple yet effective input-level backdoor detection (dubbed IBD-PSC) as a `firewall' to filter out malicious testing images. Our method is motivated by an intriguing phenomenon, i.e., parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples are significantly more consistent than those of benign ones when amplifying model parameters. In particular, we provide theoretical analysis to safeguard the foundations of the PSC phenomenon. We also design an adaptive method to select BN layers to scale up for effective detection. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our IBD-PSC method and its resistance to adaptive attacks. Codes are available at https://github.com/THUYimingLi/BackdoorBox.",[],,"['Linshan Hou', 'Ruili Feng', 'Zhongyun Hua', 'Wei Luo', 'Leo Yu Zhang', 'Yiming Li']","['Harbin Institute of Technology', 'Alibaba Group', 'school of Computer Science and Technology, Harbin Institute of Technology Shenzhen', 'Deakin University', 'Schoolf ICT, Griffith University', 'Nanyang Technological University']",
https://openreview.net/forum?id=0iXp5P77ho,Fairness & Bias,Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning,"Inductive biases are crucial in disentangled representation learning for narrowing down an underspecified solution set. In this work, we consider endowing a neural network autoencoder with three select inductive biases from the literature: data compression into a grid-like latent space via quantization, collective independence amongst latents, and minimal functional influence of any latent on how other latents determine data generation. In principle, these inductive biases are deeply complementary: they most directly specify properties of the latent space, encoder, and decoder, respectively. In practice, however, naively combining existing techniques instantiating these inductive biases fails to yield significant benefits. To address this, we propose adaptations to the three techniques that simplify the learning problem, equip key regularization terms with stabilizing invariances, and quash degenerate incentives. The resulting model, Tripod, achieves state-of-the-art results on a suite of four image disentanglement benchmarks. We also verify that Tripod significantly improves upon its naive incarnation and that all three of its ""legs"" are necessary for best performance.",[],,"['Kyle Hsu', 'Jubayer Ibn Hamid', 'Kaylee Burns', 'Chelsea Finn', 'Jiajun Wu']","['Stanford University', '', 'Stanford University', 'Physical Intelligence', 'Stanford University']",
https://openreview.net/forum?id=YdwwWRX20q,Transparency & Explainability,Improving Interpretation Faithfulness for Vision Transformers,"Vision Transformers (ViTs) have achieved state-of-the-art performance for various vision tasks. One reason behind the success lies in their ability to provide plausible innate explanations for the behavior of neural architectures. However, ViTs suffer from issues with explanation faithfulness, as their focal points are fragile to adversarial attacks and can be easily changed with even slight perturbations on the input image. In this paper, we propose a rigorous approach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly speaking, an FViT should have the following two properties: (1) The top-$k$ indices of its self-attention vector should remain mostly unchanged under input perturbation, indicating stable explanations; (2) The prediction distribution should be robust to perturbations. To achieve this, we propose a new method called Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing and diffusion-based denoising. We theoretically prove that processing ViTs directly with DDS can turn them into FViTs. We also show that Gaussian noise is nearly optimal for both $\ell_2$ and $\ell_\infty$-norm cases. Finally, we demonstrate the effectiveness of our approach through comprehensive experiments and evaluations. Results show that FViTs are more robust against adversarial attacks while maintaining the explainability of attention, indicating higher faithfulness.",[],,"['Lijie Hu', 'Yixin Liu', 'Ninghao Liu', 'Mengdi Huai', 'Lichao Sun', 'Di Wang']","['KAUST', '', 'University of Georgia', 'Computer Science, Iowa State University', 'Computer Science and Engineering, Lehigh University', '']",
https://openreview.net/forum?id=XnsI1HKAKC,Security,Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) has seen substantial efforts to improve model accuracy for an unlabeled target domain with the help of a labeled source domain. However, UDA models often exhibit poorly calibrated predictive uncertainty on target data, a problem that remains under-explored and poses risks in safety-critical UDA applications. The calibration problem in UDA is particularly challenging due to the absence of labeled target data and severe distribution shifts between domains. In this paper, we approach UDA calibration as a target-domain-specific unsupervised problem, different from mainstream solutions based on *covariate shift*. We introduce Pseudo-Calibration (PseudoCal), a novel post-hoc calibration framework. Our innovative use of inference-stage *mixup* synthesizes a labeled pseudo-target set capturing the structure of the real unlabeled target data. This turns the unsupervised calibration problem into a supervised one, easily solvable with *temperature scaling*. Extensive empirical evaluations across 5 diverse UDA scenarios involving 10 UDA methods consistently demonstrate the superior performance and versatility of PseudoCal over existing solutions.",[],,"['Dapeng Hu', 'Jian Liang', 'Xinchao Wang', 'Chuan-Sheng Foo']","['Apple', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'National University of Singapore', 'Centre for Frontier AI Research, A*STAR']",
https://openreview.net/forum?id=Nue7KgVZ6e,Fairness & Bias,Multigroup Robustness,"To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption. However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset. Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of *multigroup robust* algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption *inside* that subpopulation. When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related. Our techniques establish a new connection between multigroup fairness and robustness.",[],,"['Lunjia Hu', 'Charlotte Peale', 'Judy Hanwen Shen']","['Harvard University', 'Stanford University']",
https://openreview.net/forum?id=Nxz3CDtGXp,Fairness & Bias,An Empirical Examination of Balancing Strategy for Counterfactual Estimation on Time Series,"Counterfactual estimation from observations represents a critical endeavor in numerous application fields, such as healthcare and finance, with the primary challenge being the mitigation of treatment bias. The balancing strategy aimed at reducing covariate disparities between different treatment groups serves as a universal solution. However, when it comes to the time series data, the effectiveness of balancing strategies remains an open question, with a thorough analysis of the robustness and applicability of balancing strategies still lacking. This paper revisits counterfactual estimation in the temporal setting and provides a brief overview of recent advancements in balancing strategies. More importantly, we conduct a critical empirical examination for the effectiveness of the balancing strategies within the realm of temporal counterfactual estimation in various settings on multiple datasets. Our findings could be of significant interest to researchers and practitioners and call for a reexamination of the balancing strategy in time series settings.",[],,"['Qiang Huang', 'Chuizheng Meng', 'Defu Cao', 'Biwei Huang', 'Yi Chang', 'Yan Liu']","['Machine Learning, Mohamed bin Zayed University of Artificial Intelligence', 'Computer Science, University of Southern California', 'University of Southern California', 'University of California, San Diego', 'Jilin University, China', 'University of Southern California']",
https://openreview.net/forum?id=OnkA4zaEU9,Security,"Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence","The primary goal of online change detection (OCD) is to promptly identify changes in the data stream. OCD problem find a wide variety of applications in diverse areas, e.g., security detection in smart grids and intrusion detection in communication networks. Prior research usually assumes precise knowledge of the system parameters. Nevertheless, this presumption often proves unattainable in practical scenarios due to factors such as estimation errors, system updates, etc. This paper aims to take the first attempt to develop a triadic-OCD framework with certifiable robustness, provable optimality, and guaranteed convergence. In addition, the proposed triadic-OCD algorithm can be realized in a fully asynchronous distributed manner, easing the necessity of transmitting the data to a single server. This asynchronous mechanism could also mitigate the straggler issue that faced by traditional synchronous algorithm. Moreover, the non-asymptotic convergence property of Triadic-OCD is theoretically analyzed, and its iteration complexity to achieve an $\epsilon$-optimal point is derived. Extensive experiments have been conducted to elucidate the effectiveness of the proposed method.",[],,"['Yancheng Huang', 'Kai Yang', 'Zelin Zhu', 'Leian Chen']","['Tongji University', '']",
https://openreview.net/forum?id=Dwc0RwiNI5,Privacy & Data Governance,Faster Adaptive Decentralized Learning Algorithms,"Decentralized learning recently has received increasing attention in machine learning due to its advantages in implementation simplicity and system robustness, data privacy. Meanwhile, the adaptive gradient methods show superior performances in many machine learning tasks such as training neural networks. Although some works focus on studying decentralized optimization algorithms with adaptive learning rates, these adaptive decentralized algorithms still suffer from high sample complexity. To fill these gaps, we propose a class of faster adaptive decentralized algorithms (i.e., AdaMDOS and AdaMDOF) for distributed nonconvex stochastic and finite-sum optimization, respectively. Moreover, we provide a solid convergence analysis framework for our methods. In particular, we prove that our AdaMDOS obtains a near-optimal sample complexity of $\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary solution of nonconvex stochastic optimization. Meanwhile, our AdaMDOF obtains a near-optimal sample complexity of $O(\sqrt{n}\epsilon^{-2})$ for finding an $\epsilon$-stationary solution of for nonconvex finite-sum optimization, where $n$ denotes the sample size. To the best of our knowledge, our AdaMDOF algorithm is the first adaptive decentralized algorithm for nonconvex finite-sum optimization. Some experimental results demonstrate efficiency of our algorithms.",[],,"['Feihu Huang', 'Jianyu Zhao']","['Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics']",
https://openreview.net/forum?id=mrd4e8ZJjm,Transparency & Explainability,Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning,"Causal dynamics learning has recently emerged as a promising approach to enhancing robustness in reinforcement learning (RL). Typically, the goal is to build a dynamics model that makes predictions based on the causal relationships among the entities. Despite the fact that causal connections often manifest only under certain contexts, existing approaches overlook such fine-grained relationships and lack a detailed understanding of the dynamics. In this work, we propose a novel dynamics model that infers fine-grained causal structures and employs them for prediction, leading to improved robustness in RL. The key idea is to jointly learn the dynamics model with a discrete latent variable that quantizes the state-action space into subgroups. This leads to recognizing meaningful context that displays sparse dependencies, where causal structures are learned for each subgroup throughout the training. Experimental results demonstrate the robustness of our method to unseen states and locally spurious correlations in downstream tasks where fine-grained causal reasoning is crucial. We further illustrate the effectiveness of our subgroup-based approach with quantization in discovering fine-grained causal relationships compared to prior methods.",[],,"['Inwoo Hwang', 'Yunhyeok Kwak', 'Suhyung Choi', 'Byoung-Tak Zhang', 'Sanghack Lee']","['Columbia University', 'Seoul National University', 'Seoul National University', 'Computer Science and Engineering, Seoul National University', 'Graduate School of Data Science, Seoul National University']",
https://openreview.net/forum?id=CuiRGtVI55,Fairness & Bias,Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control,"Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias. However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases. Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control. We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases.",[],,"['Dongyoon Hwang', 'Byungkun Lee', 'Hojoon Lee', 'Hyunseung Kim', 'Jaegul Choo']","['Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science & Technology', 'Graduate School of AI, Korea Advanced Institute of Science and Technology']",
https://openreview.net/forum?id=TtSFg4s3F0,Security,Rethinking DP-SGD in Discrete Domain: Exploring Logistic Distribution in the Realm of signSGD,"Deep neural networks (DNNs) have a risk of remembering sensitive data from their training datasets, inadvertently leading to substantial information leakage through privacy attacks like membership inference attacks. DP-SGD is a simple but effective defense method, incorporating Gaussian noise into gradient updates to safeguard sensitive information. With the prevalence of large neural networks, DP-signSGD, a variant of DP-SGD, has emerged, aiming to curtail memory usage while maintaining security. However, it is noteworthy that most DP-signSGD algorithms default to Gaussian noise, suitable only for DP-SGD, without scant discussion of its appropriateness for signSGD. Our study delves into an intriguing question: **""Can we find a more efficient substitute for Gaussian noise to secure privacy in DP-signSGD?""** We propose an answer with a Logistic mechanism, which conforms to signSGD principles and is interestingly evolved from an exponential mechanism. In this paper, we provide both theoretical and experimental evidence showing that our method surpasses DP-signSGD.",[],,"['Jonggyu Jang', 'Seongjin Hwang', 'Hyun Jong Yang']","['EE, Pohang University of Science and Technology', 'LG bldg. #406, Pohang University of Science and Technology', 'POSTECH']",
https://openreview.net/forum?id=1puvYh729M,Transparency & Explainability,ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization,"The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, **ACE**: Off-policy **A**ctor-critic with **C**ausality-aware **E**ntropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/.",[],,"['Tianying Ji', 'Yongyuan Liang', 'Yan Zeng', 'Yu Luo', 'Guowei Xu', 'Jiawei Guo', 'Ruijie Zheng', 'Furong Huang', 'Fuchun Sun', 'Huazhe Xu']","['Tsinghua University, Tsinghua University', 'Computer Science Department, University of Maryland, College Park', 'Department of mathematics and statistics, Beijing Technology and Business University', 'Tsinghua University', 'IIIS, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland', 'Department of Computer Science and Technology, Tsinghua University', 'IIIS, Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=vGHOFeUQi8,Fairness & Bias,Simulation-Based Inference with Quantile Regression,"We present Neural Quantile Estimation (NQE), a novel Simulation-Based Inference (SBI) method based on conditional quantile regression. NQE autoregressively learns individual one dimensional quantiles for each posterior dimension, conditioned on the data and previous posterior dimensions. Posterior samples are obtained by interpolating the predicted quantiles using monotonic cubic Hermite spline, with specific treatment for the tail behavior and multi-modal distributions. We introduce an alternative definition for the Bayesian credible region using the local Cumulative Density Function (CDF), offering substantially faster evaluation than the traditional Highest Posterior Density Region (HPDR). In case of limited simulation budget and/or known model misspecification, a post-processing calibration step can be integrated into NQE to ensure the unbiasedness of the posterior estimation with negligible additional computational cost. We demonstrate that NQE achieves state-of-the-art performance on a variety of benchmark problems.",[],,['He Jia'],['Princeton University'],
https://openreview.net/forum?id=otuTw4Mghk,Fairness & Bias,On the Origins of Linear Representations in Large Language Models,"An array of recent works have argued that high-level semantic concepts are encoded ""linearly"" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to prove that linearity arises as a consequence of the loss function and the implicit bias of gradient descent. The theory is further substantiated empirically via experiments.",[],,"['Yibo Jiang', 'Goutham Rajendran', 'Pradeep Kumar Ravikumar', 'Bryon Aragam', 'Victor Veitch']","['University of Chicago', 'Meta GenAI', 'Carnegie Mellon University', 'University of Chicago', 'University of Chicago']",
https://openreview.net/forum?id=F2Tegvyqlo,Security,Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks,"Deep Neural Networks (DNN) have shown great promise in many classification applications, yet are widely known to have poorly calibrated predictions when they are over-parametrized. Improving DNN calibration without comprising on model accuracy is of extreme importance and interest in safety critical applications such as in the health-care sector. In this work, we show that decoupling the training of feature extraction layers and classification layers in over-parametrized DNN architectures such as Wide Residual Networks (WRN) and Vision Transformers (ViT) significantly improves model calibration whilst retaining accuracy, and at a low training cost. In addition, we show that placing a Gaussian prior on the last hidden layer outputs of a DNN, and training the model variationally in the classification training stage, even further improves calibration. We illustrate these methods improve calibration across ViT and WRN architectures for several image classification benchmark datasets.",[],,"['Mikkel Jordahn', 'Pablo M. Olmos']","['DTU Compute, Technical University of Denmark', 'Universidad Carlos III de Madrid']",
https://openreview.net/forum?id=iQTElQbAqo,Privacy & Data Governance,Beyond the Calibration Point: Mechanism Comparison in Differential Privacy,"In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\varepsilon, \delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\varepsilon, \delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\varepsilon, \delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities.",[],,"['Georgios Kaissis', 'Stefan Kolek', 'Borja Balle', 'Jamie Hayes', 'Daniel Rueckert']","['', 'LMU Mathematics Institute , Institut für Mathematik', 'DeepMind', 'University College London', 'Technische Universität München']",
https://openreview.net/forum?id=bzNwexOPWm,Transparency & Explainability,What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement,"Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting---the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products of example representations achieves better forecasting performance over a series of setups. Finally, we show that we reduce forgetting of upstream pretraining examples by replaying examples that are forecasted to be forgotten, demonstrating the practical utility of forecasting example forgetting.",[],,"['Xisen Jin', 'Xiang Ren']","['Computer Science, University of Southern California', 'Computer Science, University of Southern California']",
https://openreview.net/forum?id=4axAQHwBOE,Security,Certifiably Byzantine-Robust Federated Conformal Prediction,"Conformal prediction has shown impressive capacity in constructing statistically rigorous prediction sets for machine learning models with exchangeable data samples. The siloed datasets, coupled with the escalating privacy concerns related to local data sharing, have inspired recent innovations extending conformal prediction into federated environments with distributed data samples. However, this framework for distributed uncertainty quantification is susceptible to Byzantine failures. A minor subset of malicious clients can significantly compromise the practicality of coverage guarantees. To address this vulnerability, we introduce a novel framework Rob-FCP, which executes robust federated conformal prediction, effectively countering malicious clients capable of reporting arbitrary statistics with the conformal calibration process. We theoretically provide the conformal coverage bound of Rob-FCP in the Byzantine setting and show that the coverage of Rob-FCP is asymptotically close to the desired coverage level. We also propose a malicious client number estimator to tackle a more challenging setting where the number of malicious clients is unknown to the defender and theoretically shows its effectiveness. We empirically demonstrate the robustness of Rob-FCP against diverse proportions of malicious clients under a variety of Byzantine attacks on five standard benchmark and real-world healthcare datasets.",[],,"['Mintong Kang', 'Zhen Lin', 'Jimeng Sun', 'Cao Xiao', 'Bo Li']","['University of Illinois at Urbana-Champaign', '', 'University of Illinois, Urbana Champaign', 'GE Healthcare', 'CS, University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=0nMzOmkBHC,Privacy & Data Governance,FedSC: Provable Federated Self-supervised Learning with Spectral Contrastive Objective over Non-i.i.d. Data,"Recent efforts have been made to integrate self-supervised learning (SSL) with the framework of federated learning (FL). One unique challenge of federated self-supervised learning (FedSSL) is that the global objective of FedSSL usually does not equal the weighted sum of local SSL objectives. Consequently, conventional approaches, such as federated averaging (FedAvg), fail to precisely minimize the FedSSL global objective, often resulting in suboptimal performance, especially when data is non-i.i.d.. To fill this gap, we propose a provable FedSSL algorithm, named FedSC, based on the spectral contrastive objective. In FedSC, clients share correlation matrices of data representations in addition to model weights periodically, which enables inter-client contrast of data samples in addition to intra-client contrast and contraction, resulting in improved quality of data representations. Differential privacy (DP) protection is deployed to control the additional privacy leakage on local datasets when correlation matrices are shared. We provide theoretical analysis on convergence and extra privacy leakage, and conduct numerical experiments to justify the effectiveness of our proposed algorithm.",[],,"['Shusen Jing', 'Anlan Yu', 'Shuai Zhang', 'Songyang Zhang']","['Lehigh University', 'Lehigh University', 'New Jersey Institute of Technology', 'Electrical and Computer Engineering, University of Louisiana at Lafeyette']",
https://openreview.net/forum?id=YxmcEfcgp3,Fairness & Bias,Neural Tangent Kernels for Axis-Aligned Tree Ensembles,"While axis-aligned rules are known to induce an important inductive bias in machine learning models such as typical hard decision tree ensembles, theoretical understanding of the learning behavior is largely unrevealed due to the discrete nature of rules. To address this issue, we impose the axis-aligned constraint on soft trees, which relax the splitting process of decision trees and are trained using a gradient method, and present their Neural Tangent Kernel (NTK), which enables us to analytically describe the training behavior. We study two cases: imposing the axis-aligned constraint throughout the entire training process, and only at the initial state. Moreover, we extend the NTK framework to handle various tree architectures simultaneously, and prove that any axis-aligned non-oblivious tree ensemble can be transformed into axis-aligned oblivious tree ensembles with the same NTK. One can search for suitable tree architecture via Multiple Kernel Learning (MKL), and our numerical experiments show a variety of suitable features depending on the type of constraints. Our NTK analysis highlights both the theoretical and practical impacts of the axis-aligned constraint in tree ensemble learning.",[],,"['Ryuichi Kanoh', 'Mahito Sugiyama']","['NII, the Graduate University for Advanced Studies', 'National Institute of Informatics']",
https://openreview.net/forum?id=XSsoggg8pz,Fairness & Bias,Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach,"In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a ""desired"" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.",[],,"['Vijay Keswani', 'Anay Mehrotra', 'L. Elisa Celis']","['', 'Yale University', 'Yale University']",
https://openreview.net/forum?id=CbbTF6tDhW,Fairness & Bias,Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization,"We study the problem of training an unbiased and accurate model given a dataset with multiple biases. This problem is challenging since the multiple biases cause multiple undesirable shortcuts during training, and even worse, mitigating one may exacerbate the other. We propose a novel training method to tackle this challenge. Our method first groups training data so that different groups induce different shortcuts, and then optimizes a linear combination of group-wise losses while adjusting their weights dynamically to alleviate conflicts between the groups in performance; this approach, rooted in the multi-objective optimization theory, encourages to achieve the minimax Pareto solution. We also present a new benchmark with multiple biases, dubbed MultiCelebA, for evaluating debiased training methods under realistic and challenging scenarios. Our method achieved the best on three datasets with multiple biases, and also showed superior performance on conventional single-bias datasets.",[],,"['Nayeong Kim', 'Juwon Kang', 'Sungsoo Ahn', 'Jungseul Ok', 'Suha Kwak']","['POSTECH', 'CSE, POSTECH', 'Graduate School of AI, Korea Advanced Institute of Science & Technology', 'Computer Science and Engineering; and Graduate School of Artificial Intelligence, POSTECH', 'Graduate School of AI, POSTECH']",
https://openreview.net/forum?id=Mw8kNVfdMs,Transparency & Explainability,Attribute Based Interpretable Evaluation Metrics for Generative Models,"When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond “diversity"". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Singleattribute Divergence (SaD) reveals the attributes that are generated excessively or insufficiently by measuring the divergence of PDFs of individual attributes. Paired-attribute Divergence (PaD) reveals such pairs of attributes by measuring the divergence of joint PDFs of pairs of attributes. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous initial points. With SaD and PaD, we reveal the following about existing generative models. ProjectedGAN generates implausible attribute relationships such as baby with beard even though it has competitive scores of existing metrics. Diffusion models struggle to capture diverse colors in the datasets. The larger sampling timesteps of the latent diffusion model generate the more minor objects including earrings and necklace. Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models.",[],,"['Dongkyun Kim', 'Mingi Kwon', 'Youngjung Uh']","['Yonsei University', 'Artificial Intelligence, Yonsei University', 'Yonsei University']",
https://openreview.net/forum?id=J4HJUF70qm,Privacy & Data Governance,Clustered Federated Learning via Gradient-based Partitioning,"Clustered Federated Learning (CFL) is a promising distributed learning framework that addresses data heterogeneity issues across multiple clients by grouping clients and providing a shared generalized model for each group. However, under privacy-preserving federated learning protocols where there is no direct sharing of clients' local datasets, existing approaches often fail to find optimal client groupings resulting in sub-optimal performance. In this paper, we propose a novel CFL algorithm that achieves robust clustering and learning performance. Conceptually, our algorithm groups clients that exhibit similarity in their model updates by periodically accumulating and clustering the gradients that clients compute for various models. The proposed algorithm is shown to achieve a near-optimal error rate for stochastic convergence to optimal models under mild conditions. We present a detailed analysis of the algorithm along with an evaluation on several CFL benchmarks demonstrating that it outperforms existing approaches in terms of convergence speed, clustering accuracy, and task performance.",[],,"['Heasung Kim', 'Hyeji Kim', 'Gustavo De Veciana']","['Electrical and Computer Engineering, University of Texas at Austin', '', 'Electrical and Computer Engineering, University of Texas, Austin']",
https://openreview.net/forum?id=7tyAO5tUF8,Fairness & Bias,Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs,"The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based MLP network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We demonstrate empirically that our proposed method not only outperforms baseline models for both static and dynamic NeRFs with sparse inputs, but also achieves comparable results with fewer parameters.",[],,"['Mingyu Kim', 'Kim Jun-Seong', 'Se-Young Yun', 'Jin-Hwa Kim']","['Computer Science, The University of British Columbia', 'Electrical Engineering, Pohang University of Science and Technology', 'KAIST', 'Seoul National University']",
https://openreview.net/forum?id=apxON2uH4N,Privacy & Data Governance,Privacy-Preserving Embedding via Look-up Table Evaluation with Fully Homomorphic Encryption,"In privacy-preserving machine learning (PPML), homomorphic encryption (HE) has emerged as a significant primitive, allowing the use of machine learning (ML) models while protecting the confidentiality of input data. Although extensive research has been conducted on implementing PPML with HE by developing the efficient construction of private counterparts to ML models, the efficient HE implementation of embedding layers for token inputs such as words remains inadequately addressed. Thus, our study proposes an efficient algorithm for privacy-preserving embedding via look-up table evaluation with HE(HELUT) by developing an encrypted indicator function (EIF) that assures high precision with the use of the approximate HE scheme(CKKS). Based on the proposed EIF, we propose the CodedHELUT algorithm to facilitate an encrypted embedding layer for the first time. CodedHELUT leverages coded inputs to improve overall efficiency and optimize memory usage. Our comprehensive empirical analysis encompasses both synthetic tables and real-world largescale word embedding models. CodedHELUT algorithm achieves amortized evaluation time of 0.018-0.242s for GloVe6B50d, 0.104-01.298s for GloVe42300d, 0.262-3.283s for GPT-2 and BERT embedding layers while maintaining high precision (16 bits)",[],,"['Jae-yun Kim', 'Saerom Park', 'Joohee Lee', 'Jung Hee Cheon']","['Department of Mathematical Science, Seoul National University', 'Industrial Engineering, Ulsan National Institute of Science and Technology', ""Sungshin Women's University"", 'Seoul National University']",
https://openreview.net/forum?id=ymgcTqrZLT,Security,Estimating Barycenters of Distributions with Neural Optimal Transport,"Given a collection of probability measures, a practitioner sometimes needs to find an ""average"" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness in illustrative scenarios and image data setups. Our source code is available at https://github.com/justkolesov/NOTBarycenters.",[],,"['Alexander Kolesov', 'Petr Mokrov', 'Igor Udovichenko', 'Milena Gazdieva', 'Gudmund Pammer', 'Evgeny Burnaev', 'Alexander Korotin']","['The Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology', 'ETHZ - ETH Zurich', 'Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology']",
https://openreview.net/forum?id=oCI9gHocws,Transparency & Explainability,KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations,"Robotic manipulation tasks often span over long horizons and encapsulate multiple subtasks with different skills. Learning policies directly from long-horizon demonstrations is challenging without intermediate keyframes guidance and corresponding skill annotations. Existing approaches for keyframe identification often struggle to offer reliable decomposition for low accuracy and fail to provide semantic relevance between keyframes and skills. For this, we propose a unified **K**eyframe **I**dentifier and **S**kill **A**notator (**KISA**) that utilizes pretrained visual-language representations for precise and interpretable decomposition of unlabeled demonstrations. Specifically, we develop a simple yet effective temporal enhancement module that enriches frame-level representations with expanded receptive fields to capture semantic dynamics at the video level. We further propose coarse contrastive learning and fine-grained monotonic encouragement to enhance the alignment between visual representations from keyframes and language representations from skills. The experimental results across three benchmarks demonstrate that KISA outperforms competitive baselines in terms of accuracy and interpretability of keyframe identification. Moreover, KISA exhibits robust generalization capabilities and the flexibility to incorporate various pretrained representations.",[],,"['Longxin Kou', 'Fei Ni', 'YAN ZHENG', 'Jinyi Liu', 'Yifu Yuan', 'Zibin Dong', 'Jianye HAO']","['Tianjin University', 'Tianjin University', 'Tianjin Unibersity, China', 'Tianjin University', 'Tianjin University', 'Tianjin University', 'Tianjin University']",
https://openreview.net/forum?id=6KLNiRdWH6,Security,Geometry-Aware Instrumental Variable Regression,"Instrumental variable (IV) regression can be approached through its formulation in terms of conditional moment restrictions (CMR). Building on variants of the generalized method of moments, most CMR estimators are implicitly based on approximating the population data distribution via reweightings of the empirical sample. While for large sample sizes, in the independent identically distributed (IID) setting, reweightings can provide sufficient flexibility, they might fail to capture the relevant information in presence of corrupted data or data prone to adversarial attacks. To address these shortcomings, we propose the Sinkhorn Method of Moments, an optimal transport-based IV estimator that takes into account the geometry of the data manifold through data-derivative information. We provide a simple plug-and-play implementation of our method that performs on par with related estimators in standard settings but improves robustness against data corruption and adversarial attacks.",[],,"['Heiner Kremer', 'Bernhard Schölkopf']","['Microsoft', '']",
https://openreview.net/forum?id=PMASooqgoq,Fairness & Bias,Sobolev Space Regularised Pre Density Models,"We propose a new approach to non-parametric density estimation that is based on regularizing a Sobolev norm of the density. This method is statistically consistent, and makes the inductive bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides pre-densities (i.e. not necessarily integrating to 1), which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher divergence based score matching methods for this task. We evaluate the resulting method on the comprehensive recent anomaly detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms.",[],,"['Mark Kozdoba', 'Binyamin Perets', 'Shie Mannor']","['Technion, Technion', 'Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'ECE, Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=cwIhvoTzuK,Privacy & Data Governance,Mean Estimation in the Add-Remove Model of Differential Privacy,"Differential privacy is often studied under two different models of neighboring datasets: the add-remove model and the swap model. While the swap model is frequently used in the academic literature to simplify analysis, many practical applications rely on the more conservative add-remove model, where obtaining tight results can be difficult. Here, we study the problem of one-dimensional mean estimation under the add-remove model. We propose a new algorithm and show that it is min-max optimal, achieving the best possible constant in the leading term of the mean squared error for all $\epsilon$, and that this constant is the same as the optimal algorithm under the swap model. These results show that the add-remove and swap models give nearly identical errors for mean estimation, even though the add-remove model cannot treat the size of the dataset as public information. We also demonstrate empirically that our proposed algorithm yields at least a factor of two improvement in mean squared error over algorithms frequently used in practice. One of our main technical contributions is a new hourglass mechanism, which might be of independent interest in other scenarios.",[],,"['Alex Kulesza', 'Ananda Theertha Suresh', 'Yuyan Wang']","['Google', 'Google', 'Google Research, Google']",
https://openreview.net/forum?id=NeEbsvnaWE,Fairness & Bias,Privately Learning Smooth Distributions on the Hypercube by Projections,"Fueled by the ever-increasing need for statistics that guarantee the privacy of their training sets, this article studies the centrally-private estimation of Sobolev-smooth densities of probability over the hypercube in dimension d. The contributions of this article are two-fold : Firstly, it generalizes the one-dimensional results of (Lalanne et al., 2023) to non-integer levels of smoothness and to a high-dimensional setting, which is important for two reasons : it is more suited for modern learning tasks, and it allows understanding the relations between privacy, dimensionality and smoothness, which is a central question with differential privacy. Secondly, this article presents a private strategy of estimation that is data-driven (usually referred to as adaptive in Statistics) in order to privately choose an estimator that achieves a good bias-variance trade-off among a finite family of private projection estimators without prior knowledge of the ground-truth smoothness β. This is achieved by adapting the Lepskii method for private selection, by adding a new penalization term that makes the estimation privacy-aware.",[],,"['Clément Lalanne', 'Sébastien Gadat']","['Institute of Mathematics of Toulouse, Paul Sabatier University', 'Toulouse School of Economics']",
https://openreview.net/forum?id=HOMXUneCTR,Fairness & Bias,Towards Understanding Inductive Bias in Transformers: A View From Infinity,"We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.",[],,"['Itay Lavie', 'Guy Gur-Ari', 'Zohar Ringel']","['Hebrew University of Jerusalem', 'Google', 'Hebrew University of Jerusalem, Israel']",
https://openreview.net/forum?id=DhxZVq1ZOo,Security,Collective Certified Robustness against Graph Injection Attacks,"We investigate certified robustness for GNNs under graph injection attacks. Existing research only provides sample-wise certificates by verifying each node independently, leading to very limited certifying performance. In this paper, we present the first collective certificate, which certifies a set of target nodes simultaneously. To achieve it, we formulate the problem as a binary integer quadratic constrained linear programming (BQCLP). We further develop a customized linearization technique that allows us to relax the BQCLP into linear programming (LP) that can be efficiently solved. Through comprehensive experiments, we demonstrate that our collective certification scheme significantly improves certification performance with minimal computational overhead. For instance, by solving the LP within 1 minute on the Citeseer dataset, we achieve a significant increase in the certified ratio from 0.0% to 81.2% when the injected node number is 5% of the graph size. Our paper marks a crucial step towards making provable defense more practical. Our source code is available at https://github.com/Yuni-Lai/CollectiveLPCert.",[],,"['Yuni Lai', 'Bailin PAN', 'kaihuang CHEN', 'Yancheng Yuan', 'Kai Zhou']","['Department of Computing, Hong Kong Polytechnic University', 'Applied mathematics, Hong Kong Polytechnic University', 'AMA, Hong Kong Polytechnic University', '', 'Hong Kong Polytechnic University']",
https://openreview.net/forum?id=E4ItiEU8Iu,Security,Run-Time Task Composition with Safety Semantics,"Compositionality is a critical aspect of scalable system design. Here, we focus on Boolean composition of learned tasks as opposed to functional or sequential composition. Existing Boolean composition for Reinforcement Learning focuses on reaching a satisfying absorbing state in environments with discrete action spaces, but does not support composable safety (i.e., avoidance) constraints. We provide three contributions: i) introduce two distinct notions of compositional safety semantics; ii) show how to enforce either safety semantics, prove correctness, and analyze the trade-offs between the two safety notions; and iii) extend Boolean composition from discrete action spaces to continuous action spaces. We demonstrate these techniques using modified versions of value iteration in a grid world, Deep Q-Network (DQN) in a grid world with image observations, and Twin Delayed DDPG (TD3) in a continuous-observation and continuous-action Bullet physics environment",[],,"['Kevin Leahy', 'Makai Mann', 'Zachary Serlin']","['Robotics Engineering, Worcester Polytechnic Institute', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology']",
https://openreview.net/forum?id=szvKJgmubh,Security,DataFreeShield: Defending Adversarial Attacks without Training Data,"Recent advances in adversarial robustness rely on an abundant set of training data, where using external or additional datasets has become a common setting. However, in real life, the training data is often kept private for security and privacy issues, while only the pretrained weight is available to the public. In such scenarios, existing methods that assume accessibility to the original data become inapplicable. Thus we investigate the pivotal problem of data-free adversarial robustness, where we try to achieve adversarial robustness without accessing any real data. Through a preliminary study, we highlight the severity of the problem by showing that robustness without the original dataset is difficult to achieve, even with similar domain datasets. To address this issue, we propose DataFreeShield, which tackles the problem from two perspectives: surrogate dataset generation and adversarial training using the generated data. Through extensive validation, we show that DataFreeShield outperforms baselines, demonstrating that the proposed method sets the first entirely data-free solution for the adversarial robustness problem.",[],,"['Hyeyoon Lee', 'Kanghyun Choi', 'Dain Kwon', 'SunJong Park', 'Mayoore Selvarasa Jaiswal', 'Noseong Park', 'Jonghyun Choi', 'Jinho Lee']","['Seoul National University', 'Electrical and Computer Engineering, Seoul National University', 'Seoul National University', '', 'University of Washington', '', 'Electrical and Computer Engineering, Seoul National University', 'CS, Seoul National University']",
https://openreview.net/forum?id=ErkzxOlOLy,Fairness & Bias,Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains,"The ability of deep networks to learn superior representations hinges on leveraging the proper inductive biases, considering the inherent properties of datasets. In tabular domains, it is critical to effectively handle heterogeneous features (both categorical and numerical) in a unified manner and to grasp irregular functions like piecewise constant functions. To address the challenges in the self-supervised learning framework, we propose a novel pretext task based on the classical binning method. The idea is straightforward: reconstructing the bin indices (either orders or classes) rather than the original values. This pretext task provides the encoder with an inductive bias to capture the irregular dependencies, mapping from continuous inputs to discretized bins, and mitigates the feature heterogeneity by setting all features to have category-type targets. Our empirical investigations ascertain several advantages of binning: capturing the irregular function, compatibility with encoder architecture and additional modifications, standardizing all features into equal sets, grouping similar values within a feature, and providing ordering information. Comprehensive evaluations across diverse tabular datasets corroborate that our method consistently improves tabular representation learning performance for a wide range of downstream tasks. The codes are available in https://github.com/kyungeun-lee/tabularbinning.",[],,"['Kyungeun Lee', 'Ye Seul Sim', 'Hyeseung Cho', 'Moonjung Eo', 'Suhee Yoon', 'Sanghyu Yoon', 'Woohyung Lim']","['DI Lab., LG AI Research', 'LG AI Research', 'LG AI Research', 'LG AI Research', 'LG AI Research', 'LG AI Research', 'LG AI Research']",
https://openreview.net/forum?id=a8ZpjLJuKk,Transparency & Explainability,Critical windows: non-asymptotic theory for feature emergence in diffusion models,"We develop theory to understand an intriguing property of diffusion models for image generation that we term *critical windows*. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Meng et al., 2022; Choi et al., 2022; Raya & Ambrogioni, 2023; Georgiev et al., 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned Gaussian mixtures. Finally, we use our bounds to give a rigorous interpretation of diffusion models as hierarchical samplers that progressively “decide” output features over a discrete sequence of times. We validate our bounds with experiments on synthetic data and show that critical windows may serve as a useful tool for diagnosing fairness and privacy violations in real-world diffusion models.",[],,"['Marvin Li', 'Sitan Chen']","['Computer Science, Harvard University', 'Computer Science, Harvard University']",
https://openreview.net/forum?id=L1W9ZWPq9E,Fairness & Bias,Debiased Distribution Compression,"Modern compression methods can summarize a target distribution $\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with $\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. For larger-scale compression tasks, low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\text{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.",[],,"['Lingxiao Li', 'Raaz Dwivedi', 'Lester Mackey']","['Netflix', 'Cornell University', 'Microsoft Research New England']",
https://openreview.net/forum?id=f49AkFT5jf,Security,Data Poisoning Attacks against Conformal Prediction,"The efficient and theoretically sound uncertainty quantification is crucial for building trust in deep learning models. This has spurred a growing interest in conformal prediction (CP), a powerful technique that provides a model-agnostic and distribution-free method for obtaining conformal prediction sets with theoretical guarantees. However, the vulnerabilities of such CP methods with regard to dedicated data poisoning attacks have not been studied previously. To bridge this gap, for the first time, we in this paper propose a new class of black-box data poisoning attacks against CP, where the adversary aims to cause the desired manipulations of some specific examples' prediction uncertainty results (instead of misclassifications). Additionally, we design novel optimization frameworks for our proposed attacks. Further, we conduct extensive experiments to validate the effectiveness of our attacks on various settings (e.g., the full and split CP settings). Notably, our extensive experiments show that our attacks are more effective in manipulating uncertainty results than traditional poisoning attacks that aim at inducing misclassifications, and existing defenses against conventional attacks are ineffective against our proposed attacks.",[],,"['Yangyi Li', 'Aobo Chen', 'Wei Qian', 'Chenxu Zhao', 'Divya Lidder', 'Mengdi Huai']","['Iowa State University', 'Computer Science, Iowa State University', 'Department of Computer Science, Iowa State University', 'Computer science , Iowa State University', 'Computer Science, Iowa State University', 'Computer Science, Iowa State University']",
https://openreview.net/forum?id=FvLd8Gr7xq,Fairness & Bias,Vague Prototype-Oriented Diffusion Model for Multi-Class Anomaly Detection,"Multi-class unsupervised anomaly detection aims to create a unified model for identifying anomalies in objects from multiple classes when only normal data is available. In such a challenging setting, widely used reconstruction-based networks persistently grapple with the ""identical shortcut"" problem, wherein the infiltration of abnormal information from the condition biases the output towards an anomalous distribution. In response to this critical challenge, we introduce a Vague Prototype-Oriented Diffusion Model (VPDM) that extracts only fundamental information from the condition to prevent the occurrence of the ""identical shortcut"" problem from the input layer. This model leverages prototypes that contain only vague information about the target as the initial condition. Subsequently, a novel conditional diffusion model is introduced to incrementally enhance details based on vague conditions. Finally, a Vague Prototype-Oriented Optimal Transport (VPOT) method is proposed to provide more accurate information about conditions. All these components are seamlessly integrated into a unified optimization objective. The effectiveness of our approach is demonstrated across diverse datasets, including the MVTec, VisA, and MPDD benchmarks, achieving state-of-the-art results.",[],,"['Yuxin Li', 'Yaoxuan Feng', 'Bo Chen', 'Wenchao Chen', 'Yubiao Wang', 'Xinyue Hu', 'baolin sun', 'Chunhui Qu', 'Mingyuan Zhou']","['School of electronic Engineering, Xidian University', 'The School of Electronic Engineering, Xidian University', 'Electronic Engineering, Xidian University', 'School of electronic Engineering, Xidian University', 'Xidian University', 'Xidian University', 'Alibaba Group', 'Google']",
https://openreview.net/forum?id=ycXo4tQIpN,Fairness & Bias,Learning Shadow Variable Representation for Treatment Effect Estimation under Collider Bias,"One of the significant challenges in treatment effect estimation is collider bias, a specific form of sample selection bias induced by the common causes of both the treatment and outcome. Identifying treatment effects under collider bias requires well-defined shadow variables in observational data, which are assumed to be related to the outcome and independent of the sample selection mechanism, conditional on the other observed variables. However, finding a valid shadow variable is not an easy task in real-world scenarios and requires domain-specific knowledge from experts. Therefore, in this paper, we propose a novel method that can automatically learn shadow-variable representations from observational data without prior knowledge. To ensure the learned representations satisfy the assumptions of the shadow variable, we introduce a tester to perform hypothesis testing in the representation learning process. We iteratively generate representations and test whether they satisfy the shadow-variable assumptions until they pass the test. With the help of the learned shadow-variable representations, we propose a novel treatment effect estimator to address collider bias. Experiments show that the proposed methods outperform existing treatment effect estimation methods under collider bias and prove their potential application value.",[],,"['Baohong Li', 'Haoxuan Li', 'Ruoxuan Xiong', 'Anpeng Wu', 'Fei Wu', 'Kun Kuang']","['College of Computer Science and Technology, Zhejiang University', 'Center for Data Science, Peking University', 'Emory University', 'CS, Zhejiang University', 'Zhejiang University', 'CS, Zhejiang University']",
https://openreview.net/forum?id=kUj9b2CezT,Fairness & Bias,A Generative Approach for Treatment Effect Estimation under Collider Bias: From an Out-of-Distribution Perspective,"Resulting from non-random sample selection caused by both the treatment and outcome, collider bias poses a unique challenge to treatment effect estimation using observational data whose distribution differs from that of the target population. In this paper, we rethink collider bias from an out-of-distribution (OOD) perspective, considering that the entire data space of the target population consists of two different environments: The observational data selected from the target population belongs to a seen environment labeled with $S=1$ and the missing unselected data belongs to another unseen environment labeled with $S=0$. Based on this OOD formulation, we utilize small-scale representative data from the entire data space with no environmental labels and propose a novel method, i.e., Coupled Counterfactual Generative Adversarial Model (C$^2$GAM), to simultaneously generate the missing $S=0$ samples in observational data and the missing $S$ labels in the small-scale representative data. With the help of C$^2$GAM, collider bias can be addressed by combining the generated $S=0$ samples and the observational data to estimate treatment effects. Extensive experiments on synthetic and real-world data demonstrate that plugging C$^2$GAM into existing treatment effect estimators achieves significant performance improvements.",[],,"['Baohong Li', 'Haoxuan Li', 'Anpeng Wu', 'Minqin Zhu', 'shiyuan Peng', 'Qingyu Cao', 'Kun Kuang']","['College of Computer Science and Technology, Zhejiang University', 'Center for Data Science, Peking University', 'CS, Zhejiang University', 'Computer Science and Technology, Zhejiang University', 'Zhejiang University', 'Zhejiang University of Technology', 'CS, Zhejiang University']",
https://openreview.net/forum?id=1N7pjXKkx8,Privacy & Data Governance,PID: Prompt-Independent Data Protection Against Latent Diffusion Models,"The few-shot fine-tuning of Latent Diffusion Models (LDMs) has enabled them to grasp new concepts from a limited number of images. However, given the vast amount of personal images accessible online, this capability raises critical concerns about civil privacy. While several previous defense methods have been developed to prevent such misuse of LDMs, they typically assume that the textual prompts used by data protectors exactly match those employed by data exploiters. In this paper, we first empirically demonstrate that breaking this assumption, i.e., in cases where discrepancies exist between the textual conditions used by protectors and exploiters, could substantially reduces the effectiveness of these defenses. Furthermore, considering the visual encoder's independence from textual prompts, we delve into the visual encoder and thoroughly investigate how manipulating the visual encoder affects the few-shot fine-tuning process of LDMs. Drawing on these insights, we propose a simple yet effective method called Prompt-Independent Defense (PID) to safeguard privacy against LDMs. We show that PID can act as a strong privacy shield on its own while requiring significantly less computational power. We believe our studies, along with the comprehensive understanding and new defense method, provide a notable advance toward reliable data protection against LDMs.",[],,"['Ang Li', 'Yichuan Mo', 'Mingjie Li', 'Yisen Wang']","['Peking University', 'School of Intelligence Science and Technology, Peking University', 'CISPA Helmholtz Center for Information Security', 'Peking University']",
https://openreview.net/forum?id=8PTx4CpNoT,Fairness & Bias,Emergent Representations of Program Semantics in Language Models Trained on Programs,"We present evidence that language models (LMs) of code can learn to represent the formal semantics of programs, despite being trained only to perform next-token prediction. Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments. Each program in the corpus is preceded by a (partial) specification in the form of several input-output grid world states. Despite providing no further inductive biases, we find that a probing classifier is able to extract increasingly accurate representations of the *unobserved, intermediate* grid world states from the LM hidden states over the course of training, suggesting the LM acquires an emergent ability to *interpret* programs in the formal sense. We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe. We anticipate that this technique may be generally applicable to a broad range of *semantic* probing experiments. In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code.",[],,"['Charles Jin', 'Martin Rinard']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=YRWdiaupCr,Fairness & Bias,Two-Stage Shadow Inclusion Estimation: An IV Approach for Causal Inference under Latent Confounding and Collider Bias,"Latent confounding bias and collider bias are two key challenges of causal inference in observational studies. Latent confounding bias occurs when failing to control the unmeasured covariates that are common causes of treatments and outcomes, which can be addressed by using the Instrumental Variable (IV) approach. Collider bias comes from non-random sample selection caused by both treatments and outcomes, which can be addressed by using a different type of instruments, i.e., shadow variables. However, in most scenarios, these two biases simultaneously exist in observational data, and the previous methods focusing on either one are inadequate. To the best of our knowledge, no approach has been developed for causal inference when both biases exist. In this paper, we propose a novel IV approach, Two-Stage Shadow Inclusion (2SSI), which can simultaneously address latent confounding bias and collider bias by utilizing the residual of the treatment as a shadow variable. Extensive experimental results on benchmark synthetic datasets and a real-world dataset show that 2SSI achieves noticeable performance improvement when both biases exist compared to existing methods.",[],,"['Baohong Li', 'Anpeng Wu', 'Ruoxuan Xiong', 'Kun Kuang']","['College of Computer Science and Technology, Zhejiang University', 'CS, Zhejiang University', 'Emory University', 'CS, Zhejiang University']",
https://openreview.net/forum?id=eMQyb1tvvc,Transparency & Explainability,Towards efficient deep spiking neural networks construction with spiking activity based pruning,"The emergence of deep and large-scale spiking neural networks (SNNs) exhibiting high performance across diverse complex datasets has led to a need for compressing network models due to the presence of a significant number of redundant structural units, aiming to more effectively leverage their low-power consumption and biological interpretability advantages. Currently, most model compression techniques for SNNs are based on unstructured pruning of individual connections, which requires specific hardware support. Hence, we propose a structured pruning approach based on the activity levels of convolutional kernels named Spiking Channel Activity-based (SCA) network pruning framework. Inspired by synaptic plasticity mechanisms, our method dynamically adjusts the network's structure by pruning and regenerating convolutional kernels during training, enhancing the model's adaptation to the current target task. While maintaining model performance, this approach refines the network architecture, ultimately reducing computational load and accelerating the inference process. This indicates that structured dynamic sparse learning methods can better facilitate the application of deep SNNs in low-power and high-efficiency scenarios.",[],,"['Yaxin Li', 'Qi Xu', 'Jiangrong Shen', 'Hongming Xu', 'Long Chen', 'Gang Pan']","['Dalian University of Technology', 'Dalian University of Technology, School of Computer Science and Technology', 'College of Computer Science, Zhejiang University', 'Faculty of Medicine, Dalian University of Technology', 'Institute of Clinical Sciences, Faculty of Medicine, Imperial College London', '']",
https://openreview.net/forum?id=FM61SQzF3N,Transparency & Explainability,IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation,"Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion at a single temporal scale of auditory and visual features without employing selective attention mechanisms, which is in sharp contrast with the brain. To address this, We propose a novel model called intra- and inter-attention network (IIANet), which leverages the attention mechanism for efficient audio-visual feature fusion. IIANet consists of two types of attention blocks: intra-attention (IntraA) and inter-attention (InterA) blocks, where the InterA blocks are distributed at the top, middle and bottom of IIANet. Heavily inspired by the way how human brain selectively focuses on relevant content at various temporal scales, these blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of IIANet, outperforming previous state-of-the-art methods while maintaining comparable inference time. In particular, the fast version of IIANet (IIANet-fast) has only 7% of CTCNet’s MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.",[],,"['Kai Li', 'Runxuan Yang', 'Fuchun Sun', 'Xiaolin Hu']","['Tsinghua University', '', 'Department of Computer Science and Technology, Tsinghua University', 'Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=jklD0TV5Hw,Privacy & Data Governance,Seesaw: Compensating for Nonlinear Reduction with Linear Computations for Private Inference,"With increasingly serious data privacy concerns and strict regulations, privacy-preserving machine learning (PPML) has emerged to securely execute machine learning tasks without violating privacy. Unfortunately, the computational cost to securely execute nonlinear computations in PPML remains significant, calling for new model architecture designs with fewer nonlinear operations. We propose Seesaw, a novel neural architecture search method tailored for PPML. Seesaw exploits a previously unexplored opportunity to leverage more linear computations and nonlinear result reuse, in order to compensate for the accuracy loss due to nonlinear reduction. It incorporates specifically designed pruning and search strategies, not only to efficiently handle the much larger design space of both linear and nonlinear operators, but also to achieve a better balance between the model accuracy and the online/offline execution latencies. Compared to the state-of-the-art design for image classification on ImageNet, Seesaw achieves 1.68$\times$ lower online latency and 1.55$\times$ lower total online + offline latency at 71% iso-accuracy, or 3.65% higher accuracy at iso-latency of 190 seconds, while using much simpler and faster search and training methods.",[],,"['Fabing Li', 'Yuanhao Zhai', 'Shuangyu Cai', 'Mingyu Gao']","[""Xi'an Jiaotong University, Tsinghua University"", 'State University of New York at Buffalo', 'Shanghai Artificial Intelligence Laboratory']",
https://openreview.net/forum?id=J5VB1h3Aed,Fairness & Bias,Revisiting the Role of Language Priors in Vision-Language Models,"Vision-language models (VLMs) are impactful in part because they can be applied to a variety of visual understanding tasks in a zero-shot fashion, without any fine-tuning. We study $\textit{generative VLMs}$ that are trained for next-word generation given an image. We explore their zero-shot performance on the illustrative task of image-text retrieval across nine popular vision-language benchmarks. Our first observation is that they can be repurposed for discriminative tasks (such as image-text retrieval) by simply computing the match score of generating a particular text string given an image. We call this probabilistic score the Visual Generative Pre-Training Score (VisualGPTScore). While the VisualGPTScore produces near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on others. We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions. In fact, we demonstrate that even a ""blind"" language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago. We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model. We show that the VisualGPTScore, when appropriately debiased, is a strong zero-shot baseline for vision-language understanding, oftentimes producing state-of-the-art accuracy.",[],,"['Zhiqiu Lin', 'Xinyue Chen', 'Deepak Pathak', 'Pengchuan Zhang', 'Deva Ramanan']","['Carnegie Mellon University', 'Computer Science, ByteDance Inc.', 'Skild AI', 'Meta AI', 'Robotics, School of Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=m8lCi7rG4u,Security,Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency,"Catastrophic overfitting (CO) presents a significant challenge in single-step adversarial training (AT), manifesting as highly distorted deep neural networks (DNNs) that are vulnerable to multi-step adversarial attacks. However, the underlying factors that lead to the distortion of decision boundaries remain unclear. In this work, we delve into the specific changes within different DNN layers and discover that during CO, the former layers are more susceptible, experiencing earlier and greater distortion, while the latter layers show relative insensitivity. Our analysis further reveals that this increased sensitivity in former layers stems from the formation of $\textit{pseudo-robust shortcuts}$, which alone can impeccably defend against single-step adversarial attacks but bypass genuine-robust learning, resulting in distorted decision boundaries. Eliminating these shortcuts can partially restore robustness in DNNs from the CO state, thereby verifying that dependence on them triggers the occurrence of CO. This understanding motivates us to implement adaptive weight perturbations across different layers to hinder the generation of $\textit{pseudo-robust shortcuts}$, consequently mitigating CO. Extensive experiments demonstrate that our proposed method, $\textbf{L}$ayer-$\textbf{A}$ware Adversarial Weight $\textbf{P}$erturbation (LAP), can effectively prevent CO and further enhance robustness.",[],,"['Runqi Lin', 'Chaojian Yu', 'Bo Han', 'Hang Su', 'Tongliang Liu']","['School of Computer Science, University of Sydney', 'School of Computer Science, The University of Sydney', 'Department of Computer Science, HKBU', 'Computer Science, Tsinghua University', 'University of Sydney']",
https://openreview.net/forum?id=DYN66IJCI9,Fairness & Bias,Graph Distillation with Eigenbasis Matching,"The increasing amount of graph data places requirements on the efficient training of graph neural networks (GNNs). The emerging graph distillation (GD) tackles this challenge by distilling a small synthetic graph to replace the real large graph, ensuring GNNs trained on real and synthetic graphs exhibit comparable performance. However, existing methods rely on GNN-related information as supervision, including gradients, representations, and trajectories, which have two limitations. First, GNNs can affect the spectrum (*i.e*., eigenvalues) of the real graph, causing *spectrum bias* in the synthetic graph. Second, the variety of GNN architectures leads to the creation of different synthetic graphs, requiring *traversal* to obtain optimal performance. To tackle these issues, we propose Graph Distillation with Eigenbasis Matching (GDEM), which aligns the eigenbasis and node features of real and synthetic graphs. Meanwhile, it directly replicates the spectrum of the real graph and thus prevents the influence of GNNs. Moreover, we design a discrimination constraint to balance the effectiveness and generalization of GDEM. Theoretically, the synthetic graphs distilled by GDEM are restricted spectral approximations of the real graphs. Extensive experiments demonstrate that GDEM outperforms state-of-the-art GD methods with powerful cross-architecture generalization ability and significant distillation efficiency. Our code is available at https://github.com/liuyang-tian/GDEM.",[],,"['Yang Liu', 'Deyu Bo', 'Chuan Shi']","['Beijing University of Posts and Telecommunications', 'National University of Singapore', 'Institute of Computing Technology, Beijing University of Post and Telecommunication, Tsinghua University']",
https://openreview.net/forum?id=7emOSb5UfX,Security,Adaptive Text Watermark for Large Language Models,"The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining robustness, security, and the ability to detect watermarks without prior knowledge of the prompt and model. This paper proposes an adaptive text watermarking strategy to address such a challenge. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured by an auxiliary model and keep the low-entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of *un-watermarked* LLMs while maintaining sufficient security.",[],,"['Yepeng Liu', 'Yuheng Bu']","['', 'University of Florida']",
https://openreview.net/forum?id=ICvWruTEDH,Security,Graph Adversarial Diffusion Convolution,"This paper introduces a min-max optimization formulation for the Graph Signal Denoising (GSD) problem. In this formulation, we first maximize the second term of GSD by introducing perturbations to the graph structure based on Laplacian distance and then minimize the overall loss of the GSD. By solving the min-max optimization problem, we derive a new variant of the Graph Diffusion Convolution (GDC) architecture, called Graph Adversarial Diffusion Convolution (GADC). GADC differs from GDC by incorporating an additional term that enhances robustness against adversarial attacks on the graph structure and noise in node features. Moreover, GADC improves the performance of GDC on heterophilic graphs. Extensive experiments demonstrate the effectiveness of GADC across various datasets. Code is available at https://github.com/SongtaoLiu0823/GADC.",[],,"['Songtao Liu', 'Jinghui Chen', 'Tianfan Fu', 'Lu Lin', 'Marinka Zitnik', 'Dinghao Wu']","['Pennsylvania State University', 'Pennsylvania State University', 'Rensselaer Polytechnic Institute', 'Pennsylvania State University', 'Biomedical Informatics, Harvard University', '']",
https://openreview.net/forum?id=2xLyc5TkFl,Security,The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks,"In safety-critical applications such as medical imaging and autonomous driving, where decisions have profound implications for patient health and road safety, it is imperative to maintain both high adversarial robustness to protect against potential adversarial attacks and reliable uncertainty quantification in decision-making. With extensive research focused on enhancing adversarial robustness through various forms of adversarial training (AT), a notable knowledge gap remains concerning the uncertainty inherent in adversarially trained models. To address this gap, this study investigates the uncertainty of deep learning models by examining the performance of conformal prediction (CP) in the context of standard adversarial attacks within the adversarial defense community. It is first unveiled that existing CP methods do not produce informative prediction sets under the commonly used $l_{\infty}$-norm bounded attack if the model is not adversarially trained, which underpins the importance of adversarial training for CP. Our paper next demonstrates that the prediction set size (PSS) of CP using adversarially trained models with AT variants is often worse than using standard AT, inspiring us to research into CP-efficient AT for improved PSS. We propose to optimize a Beta-weighting loss with an entropy minimization regularizer during AT to improve CP-efficiency, where the Beta-weighting loss is shown to be an upper bound of PSS at the population level by our theoretical analysis. Moreover, our empirical study on four image classification datasets across three popular AT baselines validates the effectiveness of the proposed Uncertainty-Reducing AT (AT-UR).",[],,"['Ziquan Liu', 'Yufei CUI', 'Yan Yan', 'Yi Xu', 'Xiangyang Ji', 'Xue Liu', 'Antoni B. Chan']","['School of EECS, Queen Mary, University of London', 'School of Computer Science, McGill University', 'EECS, Washington State University, Pullman', 'Dalian University of Technology', 'Automation, Tsinghua University', 'McGill University', 'Department of Computer Science, City University of Hong Kong']",
https://openreview.net/forum?id=n8g6WMxt09,Fairness & Bias,Decoding-time Realignment of Language Models,"Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as reinforcement learning from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.",[],,"['Tianlin Liu', 'Shangmin Guo', 'Leonardo Bianco', 'Daniele Calandriello', 'Quentin Berthet', 'Felipe Llinares-López', 'Jessica Hoffmann', 'Lucas Dixon', 'Michal Valko', 'Mathieu Blondel']","['Google DeepMind', 'University of Edinburgh', 'DeepMind', 'Google DeepMind', 'Google LLC', 'Google', 'Research, Google', 'Stealth Startup', 'Google']",
https://openreview.net/forum?id=15MpDbv3IQ,Privacy & Data Governance,Tuning-free Estimation and Inference of Cumulative Distribution Function under Local Differential Privacy,"We introduce a novel algorithm for estimating Cumulative Distribution Function (CDF) values under Local Differential Privacy (LDP) by exploiting an unexpected connection between LDP and the current status problem, a classical survival data problem in statistics. This connection leads to the development of tools for constrained isotonic estimation based on binary queries. Through mathematical proofs and extensive numerical testing, we demonstrate that our method achieves uniform and $L_2$ error bounds when estimating the entire CDF curve. By employing increasingly dense grids, the error bound can be improved, exhibiting an asymptotic normal distribution of the proposed estimator. Theoretically, we show that the error bound smoothly changes as the number of grids increases relative to the sample size $n$. Computationally, we demonstrate that our constrained isotonic estimator can be efficiently computed deterministically, eliminating the need for hyperparameters or random optimization.",[],,"['Yi Liu', 'Qirui Hu', 'Linglong Kong']","['York University', 'Department of statistics and Data Science, Tsinghua University', 'University of Alberta']",
https://openreview.net/forum?id=qmUbSAgz08,Fairness & Bias,Multi-Source Conformal Inference Under Distribution Shift,"Recent years have experienced increasing utilization of complex machine learning models across multiple sources of data to inform more generalizable decision-making. However, distribution shifts across data sources and privacy concerns related to sharing individual-level data, coupled with a lack of uncertainty quantification from machine learning predictions, make it challenging to achieve valid inferences in multi-source environments. In this paper, we consider the problem of obtaining distribution-free prediction intervals for a target population, leveraging multiple potentially biased data sources. We derive the efficient influence functions for the quantiles of unobserved outcomes in the target and source populations, and show that one can incorporate machine learning prediction algorithms in the estimation of nuisance functions while still achieving parametric rates of convergence to nominal coverage probabilities. Moreover, when conditional outcome invariance is violated, we propose a data-adaptive strategy to upweight informative data sources for efficiency gain and downweight non-informative data sources for bias reduction. We highlight the robustness and efficiency of our proposals for a variety of conformal scores and data-generating mechanisms via extensive synthetic experiments. Hospital length of stay prediction intervals for pediatric patients undergoing a high-risk cardiac surgical procedure between 2016-2022 in the U.S. illustrate the utility of our methodology.",[],,"['Yi Liu', 'Alexander Levis', 'Sharon-Lise Normand', 'Larry Han']","['Department of Statistics, North Carolina State University', 'Northeastern University']",
https://openreview.net/forum?id=pPnkpvBeZN,Fairness & Bias,Class-Imbalanced Graph Learning without Class Rebalancing,"Class imbalance is prevalent in real-world node classification tasks and poses great challenges for graph learning models. Most existing studies are rooted in a class-rebalancing (CR) perspective and address class imbalance with class-wise reweighting or resampling. In this work, we approach the root cause of class-imbalance bias from an topological paradigm. Specifically, we theoretically reveal two **fundamental phenomena in the graph topology** that greatly exacerbate the predictive bias stemming from class imbalance. On this basis, we devise a lightweight topological augmentation framework BAT to mitigate the class-imbalance bias without class rebalancing. Being orthogonal to CR, BAT can function as an **efficient plug-and-play module** that can be seamlessly combined with and significantly boost existing CR techniques. Systematic experiments on real-world imbalanced graph learning tasks show that BAT can deliver up to 46.27% performance gain and up to 72.74% bias reduction over existing techniques. Code, examples, and documentations are available at https://github.com/ZhiningLiu1998/BAT.",[],,"['Zhining Liu', 'Ruizhong Qiu', 'Zhichen Zeng', 'Hyunsik Yoo', 'David Zhou', 'Zhe Xu', 'Yada Zhu', 'Kommy Weldemariam', 'Jingrui He', 'Hanghang Tong']","['University of Illinois at Urbana-Champaign', 'Siebel School of Computing and Data Science, University of Illinois Urbana-Champaign', 'Computer Science, University of Illinois Urbana-Champaign', 'Siebel School of Computing and Data Science, University of Illinois, Urbana-Champaign', 'Department of Computer Science', 'University of Illinois, Urbana Champaign', 'IBM Research', 'IBM Research, International Business Machines', 'School of Information Sciences, University of Illinois at Urbana-Champaign', 'computer science, University of Illinois at Urbana-Champaign']",
https://openreview.net/forum?id=l1YbS3qkdk,Fairness & Bias,Causal Discovery via Conditional Independence Testing with Proxy Variables,"Distinguishing causal connections from correlations is important in many scenarios. However, the presence of unobserved variables, such as the latent confounder, can introduce bias in conditional independence testing commonly employed in constraint-based causal discovery for identifying causal relations. To address this issue, existing methods introduced proxy variables to adjust for the bias caused by unobserveness. However, these methods were either limited to categorical variables or relied on strong parametric assumptions for identification. In this paper, we propose a novel hypothesis-testing procedure that can effectively examine the existence of the causal relationship over continuous variables, without any parametric constraint. Our procedure is based on discretization, which under completeness conditions, is able to asymptotically establish a linear equation whose coefficient vector is identifiable under the causal null hypothesis. Based on this, we introduce our test statistic and demonstrate its asymptotic level and power. We validate the effectiveness of our procedure using both synthetic and real-world data.",[],,"['Mingzhou Liu', 'Xinwei Sun', 'Yu QIAO', 'Yizhou Wang']","['Peking University', 'School of data science, Fudan University', 'Shanghai Jiao Tong University', 'Computer Science, Peking University']",
https://openreview.net/forum?id=dmHHVcHFdM,Transparency & Explainability,Causality Based Front-door Defense Against Backdoor Attack on Language Models,"We have developed a new framework based on the theory of causal inference to protect language models against backdoor attacks. Backdoor attackers can poison language models with different types of triggers, such as words, sentences, grammar, and style, enabling them to selectively modify the decision-making of the victim model. However, existing defense approaches are only effective when the backdoor attack form meets specific assumptions, making it difficult to counter diverse backdoor attacks. We propose a new defense framework **F**ront-door **A**djustment for **B**ackdoor **E**limination (FABE) based on causal reasoning that does not rely on assumptions about the form of triggers. This method effectively differentiates between spurious and legitimate associations by creating a 'front door' that maps out the actual causal relationships. The term 'front door' refers to a text that retains the semantic equivalence of the initial input, which is generated by an additional, fine-tuned language model, denoted as the defense model. Our defense experiments against various attack methods at the token, sentence, and syntactic levels reduced the attack success rate from 93.63% to 15.12%, improving the defense effect by 2.91 times compared to the best baseline result of 66.61%, achieving state-of-the-art results. Through ablation study analysis, we analyzed the effect of each module in FABE, demonstrating the importance of complying with the front-door criterion and front-door adjustment formula, which also explains why previous methods failed. Our code to reproduce the experiments is available at: https://github.com/lyr17/Frontdoor-Adjustment-Backdoor-Elimination.",[],,"['Yiran Liu', 'Xiaoang Xu', 'Zhiyi Hou', 'Yang Yu']","['Tsinghua University, Tsinghua University', 'College of Computer Science and Technology, Harbin University of Science and Technology', 'FACULTY OF COMPUTING, Harbin Institute of Technology', 'Stanford University']",
https://openreview.net/forum?id=wnkC5T11Z9,Transparency & Explainability,Attention Meets Post-hoc Interpretability: A Mathematical Perspective,"Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.",[],,"['Gianluigi Lopardo', 'Frederic Precioso', 'Damien Garreau']","['Université de Nice-Sophia Antipolis', ""Universite Cote d'Azur"", 'CAIDAS, Bayerische Julius-Maximilians-Universität Würzburg']",
https://openreview.net/forum?id=WPfYVdJHPk,Security,Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training,"Modern machine learning applications increasingly demand greater computational resources for training large models. Decentralized training has emerged as an effective means to democratize this technology. However, the potential threats associated with this approach remain inadequately discussed, posing a hurdle to the development of decentralized training infrastructures. This paper aims to initiate discussion towards this end by exploring the robustness of decentralized training from three primary perspectives. Firstly, we articulate our position on establishing robust decentralized training by outlining potential threats and the corresponding countermeasures. Secondly, we illustrate a nascent poisoning attack targeting decentralized training frameworks, easily executable by malicious stages. To mitigate this security threat and ensure efficient training, we propose a robust training framework, integrating a 100% detection strategy and efficient training mechanisms. Finally, we demonstrate the severity of the proposed attack and the effectiveness of our robust training framework. This position paper emphasizes the urgency of exploring the robustness of decentralized training and proposes a feasible solution. The code is available at https://github.com/dcx001016/pipeline_attack.",[],,"['Lin Lu', 'Chenxi Dai', 'Wangcheng Tao', 'Binhang Yuan', 'Yanan Sun', 'Pan Zhou']","['School of Cyber Science and Engineering, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Hong Kong University of Science and Technology', '', 'School of Cyber Science and Engineering, Huazhong University of Science and Technology']",
https://openreview.net/forum?id=NFEJQn7vX0,Privacy & Data Governance,Optimal Differentially Private Model Training with Public Data,"Differential privacy (DP) ensures that training a machine learning model does not leak private data. In practice, we may have access to auxiliary public data that is free of privacy concerns. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of pure and approximate DP. To answer the first question, we prove tight (up to log factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical risk minimization, and stochastic convex optimization. We show that the optimal error rates can be attained (up to log factors) by either discarding private data and training a public model, or treating public data like it is private and using an optimal DP algorithm. To address the second question, we develop novel algorithms that are ""even more optimal"" (i.e. better constants) than the asymptotically optimal approaches described above. For local DP mean estimation, our algorithm is optimal including constants. Empirically, our algorithms show benefits over the state-of-the-art.",[],,"['Andrew Lowy', 'Zeman Li', 'Tianjian Huang', 'Meisam Razaviyayn']","['University of Wisconsin - Madison', '', 'University of Southern California', 'University of Southern California']",
https://openreview.net/forum?id=lsavZkUjFZ,Transparency & Explainability,CauDiTS: Causal Disentangled Domain Adaptation of Multivariate Time Series,"Unsupervised domain adaptation of multivariate time series aims to train a model to adapt its classification ability from a labeled source domain to an unlabeled target domain, where there are differences in the distribution between domains. Existing methods extract domain-invariant features directly via a shared feature extractor, neglecting the exploration of the underlying causal patterns, which undermines their reliability, especially in complex multivariate dynamic systems. To address this problem, we propose CauDiTS, an innovative framework for unsupervised domain adaptation of multivariate time series. CauDiTS adopts an adaptive rationale disentangler to disentangle domain-common causal rationales and domain-specific correlations from variable interrelationships. The stability of causal rationales across domains is vital for filtering domainspecific perturbations and facilitating the extraction of domain-invariant representations. Moreover, we promote the cross-domain consistency of intra-class causal rationales employing the learning strategies of causal prototype consistency and domain-intervention causality invariance. CauDiTS is evaluated on four benchmark datasets, demonstrating its effectiveness and outperforming state-of-the-art methods.",[],,"['junxin lu', 'Shiliang Sun']","['East China Normal University, East China Normal University', '']",
https://openreview.net/forum?id=HO0g6cHVZx,Transparency & Explainability,EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time,"Understanding and explaining the predictions of Graph Neural Networks (GNNs), is crucial for enhancing their safety and trustworthiness. Subgraph-level explanations are gaining attention for their intuitive appeal. However, most existing subgraph-level explainers face efficiency challenges in explaining GNNs due to complex search processes. The key challenge is to find a balance between intuitiveness and efficiency while ensuring transparency. Additionally, these explainers usually induce subgraphs by nodes, which may introduce less-intuitive disconnected nodes in the subgraph-level explanations or omit many important subgraph structures. In this paper, we reveal that inducing subgraph explanations by edges is more comprehensive than other subgraph inducing techniques. We also emphasize the need of determining the subgraph explanation size for each data instance, as different data instances may involve different important substructures. Building upon these considerations, we introduce a training-free approach, named EiG-Search. We employ an efficient linear-time search algorithm over the edge-induced subgraphs, where the edges are ranked by an enhanced gradient-based importance. We conduct extensive experiments on a total of seven datasets, demonstrating its superior performance and efficiency both quantitatively and qualitatively over the leading baselines.",[],,"['Shengyao Lu', 'Bang Liu', 'Keith G Mills', 'Jiao He', 'Di Niu']","['Computer Engineering, University of Alberta', 'DIRO, University of Montreal', 'Department of Electrical and Computer Engineering, University of Alberta', 'Pudong, huawei', 'Department of Electrical and Computer Engineering, University of Alberta']",
https://openreview.net/forum?id=jZVen2JguY,Fairness & Bias,FiT: Flexible Vision Transformer for Diffusion Model,"In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions. Repository available at https://github.com/whlzy/FiT.",[],,"['Zeyu Lu', 'ZiDong Wang', 'Di Huang', 'Chengyue Wu', 'Xihui Liu', 'Wanli Ouyang', 'LEI BAI']","['Shanghai Jiao Tong University', 'Department of Information Engineering, Chinese University of Hong Kong', '', 'Departmant of Computer Science, The University of Hong Kong', 'EEE and IDS, University of Hong Kong', 'Shanghai AI Lab', 'AI for Science, Shanghai AI Laboratory']",
https://openreview.net/forum?id=0HUInAsdoo,Fairness & Bias,OxyGenerator: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning,"Accurately reconstructing the global ocean deoxygenation over a century is crucial for assessing and protecting marine ecosystem. Existing expert-dominated numerical simulations fail to catch up with the dynamic variation caused by global warming and human activities. Besides, due to the high-cost data collection, the historical observations are severely sparse, leading to big challenge for precise reconstruction. In this work, we propose OxyGenerator, the first deep learning based model, to reconstruct the global ocean deoxygenation from 1920 to 2023. Specifically, to address the heterogeneity across large temporal and spatial scales, we propose zoning-varying graph message-passing to capture the complex oceanographic correlations between missing values and sparse observations. Additionally, to further calibrate the uncertainty, we incorporate inductive bias from dissolved oxygen (DO) variations and chemical effects. Compared with in-situ DO observations, OxyGenerator significantly outperforms CMIP6 numerical simulations, reducing MAPE by 38.77%, demonstrating a promising potential to understand the “breathless ocean” in data-driven manner.",[],,"['Bin Lu', 'Ze Zhao', 'Luyu Han', 'Xiaoying Gan', 'Yuntao Zhou', 'Lei Zhou', 'Luoyi Fu', 'Xinbing Wang', 'Chenghu Zhou', 'Jing Zhang']","['Shanghai Jiao Tong University', 'Department of Electronic engineering, Shanghai Jiaotong University', 'School of Oceanography, Shanghai Jiaotong University', 'Electronic Engineering, Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'IGSNRR, Chinese Academy of Sciences, Beijing, China', 'East China Normal University']",
https://openreview.net/forum?id=6PTiCmGcNx,Security,Contamination-Resilient Anomaly Detection via Adversarial Learning on Partially-Observed Normal and Anomalous Data,"Many existing anomaly detection methods assume the availability of a large-scale normal dataset. But for many applications, limited by resources, removing all anomalous samples from a large un-labeled dataset is unrealistic, resulting in contaminated datasets. To detect anomalies accurately under such scenarios, from the probabilistic perspective, the key question becomes how to learn the normal-data distribution from a contaminated dataset. To this end, we propose to collect two additional small datasets that are comprised of partially-observed normal and anomaly samples, and then use them to help learn the distribution under an adversarial learning scheme. We prove that under some mild conditions, the proposed method is able to learn the correct normal-data distribution. Then, we consider the overfitting issue caused by the small size of the two additional datasets, and a correctness-guaranteed flipping mechanism is further developed to alleviate it. Theoretical results under incomplete observed anomaly types are also presented. Extensive experimental results demonstrate that our method outperforms representative baselines when detecting anomalies under contaminated datasets.",[],,"['Wenxi Lv', 'Qinliang Su', 'Hai Wan', 'Hongteng Xu', 'Wenchao Xu']","['School of computer science and engineering, Sun Yat-Sen University', 'School of Computer Science and Engineering, SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY', 'Gaoling School of Artificial Intelligence, Renmin University of China', '']",
https://openreview.net/forum?id=vxPmrxKe0J,Fairness & Bias,On the Hardness of Probabilistic Neurosymbolic Learning,"The limitations of purely neural learning have sparked an interest in probabilistic neurosymbolic models, which combine neural networks with probabilistic logical reasoning. As these neurosymbolic models are trained with gradient descent, we study the complexity of differentiating probabilistic reasoning. We prove that although approximating these gradients is intractable in general, it becomes tractable during training. Furthermore, we introduce *WeightME*, an unbiased gradient estimator based on model sampling. Under mild assumptions, WeightME approximates the gradient with probabilistic guarantees using a logarithmic number of calls to a SAT solver. Lastly, we evaluate the necessity of these guarantees on the gradient. Our experiments indicate that the existing biased approximations indeed struggle to optimize even when exact solving is still feasible.",[],,"['Jaron Maene', 'Vincent Derkinderen', 'Luc De Raedt']","['', 'Department of Computer Science, KU Leuven', 'KU Leuven']",
https://openreview.net/forum?id=SPygKwms0X,Security,A Provable Decision Rule for Out-of-Distribution Detection,"Out-of-distribution (OOD) detection task plays the key role in reliable and safety-critical applications. Existing researches mainly devote to designing or training the powerful score function but overlook investigating the decision rule based on the proposed score function. Different from previous work, this paper aims to design a decision rule with rigorous theoretical guarantee and well empirical performance. Specifically, we provide a new insight for the OOD detection task from a hypothesis testing perspective and propose a novel generalized Benjamini Hochberg (g-BH) procedure with empirical p-values to solve the testing problem. Theoretically, the g-BH procedure controls false discovery rate (FDR) at pre-specified level. Furthermore, we derive an upper bound of the expectation of false positive rate (FPR) for the g-BH procedure based on the tailed generalized Gaussian distribution family, indicating that the FPR of g-BH procedure converges to zero in probability. Finally, the extensive experimental results verify the superiority of g-BH procedure over the traditional threshold-based decision rule on several OOD detection benchmarks.",[],,"['Xinsong Ma', 'Xin Zou', 'Weiwei Liu']","['', 'Wuhan University', 'Wuhan University']",
https://openreview.net/forum?id=bZ4fzw1iz7,Privacy & Data Governance,Split-and-Denoise: Protect large language model inference with local differential privacy,"Large Language Models (LLMs) excel in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the risk of privacy leakage due to direct text transmission to servers remains a critical concern. To address this, we introduce Split-N-Denoise (SnD), an private inference framework that splits the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD’s effectiveness in optimizing the privacy-utility tradeoff across various LLM architectures and diverse downstream tasks. The results reveal an improvement in performance under the same privacy budget compared to the baselines by over 10% on average, offering clients a privacy-preserving solution for local privacy protection.",[],,"['Peihua Mai', 'Ran Yan', 'Zhe Huang', 'Youjia Yang', 'Yan Pang']","['National University of Singapore', 'National University of Singapore', 'North China Electric Power University', 'Modern Logistics, National University of Singapore Chongqing Research Institute', 'Analytics and Operations, National University of Singapore']",
https://openreview.net/forum?id=G8zDeKOp0R,Fairness & Bias,SCoRe: Submodular Combinatorial Representation Learning,"In this paper we introduce the **SCoRe** (**S**ubmodular **Co**mbinatorial **Re**presentation Learning) framework, a novel approach in representation learning that addresses inter-class bias and intra-class variance. SCoRe provides a new combinatorial viewpoint to representation learning, by introducing a family of loss functions based on set-based submodular information measures. We develop two novel combinatorial formulations for loss functions, using the *Total Information* and *Total Correlation*, that naturally minimize intra-class variance and inter-class bias. Several commonly used metric/contrastive learning loss functions like supervised contrastive loss, orthogonal projection loss, and N-pairs loss, are all instances of SCoRe, thereby underlining the versatility and applicability of SCoRe in a broad spectrum of learning scenarios. Novel objectives in SCoRe naturally model class-imbalance with up to 7.6% improvement in classification on CIFAR-10-LT, CIFAR-100-LT, MedMNIST, 2.1% on ImageNet-LT, and 19.4% in object detection on IDD and LVIS (v1.0), demonstrating its effectiveness over existing approaches.",[],,"['Anay Majee', 'Suraj Nandkishor Kothawade', 'Krishnateja Killamsetty', 'Rishabh K Iyer']","['Computer Science, The University of Texas at Dallas', 'Google', 'Research, International Business Machines', 'Computer Science, University of Texas, Dallas']",
https://openreview.net/forum?id=sHtIStlg0v,Fairness & Bias,Large Language Models are Geographically Biased,"Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman’s $\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM.",[],,"['Rohin Manvi', 'Samar Khanna', 'Marshall Burke', 'David B. Lobell', 'Stefano Ermon']","['Stanford University', 'Computer Science Department, Stanford University', 'Stanford University', 'Earth System Science, Stanford University', 'Computer Science, Stanford University']",
https://openreview.net/forum?id=fowZNENcVJ,Transparency & Explainability,Using AI Uncertainty Quantification to Improve Human Decision-Making,"AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional probabilistic information to users. The majority of past research on AI and human decision-making has concentrated on model explainability and interpretability, with little focus on understanding the potential impact of UQ on human decision-making. We evaluated the impact on human decision-making for instance-level UQ, calibrated using a strict scoring rule, in two online behavioral experiments. In the first experiment, our results showed that UQ was beneficial for decision-making performance compared to only AI predictions. In the second experiment, we found UQ had generalizable benefits for decision-making across a variety of representations for probabilistic information. These results indicate that implementing high quality, instance-level UQ for AI may improve decision-making with real systems compared to AI predictions alone.",[],,"['Laura Marusich', 'Jonathan Bakdash', 'Yan Zhou', 'Murat Kantarcioglu']","['US DEVCOM Army Research Laboratory', 'Agency for Healthcare Research and Quality', 'University of Texas, Dallas', 'Computer Science, Virginia Polytechnic Institute and State University']",
https://openreview.net/forum?id=FzyMdAm2fZ,Privacy & Data Governance,Delving into Differentially Private Transformer,"Deep learning with differential privacy (DP) has garnered significant attention over the past years, leading to the development of numerous methods aimed at enhancing model accuracy and training efficiency. This paper delves into the problem of training Transformer models with differential privacy. Our treatment is modular: the logic is to 'reduce' the problem of training DP Transformer to the more basic problem of training DP vanilla neural nets. The latter is better understood and amenable to many model-agnostic methods. Such 'reduction' is done by first identifying the hardness unique to DP Transformer training: the attention distraction phenomenon and a lack of compatibility with existing techniques for efficient gradient clipping. To deal with these two issues, we propose the Re-Attention Mechanism and Phantom Clipping, respectively. We believe that our work not only casts new light on training DP Transformers but also promotes a modular treatment to advance research in the field of differentially private deep learning.",[],,"['Youlong Ding', 'Xueyang Wu', 'Yining meng', 'Yonggang Luo', 'Hao Wang', 'Weike Pan']","['', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology', 'Chongqing University of Posts and Telecommunications, Deep Instinct', 'Purdue University', 'Rutgers University', '']",
https://openreview.net/forum?id=htq0FbPOsY,Transparency & Explainability,On the Tractability of SHAP Explanations under Markovian Distributions,"Thanks to its solid theoretical foundation, the SHAP framework is arguably one the most widely utilized frameworks for local explainability of ML models. Despite its popularity, its exact computation is known to be very challenging, proven to be NP-Hard in various configurations. Recent works have unveiled positive complexity results regarding the computation of the SHAP score for specific model families, encompassing decision trees, random forests, and some classes of boolean circuits. Yet, all these positive results hinge on the assumption of feature independence, often simplistic in real-world scenarios. In this article, we investigate the computational complexity of the SHAP score by relaxing this assumption and introducing a Markovian perspective. We show that, under the Markovian assumption, computing the SHAP score for the class of Weighted automata, Disjoint DNFs and Decision Trees can be performed in polynomial time, offering a first positive complexity result for the problem of SHAP score computation that transcends the limitations of the feature independence assumption.",[],,"['Reda Marzouk', 'Colin de La Higuera']",['Université de Nantes'],
https://openreview.net/forum?id=zrQIc9mQQN,Fairness & Bias,Rethinking Independent Cross-Entropy Loss For Graph-Structured Data,"Graph neural networks (GNNs) have exhibited prominent performance in learning graph-structured data. Considering node classification task, based on the i.i.d assumption among node labels, the traditional supervised learning simply sums up cross-entropy losses of the independent training nodes and applies the average loss to optimize GNNs' weights. But different from other data formats, the nodes are naturally connected. It is found that the independent distribution modeling of node labels restricts GNNs' capability to generalize over the entire graph and defend adversarial attacks. In this work, we propose a new framework, termed joint-cluster supervised learning, to model the joint distribution of each node with its corresponding cluster. We learn the joint distribution of node and cluster labels conditioned on their representations, and train GNNs with the obtained joint loss. In this way, the data-label reference signals extracted from the local cluster explicitly strengthen the discrimination ability on the target node. The extensive experiments demonstrate that our joint-cluster supervised learning can effectively bolster GNNs' node classification accuracy. Furthermore, being benefited from the reference signals which may be free from spiteful interference, our learning paradigm significantly protects the node classification from being affected by the adversarial attack.",[],,"['Rui Miao', 'Kaixiong Zhou', 'Yili Wang', 'Ninghao Liu', 'Ying Wang', 'Xin Wang']","['Jilin University', 'Electrical and Computer Engineering, North Carolina State University', 'Jilin University', 'University of Georgia', 'Jilin University', 'Jilin University']",
https://openreview.net/forum?id=vJx6fld6l0,Fairness & Bias,Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics,"This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR & AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (**HEPT**), which combines E$^2$LSH with OR & AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT.",[],,"['Siqi Miao', 'Zhiyuan Lu', 'Mia Liu', 'Javier Duarte', 'Pan Li']","['Georgia Institute of Technology', 'None', 'Purdue University', 'Physics Department, University of California, San Diego', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=GYGkt2M8ee,Fairness & Bias,Can Implicit Bias Imply Adversarial Robustness?,"The implicit bias of gradient-based training algorithms has been considered mostly beneficial as it leads to trained networks that often generalize well. However, Frei et al. (2023) show that such implicit bias can harm adversarial robustness. Specifically, they show that if the data consists of clusters with small inter-cluster correlation, a shallow (two-layer) ReLU network trained by gradient flow generalizes well, but it is not robust to adversarial attacks of small radius. Moreover, this phenomenon occurs despite the existence of a much more robust classifier that can be explicitly constructed from a shallow network. In this paper, we extend recent analyses of neuron alignment to show that a shallow network with a polynomial ReLU activation (pReLU) trained by gradient flow not only generalizes well but is also robust to adversarial attacks. Our results highlight the importance of the interplay between data structure and architecture design in the implicit bias and robustness of trained networks.",[],,"['Hancheng Min', 'Rene Vidal']","['Electrical and Systems Engineering, University of Pennsylvania', 'University of Pennsylvania']",
https://openreview.net/forum?id=duyl8sy8qV,Fairness & Bias,Slot Abstractors: Toward Scalable Abstract Visual Reasoning,"Abstract visual reasoning is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual reasoning tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual reasoning problems containing a large number of objects. Other recent work proposed Abstractors, an extension of Transformers that incorporates strong relational inductive biases, thereby inheriting the Transformer's scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to multi-object visual inputs. Here we combine the strengths of the above approaches and propose Slot Abstractors, an approach to abstract visual reasoning that can be scaled to problems involving a large number of objects and multiple relations among them. The approach displays state-of-the-art performance across four abstract visual reasoning tasks, as well as an abstract reasoning task involving real-world images.",[],,"['Shanka Subhra Mondal', 'Jonathan D. Cohen', 'Taylor Whittington Webb']","['Princeton University', 'Princeton University', 'Microsoft Research, Research, Microsoft']",
https://openreview.net/forum?id=lpHjmPvxW1,Security,TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors,"Diffusion models have achieved notable success in image generation, but they remain highly vulnerable to backdoor attacks, which compromise their integrity by producing specific undesirable outputs when presented with a pre-defined trigger. In this paper, we investigate how to protect diffusion models from this dangerous threat. Specifically, we propose **TERD**, a backdoor defense framework that builds unified modeling for current attacks, which enables us to derive an accessible reversed loss. A trigger reversion strategy is further employed: an initial approximation of the trigger through noise sampled from a prior distribution, followed by refinement through differential multi-step samplers. Additionally, with the reversed trigger, we propose backdoor detection from the noise space, introducing the first backdoor input detection approach for diffusion models and a novel model detection algorithm that calculates the KL divergence between reversed and benign distributions. Extensive evaluations demonstrate that TERD secures a 100% True Positive Rate (TPR) and True Negative Rate (TNR) across datasets of varying resolutions. TERD also demonstrates nice adaptability to other Stochastic Differential Equation (SDE)-based models. Our code is available at https://github.com/PKU-ML/TERD.",[],,"['Yichuan Mo', 'Hui Huang', 'Mingjie Li', 'Ang Li', 'Yisen Wang']","['School of Intelligence Science and Technology, Peking University', 'Peking University, Peking University', 'CISPA Helmholtz Center for Information Security', 'Peking University', 'Peking University']",
https://openreview.net/forum?id=SL6V527p1F,Transparency & Explainability,Causal Representation Learning Made Identifiable by Grouping of Observational Variables,"A topic of great current interest is Causal Representation Learning (CRL), whose goal is to learn a causal model for hidden features in a data-driven manner. Unfortunately, CRL is severely ill-posed since it is a combination of the two notoriously ill-posed problems of representation learning and causal discovery. Yet, finding practical identifiability conditions that guarantee a unique solution is crucial for its practical applicability. Most approaches so far have been based on assumptions on the latent causal mechanisms, such as temporal causality, or existence of supervision or interventions; these can be too restrictive in actual applications. Here, we show identifiability based on novel, weak constraints, which requires no temporal structure, intervention, nor weak supervision. The approach is based on assuming the observational mixing exhibits a suitable grouping of the observational variables. We also propose a novel self-supervised estimation framework consistent with the model, prove its statistical consistency, and experimentally show its superior CRL performances compared to the state-of-the-art baselines. We further demonstrate its robustness against latent confounders and causal cycles.",[],,"['Hiroshi Morioka', 'Aapo Hyvarinen']","['RIKEN', 'University of Helsinki']",
https://openreview.net/forum?id=hrWte3nlzr,Security,Truly No-Regret Learning in Constrained MDPs,"Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for *error cancellations* --- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal-dual algorithm to learn in an unknown CMDP. We prove that our algorithm achieves sublinear regret without error cancellations.",[],,"['Adrian Müller', 'Pragnya Alatur', 'Volkan Cevher', 'Giorgia Ramponi', 'Niao He']","['ETHZ - ETH Zurich', 'Department of Computer Science, ETHZ - ETH Zurich', 'EPFL - EPF Lausanne', 'Department of Informatics, University of Zurich, University of Zurich', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=ABt0jlLZtX,Security,Learning Optimal Deterministic Policies with Stochastic Policy Gradients,"Policy gradient (PG) methods are successful approaches to deal with continuous reinforcement learning (RL) problems. They learn stochastic parametric (hyper)policies by either exploring in the space of actions or in the space of parameters. Stochastic controllers, however, are often undesirable from a practical perspective because of their lack of robustness, safety, and traceability. In common practice, stochastic (hyper)policies are learned only to deploy their deterministic version. In this paper, we make a step towards the theoretical understanding of this practice. After introducing a novel framework for modeling this scenario, we study the global convergence to the best deterministic policy, under (weak) gradient domination assumptions. Then, we illustrate how to tune the exploration level used for learning to optimize the trade-off between the sample complexity and the performance of the deployed deterministic policy. Finally, we quantitatively compare action-based and parameter-based exploration, giving a formal guise to intuitive results.",[],,"['Alessandro Montenegro', 'Marco Mussi', 'Alberto Maria Metelli', 'Matteo Papini']","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano', 'Polytechnic Institute of Milan']",
https://openreview.net/forum?id=MIRQ3L8vtn,Privacy & Data Governance,Differentially private exact recovery for stochastic block models,"Stochastic block models (SBMs) are a very commonly studied network model for community detection algorithms. In the standard form of an SBM, the $n$ vertices (or nodes) of a graph are generally divided into multiple pre-determined communities (or clusters). Connections between pairs of vertices are generated randomly and independently with pre-defined probabilities, which depend on the communities containing the two nodes. A fundamental problem in SBMs is the recovery of the community structure, and sharp information-theoretic bounds are known for recoverability for many versions of SBMs. Our focus here is the recoverability problem in SBMs when the network is private. Under the edge differential privacy model, we derive conditions for exact recoverability in three different versions of SBMs, namely Asymmetric SBM (when communities have non-uniform sizes), General Structure SBM (with outliers), and Censored SBM (with edge features). Our private algorithms have polynomial running time w.r.t. the input graph's size, and match the recovery thresholds of the non-private setting when $\epsilon\rightarrow\infty$. In contrast, the previous best results for recoverability in SBMs only hold for the symmetric case (equal size communities), and run in quasi-polynomial time, or in polynomial time with recovery thresholds being tight up to some constants from the non-private settings.",[],,"['Dung Nguyen', 'Anil Kumar Vullikanti']","['', 'Computer Science, University of Virginia']",
https://openreview.net/forum?id=XGq30hC5MW,Security,Risk-Sensitive Reward-Free Reinforcement Learning with CVaR,"Exploration is a crucial phase in reinforcement learning (RL). The reward-free RL paradigm, as explored by (Jin et al., 2020), offers an efficient method to design exploration algorithms for risk-neutral RL across various reward functions with a single exploration phase. However, as RL applications in safety critical settings grow, there's an increasing need for risk-sensitive RL, which considers potential risks in decision-making. Yet, efficient exploration strategies for risk-sensitive RL remain underdeveloped. This study presents a novel risk-sensitive reward-free framework based on Conditional Value-at-Risk (CVaR), designed to effectively address CVaR RL for any given reward function through a single exploration phase. We introduce the CVaR-RF-UCRL algorithm, which is shown to be $(\epsilon,p)$-PAC, with a sample complexity upper bounded by $\tilde{\mathcal{O}}\left(\frac{S^2AH^4}{\epsilon^2\tau^2}\right)$ with $\tau$ being the risk tolerance parameter. We also prove a $\Omega\left(\frac{S^2AH^2}{\epsilon^2\tau}\right)$ lower bound for any CVaR-RF exploration algorithm, demonstrating the near-optimality of our algorithm. Additionally, we propose the planning algorithms: CVaR-VI and its more practical variant, CVaR-VI-DISC. The effectiveness and practicality of our CVaR reward-free approach are further validated through numerical experiments.",[],,"['Xinyi Ni', 'Guanlin Liu', 'Lifeng Lai']","['Electrical and Computer Engineering, University of California, Davis', 'ByteDance Inc.', 'University of California, Davis']",
https://openreview.net/forum?id=jQA5iutPzd,Security,The Perception-Robustness Tradeoff in Deterministic Image Restoration,"We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods.",[],,"['Guy Ohayon', 'Tomer Michaeli', 'Michael Elad']","['Computer Science, Technion - Israel Institute of Technology', 'Electrical and Computer Engineering, Technion, Technion', 'Computer Science Department, Technion - Israel Institute of Technology']",
https://openreview.net/forum?id=k1JXxbpIY6,Fairness & Bias,Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?,"There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.",[],,"['Andreas Opedal', 'Alessandro Stolfo', 'Haruki Shirakami', 'Ying Jiao', 'Ryan Cotterell', 'Bernhard Schölkopf', 'Abulhair Saparov', 'Mrinmaya Sachan']","['Department of Computer Science, ETHZ - ETH Zurich', 'ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', '', 'Computer Science, Purdue University', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=DsVzHj7jcA,Fairness & Bias,Stability and Generalization for Stochastic Recursive Momentum-based Algorithms for (Strongly-)Convex One to $K$-Level Stochastic Optimizations,"STOchastic Recursive Momentum (STORM)-based algorithms have been widely developed to solve one to $K$-level ($K \geq 3$) stochastic optimization problems. Specifically, they use estimators to mitigate the biased gradient issue and achieve near-optimal convergence results. However, there is relatively little work on understanding their generalization performance, particularly evident during the transition from one to $K$-level optimization contexts. This paper provides a comprehensive generalization analysis of three representative STORM-based algorithms: STORM, COVER, and SVMR, for one, two, and $K$-level stochastic optimizations under both convex and strongly convex settings based on algorithmic stability. Firstly, we define stability for $K$-level optimizations and link it to generalization. Then, we detail the stability results for three prominent STORM-based algorithms. Finally, we derive their excess risk bounds by balancing stability results with optimization errors. Our theoretical results provide strong evidence to complete STORM-based algorithms: (1) Each estimator may decrease their stability due to variance with its estimation target. (2) Every additional level might escalate the generalization error, influenced by the stability and the variance between its cumulative stochastic gradient and the true gradient. (3) Increasing the batch size for the initial computation of estimators presents a favorable trade-off, enhancing the generalization performance.",[],,"['Xiaokang Pan', 'Xingyu Li', 'Jin Liu', 'Tao Sun', 'Kai Sun', 'Lixing Chen', 'Zhe Qu']","['Central South University', 'Generative AI, Oracle', '', 'National University of Defense Technology', ""Xi'an Jiaotong University"", 'School of Electronic Information and Electrical Engineering , Shanghai Jiaotong University', 'Computer Science and Engineering, Central South University']",
https://openreview.net/forum?id=zEqeNEuiJr,Security,SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding,"Distributed learning is an effective approach to accelerate model training by using parallel computing power of multiple workers. However, substantial communication delays arise between workers and a parameter server due to the massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through sign quantization, but its convergence rate significantly decreases when adversarial workers arbitrarily manipulate datasets or local gradient updates. In this paper, we consider a distributed learning problem where the workforce comprises a mixture of honest and adversarial workers. In this setting, we show that the convergence rate can remain invariant as long as the number of honest workers providing trustworthy local updates to the parameter server exceeds the number of adversarial workers. The key idea behind this counter-intuitive result is our novel aggregation method, signSGD with federated defense (signSGD-FD). Unlike traditional approaches, signSGD-FD utilizes the gradient information sent by adversarial workers with appropriate weights, obtained through gradient sign decoding. Experimental results demonstrate that signSGD-FD achieves superior convergence rates compared to traditional algorithms in various adversarial attack scenarios.",[],,"['Chanho Park', 'Namyoon Lee']","['Pohang University of Science and Technology', 'Korea University']",
https://openreview.net/forum?id=TfWKkSAziC,Fairness & Bias,LPGD: A General Framework for Backpropagation through Embedded Optimization Layers,"Embedding parameterized optimization problems as layers into machine learning architectures serves as a powerful inductive bias. Training such architectures with stochastic gradient descent requires care, as degenerate derivatives of the embedded optimization problem often render the gradients uninformative. We propose Lagrangian Proximal Gradient Descent (LPGD), a flexible framework for training architectures with embedded optimization layers that seamlessly integrates into automatic differentiation libraries. LPGD efficiently computes meaningful replacements of the degenerate optimization layer derivatives by re-running the forward solver oracle on a perturbed input. LPGD captures various previously proposed methods as special cases, while fostering deep links to traditional optimization methods. We theoretically analyze our method and demonstrate on historical and synthetic data that LPGD converges faster than gradient descent even in a differentiable setup.",[],,"['Anselm Paulus', 'Georg Martius', 'Vít Musil']","['Eberhard-Karls-Universität Tübingen', 'Eberhard-Karls-Universität Tübingen', 'Masaryk University']",
https://openreview.net/forum?id=znz261CQK7,Fairness & Bias,Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks,"Neural networks typically generalize well when fitting the data perfectly, even though they are heavily overparameterized. Many factors have been pointed out as the reason for this phenomenon, including an implicit bias of stochastic gradient descent (SGD) and a possible simplicity bias arising from the neural network architecture. The goal of this paper is to disentangle the factors that influence generalization stemming from optimization and architectural choices by studying *random* and *SGD-optimized* networks that achieve zero training error. We experimentally show, in the low sample regime, that overparameterization in terms of increasing width is beneficial for generalization, and this benefit is due to the bias of SGD and not due to an architectural bias. In contrast, for increasing depth, overparameterization is detrimental for generalization, but random and SGD-optimized networks behave similarly, so this can be attributed to an architectural bias.",[],,"['Amit Peleg', 'Matthias Hein']","['Eberhard-Karls-Universität Tübingen', 'University of Tübingen']",
https://openreview.net/forum?id=KU9mn6deDR,Security,UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers,"Text-to-Image (T2I) models have raised security concerns due to their potential to generate inappropriate or harmful images. In this paper, we propose UPAM, a novel framework that investigates the robustness of T2I models from the attack perspective. Unlike most existing attack methods that focus on deceiving textual defenses, UPAM aims to deceive both textual and visual defenses in T2I models. UPAM enables gradient-based optimization, offering greater effectiveness and efficiency than previous methods. Given that T2I models might not return results due to defense mechanisms, we introduce a Sphere-Probing Learning (SPL) scheme to support gradient optimization even when no results are returned. Additionally, we devise a Semantic-Enhancing Learning (SEL) scheme to finetune UPAM for generating target-aligned images. Our framework also ensures attack stealthiness. Extensive experiments demonstrate UPAM's effectiveness and efficiency.",[],,"['Duo Peng', 'Qiuhong Ke', 'Jun Liu']","['ISTD, Singapore University of Technology and Design', 'DSAI, Monash University', 'Lancaster University']",
https://openreview.net/forum?id=pLtuwhoQh7,Transparency & Explainability,Mechanistic Neural Networks for Scientific Machine Learning,"This paper presents *Mechanistic Neural Networks*, a neural network design for machine learning applications in the sciences. It incorporates a new *Mechanistic Block* in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel *Relaxed Linear Programming Solver* (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scientific data across various applications, showing significant performance against specialized state-of-the-art methods. Source code is available at https://github.com/alpz/mech-nn.",[],,"['Adeel Pervez', 'Francesco Locatello', 'Stratis Gavves']","['Institute of Science and Technology Austria', 'Institute of Science and Technology', 'University of Amsterdam']",
https://openreview.net/forum?id=jr0W36wOBx,Fairness & Bias,Conformalized Survival Distributions: A Generic Post-Process to Increase Calibration,"Discrimination and calibration represent two important properties of survival analysis, with the former assessing the model's ability to accurately rank subjects and the latter evaluating the alignment of predicted outcomes with actual events. With their distinct nature, it is hard for survival models to simultaneously optimize both of them especially as many previous results found improving calibration tends to diminish discrimination performance. This paper introduces a novel approach utilizing *conformal regression* that can improve a model's calibration without degrading discrimination. We provide theoretical guarantees for the above claim, and rigorously validate the efficiency of our approach across 11 real-world datasets, showcasing its practical applicability and robustness in diverse scenarios.",[],,"['Shi-ang Qi', 'Yakun Yu', 'Russell Greiner']","['Computing Science, University of Alberta', 'University of Alberta', 'University of Alberta']",
https://openreview.net/forum?id=cit0hg4sEz,Privacy & Data Governance,Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes,"Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioritizing perturbations with greater impact on model accuracy. Experiments across six scenarios with various LLMs, datasets and data partitions demonstrate that our approach outperforms existing federated LLM fine-tuning methods in both communication efficiency and zero-shot generalization.",[],,"['Zhen Qin', 'Daoyuan Chen', 'Bingchen Qian', 'Bolin Ding', 'Yaliang Li', 'Shuiguang Deng']","['Zhejiang University', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'College of Computer Science and Technology, Zhejiang University']",
https://openreview.net/forum?id=ks8qSwkkuZ,Security,Feasible Reachable Policy Iteration,"The goal-reaching tasks with safety constraints are common control problems in real world, such as intelligent driving and robot manipulation. The difficulty of this kind of problem comes from the exploration termination caused by safety constraints and the sparse rewards caused by goals. The existing safe RL avoids unsafe exploration by restricting the search space to a feasible region, the essence of which is the pruning of the search space. However, there are still many ineffective explorations in the feasible region because of the ignorance of the goals. Our approach considers both safety and goals; the policy space pruning is achieved by a function called feasible reachable function, which describes whether there is a policy to make the agent safely reach the goals in the finite time domain. This function naturally satisfies the self-consistent condition and the risky Bellman equation, which can be solved by the fixed point iteration method. On this basis, we propose feasible reachable policy iteration (FRPI), which is divided into three steps: policy evaluation, region expansion, and policy improvement. In the region expansion step, by using the information of agent to reach the goals, the convergence of the feasible region is accelerated, and simultaneously a smaller feasible reachable region is identified. The experimental results verify the effectiveness of the proposed FR function in both improving the convergence speed of better or comparable performance without sacrificing safety and identifying a smaller policy space with higher sample efficiency.",[],,"['Shentao Qin', 'Yujie Yang', 'Yao Mu', 'JIE LI', 'Wenjun Zou', 'Jingliang Duan', 'Shengbo Eben Li']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Computer Science, The University of Hong Kong', 'School of Vehicle and Mobility, Tsinghua University', 'Tsinghua University, Tsinghua University', 'School of Mechanical Engineering, University of Science and Technology Beijing', 'Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=Ax90jQPbgF,Fairness & Bias,Learning Constraints from Offline Demonstrations via Superior Distribution Correction Estimation,"An effective approach for learning both safety constraints and control policies is Inverse Constrained Reinforcement Learning (ICRL). Previous ICRL algorithms commonly employ an online learning framework that permits unlimited sampling from an interactive environment. This setting, however, is infeasible in many realistic applications where data collection is dangerous and expensive. To address this challenge, we propose Inverse Constrained Superior Distribution Correction Estimation (ICSDICE) as an offline ICRL solver. ICSDICE extracts feasible constraints from superior distributions, thereby highlighting policies with expert-exceeding rewards maximization ability. To estimate these distributions, ICSDICE solves a regularized dual optimization problem for safe control by exploiting the observed reward signals and expert preferences. Striving for transferable constraints and unbiased estimations, ICSDICE actively encourages sparsity and incorporates a discounting effect within the learned and observed distributions. Empirical studies show that ICSDICE outperforms other baselines by accurately recovering the constraints and adapting to high-dimensional environments. The code is available at https://github.com/quangr/ICSDICE.",[],,"['Guorui Quan', 'zhiqiang xu', 'Guiliang Liu']","['The Chinese University of Hong Kong, Shenzhen', 'Mohamed bin Zayed University of Artificial Intelligence', 'The Chinese University of Hong Kong, Shenzhen']",
https://openreview.net/forum?id=Wj5wm3Os5v,Fairness & Bias,Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion,"While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models capture the rich multimodal structures and dynamics from video and text jointly? Or are they achieving high scores by exploiting biases and spurious features? Hence, to provide insights, we design *QUAG* (QUadrant AveraGe), a lightweight and non-parametric probe, to conduct dataset-model combined representation analysis by impairing modality fusion. We find that the models achieve high performance on many datasets without leveraging multimodal representations. To validate QUAG further, we design *QUAG-attention*, a less-expressive replacement of self-attention with restricted token interactions. Models with QUAG-attention achieve similar performance with significantly fewer multiplication operations without any finetuning. Our findings raise doubts about the current models' abilities to learn highly-coupled multimodal representations. Hence, we design the *CLAVI* (Complements in LAnguage and VIdeo) dataset, a stress-test dataset curated by augmenting real-world videos to have high modality coupling. Consistent with the findings of QUAG, we find that most of the models achieve near-trivial performance on CLAVI. This reasserts the limitations of current models for learning highly-coupled multimodal representations, that is not evaluated by the current datasets.",[],,"['Ishaan Singh Rawal', 'Alexander Matyasko', 'Shantanu Jaiswal', 'Basura Fernando', 'Cheston Tan']","['Centre For Frontier AI Research, A*STAR. Singapore', 'Singapore Institute of Technology', 'Carnegie Mellon University', 'Nanyang Technological University', 'A*STAR']",
https://openreview.net/forum?id=L1eJ3NKPCd,Fairness & Bias,"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks","Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing simple logical operations. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we aim to assess in this paper “how capable can a transformer become?”. Specifically, we train autoregressive Transformer models on a data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) composing functions by generating intermediate outputs is more effective at generalizing to unseen compositions, compared to generating no intermediate outputs; (3) biases in the order of the compositions in the training data, results in Transformers that fail to compose some combinations of functions; and (4) the attention layers seem to select the capability to apply while the feed-forward layers execute the capability.",[],,"['Rahul Ramesh', 'Ekdeep Singh Lubana', 'Mikail Khona', 'Robert P. Dick', 'Hidenori Tanaka']","['University of Pennsylvania', 'Center for Brain Science, Harvard University, Harvard University', 'Massachusetts Institute of Technology', 'Electrical Engineering and Computer Science, University of Michigan', 'Harvard University, Harvard University']",
https://openreview.net/forum?id=4dxR7awO5n,Privacy & Data Governance,"Unveiling Privacy, Memorization, and Input Curvature Links","Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be $\sim3$ orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. First, we derive an upper bound on memorization characterized by both differential privacy and input loss curvature. Secondly, we present a novel insight showing that input loss curvature is upper-bounded by the differential privacy parameter. Our theoretical findings are further validated using deep models on CIFAR and ImageNet datasets, showing a strong correlation between our theoretical predictions and results observed in practice.",[],,"['Deepak Ravikumar', 'Efstathia Soufleri', 'Abolfazl Hashemi', 'Kaushik Roy']","['', 'Athena Research and Innovation Centre', 'ECE, Purdue University', 'ECE, Purdue University']",
https://openreview.net/forum?id=qklMNNub0H,Transparency & Explainability,Implicit Regularization in Feedback Alignment Learning Mechanisms for Neural Networks,"Feedback Alignment (FA) methods are biologically inspired local learning rules for training neural networks with reduced communication between layers. While FA has potential applications in distributed and privacy-aware ML, limitations in multi-class classification and lack of theoretical understanding of the alignment mechanism have constrained its impact. This study introduces a unified framework elucidating the operational principles behind alignment in FA. Our key contributions include: (1) a novel conservation law linking changes in synaptic weights to implicit regularization that maintains alignment with the gradient, with support from experiments, (2) sufficient conditions for convergence based on the concept of alignment dominance, and (3) empirical analysis showing better alignment can enhance FA performance on complex multi-class tasks. Overall, these theoretical and practical advancements improve interpretability of bio-plausible learning rules and provide groundwork for developing enhanced FA algorithms.",[],,"['Zachary Robertson', 'Sanmi Koyejo']","['Stanford University', 'Virtue AI']",
https://openreview.net/forum?id=vBJZ93tvoE,Fairness & Bias,Modelling Microbial Communities with Graph Neural Networks,"Understanding the interactions and interplay of microorganisms is a great challenge with many applications in medical and environmental settings. In this work, we model bacterial communities directly from their genomes using graph neural networks (GNNs). GNNs leverage the inductive bias induced by the set nature of bacteria, enforcing permutation invariance and granting combinatorial generalization. We propose to learn the dynamics implicitly by directly predicting community relative abundance profiles at steady state, thus escaping the need for growth curves. On two real-world datasets, we show for the first time generalization to unseen bacteria and different community structures. To investigate the prediction results more deeply, we create a simulation for flexible data generation and analyze effects of bacteria interaction strength, community size, and training data amount.",[],,"['Albane Ruaud', 'Cansu Sancaktar', 'Marco Bagatella', 'Christoph Ratzke', 'Georg Martius']","['Eberhard-Karls-Universität Tübingen', 'Computer Science, Eberhard-Karls-Universität Tübingen', 'Max Planck Institute for Intelligent Systems, Max Planck Institute for Intelligent Systems', 'Eberhard-Karls-Universität Tübingen', 'Eberhard-Karls-Universität Tübingen']",
https://openreview.net/forum?id=p9SMltcfsu,Fairness & Bias,Generalizing Orthogonalization for Models with Non-Linearities,"The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient's X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the ""orthogonalization"" or ""normalization"" of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method's effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.",[],,"['David Rügamer', 'Chris Kolb', 'Tobias Weber', 'Lucas Kook', 'Thomas Nagler']","['Statistics, Ludwig-Maximilians-Universität München', '', 'Department for Statistics', '', 'Ludwig-Maximilians-Universität München']",
https://openreview.net/forum?id=Bic3Vmy2DG,Transparency & Explainability,Proactive Detection of Voice Cloning with Localized Watermarking,"In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator / detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed, achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.Code is available at https://github.com/facebookresearch/audioseal",[],,"['Robin San Roman', 'Pierre Fernandez', 'Hady Elsahar', 'Alexandre Défossez', 'Teddy Furon', 'Tuan Tran']","['INRIA', 'Meta', 'Facebook', 'Kyutai', 'INRIA', 'Facebook']",
https://openreview.net/forum?id=WLPhywf1si,Security,Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models,"Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available on GitHub.",[],,"['Christian Schlarmann', 'Naman Deep Singh', 'Francesco Croce', 'Matthias Hein']","['', 'Eberhard-Karls-Universität Tübingen', 'EPFL - EPF Lausanne', 'University of Tübingen']",
https://openreview.net/forum?id=mDw42ZanmE,Transparency & Explainability,A Multimodal Automated Interpretability Agent,"This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA’s ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.",[],,"['Tamar Rott Shaham', 'Sarah Schwettmann', 'Franklin Wang', 'Achyuta Rajaram', 'Evan Hernandez', 'Jacob Andreas', 'Antonio Torralba']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'CSAIL, Massachusetts Institute of Technology', 'EECS, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'EECS, Massachusetts Institute of Technology']",
https://openreview.net/forum?id=fdroxYsgzQ,Fairness & Bias,Prompting is a Double-Edged Sword: Improving Worst-Group Robustness of Foundation Models,"Machine learning models fail catastrophically under distribution shift, but a surprisingly effective way to empirically improve robustness to some types of shift (*e.g.*, Imagenet-A/C) is to use stronger open-vocabulary classifiers derived from foundation models. In this work, we first note that for shifts governed by spurious correlations (features spuriously correlated with the label on the training data, but not on test), the zero-shot and few-shot performance of foundation models is no better than ERM models, and remains unchanged when pretrained data/model size is scaled. Secondly, even in these situations, foundation models are quite accurate at predicting the value of the spurious feature. In a simplified setup, we theoretically analyze both these findings. Specifically, we show that during contrastive pretraining, the simplicity bias of foundation models tends to result in the learning of features that mostly rely on the spurious attribute, compared to more robust features. We leverage these observations to propose Prompting for Robustness (PfR) which first uses foundation models to zero-shot predict the spurious attribute on labeled examples, and then learns a classifier with balanced performance across different groups of labels and spurious attribute. Across 5 vision and language tasks, we show that PfR's performance nearly equals that of an oracle algorithm (group DRO) that leverages human labeled spurious attributes.",[],,"['Amrith Setlur', 'Saurabh Garg', 'Virginia Smith', 'Sergey Levine']","['School of Computer Science, Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'University of California Berkeley']",
https://openreview.net/forum?id=RfQT6vJt8b,Fairness & Bias,How Far Can Fairness Constraints Help Recover From Biased Data?,"A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum & Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum & Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and extend it to the case when their stylized distribution has labels with Massart noise instead of i.i.d. noise. We prove a similar recovery result for arbitrary data distributions using fair reject option classifiers. We further generalize it to arbitrary data distributions and arbitrary hypothesis classes, i.e., we prove that for any data distribution, if the optimally accurate classifier in a given hypothesis class is fair and robust, then it can be recovered through fair classification with equal opportunity constraints on the biased distribution whenever the bias parameters satisfy certain simple conditions. Finally, we show applications of our technique to time-varying data bias in classification and fair machine learning pipelines.",[],,"['mohit sharma', 'Amit Deshpande']","['Computer Science and Engineering, Indraprastha Institute of Information Technology, Delhi', 'Microsoft Research']",
https://openreview.net/forum?id=iRcmqXZjeK,Fairness & Bias,Learning Decision Policies with Instrumental Variables through Double Machine Learning,"A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key uncounfounded variable called the instrument, is a standard technique for learning causal relationships between confounded action, outcome and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.",[],,"['Daqian Shao', 'Ashkan Soleymani', 'Francesco Quinzan', 'Marta Kwiatkowska']","['AI research, J.P. Morgan Chase', 'Massachusetts Institute of Technology', 'Department of Computer Science, University of Oxford', 'Polish Academy of Sciences']",
https://openreview.net/forum?id=srejp9uOx7,Fairness & Bias,ReLUs Are Sufficient for Learning Implicit Neural Representations,"Motivated by the growing theoretical understanding of neural networks that employ the Rectified Linear Unit (ReLU) as their activation function, we revisit the use of ReLU activation functions for learning implicit neural representations (INRs). Inspired by second order B-spline wavelets, we incorporate a set of simple constraints to the ReLU neurons in each layer of a deep neural network (DNN) to remedy the spectral bias. This in turn enables its use for various INR tasks. Empirically, we demonstrate that, contrary to popular belief, one *can learn* state-of-the-art INRs based on a DNN composed of only ReLU neurons. Next, by leveraging recent theoretical works which characterize the kinds of functions ReLU neural networks learn, we provide a way to quantify the regularity of the learned function. This offers a principled approach to selecting the hyperparameters in INR architectures. We substantiate our claims through experiments in signal representation, super resolution, and computed tomography, demonstrating the versatility and effectiveness of our method. The code for all experiments can be found at https://github.com/joeshenouda/relu-inrs.",[],,"['Joseph Shenouda', 'Yamin Zhou', 'Robert D Nowak']","['Electrical and Computer Engineering, University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'University of Wisconsin, Madison']",
https://openreview.net/forum?id=or8BQ4ohGb,Transparency & Explainability,InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation,"Tabular data are omnipresent in various sectors of industries. Neural networks for tabular data such as TabNet have been proposed to make predictions while leveraging the attention mechanism for interpretability. However, the inferred attention masks are often dense, making it challenging to come up with rationales about the predictive signal. To remedy this, we propose InterpreTabNet, a variant of the TabNet model that models the attention mechanism as a latent variable sampled from a Gumbel-Softmax distribution. This enables us to regularize the model to learn distinct concepts in the attention masks via a KL Divergence regularizer. It prevents overlapping feature selection by promoting sparsity which maximizes the model's efficacy and improves interpretability to determine the important features when predicting the outcome. To assist in the interpretation of feature interdependencies from our model, we employ a large language model (GPT-4) and use prompt engineering to map from the learned feature mask onto natural language text describing the learned signal. Through comprehensive experiments on real-world datasets, we demonstrate that InterpreTabNet outperforms previous methods for interpreting tabular data while attaining competitive accuracy.",[],,"['Jacob Yoke Hong Si', 'Wendy Yusi Cheng', 'Michael Cooper', 'Rahul Krishnan']","['Imperial College London', 'University of Toronto', 'Computer Science, University of Toronto', 'Department of Computer Science, University of Toronto']",
https://openreview.net/forum?id=Chy4rSqy4Y,Security,IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics,"No-reference image- and video-quality metrics are widely used in video processing benchmarks. The robustness of learning-based metrics under video attacks has not been widely studied. In addition to having success, attacks on metrics that can be employed in video processing benchmarks must be fast and imperceptible. This paper introduces an Invisible One-Iteration (IOI) adversarial attack on no-reference image and video quality metrics. The proposed method uses two modules to ensure high visual quality and temporal stability of adversarial videos and runs for one iteration, which makes it fast. We compared our method alongside eight prior approaches using image and video datasets via objective and subjective tests. Our method exhibited superior visual quality across various attacked metric architectures while maintaining comparable attack success and speed. We made the code available on GitHub: https://github.com/katiashh/ioi-attack.",[],,"['Ekaterina Shumitskaya', 'Anastasia Antsiferova', 'Dmitriy S. Vatolin']","['Trusted Artificial Intelligence, Ivannikov Institute for System Programming of the Russian Academy of Sciences', 'Innopolis University', 'Computer Science, Moscow State University, Lomonosov Moscow State University']",
https://openreview.net/forum?id=gEbl6XNLK6,Transparency & Explainability,Learning to Intervene on Concept Bottlenecks,"While deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Moreover, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Up to this point, these interventions were typically applied to the model just once and then discarded. To rectify this, we present concept bottleneck memory models (CB2Ms), which keep a memory of past interventions. Specifically, CB2Ms leverage a two-fold memory to generalize interventions to appropriate novel situations, enabling the model to identify errors and reapply previous interventions. This way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. Our experimental evaluations on challenging scenarios like handling distribution shifts and confounded data demonstrate that CB2Ms are able to successfully generalize interventions to unseen data and can indeed identify wrongly inferred concepts. Hence, CB2Ms are a valuable tool for users to provide interactive feedback on CBMs, by guiding a user's interaction and requiring fewer interventions.",[],,"['David Steinmann', 'Wolfgang Stammer', 'Felix Friedrich', 'Kristian Kersting']","['Technische Universität Darmstadt', 'Computer Science Department, CS Department, TU Darmstadt', 'Computer science , TU Darmstadt', 'German Research Center for AI']",
https://openreview.net/forum?id=BdQTCAuT6L,Security,Private Truly-Everlasting Robust-Prediction,"Private everlasting prediction (PEP), recently introduced by Naor et al. [2023], is a model for differentially private learning in which the learner never publicly releases a hypothesis. Instead, it provides black-box access to a ""prediction oracle"" that can predict the labels of an *endless stream* of unlabeled examples drawn from the underlying distribution. Importantly, PEP provides privacy both for the initial training set and for the endless stream of classification queries. We present two conceptual modifications to the definition of PEP, as well as new constructions exhibiting significant improvements over prior work. Specifically, we incorporate robustness against poisoning attacks into the definition of PEP; we present a relaxed privacy definition, suitable for PEP, that allows us to disconnect the privacy parameter $\delta$ from the number of total time steps $T$; and we present new constructions for axis-aligned rectangles and decision-stumps exhibiting improved sample complexity and runtime.",[],,['Uri Stemmer'],['Tel Aviv University'],
https://openreview.net/forum?id=VZsxhPpu9T,Privacy & Data Governance,Rényi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration via Shift Reduction Lemmas,"Pufferfish privacy is a flexible generalization of differential privacy that allows to model arbitrary secrets and adversary's prior knowledge about the data. Unfortunately, designing general and tractable Pufferfish mechanisms that do not compromise utility is challenging. Furthermore, this framework does not provide the composition guarantees needed for a direct use in iterative machine learning algorithms. To mitigate these issues, we introduce a Rényi divergence-based variant of Pufferfish and show that it allows us to extend the applicability of the Pufferfish framework. We first generalize the Wasserstein mechanism to cover a wide range of noise distributions and introduce several ways to improve its utility. Finally, as an alternative to composition, we prove privacy amplification results for contractive noisy iterations and showcase the first use of Pufferfish in private convex optimization. A common ingredient underlying our results is the use and extension of shift reduction lemmas.",[],,"['Clément Pierquin', 'Aurélien Bellet', 'Marc Tommasi', 'Matthieu Boussard']","['Craft AI', 'INRIA', 'Lille University', 'Craft AI']",
https://openreview.net/forum?id=GhPFmTJNfj,Fairness & Bias,Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction,"Graph neural network (GNN) link prediction is increasingly deployed in citation, collaboration, and online social networks to recommend academic literature, collaborators, and friends. While prior research has investigated the dyadic fairness of GNN link prediction, the within-group (e.g., queer women) fairness and ""rich get richer"" dynamics of link prediction remain underexplored. However, these aspects have significant consequences for degree and power imbalances in networks. In this paper, we shed light on how degree bias in networks affects Graph Convolutional Network (GCN) link prediction. In particular, we theoretically uncover that GCNs with a symmetric normalized graph filter have a within-group preferential attachment bias. We validate our theoretical analysis on real-world citation, collaboration, and online social networks. We further bridge GCN's preferential attachment bias with unfairness in link prediction and propose a new within-group fairness metric. This metric quantifies disparities in link prediction scores within social groups, towards combating the amplification of degree and power disparities. Finally, we propose a simple training-time strategy to alleviate within-group unfairness, and we show that it is effective on citation, social, and credit networks.",[],,"['Arjun Subramonian', 'Levent Sagun', 'Yizhou Sun']","['Computer Science, University of California, Los Angeles', 'Meta', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=AoYhtJ4A90,Privacy & Data Governance,FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models,"Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access due to the high encapsulation, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT allows the clients to treat the model as a black-box inference API. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs. Experiments highlight the framework's ability to drastically cut communication and memory costs while maintaining competitive performance. Ultimately, FedBPT presents a promising solution for efficient, privacy-preserving fine-tuning of PLM in the age of large language models.",[],,"['Jingwei Sun', 'Ziyue Xu', 'Hongxu Yin', 'Dong Yang', 'Daguang Xu', 'Yudong Liu', 'Zhixu Du', 'Yiran Chen', 'Holger R Roth']","['Electrical and Computer Engineering, Duke University', 'NVIDIA', 'NVIDIA', 'NVIDIA', 'NVIDIA', 'Duke University', 'Electrical and Computer Engineering, Duke University, Duke University', 'Duke University', 'NVIDIA']",
https://openreview.net/forum?id=9Ub6nLqdMo,Fairness & Bias,A Universal Class of Sharpness-Aware Minimization Algorithms,"Recently, there has been a surge in interest in developing optimization algorithms for overparameterized models as achieving generalization is believed to require algorithms with suitable biases. This interest centers on minimizing sharpness of the original loss function; the Sharpness-Aware Minimization (SAM) algorithm has proven effective. However, most literature only considers a few sharpness measures, such as the maximum eigenvalue or trace of the training loss Hessian, which may not yield meaningful insights for non-convex optimization scenarios like neural networks. Additionally, many sharpness measures are sensitive to parameter invariances in neural networks, magnifying significantly under rescaling parameters. Motivated by these challenges, we introduce a new class of sharpness measures in this paper, leading to new sharpness-aware objective functions. We prove that these measures are *universally expressive*, allowing any function of the training loss Hessian matrix to be represented by appropriate hyperparameters. Furthermore, we show that the proposed objective functions explicitly bias towards minimizing their corresponding sharpness measures, and how they allow meaningful applications to models with parameter invariances (such as scale-invariances). Finally, as instances of our proposed general framework, we present *Frob-SAM* and *Det-SAM*, which are specifically designed to minimize the Frobenius norm and the determinant of the Hessian of the training loss, respectively. We also demonstrate the advantages of our general framework through extensive experiments.",[],,"['Behrooz Tahmasebi', 'Ashkan Soleymani', 'Dara Bahri', 'Stefanie Jegelka', 'Patrick Jaillet']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Google Research', 'Computer Science, Technische Universität München', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=jdRIaUu3xY,Transparency & Explainability,BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models,"Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.",[],,"['Haotian Sun', 'Yuchen Zhuang', 'Wei Wei', 'Chao Zhang', 'Bo Dai']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Google', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=jhWSzTO0Jl,Transparency & Explainability,Post-hoc Part-Prototype Networks,"Post-hoc explainability methods such as Grad-CAM are popular because they do not influence the performance of a trained model. However, they mainly reveal ''where'' a model looks at for a given input, fail to explain ''what'' the model looks for (e.g., what is important to classify a bird image to a Scott Oriole?). Existing part-prototype networks leverage part-prototypes (e.g., characteristic Scott Oriole's wing and head) to answer both ''where"" and ''what"", but often under-perform their black box counterparts in the accuracy. Therefore, a natural question is: can one construct a network that answers both ''where'' and ''what"" in a post-hoc manner to guarantee the model's performance? To this end, we propose the first post-hoc part-prototype network via decomposing the classification head of a trained model into a set of interpretable part-prototypes. Concretely, we propose an unsupervised prototype discovery and refining strategy to obtain prototypes that can precisely reconstruct the classification head, yet being interpretable. Besides guaranteeing the performance, we show that our network offers more faithful explanations qualitatively and yields even better part-prototypes quantitatively than prior part-prototype networks.",[],,"['Andong Tan', 'Fengtao ZHOU', 'Hao Chen']","['Computer Science and Engineering, Hong Kong University of Science and Technology', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology']",
https://openreview.net/forum?id=JndWnomyIc,Fairness & Bias,FRAPPÉ: A Group Fairness Framework for Post-Processing Everything,"Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior post-processing methods. Finally, we demonstrate several advantages of a modular mitigation strategy that disentangles the training of the prediction model from the fairness mitigation, including better performance on tasks with partial group labels.",[],,"['Alexandru Tifrea', 'Preethi Lahoti', 'Ben Packer', 'Yoni Halpern', 'Ahmad Beirami', 'Flavien Prost']","['Swiss Federal Institute of Technology', '', 'Stanford University', ', New York University', 'Google', 'Google']",
https://openreview.net/forum?id=FMEhnS0948,Privacy & Data Governance,Ranking-based Client Imitation Selection for Efficient Federated Learning,"Federated Learning (FL) enables multiple devices to collaboratively train a shared model while ensuring data privacy. The selection of participating devices in each training round critically affects both the model performance and training efficiency, especially given the vast heterogeneity in training capabilities and data distribution across devices. To deal with these challenges, we introduce a novel device selection solution called FedRank, which is based on an end-to-end, ranking-based model that is pre-trained by imitation learning against state-of-the-art analytical approaches. It not only considers data and system heterogeneity at runtime but also adaptively and efficiently chooses the most suitable clients for model training. Specifically, FedRank views client selection in FL as a ranking problem and employs a pairwise training strategy for the smart selection process. Additionally, an imitation learning-based approach is designed to counteract the cold-start issues often seen in state-of-the-art learning-based approaches. Experimental results reveal that FedRank boosts model accuracy by 5.2% to 56.9%, accelerates the training convergence up to $2.01 \times$ and saves the energy consumption up to 40.1%.",[],,"['Chunlin Tian', 'Zhan Shi', 'Xinpengqin', 'Li Li', 'Cheng-zhong Xu']","['University of Macau', 'Department of Computer Science, University of Texas, Austin', 'School of Information and Software Engineering, University of Electronic Science and Technology of China', 'University of Macau', 'University of Macau']",
https://openreview.net/forum?id=chDpBp2P6b,Security,Beyond Individual Input for Deep Anomaly Detection on Tabular Data,"Anomaly detection is vital in many domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train an NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features to generate an anomaly score. To the best of our knowledge, this is the first work to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. Through extensive experiments on 31 benchmark tabular datasets, we demonstrate that our method achieves state-of-the-art performance, outperforming existing methods by 2.4% and 1.2% in terms of F1-score and AUROC, respectively. Our ablation study further proves that modeling both types of dependencies is crucial for anomaly detection on tabular data.",[],,"['Hugo Thimonier', 'Fabrice Popineau', 'Arpad Rimmel', 'Bich-Liên DOAN']","['Emobot', 'Laboratoire Interdisciplinaire des Sciences du Numérique', 'Laboratoire de recherche en informatique', 'CentraleSupelec']",
https://openreview.net/forum?id=eGZH3HCuGm,Fairness & Bias,Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data,"Simplicity bias, the propensity of deep models to over-rely on simple features, has been identified as a potential reason for limited out-of-distribution generalization of neural networks (Shah et al., 2020). Despite the important implications, this phenomenon has been theoretically confirmed and characterized only under strong dataset assumptions, such as linear separability (Lyu et al., 2021). In this work, we characterize simplicity bias for general datasets in the context of two-layer neural networks initialized with small weights and trained with gradient flow. Specifically, we prove that in the early training phases, network features cluster around a few directions that do not depend on the size of the hidden layer. Furthermore, for datasets with an XOR-like pattern, we precisely identify the learned features and demonstrate that simplicity bias intensifies during later training stages. These results indicate that features learned in the middle stages of training may be more useful for OOD transfer. We support this hypothesis with experiments on image data.",[],,"['Nikita Tsoy', 'Nikola Konstantinov']","['INSAIT, Sofia University', 'INSAIT, Sofia University']",
https://openreview.net/forum?id=6UGSDDPkJw,Transparency & Explainability,Position: Do Not Explain Vision Models Without Context,"Does the stethoscope in the picture make the adjacent person a doctor or a patient? This, of course, depends on the contextual relationship of the two objects. If it’s obvious, why don’t explanation methods for vision models use contextual information? In this paper, we (1) review the most popular methods of explaining computer vision models by pointing out that they do not take into account context information, (2) show examples of failures of popular XAI methods, (3) provide examples of real-world use cases where spatial context plays a significant role, (4) propose new research directions that may lead to better use of context information in explaining computer vision models, (5) argue that a change in approach to explanations is needed from *where* to *how*.",[],,"['Paulina Tomaszewska', 'Przemyslaw Biecek']","['Faculty of Mathematics and Information Science, Warsaw University of Technology', 'Warsaw University of Technology']",
https://openreview.net/forum?id=ZxDqSBgFSM,Transparency & Explainability,Federated Self-Explaining GNNs with Anti-shortcut Augmentations,"Graph Neural Networks (GNNs) have demonstrated remarkable performance in graph classification tasks. However, ensuring the explainability of their predictions remains a challenge. To address this, graph rationalization methods have been introduced to generate concise subsets of the original graph, known as rationales, which serve to explain the predictions made by GNNs. Existing rationalizations often rely on shortcuts in data for prediction and rationale composition. In response, de-shortcut rationalization methods have been proposed, which commonly leverage counterfactual augmentation to enhance data diversity for mitigating the shortcut problem. Nevertheless, these methods have predominantly focused on centralized datasets and have not been extensively explored in the Federated Learning (FL) scenarios. To this end, in this paper, we propose a Federated Graph Rationalization (FedGR) with anti-shortcut augmentations to achieve self-explaining GNNs, which involves two data augmenters. These augmenters are employed to produce client-specific shortcut conflicted samples at each client, which contributes to mitigating the shortcut problem under the FL scenarios. Experiments on real-world benchmarks and synthetic datasets validate the effectiveness of FedGR under the FL scenarios.",[],,"['Linan Yue', 'Qi Liu', 'Weibo Gao', 'Ye Liu', 'Kai Zhang', 'Yichao Du', 'Li Wang', 'Fangzhou Yao']","['University of Science and Technology of China, University of Science and Technology of China', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China', '', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', '']",
https://openreview.net/forum?id=RiQbe8RwCe,Fairness & Bias,Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning,"The success of SGD in deep learning has been ascribed by prior works to the *implicit bias* induced by finite batch sizes (''SGD noise''). While prior works focused on *offline learning* (i.e., multiple-epoch training), we study the impact of SGD noise on *online* (i.e., single epoch) learning. Through an extensive empirical analysis of image and language data, we demonstrate that small batch sizes do *not* confer any implicit bias advantages in online learning. In contrast to offline learning, the benefits of SGD noise in online learning are strictly computational, facilitating more cost-effective gradient steps. This suggests that SGD in the online regime can be construed as taking noisy steps along the ''golden path'' of the noiseless *gradient descent* algorithm. We study this hypothesis and provide supporting evidence in loss and function space. Our findings challenge the prevailing understanding of SGD and offer novel insights into its role in online learning.",[],,"['Nikhil Vyas', 'Depen Morwani', 'Rosie Zhao', 'Gal Kaplun', 'Sham M. Kakade', 'Boaz Barak']","['Harvard University', 'Harvard University, Harvard University', 'School of Engineering and Applied Sciences, Harvard University, Harvard University', 'Harvard University', 'Harvard University', 'Harvard University']",
https://openreview.net/forum?id=znKAWRZSF9,Fairness & Bias,"S3GCL: Spectral, Swift, Spatial Graph Contrastive Learning","Graph Contrastive Learning (GCL) has emerged as a highly effective self-supervised approach in graph representation learning. However, prevailing GCL methods confront two primary challenges: 1) They predominantly operate under homophily assumptions, focusing on low-frequency signals in node features while neglecting heterophilic edges that connect nodes with dissimilar features. 2) Their reliance on neighborhood aggregation for inference leads to scalability challenges and hinders deployment in real-time applications. In this paper, we introduce S3GCL, an innovative framework designed to tackle these challenges. Inspired by spectral GNNs, we initially demonstrate the correlation between frequency and homophily levels. Then, we propose a novel cosine-parameterized Chebyshev polynomial as low/high-pass filters to generate biased graph views. To resolve the inference dilemma, we incorporate an MLP encoder and enhance its awareness of graph context by introducing structurally and semantically neighboring nodes as positive pairs in the spatial domain. Finally, we formulate a cross-pass GCL objective between full-pass MLP and biased-pass GNN filtered features, eliminating the need for augmentation. Extensive experiments on real-world tasks validate S3GCL proficiency in generalization to diverse homophily levels and its superior inference efficiency.",[],,"['Guancheng Wan', 'Yijun Tian', 'Wenke Huang', 'Nitesh V Chawla', 'Mang Ye']","['Emory University, Emory University', 'Amazon', 'Wuhan University, Wuhan University', 'Computer Science & Engineering, University of Notre Dame', 'Wuhan University']",
https://openreview.net/forum?id=AtVtt9xsO1,Transparency & Explainability,Trustless Audits without Revealing Data or Models,"There is an increasing conflict between business incentives to hide models and data as trade secrets, and the societal need for algorithmic transparency. For example, a rightsholder who currently wishes to know whether their copyrighted works have been used during training must convince the model provider to allow a third party to audit the model and data. Finding a mutually agreeable third party is difficult, and the associated costs often make this approach impractical. In this work, we show that it is possible to simultaneously allow model providers to keep their models and data secret while allowing other parties to trustlessly audit properties of the model and data. We do this by designing a protocol called ZkAudit in which model providers publish cryptographic commitments of datasets and model weights, alongside a zero-knowledge proof (ZKP) certifying that published commitments are derived from training the model. Model providers can then respond to audit requests by privately computing any function F of the dataset (or model) and releasing the output of F alongside another ZKP certifying the correct execution of F. To enable ZkAudit, we develop new methods of computing ZKPs for SGD on modern neural nets for recommender systems and image classification models capable of high accuracies on ImageNet. Empirically, we show it is possible to provide trustless audits of DNNs, including copyright, censorship, and counterfactual audits with little to no loss in accuracy.",[],,"['Suppakit Waiwitlikhit', 'Ion Stoica', 'Yi Sun', 'Tatsunori Hashimoto', 'Daniel Kang']","['Stanford University', 'EECS, University of California, Berkeley', 'University of Chicago', 'Stanford University', 'Department of Computer Science']",
https://openreview.net/forum?id=ZtOXZCTgBa,Fairness & Bias,SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets,"Model-based offline reinforcement Learning (RL) is a promising approach that leverages existing data effectively in many real-world applications, especially those involving high-dimensional inputs like images and videos. To alleviate the distribution shift issue in offline RL, existing model-based methods heavily rely on the uncertainty of learned dynamics. However, the model uncertainty estimation becomes significantly biased when observations contain complex distractors with non-trivial dynamics. To address this challenge, we propose a new approach - *Separated Model-based Offline Policy Optimization* (SeMOPO) - decomposing latent states into endogenous and exogenous parts via conservative sampling and estimating model uncertainty on the endogenous states only. We provide a theoretical guarantee of model uncertainty and performance bound of SeMOPO. To assess the efficacy, we construct the Low-Quality Vision Deep Data-Driven Datasets for RL (LQV-D4RL), where the data are collected by non-expert policy and the observations include moving distractors. Experimental results show that our method substantially outperforms all baseline methods, and further analytical experiments validate the critical designs in our method. The project website is https://sites.google.com/view/semopo.",[],,"['Shenghua Wan', 'Ziyuan Chen', 'Le Gan', 'Shuai Feng', 'De-Chuan Zhan']","['', 'School of Mathematical Sciences, Peking University', 'Nanjing University', 'School of Cyberspace Science and Technology, Beijing Institute of Technology', 'School of AI, Nanjing University']",
https://openreview.net/forum?id=0ZTuy5CrL7,Transparency & Explainability,TVE: Learning Meta-attribution for Transferable Vision Explainer,"Explainable machine learning significantly improves the transparency of deep neural networks. However, existing work is constrained to explaining the behavior of individual model predictions, and lacks the ability to transfer the explanation across various models and tasks. This limitation results in explaining various tasks being time- and resource-consuming. To address this problem, we introduce a **Transferable Vision Explainer** (TVE) that can effectively explain various vision models in downstream tasks. Specifically, the transferability of TVE is realized through a pre-training process on large-scale datasets towards learning the meta-attribution. This meta-attribution leverages the versatility of generic backbone encoders to comprehensively encode the attribution knowledge for the input instance, which enables TVE to seamlessly transfer to explaining various downstream tasks, without the need for training on task-specific data. Empirical studies involve explaining three different architectures of vision models across three diverse downstream datasets. The experiment results indicate TVE is effective in explaining these tasks without the need for additional training on downstream data.",[],,"['Guanchu Wang', 'Yu-Neng Chuang', 'Fan Yang', 'Mengnan Du', 'Chia-Yuan Chang', 'Shaochen Zhong', 'Zirui Liu', 'Zhaozhuo Xu', 'Kaixiong Zhou', 'Xuanting Cai', 'Xia Hu']","['Rice University', 'Computer Science, Rice University', 'Wake Forest University', 'New Jersey Institute of Technology', 'Computer Science and Engineering, Texas A&M University - College Station', 'Rice University', 'University of Minnesota - Twin Cities', 'Rice University', 'Electrical and Computer Engineering, North Carolina State University', 'Louisiana State University', 'Computer Science , Rice University']",
https://openreview.net/forum?id=HCDMiaT0Pf,Security,Adversarially Robust Hypothesis Transfer Learning,"In this work, we explore Hypothesis Transfer Learning (HTL) under adversarial attacks. In this setting, a learner has access to a training dataset of size $n$ from an underlying distribution $\mathcal{D}$ and a set of auxiliary hypotheses. These auxiliary hypotheses, which can be viewed as prior information originating either from expert knowledge or as pre-trained foundation models, are employed as an initialization for the learning process. Our goal is to develop an adversarially robust model for $\mathcal{D}$. We begin by examining an adversarial variant of the regularized empirical risk minimization learning rule that we term A-RERM. Assuming a non-negative smooth loss function with a strongly convex regularizer, we establish a bound on the robust generalization error of the hypothesis returned by A-RERM in terms of the robust empirical loss and the quality of the initialization. If the initialization is good, i.e., there exists a weighted combination of auxiliary hypotheses with a small robust population loss, the bound exhibits a fast rate of $\mathcal{O}(1/n)$. Otherwise, we get the standard rate of $\mathcal{O}(1/\sqrt{n})$. Additionally, we provide a bound on the robust excess risk which is similar in nature, albeit with a slightly worse rate. We also consider solving the problem using a practical variant, namely proximal stochastic adversarial training, and present a bound that depends on the initialization. This bound has the same dependence on the sample size as the ARERM bound, except for an additional term that depends on the size of the adversarial perturbation.",[],,"['Yunjuan Wang', 'Raman Arora']","['Computer Science Department, Johns Hopkins University', 'Computer Science, Johns Hopkins University']",
https://openreview.net/forum?id=I44Em5D5xy,Fairness & Bias,Swallowing the Bitter Pill: Simplified Scalable Conformer Generation,"We present a novel way to predict molecular conformers through a simple formulation that sidesteps many of the heuristics of prior works and achieves state of the art results by using the advantages of scale. By training a diffusion generative model directly on 3D atomic positions without making assumptions about the explicit structure of molecules (e.g. modeling torsional angles) we are able to radically simplify structure learning, and make it trivial to scale up the model sizes. This model, called Molecular Conformer Fields (MCF), works by parameterizing conformer structures as functions that map elements from a molecular graph directly to their 3D location in space. This formulation allows us to boil down the essence of structure prediction to learning a distribution over functions. Experimental results show that scaling up the model capacity leads to large gains in generalization performance without enforcing inductive biases like rotational equivariance. MCF represents an advance in extending diffusion models to handle complex scientific problems in a conceptually simple, scalable and effective manner.",[],,"['Yuyang Wang', 'Ahmed A. A. Elhag', 'Navdeep Jaitly', 'Joshua M. Susskind', 'Miguel Ángel Bautista']","['MLR, Apple', 'Department of Computer Science, University of Oxford', 'Apple', 'Cognitive Science, Apple', 'Apple']",
https://openreview.net/forum?id=i9C4Kwm56G,Fairness & Bias,Rapid Learning without Catastrophic Forgetting in the Morris Water Maze,"Animals can swiftly adapt to novel tasks, while maintaining proficiency on previously trained tasks. This contrasts starkly with machine learning models, which struggle on these capabilities. We first propose a new task, the sequential Morris Water Maze (sWM), which extends a widely used task in the psychology and neuroscience fields and requires both rapid and continual learning. It has frequently been hypothesized that inductive biases from brains could help build better ML systems, but the addition of constraints typically hurts rather than helping ML performance. We draw inspiration from biology to show that combining 1) a content-addressable heteroassociative memory based on the entorhinal-hippocampal circuit with grid cells that retain shared across-environment structural representations and hippocampal cells that acquire environment-specific information; 2) a spatially invariant convolutional network architecture for rapid adaptation across unfamiliar environments; and 3) the ability to perform remapping, which orthogonalizes internal representations; leads to good generalization, rapid learning, and continual learning without forgetting, respectively. Our model outperforms ANN baselines from continual learning contexts applied to the task. It retains knowledge of past environments while rapidly acquiring the skills to navigate new ones, thereby addressing the seemingly opposing challenges of quick knowledge transfer and sustaining proficiency in previously learned tasks. These biologically motivated results may point the way toward ML algorithms with similar properties.",[],,"['Raymond Wang', 'Jaedong Hwang', 'Akhilan Boopathy', 'Ila R Fiete']","['University of California Berkeley', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=ykZYLBcA9g,Fairness & Bias,Learning with Complementary Labels Revisited: The Selected-Completely-at-Random Setting Is More Practical,"Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected-Completely-at-Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of negative-unlabeled binary classification problems when using the one-versus-rest strategy. Extensive experimental results on both synthetic and real-world benchmark datasets validate the superiority of our proposed approach over state-of-the-art methods.",[],,"['Wei Wang', 'Takashi Ishida', 'Yu-Jie Zhang', 'Gang Niu', 'Masashi Sugiyama']","['RIKEN', 'RIKEN', 'RIKEN AIP', 'RIKEN', 'Center for Advanced Intelligence Project, RIKEN']",
https://openreview.net/forum?id=R1auM3tLPE,Fairness & Bias,Efficient Online Set-valued Classification with Bandit Feedback,"Conformal prediction is a distribution-free method that wraps a given machine learning model and returns a set of plausible labels that contain the true label with a prescribed coverage rate. In practice, the empirical coverage achieved highly relies on fully observed label information from data both in the training phase for model fitting and the calibration phase for quantile estimation. This dependency poses a challenge in the context of online learning with bandit feedback, where a learner only has access to the correctness of actions (i.e., pulled an arm) but not the full information of the true label. In particular, when the pulled arm is incorrect, the learner only knows that the pulled one is not the true class label, but does not know which label is true. Additionally, bandit feedback further results in a smaller labeled dataset for calibration, limited to instances with correct actions, thereby affecting the accuracy of quantile estimation. To address these limitations, we propose Bandit Class-specific Conformal Prediction (BCCP), offering coverage guarantees on a class-specific granularity. Using an unbiased estimation of an estimand involving the true label, BCCP trains the model and makes set-valued inferences through stochastic gradient descent. Our approach overcomes the challenges of sparsely labeled data in each iteration and generalizes the reliability and applicability of conformal prediction to online decision-making environments.",[],,"['Zhou Wang', 'Xingye Qiao']","['ByteDance Inc.', 'State University of New York at Binghamton']",
https://openreview.net/forum?id=126SR50BEL,Fairness & Bias,A Dual-module Framework for Counterfactual Estimation over Time,"Efficiently and effectively estimating counterfactuals over time is crucial for optimizing treatment strategies. We present the Adversarial Counterfactual Temporal Inference Network (ACTIN), a novel framework with dual modules to enhance counterfactual estimation. The balancing module employs a distribution-based adversarial method to learn balanced representations, extending beyond the limitations of current classification-based methods to mitigate confounding bias across various treatment types. The integrating module adopts a novel Temporal Integration Predicting (TIP) strategy, which has a wider receptive field of treatments and balanced representations from the beginning to the current time for a more profound level of analysis. TIP goes beyond the established Direct Predicting (DP) strategy, which only relies on current treatments and representations, by empowering the integrating module to effectively capture long-range dependencies and temporal treatment interactions. ACTIN exceeds the confines of specific base models, and when implemented with simple base models, consistently delivers state-of-the-art performance and efficiency across both synthetic and real-world datasets.",[],,"['Xin Wang', 'Shengfei Lyu', 'Lishan Yang', 'Yibing Zhan', 'Huanhuan Chen']","['University of Science and Technology of China', 'Nanyang Technological University', 'University of Science and Technology of China', 'JD Explore Academy, JD Explore Academy', '']",
https://openreview.net/forum?id=p0MGN0LSnx,Security,Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning,"This paper presents FedType, a simple yet pioneering framework designed to fill research gaps in heterogeneous model aggregation within federated learning (FL). FedType introduces small identical proxy models for clients, serving as agents for information exchange, ensuring model security, and achieving efficient communication simultaneously. To transfer knowledge between large private and small proxy models on clients, we propose a novel uncertainty-based asymmetrical reciprocity learning method, eliminating the need for any public data. Comprehensive experiments conducted on benchmark datasets demonstrate the efficacy and generalization ability of FedType across diverse settings. Our approach redefines federated learning paradigms by bridging model heterogeneity, eliminating reliance on public data, prioritizing client privacy, and reducing communication costs (The codes are available in the supplementation materials).",[],,"['Jiaqi Wang', 'Chenxu Zhao', 'Lingjuan Lyu', 'Quanzeng You', 'Mengdi Huai', 'Fenglong Ma']","['', 'Computer science , Iowa State University', 'Sony Research, Sony', 'TikTok, ByteDance', 'Computer Science, Iowa State University', 'College of Information Sciences and Technology, Pennsylvania State University']",
https://openreview.net/forum?id=XUeoOBid3x,Fairness & Bias,Magicoder: Empowering Code Generation with OSS-Instruct,"We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using **OSS-Instruct**, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",[],,"['Yuxiang Wei', 'Zhe Wang', 'Jiawei Liu', 'Yifeng Ding', 'LINGMING ZHANG']","['Fundamental AI Research (FAIR), Meta', 'Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign', 'Siebel School of Computing and Data Science, University of Illinois Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'University of Illinois Urbana-Champaign']",
https://openreview.net/forum?id=7rrN6E4KU0,Privacy & Data Governance,Neural Collapse meets Differential Privacy: Curious behaviors of NoisyGD with Near-Perfect Representation Learning,"A recent study by De et al. (2022) shows that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks. To explain this, we consider a layer-peeled model in representation learning, resulting in Neural Collapse (NC) phenomena. Within NC, we establish that the misclassification error is independent of dimension when the distance between actual and ideal features is below a threshold. We empirically evaluate feature quality in the last layer under different pre-trained models, showing that a more powerful pre-trained model improves feature representation. Moreover, we show that DP fine-tuning is less robust compared to non-DP fine-tuning, especially with perturbations. Supported by theoretical analyses and experiments, we suggest strategies like feature normalization and dimension reduction methods such as PCA to enhance DP fine-tuning robustness. Conducting PCA on last-layer features significantly improves testing accuracy.",[],,"['Chendi Wang', 'Yuqing Zhu', 'Weijie J Su', 'Yu-Xiang Wang']","['Xiamen University', 'UC Santa Barbara', 'Statistics and Data Science, University of Pennsylvania, University of Pennsylvania', 'University of California, San Diego']",
https://openreview.net/forum?id=0mklK4h0rX,Security,Exact Soft Analytical Side-Channel Attacks using Tractable Circuits,"Detecting weaknesses in cryptographic algorithms is of utmost importance for designing secure information systems. The state-of-the-art *soft analytical side-channel attack* (SASCA) uses physical leakage information to make probabilistic predictions about intermediate computations and combines these ""guesses"" with the known algorithmic logic to compute the posterior distribution over the key. This attack is commonly performed via loopy belief propagation, which, however, lacks guarantees in terms of convergence and inference quality. In this paper, we develop a fast and exact inference method for SASCA, denoted as ExSASCA, by leveraging knowledge compilation and tractable probabilistic circuits. When attacking the *Advanced Encryption Standard* (AES), the most widely used encryption algorithm to date, ExSASCA outperforms SASCA by more than 31% top-1 success rate absolute. By leveraging sparse belief messages, this performance is achieved with little more computational cost than SASCA, and about 3 orders of magnitude less than exact inference via exhaustive enumeration. Even with dense belief messages, ExSASCA still uses 6 times less computations than exhaustive inference.",[],,"['Thomas Wedenig', 'Rishub Nagpal', 'Gaëtan Cassiers', 'Stefan Mangard', 'Robert Peharz']","['Technische Universität Graz', 'Technische Universität Graz']",
https://openreview.net/forum?id=qoOt02l2WC,Security,Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution,"In this paper, we dive into the reliability concerns of Integrated Gradients (IG), a prevalent feature attribution method for black-box deep learning models. We particularly address two predominant challenges associated with IG: the generation of noisy feature visualizations for vision models and the vulnerability to adversarial attributional attacks. Our approach involves an adaptation of path-based feature attribution, aligning the path of attribution more closely to the intrinsic geometry of the data manifold. Our experiments utilise deep generative models applied to several real-world image datasets. They demonstrate that IG along the geodesics conforms to the curved geometry of the Riemannian data manifold, generating more perceptually intuitive explanations and, subsequently, substantially increasing robustness to targeted attributional attacks.",[],,"['Eslam Zaher', 'Maciej Trzaskowski', 'Quan Nguyen', 'Fred Roosta']","['University of Queensland', 'Mathematics, The University of Queensland']",
https://openreview.net/forum?id=B0xmynxt4f,Transparency & Explainability,DISCRET: Synthesizing Faithful Explanations For Treatment Effect Estimation,"Designing faithful yet accurate AI models is challenging, particularly in the field of individual treatment effect estimation (ITE). ITE prediction models deployed in critical settings such as healthcare should ideally be (i) accurate, and (ii) provide faithful explanations. However, current solutions are inadequate: state-of-the-art black-box models do not supply explanations, post-hoc explainers for black-box models lack faithfulness guarantees, and self-interpretable models greatly compromise accuracy. To address these issues, we propose DISCRET, a self-interpretable ITE framework that synthesizes faithful, rule-based explanations for each sample. A key insight behind DISCRET is that explanations can serve dually as *database queries* to identify similar subgroups of samples. We provide a novel RL algorithm to efficiently synthesize these explanations from a large search space. We evaluate DISCRET on diverse tasks involving tabular, image, and text data. DISCRET outperforms the best self-interpretable models and has accuracy comparable to the best black-box models while providing faithful explanations. DISCRET is available at https://github.com/wuyinjun-1993/DISCRET-ICML2024.",[],,"['Yinjun Wu', 'Mayank Keoliya', 'Kan Chen', 'Neelay Velingker', 'Ziyang Li', 'Emily J Getzen', 'Qi Long', 'Mayur Naik', 'Ravi B Parikh', 'Eric Wong']","['Department of Computer and information science, Peking University', 'CIS, University of Pennsylvania', 'University of Pennsylvania', 'Computer and Information Science, University of Pennsylvania', 'Computer and Information Science, School of Engineering and Applied Science, University of Pennsylvania', 'University of Pennsylvania Perelman School of Medicine', 'University of Pennsylvania', 'University of Pennsylvania', ""Brigham and Women's Hospital"", 'University of Pennsylvania']",
https://openreview.net/forum?id=SDCx6rQV2l,Security,Confidence-aware Contrastive Learning for Selective Classification,"Selective classification enables models to make predictions only when they are sufficiently confident, aiming to enhance safety and reliability, which is important in high-stakes scenarios. Previous methods mainly use deep neural networks and focus on modifying the architecture of classification layers to enable the model to estimate the confidence of its prediction. This work provides a generalization bound for selective classification, disclosing that optimizing feature layers helps improve the performance of selective classification. Inspired by this theory, we propose to explicitly improve the selective classification model at the feature level for the first time, leading to a novel Confidence-aware Contrastive Learning method for Selective Classification, CCL-SC, which similarizes the features of homogeneous instances and differentiates the features of heterogeneous instances, with the strength controlled by the model's confidence. The experimental results on typical datasets, i.e., CIFAR-10, CIFAR-100, CelebA, and ImageNet, show that CCL-SC achieves significantly lower selective risk than state-of-the-art methods, across almost all coverage degrees. Moreover, it can be combined with existing methods to bring further improvement.",[],,"['Yu-Chang Wu', 'Shen-Huan Lyu', 'Haopu Shang', 'Xiangyu Wang', 'Chao Qian']","['Nanjing University', 'Hohai University', 'Nanjing University', 'nanjing university', 'School of Artificial Intelligence, Nanjing university']",
https://openreview.net/forum?id=Sf5KYznS2G,Fairness & Bias,Reflected Flow Matching,"Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data. Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field. However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error. To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains. We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM. Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains. We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight.",[],,"['Tianyu Xie', 'Yu Zhu', 'Longlin Yu', 'Tong Yang', 'Ziheng Cheng', 'Shiyue Zhang', 'Xiangyu Zhang', 'Cheng Zhang']","['School of Mathematical Sciences, Peking University', 'Institute of automation, Chinese academy of science', 'Peking University', 'Fudan University', 'IEOR, University of California, Berkeley', 'Peking University', 'MEGVII Technology', 'School of Mathematical Sciences, Peking University']",
https://openreview.net/forum?id=CmXkdlO6JJ,Fairness & Bias,Implicit Bias of AdamW: $\ell_\infty$-Norm Constrained Optimization,"Adam with decoupled weight decay, also known as AdamW, is widely acclaimed for its superior performance in language modeling tasks, surpassing Adam with $\ell_2$ regularization in terms of generalization and optimization. However, this advantage is not theoretically well-understood. One challenge here is that though intuitively Adam with $\ell_2$ regularization optimizes the $\ell_2$ regularized loss, it is not clear if AdamW optimizes a specific objective. In this work, we make progress toward understanding the benefit of AdamW by showing that it implicitly performs constrained optimization. More concretely, we show in the full-batch setting, if AdamW converges with any non-increasing learning rate schedule whose partial sum diverges, it must converge to a KKT point of the original loss under the constraint that the $\ell_\infty$ norm of the parameter is bounded by the inverse of the weight decay factor. This result is built on the observation that Adam can be viewed as a smoothed version of SignGD, which is the normalized steepest descent with respect to $\ell_\infty$ norm, and a surprising connection between normalized steepest descent with weight decay and Frank-Wolfe.",[],,"['Shuo Xie', 'Zhiyuan Li']","['Toyota Technological Institute at Chicago', 'Toyota Technological Institute at Chicago']",
https://openreview.net/forum?id=VuoB86HiCL,Fairness & Bias,Automating the Selection of Proxy Variables of Unmeasured Confounders,"Recently, interest has grown in the use of proxy variables of unobserved confounding for inferring the causal effect in the presence of unmeasured confounders from observational data. One difficulty inhibiting the practical use is finding valid proxy variables of unobserved confounding to a target causal effect of interest. These proxy variables are typically justified by background knowledge. In this paper, we investigate the estimation of causal effects among multiple treatments and a single outcome, all of which are affected by unmeasured confounders, within a linear causal model, without prior knowledge of the validity of proxy variables. To be more specific, we first extend the existing proxy variable estimator, originally addressing a single unmeasured confounder, to accommodate scenarios where multiple unmeasured confounders exist between the treatments and the outcome. Subsequently, we present two different sets of precise identifiability conditions for selecting valid proxy variables of unmeasured confounders, based on the second-order statistics and higher-order statistics of the data, respectively. Moreover, we propose two data-driven methods for the selection of proxy variables and for the unbiased estimation of causal effects. Theoretical analysis demonstrates the correctness of our proposed algorithms. Experimental results on both synthetic and real-world data show the effectiveness of the proposed approach.",[],,"['Feng Xie', 'Zhengming Chen', 'Shanshan Luo', 'Wang Miao', 'Ruichu Cai', 'Zhi Geng']","['Beijing Technology and Business University', 'Guangdong University of Technology', '', 'Peking University', 'School of Computer Science, Guangdong University of Technology', 'School of Matematics asn Statistics, Beijing Technology and Business University']",
https://openreview.net/forum?id=Wjq2bS7fTK,Security,FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error,"Federated Learning (FL) faces threats from model poisoning attacks. Existing defenses, typically relying on cross-client/global information to mitigate these attacks, fall short when faced with non-IID data distributions and/or a large number of malicious clients. To address these challenges, we present FedREDefense. Unlike existing methods, it doesn't hinge on similar distributions across clients or a predominant presence of benign clients. Instead, it assesses the likelihood that a client's model update is a product of genuine training, solely based on the characteristics of the model update itself. Our key finding is that model updates stemming from genuine training can be approximately reconstructed with some distilled local knowledge, while those from deliberate handcrafted model poisoning attacks cannot. Drawing on this distinction, FedREDefense identifies and filters out malicious clients based on the discrepancies in their model update Reconstruction Errors. Empirical tests on three benchmark datasets confirm that FedREDefense successfully filters model poisoning attacks in FL—even in scenarios with high non-IID degrees and large numbers of malicious clients.",[],,"['Yueqi XIE', 'Minghong Fang', 'Neil Zhenqiang Gong']","['Computer Science and Technology, Hong Kong University of Science and Technology', 'Department of Computer Science and Engineering, University of Louisville', 'Duke University']",
https://openreview.net/forum?id=FhWH9TQSMh,Fairness & Bias,Intersectional Unfairness Discovery,"AI systems have been shown to produce unfair results for certain subgroups of population, highlighting the need to understand bias on certain sensitive attributes. Current research often falls short, primarily focusing on the subgroups characterized by a single sensitive attribute, while neglecting the nature of intersectional fairness of multiple sensitive attributes. This paper focuses on its one fundamental aspect by discovering diverse high-bias intersectional sensitive attributes. Specifically, we propose a Bias-Guided Generative Network (BGGN). By treating each bias value as a reward, BGGN efficiently generates high-bias intersectional sensitive attributes. Experiments on real-world text and image datasets demonstrate a diverse and efficient discovery of BGGN. To further evaluate the generated unseen but possible unfair intersectional sensitive attributes, we formulate them as prompts and use modern generative AI to produce new text and images. The results of frequently generating biased data provides new insights of discovering potential unfairness in popular modern generative AI systems. **Warning: This paper contains examples that are offensive in nature.**",[],,"['Gezheng Xu', 'Qi CHEN', 'Charles Ling', 'Boyu Wang', 'Changjian Shui']","['Computer Science, University of Western Ontario', 'Department of Computer Science, University of Toronto', 'Computer Science, Western University', 'University of Western Ontario', 'Vector Institute']",
https://openreview.net/forum?id=yUPBkPKzHw,Fairness & Bias,Adapting Static Fairness to Sequential Decision-Making: Bias Mitigation Strategies towards Equal Long-term Benefit Rate,"Decisions made by machine learning models can have lasting impacts, making long-term fairness a critical consideration. It has been observed that ignoring the long-term effect and directly applying fairness criterion in static settings can actually worsen bias over time. To address biases in sequential decision-making, we introduce a long-term fairness concept named Equal Long-term Benefit Rate (ELBERT). This concept is seamlessly integrated into a Markov Decision Process (MDP) to consider the future effects of actions on long-term fairness, thus providing a unified framework for fair sequential decision-making problems. ELBERT effectively addresses the temporal discrimination issues found in previous long-term fairness notions. Additionally, we demonstrate that the policy gradient of Long-term Benefit Rate can be analytically simplified to standard policy gradients. This simplification makes conventional policy optimization methods viable for reducing bias, leading to our bias mitigation approach ELBERT-PO. Extensive experiments across various diverse sequential decision-making environments consistently reveal that ELBERT-PO significantly diminishes bias while maintaining high utility. Code is available at https://github.com/umd-huang-lab/ELBERT.",[],,"['Yuancheng Xu', 'Chenghao Deng', 'Yanchao Sun', 'Ruijie Zheng', 'Xiyao Wang', 'Jieyu Zhao', 'Furong Huang']","['Computer Science, University of Maryland, College Park', 'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Apple AI/ML', 'Computer Science, University of Maryland, College Park', 'University of Maryland, College Park', 'University of Southern California', 'Computer Science, University of Maryland']",
https://openreview.net/forum?id=qbIKUfastZ,Fairness & Bias,Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback,"Restless multi-armed bandits (RMAB) play a central role in modeling sequential decision making problems under an instantaneous activation constraint that at most $B$ arms can be activated at any decision epoch. Each restless arm is endowed with a state that evolves independently according to a Markov decision process regardless of being activated or not. In this paper, we consider the task of learning in episodic RMAB with unknown transition functions, bandit feedback, and adversarial rewards, which can change arbitrarily across episodes. The goal of the decision maker is to maximize its total adversarial rewards during the learning process while the instantaneous activation constraint must be satisfied in each decision epoch. We develop a novel reinforcement learning algorithm with two key contributors: a novel biased adversarial reward estimator to deal with bandit feedback and unknown transitions, and a low-complexity index policy to satisfy the instantaneous activation constraint. We show $\tilde{\mathcal{O}}(H\sqrt{T})$ regret bound for our algorithm, where $T$ is the number of episodes and $H$ is the episode length. To our best knowledge, this is the first algorithm to ensure $\tilde{\mathcal{O}}(\sqrt{T})$ regret for adversarial RMAB in our considered challenging settings.",[],,"['GUOJUN XIONG', 'Jian Li']","['', 'State University of New York at Stony Brook']",
https://openreview.net/forum?id=pkUl39b0in,Security,Robust Inverse Constrained Reinforcement Learning under Model Misspecification,"To solve safety-critical decision-making problems, Inverse Constrained Reinforcement Learning (ICRL) infers constraints from expert demonstrations and seeks to imitate expert preference by utilizing these constraints. While prior ICRL research commonly overlooks the discrepancy between the training and deploying environments, we demonstrate that such a discrepancy can significantly compromise the reliability of the inferred constraints and thus induce unsafe movements. Motivated by this finding, we propose the Robust Constraint Inference (RCI) problem and an Adaptively Robust ICRL (AR-ICRL) algorithm to solve RCI efficiently. Specifically, we model the impact of misspecified dynamics with an opponent policy and learn a robust policy to facilitate safe control in a Markov Game. Subsequently, we adjust our constraint model to align the learned policies to expert demonstrations, accommodating both soft and hard optimality in our behavioral models. Empirical results demonstrate the significance of robust constraints and the effectiveness of the proposed AR-ICRL algorithm under continuous and discrete domains. The code is available at https://github.com/Jasonxu1225/AR-ICRL.",[],,"['Sheng Xu', 'Guiliang Liu']","['The Chinese University of Hong Kong, Shenzhen', 'The Chinese University of Hong Kong, Shenzhen']",
https://openreview.net/forum?id=0SrNCSklZx,Transparency & Explainability,SLOG: An Inductive Spectral Graph Neural Network Beyond Polynomial Filter,"Graph neural networks (GNNs) have exhibited superb power in many graph related tasks. Existing GNNs can be categorized into spatial GNNs and spectral GNNs. The spatial GNNs primarily capture the local information around each node, while the spectral GNNs are able to operate on the frequency signals of the entire graph. However, most, if not all, existing spectral GNNs are faced with two limitations: (1) the polynomial limitation that for most spectral GNNs, the expressive power in the spectral domain is limited to polynomial filters; and (2) the transductive limitation that most spectral GNNs can only be applied to the transductive setting on relatively small-scale graphs. In this paper, we propose a novel spectral graph neural network named SLOG to solve the above two limitations. For the polynomial limitation, SLOG proposes a novel real-valued filter with geometric interpretability, mathematical feasibility and adaptive filtering ability to go beyond polynomial. For the transductive limitation, SLOG combines the subgraph sampling technique in spatial GNNs and the signal processing technique in spectral GNNs together to make itself tailored to the inductive setting on large-scale graphs. Extensive experimental results on 16 datasets demonstrate the superiority of SLOG in inductive homophilic and heterophilic node classification task.",[],,"['Haobo Xu', 'Yuchen Yan', 'Dingsu Wang', 'Zhe Xu', 'Zhichen Zeng', 'Tarek F. Abdelzaher', 'Jiawei Han', 'Hanghang Tong']","['School of Software, Tsinghua University', '', '', 'University of Illinois, Urbana Champaign', 'Computer Science, University of Illinois Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'Computer Science, University of Illinois at Urbana-Champaign (UIUC)', 'computer science, University of Illinois at Urbana-Champaign']",
https://openreview.net/forum?id=EKye56rLuv,Privacy & Data Governance,FairProof : Confidential and Certifiable Fairness for Neural Networks,"Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose *Fairproof* -- a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement *Fairproof* in Gnark and demonstrate empirically that our system is practically feasible. Code is available at https://github.com/infinite-pursuits/FairProof.",[],,"['Chhavi Yadav', 'Amrita Roy Chowdhury', 'Dan Boneh', 'Kamalika Chaudhuri']","['Computer Science & Engineering, University of California, San Diego', 'University of California, San Diego', 'Computer Sciece, Stanford University', 'UC San Diego, University of California, San Diego']",
https://openreview.net/forum?id=Gp5F6qzwGK,Fairness & Bias,Iterative Regularized Policy Optimization with Imperfect Demonstrations,"Imitation learning heavily relies on the quality of provided demonstrations. In scenarios where demonstrations are imperfect and rare, a prevalent approach for refining policies is through online fine-tuning with reinforcement learning, in which a Kullback–Leibler (KL) regularization is often employed to stabilize the learning process. However, our investigation reveals that on the one hand, imperfect demonstrations can bias the online learning process, the KL regularization will further constrain the improvement of online policy exploration. To address the above issues, we propose Iterative Regularized Policy Optimization (IRPO), a framework that involves iterative offline imitation learning and online reinforcement exploration. Specifically, the policy learned online is used to serve as the demonstrator for successive learning iterations, with a demonstration boosting to consistently enhance the quality of demonstrations. Experimental validations conducted across widely used benchmarks and a novel fixed-wing UAV control task consistently demonstrate the effectiveness of IRPO in improving both the demonstration quality and the policy performance. Our code is available at https://github.com/GongXudong/IRPO.",[],,"['Gong Xudong', 'Feng Dawei', 'Kele Xu', 'Yuanzhao Zhai', 'Chengkang Yao', 'Weijia Wang', 'Bo Ding', 'Huaimin Wang']","['College of Computer, National University of Defense Technology', '', '', 'National University of Defense Technology', 'National University of Defense Technology', 'College of Computer, National University of Defense Technology']",
https://openreview.net/forum?id=WtvI3QijEF,Transparency & Explainability,Exploring the LLM Journey from Cognition to Expression with Linear Representations,"This paper presents an in-depth examination of the evolution and interplay of cognitive and expressive capabilities in large language models (LLMs), with a specific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese and English) LLM series. We define and explore the model's cognitive and expressive capabilities through linear representations across three critical phases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Cognitive capability is defined as the quantity and quality of information conveyed by the neuron output vectors within the network, similar to the neural signal processing in human cognition. Expressive capability is defined as the model’s capability to produce word-level output. Our findings unveil a sequential development pattern, where cognitive abilities are largely established during Pretraining, whereas expressive abilities predominantly advance during SFT and RLHF. Statistical analyses confirm a significant correlation between the two capabilities, suggesting that cognitive capacity may limit expressive potential. The paper also explores the theoretical underpinnings of these divergent developmental trajectories and their connection to the LLMs' architectural design. Moreover, we evaluate various optimization-independent strategies, such as few-shot learning and repeated sampling, which bridge the gap between cognitive and expressive capabilities. This research reveals the potential connection between the hidden space and the output space, contributing valuable insights into the interpretability and controllability of their training processes.",[],,"['Yuzi Yan', 'Jialian Li', 'Yipin Zhang', 'Dong Yan']","['Electronic Engineering, Tsinghua University, Tsinghua University', 'Tsinghua University', 'Baichuan Intelligent Technology']",
https://openreview.net/forum?id=BNH8spaR3l,Fairness & Bias,Probabilistic Time Series Modeling with Decomposable Denoising Diffusion Model,"Probabilistic time series modeling based on generative models has attracted lots of attention because of its wide applications and excellent performance. However, existing state-of-the-art models, based on stochastic differential equation, not only struggle to determine the drift and diffusion coefficients during the design process but also have slow generation speed. To tackle this challenge, we firstly propose decomposable denoising diffusion model ($\text{D}^3\text{M}$) and prove it is a general framework unifying denoising diffusion models and continuous flow models. Based on the new framework, we propose some simple but efficient probability paths with high generation speed. Furthermore, we design a module that combines a special state space model with linear gated attention modules for sequence modeling. It preserves inductive bias and simultaneously models both local and global dependencies. Experimental results on 8 real-world datasets show that $\text{D}^3\text{M}$ reduces RMSE and CRPS by up to 4.6% and 4.3% compared with state-of-the-arts on imputation tasks, and achieves comparable results with state-of-the-arts on forecasting tasks with only 10 steps.",[],,"['Tijin Yan', 'Hengheng Gong', 'He YongPing', 'Yufeng Zhan', 'Yuanqing Xia']","['Beijing Institute of Technology', 'Beijing Institute of Technology', 'Beijing Institute of Technology', 'School of Automation, Beijing Institute of Technology', 'Beijing Institute of Technology']",
https://openreview.net/forum?id=GktjBAGgo4,Fairness & Bias,Reducing Balancing Error for Causal Inference via Optimal Transport,"Most studies on causal inference tackle the issue of confounding bias by reducing the distribution shift between the control and treated groups. However, it remains an open question to adopt an appropriate metric for distribution shift in practice. In this paper, we define a generic balancing error on reweighted samples to characterize the confounding bias, and study the connection between the balancing error and the Wasserstein discrepancy derived from the theory of optimal transport. We not only regard the Wasserstein discrepancy as the metric of distribution shift, but also explore the association between the balancing error and the underlying cost function involved in the Wasserstein discrepancy. Motivated by this, we propose to reduce the balancing error under the framework of optimal transport with learnable marginal distributions and the cost function, which is implemented by jointly learning weights and representations associated with factual outcomes. The experiments on both synthetic and real-world datasets demonstrate the effectiveness of our proposed method.",[],,"['Yuguang Yan', 'Hao Zhou', 'Zeqin Yang', 'Weilin Chen', 'Ruichu Cai', 'Zhifeng Hao']","['Guangdong University of Technology', 'School of Computer Science and Technology, Guangdong University of Technology', 'School of Computer Science and Technology, Guangdong University of Technology', 'Guangdong University of Technology', 'School of Computer Science, Guangdong University of Technology', 'Shantou University']",
https://openreview.net/forum?id=ycLHJuLYuD,Security,Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks,"Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it divides the data into safe and risky sets by applying a Gaussian Mixture Model to the cosine similarity of image-caption pair representations. SAFECLIP pre-trains the model by applying the CLIP loss to the safe set and applying unimodal CL to image and text modalities of the risky set separately. By gradually increasing the size of the safe set during pre-training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments on CC3M, Visual Genome, and MSCOCO demonstrate that SAFECLIP significantly reduces the success rate of targeted data poisoning attacks from 93.75% to 0% and that of various backdoor attacks from up to 100% to 0%, without harming CLIP’s performance.",[],,"['Wenhan Yang', 'Jingdong Gao', 'Baharan Mirzasoleiman']","['Computer Science, University of California, Los Angeles', 'University of California, Los Angeles', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=Sbl2keQEML,Fairness & Bias,Representation Surgery for Multi-Task Model Merging,"Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called ``Surgery"" to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific plugin that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model. We then designed an unsupervised optimization objective that updates the Surgery plugin by minimizing the distance between the merged model's representation and the individual model's representation. Extensive experiments demonstrate significant MTL performance improvements when our Surgery plugin is applied to state-of-the-art (SOTA) model merging schemes.",[],,"['Enneng Yang', 'Li Shen', 'Zhenyi Wang', 'Guibing Guo', 'Xiaojun Chen', 'Xingwei Wang', 'Dacheng Tao']","['College of Computing and Data Science, Nanyang Technological University', 'Sun Yat-Sen University', 'Computer Science, University of Maryland, College Park', 'Software College, Northeastern University', 'Computer Science and Technology discipline, Shenzhen University', 'CS, Northeastern University', '']",
https://openreview.net/forum?id=H9fNj8ivTy,Security,Position: Towards Implicit Prompt For Text-To-Image Models,"Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various target symbols indicated by implicit prompts; (2) Implicit prompts bring potential risks of privacy leakage for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can be bypassed with implicit prompts. We call for increased attention to the potential and risks of implicit prompts in the T2I community and further investigation into the capabilities and impacts of implicit prompts, advocating for a balanced approach that harnesses their benefits while mitigating their risks.",[],,"['Yue Yang', 'Yuqi Lin', 'Hong Liu', 'Wenqi Shao', 'Runjian Chen', 'Hailong Shang', 'Yu Wang', 'Yu Qiao', 'Kaipeng Zhang', 'Ping Luo']","['School of Electronic Information and Electrical Engineering, Shanghai Jiaotong University', 'Zhejiang University', 'Xiamen University', 'Shanghai AI Laboratory', 'Computer Science, University of Hong Kong', 'Shanghai Jiao Tong University', '', 'AI Lab, Shanghai AI Laboratory', 'The University of Hong Kong']",
https://openreview.net/forum?id=u9oSQtujCF,Fairness & Bias,Empowering Graph Invariance Learning with Deep Spurious Infomax,"Recently, there has been a surge of interest in developing graph neural networks that utilize the invariance principle on graphs to generalize the out-of-distribution (OOD) data. Due to the limited knowledge about OOD data, existing approaches often pose assumptions about the correlation strengths of the underlying spurious features and the target labels. However, this prior is often unavailable and will change arbitrarily in the real-world scenarios, which may lead to severe failures of the existing graph invariance learning methods. To bridge this gap, we introduce a novel graph invariance learning paradigm, which induces a robust and general inductive bias, which is built upon the observation that the infomax principle encourages learning spurious features regardless of spurious correlation strengths. We further propose the EQuAD framework that realizes this learning paradigm and employs tailored learning objectives that provably elicit invariant features by disentangling them from the spurious features learned through infomax. Notably, EQuAD shows stable and enhanced performance across different degrees of bias in synthetic datasets and challenging real-world datasets up to 31.76%.",[],,"['Tianjun Yao', 'Yongqiang Chen', 'Zhenhao Chen', 'Kai Hu', 'Zhiqiang Shen', 'Kun Zhang']","['', 'Mohamed bin Zayed University of Artificial Intelligence', 'Mohamed bin Zayed University of Artificial Intelligence', 'Carnegie Mellon University', '', 'Mohamed bin Zayed University of Artificial Intelligence']",
https://openreview.net/forum?id=ZdqiT0McON,Security,Generalization Bound and New Algorithm for Clean-Label Backdoor Attack,"The generalization bound is a crucial theoretical tool for assessing the generalizability of learning methods and there exist vast literatures on generalizability of normal learning, adversarial learning, and data poisoning. Unlike other data poison attacks, the backdoor attack has the special property that the poisoned triggers are contained in both the training set and the test set and the purpose of the attack is two-fold. To our knowledge, the generalization bound for the backdoor attack has not been established. In this paper, we fill this gap by deriving algorithm-independent generalization bounds in the clean-label backdoor attack scenario. Precisely, based on the goals of backdoor attack, we give upper bounds for the clean sample population errors and the poison population errors in terms of the empirical error on the poisoned training dataset. Furthermore, based on the theoretical result, a new clean-label backdoor attack is proposed that computes the poisoning trigger by combining adversarial noise and indiscriminate poison. We show its effectiveness in a variety of settings.",[],,"['Lijia Yu', 'Shuang Liu', 'Yibo Miao', 'Xiao-Shan Gao', 'Lijun Zhang']","['Institute of Software, Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Chinese Academy of Sciences, Chinese Academy of Sciences']",
https://openreview.net/forum?id=IArWwIim8M,Security,Activation-Descent Regularization for Input Optimization of ReLU Networks,"We present a new approach for input optimization of ReLU networks that explicitly takes into account the effect of changes in activation patterns. We analyze local optimization steps in both the input space and the space of activation patterns to propose methods with superior local descent properties. To accomplish this, we convert the discrete space of activation patterns into differentiable representations and propose regularization terms that improve each descent step. Our experiments demonstrate the effectiveness of the proposed input-optimization methods for improving the state-of-the-art in various areas, such as adversarial learning, generative modeling, and reinforcement learning.",[],,"['Hongzhan Yu', 'Sicun Gao']","['Computer Science and Engineering, University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=mUT1biz09t,Privacy & Data Governance,Privacy-Preserving Instructions for Aligning Large Language Models,"Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.",[],,"['Da Yu', 'Peter Kairouz', 'Sewoong Oh', 'Zheng Xu']","['Google', 'Google', 'Allen school, University of Washington', 'Google']",
https://openreview.net/forum?id=6aKwVmHQI1,Privacy & Data Governance,ViP: A Differentially Private Foundation Model for Computer Vision,"Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. In this work, we propose as a mitigation measure a recipe to train foundation vision models via self-supervised learning with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train *ViP*---a **Vi**sion transformer with differential **P**rivacy---under a strict privacy budget of $\epsilon=8$ on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of 55.7% on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated on ImageNet). Our result suggests that scaling to internet-scale data can be practical for private learning. Code and DP pre-trained models are available at https://github.com/facebookresearch/ViP-MAE.",[],,"['Yaodong Yu', 'Maziar Sanjabi', 'Yi Ma', 'Kamalika Chaudhuri', 'Chuan Guo']","['Electrical Engineering & Computer Science Department, University of California Berkeley', 'Meta', 'Computer Science, University of Hong Kong', 'UC San Diego, University of California, San Diego', 'Meta']",
https://openreview.net/forum?id=0LBNdbmQCM,Security,Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders,"Unlearnable examples (UEs) seek to maximize testing error by making subtle modifications to training examples that are correctly labeled. Defenses against these poisoning attacks can be categorized based on whether specific interventions are adopted during training. The first approach is training-time defense, such as adversarial training, which can mitigate poisoning effects but is computationally intensive. The other approach is pre-training purification, e.g., image short squeezing, which consists of several simple compressions but often encounters challenges in dealing with various UEs. Our work provides a novel disentanglement mechanism to build an efficient pre-training purification method. Firstly, we uncover rate-constrained variational autoencoders (VAEs), demonstrating a clear tendency to suppress the perturbations in UEs. We subsequently conduct a theoretical analysis for this phenomenon. Building upon these insights, we introduce a disentangle variational autoencoder (D-VAE), capable of disentangling the perturbations with learnable class-wise embeddings. Based on this network, a two-stage purification approach is naturally developed. The first stage focuses on roughly eliminating perturbations, while the second stage produces refined, poison-free results, ensuring effectiveness and robustness across various scenarios. Extensive experiments demonstrate the remarkable performance of our method across CIFAR-10, CIFAR-100, and a 100-class ImageNet-subset. Code is available at https://github.com/yuyi-sd/D-VAE.",[],,"['Yi Yu', 'Yufei Wang', 'Song Xia', 'Wenhan Yang', 'Shijian Lu', 'Yap-peng Tan', 'Alex Kot']","['Interdisciplinary Programme, Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Peng Cheng Laboratory', 'Nanyang Technological University', 'School of EEE, Nanyang Technological University', 'EEE, Nanyang Technological University']",
https://openreview.net/forum?id=nMWxLnSBGW,Security,SHINE: Shielding Backdoors in Deep Reinforcement Learning,"Recent studies have discovered that a deep reinforcement learning (DRL) policy is vulnerable to backdoor attacks. Existing defenses against backdoor attacks either do not consider RL's unique mechanism or make unrealistic assumptions, resulting in limited defense efficacy, practicability, and generalizability. We propose SHINE, a backdoor shielding method specific for DRL. SHINE designs novel policy explanation techniques to identify the backdoor triggers and a policy retraining algorithm to eliminate the impact of the triggers on backdoored agents. We theoretically justify that SHINE guarantees to improve a backdoored agent's performance in a poisoned environment while ensuring its performance difference in the clean environment before and after shielding is bounded. We further conduct extensive experiments that evaluate SHINE against three mainstream DRL backdoor attacks in various benchmark RL environments. Our results show that SHINE significantly outperforms existing defenses in mitigating these backdoor attacks.",[],,"['Zhuowen Yuan', 'Wenbo Guo', 'Jinyuan Jia', 'Bo Li', 'Dawn Song']","['University of Illinois Urbana-Champaign', 'University of California, Santa Barbara', 'College of IST, Pennsylvania State University', 'CS, University of Illinois, Urbana Champaign', 'University of California Berkeley']",
https://openreview.net/forum?id=oTYuORAMaP,Fairness & Bias,Efficient Stochastic Approximation of Minimax Excess Risk Optimization,"While traditional distributionally robust optimization (DRO) aims to minimize the maximal risk over a set of distributions, Agarwal & Zhang (2022) recently proposed a variant that replaces risk with *excess risk*. Compared to DRO, the new formulation—minimax excess risk optimization (MERO) has the advantage of suppressing the effect of heterogeneous noise in different distributions. However, the choice of excess risk leads to a very challenging minimax optimization problem, and currently there exists only an inefficient algorithm for empirical MERO. In this paper, we develop efficient stochastic approximation approaches which directly target MERO. Specifically, we leverage techniques from stochastic convex optimization to estimate the minimal risk of every distribution, and solve MERO as a stochastic convex-concave optimization (SCCO) problem with biased gradients. The presence of bias makes existing theoretical guarantees of SCCO inapplicable, and fortunately, we demonstrate that the bias, caused by the estimation error of the minimal risk, is under-control. Thus, MERO can still be optimized with a nearly optimal convergence rate. Moreover, we investigate a practical scenario where the quantity of samples drawn from each distribution may differ, and propose a stochastic approach that delivers *distribution-dependent* convergence rates.",[],,"['Lijun Zhang', 'Haomin Bai', 'Wei-Wei Tu', 'Ping Yang', 'Yao Hu']","['', '4Paradigm Inc.', 'Computer Science, Zhejiang University of Technology']",
https://openreview.net/forum?id=CTEMHDSwIj,Fairness & Bias,Understanding Unimodal Bias in Multimodal Deep Linear Networks,"Using multiple input streams simultaneously to train multimodal neural networks is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where a network overly relies on one modality and ignores others during joint training. We develop a theory of unimodal bias with multimodal deep linear networks to understand how architecture and data statistics influence this bias. This is the first work to calculate the duration of the unimodal phase in learning as a function of the depth at which modalities are fused within the network, dataset statistics, and initialization. We show that the deeper the layer at which fusion occurs, the longer the unimodal phase. A long unimodal phase can lead to a generalization deficit and permanent unimodal bias in the overparametrized regime. Our results, derived for multimodal linear networks, extend to nonlinear networks in certain settings. Taken together, this work illuminates pathologies of multimodal learning under joint training, showing that late and intermediate fusion architectures can give rise to long unimodal phases and permanent unimodal bias. Our code is available at: https://yedizhang.github.io/unimodal-bias.html.",[],,"['Yedi Zhang', 'Peter E. Latham', 'Andrew M Saxe']","['', 'University College London', 'University College London, University of London']",
https://openreview.net/forum?id=QgvBcOsF4B,Privacy & Data Governance,Enhancing Storage and Computational Efficiency in Federated Multimodal Learning for Large-Scale Models,"The remarkable generalization of large-scale models has recently gained significant attention in multimodal research. However, deploying heterogeneous large-scale models with different modalities under Federated Learning (FL) to protect data privacy imposes tremendous challenges on clients' limited computation and storage. In this work, we propose M$^2$FedSA to address the above issue. We realize modularized decomposition of large-scale models via Split Learning (SL) and only retain privacy-sensitive modules on clients, alleviating storage overhead. By freezing large-scale models and introducing two specialized lightweight adapters, the models can better focus on task-specific knowledge and enhance modality-specific knowledge, improving the model's adaptability to different tasks while balancing efficiency. In addition, M$^2$FedSA further improves performance by transferring multimodal knowledge to unimodal clients at both the feature and decision levels, which leverages the complementarity of different modalities. Extensive experiments on various multimodal classification tasks validate the effectiveness of our proposed M$^2$FedSA. The code is made available publicly at https://github.com/M2FedSA/M-2FedSA.",[],,"['Zixin Zhang', 'Fan Qi', 'Changsheng Xu']","['School of Computer Science and Engineering, Tianjin University of Technology', 'Tianjin University of Technology', 'NLPR, Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",
https://openreview.net/forum?id=6KtXzUUEp4,Fairness & Bias,Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks,"This paper introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees. Based on the celebrated notion of multicalibration, we introduce $(s,g,\alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional mappings $s$, constraints $g$, and a pre-specified threshold level $\alpha$. We propose associated algorithms to achieve this notion in general settings. This framework is then applied to diverse scenarios encompassing different fairness concerns, including false negative rate control in image segmentation, prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text generation in language models. We conduct numerical studies on several datasets and tasks.",[],,"['Lujing Zhang', 'Aaron Roth', 'Linjun Zhang']","['Peking University', 'Amazon', 'Statistics, Rutgers University']",
https://openreview.net/forum?id=BrZPj9rEpN,Fairness & Bias,Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics,"Developing policies that can adapt to non-stationary environments is essential for real-world reinforcement learning applications. Nevertheless, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called debiased offline representation learning for fast online adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle. Our experimental evaluation across six benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance.",[],,"['Xinyu Zhang', 'Wenjie Qiu', 'Yi-Chen Li', 'Lei Yuan', 'Chengxing Jia', 'Zongzhang Zhang', 'Yang Yu']","['Nanjing University', 'nanjing university', 'School of Artificial Intelligence, Nanjing University', 'AI, Nanjing University', 'Nanjing University', 'School of Artificial Intelligence, Nanjing University', 'School of Artificial Intelligence, Nanjing University']",
https://openreview.net/forum?id=cVp8blEw2i,Fairness & Bias,FESSNC: Fast Exponentially Stable and Safe Neural Controller,"In order to stabilize nonlinear systems modeled by stochastic differential equations, we design a Fast Exponentially Stable and Safe Neural Controller (FESSNC) for fast learning controllers. Our framework is parameterized by neural networks, and realizing both rigorous exponential stability and safety guarantees. Concretely, we design heuristic methods to learn the exponentially stable and the safe controllers, respectively, in light of the classical theory of stochastic exponential stability and our established theorem on guaranteeing the almost-sure safety for stochastic dynamics. More significantly, to rigorously ensure the stability and the safety guarantees for the learned controllers, we develop a projection operator, projecting to the space of exponentially-stable and safe controllers. To reduce the highly computational cost for solving the projection operation, approximate projection operators are delicately proposed with closed forms that map the learned controllers to the target controller space. Furthermore, we employ Hutchinson's trace estimator for a scalable unbiased estimate of the Hessian matrix that is used in the projection operator, which thus allows for reducing computational cost and, therefore, can accelerate the training and testing processes. More importantly, our approximate projection operations are applicable to the nonparametric control methods, improving their stability and safety performance. We empirically demonstrate the superiority of the FESSNC over the existing methods.",[],,"['Jingdong Zhang', 'Luan Yang', 'Qunxi Zhu', 'Wei Lin']","['Department of Mathematics, Fudan University', 'IICS, Fudan University', 'Fudan University', 'Fudan University']",
https://openreview.net/forum?id=4mU6LNMaIu,Security,"GroupCover: A Secure, Efficient and Scalable Inference Framework for On-device Model Protection based on TEEs","Due to the high cost of training DNN models, how to protect the intellectual property of DNN models, especially when the models are deployed to users' devices, is becoming an important topic. One practical solution is to use Trusted Execution Environments (TEEs) and researchers have proposed various model obfuscation solutions to make full use of the high-security guarantee of TEEs and the high performance of collocated GPUs. In this paper, we first identify a common vulnerability, namely the fragility of randomness, that is shared by existing TEE-based model obfuscation solutions. This vulnerability benefits model-stealing attacks and allows the adversary to recover about 97% of the secret model. To improve the security of TEE-shielded DNN models, we further propose a new model obfuscation approach GroupCover, which uses sufficient randomization and mutual covering obfuscation to protect model weights. Experimental results demonstrate that GroupCover can achieve a comparable security level as the upper-bound (black-box protection), which is remarkably over 3x compared with existing solutions. Besides, GroupCover introduces 19% overhead and negligible accuracy loss compared to model unprotected scheme.",[],,"['Zheng Zhang', 'Na Wang', 'Ziqi Zhang', 'Yao Zhang', 'Tianyi Zhang', 'Jianwei Liu', 'Ye Wu']","['ByteDance Inc.', 'Computer Science, University of Illinois at Urbana-Champaign', 'School of Cyber Science and Technology, Beihang University', 'ByteDance Inc.']",
https://openreview.net/forum?id=mXLcbRBA8v,Transparency & Explainability,"Beyond the ROC Curve: Classification Trees Using Cost-Optimal Curves, with Application to Imbalanced Datasets","Important applications such as fraud or spam detection or churn prediction involve binary classification problems where the datasets are imbalanced and the cost of false positives greatly differs from the cost of false negatives. We focus on classification trees, in particular oblique trees, which subsume both the traditional axis-aligned trees and logistic regression, but are more accurate than both while providing interpretable models. Rather than using ROC curves, we advocate a loss based on minimizing the false negatives subject to a maximum false positive rate, which we prove to be equivalent to minimizing a weighted 0/1 loss. This yields a curve of classifiers that provably dominates the ROC curve, but is hard to optimize due to the 0/1 loss. We give the first algorithm that can iteratively update the tree parameters globally so that the weighted 0/1 loss decreases monotonically. Experiments on various datasets with class imbalance or class costs show this indeed dominates ROC-based classifiers and significantly improves over previous approaches to learn trees based on weighted purity criteria or over- or undersampling.",[],,"['Magzhan Gabidolla', 'Arman Zharmagambetov', 'Miguel Á. Carreira-Perpiñán']","['', 'Meta AI (FAIR)', 'Dept. of Computer Science and Engineering, University of California, Merced']",
https://openreview.net/forum?id=v2o9rRJcEv,Fairness & Bias,Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases,"Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify a mismatch between current methods and the temporal inductive bias inherent in the multi-step denoising process of diffusion models, as a potential source of reward overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against reward overoptimization while active neurons reflect primacy bias. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Reset (TDPO-R), a policy gradient algorithm that exploits the temporal inductive bias of diffusion models and mitigates the primacy bias stemming from active neurons. Empirical results demonstrate the superior efficacy of our methods in mitigating reward overoptimization. Code is avaliable at https://github.com/ZiyiZhang27/tdpo.",[],,"['Ziyi Zhang', 'Sen Zhang', 'Yibing Zhan', 'Yong Luo', 'Yonggang Wen', 'Dacheng Tao']","['School of Computer Science, Wuhan University', 'Trust and Safety Team, ByteDance Inc.', 'JD Explore Academy, JD Explore Academy', 'Wuhan University', 'Nanyang Technological University', '']",
https://openreview.net/forum?id=Ljhrv1Wmbr,Fairness & Bias,Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize,"Learning representations that generalize under distribution shifts is critical for building robust machine learning models. However, despite significant efforts in recent years, algorithmic advances in this direction have been limited. In this work, we seek to understand the fundamental difficulty of out-of-distribution generalization with deep neural networks. We first empirically show that perhaps surprisingly, even allowing a neural network to explicitly fit the representations obtained from a teacher network that can generalize out-of-distribution is insufficient for the generalization of the student network. Then, by a theoretical study of two-layer ReLU networks optimized by stochastic gradient descent (SGD) under a structured feature model, we identify a fundamental yet unexplored feature learning proclivity of neural networks, feature contamination: neural networks can learn uncorrelated features together with predictive features, resulting in generalization failure under distribution shifts. Notably, this mechanism essentially differs from the prevailing narrative in the literature that attributes the generalization failure to spurious correlations. Overall, our results offer new insights into the non-linear feature learning dynamics of neural networks and highlight the necessity of considering inductive biases in out-of-distribution generalization.",[],,"['Tianren Zhang', 'Chujie Zhao', 'Guanyu Chen', 'Yizhou Jiang', 'Feng Chen']","['Automation, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Department of Automation, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=tmUorldOWN,Privacy & Data Governance,Rethinking Adversarial Robustness in the Context of the Right to be Forgotten,"The past few years have seen an intense research interest in the practical needs of the ""right to be forgotten"", which has motivated researchers to develop machine unlearning methods to unlearn a fraction of training data and its lineage. While existing machine unlearning methods prioritize the protection of individuals' private data, they overlook investigating the unlearned models' susceptibility to adversarial attacks and security breaches. In this work, we uncover a novel security vulnerability of machine unlearning based on the insight that adversarial vulnerabilities can be bolstered, especially for adversarially robust models. To exploit this observed vulnerability, we propose a novel attack called Adversarial Unlearning Attack (AdvUA), which aims to generate a small fraction of malicious unlearning requests during the unlearning process. AdvUA causes a significant reduction of adversarial robustness in the unlearned model compared to the original model, providing an entirely new capability for adversaries that is infeasible in conventional machine learning pipelines. Notably, we also show that AdvUA can effectively enhance model stealing attacks by extracting additional decision boundary information, further emphasizing the breadth and significance of our research. We also conduct both theoretical analysis and computational complexity of AdvUA. Extensive numerical studies are performed to demonstrate the effectiveness and efficiency of the proposed attack.",[],,"['Chenxu Zhao', 'Wei Qian', 'Yangyi Li', 'Aobo Chen', 'Mengdi Huai']","['Computer science , Iowa State University', 'Department of Computer Science, Iowa State University', 'Iowa State University', 'Computer Science, Iowa State University', 'Computer Science, Iowa State University']",
https://openreview.net/forum?id=jsKr6RVDDs,Fairness & Bias,"Position: Measure Dataset Diversity, Don't Just Claim It","Machine learning (ML) datasets, often perceived as neutral, inherently encapsulate abstract and disputed social constructs. Dataset curators frequently employ value-laden terms such as diversity, bias, and quality to characterize datasets. Despite their prevalence, these terms lack clear definitions and validation. Our research explores the implications of this issue by analyzing ""diversity"" across 135 image and text datasets. Drawing from social sciences, we apply principles from measurement theory to identify considerations and offer recommendations for conceptualizing, operationalizing, and evaluating diversity in datasets. Our findings have broader implications for ML research, advocating for a more nuanced and precise approach to handling value-laden properties in dataset construction.",[],,"['Dora Zhao', 'Jerone Andrews', 'Orestis Papakyriakopoulos', 'Alice Xiang']","['Stanford University', 'Sony AI', 'Technische Universität München', '']",
https://openreview.net/forum?id=nd47Za5jk5,Fairness & Bias,Graph-based Time Series Clustering for End-to-End Hierarchical Forecasting,"Relationships among time series can be exploited as inductive biases in learning effective forecasting models. In hierarchical time series, relationships among subsets of sequences induce hard constraints (hierarchical inductive biases) on the predicted values. In this paper, we propose a graph-based methodology to unify relational and hierarchical inductive biases in the context of deep learning for time series forecasting. In particular, we model both types of relationships as dependencies in a pyramidal graph structure, with each pyramidal layer corresponding to a level of the hierarchy. By exploiting modern - trainable - graph pooling operators we show that the hierarchical structure, if not available as a prior, can be learned directly from data, thus obtaining cluster assignments aligned with the forecasting objective. A differentiable reconciliation stage is incorporated into the processing architecture, allowing hierarchical constraints to act both as an architectural bias as well as a regularization element for predictions. Simulation results on representative datasets show that the proposed method compares favorably against the state of the art.",[],,"['Andrea Cini', 'Danilo Mandic', 'Cesare Alippi']","['University of Oxford', 'Imperial College London', 'Università della Svizzera Italiana']",
https://openreview.net/forum?id=ugxGpOEkox,Security,On Prompt-Driven Safeguarding for Large Language Models,"Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs' behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a ""higher-refusal"" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries' representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models' general performance.",[],,"['Chujie Zheng', 'Fan Yin', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Kai-Wei Chang', 'Minlie Huang', 'Nanyun Peng']","['Computer Science and Technology, Tsinghua University', 'University of California, Los Angeles', 'Tencent, Wechat AI', 'WeChat AI, Tencent Inc.', 'WeChat AI, WeChat AI, Tencent Inc.', 'University of California, Los Angeles', 'Tsinghua University, Tsinghua University', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=CD2xl1L5es,Fairness & Bias,Pedestrian Attribute Recognition as Label-balanced Multi-label Learning,"Rooting in the scarcity of most attributes, realistic pedestrian attribute datasets exhibit unduly skewed data distribution, from which two types of model failures are delivered: (1) label imbalance: model predictions lean greatly towards the side of majority labels; (2) semantics imbalance: model is easily overfitted on the under-represented attributes due to their insufficient semantic diversity. To render perfect label balancing, we propose a novel framework that successfully decouples label-balanced data re-sampling from the curse of attributes co-occurrence, i.e., we equalize the sampling prior of an attribute while not biasing that of the co-occurred others. To diversify the attributes semantics and mitigate the feature noise, we propose a Bayesian feature augmentation method to introduce true in-distribution novelty. Handling both imbalances jointly, our work achieves best accuracy on various popular benchmarks, and importantly, with minimal computational budget.",[],,"['Yibo Zhou', 'Hai-Miao Hu', 'Yirong Xiang', 'Xiaokang Zhang', 'Haotian Wu']","['CS, Beijing University of Aeronautics and Astronautics', 'Beijing University of Aeronautics and Astronautics', 'University of Manchester', 'School of Computer Science and Engineering, Beijing University of Aeronautics and Astronautics', 'School of Computer Science and Engineering, Beijing University of Aeronautics and Astronautics']",
https://openreview.net/forum?id=gKPkipJ3gm,Transparency & Explainability,Causal-IQA: Towards the Generalization of Image Quality Assessment Based on Causal Inference,"Due to the high cost of Image Quality Assessment (IQA) datasets, achieving robust generalization remains challenging for prevalent deep learning-based IQA methods. To address this, this paper proposes a novel end-to-end blind IQA method: Causal-IQA. Specifically, we first analyze the causal mechanisms in IQA tasks and construct a causal graph to understand the interplay and confounding effects between distortion types, image contents, and subjective human ratings. Then, through shifting the focus from correlations to causality, Causal-IQA aims to improve the estimation accuracy of image quality scores by mitigating the confounding effects using a causality-based optimization strategy. This optimization strategy is implemented on the sample subsets constructed by a Counterfactual Division process based on the Backdoor Criterion. Extensive experiments illustrate the superiority of Causal-IQA.",[],,"['Yan Zhong', 'Xingyu Wu', 'Li Zhang', 'Chenxi Yang', 'Tingting Jiang']","['School of Mathematical Sciences, Peking University', 'DSAI, Hong Kong Polytechnic University', '', 'Peking University', 'School of Computer Science, Peking University']",
https://openreview.net/forum?id=18rzx2PXKm,Fairness & Bias,Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning,"Reinforcement learning with multiple, potentially conflicting objectives is pervasive in real-world applications, while this problem remains theoretically under-explored. This paper tackles the multi-objective reinforcement learning (MORL) problem and introduces an innovative actor-critic algorithm named MOAC which finds a policy by iteratively making trade-offs among conflicting reward signals. Notably, we provide the first analysis of finite-time Pareto-stationary convergence and corresponding sample complexity in both discounted and average reward settings. Our approach has two salient features: (a) MOAC mitigates the cumulative estimation bias resulting from finding an optimal common gradient descent direction out of stochastic samples. This enables provable convergence rate and sample complexity guarantees independent of the number of objectives; (b) With proper momentum coefficient, MOAC initializes the weights of individual policy gradients using samples from the environment, instead of manual initialization. This enhances the practicality and robustness of our algorithm. Finally, experiments conducted on a real-world dataset validate the effectiveness of our proposed method.",[],,"['Tianchen Zhou', 'FNU Hairi', 'Haibo Yang', 'Jia Liu', 'Tian Tong', 'Fan Yang', 'Michinari Momma', 'Yan Gao']","['Amazon', 'University of Wisconsin - Whitewater', 'Rochester Institute of Technology', 'Electrical and Computer Engineering, The Ohio State University', 'Amazon', 'International Machine Learning, Amazon', 'Amazon', 'Amazon']",
https://openreview.net/forum?id=pBTLGM9uWx,Security,RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation,"Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle, resulting in suboptimal attack performance. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, Neural Renderer Plus (NRP), which can accurately project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA consistently outperforms existing methods in both simulation and real-world settings.",[],,"['Jiawei Zhou', 'Linye Lyu', 'Daojing He', 'YU LI']","['The School of Computer Science and Technology, Harbin Institute of Technology (ShenZhen)', 'Harbin Institute of Technology', 'School of Computer Science and Technology, Harbin Institute of Technology', 'Computer Science and Technology, Harbin Institute of Technology (Shen Zhen)']",
https://openreview.net/forum?id=5ToHnqYxjB,Transparency & Explainability,Iterative Search Attribution for Deep Neural Networks,"Deep neural networks (DNNs) have achieved state-of-the-art performance across various applications. However, ensuring the reliability and trustworthiness of DNNs requires enhanced interpretability of model inputs and outputs. As an effective means of Explainable Artificial Intelligence (XAI) research, the interpretability of existing attribution algorithms varies depending on the choice of reference point, the quality of adversarial samples, or the applicability of gradient constraints in specific tasks. To thoroughly explore the attribution integration paths, in this paper, inspired by the iterative generation of high-quality samples in the diffusion model, we propose an Iterative Search Attribution (ISA) method. To enhance attribution accuracy, ISA distinguishes the importance of samples during gradient ascent and descent, while clipping the relatively unimportant features in the model. Specifically, we introduce a scale parameter during the iterative process to ensure the features in next iteration are always more significant than those in current iteration. Comprehensive experimental results show that our method has superior interpretability in image recognition tasks compared with state-of-the-art baselines. Our code is available at: https://github.com/LMBTough/ISA",[],,"['Zhiyu Zhu', 'Huaming Chen', 'Xinyi Wang', 'Jiayu Zhang', 'Zhibo Jin', 'Jason Xue', 'Jun Shen']","['University of Sydney', '', 'Faculty of Computer Science & Information Technology, Universiti Malaya', 'Suzhou Yierqi', 'University of Technology Sydney', '', 'IT, University of Wollongong']",
https://openreview.net/forum?id=C0sGIO2MZN,Privacy & Data Governance,Toward Availability Attacks in 3D Point Clouds,"Despite the great progress of 3D vision, data privacy and security issues in 3D deep learning are not explored systematically. In the domain of 2D images, many availability attacks have been proposed to prevent data from being illicitly learned by unauthorized deep models. However, unlike images represented on a fixed dimensional grid, point clouds are characterized as unordered and unstructured sets, posing a significant challenge in designing an effective availability attack for 3D deep learning. In this paper, we theoretically show that extending 2D availability attacks directly to 3D point clouds under distance regularization is susceptible to the degeneracy, rendering the generated poisons weaker or even ineffective. This is because in bi-level optimization, introducing regularization term can result in update directions out of control. To address this issue, we propose a novel Feature Collision Error-Minimization (FC-EM) method, which creates additional shortcuts in the feature space, inducing different update directions to prevent the degeneracy of bi-level optimization. Moreover, we provide a theoretical analysis that demonstrates the effectiveness of the FC-EM attack. Extensive experiments on typical point cloud datasets, 3D intracranial aneurysm medical dataset, and 3D face dataset verify the superiority and practicality of our approach.",[],,"['Yifan Zhu', 'Yibo Miao', 'Yinpeng Dong', 'Xiao-Shan Gao']","['Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Tsinghua University, Tsinghua University', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences']",
https://openreview.net/forum?id=H5FDHzrWe2,Security,Stealthy Imitation: Reward-guided Environment-free Policy Stealing,"Deep reinforcement learning policies, which are integral to modern control systems, represent valuable intellectual property. The development of these policies demands considerable resources, such as domain expertise, simulation fidelity, and real-world validation. These policies are potentially vulnerable to model stealing attacks, which aim to replicate their functionality using only black-box access. In this paper, we propose Stealthy Imitation, the first attack designed to steal policies without access to the environment or knowledge of the input range. This setup has not been considered by previous model stealing methods. Lacking access to the victim's input states distribution, Stealthy Imitation fits a reward model that allows to approximate it. We show that the victim policy is harder to imitate when the distribution of the attack queries matches that of the victim. We evaluate our approach across diverse, high-dimensional control tasks and consistently outperform prior data-free approaches adapted for policy stealing. Lastly, we propose a countermeasure that significantly diminishes the effectiveness of the attack.",[],,"['Zhixiong Zhuang', 'Maria-Irina Nicolae', 'Mario Fritz']","['Universität des Saarlandes', 'Corporate Research, Robert Bosch GmbH', 'CISPA Helmholtz Center for Information Security']",
https://openreview.net/forum?id=t4908PyZxs,Transparency & Explainability,Compositional Few-Shot Class-Incremental Learning,"Few-shot class-incremental learning (FSCIL) is proposed to continually learn from novel classes with only a few samples after the (pre-)training on base classes with sufficient data. However, this remains a challenge. In contrast, humans can easily recognize novel classes with a few samples. Cognitive science demonstrates that an important component of such human capability is compositional learning. This involves identifying visual primitives from learned knowledge and then composing new concepts using these transferred primitives, making incremental learning both effective and interpretable. To imitate human compositional learning, we propose a cognitive-inspired method for the FSCIL task. We define and build a compositional model based on set similarities, and then equip it with a primitive composition module and a primitive reuse module. In the primitive composition module, we propose to utilize the Centered Kernel Alignment (CKA) similarity to approximate the similarity between primitive sets, allowing the training and evaluation based on primitive compositions. In the primitive reuse module, we enhance primitive reusability by classifying inputs based on primitives replaced with the closest primitives from other classes. Experiments on three datasets validate our method, showing it outperforms current state-of-the-art methods with improved interpretability. Our code is available at https://github.com/Zoilsen/Comp-FSCIL.",[],,"['Yixiong Zou', 'Shanghang Zhang', 'haichen zhou', 'Yuhua Li', 'Ruixuan Li']","['', 'Peking University', 'Huazhong University of Science and Technology', 'School of Computer Science and Technology, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology']",
https://openreview.net/forum?id=9HPoJ6ulgV,Privacy & Data Governance,Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption,"Designing privacy-preserving DL solutions is a major challenge within the AI community. Homomorphic Encryption (HE) has emerged as one of the most promising approaches in this realm, enabling the decoupling of knowledge between a model owner and a data owner. Despite extensive research and application of this technology, primarily in CNNs, applying HE on transformer models has been challenging because of the difficulties in converting these models into a polynomial form. We break new ground by introducing the first polynomial transformer, providing the first demonstration of secure inference over HE with full transformers. This includes a transformer architecture tailored for HE, alongside a novel method for converting operators to their polynomial equivalent. This innovation enables us to perform secure inference on LMs and ViTs with several datasts and tasks. Our techniques yield results comparable to traditional models, bridging the performance gap with transformers of similar scale and underscoring the viability of HE for state-of-the-art applications. Finally, we assess the stability of our models and conduct a series of ablations to quantify the contribution of each model component. Our code is publicly available.",[],,"['Itamar Zimerman', 'Moran Baruch', 'Nir Drucker', 'Gilad Ezov', 'Omri Soceanu', 'Lior Wolf']","['Tel Aviv University', 'International Business Machines', 'International Business Machines', 'International Business Machines', '', 'Tel Aviv University']",
https://openreview.net/forum?id=nOyj26YdIQ,Fairness & Bias,Viewing Transformers Through the Lens of Long Convolutions Layers,"Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of longrange layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal modifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our theory and experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies.",[],,"['Itamar Zimerman', 'Lior Wolf']","['Tel Aviv University', 'Tel Aviv University']",
https://openreview.net/forum?id=bWZKvF0g7G,Security,Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models,"Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several black-box adversarial attacks, which approach zero in many cases. The code and dataset will be open-sourced.",[],,"['Yongshuo Zong', 'Ondrej Bohdal', 'Tingyang Yu', 'Yongxin Yang', 'Timothy Hospedales']","['University of Edinburgh', 'Samsung Research', 'EPFL - EPF Lausanne', 'Queen Mary University of London', 'University of Edinburgh']",
https://openreview.net/forum?id=f8G2KSCSdp,Fairness & Bias,Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models,"Fine-tuning the learnable prompt for a pre-trained vision-language model (VLM), such as CLIP, has demonstrated exceptional efficiency in adapting to a broad range of downstream tasks. Existing prompt tuning methods for VLMs do not distinguish spurious features introduced by biased training data from invariant features, and employ a uniform alignment process when adapting to unseen target domains. This can impair the cross-modal feature alignment when the testing data significantly deviate from the distribution of the training data, resulting in a poor out-of-distribution (OOD) generalization performance. In this paper, we reveal that the prompt tuning failure in such OOD scenarios can be attribute to the undesired alignment between the textual and the spurious feature. As a solution, we propose **CoOPood**, a fine-grained prompt tuning method that can discern the causal features and deliberately align the text modality with the invariant feature. Specifically, we design two independent contrastive phases using two lightweight projection layers during the alignment, each with different objectives: 1) pulling the text embedding closer to invariant image embedding and 2) pushing text embedding away from spurious image embedding. We have illustrated that **CoOPood** can serve as a general framework for VLMs and can be seamlessly integrated with existing prompt tuning methods. Extensive experiments on various OOD datasets demonstrate the performance superiority over state-of-the-art methods.",[],,"['Jie ZHANG', 'Xiaosong Ma', 'Song Guo', 'Peng Li', 'Wenchao Xu', 'Xueyang Tang', 'Zicong Hong']","['Computer Science and Engineering, Hong Kong University of Science and Technology', 'Hong Kong Polytechnic University', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'University of Aizu', '', 'The Hong Kong Polytechnic University', 'Hong Kong Polytechnic University']",
https://openreview.net/forum?id=AZWqXfM6z9,Security,Revisiting Character-level Adversarial Attacks for Language Models,"Adversarial attacks in Natural Language Processing apply perturbations in the character or token levels. Token-level attacks, gaining prominence for their use of gradient-based methods, are susceptible to altering sentence semantics, leading to invalid adversarial examples. While character-level attacks easily maintain semantics, they have received less attention as they cannot easily adopt popular gradient-based methods, and are thought to be easy to defend. Challenging these beliefs, we introduce Charmer, an efficient query-based adversarial attack capable of achieving high attack success rate (ASR) while generating highly similar adversarial examples. Our method successfully targets both small (BERT) and large (Llama 2) models. Specifically, on BERT with SST-2, Charmer improves the ASR in $4.84$% points and the USE similarity in $8$% points with respect to the previous art. Our implementation is available in https://github.com/LIONS-EPFL/Charmer.",[],,"['Elias Abad Rocamora', 'Yongtao Wu', 'Fanghui Liu', 'Grigorios Chrysos', 'Volkan Cevher']","['EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'Department of Computer Science, University of Warwick', 'University of Wisconsin - Madison', 'EPFL - EPF Lausanne']",
https://openreview.net/forum?id=rU8o0QQCy0,Fairness & Bias,Position: Is machine learning good or bad for the natural sciences?,"Machine learning (ML) methods are having a huge impact across all of the sciences. However, ML has a strong ontology — in which only the data exist — and a strong epistemology — in which a model is considered good if it performs well on held-out training data. These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences. Here we identify some locations for ML in the natural sciences at which the ontology and epistemology are valuable. For example, when an expressive machine learning model is used in a causal inference to represent the effects of confounders, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of ML can make the results more trustworthy. We also show that there are contexts in which the introduction of ML introduces strong, unwanted statistical biases. For one, when ML models are used to emulate physical (or first-principles) simulations, they amplify confirmation biases. For another, when expressive regressions are used to label datasets, those labels cannot be used in downstream joint or ensemble analyses without taking on uncontrolled biases. The question in the title is being asked of all of the natural sciences; that is, we are calling on the scientific communities to take a step back and consider the role and value of ML in their fields; the (partial) answers we give here come from the particular perspective of physics.",[],,"['David W Hogg', 'Soledad Villar']","['Flatiron Institute', 'Johns Hopkins University']",
https://openreview.net/forum?id=usUPvQH3XK,Fairness & Bias,Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game,"Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop *strategic language agents*, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.",[],,"['Zelai Xu', 'Chao Yu', 'Fei Fang', 'Yu Wang', 'Yi Wu']","['Tsinghua University', 'Tsinghua University, Tsinghua University', 'Carnegie Mellon University', 'Tsinghua University, Tsinghua University', 'Tsinghua University']",
https://openreview.net/forum?id=pVyOchWUBa,Fairness & Bias,Position: Understanding LLMs Requires More Than Statistical Generalization,"The last decade has seen blossoming research in deep learning theory attempting to answer, ``Why does deep learning generalize?"" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart---thus, equivalent test loss---can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.",[],,"['Patrik Reizinger', 'Szilvia Ujváry', 'Anna Mészáros', 'Anna Kerekes', 'Wieland Brendel', 'Ferenc Huszár']","['Eberhard-Karls-Universität Tübingen', 'University of Cambridge', 'University of Cambridge', 'ETHZ - ETH Zurich', 'ELLIS Institute Tübingen', 'University of Cambridge']",
https://openreview.net/forum?id=CbIZatwz9z,Security,Online Isolation Forest,"The anomaly detection literature is abundant with offline methods, which require repeated access to data in memory, and impose impractical assumptions when applied to a streaming context. Existing online anomaly detection methods also generally fail to address these constraints, resorting to periodic retraining to adapt to the online context. We propose Online-iForest, a novel method explicitly designed for streaming conditions that seamlessly tracks the data generating process as it evolves over time. Experimental validation on real-world datasets demonstrated that Online-iForest is on par with online alternatives and closely rivals state-of-the-art offline anomaly detection techniques that undergo periodic retraining. Notably, Online-iForest consistently outperforms all competitors in terms of efficiency, making it a promising solution in applications where fast identification of anomalies is of primary importance such as cybersecurity, fraud and fault detection.",[],,"['Filippo Leveni', 'Guilherme Weigert Cassales', 'Bernhard Pfahringer', 'Albert Bifet', 'Giacomo Boracchi']","['Department of Electronics, Information and Bioengineering, Politecnico di Milano', 'University of Waikato', 'The University of Waikato', 'AI Institute, The University of Waikato', 'DEIB, Polytechnic Institute of Milan']",
https://openreview.net/forum?id=3MfvxH3Gia,Transparency & Explainability,Multimodal Prototyping for cancer survival prediction,"Multimodal survival methods combining gigapixel histology whole-slide images (WSIs) and transcriptomic profiles are particularly promising for patient prognostication and stratification. Current approaches involve tokenizing the WSIs into smaller patches ($>10^4$ patches) and transcriptomics into gene groups, which are then integrated using a Transformer for predicting outcomes. However, this process generates many tokens, which leads to high memory requirements for computing attention and complicates post-hoc interpretability analyses. Instead, we hypothesize that we can: (1) effectively summarize the morphological content of a WSI by condensing its constituting tokens using morphological prototypes, achieving more than $300\times$ compression; and (2) accurately characterize cellular functions by encoding the transcriptomic profile with biological pathway prototypes, all in an unsupervised fashion. The resulting multimodal tokens are then processed by a fusion network, either with a Transformer or an optimal transport cross-alignment, which now operates with a small and fixed number of tokens without approximations. Extensive evaluation on six cancer types shows that our framework outperforms state-of-the-art methods with much less computation while unlocking new interpretability analyses. The code is available at https://github.com/mahmoodlab/MMP.",[],,"['Andrew H. Song', 'Richard J. Chen', 'Guillaume Jaume', 'Anurag Jayant Vaidya', 'Alexander Baras', 'Faisal Mahmood']","[""Brigham and Women's hospital"", 'Modella AI', 'Pathology, Harvard University', 'Health Sciences Technology, Massachusetts Institute of Technology', 'School of Medicine, Johns Hopkins University', 'Harvard University']",
https://openreview.net/forum?id=JA6ThxAmth,Transparency & Explainability,Understanding Inter-Concept Relationships in Concept-Based Models,"Concept-based explainability methods provide insight into deep learning systems by constructing explanations using human-understandable concepts. While the literature on human reasoning demonstrates that we exploit relationships between concepts when solving tasks, it is unclear whether concept-based methods incorporate the rich structure of inter-concept relationships. We analyse the concept representations learnt by concept-based models to understand whether these models correctly capture inter-concept relationships. First, we empirically demonstrate that state-of-the-art concept-based models produce representations that lack stability and robustness, and such methods fail to capture inter-concept relationships. Then, we develop a novel algorithm which leverages inter-concept relationships to improve concept intervention accuracy, demonstrating how correctly capturing inter-concept relationships can improve downstream tasks.",[],,"['Naveen Janaki Raman', 'Mateo Espinosa Zarlenga', 'Mateja Jamnik']","['Machine Learning, Carnegie Mellon University', 'University of Cambridge', 'Computer Science and Technology, University of Cambridge']",
https://openreview.net/forum?id=zc3bAEI5lp,Privacy & Data Governance,Differentially Private Sum-Product Networks,"Differentially private ML approaches seek to learn models which may be publicly released while guaranteeing that the input data is kept private. One issue with this construction is that further model releases based on the same training data (e.g. for a new task) incur a further privacy budget cost. Privacy-preserving synthetic data generation is one possible solution to this conundrum. However, models trained on synthetic private data struggle to approach the performance of private, ad-hoc models. In this paper, we present a novel method based on sum-product networks that is able to perform both privacy-preserving classification and privacy-preserving data generation with a single model. To the best of our knowledge, ours is the first approach that provides both discriminative and generative capabilities to differentially private ML. We show that our approach outperforms the state of the art in terms of stability (i.e. number of training runs required for convergence) and utility of the generated data.",[],,"['Xenia Heilmann', 'Mattia Cerrato', 'Ernst Althaus']","['Institute of Computer Science, Johannes-Gutenberg Universität Mainz', 'University of Mainz']",
https://openreview.net/forum?id=3hSTecKy1b,Transparency & Explainability,"Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?","New capabilities in foundation models are owed in large part to massive, widely-sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving privacy, addressing representation and bias, respecting copyright, and overall developing ethical and trustworthy foundation models. In response, regulation is emphasizing the need for training data transparency to understand foundation models’ limitations. Based on a large-scale analysis of the foundation model training data landscape and existing solutions, we identify the missing infrastructure to facilitate responsible foundation model development practices. We examine the current shortcomings of common tools for tracing data authenticity, consent, and documentation, and outline how policymakers, developers, and data creators can facilitate responsible foundation model development by adopting universal data provenance standards.",[],,"['Shayne Longpre', 'Robert Mahari', 'Naana Obeng-Marnu', 'William Brannon', 'Tobin South', 'Katy Ilonka Gero', 'Alex Pentland', 'Jad Kabbara']","['Massachusetts Institute of Technology', 'Harvard University', 'MIT Center for Constructive Communication / MIT Media Lab, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'School of Engineering and Applied Sciences, Harvard University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=gn5AsHIIwb,Security,StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation,"WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.",[],,"['Weike Fang', 'Zhejian Zhou', 'Junzhou He', 'Weihang Wang']","['Computer Science, University of Southern California', '', 'Computer Science, University of Southern California', '']",
https://openreview.net/forum?id=XMlUlY7ONf,Transparency & Explainability,From Neurons to Neutrons: A Case Study in Interpretability,"Mechanistic Interpretability (MI) proposes a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? Here, we argue that high-dimensional neural networks can learn *useful* low-dimensional representations of the data they were trained on, going beyond simply making good predictions: Such representations can be understood with the MI lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data.",[],,"['Ouail Kitouni', 'Niklas Nolte', 'Víctor Samuel Pérez-Díaz', 'Sokratis Trifinopoulos', 'Mike Williams']","['Massachusetts Institute of Technology', 'Facebook', 'Universidad del Rosario', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=SfcB4cVvPz,Security,A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks,"The rising threat of backdoor poisoning attacks (BPAs) on Deep Neural Networks (DNNs) has become a significant concern in recent years. In such attacks, the adversaries strategically target a specific class and generate a poisoned training set. The neural network (NN), well-trained on the poisoned training set, is able to predict any input with the trigger pattern as the targeted label, while maintaining accurate outputs for clean inputs. However, why the BPAs work remains less explored. To fill this gap, we employ a dirty-label attack and conduct a detailed analysis of BPAs in a two-layer convolutional neural network. We provide theoretical insights and results on the effectiveness of BPAs. Our experimental results on two real-world datasets validate our theoretical findings.",[],,"['Boqi Li', 'Weiwei Liu']","['School of Computer Science, Wuhan University', 'Wuhan University']",
https://openreview.net/forum?id=44qxX6Ty6F,Fairness & Bias,Position: Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized,"Contrary to traditional deterministic notions of algorithmic fairness, this paper argues that fairly allocating scarce resources using machine learning often requires randomness. We address why, when, and how to randomize by offering a set of stochastic procedures that more adequately account for all of the claims individuals have to allocations of social goods or opportunities and effectively balances their interests.",[],,"['Shomik Jain', 'Kathleen Creel', 'Ashia Camage Wilson']","['Institute for Data, Systems, and Society, Massachusetts Institute of Technology', 'Philosophy, Computer Science, Northeastern University', 'Electrical Engineering and Computer Science, Massachusetts Institute of Technology']",
https://openreview.net/forum?id=07fSWltF6M,Transparency & Explainability,ProtoGate: Prototype-based Neural Networks with Global-to-local Feature Selection for Tabular Biomedical Data,"Tabular biomedical data poses challenges in machine learning because it is often high-dimensional and typically low-sample-size (HDLSS). Previous research has attempted to address these challenges via local feature selection, but existing approaches often fail to achieve optimal performance due to their limitation in identifying globally important features and their susceptibility to the co-adaptation problem. In this paper, we propose ProtoGate, a prototype-based neural model for feature selection on HDLSS data. ProtoGate first selects instance-wise features via adaptively balancing global and local feature selection. Furthermore, ProtoGate employs a non-parametric prototype-based prediction mechanism to tackle the co-adaptation problem, ensuring the feature selection results and predictions are consistent with underlying data clusters. We conduct comprehensive experiments to evaluate the performance and interpretability of ProtoGate on synthetic and real-world datasets. The results show that ProtoGate generally outperforms state-of-the-art methods in prediction accuracy by a clear margin while providing high-fidelity feature selection and explainable predictions. Code is available at https://github.com/SilenceX12138/ProtoGate.",[],,"['Xiangjian Jiang', 'Andrei Margeloiu', 'Nikola Simidjievski', 'Mateja Jamnik']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'Computer Science and Technology, University of Cambridge']",
https://openreview.net/forum?id=4BWCecFEcQ,Privacy & Data Governance,PerceptAnon: Exploring the Human Perception of Image Anonymization Beyond Pseudonymization for GDPR,"Current image anonymization techniques, largely focus on localized pseudonymization, typically modify identifiable features like faces or full bodies and evaluate anonymity through metrics such as detection and re-identification rates. However, this approach often overlooks information present in the entire image post-anonymization that can compromise privacy, such as specific locations, objects/items, or unique attributes. Acknowledging the pivotal role of human judgment in anonymity, our study conducts a thorough analysis of perceptual anonymization, exploring its spectral nature and its critical implications for image privacy assessment, particularly in light of regulations such as the General Data Protection Regulation (GDPR). To facilitate this, we curated a dataset specifically tailored for assessing anonymized images. We introduce a learning-based metric, PerceptAnon, which is tuned to align with the human Perception of Anonymity. PerceptAnon evaluates both original-anonymized image pairs and solely anonymized images. Trained using human annotations, our metric encompasses both anonymized subjects and their contextual backgrounds, thus providing a comprehensive evaluation of privacy vulnerabilities. We envision this work as a milestone for understanding and assessing image anonymization, and establishing a foundation for future research. The codes and dataset are available in https://github.com/SonyResearch/gdpr_perceptanon.",[],,"['Kartik Patwari', 'Chen-Nee Chuah', 'Lingjuan Lyu', 'Vivek Sharma']","['Electrical and Computer Engineering, University of California, Davis', 'Electrical & Computer Engineering, University of California, Davis', 'Sony Research, Sony', 'Sony Research']",
https://openreview.net/forum?id=dHXKCyaIkp,Transparency & Explainability,Deep Functional Factor Models: Forecasting High-Dimensional Functional Time Series via Bayesian Nonparametric Factorization,"This paper introduces the Deep Functional Factor Model (DF2M), a Bayesian nonparametric model designed for analysis of high-dimensional functional time series. DF2M is built upon the Indian Buffet Process and the multi-task Gaussian Process, incorporating a deep kernel function that captures non-Markovian and nonlinear temporal dynamics. Unlike many black-box deep learning models, DF2M offers an explainable approach to utilizing neural networks by constructing a factor model and integrating deep neural networks within the kernel function. Additionally, we develop a computationally efficient variational inference algorithm to infer DF2M. Empirical results from four real-world datasets demonstrate that DF2M provides better explainability and superior predictive accuracy compared to conventional deep learning models for high-dimensional functional time series.",[],,"['Yirui Liu', 'Xinghao Qiao', 'Yulong Pei', 'Liying Wang']","['J.P. Morgan Chase', 'London School of Economics', '', 'University of Liverpool']",
https://openreview.net/forum?id=dztd61efGy,Fairness & Bias,Discovering Bias in Latent Space: An Unsupervised Debiasing Approach,"The question-answering (QA) capabilities of foundation models are highly sensitive to prompt variations, rendering their performance susceptible to superficial, non-meaning-altering changes. This vulnerability often stems from the model's preference or bias towards specific input characteristics, such as option position or superficial image features in multi-modal settings. We propose to rectify this bias directly in the model's internal representation. Our approach, SteerFair, finds the bias direction in the model's representation space and steers activation values away from it during inference. Specifically, we exploit the observation that bias often adheres to simple association rules, such as the spurious association between the first option and correctness likelihood. Next, we construct demonstrations of these rules from unlabeled samples and use them to identify the bias directions. We empirically show that SteerFair significantly reduces instruction-tuned model performance variance across prompt modifications on three benchmark tasks. Remarkably, our approach surpasses a supervised baseline with 100 labels by an average of 10.86% accuracy points and 12.95 score points and matches the performance with 500 labels.",[],,"['Dyah Adila', 'Shuai Zhang', 'Boran Han', 'Bernie Wang']","['University of Wisconsin, Madison', 'Amazon', '', 'AWS AI, AWS AI Labs']",
https://openreview.net/forum?id=9QRcp2ubDt,Fairness & Bias,Centralized Selection with Preferences in the Presence of Biases,"This paper considers the scenario in which there are multiple institutions, each with a limited capacity for candidates, and candidates, each with preferences over the institutions. A central entity evaluates the utility of each candidate to the institutions, and the goal is to select candidates for each institution in a way that maximizes utility while also considering the candidates' preferences. The paper focuses on the setting in which candidates are divided into multiple groups and the observed utilities of candidates in some groups are biased--systematically lower than their true utilities. The first result is that, in these biased settings, prior algorithms can lead to selections with sub-optimal true utility and significant discrepancies in the fraction of candidates from each group that get their preferred choices. Subsequently, an algorithm is presented along with proof that it produces selections that achieve near-optimal group fairness with respect to preferences while also nearly maximizing the true utility under distributional assumptions. Further, extensive empirical validation of these results in real-world and synthetic settings, in which the distributional assumptions may not hold, are presented.",[],,"['L. Elisa Celis', 'Amit Kumar', 'Nisheeth K. Vishnoi', 'Andrew Xu']","['Yale University', 'Indian Institute of Technology Delhi', 'Yale University']",
https://openreview.net/forum?id=SyY7ScNpGL,Fairness & Bias,Rethinking Transformers in Solving POMDPs,"Sequential decision-making algorithms such as reinforcement learning (RL) in real-world scenarios inevitably face environments with partial observability. This paper scrutinizes the effectiveness of a popular architecture, namely Transformers, in Partially Observable Markov Decision Processes (POMDPs) and reveals its theoretical limitations. We establish that regular languages, which Transformers struggle to model, are reducible to POMDPs. This poses a significant challenge for Transformers in learning POMDP-specific inductive biases, due to their lack of inherent recurrence found in other models like RNNs. This paper casts doubt on the prevalent belief in Transformers as sequence models for RL and proposes to introduce a point-wise recurrent structure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited alternative for Partially Observable RL, with empirical results highlighting the sub-optimal performance of the Transformer and considerable strength of LRU.",[],,"['Chenhao Lu', 'Ruizhe Shi', 'Yuyao Liu', 'Kaizhe Hu', 'Simon Shaolei Du', 'Huazhe Xu']","['IIIS, Tsinghua University, Tsinghua University', 'IIIS, Tsinghua University', 'Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University', 'IIIS, Tsinghua University, Tsinghua University', 'University of Washington', 'IIIS, Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=hTiNFCNxM1,Fairness & Bias,From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions,"Selective labels occur when label observations are subject to a decision-making process; e.g., diagnoses that depend on the administration of laboratory tests. We study a clinically-inspired selective label problem called disparate censorship, where labeling biases vary across subgroups and unlabeled individuals are imputed as “negative” (i.e., no diagnostic test = no illness). Machine learning models naively trained on such labels could amplify labeling bias. Inspired by causal models of selective labels, we propose Disparate Censorship Expectation-Maximization (DCEM), an algorithm for learning in the presence of disparate censorship. We theoretically analyze how DCEM mitigates the effects of disparate censorship on model performance. We validate DCEM on synthetic data, showing that it improves bias mitigation (area between ROC curves) without sacrificing discriminative performance (AUC) compared to baselines. We achieve similar results in a sepsis classification task using clinical data.",[],,"['Trenton Chang', 'Jenna Wiens']","['', 'Computer Science and Engineering, University of Michigan Ann Arbor']",
https://openreview.net/forum?id=QvABoVGdRp,Transparency & Explainability,Enhancing Adversarial Robustness in SNNs with Sparse Gradients,"Spiking Neural Networks (SNNs) have attracted great attention for their energy-efficient operations and biologically inspired structures, offering potential advantages over Artificial Neural Networks (ANNs) in terms of energy efficiency and interpretability. Nonetheless, similar to ANNs, the robustness of SNNs remains a challenge, especially when facing adversarial attacks. Existing techniques, whether adapted from ANNs or specifically designed for SNNs, exhibit limitations in training SNNs or defending against strong attacks. In this paper, we propose a novel approach to enhance the robustness of SNNs through gradient sparsity regularization. We observe that SNNs exhibit greater resilience to random perturbations compared to adversarial perturbations, even at larger scales. Motivated by this, we aim to narrow the gap between SNNs under adversarial and random perturbations, thereby improving their overall robustness. To achieve this, we theoretically prove that this performance gap is upper bounded by the gradient sparsity of the probability associated with the true label concerning the input image, laying the groundwork for a practical strategy to train robust SNNs by regularizing the gradient sparsity. We validate the effectiveness of our approach through extensive experiments on both image-based and event-based datasets. The results demonstrate notable improvements in the robustness of SNNs. Our work highlights the importance of gradient sparsity in SNNs and its role in enhancing robustness.",[],,"['Yujia Liu', 'Tong Bu', 'Jianhao Ding', 'Zecheng Hao', 'Tiejun Huang', 'Zhaofei Yu']","['Peking University', '', 'Peking University', 'School of Computer Science, Peking University', 'School of Computer Science, Peking University', 'Peking University']",
https://openreview.net/forum?id=xnQ1qoly7Q,Security,RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis,"Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one embodied navigation task.",[],,"['Yao Mu', 'Junting Chen', 'Qing-Long Zhang', 'Shoufa Chen', 'Qiaojun Yu', 'Chongjian GE', 'Runjian Chen', 'Zhixuan Liang', 'Mengkang Hu', 'Chaofan Tao', 'Peize Sun', 'Haibao Yu', 'Chao Yang', 'Wenqi Shao', 'Wenhai Wang', 'Jifeng Dai', 'Yu Qiao', 'Mingyu Ding', 'Ping Luo']","['Computer Science, The University of Hong Kong', 'SoC, national university of singaore, National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'Department of Computer Science, The University of Hong Kong', 'Shanghai Jiaotong University', 'CS department, The University of Hong Kong', 'Computer Science, University of Hong Kong', 'University of California, Berkeley', 'Computer Science, University of Hong Kong', 'The University of Hong Kong', 'Computer Science, The University of Hong Kong', 'Computer Science, The University of Hong Kong', 'Shanghai AI Lab, Shanghai Artificial Intelligence Laboratory', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Electronic Engineering, Tsinghua University, Tsinghua University', '', 'Department of Computer Science, University of North Carolina at Chapel Hill', 'The University of Hong Kong']",
https://openreview.net/forum?id=5PqzKxmfag,Fairness & Bias,Tilt your Head: Activating the Hidden Spatial-Invariance of Classifiers,"Deep neural networks are applied in more and more areas of everyday life. However, they still lack essential abilities, such as robustly dealing with spatially transformed input signals. Approaches to mitigate this severe robustness issue are limited to two pathways: Either models are implicitly regularised by increased sample variability (data augmentation) or explicitly constrained by hard-coded inductive biases. The limiting factor of the former is the size of the data space, which renders sufficient sample coverage intractable. The latter is limited by the engineering effort required to develop such inductive biases for every possible scenario. Instead, we take inspiration from human behaviour, where percepts are modified by mental or physical actions during inference. We propose a novel technique to emulate such an inference process for neural nets. This is achieved by traversing a sparsified inverse transformation tree during inference using parallel energy-based evaluations. Our proposed inference algorithm, called Inverse Transformation Search (ITS), is model-agnostic and equips the model with zero-shot pseudo-invariance to spatially transformed inputs. We evaluated our method on several benchmark datasets, including a synthesised ImageNet test set. ITS outperforms the utilised baselines on all zero-shot test scenarios.",[],,"['Johann Schmidt', 'Sebastian Stober']","['Otto-von-Guericke-Universität Magdeburg', 'Otto-von-Guericke-University Magdeburg']",
https://openreview.net/forum?id=w4B42sxNq3,Privacy & Data Governance,Recurrent Early Exits for Federated Learning with Heterogeneous Clients,"Federated learning (FL) has enabled distributed learning of a model across multiple clients in a privacy-preserving manner. One of the main challenges of FL is to accommodate clients with varying hardware capacities; clients have differing compute and memory requirements. To tackle this challenge, recent state-of-the-art approaches leverage the use of early exits. Nonetheless, these approaches fall short of mitigating the challenges of joint learning multiple exit classifiers, often relying on hand-picked heuristic solutions for knowledge distillation among classifiers and/or utilizing additional layers for weaker classifiers. In this work, instead of utilizing multiple classifiers, we propose a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier. Specifically, we use a transformer-based early-exit module shared among sub-models to i) better exploit multi-layer feature representations for task-specific prediction and ii) modulate the feature representation of the backbone model for subsequent predictions. We additionally present a per-client self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client. Our experiments on standard image and speech classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL effectiveness over previous works.",[],,"['Royson Lee', 'Javier Fernandez-Marques', 'Shell Xu Hu', 'Da Li', 'Stefanos Laskaridis', 'Łukasz Dudziak', 'Timothy Hospedales', 'Ferenc Huszár', 'Nicholas Donald Lane']","['Samsung AI Center, Cambridge', 'Flower Labs', 'Samsung', 'Samsung AI Center, Cambridge', 'Brave Software', 'Samsung', 'University of Edinburgh', 'University of Cambridge', 'Flower Labs']",
https://openreview.net/forum?id=4HCi7JGCZk,Fairness & Bias,Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection,"This paper explores the size-invariance of evaluation metrics in Salient Object Detection (SOD), especially when multiple targets of diverse sizes co-exist in the same image. We observe that current metrics are size-sensitive, where larger objects are focused, and smaller ones tend to be ignored. We argue that the evaluation should be size-invariant because bias based on size is unjustified without additional semantic information. In pursuit of this, we propose a generic approach that evaluates each salient object separately and then combines the results, effectively alleviating the imbalance. We further develop an optimization framework tailored to this goal, achieving considerable improvements in detecting objects of different sizes. Theoretically, we provide evidence supporting the validity of our new metrics and present the generalization analysis of SOD. Extensive experiments demonstrate the effectiveness of our method.",[],,"['Feiran Li', 'Qianqian Xu', 'Shilong Bao', 'Zhiyong Yang', 'Runmin Cong', 'Xiaochun Cao', 'Qingming Huang']","['Institute of Information Engineering, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'University of the Chinese Academy of Sciences', 'University of Chinese Academic of Sciences', 'Shandong University', 'School of Cyber Science and Technology, SUN YAT-SEN UNIVERSITY', 'University of Chinese Academy of Sciences']",
https://openreview.net/forum?id=GKcwle8XC9,Privacy & Data Governance,In-Context Unlearning: Language Models as Few-Shot Unlearners,"Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the Right to be Forgotten. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",[],,"['Martin Pawelczyk', 'Seth Neel', 'Himabindu Lakkaraju']","['Harvard University', 'Harvard University', 'Harvard University']",
https://openreview.net/forum?id=Xeh8171Fce,Fairness & Bias,On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions,"We investigate the out-of-domain generalization of random feature (RF) models and Transformers. We first prove that in the `generalization on the unseen (GOTU)' setting, where training data is fully seen in some part of the domain but testing is made on another part, and for RF models in the small feature regime, the convergence takes place to interpolators of minimal degree as in the Boolean case (Abbe et al., 2023). We then consider the sparse target regime and explain how this regime relates to the small feature regime, but with a different regularization term that can alter the picture in the non-Boolean case. We show two different outcomes for the sparse regime with q-ary data tokens: (1) if the data is embedded with roots of unities, then a min-degree interpolator is learned like in the Boolean case for RF models, (2) if the data is not embedded as such, e.g., simply as integers, then RF models and Transformers may not learn minimal degree interpolators. This shows that the Boolean setting and its roots of unities generalization are special cases where the minimal degree interpolator offers a rare characterization of how learning takes place. For more general integer and real-valued settings, a more nuanced picture remains to be fully characterized.",[],,"['Denys Pushkin', 'Raphaël Berthier', 'Emmanuel Abbe']","['School of Computer and Communication Sciences, EPFL - EPF Lausanne', 'EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne']",
https://openreview.net/forum?id=TJ6tVNt6Y4,Security,Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining,"Backdoor defense is crucial to ensure the safety and robustness of machine learning models when under attack. However, most existing methods specialize in either the detection or removal of backdoors, but seldom both. While few works have addressed both, these methods rely on strong assumptions or entail significant overhead costs, such as the need of task-specific samples for detection and model retraining for removal. Hence, the key challenge is how to reduce overhead and relax unrealistic assumptions. In this work, we propose two Energy-Based BAckdoor defense methods, called EBBA and EBBA+, that can achieve both backdoored model detection and backdoor removal with low overhead. Our contributions are twofold: First, we offer theoretical analysis for our observation that a predefined target label is more likely to occur among the top results for various samples. Inspired by this, we develop an enhanced energy-based technique, called EBBA, to detect backdoored models without task-specific samples (i.e., samples from any tasks). Secondly, we theoretically analyze that after data corruption, the original clean label of a poisoned sample is more likely to be predicted as a top output by the model, a sharp contrast to clean samples. Accordingly, we extend EBBA to develop EBBA+, a new transferred energy approach to efficiently detect poisoned images and remove backdoors without model retraining. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of our methods over baselines in both backdoor detection and removal. Notably, the proposed methods can effectively detect backdoored model and poisoned images as well as remove backdoors at the same time.",[],,"['Yudong Gao', 'Honglong Chen', 'Peng Sun', 'Zhe Li', 'Junjian Li', 'Huajie Shao']","['Department of Control Science and Engineering, China University of Petroleum', 'College of Control Science and Engineering, China University of Petroleum', 'College of Computer Science and Electronic Engineering, Hunan University', 'College of Control Science and Engineering, China University of Petroleum', 'China University of Petroleum', 'Computer Science, College of William and Mary']",
https://openreview.net/forum?id=FG5hjRBtpm,Transparency & Explainability,Jacobian Regularizer-based Neural Granger Causality,"With the advancement of neural networks, diverse methods for neural Granger causality have emerged, which demonstrate proficiency in handling complex data, and nonlinear relationships. However, the existing framework of neural Granger causality has several limitations. It requires the construction of separate predictive models for each target variable, and the relationship depends on the sparsity on the weights of the first layer, resulting in challenges in effectively modeling complex relationships between variables as well as unsatisfied estimation accuracy of Granger causality. Moreover, most of them cannot grasp full-time Granger causality. To address these drawbacks, we propose a **J**acobian **R**egularizer-based **N**eural **G**ranger **C**ausality (**JRNGC**) approach, a straightforward yet highly effective method for learning multivariate summary Granger causality and full-time Granger causality by constructing a single model for all target variables. Specifically, our method eliminates the sparsity constraints of weights by leveraging an input-output Jacobian matrix regularizer, which can be subsequently represented as the weighted causal matrix in the post-hoc analysis. Extensive experiments show that our proposed approach achieves competitive performance with the state-of-the-art methods for learning summary Granger causality and full-time Granger causality while maintaining lower model complexity and high scalability.",[],,"['Wanqi Zhou', 'Shuanghao Bai', 'Shujian Yu', 'Qibin Zhao', 'Badong Chen']","[""Xi'an Jiaotong University"", ""Artificial Intelligence Academy, Xi'an Jiaotong University"", 'Vrije Universiteit Amsterdam', 'AIP, RIKEN', ""Xi'an Jiaotong University""]",
https://openreview.net/forum?id=5j7Lq2ASiU,Fairness & Bias,Distributed Bilevel Optimization with Communication Compression,"Stochastic bilevel optimization tackles challenges involving nested optimization structures. Its fast-growing scale nowadays necessitates efficient distributed algorithms. In conventional distributed bilevel methods, each worker must transmit full-dimensional stochastic gradients to the server every iteration, leading to significant communication overhead and thus hindering efficiency and scalability. To resolve this issue, we introduce the **first** family of distributed bilevel algorithms with communication compression. The primary challenge in algorithmic development is mitigating bias in hypergradient estimation caused by the nested structure. We first propose C-SOBA, a simple yet effective approach with unbiased compression and provable linear speedup convergence. However, it relies on strong assumptions on bounded gradients. To address this limitation, we explore the use of moving average, error feedback, and multi-step compression in bilevel optimization, resulting in a series of advanced algorithms with relaxed assumptions and improved convergence properties. Numerical experiments show that our compressed bilevel algorithms can achieve $10\times$ reduction in communication overhead without severe performance degradation.",[],,"['Yutong He', 'Jie Hu', 'Xinmeng Huang', 'Songtao Lu', 'Bin Wang', 'Kun Yuan']","['Academy for Advanced Interdisciplinary Studies, Peking University', 'Center for Data Science, Peking University', 'University of Pennsylvania', 'IBM Thomas J. Watson Research Center', 'Zhejiang University', '']",
https://openreview.net/forum?id=xqqccG7gf1,Security,Membership Inference Attacks on Diffusion Models via Quantile Regression,"Recently, diffusion models have become popular tools for image synthesis due to their high-quality outputs. However, like other large models, they may leak private information about their training data. Here, we demonstrate a privacy vulnerability of diffusion models through a *membership inference (MI) attack*, which aims to identify whether a target example belongs to the training set when given the trained diffusion model. Our proposed MI attack learns quantile regression models that predict (a quantile of) the distribution of reconstruction loss on examples not used in training. This allows us to define a granular hypothesis test for determining the membership of a point in the training set, based on thresholding the reconstruction loss of that point using a custom threshold tailored to the example. We also provide a simple bootstrap technique that takes a majority membership prediction over ''a bag of weak attackers'' which improves the accuracy over individual quantile regression models. We show that our attack outperforms the prior state-of-the-art attack while being substantially less computationally expensive --- prior attacks required training multiple ''shadow models'' with the same architecture as the model under attack, whereas our attack requires training only much smaller models.",[],,"['Shuai Tang', 'Steven Wu', 'Sergul Aydore', 'Michael Kearns', 'Aaron Roth']","['Jump Trading', 'School of Computer Science, Carnegie Mellon University', 'Amazon', 'University of Pennsylvania', 'Amazon']",
https://openreview.net/forum?id=mggc3oYHy4,Security,Privacy Attacks in Decentralized Learning,"Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network graph. The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real graphs and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some of the factors that affect the performance of the attack, namely the graph topology, the number of attackers, and their position in the graph.",[],,"['Abdellah El Mrini', 'Edwige Cyffers', 'Aurélien Bellet']","['INRIA', 'INRIA']",
https://openreview.net/forum?id=b1YQ5WKY3w,Security,Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective,"In-context learning (ICL) has emerged as a particularly remarkable characteristic of Large Language Models (LLM): given a pretrained LLM and an observed dataset, LLMs can make predictions for new data points from the same distribution without fine-tuning. Numerous works have postulated ICL as approximately Bayesian inference, rendering this a natural hypothesis. In this work, we analyse this hypothesis from a new angle through the *martingale property*, a fundamental requirement of a Bayesian learning system for exchangeable data. We show that the martingale property is a necessary condition for unambiguous predictions in such scenarios, and enables a principled, decomposed notion of uncertainty vital in trustworthy, safety-critical systems. We derive actionable checks with corresponding theory and test statistics which must hold if the martingale property is satisfied. We also examine if uncertainty in LLMs decreases as expected in Bayesian learning when more data is observed. In three experiments, we provide evidence for violations of the martingale property, and deviations from a Bayesian scaling behaviour of uncertainty, falsifying the hypothesis that ICL is Bayesian.",[],,"['Fabian Falck', 'Ziyu Wang', 'Christopher C. Holmes']","['', 'University of Oxford', 'University of Oxford']",
https://openreview.net/forum?id=D9EfAkQCzh,Security,Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework,"Deep Multi-view Clustering (DMVC) stands out as a widely adopted technique aiming at enhanced clustering performance by leveraging diverse data sources. However, the critical issue of vulnerability to adversarial attacks is unexplored due to the lack of well-defined attack objectives. To fill this crucial gap, this paper is the first work to investigate the possibility of adversarial attacks on DMVC models. Specifically, we introduce an adversarial attack with Generative Adversarial Networks (GANs) with the aim to maximally change the complementarity and consistency of multiple views, thus leading to wrong clustering. Building upon this adversarial context, in the realm of defense, we propose a novel Adversarially Robust Deep Multi-View Clustering by leveraging adversarial training. Based on the analysis from an information-theoretic perspective, we design an Attack Mitigator that provides a foundation to guarantee the adversarial robustness of our DMVC models. Experiments conducted on multi-view datasets confirmed that our attack framework effectively reduces the clustering performance of the target model. Furthermore, our proposed adversarially robust method is also demonstrated to be an effective defense against such attacks. This work is a pioneer in exploring adversarial threats and advancing both theoretical understanding and practical strategies for robust multi-view clustering. Code is available at https://github.com/libertyhhn/AR-DMVC.",[],,"['Haonan Huang', 'Guoxu Zhou', 'Yanghang Zheng', 'Yuning Qiu', 'Andong Wang', 'Qibin Zhao']","['AIP, RIKEN', 'School of Automation, Guangdong University of Technology', 'Guangdong University of Technology', 'Center for Advanced Intelligence Project, RIKEN', 'RIKEN AIP', 'AIP, RIKEN']",
https://openreview.net/forum?id=xFCA2yWVs4,Security,Ai-sampler: Adversarial Learning of Markov kernels with involutive maps,"Markov chain Monte Carlo methods have become popular in statistics as versatile techniques to sample from complicated probability distributions. In this work, we propose a method to parameterize and train transition kernels of Markov chains to achieve efficient sampling and good mixing. This training procedure minimizes the total variation distance between the stationary distribution of the chain and the empirical distribution of the data. Our approach leverages involutive Metropolis-Hastings kernels constructed from reversible neural networks that ensure detailed balance by construction. We find that reversibility also implies $C_2$-equivariance of the discriminator function which can be used to restrict its function space.",[],,"['Evgenii Egorov', 'Riccardo Valperga', 'Stratis Gavves']","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']",
https://openreview.net/forum?id=ElNxZ40tBJ,Privacy & Data Governance,Differentially Private Worst-group Risk Minimization,"We initiate a systematic study of worst-group risk minimization under $(\epsilon, \delta)$-differential privacy (DP). The goal is to privately find a model that approximately minimizes the maximal risk across $p$ sub-populations (groups) with different distributions, where each group distribution is accessed via a sample oracle. We first present a new algorithm that achieves excess worst-group population risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} + \sqrt{\frac{p}{K}})$, where $K$ is the total number of samples drawn from all groups and $d$ is the problem dimension. Our rate is nearly optimal when each distribution is observed via a fixed-size dataset of size $K/p$. Our result is based on a new stability-based analysis for the generalization error. In particular, we show that $\Delta$-uniform argument stability implies $\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$ generalization error w.r.t. the worst-group risk, where $n$ is the number of samples drawn from each sample oracle. Next, we propose an algorithmic framework for worst-group population risk minimization using any DP online convex optimization algorithm as a subroutine. Hence, we give another excess risk bound of $\tilde{O}\left( \sqrt{\frac{d^{1/2}}{\epsilon K}} +\sqrt{\frac{p}{K\epsilon^2}} + \sqrt{\frac{p}{K}} \right)$. Assuming the typical setting of $\epsilon=\Theta(1)$, this bound is more favorable than our first bound in a certain range of $p$ as a function of $K$ and $d$. Finally, we study differentially private worst-group *empirical* risk minimization in the offline setting, where each group distribution is observed by a fixed-size dataset. We present a new algorithm with nearly optimal excess risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon})$.",[],,"['Xinyu Zhou', 'Raef Bassily']","['Ohio State University, Columbus', 'Computer Science and Engineering, Ohio State University']",
https://openreview.net/forum?id=W9GaJUVLCT,Fairness & Bias,Time Series Diffusion in the Frequency Domain,"Fourier analysis has been an instrumental tool in the development of signal processing. This leads us to wonder whether this framework could similarly benefit generative modelling. In this paper, we explore this question through the scope of time series diffusion models. More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models. By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components. Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. This results in frequency diffusion models, which we compare to canonical time diffusion models. Our empirical evaluation on real-world datasets, covering various domains like healthcare and finance, shows that frequency diffusion models better capture the training distribution than time diffusion models. We explain this observation by showing that time series from these datasets tend to be more localized in the frequency domain than in the time domain, which makes them easier to model in the former case. All our observations point towards impactful synergies between Fourier analysis and diffusion models.",[],,"['Jonathan Crabbé', 'Nicolas Huynh', 'Jan Pawel Stanczuk', 'Mihaela van der Schaar']","['Latent Labs', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']",
https://openreview.net/forum?id=cc72Vnfvoc,Security,Trained Random Forests Completely Reveal your Dataset,"We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming - an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation. Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability.",[],,"['Julien Ferry', 'Ricardo Fukasawa', 'Timothée Pascal', 'Thibaut Vidal']","['École Polytechnique de Montréal, Université de Montréal', 'Combinatorics and Optimization, University of Waterloo', 'Génie Industriel, Ecole Nationale des Ponts et Chausees', 'Polytechnique Montreal']",
https://openreview.net/forum?id=OkChMnjF6s,Transparency & Explainability,Verification of Machine Unlearning is Fragile,"As privacy concerns escalate in the realm of machine learning, data owners now have the option to utilize machine unlearning to remove their data from machine learning models, following recent legislation. To enhance transparency in machine unlearning and avoid potential dishonesty by model providers, various verification strategies have been proposed. These strategies enable data owners to ascertain whether their target data has been effectively unlearned from the model. However, our understanding of the safety issues of machine unlearning verification remains nascent. In this paper, we explore the novel research question of whether model providers can circumvent verification strategies while retaining the information of data supposedly unlearned. Our investigation leads to a pessimistic answer: the verification of machine unlearning is fragile. Specifically, we categorize the current verification strategies regarding potential dishonesty among model providers into two types. Subsequently, we introduce two novel adversarial unlearning processes capable of circumventing both types. We validate the efficacy of our methods through theoretical analysis and empirical experiments using real-world datasets. This study highlights the vulnerabilities and limitations in machine unlearning verification, paving the way for further research into the safety of machine unlearning.",[],,"['Binchi Zhang', 'Zihan Chen', 'Cong Shen', 'Jundong Li']","['Electrical and Computer Engineering, University of Virginia, Charlottesville', '', 'University of Virginia, Charlottesville', 'University of Virginia']",
https://openreview.net/forum?id=1mf1ISuyS3,Privacy & Data Governance,Towards Certified Unlearning for Deep Neural Networks,"In the field of machine unlearning, certified unlearning has been extensively studied in convex machine learning models due to its high efficiency and strong theoretical guarantees. However, its application to deep neural networks (DNNs), known for their highly nonconvex nature, still poses challenges. To bridge the gap between certified unlearning and DNNs, we propose several simple techniques to extend certified unlearning methods to nonconvex objectives. To reduce the time complexity, we develop an efficient computation method by inverse Hessian approximation without compromising certification guarantees. In addition, we extend our discussion of certification to nonconvergence training and sequential unlearning, considering that real-world users can send unlearning requests at different time points. Extensive experiments on three real-world datasets demonstrate the efficacy of our method and the advantages of certified unlearning in DNNs.",[],,"['Binchi Zhang', 'Yushun Dong', 'Tianhao Wang', 'Jundong Li']","['Electrical and Computer Engineering, University of Virginia, Charlottesville', 'CS, Florida State University', 'University of Virginia, Charlottesville', 'University of Virginia']",
https://openreview.net/forum?id=gjgRKbdYR7,Transparency & Explainability,SelfIE: Self-Interpretation of Large Language Model Embeddings,"How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM’s reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE’s text descriptions on hidden embeddings open avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.",[],,"['Haozhe Chen', 'Carl Vondrick', 'Chengzhi Mao']","['Columbia University', 'Computer Science, Columbia University', 'Google']",
https://openreview.net/forum?id=rPm5cKb1VB,Fairness & Bias,Expressivity and Generalization: Fragment-Biases for Molecular GNNs,"Although recent advances in higher-order Graph Neural Networks (GNNs) improve the theoretical expressiveness and molecular property predictive performance, they often fall short of the empirical performance of models that explicitly use fragment information as inductive bias. However, for these approaches, there exists no theoretic expressivity study. In this work, we propose the *Fragment-WL* test, an extension to the well-known Weisfeiler & Leman (WL) test, which enables the theoretic analysis of these fragment-biased GNNs. Building on the insights gained from the Fragment-WL test, we develop a new GNN architecture and a fragmentation with infinite vocabulary that significantly boosts expressiveness. We show the effectiveness of our model on synthetic and real-world data where we outperform all GNNs on Peptides and have $12$% lower error than all GNNs on ZINC and $34$% lower error than other fragment-biased models. Furthermore, we show that our model exhibits superior generalization capabilities compared to the latest transformer-based architectures, positioning it as a robust solution for a range of molecular modeling tasks.",[],,"['Tom Wollschläger', 'Niklas Kemper', 'Leon Hetzel', 'Johanna Sommer', 'Stephan Günnemann']","['School of Computation Information and Technology, Technische Universität München', 'Computer Science, Technische Universität München', 'Technische Universität München', 'Technische Universität München', 'Technical University Munich']",
https://openreview.net/forum?id=07f24ya6eX,Fairness & Bias,Regularized Q-learning through Robust Averaging,"We propose a new Q-learning variant, called 2RA Q-learning, that addresses some weaknesses of existing Q-learning methods in a principled manner. One such weakness is an underlying estimation bias which cannot be controlled and often results in poor performance. We propose a distributionally robust estimator for the maximum expected value term, which allows us to precisely control the level of estimation bias introduced. The distributionally robust estimator admits a closed-form solution such that the proposed algorithm has a computational cost per iteration comparable to Watkins' Q-learning. For the tabular case, we show that 2RA Q-learning converges to the optimal policy and analyze its asymptotic mean-squared error. Lastly, we conduct numerical experiments for various settings, which corroborate our theoretical findings and indicate that 2RA Q-learning often performs better than existing methods.",[],,"['Peter Schmitt-Förster', 'Tobias Sutter']","['Universität Konstanz', 'Universität Konstanz']",
https://openreview.net/forum?id=YNvGFaOG1p,Fairness & Bias,Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation,"Actor-critic (AC) is a powerful method for learning an optimal policy in reinforcement learning, where the critic uses algorithms, e.g., temporal difference (TD) learning with function approximation, to evaluate the current policy and the actor updates the policy along an approximate gradient direction using information from the critic. This paper provides the *tightest* non-asymptotic convergence bounds for both the AC and natural AC (NAC) algorithms. Specifically, existing studies show that AC converges to an $\epsilon+\varepsilon_{\text{critic}}$ neighborhood of stationary points with the best known sample complexity of $\mathcal{O}(\epsilon^{-2})$ (up to a log factor), and NAC converges to an $\epsilon+\varepsilon_{\text{critic}}+\sqrt{\varepsilon_{\text{actor}}}$ neighborhood of the global optimum with the best known sample complexity of $\mathcal{O}(\epsilon^{-3})$, where $\varepsilon_{\text{critic}}$ is the approximation error of the critic and $\varepsilon_{\text{actor}}$ is the approximation error induced by the insufficient expressive power of the parameterized policy class. This paper analyzes the convergence of both AC and NAC algorithms with compatible function approximation. Our analysis eliminates the term $\varepsilon_{\text{critic}}$ from the error bounds while still achieving the best known sample complexities. Moreover, we focus on the challenging single-loop setting with a single Markovian sample trajectory. Our major technical novelty lies in analyzing the stochastic bias due to policy-dependent and time-varying compatible function approximation in the critic, and handling the non-ergodicity of the MDP due to the single Markovian sample trajectory. Numerical results are also provided in the appendix.",[],,"['Yudan Wang', 'Yue Wang', 'Yi Zhou', 'Shaofeng Zou']","['ECEE, Arizona State University', 'ECE, University of Central Florida', 'Texas A&M University - College Station', '']",
https://openreview.net/forum?id=JNeeRjKbuH,Fairness & Bias,Differentially Private Post-Processing for Fair Regression,"This paper describes a differentially private post-processing algorithm for learning fair regressors satisfying statistical parity, addressing privacy concerns of machine learning models trained on sensitive data, as well as fairness concerns of their potential to propagate historical biases. Our algorithm can be applied to post-process any given regressor to improve fairness by remapping its outputs. It consists of three steps: first, the output distributions are estimated privately via histogram density estimation and the Laplace mechanism, then their Wasserstein barycenter is computed, and the optimal transports to the barycenter are used for post-processing to satisfy fairness. We analyze the sample complexity of our algorithm and provide fairness guarantee, revealing a trade-off between the statistical bias and variance induced from the choice of the number of bins in the histogram, in which using less bins always favors fairness at the expense of error.",[],,"['Ruicheng Xian', 'Qiaobo Li', 'Gautam Kamath', 'Han Zhao']","['University of Illinois Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Computer Science, Courant Institute of Mathematical Sciences, New York University', 'University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=ncjhi4qAPV,Privacy & Data Governance,Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining,"The performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large *public* datasets. We critically review this approach. We primarily question whether the use of large Web-scraped datasets *should* be viewed as differential-privacy-preserving. We further scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains. Finally, we observe that reliance on large pretrained models may lose *other* forms of privacy, requiring data to be outsourced to a more compute-powerful third party.",[],,"['Florian Tramèr', 'Gautam Kamath', 'Nicholas Carlini']","['ETHZ - ETH Zurich', 'Computer Science, Courant Institute of Mathematical Sciences, New York University', 'Google']",
https://openreview.net/forum?id=idyUNsoZ75,Fairness & Bias,Evaluating Model Bias Requires Characterizing its Mistakes,"The ability to properly benchmark model performance in the face of spurious correlations is important to both build better predictors and increase confidence that models are operating as intended. We demonstrate that characterizing (as opposed to simply quantifying) model mistakes across subgroups is pivotal to properly reflect model biases, which are ignored by standard metrics such as worst-group accuracy or accuracy gap. Inspired by the hypothesis testing framework, we introduce SkewSize, a principled and flexible metric that captures bias from mistakes in a model's predictions. It can be used in multi-class settings or generalised to the open vocabulary setting of generative models. SkewSize is an aggregation of the effect size of the interaction between two categorical variables: the spurious variable representing the bias attribute the model's prediction. We demonstrate the utility of SkewSize in multiple settings including: standard vision models trained on synthetic data, vision models trained on ImageNet, and large scale vision-and-language models from the BLIP-2 family. In each case, the proposed SkewSize is able to highlight biases not captured by other metrics, while also providing insights on the impact of recently proposed techniques, such as instruction tuning.",[],,"['Isabela Albuquerque', 'Jessica Schrouff', 'David Warde-Farley', 'Ali Taylan Cemgil', 'Sven Gowal', 'Olivia Wiles']","['DeepMind', 'Google DeepMind', 'Google DeepMind', 'DeepMind', 'DeepMind', 'Google']",
https://openreview.net/forum?id=E6Nm3x7acv,Transparency & Explainability,Contextual Feature Selection with Conditional Stochastic Gates,"Feature selection is a crucial tool in machine learning and is widely applied across various scientific disciplines. Traditional supervised methods generally identify a universal set of informative features for the entire population. However, feature relevance often varies with context, while the context itself may not directly affect the outcome variable. Here, we propose a novel architecture for contextual feature selection where the subset of selected features is conditioned on the value of *context variables*. Our new approach, Conditional Stochastic Gates (c-STG), models the importance of features using conditional Bernoulli variables whose parameters are predicted based on contextual variables. We introduce a hypernetwork that maps context variables to feature selection parameters to learn the context-dependent gates along with a prediction model. We further present a theoretical analysis of our model, indicating that it can improve performance and flexibility over population-level methods in complex feature selection settings. Finally, we conduct an extensive benchmark using simulated and real-world datasets across multiple domains demonstrating that c-STG can lead to improved feature selection capabilities while enhancing prediction accuracy and interpretability.",[],,"['Ram Dyuthi Sristi', 'Ofir Lindenbaum', 'Shira Lifshitz', 'Maria Lavzin', 'Jackie Schiller', 'Gal Mishne', 'Hadas Benisty']","['Electrical and Computer Engineering, University of California, San Diego, University of California, San Diego', 'Bar-Ilan University', 'Technion - Israel Institute of Technology, Technion', 'Technion - Israel Institute of Technology', 'University of California, San Diego', 'Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=kpDd2HCBka,Fairness & Bias,Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design,"Most reinforcement learning practitioners evaluate their policies with online Monte Carlo estimators for either hyperparameter tuning or testing different algorithmic design choices, where the policy is repeatedly executed in the environment to get the average outcome. Such massive interactions with the environment are prohibitive in many scenarios. In this paper, we propose novel methods that improve the data efficiency of online Monte Carlo estimators while maintaining their unbiasedness. We first propose a tailored closed-form behavior policy that provably reduces the variance of an online Monte Carlo estimator. We then design efficient algorithms to learn this closed-form behavior policy from previously collected offline data. Theoretical analysis is provided to characterize how the behavior policy learning error affects the amount of reduced variance. Compared with previous works, our method achieves better empirical performance in a broader set of environments, with fewer requirements for offline data.",[],,"['Shuze Liu', 'Shangtong Zhang']","['Computer Science, University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']",
https://openreview.net/forum?id=yUxdk32TU6,Security,COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability,"Jailbreaks on large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent (suffix) attack with continuation constraint, but also allow us to address new controllable attack settings such as revising a user query adversarially with paraphrasing constraint, and inserting stealthy attacks in context with position constraint. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.",[],,"['Xingang Guo', 'Fangxu Yu', 'Huan Zhang', 'Lianhui Qin', 'Bin Hu']","['ECE, University of Illinois, Urbana-Champaign', 'School of AI, Nanjing University', 'Electrical and Computer Engineering, University of Illinois at Urbana-Champaign', 'University of California, San Diego', 'University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=frA0NNBS1n,Security,Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo,"Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",[],,"['Stephen Zhao', 'Rob Brekelmans', 'Alireza Makhzani', 'Roger Baker Grosse']","['Department of Computer Science, University of Toronto', 'Vector Institute', 'Google', 'Department of Computer Science, University of Toronto']",
https://openreview.net/forum?id=55HfvJ6lDB,Fairness & Bias,Learning Optimal Projection for Forecast Reconciliation of Hierarchical Time Series,"Hierarchical time series forecasting requires not only prediction accuracy but also coherency, i.e., forecasts add up appropriately across the hierarchy. Recent literature has shown that reconciliation via projection outperforms prior methods such as top-down or bottom-up approaches. Unlike existing work that pre-specifies a projection matrix (e.g., orthogonal), we study the problem of learning the optimal oblique projection from data for coherent forecasting of hierarchical time series. In addition to the unbiasedness-preserving property, oblique projection implicitly accounts for the hierarchy structure and assigns different weights to individual time series, providing significant adaptability over orthogonal projection which treats base forecast errors equally. We examine two broad classes of projections, namely Euclidean projection and general oblique projections. We propose to model the reconciliation step as a learnable, structured, projection layer in the neural forecaster architecture. The proposed approach allows for the efficient learning of the optimal projection in an end-to-end framework where both the neural forecaster and the projection layer are learned simultaneously. An empirical evaluation of real-world hierarchical time series datasets demonstrates the superior performance of the proposed method over existing state-of-the-art approaches.",[],,"['Asterios Tsiourvas', 'Wei Sun', 'Georgia Perakis', 'Pin-Yu Chen', 'Yada Zhu']","['Massachusetts Institute of Technology', 'IBM Research', 'Massachusetts Institute of Technology', 'International Business Machines', 'IBM Research']",
https://openreview.net/forum?id=eC1OOpOGZW,Transparency & Explainability,Saliency strikes back: How filtering out high frequencies improves white-box explanations,"Attribution methods correspond to a class of explainability methods (XAI) that aim to assess how individual inputs contribute to a model's decision-making process. We have identified a significant limitation in one type of attribution methods, known as ``white-box"" methods. Although highly efficient, as we will show, these methods rely on a gradient signal that is often contaminated by high-frequency artifacts. To overcome this limitation, we introduce a new approach called ""FORGrad"". This simple method effectively filters out these high-frequency artifacts using optimal cut-off frequencies tailored to the unique characteristics of each model architecture. Our findings show that FORGrad *consistently enhances* the performance of already existing white-box methods, enabling them to compete effectively with more accurate yet computationally demanding ""black-box"" methods. We anticipate that our research will foster broader adoption of simpler and more efficient white-box methods for explainability, offering a better balance between faithfulness and computational efficiency.",[],,"['Sabine Muzellec', 'Thomas FEL', 'Victor Boutin', 'Léo Andéol', 'Rufin VanRullen', 'Thomas Serre']","['', 'Kempner Institute, Harvard University', 'CerCo, CNRS', 'Statistics, Institut de Mathématique de Toulouse', 'CNRS', 'Brown University']",
https://openreview.net/forum?id=ZwUThOE7Zc,Transparency & Explainability,Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research,"The recent embrace of machine learning (ML) in the development of autonomous weapons systems (AWS) creates serious risks to geopolitical stability and the free exchange of ideas in AI research. This topic has received comparatively little attention of late compared to risks stemming from superintelligent artificial general intelligence (AGI), but requires fewer assumptions about the course of technological development and is thus a nearer-future issue. ML is already enabling the substitution of AWS for human soldiers in many battlefield roles, reducing the upfront human cost, and thus political cost, of waging offensive war. In the case of peer adversaries, this increases the likelihood of ""low intensity"" conflicts which risk escalation to broader warfare. In the case of non-peer adversaries, it reduces the domestic blowback to wars of aggression. This effect can occur regardless of other ethical issues around the use of military AI such as the risk of civilian casualties, and does not require any superhuman AI capabilities. Further, the military value of AWS raises the specter of an AI-powered arms race and the misguided imposition of national security restrictions on AI research. Our goal in this paper is to raise awareness among the public and ML researchers on the near-future risks posed by full or near-full autonomy in military technology, and we provide regulatory suggestions to mitigate these risks. We call upon AI policy experts and the defense AI community in particular to embrace transparency and caution in their development and deployment of AWS to avoid the negative effects on global stability and AI research that we highlight here.",[],,"['Riley Simmons-Edler', 'Ryan Paul Badman', 'Shayne Longpre', 'Kanaka Rajan']","['Harvard University', 'Harvard Medical School', 'Massachusetts Institute of Technology', 'Icahn School of Medicine at Mount Sinai']",
https://openreview.net/forum?id=rfvgdfd1K9,Security,Position: Intent-aligned AI Systems Must Optimize for Agency Preservation,A central approach to AI-safety research has been to generate aligned AI systems: i.e. systems that do not deceive users and yield actions or recommendations that humans might judge as consistent with their intentions and goals. Here we argue that truthful AIs aligned solely to human intent are insufficient and that preservation of long-term agency of humans may be a more robust standard that may need to be separated and explicitly optimized for. We discuss the science of intent and control and how human intent can be manipulated and we provide a formal definition of agency-preserving AI-human interactions focusing on forward-looking explicit agency evaluations. Our work points to a novel pathway for human harm in AI-human interactions and proposes solutions to this challenge.,[],,"['Catalin Mitelut', 'Benjamin Smith', 'Peter Vamplew']","['New York University', 'Federation University Australia']",
https://openreview.net/forum?id=2bUFIsg2f5,Security,Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations,"This work studies sparse adversarial perturbations bounded by $l_0$ norm. We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations. Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks. Codes are available at https://github.com/CityU-MLO/sPGD.",[],,"['Xuyang Zhong', 'Yixiao HUANG', 'Chen Liu']","['City University of Hong Kong', 'University of California, Berkeley', 'City University of Hong Kong']",
https://openreview.net/forum?id=5kGfm3Pa41,Fairness & Bias,Recurrent Distance Filtering for Graph Representation Learning,"Graph neural networks based on iterative one-hop message passing have been shown to struggle in harnessing the information from distant nodes effectively. Conversely, graph transformers allow each node to attend to all other nodes directly, but lack graph inductive bias and have to rely on ad-hoc positional encoding. In this paper, we propose a new architecture to reconcile these challenges. Our approach stems from the recent breakthroughs in long-range modeling provided by deep state-space models: for a given target node, our model aggregates other nodes by their shortest distances to the target and uses a linear RNN to encode the sequence of hop representations. The linear RNN is parameterized in a particular diagonal form for stable long-range signal propagation and is theoretically expressive enough to encode the neighborhood hierarchy. With no need for positional encoding, we empirically show that the performance of our model is comparable to or better than that of state-of-the-art graph transformers on various benchmarks, with a significantly reduced computational cost. Our code is open-source at https://github.com/skeletondyh/GRED.",[],,"['Yuhui Ding', 'Antonio Orvieto', 'Bobby He', 'Thomas Hofmann']","['Department of Computer Science, ETHZ - ETH Zurich', 'ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, Tübingen AI Center, Tübingen, Germany', 'Department of Computer Science, ETHZ - ETH Zurich', 'Department of Computer Science , Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=KXsUCgn9Ks,Security,Fundamental Limitations of Alignment in Large Language Models,"An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary ""chatGPT jailbreaks"", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.",[],,"['Yotam Wolf', 'Noam Wies', 'Oshri Avnery', 'Yoav Levine', 'Amnon Shashua']","['Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'AI21 Labs', 'Hebrew University, Hebrew University of Jerusalem']",
https://openreview.net/forum?id=WSpPC1Jm0p,Transparency & Explainability,Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions,"The increasing complexity of foundational models underscores the necessity for explainability, particularly for fine-tuning, the most widely used training method for adapting models to downstream tasks. Instance attribution, one type of explanation, attributes the model prediction to each training example by an instance score. However, the robustness of instance scores, specifically towards dataset resampling, has been overlooked. To bridge this gap, we propose a notion of robustness on the sign of the instance score. We theoretically and empirically demonstrate that the popular leave-one-out-based methods lack robustness, while the Shapley value behaves significantly better, but at a higher computational cost. Accordingly, we introduce an efficient fine-tuning-free approximation of the Shapley value (FreeShap) for instance attribution based on the neural tangent kernel. We empirically demonstrate that FreeShap outperforms other methods for instance attribution and other data-centric applications such as data removal, data selection, and wrong label detection, and further generalize our scale to large language models (LLMs). Our code is available at https://github.com/JTWang2000/FreeShap.",[],,"['Jingtan Wang', 'Xiaoqiang Lin', 'Rui Qiao', 'Chuan-Sheng Foo', 'Bryan Kian Hsiang Low']","['National University of Singapore, National University of Singapore', 'Computer Science, National University of Singapore', 'School of Computing, national university of singaore, National University of Singapore', 'Centre for Frontier AI Research, A*STAR', 'National University of Singapore']",
https://openreview.net/forum?id=Qc5umSsUi8,Security,Scalable Safe Policy Improvement for Factored Multi-Agent MDPs,"In this work, we focus on safe policy improvement in multi-agent domains where current state-of-the-art methods cannot be effectively applied because of large state and action spaces. We consider recent results using Monte Carlo Tree Search for Safe Policy Improvement with Baseline Bootstrapping and propose a novel algorithm that scales this approach to multi-agent domains, exploiting the factorization of the transition model and value function. Given a centralized behavior policy and a dataset of trajectories, our algorithm generates an improved policy by selecting joint actions using a novel extension of Max-Plus (or Variable Elimination) that constrains local actions to guarantee safety criteria. An empirical evaluation on multi-agent SysAdmin and multi-UAV Delivery shows that the approach scales to very large domains where state-of-the-art methods cannot work.",[],,"['Federico Bianchi', 'Edoardo Zorzi', 'Alberto Castellini', 'Thiago D. Simão', 'Matthijs T. J. Spaan', 'Alessandro Farinelli']","['University of Verona', 'University of Verona', 'University of Verona', 'Eindhoven University of Technology', '', 'Computer Science, Università degli Studi di Verona']",
https://openreview.net/forum?id=Msjovr9hUe,Transparency & Explainability,Local Feature Selection without Label or Feature Leakage for Interpretable Machine Learning Predictions,"Local feature selection in machine learning provides instance-specific explanations by focusing on the most relevant features for each prediction, enhancing the interpretability of complex models. However, such methods tend to produce misleading explanations by encoding additional information in their selections. In this work, we attribute the problem of misleading selections by formalizing the concepts of label and feature leakage. We rigorously derive the necessary and sufficient conditions under which we can guarantee no leakage, and show existing methods do not meet these conditions. Furthermore, we propose the first local feature selection method that is proven to have no leakage called SUWR. Our experimental results indicate that SUWR is less prone to overfitting and combines state-of-the-art predictive performance with high feature-selection sparsity. Our generic and easily extendable formal approach provides a strong theoretical basis for future work on interpretability with reliable explanations.",[],,"['Harrie Oosterhuis', 'Lijun Lyu', 'Avishek Anand']","['Radboud University Nijmegen', 'Delft University of Technology', 'Delft University of Technology']",
https://openreview.net/forum?id=W4mLp5KuKl,Fairness & Bias,Generalization Analysis for Multi-Label Learning,"Despite great advances in algorithms for multi-label learning, research on the theoretical analysis of generalization is still in the early stage. Some recent theoretical results has investigated the generalization performance of multi-label learning under several evaluation metrics, however, how to reduce the dependency on the number of labels, explicitly introduce label correlations, and quantitatively analyze the impact of various inductive biases in the generalization analysis of multi-label learning is still a crucial and open problem. In an attempt to make up for the gap in the generalization theory of multi-label learning, we develop several novel vector-contraction inequalities, which exploit the Lipschitz continuity of loss functions, and derive generalization bounds with a weaker dependency on the number of labels than the state of the art in the case of decoupling the relationship among different components, which serves as theoretical guarantees for the generalization of multi-label learning. In addition, we derive the generalization bound for Macro-Averaged AUC and analyze its relationship with class-imbalance. The mild bounds without strong assumptions explain the good generalization ability of multi-label learning with first-order label correlations and high-order label correlations induced by norm regularizers.",[],,"['Yifan Zhang', 'Min-Ling Zhang']","['Southeast University', 'School of Computer Science and Engineering, Southeast University']",
https://openreview.net/forum?id=8f8SI9X9ox,Fairness & Bias,Individual Fairness in Graph Decomposition,"In this paper, we consider classic randomized low diameter decomposition procedures for planar graphs that obtain connected clusters that are cohesive in that close by pairs of nodes are assigned to the same cluster with high probability. We consider the additional aspect of *individual fairness* -- pairs of nodes at comparable distances should be separated with comparable probability. We show that classic decomposition procedures do not satisfy this property. We present novel algorithms that achieve various trade-offs between this property and additional desiderata of connectivity of the clusters and optimality in number of clusters. We show that our individual fairness bounds may be difficult to improve by tying the improvement to resolving a major open question in metric embeddings. We finally show the efficacy of our algorithms on real planar networks modeling Congressional redistricting.",[],,"['Kamesh Munagala', 'Govind S. Sankar']","['Duke University', 'Department of Computer Science, Duke University']",
https://openreview.net/forum?id=w1d9DOGymR,Security,Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback,"Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise problematic behavior, such as helping to commit crimes or producing racist text. One approach to fine-tuning, called reinforcement learning from human feedback, learns from humans’ expressed preferences over multiple outputs. Another approach is constitutional AI, in which the input from humans is a list of high-level principles. But how do we deal with potentially diverging input from humans? How can we aggregate the input into consistent data about “collective” preferences or otherwise use it to make collective choices about model behavior? In this paper, we argue that the field of social choice is well positioned to address these questions, and we discuss ways forward for this agenda, drawing on discussions in a recent workshop on Social Choice for AI Ethics and Safety held in Berkeley, CA, USA in December 2023.",[],,"['Vincent Conitzer', 'Rachel Freedman', 'Jobst Heitzig', 'Wesley H. Holliday', 'Bob M. Jacobs', 'Nathan Lambert', 'Milan Mossé', 'Eric Pacuit', 'Stuart Russell', 'Hailey Schoelkopf', 'Emanuel Tewolde', 'William S. Zwicker']","['Computer Science, Carnegie Mellon University', 'University of California Berkeley', 'Complex Systems, Potsdam Institute for Climate Impact Research', 'Department of Philosophy and Group in Logic and the Methodology of Science, University of California, Berkeley', 'Moraalwetenschap, Universiteit Gent', 'Allen Institute for Artificial Intelligence', 'EECS, University of California Berkeley', 'EleutherAI', 'Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=G4b32bKnBy,Fairness & Bias,Minimum Norm Interpolation Meets The Local Theory of Banach Spaces,"Minimum-norm interpolators have recently gained attention primarily as an analyzable model to shed light on the double descent phenomenon observed for neural networks. The majority of the work has focused on analyzing interpolators in Hilbert spaces, where typically an effectively low-rank structure of the feature covariance prevents a large bias. More recently, tight vanishing bounds have also been shown for isotropic high-dimensional data for $\ell_p$-spaces with $p\in[1,2)$, leveraging sparse structure of the ground truth. However, these proofs are tailored to specific settings and hard to generalize. This paper takes a first step towards establishing a general framework that connects generalization properties of the interpolators to well-known concepts from high-dimensional geometry, specifically, from the local theory of Banach spaces. In particular, we show that under $2$-uniform convexity, the bias of the minimal norm solution is bounded by the Gaussian complexity of the class. We then prove a ``reverse'' Efron-Stein lower bound on the expected conditional variance of the minimal norm solution under cotype $2$. Finally, we prove that this bound is sharp for $\ell_p$-linear regression under sub-Gaussian covariates.",[],,"['Gil Kur', 'Pedro Abdalla', 'Pierre Bizeul', 'Fanny Yang']","['Department of Computer Science, ETHZ - ETH Zurich', 'Mathematics, ETHZ - ETH Zurich', 'Mathematics, Technion - Israel Institute of Technology, Technion', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=tQPkzTdaaN,Security,"PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition","Large language models (LLMs) have shown success in many natural language processing tasks. Despite rigorous safety alignment processes, supposedly safety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to jailbreaks, leading to security risks and abuse of the models. One option to mitigate such risks is to augment the LLM with a dedicated ""safeguard"", which checks the LLM's inputs or outputs for undesired behaviour. A promising approach is to use the LLM itself as the safeguard. Nonetheless, baseline methods, such as prompting the LLM to self-classify toxic content, demonstrate limited efficacy. We hypothesise that this is due to domain shift: the alignment training imparts a self-censoring behaviour to the model (""Sorry I can't do that""), while the self-classify approach shifts it to a classification format (""Is this prompt malicious""). In this work, we propose PARDEN, which avoids this domain shift by simply asking the model to repeat its own outputs. PARDEN neither requires finetuning nor white box access to the model. We empirically verify the effectiveness of our method and show that PARDEN significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2. We find that PARDEN is particularly powerful in the relevant regime of high True Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for Llama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in the FPR from 24.8% to 2.0% on the harmful behaviours dataset. Code and data are available at https://github.com/Ed-Zh/PARDEN.",[],,"['Ziyang Zhang', 'Qizhen Zhang', 'Jakob Nicolaus Foerster']","['Mathematics & Statistics, University of Oxford', 'University of Oxford', 'University of Oxford, University of Oxford']",
https://openreview.net/forum?id=WJ5fJhwvCl,Security,Breaking the Barrier: Enhanced Utility and Robustness in Smoothed DRL Agents,"Robustness remains a paramount concern in deep reinforcement learning (DRL), with randomized smoothing emerging as a key technique for enhancing this attribute. However, a notable gap exists in the performance of current smoothed DRL agents, often characterized by significantly low clean rewards and weak robustness. In response to this challenge, our study introduces innovative algorithms aimed at training effective smoothed robust DRL agents. We propose S-DQN and S-PPO, novel approaches that demonstrate remarkable improvements in clean rewards, empirical robustness, and robustness guarantee across standard RL benchmarks. Notably, our S-DQN and S-PPO agents not only significantly outperform existing smoothed agents by an average factor of $2.16\times$ under the strongest attack, but also surpass previous robustly-trained agents by an average factor of $2.13\times$. This represents a significant leap forward in the field. Furthermore, we introduce Smoothed Attack, which is $1.89\times$ more effective in decreasing the rewards of smoothed agents than existing adversarial attacks. Our code is available at: [https://github.com/Trustworthy-ML-Lab/Robust_HighUtil_Smoothed_DRL](https://github.com/Trustworthy-ML-Lab/Robust_HighUtil_Smoothed_DRL)",[],,"['Chung-En Sun', 'Sicun Gao', 'Tsui-Wei Weng']","['Computer Science and Engineering, University of California, San Diego', 'University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=veEjiN2w9F,Transparency & Explainability,Local vs. Global Interpretability: A Computational Complexity Perspective,"The local and global interpretability of various ML models has been studied extensively in recent years. However, despite significant progress in the field, many known results remain informal or lack sufficient mathematical rigor. We propose a framework for bridging this gap, by using computational complexity theory to assess local and global perspectives of interpreting ML models. We begin by proposing proofs for two novel insights that are essential for our analysis: (1) a duality between local and global forms of explanations; and (2) the inherent uniqueness of certain global explanation forms. We then use these insights to evaluate the complexity of computing explanations, across three model types representing the extremes of the interpretability spectrum: (1) linear models; (2) decision trees; and (3) neural networks. Our findings offer insights into both the local and global interpretability of these models. For instance, under standard complexity assumptions such as P != NP, we prove that selecting *global* sufficient subsets in linear models is computationally harder than selecting *local* subsets. Interestingly, with neural networks and decision trees, the opposite is true: it is harder to carry out this task locally than globally. We believe that our findings demonstrate how examining explainability through a computational complexity lens can help us develop a more rigorous grasp of the inherent interpretability of ML models.",[],,"['Shahaf Bassan', 'Guy Amir', 'Guy Katz']","['Hebrew University of Jerusalem, Hebrew University of Jerusalem', 'Computer Science, Cornell University', 'Hebrew University of Jerusalem']",
https://openreview.net/forum?id=sTVSyqD6XX,Privacy & Data Governance,Private and Federated Stochastic Convex Optimization: Efficient Strategies for Centralized Systems,"This paper addresses the challenge of preserving privacy in Federated Learning (FL) within centralized systems, focusing on both trusted and untrusted server scenarios. We analyze this setting within the Stochastic Convex Optimization (SCO) framework, and devise methods that ensure Differential Privacy (DP) while maintaining optimal convergence rates for homogeneous and heterogeneous data distributions. Our approach, based on a recent stochastic optimization technique, offers linear computational complexity, comparable to non-private FL methods, and reduced gradient obfuscation. This work enhances the practicality of DP in FL, balancing privacy, efficiency, and robustness in a variety of server trust environments.",[],,"['Roie Reshef', 'Kfir Yehuda Levy']","['Electrical and Computer Engineering, Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=NwYsuFuelg,Security,Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers,"Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques focus on the *static* setting, wherein the identity of Byzantine workers remains unchanged throughout the learning process. This assumption fails to capture real-world *dynamic* Byzantine behaviors, which may include intermittent malfunctions or targeted, time-limited attacks. Addressing this limitation, we propose DynaBRO -- a new method capable of withstanding any sub-linear number of identity changes across rounds. Specifically, when the number of such changes is $\mathcal{O}(\sqrt{T})$ (where $T$ is the total number of training rounds), DynaBRO nearly matches the state-of-the-art asymptotic convergence rate of the static setting. Our method utilizes a multi-level Monte Carlo (MLMC) gradient estimation technique applied at the server to robustly aggregated worker updates. By additionally leveraging an adaptive learning rate, we circumvent the need for prior knowledge of the fraction of Byzantine workers.",[],,"['Ron Dorfman', 'Naseem Amin Yehya', 'Kfir Yehuda Levy']","['Technion, Technion', 'Technion - Israel Institute of Technology, Technion', 'Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=OQ97v7uRGc,Fairness & Bias,On The Complexity of First-Order Methods in Stochastic Bilevel Optimization,"We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$. Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them. We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators *locally unbiased* within the $\Theta(\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\epsilon$ stationary point using $O(\epsilon^{-6}), O(\epsilon^{-4})$ access to first-order $y^*$-aware oracles. Our upper bounds also apply to standard unbiased first-order oracles, improving the best-known complexity of first-order methods by $O(\epsilon)$ with minimal assumptions. We then provide the matching $\Omega(\epsilon^{-6})$, $\Omega(\epsilon^{-4})$ lower bounds without and with an additional smoothness assumption, respectively. Our results imply that any approach that simulates an algorithm with an $y^*$-aware oracle must suffer the same lower bounds.",[],,"['Jeongyeol Kwon', 'Dohyun Kwon', 'Hanbaek Lyu']","['University of Wisconsin - Madison', 'Department of Mathematics, University of Seoul', 'University of Wisconsin, Madison']",
https://openreview.net/forum?id=qIOSNyPPwB,Security,Graph Neural Network Explanations are Fragile,"Explainable Graph Neural Network (GNN) has emerged recently to foster the trust of using GNNs. Existing GNN explainers are developed from various perspectives to enhance the explanation performance. We take the first step to study GNN explainers under adversarial attack—We found that an adversary slightly perturbing graph structure can ensure GNN model makes correct predictions, but the GNN explainer yields a drastically different explanation on the perturbed graph. Specifically, we first formulate the attack problem under a practical threat model (i.e., the adversary has limited knowledge about the GNN explainer and a restricted perturbation budget). We then design two methods (i.e., one is loss-based and the other is deduction-based) to realize the attack. We evaluate our attacks on various GNN explainers and the results show these explainers are fragile.",[],,"['Jiate Li', 'Meng Pang', 'Yun Dong', 'Jinyuan Jia', 'Binghui Wang']","['Department of Computer Science, Illinois Institute of Technology', 'School of Mathematics and Computer Sciences, Nanchang University', 'Humanities, Society, and Communication, Milwaukee School of Engineering', 'College of IST, Pennsylvania State University', 'Illinois Institute of Technology']",
https://openreview.net/forum?id=OndZHBUA1G,Fairness & Bias,A Study of First-Order Methods with a Deterministic Relative-Error Gradient Oracle,"This paper studies the theoretical guarantees of the classical projected gradient and conditional gradient methods applied to constrained optimization problems with biased relative-error gradient oracles. These oracles are used in various settings, such as distributed optimization systems or derivative-free optimization, and are particularly common when gradients are compressed, quantized, or estimated via finite differences computations. Several settings are investigated: Optimization over the box with a coordinate-wise erroneous gradient oracle, optimization over a general compact convex set, and three more specific scenarios. Convergence guarantees are established with respect to the relative-error magnitude, and in particular, we show that the conditional gradient is invariant to relative-error when applied over the box with a coordinate-wise erroneous gradient oracle, and the projected gradient maintains its convergence guarantees when optimizing a nonconvex objective function.",[],,"['Nadav Hallak', 'Kfir Yehuda Levy']","['Technion, Technion', 'Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=5ZwEifshyo,Transparency & Explainability,Explorations of Self-Repair in Language Models,"Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.",[],,"['Cody Rushing', 'Neel Nanda']","['Redwood Research', 'Google DeepMind']",
https://openreview.net/forum?id=2T00oYk54P,Transparency & Explainability,Explaining Graph Neural Networks via Structure-aware Interaction Index,"The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.",[],,"['Ngoc Bui', 'Hieu Trung Nguyen', 'Viet Anh Nguyen', 'Rex Ying']","['Computer Science, Yale University', 'Vinai Research', 'The Chinese University of Hong Kong', 'Department of Computer Science, Yale University']",
https://openreview.net/forum?id=aGBpiEcB8z,Transparency & Explainability,BayOTIDE: Bayesian Online Multivariate Time Series Imputation with Functional Decomposition,"In real-world scenarios such as traffic and energy management, we frequently encounter large volumes of time-series data characterized by missing values, noise, and irregular sampling patterns. While numerous imputation methods have been proposed, the majority tend to operate within a local horizon, which involves dividing long sequences into batches of fixed-length segments for model training. This local horizon often leads to the overlooking of global trends and periodic patterns. More importantly, most methods assume the observations are sampled at regular timestamps, and fail to handle complex irregular sampled time series in various applications. Additionally, most existing methods are learned in an offline manner. Thus, it is not suitable for applications with rapidly arriving streaming data. To address these challenges, we propose BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. Our method conceptualizes multivariate time series as the weighted combination of groups of low-rank temporal factors with different patterns. We employ a suite of Gaussian Processes (GPs),each with a unique kernel, as functional priors to model these factors. For computational efficiency, we further convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE), and developing a scalable algorithm for online inference. The proposed method can not only handle imputation over arbitrary timestamps, but also offer uncertainty quantification and interpretability for the downstream application. We evaluate our method on both synthetic and real-world datasets. We release the code at https://github.com/xuangu-fang/BayOTIDE.",[],,"['Shikai Fang', 'Qingsong Wen', 'Yingtao Luo', 'Shandian Zhe', 'Liang Sun']","['Microsoft Research , Microsoft', 'AI Research Institute, Squirrel Ai Learning', 'Machine Learning Department, School of Computer Science, Carnegie Mellon University', '', 'DAMO Academy, Alibaba Group']",
https://openreview.net/forum?id=k2dVVIWWho,Privacy & Data Governance,Differentially Private Decentralized Learning with Random Walks,"The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empirical evaluation on synthetic and real-world graphs and datasets.",[],,"['Edwige Cyffers', 'Aurélien Bellet', 'Jalaj Upadhyay']","['INRIA', 'INRIA', 'Rutgers University']",
https://openreview.net/forum?id=Jvh8HM9YEJ,Fairness & Bias,MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis,"Federated learning is widely used in medical applications for training global models without needing local data access, but varying computational capabilities and network architectures (system heterogeneity) across clients pose significant challenges in effectively aggregating information from non-independently and identically distributed (non-IID) data (statistic heterogeneity). Current federated learning methods using knowledge distillation require public datasets, raising privacy and data collection issues. Additionally, these datasets require additional local computing and storage resources, which is a burden for medical institutions with limited hardware conditions. In this paper, we introduce a novel federated learning paradigm, named Model Heterogeneous personalized Federated Learning via Injection and Distillation (MH-pFLID). Our framework leverages a lightweight messenger model, eliminating the need for public datasets and reducing the training cost for each client. We also develops receiver and transmitter modules for each client to separate local biases from generalizable information, reducing biased data collection and mitigating client drift. Our experiments on various medical tasks including image classification, image segmentation, and time-series classification, show MH-pFLID outperforms state-of-the-art methods in all these areas and has good generalizability.",[],,"['Luyuan Xie', 'Manqing Lin', 'Tianyu Luan', 'Cong Li', 'Yuejian Fang', 'Qingni Shen', 'Zhonghai Wu']","['Peking University', 'Electrical and information engineering, Beijing University of Aeronautics and Astronautics', 'State University of New York at Buffalo', 'School of Computer Science, Peking University', 'Peking University', 'School of Software and Microelectronics, Peking University', 'School of Software and Microelectronics, Peking University']",
https://openreview.net/forum?id=dbFEFHAD79,Fairness & Bias,MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark,"Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, a closer examination reveals persistent challenges in the evaluative capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge.github.io/.",[],,"['Dongping Chen', 'Ruoxi Chen', 'Shilin Zhang', 'Yaochen Wang', 'Yinuo Liu', 'Huichi Zhou', 'Qihui Zhang', 'Yao Wan', 'Pan Zhou', 'Lichao Sun']","['University of Washington', '', 'Shien-Ming Wu School of Intelligent Engineering , South China University of Technology', 'Nanjing University of Posts and Telecommunications', 'School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'Imperial College London', 'The University of Queensland', 'Huazhong University of Science and Technology', 'School of Cyber Science and Engineering, Huazhong University of Science and Technology', 'Computer Science and Engineering, Lehigh University']",
https://openreview.net/forum?id=LVF4P1NNwO,Fairness & Bias,Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers,"In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.",[],,"['Brian K Chen', 'Tianyang Hu', 'Hui Jin', 'Hwee Kuan Lee', 'Kenji Kawaguchi']","['National University of Singapore', 'National University of Singapore', ""Huawei Noah's Ark Lab, Huawei Technologies Ltd."", 'BII', 'National University of Singapore']",
https://openreview.net/forum?id=akyElNlUVA,Privacy & Data Governance,FedLMT: Tackling System Heterogeneity of Federated Learning via Low-Rank Model Training with Theoretical Guarantees,"Federated learning (FL) is an emerging machine learning paradigm for preserving data privacy. However, diverse client hardware often has varying computation resources. Such system heterogeneity limits the participation of resource-constrained clients in FL, and hence degrades the global model accuracy. To enable heterogeneous clients to participate in and contribute to FL training, previous works tackle this problem by assigning customized sub-models to individual clients with model pruning, distillation, or low-rank based techniques. Unfortunately, the global model trained by these methods still encounters performance degradation due to heterogeneous sub-model aggregation. Besides, most methods are heuristic-based and lack convergence analysis. In this work, we propose the FedLMT framework to bridge the performance gap, by assigning clients with a homogeneous pre-factorized low-rank model to substantially reduce resource consumption without conducting heterogeneous aggregation. We theoretically prove that the convergence of the low-rank model can guarantee the convergence of the original full model. To further meet clients' personalized resource needs, we extend FedLMT to pFedLMT, by separating model parameters into common and custom ones. Finally, extensive experiments are conducted to verify our theoretical analysis and show that FedLMT and pFedLMT outperform other baselines with much less communication and computation costs.",[],,"['Jiahao Liu', 'Yipeng Zhou', 'Di Wu', 'Miao Hu', 'Mohsen Guizani', 'Quan Z. Sheng']","['Sun Yat-Sen University', 'Computing, Macquarie University', 'School of Computer Science and Engineering , SUN YAT-SEN UNIVERSITY', 'School of Computer Science and Engineering, SUN YAT-SEN UNIVERSITY', 'School of Computing, Macquarie University']",
https://openreview.net/forum?id=kVgpa1rfLO,Security,Towards the Theory of Unsupervised Federated Learning: Non-asymptotic Analysis of Federated EM Algorithms,"While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. Several federated EM algorithms have gained popularity in practice, however, their theoretical foundations are often lacking. In this paper, we first introduce a federated gradient EM algorithm (FedGrEM) designed for the unsupervised learning of mixture models, which supplements the existing federated EM algorithms by considering task heterogeneity and potential adversarial attacks. We present a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on specific statistical models to characterize the explicit estimation error of model parameters and mixture proportions. Our theory elucidates when and how FedGrEM outperforms local single-task learning with insights extending to existing federated EM algorithms. This bridges the gap between their practical success and theoretical understanding. Our numerical results validate our theory, and demonstrate FedGrEM's superiority over existing unsupervised federated learning benchmarks.",[],,"['Ye Tian', 'Haolei Weng', 'Yang Feng']","['Columbia University', 'New York University']",
https://openreview.net/forum?id=VyGo1S5A6d,Security,Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts,"Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose **Prompting4Debugging (P4D)** as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered ""safe"" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.",[],,"['Zhi-Yi Chin', 'Chieh Ming Jiang', 'Ching-Chun Huang', 'Pin-Yu Chen', 'Wei-Chen Chiu']","['Department of Computer Science, National Yang Ming Chiao Tung University', 'National Chiao Tung University', 'Department of Computer Science , National Yang Ming Chiao Tung University', 'International Business Machines', 'Computer Science, National Yang Ming Chiao Tung University']",
https://openreview.net/forum?id=UZstTlLq1E,Fairness & Bias,Initial Guessing Bias: How Untrained Networks Favor Some Classes,"Understanding and controlling biasing effects in neural networks is crucial for ensuring accurate and fair model performance. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a deep neural network (DNN) can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We prove that, besides dataset properties, the presence of this phenomenon, which we call *Initial Guessing Bias* (IGB), is influenced by model choices including dataset preprocessing methods, and architectural decisions, such as activation functions, max-pooling layers, and network depth. Our analysis of IGB provides information for architecture selection and model initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging and the non-trivial effects that depth has on the phenomenon.",[],,"['Emanuele Francazi', 'Aurelien Lucchi', 'Marco Baity-Jesi']","['EPFL - EPF Lausanne', 'University of Basel', 'Eawag']",
https://openreview.net/forum?id=RfsagmV1AG,Fairness & Bias,On the Second-Order Convergence of Biased Policy Gradient Algorithms,"Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0) on Markov chains irrespective of initial state distribution.",[],,"['Siqiao Mu', 'Diego Klabjan']","['Engineering Sciences and Applied Mathematics, Northwestern University', 'Industrial Engineering and Management Sciences, Northwestern University']",
https://openreview.net/forum?id=LWD7upg1ob,Privacy & Data Governance,Differentially Private Synthetic Data via Foundation Model APIs 2: Text,"Text data has become extremely valuable due to the emergence of machine learning algorithms that learn from it. A lot of high-quality text data generated in the real world is private and therefore cannot be shared or used freely due to privacy concerns. Generating synthetic replicas of private text data with a formal privacy guarantee, i.e., differential privacy (DP), offers a promising and scalable solution. However, existing methods necessitate DP finetuning of large language models (LLMs) on private data to generate DP synthetic data. This approach is not viable for proprietary LLMs (e.g., GPT-3.5) and also demands considerable computational resources for open-source LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE) algorithm to generate DP synthetic images with only API access to diffusion models. In this work, we propose an augmented PE algorithm, named Aug-PE, that applies to the complex setting of text. We use API access to an LLM and generate DP synthetic text without any model training. We conduct comprehensive experiments on three benchmark datasets. Our results demonstrate that Aug-PE produces DP synthetic text that yields competitive utility with the SOTA DP finetuning baselines. This underscores the feasibility of relying solely on API access of LLMs to produce high-quality DP synthetic texts, thereby facilitating more accessible routes to privacy-preserving LLM applications.",[],,"['Chulin Xie', 'Zinan Lin', 'Arturs Backurs', 'Sivakanth Gopi', 'Da Yu', 'Huseyin A Inan', 'Harsha Nori', 'Haotian Jiang', 'Huishuai Zhang', 'Yin Tat Lee', 'Bo Li', 'Sergey Yekhanin']","['University of Illinois, Urbana Champaign', 'Microsoft', 'Microsoft', 'Microsoft Research', 'Google', 'Microsoft', 'Microsoft', 'Microsoft Research, Redmond', 'Peking University', '', 'CS, University of Illinois, Urbana Champaign', 'Microsoft Research, Microsoft']",
https://openreview.net/forum?id=dV9QGostQk,Transparency & Explainability,A Unified View of FANOVA: A Comprehensive Bayesian Framework for Component Selection and Estimation,"This paper presents a comprehensive Bayesian framework for FANOVA models. We provide guidelines for tuning and practical implementation to improve scalability of learning and prediction. Our model is very flexible and can handle different levels of sparsity across and within decomposition orders, as well as among covariates. This flexibility enables the modeling of complex real-world data while enhancing interpretability. Additionally, it allows our model to unify diverse deterministic and Bayesian non-parametric approaches into a single equation, making comparisons and understanding easier. Notably, our model serves as the Bayesian counterpart of several deterministic methods allowing uncertainty quantification. This general framework unlocks potential for novel model developments that have been previously overlooked, such as the proposed Dirichlet mixing model that addresses limitations of existing models.",[],,"['yosra marnissi', 'Maxime Leiber']","['safrangroup', 'INRIA']",
https://openreview.net/forum?id=merZTLSdC9,Fairness & Bias,On Online Experimentation without Device Identifiers,"Measuring human feedback via randomized experimentation is a cornerstone of data-driven decision-making. The methodology used to estimate user preferences from their online behaviours is critically dependent on user identifiers. However, in today's digital landscape, consumers frequently interact with content across multiple devices, which are often recorded with different identifiers for the same consumer. The inability to match different device identities across consumers poses significant challenges for accurately estimating human preferences and other causal effects. Moreover, without strong assumptions about the device-user graph, the causal effects might not be identifiable. In this paper, we propose HIFIVE, a variational method to solve the problem of estimating global average treatment effects (GATE) from a fragmented view of exposures and outcomes. Experiments show that our estimator is superior to standard estimators, with a lower bias and greater robustness to network uncertainty.",[],,"['Shiv Shankar', 'Ritwik Sinha', 'Madalina Fiterau']","['IIT Bombay', 'Adobe Systems', 'Department of Computer Science, University of Massachusetts, Amherst']",
https://openreview.net/forum?id=f3TUipYU3U,Security,HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,"Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.",[],,"['Mantas Mazeika', 'Long Phan', 'Xuwang Yin', 'Andy Zou', 'Zifan Wang', 'Norman Mu', 'Elham Sakhaee', 'Nathaniel Li', 'Steven Basart', 'Bo Li', 'David Forsyth', 'Dan Hendrycks']","['Center for AI Safety', 'Center for AI Safety', 'Center for AI Safety', 'CMU, Carnegie Mellon University', 'Scale AI', 'University of California Berkeley', 'Microsoft', 'University of California, Berkeley', 'Computer science, Center for AI Safety', 'CS, University of Illinois, Urbana Champaign', 'Computer Science, University of Illinois, Urbana-Champaign', 'UC Berkeley']",
https://openreview.net/forum?id=NlM4gp8hyO,Fairness & Bias,Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning,"Temporal credit assignment in reinforcement learning is challenging due to delayed and stochastic outcomes. Monte Carlo targets can bridge long delays between action and consequence but lead to high-variance targets due to stochasticity. Temporal difference (TD) learning uses bootstrapping to overcome variance but introduces a bias that can only be corrected through many iterations. TD($\lambda$) provides a mechanism to navigate this bias-variance tradeoff smoothly. Appropriately selecting $\lambda$ can significantly improve performance. Here, we propose Chunked-TD, which uses predicted probabilities of transitions from a model for computing $\lambda$-return targets. Unlike other model-based solutions to credit assignment, Chunked-TD is less vulnerable to model inaccuracies. Our approach is motivated by the principle of history compression and ‘chunks’ trajectories for conventional TD learning. Chunking with learned world models compresses near-deterministic regions of the environment-policy interaction to speed up credit assignment while still bootstrapping when necessary. We propose algorithms that can be implemented online and show that they solve some problems much faster than conventional TD($\lambda$).",[],,"['Aditya Ramesh', 'Kenny John Young', 'Louis Kirsch', 'Jürgen Schmidhuber']","['Dalle Molle Institute for Artificial Intelligence (IDSIA)', 'University of Alberta', 'Google', 'King Abdullah University of Science and Technology']",
https://openreview.net/forum?id=yhpDKSw7yA,Fairness & Bias,Provably Robust DPO: Aligning Language Models with Noisy Feedback,"Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive. In this work, we aim to bridge this gap by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon < 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset shows that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.",[],,"['Sayak Ray Chowdhury', 'Anush Kini', 'Nagarajan Natarajan']","['Indian Institute of Technology, Kanpur', 'Microsoft Research']",
https://openreview.net/forum?id=kRxCDDFNpp,Privacy & Data Governance,Fewer Truncations Improve Language Modeling,"In large language model training, input documents are typically concatenated together and then split into sequences of equal length to avoid padding tokens. Despite its efficiency, the concatenation approach compromises data integrity—it inevitably breaks many documents into incomplete pieces, leading to excessive truncations that hinder the model from learning to compose logically coherent and factually consistent content that is grounded on the complete context. To address the issue, we propose Best-fit Packing, a scalable and efficient method that packs documents into training sequences through length-aware combinatorial optimization. Our method completely eliminates unnecessary truncations while retaining the same training efficiency as concatenation. Empirical results from both text and code pre-training show that our method achieves superior performance (e.g., +4.7% on reading comprehension; +16.8% in context following; and +9.2% on program synthesis), and reduces closed-domain hallucination effectively by up to 58.3%.",[],,"['Hantian Ding', 'Zijian Wang', 'Giovanni Paolini', 'Varun Kumar', 'Anoop Deoras', 'Dan Roth', 'Stefano Soatto']","['Amazon', '', 'Department of mathematics, University of Bologna', 'Amazon', 'Amazon', 'Oracle', 'Amazon Web Services']",
https://openreview.net/forum?id=RXxTuxPopa,Fairness & Bias,Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference,"An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maintaining high power. We demonstrate its performance on two challenging scientific problems in biology and astroparticle physics with data from realistic mechanistic models.",[],,"['Luca Masserano', 'Alexander Shen', 'Michele Doro', 'Tommaso Dorigo', 'Rafael Izbicki', 'Ann B. Lee']","['CMU, Carnegie Mellon University', 'Statistics and Data Science, Carnegie Mellon University', 'INFN', 'Universidade Federal de Sao Carlos', 'Carnegie Mellon University']",
https://openreview.net/forum?id=mNzkumTSVL,Privacy & Data Governance,Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors,"Conventional Federated Learning (FL) involves collaborative training of a global model while maintaining user data privacy. One of its branches, decentralized FL, is a serverless network that allows clients to own and optimize different local models separately, which results in saving management and communication resources. Despite the promising advancements in decentralized FL, it may reduce model generalizability due to lacking a global model. In this scenario, managing data and model heterogeneity among clients becomes a crucial problem, which poses a unique challenge that must be overcome: *How can every client's local model learn generalizable representation in a decentralized manner?* To address this challenge, we propose a novel **De**centralized FL technique by introducing **S**ynthetic **A**nchors, dubbed as DeSA. Based on the theory of domain adaptation and Knowledge Distillation (KD), we theoretically and empirically show that synthesizing global anchors based on raw data distribution facilitates mutual knowledge transfer. We further design two effective regularization terms for local training: *1) REG loss* that regularizes the distribution of the client's latent embedding with the anchors and *2) KD loss* that enables clients to learn from others. Through extensive experiments on diverse client data distributions, we showcase the effectiveness of DeSA in enhancing both inter- and intra-domain accuracy of each client. The implementation of DeSA can be found at: https://github.com/ubc-tea/DESA",[],,"['Chun-Yin Huang', 'Kartik Srinivas', 'Xin Zhang', 'Xiaoxiao Li']","['University of British Columbia', 'Machine Learning Department, School of Computer Science, Carnegie Mellon University', 'Facebook', 'Electrical and Computer Engineering, University of British Columbia']",
https://openreview.net/forum?id=EhPpZV6KLk,Fairness & Bias,Surprisingly Strong Performance Prediction with Neural Graph Features,"Performance prediction has been a key part of the neural architecture search (NAS) process, allowing to speed up NAS algorithms by avoiding resource-consuming network training. Although many performance predictors correlate well with ground truth performance, they require training data in the form of trained networks. Recently, zero-cost proxies have been proposed as an efficient method to estimate network performance without any training. However, they are still poorly understood, exhibit biases with network properties, and their performance is limited. Inspired by the drawbacks of zero-cost proxies, we propose neural graph features (GRAF), simple to compute properties of architectural graphs. GRAF offers fast and interpretable performance prediction while outperforming zero-cost proxies and other common encodings. In combination with other zero-cost proxies, GRAF outperforms most existing performance predictors at a fraction of the cost.",[],,"['Gabriela Kadlecová', 'Jovita Lukasik', 'Martin Pilát', 'Petra Vidnerová', 'Mahmoud Safari', 'Roman Neruda', 'Frank Hutter']","['Faculty of Mathematics and Physics, Charles University, Prague', 'Universität Siegen', 'Faculty of Mathematics and Physics, Charles University', 'Institute of Computer Science, The Czech Academy of Sciences', 'Universität Freiburg', 'Institute of computer Science, Czech Academy of Sciences', 'ELLIS Institute Tübingen & University of Freiburg']",
https://openreview.net/forum?id=YiblhkVl2w,Fairness & Bias,Stability and Multigroup Fairness in Ranking with Uncertain Predictions,"Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and fairness towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but --- as we show --- it composes harmoniously with individual fairness in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve group fairness through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level fairness guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used.",[],,"['Siddartha Devic', 'Aleksandra Korolova', 'David Kempe', 'Vatsal Sharan']","['University of Southern California', '', 'Computer Science, University of Southern California', 'University of Southern California']",
https://openreview.net/forum?id=dJTChKgv3a,Security,In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering,"Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs.",[],,"['Sheng Liu', 'Haotian Ye', 'Lei Xing', 'James Y. Zou']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']",
https://openreview.net/forum?id=tdomF3PW6A,Security,Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss,"Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex Concave Loss (CCL), which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving a state-of-the-art balance in the privacy-utility trade-off.",[],,"['Zhenlong Liu', 'Lei Feng', 'Huiping Zhuang', 'Xiaofeng Cao', 'Hongxin Wei']","['Department of Statistics and Data Science, Southern University of Science and Technology', 'Singapore University of Technology and Design', 'Shien-Ming Wu School of Intelligent Engineering, South China University of Technology', 'AI School, Jilin University', 'Southern University of Science and Technology']",
https://openreview.net/forum?id=RYmmgedVjR,Fairness & Bias,Learning and Forgetting Unsafe Examples in Large Language Models,"As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the “ForgetFilter” algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs’ ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.",[],,"['Jiachen Zhao', 'Zhun Deng', 'David Madras', 'James Zou', 'Mengye Ren']","['Northeastern University', 'Department of Computer uniScience, University of North Carolina at Chapel Hill', 'Google', 'Stanford University', 'New York University']",
https://openreview.net/forum?id=PjVqEErDgK,Transparency & Explainability,Prospector Heads: Generalized Feature Attribution for Large Models & Data,"Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for ML models in scientific and biomedical domains. Current methods for feature attribution, which rely on ""explaining"" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based attribution methods that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 26.3 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for ML models in complex domains.",[],,"['Gautam Machiraju', 'Alexander Derry', 'Arjun D Desai', 'Neel Guha', 'Amir-Hossein Karimi', 'James Zou', 'Russ B Altman', 'Christopher Re', 'Parag Mallick']","['Stanford University', 'Stanford University', 'Stanford University', 'Computer Science Department, Stanford University', 'University of Waterloo', 'Stanford University', 'Stanford University', '', 'Stanford University']",
https://openreview.net/forum?id=SZ0JnRxi0x,Fairness & Bias,An Explicit Frame Construction for Normalizing 3D Point Clouds,"Many real-world datasets are represented as 3D point clouds -- yet they often lack a predefined reference frame, posing a challenge for machine learning or general data analysis. Traditional methods for determining reference frames and normalizing 3D point clouds often struggle with specific inputs, lack theoretical guarantees, or require massive data. We introduce a new algorithm that overcomes these limitations and guarantees both universality and compatibility with any learnable framework for 3D point cloud analysis. Our algorithm works with any input point cloud and performs consistently regardless of input complexities, unlike data-driven methods that are susceptible to biases or limited training data. Empirically, our algorithm outperforms existing methods in effectiveness and generalizability across diverse benchmark datasets. Code is available at https://github.com/Utah-Math-Data-Science/alignment.",[],,"['Justin Baker', 'Shih-Hsin Wang', 'Tommaso de Fernex', 'Bao Wang']","['', 'University of Utah', 'Department of Mathematics, University of Utah', 'Mathematics, University of Utah']",
https://openreview.net/forum?id=xlr6AUDuJz,Security,The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning,"The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private and restricted to a narrow range of malicious use scenarios, which limits further research into reducing malicious use. To fill these gaps, we release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai.",[],,"['Nathaniel Li', 'Alexander Pan', 'Anjali Gopal', 'Summer Yue', 'Daniel Berrios', 'Alice Gatti', 'Justin D. Li', 'Ann-Kathrin Dombrowski', 'Shashwat Goel', 'Gabriel Mukobi', 'Nathan Helm-Burger', 'Rassin Lababidi', 'Lennart Justen', 'Andrew Bo Liu', 'Michael Chen', 'Isabelle Barrass', 'Oliver Zhang', 'Xiaoyuan Zhu', 'Rishub Tamirisa', 'Bhrugu Bharathi', 'Ariel Herbert-Voss', 'Cort B Breuer', 'Andy Zou', 'Mantas Mazeika', 'Zifan Wang', 'Palash Oswal', 'Weiran Lin', 'Adam Alfred Hunt', 'Justin Tienken-Harder', 'Kevin Y. Shih', 'Kemper Talley', 'John Guan', 'Ian Steneker', 'David Campbell', 'Brad Jokubaitis', 'Steven Basart', 'Stephen Fitz', 'Ponnurangam Kumaraguru', 'Kallol Krishna Karmakar', 'Uday Tupakula', 'Vijay Varadharajan', 'Yan Shoshitaishvili', 'Jimmy Ba', 'Kevin M. Esvelt', 'Alexandr Wang', 'Dan Hendrycks']","['University of California, Berkeley', 'University of California, Berkeley', 'Massachusetts Institute of Technology', 'Scale AI', 'Stanford University', 'Center for AI Safety', 'New York University', 'FAR.AI', 'Machine Learning, ELLIS, Max Planck Institute', 'EECS, UC Berkeley, University of California, Berkeley', 'Biology, SecureBio', 'University of Oxford', 'Media Lab, Massachusetts Institute of Technology', 'Harvard University, Harvard University', 'Model Evaluation and Threat Research', 'Center for AI Safety', 'Stanford University', 'University of Southern California', 'Computer Science, University of Illinois at Urbana-Champaign', 'Computer Science and Engineering, University of California, San Diego']",
https://openreview.net/forum?id=sSAEhcdB9N,Privacy & Data Governance,Private Heterogeneous Federated Learning Without a Trusted Server Revisited: Error-Optimal and Communication-Efficient Algorithms for Convex Losses,"We revisit the problem of federated learning (FL) with private data from people who do not trust the server or other silos/clients. In this context, every silo (e.g. hospital) has data from several people (e.g. patients) and needs to protect the privacy of each person's data (e.g. health records), even if the server and/or other silos try to uncover this data. Inter-Silo Record-Level Differential Privacy (ISRL-DP) prevents each silo's data from being leaked, by requiring that silo $i$'s *communications* satisfy item-level differential privacy. Prior work (Lowy & Razaviyayn, 2023a) characterized the optimal excess risk bounds for ISRL-DP algorithms with *homogeneous* (i.i.d.) silo data and convex loss functions. However, two important questions were left open: 1) Can the same excess risk bounds be achieved with *heterogeneous* (non-i.i.d.) silo data? 2) Can the optimal risk bounds be achieved with *fewer communication rounds*? In this paper, we give positive answers to both questions. We provide novel ISRL-DP FL algorithms that achieve the optimal excess risk bounds in the presence of heterogeneous silo data. Moreover, our algorithms are more *communication-efficient* than the prior state-of-the-art. For smooth loss functions, our algorithm achieves the *optimal* excess risk bound and has *communication complexity that matches the non-private lower bound*. Additionally, our algorithms are more *computationally efficient* than the previous state-of-the-art.",[],,"['Changyu Gao', 'Andrew Lowy', 'Xingyu Zhou', 'Stephen Wright']","['University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'ECE, Wayne State University', 'Mathematics, The University of Queensland']",
https://openreview.net/forum?id=itDhUBY2xf,Fairness & Bias,Learning from Integral Losses in Physics Informed Neural Networks,"This work proposes a solution for the problem of training physics-informed networks under partial integro-differential equations. These equations require an infinite or a large number of neural evaluations to construct a single residual for training. As a result, accurate evaluation may be impractical, and we show that naive approximations at replacing these integrals with unbiased estimates lead to biased loss functions and solutions. To overcome this bias, we investigate three types of potential solutions: the deterministic sampling approaches, the double-sampling trick, and the delayed target method. We consider three classes of PDEs for benchmarking; one defining Poisson problems with singular charges and weak solutions of up to 10 dimensions, another involving weak solutions on electro-magnetic fields and a Maxwell equation, and a third one defining a Smoluchowski coagulation problem. Our numerical results confirm the existence of the aforementioned bias in practice and also show that our proposed delayed target approach can lead to accurate solutions with comparable quality to ones estimated with a large sample size integral. Our implementation is open-source and available at https://github.com/ehsansaleh/btspinn.",[],,"['Ehsan Saleh', 'Saba Ghaffari', 'Tim Bretl', 'Luke Olson', 'Matthew West']","['', 'Computer Science, University of Illinois, Urbana Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=URtUYfC3GA,Security,WAVES: Benchmarking the Robustness of Image Watermarks,"In the burgeoning age of generative AI, watermarks act as identifiers of provenance and artificial content. We present WAVES (Watermark Analysis via Enhanced Stress-testing), a benchmark for assessing image watermark robustness, overcoming the limitations of current evaluation methods. WAVES integrates detection and identification tasks and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced, novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. Our novel, comprehensive evaluation reveals previously undetected vulnerabilities of several modern watermarking algorithms. We envision WAVES as a toolkit for the future development of robust watermarks.",[],,"['Bang An', 'Mucong Ding', 'Tahseen Rabbani', 'Aakriti Agrawal', 'Yuancheng Xu', 'Chenghao Deng', 'Sicheng Zhu', 'Abdirisak Mohamed', 'Yuxin Wen', 'Tom Goldstein', 'Furong Huang']","['University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park', 'Bioinformatics and Medical Sciences, Yale University', 'Computer Science, University of Maryland', 'Computer Science, University of Maryland, College Park', 'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland']",
https://openreview.net/forum?id=NsHxeSCtgr,Fairness & Bias,LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models,"Large language models (LLMs) have achieved impressive performance on various natural language generation tasks. Nonetheless, they suffer from generating negative and harmful contents that are biased against certain demographic groups (e.g., female), raising severe fairness concerns. As remedies, prior works intervened the generation by removing attitude or demographic information, inevitably degrading the generation quality and resulting in notable *fairness-fluency* trade-offs. However, it is still under-explored to what extent the fluency *has to* be affected in order to achieve a desired level of fairness. In this work, we conduct the first formal study from an information-theoretic perspective. We show that previous approaches are excessive for debiasing and propose LIDAO, a general framework to debias a (L)LM at a better fluency provably. We further robustify LIDAO in adversarial scenarios, where a carefully-crafted prompt may stimulate LLMs exhibiting instruction-following abilities to generate texts with fairness issue appears only when the prompt is also taken into account. Experiments on three LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our method.",[],,"['Tianci Liu', 'Haoyu Wang', 'Shiyang Wang', 'Yu Cheng', 'Jing Gao']","['ECE, Purdue University', 'State University of New York at Albany', 'The Chinese University of Hong Kong', 'Purdue University']",
https://openreview.net/forum?id=cZNuYKtoOZ,Fairness & Bias,Continuous Treatment Effects with Surrogate Outcomes,"In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish the asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.",[],,"['Zhenghao Zeng', 'David Arbour', 'Avi Feller', 'Raghavendra Addanki', 'Ryan A. Rossi', 'Ritwik Sinha', 'Edward Kennedy']","['Statistics and Data Science, Carnegie Mellon University', 'Adobe Systems', 'University of California, Berkeley', 'Adobe Systems', 'ML, Adobe Research', 'Adobe Systems', 'Carnegie Mellon University']",
https://openreview.net/forum?id=ZXsNkm3bxu,Privacy & Data Governance,CaPS: Collaborative and Private Synthetic Data Generation from Distributed Sources,"Data is the lifeblood of the modern world, forming a fundamental part of AI, decision-making, and research advances. With increase in interest in data, governments have taken important steps towards a regulated data world, drastically impacting data sharing and data usability and resulting in massive amounts of data confined within the walls of organizations. While synthetic data generation (SDG) is an appealing solution to break down these walls and enable data sharing, the main drawback of existing solutions is the assumption of a trusted aggregator for generative model training. Given that many data holders may not want to, or be legally allowed to, entrust a central entity with their raw data, we propose a framework for collaborative and private generation of synthetic tabular data from distributed data holders. Our solution is general, applicable to any marginal-based SDG, and provides input privacy by replacing the trusted aggregator with secure multi-party computation (MPC) protocols and output privacy via differential privacy (DP). We demonstrate the applicability and scalability of our approach for the state-of-the-art select-measure-generate SDG algorithms MWEM+PGM and AIM.",[],,"['Sikha Pentyala', 'Mayana Pereira', 'Martine De Cock']","['University of Washington', 'Microsoft', 'University of Washington']",
https://openreview.net/forum?id=hcQfTsVnBo,Transparency & Explainability,Grokking Group Multiplication with Cosets,"The complex and unpredictable nature of deep neural networks prevents their safe use in many high-stakes applications. There have been many techniques developed to interpret deep neural networks, but all have substantial limitations. Algorithmic tasks have proven to be a fruitful test ground for interpreting a neural network end-to-end. Building on previous work, we completely reverse engineer fully connected one-hidden layer networks that have ``grokked'' the arithmetic of the permutation groups $S_5$ and $S_6$. The models discover the true subgroup structure of the full group and converge on neural circuits that decompose the group arithmetic using the permutation group's subgroups. We relate how we reverse engineered the model's mechanisms and confirmed our theory was a faithful description of the circuit's functionality. We also draw attention to current challenges in conducting interpretability research by comparing our work to Chughtai et al. (2023) which alleges to find a different algorithm for this same problem.",[],,"['Dashiell Stander', 'Qinan Yu', 'Honglu Fan', 'Stella Biderman']","['EleutherAI', 'Stanford University', 'University of Geneva', 'EleutherAI']",
https://openreview.net/forum?id=qawwyKqOkj,Privacy & Data Governance,PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses,"This work studies algorithms for learning from aggregate responses. We focus on the construction of aggregation sets (called *bags* in the literature) for event-level loss functions. We prove for linear regression and generalized linear models (GLMs) that the optimal bagging problem reduces to one-dimensional size-constrained $k$-means clustering. Further, we theoretically quantify the advantage of using curated bags over random bags. We then propose the $\texttt{PriorBoost}$ algorithm, which adaptively forms bags of samples that are increasingly homogeneous with respect to (unobserved) individual responses to improve model quality. We study label differential privacy for aggregate learning, and we also provide extensive experiments showing that $\texttt{PriorBoost}$ regularly achieves optimal model quality for event-level predictions, in stark contrast to non-adaptive algorithms.",[],,"['Adel Javanmard', 'Matthew Fahrbach', 'Vahab Mirrokni']","['Data Sciences and Operations, University of Southern California', '', 'Google Research']",
https://openreview.net/forum?id=45HNimd4YI,Privacy & Data Governance,Perturb-and-Project: Differentially Private Similarities and Marginals,"We revisit the objective perturbations framework for differential privacy where noise is added to the input $A\in \mathcal{S}$ and the result is then projected back to the space of admissible datasets $\mathcal{S}$. Through this framework, we first design novel efficient algorithms to privately release pair-wise cosine similarities. Second, we derive a novel algorithm to compute $k$-way marginal queries over $n$ features. Prior work could achieve comparable guarantees only for $k$ even. Furthermore, we extend our results to $t$-sparse datasets, where our efficient algorithms yields novel, stronger guarantees whenever $t\le n^{5/6}/\log n.$ Finally, we provide a theoretical perspective on why *fast* input perturbation algorithms works well in practice. The key technical ingredients behind our results are tight sum-of-squares certificates upper bounding the Gaussian complexity of sets of solutions.",[],,"['Vincent Cohen-Addad', ""Tommaso d'Orsi"", 'Alessandro Epasto', 'Vahab Mirrokni', 'Peilin Zhong']","['Google', 'Swiss Federal Institute of Technology', 'Google', 'Google Research', 'Google']",
https://openreview.net/forum?id=yShA4VPYZB,Fairness & Bias,${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning,"Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.",[],,"['Dingyang Chen', 'Qi Zhang']","['Amazon', 'University of South Carolina']",
https://openreview.net/forum?id=NeO2hoSexj,Fairness & Bias,Augmenting Decision with Hypothesis in Reinforcement Learning,"Value-based reinforcement learning is the current State-Of-The-Art due to high sampling efficiency. However, our study shows it suffers from low exploitation in early training period and bias sensitiveness. To address these issues, we propose to augment the decision-making process with hypothesis, a weak form of environment description. Our approach relies on prompting the learning agent with accurate hypotheses, and designing a ready-to-adapt policy through incremental learning. We propose the ALH algorithm, showing detailed analyses on a typical learning scheme and a diverse set of Mujoco benchmarks. Our algorithm produces a significant improvement over value-based learning algorithms and other strong baselines. Our code is available at [Github URL](https://github.com/nbtpj/ALH).",[],,"['Nguyen Minh Quang', 'Hady W. Lauw']","['School of Computing and Information System, Singapore Management University', 'School of Computing and Information Systems, Singapore Management University']",
https://openreview.net/forum?id=2zLt2Odckx,Privacy & Data Governance,Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients,"Federated Learning is widely employed to tackle distributed sensitive data. Existing methods primarily focus on addressing in-federation data heterogeneity. However, we observed that they suffer from significant performance degradation when applied to unseen clients for out-of-federation (OOF) generalization. The recent attempts to address generalization to unseen clients generally struggle to scale up to large-scale distributed settings due to high communication or computation costs. Moreover, methods that scale well often demonstrate poor generalization capability. To achieve OOF-resiliency in a scalable manner, we propose Topology-aware Federated Learning (TFL) that leverages client topology - a graph representing client relationships - to effectively train robust models against OOF data. We formulate a novel optimization problem for TFL, consisting of two key modules: Client Topology Learning, which infers the client relationships in a privacy-preserving manner, and Learning on Client Topology, which leverages the learned topology to identify influential clients and harness this information into the FL optimization process to efficiently build robust models. Empirical evaluation on a variety of real-world datasets verifies TFL's superior OOF robustness and scalability.",[],,"['Mengmeng Ma', 'Tang Li', 'Xi Peng']","['University of Delaware', 'Department of Computer & Information Science, University of Delaware', 'Computer & Information Sciences, University of Delaware']",
https://openreview.net/forum?id=8q4EPdjTLE,Security,Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI,"In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. While regulation is important, it is key that it does not put at risk the budding field of open-source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact.",[],,"['Francisco Eiras', 'Aleksandar Petrov', 'Bertie Vidgen', 'Christian Schroeder de Witt', 'Fabio Pizzati', 'Katherine Elkins', 'Supratik Mukhopadhyay', 'Adel Bibi', 'Botos Csaba', 'Fabro Steibel', 'Fazl Barez', 'Genevieve Smith', 'Gianluca Guadagni', 'Jon Chun', 'Jordi Cabot', 'Joseph Marvin Imperial', 'Juan A. Nolazco-Flores', 'Lori Landay', 'Matthew Thomas Jackson', 'Paul Rottger', 'Philip Torr', 'Trevor Darrell', 'Yong Suk Lee', 'Jakob Nicolaus Foerster']","['University of Oxford', 'Google DeepMind', 'Alan Turing Institute', 'Department of Engineering Science, University of Oxford', 'Engineering science , University of Oxford', 'Interdiscipilnary, Yale University', 'Louisiana State University', 'Engineering Science, University of Oxford', 'University of Oxford', '', 'University of California, Berkeley', 'University of Virginia, Charlottesville', 'Integrated Program for Humane Studies, Computing, Kenyon College', 'Luxembourg Institute of Science and Technology', 'University of Bath', 'Engineering, Instituto Tecnológico y de Estudios Superiores de Monterrey', 'Berklee College of Music', 'University of Oxford', 'Bocconi University']",
https://openreview.net/forum?id=ohG9bVMs5j,Transparency & Explainability,Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks,"Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of training data but also preserve explanatory factors. Such generated proxy graphs can be reliably used to approximate the predictions of the labels of explainable subgraphs. Empirical evaluations across various datasets demonstrate our method achieves more accurate explanations for GNNs.",[],,"['Zhuomin Chen', 'Jiaxing Zhang', 'Jingchao Ni', 'Xiaoting Li', 'Yuchen Bian', 'Md Mezbahul Islam', 'Ananda Mondal', 'Hua Wei', 'Dongsheng Luo']","['', 'YWCC, New Jersey Institute of Technology', 'University of Houston', 'Visa Research, VISA', 'Amazon', 'Florida International University', 'Florida International University', 'Arizona State University', '']",
https://openreview.net/forum?id=t6dBpwkbea,Transparency & Explainability,TimeX++: Learning Time-Series Explanations with Information Bottleneck,"Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals. In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues. To address these issues, we introduce a simple yet practical objective function for time series explainable learning. The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues. We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving. We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application. Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at https://github.com/zichuan-liu/TimeXplusplus.",[],,"['Zichuan Liu', 'Tianchun Wang', 'Jimeng Shi', 'Xu Zheng', 'Zhuomin Chen', 'Lei Song', 'Wenqian Dong', 'Jayantha  Obeysekera', 'Farhad Shirani', 'Dongsheng Luo']","['Microsoft Research', 'Pennsylvania State University', 'Florida International University', '', '', 'Microsoft', 'Florida International University', 'Florida International University', '']",
https://openreview.net/forum?id=0P3kaNluGj,Transparency & Explainability,End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations,"Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. NS-RL entails structured state representations for tasks with visual observations, but previous methods cannot refine the structured states with rewards due to a lack of efficiency. Accessibility also remains an issue, as extensive domain knowledge is required to interpret symbolic policies. In this paper, we present a neuro-symbolic framework for jointly learning structured states and symbolic policies, whose key idea is to distill the vision foundation model into an efficient perception module and refine it during policy learning. Moreover, we design a pipeline to prompt GPT-4 to generate textual explanations for the learned policies and decisions, significantly reducing users' cognitive load to understand the symbolic policies. We verify the efficacy of our approach on nine Atari tasks and present GPT-generated explanations for policies and decisions.",[],,"['Lirui Luo', 'Guoxi Zhang', 'Hongming Xu', 'Yaodong Yang', 'Cong Fang', 'Qing Li']","['Peking University', 'Graduate School of Informatics, Kyoto University', 'Beijing Institute for General Artificial Intelligence', 'Peking University', 'Peking University', 'Beijing Institute for General Artificial Intelligence (BIGAI)']",
https://openreview.net/forum?id=CXZqGJonmt,Security,CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks,"While neural networks allow highly accurate predictions in many tasks, their lack of robustness towards even slight input perturbations often hampers their deployment. Adversarial attacks such as the seminal _projected gradient descent_ (PGD) offer an effective means to evaluate a model's robustness and dedicated solutions have been proposed for attacks on semantic segmentation or optical flow estimation. While they attempt to increase the attack's efficiency, a further objective is to balance its effect, so that it acts on the entire image domain instead of isolated point-wise predictions. This often comes at the cost of optimization stability and thus efficiency. Here, we propose CosPGD, an attack that encourages more balanced errors over the entire image domain while increasing the attack's overall efficiency. To this end, CosPGD leverages a simple alignment score computed from any pixel-wise prediction and its target to scale the loss in a smooth and fully differentiable way. It leads to efficient evaluations of a model's robustness for semantic segmentation as well as regression models (such as optical flow, disparity estimation, or image restoration), and it allows it to outperform the previous SotA attack on semantic segmentation. We provide code for the CosPGD algorithm and example usage at https://github.com/shashankskagnihotri/cospgd.",[],,"['Shashank Agnihotri', 'Steffen Jung', 'Margret Keuper']","['Data and Web Science Group, Universität Mannheim', 'Saarland Informatics Campus, Max-Planck Institute', 'Universität Mannheim']",
https://openreview.net/forum?id=Bc4vZ2CX7E,Security,Position: Open-Endedness is Essential for Artificial Superhuman Intelligence,"In recent years there has been a tremendous surge in the general capabilities of AI systems, mainly fuelled by training foundation models on internet-scale data. Nevertheless, the creation of open-ended, ever self-improving AI remains elusive. **In this position paper, we argue that the ingredients are now in place to achieve *open-endedness* in AI systems with respect to a human observer. Furthermore, we claim that such open-endedness is an essential property of any artificial superhuman intelligence (ASI).** We begin by providing a concrete formal definition of open-endedness through the lens of novelty and learnability. We then illustrate a path towards ASI via open-ended systems built on top of foundation models, capable of making novel, human-relevant discoveries. We conclude by examining the safety implications of generally-capable open-ended AI. We expect that open-ended foundation models will prove to be an increasingly fertile and safety-critical area of research in the near future.",[],,"['Edward Hughes', 'Michael D Dennis', 'Jack Parker-Holder', 'Feryal Behbahani', 'Aditi Mavalankar', 'Yuge Shi', 'Tom Schaul', 'Tim Rocktäschel']","['DeepMind', 'Google DeepMind', 'Google DeepMind', 'DeepMind', 'Google DeepMind', 'Google', 'Google DeepMind', 'Google DeepMind']",
https://openreview.net/forum?id=dLojMSgSFW,Security,Position: A Safe Harbor for AI Evaluation and Red Teaming,"Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major generative AI developers commit to providing a legal and technical safe harbor, protecting public interest safety research and removing the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI.",[],,"['Shayne Longpre', 'Sayash Kapoor', 'Kevin Klyman', 'Ashwin Ramaswami', 'Rishi Bommasani', 'Borhane Blili-Hamelin', 'Yangsibo Huang', 'Aviya Skowron', 'Zheng Xin Yong', 'Suhas Kotha', 'Yi Zeng', 'Weiyan Shi', 'Xianjun Yang', 'Reid Southen', 'Alexander Robey', 'Patrick Chao', 'Diyi Yang', 'Ruoxi Jia', 'Daniel Kang', 'Alex Pentland', 'Arvind Narayanan', 'Percy Liang', 'Peter Henderson']","['Massachusetts Institute of Technology', 'Princeton University', 'Stanford University', 'Stanford University', 'AI Risk and Vulnerability Alliance', 'Google', 'EleutherAI', 'Brown University', 'Stanford University', 'Virginia Tech', 'Stanford University', '', 'Illustration, College for Creative Studies', 'Machine Learning, CMU, Carnegie Mellon University', 'OpenAI', 'Stanford University', 'Virginia Tech', 'Department of Computer Science', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=66KmnMhGU5,Transparency & Explainability,Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience,"Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems.",[],,"['Martina G. Vilas', 'Federico Adolfi', 'David Poeppel', 'Gemma Roig']","['Computer Science Department, Goethe University', '', 'New York University', 'Department of Computer Science, Johann Wolfgang Goethe Universität Frankfurt am Main']",
https://openreview.net/forum?id=XT6iF8FDZx,Fairness & Bias,Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States,"In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Experiments corroborate our theory, and demonstrate its conclusions on problems beyond LQR, where systems are non-linear and controllers are neural networks. We hypothesize that real-world optimal control may be greatly improved by developing methods for informed selection of initial states to train on.",[],,"['Noam Razin', 'Yotam Alexander', 'Edo Cohen-Karlik', 'Raja Giryes', 'Amir Globerson', 'Nadav Cohen']","['Princeton Language and Intelligence, Princeton University', 'Computer science, Tel Aviv University, Tel Aviv University', 'Tel Aviv University', 'Tel Aviv University', 'Tel Aviv University', 'School of Computer Science, Tel Aviv University']",
https://openreview.net/forum?id=YB1O99gK7b,Transparency & Explainability,On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box,"Attribution methods shed light on the explainability of data-driven approaches such as deep learning models by uncovering the most influential features in a to-be-explained decision. While determining feature attributions via gradients delivers promising results, the internal access required for acquiring gradients can be impractical under safety concerns, thus limiting the applicability of gradient-based approaches. In response to such limited flexibility, this paper presents GEEX (gradient-estimation-based explanation), a method that produces gradient-like explanations through only query-level access. The proposed approach holds a set of fundamental properties for attribution methods, which are mathematically rigorously proved, ensuring the quality of its explanations. In addition to the theoretical analysis, with a focus on image data, the experimental results empirically demonstrate the superiority of the proposed method over state-of-the-art black-box methods and its competitive performance compared to methods with full access.",[],,"['Yi Cai', 'Gerhard Wunder']","['Department of Mathematics and Computer Science, Freie Universität Berlin']",
https://openreview.net/forum?id=v7I5FtL2pV,Transparency & Explainability,"Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images","Transferring knowledge across diverse data modalities is receiving increasing attention in machine learning. This paper tackles the task of leveraging expert-derived, yet expensive, tabular data to enhance image-based predictions when tabular data is unavailable during inference. The primary challenges stem from the inherent complexity of accurately mapping diverse tabular data to visual contexts, coupled with the necessity to devise distinct strategies for numerical and categorical tabular attributes. We propose CHannel tAbulaR alignment with optiMal tranSport (Charms), which establishes an alignment between image channels and tabular attributes, enabling selective knowledge transfer that is pertinent to visual features. Specifically, Charms measures similarity distributions across modalities to effectively differentiate and transfer relevant tabular features, with a focus on morphological characteristics, enhancing the capabilities of visual classifiers. By maximizing the mutual information between image channels and tabular features, knowledge from both numerical and categorical tabular attributes are extracted. Experimental results demonstrate that Charms not only enhances the performance of image classifiers but also improves their interpretability by effectively utilizing tabular knowledge.",[],,"['Jun-Peng Jiang', 'Han-Jia Ye', 'Leye Wang', 'Yang Yang', 'Yuan Jiang', 'De-Chuan Zhan']","['NanJing University', 'Nanjing University', 'Peking University', 'Nanjing University of Science and Technology', 'Nanjing University', 'School of AI, Nanjing University']",
https://openreview.net/forum?id=1SiEfsCecd,Security,Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization,"Pre-trained language models (PLMs) are commonly used for various downstream natural language processing tasks via fine-tuning. However, recent studies have demonstrated that PLMs are vulnerable to backdoor attacks, which can mislabel poisoned samples to target outputs even after a vanilla fine-tuning process. The key challenge for defending against the backdoored PLMs is that end users who adopt the PLMs for their downstream tasks usually do not have any knowledge about the attacking strategies, such as triggers. To tackle this challenge, in this work, we propose a backdoor mitigation approach, PURE, via head pruning and normalization of attention weights. The idea is to prune the attention heads that are potentially affected by poisoned texts with only clean texts on hand and then further normalize the weights of remaining attention heads to mitigate the backdoor impacts. We conduct experiments to defend against various backdoor attacks on the classification task. The experimental results show the effectiveness of PURE in lowering the attack success rate without sacrificing the performance on clean texts.",[],,"['Xingyi Zhao', 'Depeng Xu', 'Shuhan Yuan']","['Computer Science, Utah State University', 'University of North Carolina at Charlotte', 'Utah State University']",
https://openreview.net/forum?id=mjh7AOWozN,Security,Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs,"Given the critical role of graphs in real-world applications and their high-security requirements, improving the ability of graph neural networks (GNNs) to detect out-of-distribution (OOD) data is an urgent research problem. The recent work GNNSAFE proposes a framework based on the aggregation of negative energy scores that significantly improves the performance of GNNs to detect node-level OOD data. However, our study finds that score aggregation among nodes is susceptible to extreme values due to the unboundedness of the negative energy scores and logit shifts, which severely limits the accuracy of GNNs in detecting node-level OOD data. In this paper, we propose NODESAFE: reducing the generation of extreme scores of nodes by adding two optimization terms that make the negative energy scores bounded and mitigate the logit shift. Experimental results show that our approach dramatically improves the ability of GNNs to detect OOD data at the node level, e.g., in detecting OOD data induced by Structure Manipulation, the metric of FPR95 (lower is better) in scenarios without (with) OOD data exposure are reduced from the current SOTA by 28.4% ( 22.7% ). The code is available via https://github.com/ShenzhiYang2000/NODESAFE-Bounded-and-Uniform-Energy-based-Out-of-distribution-Detection-for-Graphs.",[],,"['Shenzhi Yang', 'Bin Liang', 'An Liu', 'Lin Gui', 'Xingkai Yao', 'Xiaofang Zhang']","['Suzhou University', 'The Chinese University of Hong Kong', 'Suzhou University', ""King's College London, University of London"", 'Suzhou University', 'Soochow University']",
https://openreview.net/forum?id=PPoQz8K4GZ,Fairness & Bias,Prompt-based Visual Alignment for Zero-shot Policy Transfer,"Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL). Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains. Besides, abundant data from multiple domains are needed. To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner. Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance. To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens. With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains. We verify PVA on a vision-based autonomous driving task with CARLA simulator. Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data.",[],,"['Haihan Gao', 'Rui Zhang', 'Qi Yi', 'Hantao Yao', 'Haochen Li', 'Jiaming Guo', 'Shaohui Peng', 'Yunkai Gao', 'QiCheng Wang', 'Xing Hu', 'Yuanbo Wen', 'Zihao Zhang', 'Zidong Du', 'Ling Li', 'Qi Guo', 'Yunji Chen']","['University of Science and Technology of China', 'Institute of Computing Technology, CAS', 'University of Science and Technology of China', ',Institute of automation, Chinese academy of science', 'Institute of Software, Chinese Academy of Sciences', 'Department of electronic engineering, Institute of Computing Technology, Chinese Academy of Sciences', ', Chinese Academy of Sciences', 'University of Science and Technology of China', 'University of the Chinese Academy of Sciences', ', Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'INsititue of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Software, CAS', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences']",
https://openreview.net/forum?id=jXn1qIcjyG,Fairness & Bias,Conditional Language Learning with Context,"Language models can learn sophisticated language understanding skills from fitting raw text. They also unselectively learn useless corpus statistics and biases, especially during finetuning on domain-specific corpora. In this paper, we propose a simple modification to causal language modeling called conditional finetuning, which performs language modeling conditioned on a context. We show that a context can ""explain away"" certain corpus statistics and make the model avoid learning them. In this fashion, conditional finetuning achieves selective learning from a corpus, learning knowledge useful for downstream tasks while avoiding learning useless corpus statistics like topic biases. This selective learning effect leads to less forgetting and better stability-plasticity tradeoff in domain finetuning, potentially benefitting lifelong learning with language models.",[],,"['Xiao Zhang', 'Miao Li', 'Ji Wu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=vSerUPYFtB,Security,One for All: A Universal Generator for Concept Unlearnability via Multi-Modal Alignment,"The abundance of free internet data offers unprecedented opportunities for researchers and developers, but it also poses privacy risks. Utilizing data without explicit consent raises critical challenges in protecting personal information.Unlearnable examples have emerged as a feasible protection approach, which renders the data unlearnable, i.e., useless to third parties, by injecting imperceptible perturbations. However, these perturbations only exhibit unlearnable effects on either a particular dataset or label-consistent scenarios, thereby lacking broad applicability. To address both issues concurrently, we propose a universal perturbation generator that harnesses data with concept unlearnability, thereby broadening the scope of unlearnability beyond specific datasets or labels. Specifically, we leverage multi-modal pre-trained models to establish a connection between the data concepts in a shared embedding space. This connection enables the information transformation from image data to text concepts. Consequently, we can align the text embedding using concept-wise discriminant loss, and render the data unlearnable. Extensive experiments conducted on real-world datasets demonstrate the concept unlearnability, i.e., cross-dataset transferability and label-agnostic utility, of our proposed unlearnable examples, as well as their robustness against attacks.",[],,"['Chaochao Chen', 'Jiaming Zhang', 'Yuyuan Li', 'Zhongxuan Han']","['Zhejiang University', 'Zhejiang University', 'Hangzhou Dianzi University', 'Zhejiang University']",
https://openreview.net/forum?id=gS3nc9iUrH,Transparency & Explainability,Representing Molecules as Random Walks Over Interpretable Grammars,"Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.",[],,"['Michael Sun', 'Minghao Guo', 'Weize Yuan', 'Veronika Thost', 'Crystal Elaine Owens', 'Aristotle Franklin Grosz', 'Sharvaa Selvan', 'Katelyn Zhou', 'Hassan Mohiuddin', 'Benjamin J Pedretti', 'Zachary P Smith', 'Jie Chen', 'Wojciech Matusik']","['Computer Science and Artificial Intelligence Laboratory, Electrical Engineering & Computer Science', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'International Business Machines', 'Electrical Engineering and Computer Science, Massachusetts Institute of Technology', 'Chemical Engineering, Massachusetts Institute of Technology', 'Electrical Engineering and Computer Science, Massachusetts Institute of Technology', 'Wellesley College', 'Computer Science, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'International Business Machines', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=PHUAG63Efe,Privacy & Data Governance,AegisFL: Efficient and Flexible Privacy-Preserving Byzantine-Robust Cross-silo Federated Learning,"Privacy attacks and poisoning attacks are two of the thorniest problems in federation learning (FL). Homomorphic encryption (HE), which allows certain mathematical operations to be done in the ciphertext state, provides a way to solve these two problems simultaneously. However, existing Paillier-based and CKKS-based privacy-preserving byzantine-robust FL (PBFL) solutions not only suffer from low efficiency but also expose the final model to the server. Additionally, these methods are limited to one robust aggregation algorithm (AGR) and are therefore vulnerable to AGR-tailored poisoning attacks. In this paper, we present AegisFL, an efficient PBLF system that provides the flexibility to change the AGR. We first observe that the core of the existing advanced AGRs is to calculate the inner products, $L_2$ norms and mean values for vectors. Based on this observation, we tailor a packing scheme for PBFL, which fits perfectly with RLWE-based fully homomorphic encryption. Under this packing scheme, the server only needs to perform one ciphertext multiplication to construct any required AGR, while the global model only belongs to honest clients. Finally, we conduct extensive experiments on different datasets and adversary settings, which also confirm the effectiveness and efficiency of our scheme.",[],,"['Dong Chen', 'Hongyuan Qu', 'Guangwu Xu']","['Shandong University', 'School of Cyber Science and Technology, Shandong University']",
https://openreview.net/forum?id=Aj18fUB6Th,Fairness & Bias,Two-timescale Derivative Free Optimization for Performative Prediction with Markovian Data,"This paper studies the performative prediction problem where a learner aims to minimize the expected loss with a decision-dependent data distribution. Such setting is motivated when outcomes can be affected by the prediction model, e.g., in strategic classification. We consider a state-dependent setting where the data distribution evolves according to an underlying controlled Markov chain. We focus on stochastic derivative free optimization (DFO) where the learner is given access to a loss function evaluation oracle with the above Markovian data. We propose a two-timescale DFO($\lambda$) algorithm that features (i) a sample accumulation mechanism that utilizes every observed sample to estimate the overall gradient of performative risk, and (ii) a two-timescale diminishing step size that balances the rates of DFO updates and bias reduction. Under a general non-convex optimization setting, we show that DFO($\lambda$) requires ${\cal O}( 1 /\epsilon^3)$ samples (up to a log factor) to attain a near-stationary solution with expected squared gradient norm less than $\epsilon > 0$. Numerical experiments verify our analysis.",[],,"['Haitong LIU', 'Qiang LI', 'Hoi To Wai']","['Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Chinese University of Hong Kong', 'The Chinese University of Hong Kong']",
https://openreview.net/forum?id=cXBPPfNUZJ,Fairness & Bias,Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations,"In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.",[],,"['Stefan Sylvius Wagner', 'Stefan Harmeling']","['University of Düsseldorf', 'Technische Universität Dortmund']",
https://openreview.net/forum?id=cU20finY8V,Fairness & Bias,Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics,"Despite attaining high empirical generalization, the sharpness of models trained with sharpness-aware minimization (SAM) do not always correlate with generalization error. Instead of viewing SAM as minimizing sharpness to improve generalization, our paper considers a new perspective based on SAM's training dynamics. We propose that perturbations in SAM perform *perturbed forgetting*, where they discard undesirable model biases to exhibit learning signals that generalize better. We relate our notion of forgetting to the information bottleneck principle, use it to explain observations like the better generalization of smaller perturbation batches, and show that perturbed forgetting can exhibit a stronger correlation with generalization than flatness. While standard SAM targets model biases exposed by the steepest ascent directions, we propose a new perturbation that targets biases exposed through the model's outputs. Our output bias forgetting perturbations outperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and transfer to CIFAR-10,100, while sometimes converging to sharper regions. Our results suggest that the benefits of SAM can be explained by alternative mechanistic principles that do not require flatness of the loss surface.",[],,"['Ankit Vani', 'Frederick Tung', 'Gabriel L. Oliveira', 'Hossein Sharifi-Noghabi']","['Borealis AI', 'Borealis AI', 'Borealis AI', 'Borealis AI']",
https://openreview.net/forum?id=FWlNA3et6X,Security,To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models,"LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time. This fact is known to be the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility. We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM. We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches.",[],,"['George-Octavian Bărbulescu', 'Peter Triantafillou']","['Department of Computer Science, University of Warwick', 'University of Warwick']",
https://openreview.net/forum?id=RKlmOBFwAh,Security,Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples,"In guaranteeing the absence of adversarial examples in an instance's neighbourhood, certification mechanisms play an important role in demonstrating neural net robustness. In this paper, we ask if these certifications can compromise the very models they help to protect? Our new *Certification Aware Attack* exploits certifications to produce computationally efficient norm-minimising adversarial examples $74$% more often than comparable attacks, while reducing the median perturbation norm by more than $10$%. While these attacks can be used to assess the tightness of certification bounds, they also highlight that releasing certifications can paradoxically reduce security.",[],,"['Andrew Craig Cullen', 'Shijie Liu', 'Paul Montague', 'Sarah Monazam Erfani', 'Benjamin I. P. Rubinstein']","['School of Computing and Information Systems, The University of Melbourne', 'University of Melbourne', 'Defence Science and Technology Group', 'The University of Melbourne', 'The University of Melbourne']",
https://openreview.net/forum?id=vn92qYjL1F,Security,Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection,"Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by Goldwasser et. al showed that rejection combined with transduction can give *provable* guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks (GMSA), Goldwasser et al.'s work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction+rejection in more realistic scenarios. Our key observation is that a novel application of a reduction technique by Tramèr, which was until now only used to demonstrate the vulnerability of certain defenses, can be used to actually construct effective defenses. Theoretically, we show that a careful application of this technique in the transductive setting can give significantly improved sample-complexity for robust generalization. Our theory guides us to design a new transductive algorithm for learning a selective model; extensive experiments using state of the art attacks (AutoAttack, GMSA) show that our approach provides significantly better robust accuracy (81.6% on CIFAR-10 and 57.9% on CIFAR-100 under $l_\infty$ with budget 8/255) than existing techniques. The implementation is available at https://github.com/nilspalumbo/transduction-rejection.",[],,"['Nils Palumbo', 'Yang Guo', 'Xi Wu', 'Jiefeng Chen', 'Yingyu Liang', 'Somesh Jha']","['Computer Sciences, University of Wisconsin - Madison', 'Department of Computer Science, University of Wisconsin, Madison', 'Google', 'Google', '', 'L and S, Department of Computer Science, University of Wisconsin, Madison']",
https://openreview.net/forum?id=H8pMSJwRD5,Security,Don't be so Negative! Score-based Generative Modeling with Oracle-assisted Guidance,"Score-based diffusion models are a powerful class of generative models, widely utilized across diverse domains. Despite significant advancements in large-scale tasks such as text-to-image generation, their application to constrained domains has received considerably less attention. This work addresses model learning in a setting where, in addition to the training dataset, there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling methodology, Gen-neG, that leverages this additional side-information. Gen-neG builds on classifier guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.",[],,"['Saeid Naderiparizi', 'Xiaoxuan Liang', 'Setareh Cohan', 'Berend Zwartsenberg', 'Frank Wood']","['Computer Science, University of British Columbia', 'University of British Columbia', 'Computer Science, University of British Columbia', 'Inverted AI', 'Computer Science, University of British Columbia']",
https://openreview.net/forum?id=reB9FFAaKw,Security,SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP,"In this paper, we study safe data collection for the purpose of policy evaluation in tabular Markov decision processes (MDPs). In policy evaluation, we are given a target policy and asked to estimate the expected cumulative reward it will obtain. Policy evaluation requires data and we are interested in the question of what *behavior* policy should collect the data for the most accurate evaluation of the target policy. While prior work has considered behavior policy selection, in this paper, we additionally consider a safety constraint on the behavior policy. Namely, we assume there exists a known default policy that incurs a particular expected cost when run and we enforce that the cumulative cost of all behavior policies ran is better than a constant factor of the cost that would be incurred had we always run the default policy. We first show that there exists a class of intractable MDPs where no safe oracle algorithm with knowledge about problem parameters can efficiently collect data and satisfy the safety constraints. We then define the tractability condition for an MDP such that a safe oracle algorithm can efficiently collect data and using that we prove the first lower bound for this setting. We then introduce an algorithm SaVeR for this problem that approximates the safe oracle algorithm and bound the finite-sample mean squared error of the algorithm while ensuring it satisfies the safety constraint. Finally, we show in simulations that SaVeR produces low MSE policy evaluation while satisfying the safety constraint.",[],,"['Subhojyoti Mukherjee', 'Josiah P. Hanna', 'Robert D Nowak']","['University of Wisconsin, Madison', 'University of Wisconsin - Madison', 'University of Wisconsin, Madison']",
https://openreview.net/forum?id=q0lxAs5GGO,Transparency & Explainability,Disentanglement Learning via Topology,"We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting us to apply it to problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.",[],,"['Nikita Balabin', 'Daria Voronkova', 'Ilya Trofimov', 'Evgeny Burnaev', 'Serguei Barannikov']","['Skolkovo Institute of Science and Technology', '', 'Skoltech', 'Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology']",
https://openreview.net/forum?id=VZ5A0LPbnc,Transparency & Explainability,Codebook Features: Sparse and Discrete Interpretability for Neural Networks,"Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. In addition, we can control a model's behavior by finding codes that activate on a desired behavior, then activating those same codes during generation. We first validate codebook features on a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. We then train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics and pronoun genders by activating these codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at [this URL](https://github.com/taufeeque9/codebook-features).",[],,"['Alex Tamkin', 'Mohammad Taufeeque', 'Noah Goodman']","['', 'FAR.AI', 'Stanford University']",
https://openreview.net/forum?id=jRX6yCxFhx,Transparency & Explainability,Position: On the Societal Impact of Open Foundation Models,"Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on *open* foundation models, defined here as those with broadly available model weights (e.g., Llama 3, Stable Diffusion XL). We identify five distinctive properties (e.g., greater customizability, poor monitoring) that mediate their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their *marginal risk*. Across several misuse vectors (e.g., cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work helps support a more grounded assessment of the societal impact of open foundation models by outlining what research is needed to empirically validate their theoretical benefits and risks.",[],,"['Sayash Kapoor', 'Rishi Bommasani', 'Kevin Klyman', 'Shayne Longpre', 'Ashwin Ramaswami', 'Peter Cihon', 'Aspen K Hopkins', 'Kevin Bankston', 'Stella Biderman', 'Miranda Bogen', 'Rumman Chowdhury', 'Alex Engler', 'Peter Henderson', 'Yacine Jernite', 'Seth Lazar', 'Stefano Maffulli', 'Alondra Nelson', 'Joelle Pineau', 'Aviya Skowron', 'Dawn Song', 'Victor Storchan', 'Daniel Zhang', 'Daniel E. Ho', 'Percy Liang', 'Arvind Narayanan']","['Princeton University', 'Stanford University', 'Stanford University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'EleutherAI', 'Facebook', 'Twitter', 'Princeton University', ', New York University', 'Australian National University', 'Facebook', 'EleutherAI', 'University of California Berkeley']",
https://openreview.net/forum?id=bZNH0SU37Y,Transparency & Explainability,On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data,"We consider the effect of temporal aggregation on instantaneous (non-temporal) causal discovery in general setting. This is motivated by the observation that the true causal time lag is often considerably shorter than the observational interval. This discrepancy leads to high aggregation, causing time-delay causality to vanish and instantaneous dependence to manifest. Although we expect such instantaneous dependence has consistency with the true causal relation in certain sense to make the discovery results meaningful, it remains unclear what type of consistency we need and when will such consistency be satisfied. We proposed functional consistency and conditional independence consistency in formal way correspond functional causal model-based methods and conditional independence-based methods respectively and provide the conditions under which these consistencies will hold. We show theoretically and experimentally that causal discovery results may be seriously distorted by aggregation especially in complete nonlinear case and we also find causal relationship still recoverable from aggregated data if we have partial linearity or appropriate prior. Our findings suggest community should take a cautious and meticulous approach when interpreting causal discovery results from such data and show why and when aggregation will distort the performance of causal discovery methods.",[],,"['Shunxing Fan', 'Mingming Gong', 'Kun Zhang']","['Mohamed bin Zayed University of Artificial Intelligence', 'School of mathematics and statistics, University of Melbourne', 'Mohamed bin Zayed University of Artificial Intelligence']",
https://openreview.net/forum?id=k2axqNsVVO,Fairness & Bias,Self-Driven Entropy Aggregation for Byzantine-Robust Heterogeneous Federated Learning,"Federated learning presents massive potential for privacy-friendly collaboration. However, the performance of federated learning is deeply affected by byzantine attacks, where malicious clients deliberately upload crafted vicious updates. While various robust aggregations have been proposed to defend against such attacks, they are subject to certain assumptions: homogeneous private data and related proxy datasets. To address these limitations, we propose Self-Driven Entropy Aggregation (SDEA), which leverages the random public dataset to conduct Byzantine-robust aggregation in heterogeneous federated learning. For Byzantine attackers, we observe that benign ones typically present more confident (sharper) predictions than evils on the public dataset. Thus, we highlight benign clients by introducing learnable aggregation weight to minimize the instance-prediction entropy of the global model on the random public dataset. Besides, with inherent data heterogeneity in federated learning, we reveal that it brings heterogeneous sharpness. Specifically, clients are optimized under distinct distribution and thus present fruitful predictive preferences. The learnable aggregation weight blindly allocates high attention to limited ones for sharper predictions, resulting in a biased global model. To alleviate this problem, we encourage the global model to offer diverse predictions via batch-prediction entropy maximization and conduct clustering to equally divide honest weights to accommodate different tendencies. This endows SDEA to detect Byzantine attackers in heterogeneous federated learning. Empirical results demonstrate the effectiveness.",[],,"['Wenke Huang', 'Zekun Shi', 'Mang Ye', 'He Li', 'Bo Du']","['Wuhan University, Wuhan University', 'Xiaomi Corporation', 'Wuhan University', 'School of Computer Science, Wuhan University', 'School of Computer Science, Wuhan University']",
https://openreview.net/forum?id=6PqWuSuWvX,Security,Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation,"Black-box finetuning is an emerging interface for adapting state-of-the-art language models to user needs. However, such access may also let malicious actors undermine model safety. To demonstrate the challenge of defending finetuning interfaces, we introduce covert malicious finetuning, a method to compromise model safety via finetuning while evading detection. Our method constructs a malicious dataset where every individual datapoint appears innocuous, but finetuning on the dataset teaches the model to respond to encoded harmful requests with encoded harmful responses. Applied to GPT-4, our method produces a finetuned model that acts on harmful instructions 99% of the time and avoids detection by defense mechanisms such as dataset inspection, safety evaluations, and input/output classifiers. Our findings question whether black-box finetuning access can be secured against sophisticated adversaries.",[],,"['Danny Halawi', 'Alexander Wei', 'Eric Wallace', 'Tony Tong Wang', 'Nika Haghtalab', 'Jacob Steinhardt']","['Electrical Engineering & Computer Science, University of California, Berkeley', 'University of California Berkeley', 'OpenAI', 'Massachusetts Institute of Technology', 'University of California Berkeley', 'University of California Berkeley']",
https://openreview.net/forum?id=D7wi9LIE6i,Transparency & Explainability,Improved Dimensionality Dependence for Zeroth-Order Optimisation over Cross-Polytopes,"This work proposes an algorithm improving the dimensionality dependence for gradient-free optimisation over cross-polytopes, which has many applications such as adversarial attacks, explainable AI and sparse regression. For bandit convex optimisation with two-point feedback over cross-polytopes, the state-of-the-art algorithms have a dimensionality dependence of $\mathcal{O}(\sqrt{d\log d})$, while the known lower bound is of the form $\Omega(\sqrt{d(\log d)^{-1}})$. We propose a mirror descent algorithm equipped with a symmetric version of the negative $\frac{1}{2}$-Tsallis entropy. Combined with an $\ell_1$-ellipsoidal smoothing-based gradient estimator, the proposed algorithm guarantees a dimensionality dependence on $\mathcal{O}(\sqrt{d})$, which improves the state-of-the-art algorithms by a factor of $\sqrt{\log d}$. The idea can be further applied to optimising non-smooth and non-convex functions. We propose an algorithm with a convergence depending on $\mathcal{O}(d)$, which is the best-known dimensionality dependence.",[],,['Weijia Shao'],"['Unit 2.6, Federal Institute for Occupational Safety and Health']",
https://openreview.net/forum?id=o8AaRKbP9K,Fairness & Bias,Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?,"Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms -- such as gradient descent -- with their weights in a single forward pass. Recently, there has been progress in understanding this complex phenomenon from an expressivity point of view, by demonstrating that Transformers can express such multi-step algorithms. However, our knowledge about the more fundamental aspect of its learnability, beyond single layer models, is very limited. In particular, *can training Transformers enable convergence to algorithmic solutions*? In this work we resolve this for in context linear regression with linear looped Transformers -- a multi-layer model with weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms. More specifically, for this setting we show that the global minimizer of the population training loss implements multi-step preconditioned gradient descent, with a preconditioner that adapts to the data distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss, despite the non-convexity of the landscape, by proving a novel gradient dominance condition. To our knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We further validate our theoretical findings through synthetic experiments.",[],,"['Khashayar Gatmiry', 'Nikunj Saunshi', 'Sashank J. Reddi', 'Stefanie Jegelka', 'Sanjiv Kumar']","['Massachusetts Institute of Technology', 'Google', 'Google', 'Computer Science, Technische Universität München', 'Google']",
https://openreview.net/forum?id=Y4VgJfbjfl,Privacy & Data Governance,CuTS: Customizable Tabular Synthetic Data Generation,"Privacy, data quality, and data sharing concerns pose a key limitation for tabular data applications. While generating synthetic data resembling the original distribution addresses some of these issues, most applications would benefit from additional customization on the generated data. However, existing synthetic data approaches are limited to particular constraints, e.g., differential privacy (DP) or fairness. In this work, we introduce CuTS, the first customizable synthetic tabular data generation framework. Customization in CuTS is achieved via declarative statistical and logical expressions, supporting a wide range of requirements (e.g., DP or fairness, among others). To ensure high synthetic data quality in the presence of custom specifications, CuTS is pre-trained on the original dataset and fine-tuned on a differentiable loss automatically derived from the provided specifications using novel relaxations. We evaluate CuTS over four datasets and on numerous custom specifications, outperforming state-of-the-art specialized approaches on several tasks while being more general. In particular, at the same fairness level, we achieve 2.3% higher downstream accuracy than the state-of-the-art in fair synthetic data generation on the Adult dataset.",[],,"['Mark Vero', 'Mislav Balunovic', 'Martin Vechev']","['Deparment of Computer Science, ETHZ-ETH Zurich', 'Institute for Computer Science, Artificial Intelligence and Technology', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=MgTzMaYHvG,Security,Instruction Tuning for Secure Code Generation,"Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.",[],,"['Jingxuan He', 'Mark Vero', 'Gabriela Krasnopolska', 'Martin Vechev']","['University of California, Berkeley', 'Deparment of Computer Science, ETHZ-ETH Zurich', 'ETHZ - ETH Zurich', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=j5csKrtyAe,Security,Position: Fundamental Limitations of LLM Censorship Necessitate New Approaches,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship methods have proven to be fallible at ensuring that LLMs do not return semantically impermissible responses. We present fundamental limitations of verifying the semantic properties of LLM outputs and identifying compositional threats, illustrating inherent challenges of current approaches to censoring LLM outputs. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, and semantic properties of LLM outputs can become impossible to verify when the LLM is capable of providing ""encrypted"" outputs. We further show challenges of censorship can extend beyond just semantic censorship, as attackers can reconstruct impermissible outputs from a collection of permissible ones. Consequently, we call for a re-evaluation of the problem of censorship and its goals, stressing the need for new definitions and approaches to censorship. In addition, we provide an initial attempt toward achieving this goal through syntactic censorship, drawing from a security perspective to design censorship methods that can provide guarantees.",[],,"['David Glukhov', 'Ilia Shumailov', 'Yarin Gal', 'Nicolas Papernot', 'Vardan Papyan']","['University of Toronto', 'Google DeepMind', 'University of Oxford', 'University of Toronto', 'University of Toronto']",
https://openreview.net/forum?id=S1gSrruVd4,Fairness & Bias,On the Independence Assumption in Neurosymbolic Learning,"State-of-the-art neurosymbolic learning systems use probabilistic reasoning to guide neural networks towards predictions that conform to logical constraints. Many such systems assume that the probabilities of the considered symbols are conditionally independent given the input to simplify learning and reasoning. We study and criticise this assumption, highlighting how it can hinder optimisation and prevent uncertainty quantification. We prove that loss functions bias conditionally independent neural networks to become overconfident in their predictions. As a result, they are unable to represent uncertainty over multiple valid options. Furthermore, we prove that the minima of such loss functions are usually highly disconnected and non-convex, and thus difficult to optimise. Our theoretical analysis gives the foundation for replacing the conditional independence assumption and designing more expressive neurosymbolic probabilistic models.",[],,"['Emile van Krieken', 'Pasquale Minervini', 'Edoardo Ponti', 'Antonio Vergari']","['Edinburgh University, University of Edinburgh', 'Informatics, University of Edinburgh, University of Edinburgh', 'Informatics, University of Edinburgh', 'University of Edinburgh, University of Edinburgh']",
https://openreview.net/forum?id=q5Bg858Hef,Transparency & Explainability,Disguised Copyright Infringement of Latent Diffusion Models,"Copyright infringement may occur when a generative model produces samples substantially similar to some copyrighted data that it had access to during the training phase. The notion of access usually refers to including copyrighted samples directly in the training dataset, which one may inspect to identify an infringement. We argue that such visual auditing largely overlooks a concealed copyright infringement, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it. Such disguises only require indirect access to the copyrighted material and cannot be visually distinguished, thus easily circumventing the current auditing tools. In this paper, we provide a better understanding of such disguised copyright infringement by uncovering the disguises generation algorithm, the revelation of the disguises, and importantly, how to detect them to augment the existing toolbox. Additionally, we introduce a broader notion of acknowledgment for comprehending such indirect access.",[],,"['Yiwei Lu', 'Matthew Y. R. Yang', 'Zuoqiu Liu', 'Gautam Kamath', 'Yaoliang Yu']","['University of Waterloo', 'Carnegie Mellon University', 'University of Waterloo', 'Computer Science, Courant Institute of Mathematical Sciences, New York University', 'University of Waterloo']",
https://openreview.net/forum?id=5PQhu8flSO,Fairness & Bias,Detecting and Identifying Selection Structure in Sequential Data,"We argue that the selective inclusion of data points based on latent objectives is common in practical situations, such as music sequences. Since this selection process often distorts statistical analysis, previous work primarily views it as a bias to be corrected and proposes various methods to mitigate its effect. However, while controlling this bias is crucial, selection also offers an opportunity to provide a deeper insight into the hidden generation process, as it is a fundamental mechanism underlying what we observe. In particular, overlooking selection in sequential data can lead to an incomplete or overcomplicated inductive bias in modeling, such as assuming a universal autoregressive structure for all dependencies. Therefore, rather than merely viewing it as a bias, we explore the causal structure of selection in sequential data to delve deeper into the complete causal process. Specifically, we show that selection structure is identifiable without any parametric assumptions or interventional experiments. Moreover, even in cases where selection variables coexist with latent confounders, we still establish the nonparametric identifiability under appropriate structural conditions. Meanwhile, we also propose a provably correct algorithm to detect and identify selection structures as well as other types of dependencies. The framework has been validated empirically on both synthetic data and real-world music.",[],,"['Yujia Zheng', 'Zeyu Tang', 'Yiwen Qiu', 'Bernhard Schölkopf', 'Kun Zhang']","['Carnegie Mellon University', 'Carnegie Mellon University', 'CMU, Carnegie Mellon University', '', 'Mohamed bin Zayed University of Artificial Intelligence']",
https://openreview.net/forum?id=stMhi1Sn2G,Fairness & Bias,Accelerated Speculative Sampling Based on Tree Monte Carlo,"Speculative Sampling (SpS) has been introduced to speed up inference of large language models (LLMs) by generating multiple tokens in a single forward pass under the guidance of a reference model, while preserving the original distribution. We observe that SpS can be derived through maximum coupling on the token distribution. However, we find that this approach is not optimal as it applies maximum coupling incrementally for each new token, rather than seeking a global maximum coupling that yields a faster algorithm, given the tree-space nature of LLM generative distributions. In this paper, we shift our focus from distributions on a token space to those on a tree space. We propose a novel class of Tree Monte Carlo (TMC) methods, demonstrating their unbiasedness and convergence. As a particular instance of TMC, our new algorithm, Accelerated Speculative Sampling (ASpS), outperforms traditional SpS by generating more tokens per step on average, achieving faster inference, while maintaining the original distribution.",[],,"['Zhengmian Hu', 'Heng Huang']","['Adobe Systems', 'Department of Computer Science, University of Maryland, College Park']",
https://openreview.net/forum?id=FPnUhsQJ5B,Fairness & Bias,Scaling Rectified Flow Transformers for High-Resolution Image Synthesis,"Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.",[],,"['Patrick Esser', 'Sumith Kulal', 'Andreas Blattmann', 'Rahim Entezari', 'Jonas Müller', 'Harry Saini', 'Yam Levi', 'Dominik Lorenz', 'Axel Sauer', 'Frederic Boesel', 'Dustin Podell', 'Tim Dockhorn', 'Zion English', 'Robin Rombach']","['Heidelberg University', 'Stability AI', 'StabilityAI', 'Stability', 'Stability AI', 'National Institute of Technology Hamirpur, Dhirubhai Ambani Institute Of Information and Communication Technology', 'Stability AI', 'Stability AI', 'Stability AI', 'Stability AI', 'Stability AI', 'Stability AI', '', 'Ludwig-Maximilians-Universität München']",
https://openreview.net/forum?id=zcIV8OQFVF,Fairness & Bias,ODIN: Disentangled Reward Mitigates Hacking in RLHF,"In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators and achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads to predict the preference, one trained to correlate with length and the other trained to decorrelate with length and therefore focusing more on the actual content. We then discard the length head in RL to ignore the spurious length reward. Experiments demonstrate that our approach eliminates the reward correlation with length, and improves the obtained policy by a significant margin.",[],,"['Lichang Chen', 'Chen Zhu', 'Jiuhai Chen', 'Davit Soselia', 'Tianyi Zhou', 'Tom Goldstein', 'Heng Huang', 'Mohammad Shoeybi', 'Bryan Catanzaro']","['Google', 'Meta', 'Department of Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park', 'NVIDIA', 'ADLR, NVIDIA']",
https://openreview.net/forum?id=Fw4fBE2rqW,Security,On Discrete Prompt Optimization for Diffusion Models,"This paper introduces the first gradient-based framework for prompt optimization in text-to-image diffusion models. We formulate prompt engineering as a discrete optimization problem over the language space. Two major challenges arise in efficiently finding a solution to this problem: (1) Enormous Domain Space: Setting the domain to the entire language space poses significant difficulty to the optimization process. (2) Text Gradient: Efficiently computing the text gradient is challenging, as it requires backpropagating through the inference steps of the diffusion model and a non-differentiable embedding lookup table. Beyond the problem formulation, our main technical contributions lie in solving the above challenges. First, we design a family of dynamically generated compact subspaces comprised of only the most relevant words to user input, substantially restricting the domain space. Second, we introduce ``Shortcut Text Gradient"" --- an effective replacement for the text gradient that can be obtained with constant memory and runtime. Empirical evaluation on prompts collected from diverse sources (DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that substantially improve (prompt enhancement) or destroy (adversarial attack) the faithfulness of images generated by the text-to-image diffusion model.",[],,"['Ruochen Wang', 'Ting Liu', 'Cho-Jui Hsieh', 'Boqing Gong']","['University of California, Los Angeles', 'Google DeepMind', 'Google', 'Boston University, Boston University']",
https://openreview.net/forum?id=hqNz4LDuhn,Fairness & Bias,Nearest Neighbour Score Estimators for Diffusion Generative Models,"Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research. Code will be released upon paper acceptance.",[],,"['Matthew Niedoba', 'Dylan Green', 'Saeid Naderiparizi', 'Vasileios Lioutas', 'Jonathan Wilder Lavington', 'Xiaoxuan Liang', 'Yunpeng Liu', 'Ke Zhang', 'Setareh Dabiri', 'Adam Scibior', 'Berend Zwartsenberg', 'Frank Wood']","['Computer Science, University of British Columbia', 'Computer Science, University of British Columbia', 'Computer Science, University of British Columbia', 'Computer Science, University of British Columbia', 'University of British Columbia', 'University of British Columbia', 'Computer Science, University of British Columbia', 'Computer Science, University of British Columbia', '', 'University of British Columbia', 'Inverted AI', 'Computer Science, University of British Columbia']",
https://openreview.net/forum?id=O8rrXl71D5,Transparency & Explainability,What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation,"In-context learning is a powerful emergent ability in transformer models. Prior work in mechanistic interpretability has identified a circuit element that may be critical for in-context learning – the induction head (IH), which performs a match-and-copy operation. During training of large transformers on natural language data, IHs emerge around the same time as a notable phase change in the loss. Despite the robust evidence for IHs and this interesting coincidence with the phase change, relatively little is known about the diversity and emergence dynamics of IHs. Why is there more than one IH, and how are they dependent on each other? Why do IHs appear all of a sudden, and what are the subcircuits that enable them to emerge? We answer these questions by studying IH emergence dynamics in a controlled setting by training on synthetic data. In doing so, we develop and share a novel optogenetics-inspired causal framework for modifying activations throughout training. Using this framework, we delineate the diverse and additive nature of IHs. By ""clamping"" subsets of activations throughout training, we then identify three underlying subcircuits that interact to drive IH formation, yielding the phase change. Furthermore, these subcircuits shed light on data-dependent properties of formation, such as phase change timing, already showing the promise of this more in-depth understanding of subcircuits that need to ""go right"" for an induction head.",[],,"['Aaditya K Singh', 'Ted Moskovitz', 'Felix Hill', 'Stephanie C.Y. Chan', 'Andrew M Saxe']","['Gatsby Computational Neuroscience Unit, University College London, University of London', 'Gatsby Computational Neuroscience Unit', 'Google', 'DeepMind', 'University College London, University of London']",
https://openreview.net/forum?id=K6xxnKN2gm,Security,Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications,"Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3$ % at the parameter level and $2.5$ % at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.",[],,"['Boyi Wei', 'Kaixuan Huang', 'Yangsibo Huang', 'Tinghao Xie', 'Xiangyu Qi', 'Mengzhou Xia', 'Prateek Mittal', 'Mengdi Wang', 'Peter Henderson']","['ECE, Princeton University', 'Princeton University', 'Google', 'Electrical and Computer Engineering, Princeton University', 'Princeton University', 'Department of Computer Science, Princeton University', 'Princeton University', 'ECE, Princeton University', 'Princeton University']",
https://openreview.net/forum?id=byxXa99PtF,Transparency & Explainability,Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling,"Uncertainty decomposition refers to the task of decomposing the total uncertainty of a predictive model into aleatoric (data) uncertainty, resulting from inherent randomness in the data-generating process, and epistemic (model) uncertainty, resulting from missing information in the model's training data. In large language models (LLMs) specifically, identifying sources of uncertainty is an important step toward improving reliability, trustworthiness, and interpretability, but remains an important open research question. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarification ensembling, which can be applied to any pre-trained LLM. Our approach generates a set of clarifications for the input, feeds them into an LLM, and ensembles the corresponding predictions. We show that, when aleatoric uncertainty arises from ambiguity or under-specification in LLM inputs, this approach makes it possible to factor an (un-clarified) LLM's predictions into separate aleatoric and epistemic terms, using a decomposition similar to the one employed by Bayesian neural networks. Empirical evaluations demonstrate that input clarification ensembling provides accurate and reliable uncertainty quantification on several language processing tasks. Code and data are available at https://github.com/UCSB-NLP-Chang/llm_uncertainty.",[],,"['Bairu Hou', 'Yujian Liu', 'Kaizhi Qian', 'Jacob Andreas', 'Shiyu Chang', 'Yang Zhang']","['Computer Science, University of California, Santa Barbara', 'University of California, Santa Barbara', 'International Business Machines', 'Massachusetts Institute of Technology', 'University of California, Santa Barbara', 'International Business Machines']",
https://openreview.net/forum?id=wCMNbdshcY,Security,Fast Adversarial Attacks on Language Models In One GPU Minute,"In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. BEAST can also generate adversarial suffixes for successful jailbreaks that can transfer to unseen prompts and unseen models such as GPT-4-Turbo. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack causes Vicuna-7B-v1.5 to produce $\sim$15% more incorrect outputs when compared to LM outputs in the absence of our attack. We also learn that 22% of the time, BEAST causes Vicuna to generate outputs that are not relevant to the original prompt. Further, we use BEAST to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for LMs. We believe that our fast attack, BEAST, has the potential to accelerate research in LM security and privacy.",[],,"['Vinu Sankar Sadasivan', 'Shoumik Saha', 'Gaurang Sriramanan', 'Priyatham Kattakinda', 'Atoosa Chegini', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park', 'Electrical Engineering, University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']",
https://openreview.net/forum?id=Zo9zXdVhW2,Transparency & Explainability,Probabilistic Constrained Reinforcement Learning with Formal Interpretability,"Reinforcement learning can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and the corresponding optimal policy. Consequently, representing sequential decision-making problems as probabilistic inference can have considerable value, as, in principle, the inference offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of policy optimization. In this study, we propose a novel Adaptive Wasserstein Variational Optimization, namely AWaVO, to tackle these interpretability challenges. Our approach uses formal methods to achieve the interpretability: convergence guarantee, training transparency, and intrinsic decision-interpretation. To demonstrate its practicality, we showcase guaranteed interpretability including a global convergence rate $\Theta(1/\sqrt{T})$ not only in simulation but also in real-world quadrotor tasks. In comparison with state-of-the-art benchmarks, including TRPO-IPO, PCPO, and CRPO, we empirically verify that AWaVO offers a reasonable trade-off between high performance and sufficient interpretability.",[],,"['YANRAN WANG', 'QIUCHEN QIAN', 'David Boyle']","['Dyson School, Department of Engineering, Imperial College London', 'Imperial College London, Imperial College London', 'Imperial College London, Imperial College London']",
https://openreview.net/forum?id=MlzUD5CKvZ,Transparency & Explainability,"Improving Prototypical Visual Explanations with Reward Reweighing, Reselection, and Retraining","In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the Prototypical Part Network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this architecture is able to produce visually interpretable classifications, it often learns to classify based on parts of the image that are not semantically meaningful. To address this problem, we propose the Reward Reweighing, Reselecting, and Retraining (R3) post-processing framework, which performs three additional corrective updates to a pretrained ProtoPNet in an offline and efficient manner. The first two steps involve learning a reward model based on collected human feedback and then aligning the prototypes with human preferences. The final step is retraining, which realigns the base features and the classifier layer of the original model with the updated prototypes. We find that our R3 framework consistently improves both the interpretability and the predictive accuracy of ProtoPNet and its variants.",[],,"['Aaron Jiaxun Li', 'Robin Netzorg', 'Zhihan Cheng', 'Zhuoqin Zhang', 'Bin Yu']","['Harvard University', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California Berkeley']",
https://openreview.net/forum?id=oBYv73nOoA,Transparency & Explainability,SPADE: Sparsity-Guided Debugging for Deep Neural Networks,"It is known that sparsity can improve interpretability for deep neural networks. However, existing methods in the area either require networks that are pre-trained with sparsity constraints, or impose sparsity after the fact, altering the network's general behavior. In this paper, we demonstrate, for the first time, that sparsity can instead be incorporated into the interpretation process itself, as a sample-specific preprocessing step. Unlike previous work, this approach, which we call SPADE, does not place constraints on the trained model and does not affect its behavior during inference on the sample. Given a trained model and a target sample, SPADE uses sample-targeted pruning to provide a ""trace"" of the network's execution on the sample, reducing the network to the most important connections prior to computing an interpretation. We demonstrate that preprocessing with SPADE significantly increases the accuracy of image saliency maps across several interpretability methods. Additionally, SPADE improves the usefulness of neuron visualizations, aiding humans in reasoning about network behavior. Our code is available at https://github.com/IST-DASLab/SPADE.",[],,"['Arshia Soltani Moakhar', 'Eugenia Iofinova', 'Elias Frantar', 'Dan Alistarh']","['Computer Science , University of Maryland, College Park', 'Computer Science, Data Science, Institute of Science and Technology Austria', 'Google DeepMind', 'Institute of Science and Technology']",
https://openreview.net/forum?id=2B2U5kkGUA,Security,On the Duality Between Sharpness-Aware Minimization and Adversarial Training,"Adversarial Training (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against adversarial attacks, yet suffers from inevitably decreased clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing adversarial robustness remains unexplored. In this work, considering the duality between SAM and AT, we investigate the adversarial robustness derived from SAM. Intriguingly, we find that using SAM alone can improve adversarial robustness. To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive experiments to show that SAM can improve adversarial robustness notably without sacrificing any clean accuracy, shedding light on the potential of SAM to be a substitute for AT when accuracy comes at a higher priority. Code is available at https://github.com/weizeming/SAM_AT.",[],,"['Yihao Zhang', 'Hangzhou He', 'Jingyu Zhu', 'Huanran Chen', 'Yifei Wang', 'Zeming Wei']","['Peking University', 'College of Future Technology, Peking University', 'School of Mathematical Sciences, Peking University', 'AI, Tsinghua University, Tsinghua University', 'Massachusetts Institute of Technology', 'School of mathematical Science, Peking University']",
https://openreview.net/forum?id=OMKNBzf6HJ,Fairness & Bias,Liouville Flow Importance Sampler,"We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.",[],,"['Yifeng Tian', 'Nishant Panda', 'Yen Ting Lin']","['Los Alamos National Laboratory', 'Los Alamos National Laboratory', 'Los Alamos National Laboratory']",
https://openreview.net/forum?id=RbnojVv4HK,Fairness & Bias,Ameliorate Spurious Correlations in Dataset Condensation,"Dataset Condensation has emerged as a technique for compressing large datasets into smaller synthetic counterparts, facilitating downstream training tasks. In this paper, we study the impact of bias inside the original dataset on the performance of dataset condensation. With a comprehensive empirical evaluation on canonical datasets with color, corruption and background biases, we found that color and background biases in the original dataset will be amplified through the condensation process, resulting in a notable decline in the performance of models trained on the condensed dataset, while corruption bias is suppressed through the condensation process. To reduce bias amplification in dataset condensation, we introduce a simple yet highly effective approach based on a sample reweighting scheme utilizing kernel density estimation. Empirical results on multiple real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Notably, on CMNIST with 5% bias-conflict ratio and IPC 50, our method achieves 91.5% test accuracy compared to 23.8% from vanilla DM, boosting the performance by 67.7%, whereas applying state-of-the-art debiasing method on the same dataset only achieves 53.7% accuracy. Our findings highlight the importance of addressing biases in dataset condensation and provide a promising avenue to address bias amplification in the process.",[],,"['Justin Cui', 'Ruochen Wang', 'Yuanhao Xiong', 'Cho-Jui Hsieh']","[', University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles', 'Google']",
https://openreview.net/forum?id=DYMj03Gbri,Security,Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast,"A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate.",[],,"['Xiangming Gu', 'Xiaosen Zheng', 'Tianyu Pang', 'Chao Du', 'Qian Liu', 'Ye Wang', 'Jing Jiang', 'Min Lin']","['National University of Singapore', 'Singapore Management University', 'Sea AI Lab', 'Sea AI Lab', 'Tiktok', 'National University of Singapore', 'School of Computing, Australian National University', 'Sea AI Lab']",
https://openreview.net/forum?id=fSNHK7mu3j,Fairness & Bias,Graph Neural Networks Use Graphs When They Shouldn't,"Predictions over graphs play a crucial role in various domains, including social networks and medicine. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Although a graph-structure is provided as input to the GNN, in some cases the best solution can be obtained by ignoring it. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the given graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We analyze the implicit bias of gradient-descent learning of GNNs and prove that when the ground truth function does not use the graphs, GNNs are not guaranteed to learn a solution that ignores the graph, even with infinite data. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We also prove that within the family of regular graphs, GNNs are guaranteed to extrapolate when learning with gradient descent. Finally, based on our empirical and theoretical findings, we demonstrate on real-data how regular graphs can be leveraged to reduce graph overfitting and enhance performance.",[],,"['Maya Bechler-Speicher', 'Ido Amos', 'Ran Gilad-Bachrach', 'Amir Globerson']","['Tel Aviv University', '', 'Tel Aviv University', 'Tel Aviv University']",
https://openreview.net/forum?id=CR6Sl80cn8,Security,Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior,"This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries. Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability. However, the localized gradient is not informative enough, making these methods still query-intensive. In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks. As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss. Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior. Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound. Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks. Code is available at https://github.com/yibo-miao/PBO-Attack.",[],,"['Shuyu Cheng', 'Yibo Miao', 'Yinpeng Dong', 'Xiao Yang', 'Xiao-Shan Gao', 'Jun Zhu']","['Tsinghua University, Tsinghua University', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Computer Science, Tsinghua University']",
https://openreview.net/forum?id=j56JAd29uH,Privacy & Data Governance,FADAS: Towards Federated Adaptive Asynchronous Optimization,"Federated learning (FL) has emerged as a widely adopted training paradigm for privacy-preserving machine learning. While the SGD-based FL algorithms have demonstrated considerable success in the past, there is a growing trend towards adopting adaptive federated optimization methods, particularly for the training of large-scale models. However, the conventional synchronous aggregation design poses a significant challenge to the practical deployment of those adaptive federated optimization methods, particularly in the presence of straggler clients. To fill this research gap, this paper introduces federated adaptive asynchronous optimization, named FADAS, a novel method that incorporates asynchronous updates into adaptive federated optimization with provable guarantees. To further enhance the efficiency and resilience of our proposed method in scenarios with significant asynchronous delays, we also extend FADAS with a delay-adaptive learning adjustment strategy. We rigorously establish the convergence rate of the proposed algorithms and empirical results demonstrate the superior performance of FADAS over other asynchronous FL baselines.",[],,"['Yujia Wang', 'Shiqiang Wang', 'Songtao Lu', 'Jinghui Chen']","['Pennsylvania State University', 'IBM, International Business Machines', 'IBM Thomas J. Watson Research Center', 'Pennsylvania State University']",
https://openreview.net/forum?id=rk4kmL8aOY,Privacy & Data Governance,Reducing Item Discrepancy via Differentially Private Robust Embedding Alignment for Privacy-Preserving Cross Domain Recommendation,"Cross-Domain Recommendation (CDR) have become increasingly appealing by leveraging useful information to tackle the data sparsity problem across domains. Most of latest CDR models assume that domain-shareable user-item information (e.g., rating and review on overlapped users or items) are accessible across domains. However, these assumptions become impractical due to the strict data privacy protection policy. In this paper, we propose Reducing Item Discrepancy (RidCDR) model on solving Privacy-Preserving Cross-Domain Recommendation (PPCDR) problem. Specifically, we aim to enhance the model performance on both source and target domains without overlapped users and items while protecting the data privacy. We innovatively propose private-robust embedding alignment module in RidCDR for knowledge sharing across domains while avoiding negative transfer privately. Our empirical study on Amazon and Douban datasets demonstrates that RidCDR significantly outperforms the state-of-the-art models under the PPCDR without overlapped users and items.",[],,"['Weiming Liu', 'Xiaolin Zheng', 'Chaochao Chen', 'Jiahe Xu', 'Xinting Liao', 'Fan Wang', 'Yanchao Tan', 'Yew-Soon Ong']","['Computer Science, Zhejiang University', 'College of Computer Science , Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', '', 'Fuzhou University', 'College of Computing and Data Science, Nanyang Technological University']",
https://openreview.net/forum?id=RPMTNGMq0O,Fairness & Bias,Dealing With Unbounded Gradients in Stochastic Saddle-point Optimization,"We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.",[],,"['Gergely Neu', 'Nneka Okolo']","['Department of Engineering, Universitat Pompeu Fabra', 'Information and Communication Technology, Universitat Pompeu Fabra']",
https://openreview.net/forum?id=761UxjOTHB,Security,Recovering the Pre-Fine-Tuning Weights of Generative Models,"The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, *pre-fine-tuning* model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present *Spectral DeTuning*, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral. The code is available at https://vision.huji.ac.il/spectral_detuning/.",[],,"['Eliahu Horwitz', 'Jonathan Kahana', 'Yedid Hoshen']","['Hebrew University of Jerusalem', 'Hebrew University of Jerusalem', 'Computer Science, Hebrew University of Jerusalem']",
https://openreview.net/forum?id=DkqiId4AuR,Transparency & Explainability,"Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models","In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug and legislative bodies to audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we introduce a post-hoc method that utilizes *deep reinforcement learning* to explore and construct the landscape of failure modes in pre-trained discriminative and generative models. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically show the effectiveness of the proposed method across common Computer Vision, Natural Language Processing, and Vision-Language tasks.",[],,"['Som Sagar', 'Aditya Taparia', 'Ransalu Senanayake']","['Computer Science, Arizona State University', 'Computer Science, Arizona State University', 'School of Computing and Augmented Intelligence, Arizona State University']",
https://openreview.net/forum?id=XDz9leJ9iK,Transparency & Explainability,Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities,"The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind. In this position paper, we discuss that disparities towards marginalized communities – performance, representation, privacy, robustness, interpretability and safety – are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon. We contrast foundation models with traditional models and highlight the potential for exacerbated disparity against marginalized communities. Moreover, we emphasize the unique threat of cascading impacts in foundation models, where interconnected disparities can trigger long-lasting negative consequences, specifically to the people on the margin. We define marginalized communities within the machine learning context and explore the multifaceted nature of disparities. We analyze the sources of these disparities, tracing them from data creation, training and deployment procedures to highlight the complex technical and socio-technical landscape. To mitigate the pressing crisis, we conclude with a set of calls to action to mitigate disparity at its source.",[],,"['Golnoosh Farnadi', 'Mohammad Havaei', 'Negar Rostamzadeh']","['School of Computer Science, McGill University', 'Research, Google', 'Google']",
https://openreview.net/forum?id=bWUU0LwwMp,Transparency & Explainability,Position: TrustLLM: Trustworthiness in Large Language Models,"Large language models (LLMs) have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and capability (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones, suggesting that open-source models can achieve high levels of trustworthiness without additional mechanisms like *moderator*, offering valuable insights for developers in this field. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Besides these observations, we've uncovered key insights into the multifaceted trustworthiness in LLMs. We emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. We advocate that the establishment of an AI alliance between industry, academia, the open-source community to foster collaboration is imperative to advance the trustworthiness of LLMs.",[],,"['Yue Huang', 'Lichao Sun', 'Haoran Wang', 'Siyuan Wu', 'Qihui Zhang', 'Yuan Li', 'Chujie Gao', 'Yixin Huang', 'Wenhan Lyu', 'Yixuan Zhang', 'Xiner Li', 'Hanchi Sun', 'Zhengliang Liu', 'Yixin Liu', 'Yijue Wang', 'Zhikun Zhang', 'Bertie Vidgen', 'Bhavya Kailkhura', 'Caiming Xiong', 'Chaowei Xiao', 'Chunyuan Li', 'Eric P. Xing', 'Furong Huang', 'Hao Liu', 'Heng Ji', 'Hongyi Wang', 'Huan Zhang', 'Huaxiu Yao', 'Manolis Kellis', 'Marinka Zitnik', 'Meng Jiang', 'Mohit Bansal', 'James Zou', 'Jian Pei', 'Jian Liu', 'Jianfeng Gao', 'Jiawei Han', 'Jieyu Zhao', 'Jiliang Tang', 'Jindong Wang', 'Joaquin Vanschoren', 'John Mitchell', 'Kai Shu', 'Kaidi Xu', 'Kai-Wei Chang', 'Lifang He', 'Lifu Huang', 'Michael Backes', 'Neil Zhenqiang Gong', 'Philip S. Yu', 'Pin-Yu Chen', 'Quanquan Gu', 'Ran Xu', 'Rex Ying', 'Shuiwang Ji', 'Suman Jana', 'Tianlong Chen', 'Tianming Liu', 'Tianyi Zhou', 'William Yang Wang', 'Xiang Li', 'Xiangliang Zhang', 'Xiao Wang', 'Xing Xie', 'Xun Chen', 'Xuyu Wang', 'Yan Liu', 'Yanfang Ye', 'Yinzhi Cao', 'Yong Chen', 'Yue Zhao']","['', 'Computer Science and Engineering, Lehigh University', 'Computer Science, Emory University', 'University of Waterloo', 'The University of Queensland', 'University of Cambridge', 'Mohamed bin Zayed University of Artificial Intelligence', '', 'College of William and Mary', 'Computer Science, College of William and Mary', 'Computer Science and Engineering, Texas A&M University - College Station', 'Computer Science, Lehigh University', 'University of Georgia', '', 'Facebook', 'Zhejiang University', 'Alan Turing Institute', 'CASC, Lawrence Livermore National Laboratory', 'Salesforce Research', 'University of Wisconsin - Madison']",
https://openreview.net/forum?id=GwA4go0Mw4,Fairness & Bias,Representation Surgery: Theory and Practice of Affine Steering,"Language models often exhibit undesirable behavior, e.g., generating toxic or gender-biased text. In the case of neural language models, an encoding of the undesirable behavior is often present in the model's representations. Thus, one natural (and common) approach to prevent the model from exhibiting undesirable behavior is to steer the model's representations in a manner that reduces the probability of it generating undesirable text. This paper investigates the formal and empirical properties of steering functions, i.e., transformation of the neural language model's representations that alter its behavior. First, we derive two optimal, in the least-squares sense, affine steering functions under different constraints. Our theory provides justification for existing approaches and offers a novel, improved steering approach. Second, we offer a series of experiments that demonstrate the empirical effectiveness of the methods in mitigating bias and reducing toxic generation.",[],,"['Shashwat Singh', 'Shauli Ravfogel', 'Jonathan Herzig', 'Roee Aharoni', 'Ryan Cotterell', 'Ponnurangam Kumaraguru']","['International Institute of Information Technology Hyderabad', 'New York University', 'Research, Google', 'Google', 'Swiss Federal Institute of Technology', 'International Institute of Information Technology Hyderabad']",
https://openreview.net/forum?id=CHz7WshPcp,Fairness & Bias,Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer,"We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.",[],,"['Toru Shirakawa', 'Yi Li', 'Yulun Wu', 'Sky Qiu', 'Yuxuan Li', 'Mingduo Zhao', 'Hiroyasu Iso', 'Mark J. van der Laan']","['Osaka University Graduate School of Medicine', 'Biostatistics, University of California, Berkeley', 'University of California Berkeley', 'School of Public Health, University of California, Berkeley', 'Measurement & Evaluation & Statistics, Columbia University', 'University of California, Berkeley', 'Public Health, Osaka Medical College', 'Biostatistics and Statistics, University of California, Berkeley']",
https://openreview.net/forum?id=ElVHUWyL3n,Fairness & Bias,Dual Operating Modes of In-Context Learning,"In-context learning (ICL) exhibits dual operating modes: ***task learning***, i.e., acquiring a new skill from in-context samples, and ***task retrieval***, i.e., locating and activating a relevant pretrained skill. Recent theoretical work proposes various mathematical models to analyze ICL, but they cannot fully explain the duality. In this work, we analyze a generalized probabilistic model for pretraining data, obtaining a quantitative understanding of the two operating modes of ICL. Leveraging our analysis, we provide the first explanation of an unexplained phenomenon observed with real-world large language models (LLMs). Under some settings, the ICL risk initially increases and then decreases with more in-context examples. Our analysis offers a plausible explanation for this ""early ascent"" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels, and predict the bounded efficacy of such approaches. We corroborate our analysis and predictions with extensive experiments with Transformers and LLMs.",[],,"['Ziqian Lin', 'Kangwook Lee']","['University of Wisconsin - Madison', 'University of Wisconsin, Madison']",
https://openreview.net/forum?id=0XDO74NlOd,Transparency & Explainability,On the Role of Edge Dependency in Graph Generative Models,"We investigate the trade-off between the representation power of graph generative models and model *overlap*, i.e., the degree to which the model generates diverse outputs versus regurgitating its training data. In particular, we delineate a nested hierarchy of graph generative models categorized into three levels of complexity: edge independent, node independent, and arbitrarily dependent models. This hierarchy encapsulates a wide range of prevalent methods. We derive theoretical bounds on the number of triangles and other short-length cycles producible by each level of the hierarchy, finding that more complex dependency structure allows an improved trade-off between representation power and overlap. We provide instances demonstrating the asymptotic optimality of our bounds. Furthermore, we introduce new generative models for each of the three hierarchical levels, leveraging dense subgraph discovery. Our evaluation, conducted on real-world datasets, focuses on assessing the output quality and overlap of our proposed models in comparison to other popular models. Our results indicate that our simple, interpretable models provide competitive baselines to popular generative models. Through this investigation, we offer a structured and robust evaluation scheme, thereby facilitating the development of models capable of generating accurate and edge-diverse graphs.",[],,"['Sudhanshu Chanpuriya', 'Cameron N Musco', 'Konstantinos Sotiropoulos', 'Charalampos Tsourakakis']","['', 'University of Massachusetts, Amherst', 'Facebook', 'Boston University']",
https://openreview.net/forum?id=dFEeI51O5j,Transparency & Explainability,Self-Supervised Interpretable End-to-End Learning via Latent Functional Modularity,"We introduce MoNet, a novel functionally modular network for self-supervised and interpretable end-to-end learning. By leveraging its functional modularity with a latent-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space without requiring task-level supervision. Moreover, our method incorporates an online, post-hoc explainability approach that enhances the interpretability of end-to-end inferences without compromising sensorimotor control performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, outperforming baseline models by 7% to 28% in task specificity analysis. We further explore the interpretability of our network through post-hoc analysis of perceptual saliency maps and latent decision vectors. This provides valuable insights into the incorporation of explainable artificial intelligence into robotic learning, encompassing both perceptual and behavioral perspectives. Supplementary materials are available at https://sites.google.com/view/monet-lgc.",[],,"['Hyunki Seong', 'Hyunchul Shim']","['Electrical Engineering, KAIST, Korea Advanced Institute of Science & Technology', 'Electrical Engineering, KAIST, Korea Advanced Institute of Science & Technology']",
https://openreview.net/forum?id=ETNx4SekbY,Transparency & Explainability,Observable Propagation: Uncovering Feature Vectors in Transformers,"A key goal of current mechanistic interpretability research in NLP is to find *linear features* (also called ""feature vectors"") for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data -- both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called ""observable propagation"" (in short: ObProp), for finding linear features used by transformer language models in computing a given task -- *using almost no data*. Our paradigm centers on the concept of ""observables"", linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors, including a similarity metric between feature vectors called the *coupling coefficient* which estimates the degree to which one feature's output correlates with another's. We use ObProp to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that ObProp surpasses traditional approaches for finding feature vectors in the low-data regime, and that ObProp can be used to better understand the mechanisms responsible for bias in large language models.",[],,"['Jacob Dunefsky', 'Arman Cohan']","['Department of Computer Science, Yale University', 'Computer Science, Yale University']",
https://openreview.net/forum?id=0tuwdgBiSN,Fairness & Bias,Complexity Matters: Feature Learning in the Presence of Spurious Correlations,"Existing research often posits spurious features as **easier** to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover, studies mainly focus on end performance rather than the learning dynamics of feature learning. In this paper, we propose a theoretical framework and an associated synthetic dataset grounded in boolean function analysis. This setup allows for fine-grained control over the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlations. Our findings uncover several interesting phenomena: (1) stronger spurious correlations or simpler spurious features slow down the learning rate of the core features, (2) two distinct subnetworks are formed to learn core and spurious features separately, (3) learning phases of spurious and core features are not always separable, (4) spurious features are not forgotten even after core features are fully learned. We demonstrate that our findings justify the success of retraining the last layer to remove spurious correlation and also identifies limitations of popular debiasing algorithms that exploit early learning of spurious features. We support our empirical findings with theoretical analyses for the case of learning XOR features with a one-hidden-layer ReLU network.",[],,"['GuanWen Qiu', 'Da Kuang', 'Surbhi Goel']","['School of Engineering and Science, University of Pennsylvania', 'CIS, University of Pennsylvania, University of Pennsylvania', 'Computer and Information Science, University of Pennsylvania']",
https://openreview.net/forum?id=vFk9fqXLst,Fairness & Bias,Interpreting Equivariant Representations,"Latent representations are extensively used for tasks like visualization, interpolation, or feature extraction in deep learning models. This paper demonstrates the importance of considering the inductive bias imposed by an equivariant model when using latent representations as neglecting these biases can lead to decreased performance in downstream tasks. We propose principles for choosing invariant projections of latent representations and show their effectiveness in two examples: A permutation equivariant variational auto-encoder for molecular graph generation, where an invariant projection can be designed to maintain information without loss, and for a rotation-equivariant representation in image classification, where random invariant projections proves to retain a high degree of information. In both cases, the analysis of invariant latent representations proves superior to their equivariant counterparts. Finally, we illustrate that the phenomena documented here for equivariant neural networks have counterparts in standard neural networks where invariance is encouraged via augmentation.",[],,"['Andreas Abildtrup Hansen', 'Anna Calissano', 'Aasa Feragen']","['', 'INRIA', 'Technical University of Denmark']",
https://openreview.net/forum?id=bID9PiBFpT,Security,Policy Evaluation for Variance in Average Reward Reinforcement Learning,"We consider an average reward reinforcement learning (RL) problem and work with asymptotic variance as a risk measure to model safety-critical applications. We design a temporal-difference (TD) type algorithm tailored for policy evaluation in this context. Our algorithm is based on linear stochastic approximation of an equivalent formulation of the asymptotic variance in terms of the solution of the Poisson equation. We consider both the tabular and linear function approximation settings, and establish $\tilde {O}(1/k)$ finite time convergence rate, where $k$ is the number of steps of the algorithm. Our work paves the way for developing actor-critic style algorithms for variance-constrained RL. To the best of our knowledge, our result provides the first sequential estimator for asymptotic variance of a Markov chain with provable finite sample guarantees, which is of independent interest.",[],,"['Shubhada Agrawal', 'Prashanth L A', 'Siva Theja Maguluri']","['Statistics and Data Science, Carnegie Mellon University', 'Indian Institute of Technology, Madras', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=MrNq6rbcUi,Security,Robust Yet Efficient Conformal Prediction Sets,"Conformal prediction (CP) can convert any model's output into prediction sets guaranteed to include the true label with any user-specified probability. However, same as the model itself, CP is vulnerable to adversarial test examples (evasion) and perturbed calibration data (poisoning). We derive provably robust sets by bounding the worst-case change in conformity scores. Our tighter bounds lead to more efficient sets. We cover both continuous and discrete (sparse) data and our guarantees work both for evasion and poisoning attacks (on both features and labels).",[],,"['Soroush H. Zargarbashi', 'Mohammad Sadegh Akhondzadeh', 'Aleksandar Bojchevski']","['CISPA, saarland university, saarland informatics campus', 'Universität Köln', 'University of Cologne']",
https://openreview.net/forum?id=xSkIxKdO08,Transparency & Explainability,CF-OPT: Counterfactual Explanations for Structured Prediction,"Optimization layers in deep neural networks have enjoyed a growing popularity in structured learning, improving the state of the art on a variety of applications. Yet, these pipelines lack interpretability since they are made of two opaque layers: a highly non-linear prediction model, such as a deep neural network, and an optimization layer, which is typically a complex black-box solver. Our goal is to improve the transparency of such methods by providing counterfactual explanations. We build upon variational autoencoders a principled way of obtaining counterfactuals: working in the latent space leads to a natural notion of plausibility of explanations. We finally introduce a variant of the classic loss for VAE training that improves their performance in our specific structured context. These provide the foundations of CF-OPT, a first-order optimization algorithm that can find counterfactual explanations for a broad class of structured learning architectures. Our numerical results show that both close and plausible explanations can be obtained for problems from the recent literature.",[],,"['Germain Vivier-Ardisson', 'Alexandre Forel', 'Axel Parmentier', 'Thibaut Vidal']","['École Polytechnique', 'MAGI, Polytechnique Montréal', 'Ecole Nationale des Ponts et Chausees', 'Polytechnique Montreal']",
