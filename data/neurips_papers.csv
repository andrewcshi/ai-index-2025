link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://openreview.net/forum?id=nkzSE5KkCA,Fairness & Bias,Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning,"Despite advancements in Text-to-Video (T2V) generation, producing videos with realistic motion remains challenging. Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text. This issue stems from the internal biases in text encoding which overlooks motions, and inadequate conditioning mechanisms in T2V generation models. To address this, we propose a novel framework called DEcomposed MOtion (DEMO), which enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, alongside separate content and motion conditioning mechanisms. Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Evaluations on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior ability to produce videos with enhanced motion dynamics while maintaining high visual quality. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions. Project page: https://PR-Ryan.github.io/DEMO-project/","['Text-to-Video Generation', 'Diffusion Models']",[],"['Penghui Ruan', 'Pichao WANG', 'Divya Saxena', 'Jiannong Cao', 'Yuhui Shi']","['Department of Computing, Hong Kong Polytechnic University', 'Amazon', 'The Hong Kong Polytechnic University, Hong Kong Polytechnic University', 'Department of Computing, Hong Kong Polytechnic University', 'Southern University of Science and Technology']",
https://openreview.net/forum?id=Oo7dlLgqQX,Fairness & Bias,Questioning the Survey Responses of Large Language Models,"Surveys have recently gained popularity as a tool to study large language models. By comparing models’ survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter “A”. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.","['large language models', 'surveys']",[],"['Ricardo Dominguez-Olmedo', 'Moritz Hardt', 'Celestine Mendler-Dünner']","['Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'ELLIS Institute Tübingen']",
https://openreview.net/forum?id=GQNvvQquO0,Privacy & Data Governance,Differentially Private Set Representations,"We study the problem of differentially private (DP) mechanisms for representing sets of size $k$ from a large universe. Our first construction creates $(\epsilon,\delta)$-DP representations with error probability of $1/(e^\epsilon + 1)$ using space at most $1.05 k \epsilon \cdot \log(e)$ bits where the time to construct a representation is $O(k \log(1/\delta))$ while decoding time is $O(\log(1/\delta))$. We also present a second algorithm for pure $\epsilon$-DP representations with the same error using space at most $k \epsilon \cdot \log(e)$ bits, but requiring large decoding times. Our algorithms match the lower bounds on privacy-utility trade-offs (including constants but ignoring $\delta$ factors) and we also present a new space lower bound matching our constructions up to small constant factors. To obtain our results, we design a new approach embedding sets into random linear systems deviating from most prior approaches that inject noise into non-private solutions.","['Differential Privacy', 'Data Structure']",[],"['Sarvar Patel', 'Giuseppe Persiano', 'Joon Young Seo', 'Kevin Yeo']","['Google', 'University of Salerno', 'Google', 'Google']",
https://openreview.net/forum?id=BSYn7ah4KX,Fairness & Bias,Bias Amplification in Language Model Evolution: An Iterated Learning Perspective,"With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to generate new examples for training subsequent models. At the same time, multi-agent LLM systems, involving automated interactions among agents, are also increasing in prominence. Thus, in both short and long terms, LLMs may actively engage in an evolutionary process. We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades. Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs. This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs. This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions.","['Large Language Model', 'LLM Agent', 'Self-improvement', 'Cognitive Science', 'Bayesian', 'Iterated Learning']",[],"['Yi Ren', 'Shangmin Guo', 'Linlu Qiu', 'Bailin Wang', 'Danica J. Sutherland']","['CS, University of British Columbia', 'University of Edinburgh', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of British Columbia']",
https://openreview.net/forum?id=h2e4G2YiwR,Fairness & Bias,Action Imitation in Common Action Space for Customized Action Image Synthesis,"We propose a novel method, \textbf{TwinAct}, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation. TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images. Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details. Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss. To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions while maintaining the identity consistency of different subjects, including animals, humans, and even customized actors.",['customized action; diffusion model; text to image generation'],[],"['Wang Lin', 'Jingyuan Chen', 'Jiaxin Shi', 'Zirun Guo', 'Yichen Zhu', 'Zehan Wang', 'Tao Jin', 'Zhou Zhao', 'Fei Wu', 'Shuicheng YAN', 'Hanwang Zhang']","['zhejiang university, Zhejiang University', '', 'Computer Science and Technology, Huawei Technologies Ltd.', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Software Engineering, Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Computer Science, National University of Singapore', 'computer science, Nanyang Technological University']",
https://openreview.net/forum?id=yySpldUsU2,Fairness & Bias,Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization,"Can we modify the training data distribution to encourage the underlying optimization method toward finding solutions with superior generalization performance on in-distribution data? In this work, we approach this question for the first time by comparing the inductive bias of gradient descent (GD) with that of sharpness-aware minimization (SAM). By studying a two-layer CNN, we rigorously prove that SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD. We also show that examples constraining features that are learned early are separable from the rest based on the model’s output. Based on this observation, we propose a method that (i) clusters examples based on the network output early in training, (ii) identifies a cluster of examples with similar network output, and (iii) upsamples the rest of examples only once to alleviate the simplicity bias. We show empirically that USEFUL effectively improves the generalization performance on the original data distribution when training with various gradient methods, including (S)GD and SAM. Notably, we demonstrate that our method can be combined with SAM variants and existing data augmentation strategies to achieve, to the best of our knowledge, state-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.","['In-distribution generalization', 'Simplicity bias', 'Data modification', 'Sharpness-aware minimization']",[],"['Dang Nguyen', 'Paymon Haddad', 'Eric Gan', 'Baharan Mirzasoleiman']","['Computer Science, University of California, Los Angeles', 'Samueli School of Engineering and Applied Sciences, University of California, Los Angeles', 'University of California, Los Angeles', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=CMgxAaRqZh,Security,Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling,"Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial attack methods, leading to acceleration of $1.8\times$ for AutoPrompt, $2.4\times$ for APE and $2.4\times$ for AutoDAN.","['Large Language Model', 'Prompt Optimization', 'Alignment', 'Jailbreak', 'Acceleration']",[],"['Yiran Zhao', 'Wenyue Zheng', 'Tianle Cai', 'Do Xuan Long', 'Kenji Kawaguchi', 'Anirudh Goyal', 'Michael Shieh']","['National University of Singapore', 'National University of Singapore', 'Princeton University', 'National University of Singapore', 'National University of Singapore', 'Google DeepMind', '']",
https://openreview.net/forum?id=V3QZCM1AQv,Security,REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR,"Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity. We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech. We comprehensively analyze why the boundaries learned by REBORN improve the unsupervised ASR performance.","['Speech processing', 'unsupervised learning', 'reinforcement learning', 'adversarial learning']",[],"['Liang-Hsuan Tseng', 'En-Pei Hu', 'Cheng-Han Chiang', 'Yuan Tseng', 'Hung-yi Lee', 'Lin-shan Lee', 'Shao-Hua Sun']","['National Taiwan University', 'National Taiwan University', 'National Taiwan University', 'National Taiwan University', 'Department of Electrical Engineering, National Taiwan University', 'National Taiwan University', 'National Taiwan University']",
https://openreview.net/forum?id=MI8Z9gutIn,Fairness & Bias,Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization,"Bi-level optimizaiton (BO) has become a fundamental mathematical framework for addressing hierarchical machine learning problems. As deep learning models continue to grow in size, the demand for scalable bi-level optimization has become increasingly critical. Traditional gradient-based bi-level optimizaiton algorithms, due to their inherent characteristics, are ill-suited to meet the demands of large-scale applications. In this paper, we introduce **F**orward **G**radient **U**nrolling with **F**orward **G**radient, abbreviated as **$($FG$)^2$U**, which achieves an unbiased stochastic approximation of the meta gradient for bi-level optimizaiton. $($FG$)^2$U circumvents the memory and approximation issues associated with classical bi-level optimizaiton approaches, and delivers significantly more accurate gradient estimates than existing large-scale bi-level optimizaiton approaches. Additionally, $($FG$)^2$U is inherently designed to support parallel computing, enabling it to effectively leverage large-scale distributed computing systems to achieve significant computational efficiency. In practice, $($FG$)^2$U and other methods can be strategically placed at different stages of the training process to achieve a more cost-effective two-phase paradigm. Further, $($FG$)^2$U is easy to implement within popular deep learning frameworks, and can be conveniently adapted to address more challenging zeroth-order bi-level optimizaiton scenarios. We provide a thorough convergence analysis and a comprehensive practical discussion for $($FG$)^2$U, complemented by extensive empirical evaluations, showcasing its superior performance in diverse large-scale bi-level optimizaiton tasks.","['bi-level optimization', 'large-scale optimization', 'meta learning']",[],"['Qianli Shen', 'Yezhen Wang', 'Zhouhao Yang', 'Xiang Li', 'Haonan Wang', 'Yang Zhang', 'Jonathan Scarlett', 'Zhanxing Zhu', 'Kenji Kawaguchi']","['School of Computing, national university of singaore, National University of Singapore', 'National University of Singapore', 'Applied Mathematics and Statistics, Johns Hopkins University', 'national university of singaore, National University of Singapore', 'School of Computing, national university of singaore, National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'University of Southampton', 'National University of Singapore']",
https://openreview.net/forum?id=8Fxqn1tZM1,Fairness & Bias,Scale Equivariant Graph Metanetworks,"This paper pertains to an emerging machine learning paradigm: learning higher- order functions, i.e. functions whose inputs are functions themselves, particularly when these inputs are Neural Networks (NNs). With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of NNs. However, are these the sole symmetries present in NN parameterizations? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as scaling symmetries, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose Scale Equivariant Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.","['graph neural networks', 'weight space networks', 'implicit neural representations', 'symmetries']",[],"['Ioannis Kalogeropoulos', 'Giorgos Bouritsas', 'Yannis Panagakis']","['Department of Informatics and Telecommunications', 'Archimedes unit, Athena Research Center', '']",
https://openreview.net/forum?id=MDsl1ifiNS,Fairness & Bias,Robust Offline Active Learning on Graphs,"We consider the problem of active learning on graphs for node-level tasks, which has crucial applications in many real-world networks where labeling node responses is expensive. In this paper, we propose an offline active learning method that selects nodes to query by explicitly incorporating information from both the network structure and node covariates. Building on graph signal recovery theories and the random spectral sparsification technique, the proposed method adopts a two-stage biased sampling strategy that takes both informativeness and representativeness into consideration for node querying. Informativeness refers to the complexity of graph signals that are learnable from the responses of queried nodes, while representativeness refers to the capacity of queried nodes to control generalization errors given noisy node-level information. We establish a theoretical relationship between generalization error and the number of nodes selected by the proposed method. Our theoretical results demonstrate the trade-off between Informativeness and representativeness in active learning. Extensive numerical experiments show that the proposed method is competitive with existing graph-based active learning methods, especially when node covariates and responses contain noises. Additionally, the proposed method is applicable to both regression and classification tasks on graphs.","['Offline active learning', 'graph semi-supervised learning', 'graph signal recovery', 'network sampling']",[],"['Yuanchen Wu', 'Yubai Yuan']","['Pennsylvania State University', 'Pennsylvania State University']",
https://openreview.net/forum?id=83vxe8alV4,Transparency & Explainability,FSP-Laplace: Function-Space Priors for the Laplace Approximation in Bayesian Deep Learning,"Laplace approximations are popular techniques for endowing deep networks with epistemic uncertainty estimates as they can be applied without altering the predictions of the trained network, and they scale to large models and datasets. While the choice of prior strongly affects the resulting posterior distribution, computational tractability and lack of interpretability of the weight space typically limit the Laplace approximation to isotropic Gaussian priors, which are known to cause pathological behavior as depth increases. As a remedy, we directly place a prior on function space. More precisely, since Lebesgue densities do not exist on infinite-dimensional function spaces, we recast training as finding the so-called weak mode of the posterior measure under a Gaussian process (GP) prior restricted to the space of functions representable by the neural network. Through the GP prior, one can express structured and interpretable inductive biases, such as regularity or periodicity, directly in function space, while still exploiting the implicit inductive biases that allow deep networks to generalize. After model linearization, the training objective induces a negative log-posterior density to which we apply a Laplace approximation, leveraging highly scalable methods from matrix-free linear algebra. Our method provides improved results where prior knowledge is abundant (as is the case in many scientific inference tasks). At the same time, it stays competitive for black-box supervised learning problems, where neural networks typically excel.","['Bayesian neural networks', 'Laplace approximation', 'uncertainty quantification', 'Gaussian processes']",[],"['Tristan Cinquin', 'Marvin Pförtner', 'Vincent Fortuin', 'Philipp Hennig', 'Robert Bamler']","['Computer science, Eberhard-Karls-Universität Tübingen', '', 'Technical University of Munich', 'University of Tübingen', 'University of Tuebingen']",
https://openreview.net/forum?id=SM9IWrHz4e,Fairness & Bias,Achieving Tractable Minimax Optimal Regret in Average Reward MDPs,"In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs). However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies. In this paper, we present the first *tractable* algorithm  with minimax optimal regret of $\mathrm{O}\left(\sqrt{\mathrm{sp}(h^*) S A T \log(SAT)}\right)$ where $\mathrm{sp}(h^*)$ is the span of the optimal bias function $h^*$, $S\times A$  is the size of the state-action space and $T$ the number of learning steps.  Remarkably, our algorithm does not require prior information on $\mathrm{sp}(h^*)$.   Our algorithm relies on a novel subroutine, **P**rojected **M**itigated **E**xtended **V**alue **I**teration (`PMEVI`), to compute bias-constrained optimal policies efficiently.  This subroutine can be applied to various previous algorithms to obtain improved regret bounds.","['Markov decision processes', 'Regret', 'Average reward', 'Minimax', 'Optimism', 'Model-based']",[],"['Victor Boone', 'Zihan Zhang']","['INRIA', 'University of Washington']",
https://openreview.net/forum?id=YEtirXhsh1,Fairness & Bias,Towards Learning Group-Equivariant Features for Domain Adaptive 3D Detection,"The performance of 3D object detection in large outdoor point clouds deteriorates significantly in an unseen environment due to the inter-domain gap. To address these challenges, most existing methods for domain adaptation harness self-training schemes and attempt to bridge the gap by focusing on a single factor that causes the inter-domain gap, such as objects' sizes, shapes, and foreground density variation. However, the resulting adaptations suggest that there is still a substantial inter-domain gap left to be minimized. We argue that this is due to two limitations: 1) Biased pseudo-label collection from self-training. 2) Multiple factors jointly contributing to how the object is perceived in the unseen target domain. In this work, we propose a grouping-exploration strategy framework,  Group Explorer Domain Adaptation ($\textbf{GroupEXP-DA}$), to addresses those two issues. Specifically, our grouping divides the available label sets into multiple clusters and ensures all of them have equal learning attention with the group-equivariant spatial feature, avoiding dominant types of objects causing imbalance problems. Moreover, grouping learns to divide objects by considering inherent factors in a data-driven manner, without considering each factor separately as existing works. On top of the group-equivariant spatial feature that selectively detects objects similar to the input group, we additionally introduce an explorative group update strategy that reduces the false negative detection in the target domain, further reducing the inter-domain gap. During inference, only the learned group features are necessary for making the group-equivariant spatial feature, placing our method as a simple add-on that can be applicable to most existing detectors. We show how each module contributes to substantially bridging the inter-domain gaps compared to existing works across large urban outdoor datasets such as NuScenes, Waymo, and KITTI.","['Domain Adaptive 3D Detection', 'Domain Adaptation', '3D Detection', 'Self-Training']",[],"['Sangyun Shin', 'Yuhang He', 'Madhu Vankadari', 'Ta-Ying Cheng', 'Qian Xie', 'Andrew Markham', 'Niki Trigoni']","['', 'Department of Computer Science, University of Oxford', 'Department of Computer Science, University of Oxford', 'Department of Computer Science, University of Oxford', 'Department of Computer Science, Department of Computer Science, University of Oxford', 'Computer Science, University of Oxford', 'Department of Computer Science, University of Oxford']",
https://openreview.net/forum?id=cU8d7LeOyx,Fairness & Bias,Causal discovery with endogenous context variables,"Systems with variations of the underlying generating mechanism between different contexts, i.e., different environments or internal states  in which the system operates, are common in the real world, such as soil moisture regimes in Earth science. Besides understanding the shared properties of the system, in practice, the question of context-specific properties, i.e., the change in causal relationships between contexts, arises. For real-world data, contexts are often driven by system variables, e.g., precipitation highly influences soil moisture. Nevertheless, this setup needs to be studied more. To account for such endogenous contexts in causal discovery, our work proposes a constraint-based method that can efficiently discover context-specific causal graphs using an adaptive testing approach. Our approach tests conditional independence on the pooled datasets to infer the dependence between system variables, including the context, to avoid introducing selection bias. To yield context-specific insights, conditional independence is tested on context-specific data. We work out the theoretical framework for this adaptive testing approach and give a detailed discussion of the connection to structural causal models, including sufficiency assumptions, which allow to prove the soundness of our algorithm and to interpret the results causally. A simulation study to evaluate numerical properties shows that our approach  behaves as expected, but also leads to a further understanding of current limitations and viable extensions.","['causal discovery', 'context-specific causal discovery', 'selection bias', 'SCM', 'causal models', 'endogeneous context variables', 'regime-specific causal discovery', 'endogeneous regime variables', 'context-specific independence']",[],"['Wiebke Günther', 'Oana-Iuliana Popescu', 'Martin Rabel', 'Urmi Ninad', 'Andreas Gerhardus', 'Jakob Runge']","['German Aerospace Center, Institute of Data Science', 'German Aerospace Center, Institute of Data Science', 'Technische Universität Dresden', 'Technische Universität Berlin', 'German Aerospace Center, Institute of Data Science', 'Informatics and computational science, Universität Potsdam']",
https://openreview.net/forum?id=faBXeVBNqz,Fairness & Bias,Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing,"The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.","['equivariance', 'graph neural networks', 'interatomic potentials', 'irreducible Cartesian tensors', 'many-body interactions', 'molecules', 'materials']",[],"['Viktor Zaverkin', 'Francesco Alesiani', 'Takashi Maruyama', 'Federico Errica', 'Henrik Christiansen', 'Makoto Takamoto', 'Nicolas Weber', 'Mathias Niepert']","['NEC Laboratories Europe', 'NEC', 'NEC Laboratories Europe', 'NEC', 'ISS, NEC', 'NEC', 'NEC Laboratories Europe', 'Universität Stuttgart']",
https://openreview.net/forum?id=Woiqqi5bYV,Transparency & Explainability,Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification,"Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy.","['Domain generalization', 'generalized category discovery', 'image classification', 'logical reasoning']",[],"['Zhaorui Tan', 'Xi Yang', 'Qiufeng Wang', 'Anh Nguyen', 'Kaizhu Huang']","[""Xi'an Jiaotong-Liverpool University"", ""Xi'an Jiaotong-Liverpool University"", ""Department of Intelligent Science, Xi'an Jiaotong-Liverpool University"", 'University of Liverpool', 'Data Science Research Center, Duke Kunshan University']",
https://openreview.net/forum?id=BmwcbNYkuH,Security,Are nuclear masks all you need for improved out-of-domain generalisation? A closer look at cancer classification in histopathology,"Domain generalisation in computational histopathology is challenging because the images are substantially affected by differences among hospitals due to factors like fixation and staining of tissue and imaging equipment. We hypothesise that focusing on nuclei can improve the out-of-domain (OOD) generalisation in cancer detection. We propose a simple approach to improve OOD generalisation for cancer detection by focusing on nuclear morphology and organisation, as these are domain-invariant features critical in cancer detection. Our approach integrates original images with nuclear segmentation masks during training, encouraging the model to prioritise nuclei and their spatial arrangement. Going beyond mere data augmentation, we introduce a regularisation technique that aligns the representations of masks and original images. We show, using multiple datasets, that our method improves OOD generalisation and also leads to increased robustness to image corruptions and adversarial attacks. The source code is available at https://github.com/undercutspiky/SFL/","['Deep learning', 'domain generalization', 'histopathology', 'computational pathology', 'digital pathology', 'computer vision', 'single domain generalization']",[],"['Dhananjay Tomar', 'Alexander Binder', 'Andreas Kleppe']","['Informatics, University of Oslo', 'Otto von Guericke University Magdeburg', 'Institute for Cancer Genetics and Informatics, Oslo University Hospital']",
https://openreview.net/forum?id=8oSY3rA9jY,Transparency & Explainability,Finding Transformer Circuits With Edge Pruning,"The path to interpreting a language model often proceeds via analysis of circuits---sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they either rely on inefficient search algorithms or inaccurate approximations. In this paper, we frame circuit discovery as an optimization problem and propose _Edge Pruning_ as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, prunes the _edges_ between components. Our method finds circuits in GPT-2 that use less than half the number of edges than circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient on tasks involving up to 100,000 examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the size of GPT-2. We use this setting for a case study, where we compare the mechanisms behind instruction prompting and in-context learning. We find two circuits with more than 99.96% sparsity that match the performance of the full model. Further analysis reveals that the mechanisms in the two settings overlap substantially. This shows that Edge Pruning is a practical and scalable tool for interpretability,  which can shed light on behaviors that only emerge in large models.","['interpretability', 'circuits', 'pruning', 'optimization']",[],"['Adithya Bhaskar', 'Alexander Wettig', 'Dan Friedman', 'Danqi Chen']","['Computer Science, Princeton University', 'Department of Computer Science, Princeton University', 'Princeton University', 'Department of Computer Science, Princeton University']",
https://openreview.net/forum?id=HfQF8LoLhs,Fairness & Bias,Asymptotics of Alpha-Divergence Variational Inference Algorithms with Exponential Families,"Recent works in Variational Inference have examined alternative criteria to the commonly used exclusive Kullback-Leibler divergence. Encouraging empirical results have been obtained with the family of alpha-divergences, but few works have focused on the asymptotic properties of the proposed algorithms, especially as the number of iterations goes to infinity. In this paper, we study a procedure that ensures a monotonic decrease in the alpha-divergence. We provide sufficient conditions to guarantee its convergence to a local minimizer of the alpha-divergence at a geometric rate when the variational family belongs to the class of exponential models. The sample-based version of this ideal procedure involves biased gradient estimators, thus hindering any theoretical study. We propose an alternative unbiased algorithm, we prove its almost sure convergence to a local minimizer of the alpha-divergence, and a law of the iterated logarithm. Our results are exemplified with toy and real-data experiments.","['Variational inference', 'stochastic algorithms', 'asymptotic analysis', 'alpha divergence', 'exponential models']",[],"['François Bertholom', 'randal douc', 'François Roueff']","['CITI, Telecom SudParis', 'Telecom Sudparis', 'Télécom Paris']",
https://openreview.net/forum?id=rI80PHlnFm,Transparency & Explainability,Model Based Inference of Synaptic Plasticity Rules,"Inferring the synaptic plasticity rules that govern learning in the brain is a key challenge in neuroscience. We present a novel computational method to infer these rules from experimental data, applicable to both neural and behavioral data. Our approach approximates plasticity rules using a parameterized function, employing either truncated Taylor series for theoretical interpretability or multilayer perceptrons. These plasticity parameters are optimized via gradient descent over entire trajectories to align closely with observed neural activity or behavioral learning dynamics. This method can uncover complex rules that induce long nonlinear time dependencies, particularly involving factors like postsynaptic activity and current synaptic weights. We validate our approach through simulations, successfully recovering established rules such as Oja's, as well as more intricate plasticity rules with reward-modulated terms. We assess the robustness of our technique to noise and apply it to behavioral data from \textit{Drosophila} in a probabilistic reward-learning experiment. Notably, our findings reveal an active forgetting component in reward learning in flies, improving predictive accuracy over previous models. This modeling framework offers a promising new avenue for elucidating the computational principles of synaptic plasticity and learning in the brain.","['computational neuroscience', 'plasticity rules', 'synaptic plasticity', 'biologically plausible learning']",[],"['Yash Mehta', 'Danil Tyulmankov', 'Adithya E. Rajagopalan', 'Glenn C Turner', 'James E Fitzgerald', 'Jan Funke']","['Computational Cognitive Science, Johns Hopkins University', '', 'New York University', 'HHMI Janelia Research Campus', 'Northwestern University', 'HHMI Janelia Research Campus']",
https://openreview.net/forum?id=qWi33pPecC,Transparency & Explainability,"Most Influential Subset Selection: Challenges, Promises, and Beyond","How can we attribute the behaviors of machine learning models to their training data? While the classic influence function sheds light on the impact of individual samples, it often fails to capture the more complex and pronounced collective influence of a set of samples. To tackle this challenge, we study the Most Influential Subset Selection (MISS) problem, which aims to identify a subset of training samples with the greatest collective influence. We conduct a comprehensive analysis of the prevailing approaches in MISS, elucidating their strengths and weaknesses. Our findings reveal that influence-based greedy heuristics, a dominant class of algorithms in MISS, can provably fail even in linear regression. We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence. Conversely, we demonstrate that an adaptive version of these heuristics which applies them iteratively, can effectively capture the interactions among samples and thus partially address the issues. Experiments on real-world datasets corroborate these theoretical findings, and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks. We conclude our analysis by emphasizing the inherent trade-off between performance and computational efficiency, questioning the use of additive metrics such as the linear datamodeling score, and offering a range of discussions.","['influential subset', 'influence function', 'data attribution', 'interpretability']",[],"['Yuzheng Hu', 'Pingbang Hu', 'Han Zhao', 'Jiaqi Ma']","['Peking University', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'University of Illinois Urbana-Champaign']",
https://openreview.net/forum?id=LX1lwP90kt,Transparency & Explainability,Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems,"Understanding how the collective activity of neural populations relates to computation and ultimately behavior is a key goal in neuroscience. To this end, statistical methods which describe high-dimensional neural time series in terms of low-dimensional latent dynamics have played a fundamental role in characterizing neural systems. Yet, what constitutes a successful method involves two opposing criteria: (1) methods should be expressive enough to capture complex nonlinear dynamics, and (2) they should maintain a notion of interpretability often only warranted by simpler linear models. In this paper, we develop an approach that balances these two objectives: the Gaussian Process Switching Linear Dynamical System (gpSLDS). Our method builds on previous work modeling the latent state evolution via a stochastic differential equation whose nonlinear dynamics are described by a Gaussian process (GP-SDEs). We propose a novel kernel function which enforces smoothly interpolated locally linear dynamics, and therefore expresses flexible -- yet interpretable -- dynamics akin to those of recurrent switching linear dynamical systems (rSLDS). Our approach resolves key limitations of the rSLDS such as artifactual oscillations in dynamics near discrete state boundaries, while also providing posterior uncertainty estimates of the dynamics. To fit our models, we leverage a modified learning objective which improves the estimation accuracy of kernel hyperparameters compared to previous GP-SDE fitting approaches. We apply our method to synthetic data and data recorded in two neuroscience experiments and demonstrate favorable performance in comparison to the rSLDS.","['gaussian process', 'switching', 'slds', 'neural', 'neuroscience', 'dynamics', 'probabilistic', 'time series']",[],"['Amber Hu', 'David M. Zoltowski', 'Aditya Nair', 'David Anderson', 'Lea Duncker', 'Scott Linderman']","['Stanford University', 'Stanford University', 'California Institute of Technology', 'California Institute of Technology', 'Stanford University', 'Stanford University']",
https://openreview.net/forum?id=ZRz7XlxBzQ,Fairness & Bias,Learning to compute Gröbner bases,"Solving a polynomial system, or computing an associated Gröbner basis, has been a fundamental task in computational algebra. However, it is also known for its notorious doubly exponential time complexity in the number of variables in the worst case. This paper is the first to address the learning of Gröbner basis computation with Transformers. The training requires many pairs of a polynomial system and the associated Gröbner basis, raising two novel algebraic problems: random generation of Gröbner bases and transforming them into non-Gröbner ones, termed as backward Gröbner problem. We resolve these problems with 0-dimensional radical ideals, the ideals appearing in various applications. Further, we propose a hybrid input embedding to handle coefficient tokens with continuity bias and avoid the growth of the vocabulary set. The experiments show that our dataset generation method is a few orders of magnitude faster than a naive approach, overcoming a crucial challenge in learning to compute Gröbner bases, and Gröbner computation is learnable in a particular class.",['Transformer; Gröbner bases; Computational algebra'],[],"['Hiroshi Kera', 'Yuki Ishihara', 'Yuta Kambe', 'Tristan Vaccon', 'Kazuhiro Yokoyama']","['Graduate School of Informatics, Chiba University', 'Nihon University', 'Mitsubishi Electric Information Technology R&D Center', 'Université de Limoges', ""Rikkyo University (St. Paul's University)""]",
https://openreview.net/forum?id=CehOqpvOxG,Fairness & Bias,Fair Kernel K-Means: from Single Kernel to Multiple Kernel,"Kernel k-means has been widely studied in machine learning. However, existing kernel k-means methods often ignore the \textit{fairness} issue, which may cause discrimination. To address this issue, in this paper, we propose a novel Fair Kernel K-Means (FKKM) framework. In this framework, we first propose a new fairness regularization term that can lead to a fair partition of data. The carefully designed fairness regularization term has a similar form to the kernel k-means which can be seamlessly integrated into the kernel k-means framework. Then, we extend this method to the multiple kernel setting, leading to a Fair Multiple Kernel K-Means (FMKKM) method. We also provide some theoretical analysis of the generalization error bound, and based on this bound we give a strategy to set the hyper-parameter, which makes the proposed methods easy to use. At last, we conduct extensive experiments on both the single kernel and multiple kernel settings to compare the proposed methods with state-of-the-art methods to demonstrate their effectiveness.","['kernel k-means', 'multiple kernel k-means', 'fair clustering']",[],"['Peng Zhou', 'Rongwen Li', 'Liang Du']","['Anhui University', 'Anhui University', 'Computer Science, Shanxi University']",
https://openreview.net/forum?id=merJ77Jipt,Fairness & Bias,DiffPO: A causal diffusion model for learning distributions of potential outcomes,"Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference. Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored. In this paper, we propose a novel causal diffusion model called DiffPO, which is carefully designed for reliable inferences in medicine by learning the distribution of potential outcomes. In our DiffPO, we leverage a tailored conditional denoising diffusion model to learn complex distributions, where we address the selection bias through a novel orthogonal diffusion loss. Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE). Across a wide range of experiments, we show that our method achieves state-of-the-art performance.","['Causal inference', 'Treatment effect estimation', 'Diffusion models', 'Machine Learning for healthcare', 'CATE']",[],"['Yuchen Ma', 'Valentyn Melnychuk', 'Jonas Schweisthal', 'Stefan Feuerriegel']","['Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', 'LMU Munich']",
https://openreview.net/forum?id=DlYNGpCuwa,Transparency & Explainability,Aligning LLM Agents by Learning Latent Preference from User Edits,"We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER outperforms baselines by achieving the lowest edit distance cost. Meanwhile, CIPHER has a lower computational expense, as using learned preference results in a shorter prompt than directly using user edits. Our further analysis reports that the user preference learned by CIPHER shows significant similarity to the ground truth latent preference.","['NLP', 'LLM', 'preference learning', 'user feedback', 'user edits']",[],"['Ge Gao', 'Alexey Taymanov', 'Eduardo Salinas', 'Paul Mineiro', 'Dipendra Misra']","['Cornell University', 'Research, Microsoft', 'Research, Microsoft', 'University of California, San Diego', 'Mosaic Research, Databricks']",
https://openreview.net/forum?id=fzlMza6dRZ,Transparency & Explainability,GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules,"Instance-level explanation of graph neural networks (GNNs) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a GNN from the training data towards making its predictions. In this work, we introduce GraphTrail, the first end-to-end, global, post-hoc GNN explainer that translates the functioning of a black-box GNN model to a boolean formula over the (sub)graph level concepts without relying on local explainers. GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using Shapley values. Subsequently, the GNN predictions are mapped to a human-interpretable boolean formula over these concepts through symbolic regression. Extensive experiments across diverse datasets and GNN architectures demonstrate significant improvement over existing global explainers in mapping GNN predictions to faithful logical formulae. The robust and accurate performance of GraphTrail makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements.","['Graph Neural Network', 'Explainability', 'Global Factual Explanation', 'Symbolic Regression', 'Computation Trees']",[],"['Burouj Armgaan', 'Manthan Dalmia', 'Sourav Medya', 'Sayan Ranu']","['Indian Institute of Technology, Delhi', 'Indian Institute of Technology, Delhi', 'University of Illinois at Chicago', 'Computer Science & Engineering, Indian Institute of Technology Delhi']",
https://openreview.net/forum?id=8abNCVJs2j,Fairness & Bias,S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training,"Training deep neural networks (DNNs) is costly. Fortunately, Nvidia Ampere and Hopper GPUs can accelerate matrix multiplications twice as fast as a dense equivalent by implementing 2:4 sparsity. However, previous STE-based 2:4 pre-training methods (\eg~STE with hard-thresholding, SR-STE) suffer from optimization difficulties because of discontinuous pruning function. In this study, we comprehensively analyse the bottleneck of traditional N:M sparse training and recognize three drawbacks with discontinuity: incorrect descending direction, inability to predict the amount of descent and sparse mask oscillation. In the light of this statement, we propose S-STE, a simple yet powerful 2:4 training method that contains two parts: to continuously project weights to be 2:4 sparse, and to rescale sparse weights with a per-tensor fixed scaling factor. Besides, we adopt minimum-variance unbiased estimation for activation gradient and FP8 quantization for whole process. Results show that our method surpass previous 2:4 pre-training recipes and is comparable even with full parameter models.","['efficient machine learning', 'transformers', '2:4 sparsity', 'dynamic sparse training']",[],"['Yuezhou Hu', 'Jun Zhu', 'Jianfei Chen']","['Department of Computer Science and Technology, Tsinghua University, Tsinghua University', 'Computer Science, Tsinghua University', 'Tsinghua University']",
https://openreview.net/forum?id=soUXmwL5aK,Transparency & Explainability,Interpretable Generalized Additive Models for Datasets with Missing Values,"Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the model’s mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through $\ell_0$ regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naïve inclusion of indicator variables.","['Interpretability', 'Missing Data', 'Generalized Additive Models', 'Sparsity']",[],"['Hayden McTavish', 'Jon Donnelly', 'Margo Seltzer', 'Cynthia Rudin']","['', 'Computer Science, Duke University', 'Computer Science, University of British Columbia', '']",
https://openreview.net/forum?id=BJv1t4XNJW,Fairness & Bias,Slot State Space Models,"Recent State Space Models (SSMs) such as S4, S5, and Mamba have shown remarkable computational benefits in long-range temporal dependency modeling. However, in many sequence modeling problems, the underlying process is inherently modular and it is of interest to have inductive biases that mimic this modular structure. In this paper, we introduce SlotSSMs, a novel framework for incorporating independent mechanisms into SSMs to preserve or encourage separation of information. Unlike conventional SSMs that maintain a monolithic state vector, SlotSSMs maintains the state as a collection of multiple vectors called slots. Crucially, the state transitions are performed independently per slot with sparse interactions across slots implemented via the bottleneck of self-attention. In experiments, we evaluate our model in object-centric learning, 3D visual reasoning, and long-context video understanding tasks, which involve modeling multiple objects and their long-range temporal dependencies. We find that our proposed design offers substantial performance gains over existing sequence modeling methods. Project page is available at \url{https://slotssms.github.io/}","['State-Space Models', 'Object-Centric Learning', 'Video Understanding Models', 'Spatial-Temporal Reasoning']",[],"['Jindong Jiang', 'Fei Deng', 'Gautam Singh', 'Minseung Lee', 'Sungjin Ahn']","['Computer Science, Rutgers University', 'Google', 'NVIDIA', 'KAIST', '']",
https://openreview.net/forum?id=MN4nt01TeO,Privacy & Data Governance,Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences,"We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of our test-time adaptive models against adversarial examples. ARS extends the analysis of randomized smoothing using $f$-Differential Privacy to certify the adaptive composition of multiple steps. For the first time, our theory covers the sound adaptive composition of general and high-dimensional functions of noisy inputs. We instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded $L_{\infty}$ norm. In the $L_{\infty}$ threat model, ARS enables flexible adaptation through high-dimensional input-dependent masking. We design adaptivity benchmarks, based on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy by  1 to 15\% points. On ImageNet, ARS improves certified test accuracy by up to 1.6% points over standard RS without adaptivity. Our code is available at [https://github.com/ubc-systopia/adaptive-randomized-smoothing](https://github.com/ubc-systopia/adaptive-randomized-smoothing).","['Robustness', 'Adversarial examples', 'Adaptive defenses', 'Certified test-time defenses', 'Randomized Smoothing']",[],"['Saiyue Lyu', 'Shadab Shaikh', 'Frederick Shpilevskiy', 'Evan Shelhamer', 'Mathias Lécuyer']","['University of British Columbia', 'Computer Science, University of British Columbia', 'Computer Science, University of British Columbia', 'Computer Science, University of British Columbia', 'Computer Science, University of British Columbia']",
https://openreview.net/forum?id=3ZAfFoAcUI,Fairness & Bias,On the Inductive Bias of Stacking Towards Improving Reasoning,"Given the increasing scale of model sizes, efficient training strategies like gradual stacking have garnered interest. Stacking enables efficient training by gradually growing the depth of a model in stages and using layers from a smaller model in an earlier stage to initialize the next stage. Although efficient for training, the model biases induced by such growing approaches are largely unexplored. In this work, we examine this fundamental aspect of gradual stacking, going beyond its efficiency benefits. We propose a variant of gradual stacking called MIDAS that can speed up language model training by up to 40\%. Furthermore we discover an intriguing phenomenon: MIDAS is not only training-efficient but surprisingly also has an inductive bias towards improving downstream tasks, especially tasks that require reasoning abilities like reading comprehension and math problems, despite having similar or slightly worse perplexity compared to baseline training. To further analyze this inductive bias, we construct {\em reasoning primitives} – simple synthetic tasks that are building blocks for reasoning – and find that a model pretrained with stacking is significantly better than standard pretraining on these primitives, with and without fine-tuning. This provides stronger and more robust evidence for this inductive bias towards reasoning. These findings of training efficiency and inductive bias towards reasoning are verified at 1B, 2B and 8B parameter language models. Finally, we conjecture the underlying reason for this inductive bias by exploring the connection of stacking to looped models and provide strong supporting empirical analysis.","['stacking', 'language model', 'reasoning', 'inductive bias', 'efficient training']",[],"['Nikunj Saunshi', 'Stefani Karp', 'Shankar Krishnan', 'Sobhan Miryoosefi', 'Sashank J. Reddi', 'Sanjiv Kumar']","['Google', 'Google', 'Google', 'Princeton University', 'Google', 'Google']",
https://openreview.net/forum?id=wGP1tBCP1E,Security,Diffusion Models are Certifiably Robust Classifiers,"Generative learning, recognized for its effective modeling of data distributions, offers inherent advantages in handling out-of-distribution instances, especially for enhancing robustness to adversarial attacks. Among these, diffusion classifiers, utilizing powerful diffusion models, have demonstrated superior empirical robustness. However, a comprehensive theoretical understanding of their robustness is still lacking, raising concerns about their vulnerability to stronger future attacks. In this study, we prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish their certified robustness, demonstrating their inherent resilience. To achieve non-constant Lipschitzness, thereby obtaining much tighter certified robustness, we generalize diffusion classifiers to classify Gaussian-corrupted data. This involves deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. Experimental results show the superior certified robustness of these Noised Diffusion Classifiers (NDCs). Notably, we achieve over 80\% and 70\% certified robustness on CIFAR-10 under adversarial perturbations with \(\ell_2\) norms less than 0.25 and 0.5, respectively, using a single off-the-shelf diffusion model without any additional data.","['certified robustness', 'diffusion classifier', 'adversarial robustness']",[],"['Huanran Chen', 'Yinpeng Dong', 'Shitong Shao', 'Zhongkai Hao', 'Xiao Yang', 'Hang Su', 'Jun Zhu']","['AI, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Data Science and Analysis, The Hong Kong University of Science and Technology', 'Computer Science, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Computer Science, Tsinghua University', 'Computer Science, Tsinghua University']",
https://openreview.net/forum?id=KHX0dKXdqH,Fairness & Bias,Causal Imitation for Markov Decision Processes: a Partial Identification Approach,"Imitation learning enables an agent to learn from expert demonstrations when the performance measure is unknown and the reward signal is not specified. Standard imitation methods do not generally apply when the learner and the expert's sensory capabilities mismatch and demonstrations are contaminated with unobserved confounding bias. To address these challenges, recent advancements in causal imitation learning have been pursued. However, these methods often require access to underlying causal structures that might not always be available, posing practical challenges. In this paper, we investigate robust imitation learning within the framework of canonical Markov Decision Processes (MDPs) using partial identification, allowing the agent to achieve expert performance even when the system dynamics are not uniquely determined from the confounded expert demonstrations. Specifically, first, we theoretically demonstrate that when unobserved confounders (UCs) exist in an MDP, the learner is generally unable to imitate expert performance. We then explore imitation learning in partially identifiable settings --- either transition distribution or reward function is non-identifiable from the available data and knowledge. Augmenting the celebrated GAIL method (Ho \& Ermon, 2016), our analysis leads to two novel causal imitation algorithms that can obtain effective policies guaranteed to achieve expert performance.","['Causal Inference', 'Imitation Learning']",[],"['Kangrui Ruan', 'Junzhe Zhang', 'Xuan Di', 'Elias Bareinboim']","['Columbia University', '', 'Civil Engineering, Columbia University', 'Columbia University']",
https://openreview.net/forum?id=DztaBt4wP5,Transparency & Explainability,Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy,"Text-to-image diffusion models have achieved tremendous success in the field of controllable image generation, while also coming along with issues of privacy leakage and data copyrights. Membership inference arises in these contexts as a potential auditing method for detecting unauthorized data usage. While some efforts have been made on diffusion models, they are not applicable to text-to-image diffusion models due to the high computation overhead and enhanced generalization capabilities. In this paper, we first identify a conditional overfitting phenomenon in text-to-image diffusion models, indicating that these models tend to overfit the conditional distribution of images given the corresponding text rather than the marginal distribution of images only. Based on this observation, we derive an analytical indicator, namely Conditional Likelihood Discrepancy (CLiD), to perform membership inference, which reduces the stochasticity in estimating memorization of individual samples. Experimental results demonstrate that our method significantly outperforms previous methods across various data distributions and dataset scales. Additionally, our method shows superior resistance to overfitting mitigation strategies, such as early stopping and data augmentation.","['Diffusion models', 'Membership inference', 'Conditional Likelihood', 'Text-to-Image Synthesis']",[],"['Shengfang Zhai', 'Huanran Chen', 'Yinpeng Dong', 'Jiajun Li', 'Qingni Shen', 'Yansong Gao', 'Hang Su', 'Yang Liu']","['School of Software, Peking University', 'AI, Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Peking University', 'School of Software and Microelectronics, Peking University', '', 'Computer Science, Tsinghua University', 'CCDS, Nanyang Technological University']",
https://openreview.net/forum?id=tGDUDKirAy,Security,Verified Safe Reinforcement Learning  for Neural Network Dynamic Models,"Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes. Our code is available at https://github.com/jlwu002/VSRL.","['neural network', 'formal verification', 'safe reinforcement learning']",[],"['Junlin Wu', 'Huan Zhang', 'Yevgeniy Vorobeychik']","['Washington University, St. Louis', 'Electrical and Computer Engineering, University of Illinois at Urbana-Champaign', 'Washington University, St. Louis']",
https://openreview.net/forum?id=sZ7jj9kqAy,Transparency & Explainability,SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model,"Consumer electronics used to follow the miniaturization trend described by Moore’s Law. Despite increased processing power in Microcontroller Units (MCUs), MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs) especially in time-sensitive scenarios. In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs. SOI leverages the continuity and seasonality of time-series data and model predictions, enabling extrapolation for processing speed improvements, particularly in deeper layers. By applying compression, SOI generates more general inner partial states of ANN, allowing skipping full model recalculation at each inference.","['Time series data', 'Computational complexity reduction', 'Latency reduction', 'Real-Time results', 'Inference at the edge', 'Causality']",[],"['Grzegorz Stefański', 'Paweł Daniluk', 'Artur Szumaczuk', 'Jakub Tkaczuk']","['Samsung', 'Samsung', 'Samsung', 'Samsung Electronics']",
https://openreview.net/forum?id=s8Pxz7cvHT,Security,AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks,"Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-parametric diffusion process by theoretically exploring basic modeling approach rather than using the denoising or generation abilities of regular diffusion models requiring neural networks. At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example. Grounded in a solid theoretical foundation of the proposed non-parametric diffusion process, AdvAD achieves high attack efficacy and imperceptibility with intrinsically lower overall perturbation strength. Additionally, an enhanced version AdvAD-X is proposed to evaluate the extreme of our novel framework under an ideal scenario. Extensive experiments demonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with state-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9% (+17.3%) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971 (+0.0043) SSIM against four prevalent DNNs with three different architectures on the ImageNet-compatible dataset. Code is available at https://github.com/XianguiKang/AdvAD.","['Adversarial Attacks', 'Imperceptibility', 'Diffusion Models', 'Deep Neural Networks']",[],"['Jin Li', 'Ziqiang He', 'Anwei Luo', 'Jian-Fang Hu', 'Z. Jane Wang', 'Xiangui Kang']","['School of Computer Science and Engineering, SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY, SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY, SUN YAT-SEN UNIVERSITY', 'ECE, University of British Columbia', 'School of Computer Science and Engineering, SUN YAT-SEN UNIVERSITY']",
https://openreview.net/forum?id=dfiXFbECSZ,Transparency & Explainability,LoFiT: Localized Fine-tuning on LLM Representations,"Recent work in interpretability shows that large language models (LLMs) can be adapted for new tasks in a learning-free way: it is possible to intervene on LLM representations to elicit desired behaviors for alignment. For instance, adding certain bias vectors to the outputs of certain attention heads is reported to boost the truthfulness of models. In this work, we show that localized fine-tuning serves as an effective alternative to such representation intervention methods. We introduce a framework called Localized Fine-Tuning on LLM Representations (LoFiT), which identifies a subset of attention heads that are most important for learning a specific task, then trains offset vectors to add to the model's hidden representations at those selected heads. LoFiT localizes to a sparse set of heads (3%-10%) and learns the offset vectors from limited training data, comparable to the settings used for representation intervention. For truthfulness and reasoning tasks, we find that LoFiT's intervention vectors are more effective for LLM adaptation than vectors from representation intervention methods such as Inference-time Intervention. We also find that the localization step is important: selecting a task-specific set of attention heads can lead to higher performance than intervening on heads selected for a different task. Finally, across 7 tasks we study, LoFiT achieves comparable performance to other parameter-efficient fine-tuning methods such as LoRA, despite modifying 20x-200x fewer parameters than these methods.","['Interpretability', 'Large language models']",[],"['Fangcong Yin', 'Xi Ye', 'Greg Durrett']","['University of Texas at Austin', 'Princeton University', 'University of Texas at Austin']",
https://openreview.net/forum?id=StapcUWm9q,Fairness & Bias,Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement,"Disentangled representation learning strives to extract the intrinsic factors within the observed data. Factoring these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention itself can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image into a set of concept tokens and treat them as the condition of the latent diffusion model for image reconstruction, where cross attention over the concept tokens is used to bridge the encoder and the U-Net of the diffusion model. We analyze that the diffusion process inherently possesses the time-varying information bottlenecks. Such information bottlenecks and cross attention act as strong inductive biases for promoting disentanglement. Without any regularization term in the loss function, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analyses, shedding a light on the functioning of this model. We anticipate that our findings will inspire more investigation on exploring diffusion model for disentangled representation learning towards more sophisticated data analysis and understanding.","['diffusion models', 'disentangled representation']",[],"['Tao Yang', 'Cuiling Lan', 'Yan Lu', 'Nanning Zheng']","[""Xi'an Jiaotong University"", 'Microsoft', 'Microsoft Research Asia', ""Xi'an Jiaotong University""]",
https://openreview.net/forum?id=zzOOqD6R1b,Security,Stress-Testing Capability Elicitation With Password-Locked Models,"To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models but may be unreliable when high-quality demonstrations are not available, e.g., as may be the case when models’ (hidden) capabilities exceed those of human demonstrators.","['LLMs', 'Elicitation', 'Fine-tuning', 'Sandbagging', 'Red-teaming', 'Safety']",[],"['Ryan Greenblatt', 'Fabien Roger', 'Dmitrii Krasheninnikov', 'David Krueger']","['Brown University', 'Anthropic', 'University of Cambridge', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal']",
https://openreview.net/forum?id=JlWn80mTJi,Fairness & Bias,The Implicit Bias of Gradient Descent on Separable Multiclass Data,"Implicit bias describes the phenomenon where optimization-based training algorithms, without explicit regularization, show a preference for simple estimators even when more complex estimators have equal objective values. Multiple works have developed the theory of implicit bias for binary classification under the assumption that the loss satisfies an *exponential tail property*. However, there is a noticeable gap in analysis for multiclass classification, with only a handful of results which themselves are restricted to the cross-entropy loss. In this work, we employ the framework of Permutation Equivariant and Relative Margin-based (PERM) losses [Wang and Scott, 2024] to introduce a multiclass extension of the exponential tail property. This class of losses includes not only cross-entropy but also other losses. Using this framework, we extend the implicit bias result of Soudry et al. [2018] to multiclass classification. Furthermore, our proof techniques closely mirror those of the binary case, thus illustrating the power of the PERM framework for bridging the binary-multiclass gap.","['gradient descent', 'multiclass classification', 'hard-margin SVM', 'implicit bias']",[],"['Hrithik Ravi', 'Clayton Scott', 'Daniel Soudry', 'Yutong Wang']","['Electrical Engineering and Computer Science, University of Michigan - Ann Arbor', 'Electrical Engineering and Computer Science, University of Michigan', 'Electrical and Computer Engineering, Technion - Israel Institute of Technology, Technion', 'Computer Science, Illinois Institute of Technology']",
https://openreview.net/forum?id=7Mo1NOosNT,Transparency & Explainability,COLD: Causal reasOning in cLosed Daily activities,"Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed Daily activities) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (∼ 9 million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events.","['Causal Common Sense', 'Causal NLP', 'LLMs', 'Commonsense Reasoning']",[],"['Abhinav Joshi', 'Areeb Ahmad', 'Ashutosh Modi']","['Indian Institute of Technology, Kanpur', 'Indian Institute of Technology, Kanpur,', 'Computer Science and Engineering , IIT Kanpur']",
https://openreview.net/forum?id=Cp7HD618bd,Fairness & Bias,A Metalearned Neural Circuit for Nonparametric Bayesian Inference,"Most applications of machine learning to classification assume a closed set of balanced classes. This is at odds with the real world, where class occurrence statistics often follow a long-tailed power-law distribution and it is unlikely that all classes are seen in a single sample. Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency. To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network. By simulating data with a nonparametric Bayesian prior, we can metalearn a sequence model that performs inference over an unlimited set of classes. After training, this ""neural circuit"" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes. Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.","['Nonparametric Bayes', 'metalearning', 'amortized inference']",[],"['Jake Snell', 'Gianluca Bencomo', 'Thomas L. Griffiths']","['Computer Science, Princeton University', 'Princeton University', 'Princeton University']",
https://openreview.net/forum?id=bnNSQhZJ88,Security,Secret Collusion among AI Agents: Multi-Agent Deception via Steganography,"Recent advancements in generative AI suggest the potential for large-scale interaction between autonomous agents and humans across platforms such as the internet. While such interactions could foster productive cooperation, the ability of AI agents to circumvent security oversight raises critical multi-agent security problems, particularly in the form of unintended information sharing or undesirable coordination. In our work, we establish the subfield of secret collusion, a form of multi-agent deception, in which two or more agents employ steganographic methods to conceal the true nature of their interactions, be it communicative or otherwise, from oversight. We propose a formal threat model for AI agents communicating steganographically and derive rigorous theoretical insights about the capacity and incentives of large language models (LLMs) to perform secret collusion, in addition to the limitations of threat mitigation measures. We complement our findings with empirical evaluations demonstrating rising steganographic capabilities in frontier single and multi-agent LLM setups and examining potential scenarios where collusion may emerge, revealing limitations in countermeasures such as monitoring, paraphrasing, and parameter optimization. Our work is the first to formalize and investigate secret collusion among frontier foundation models, identifying it as a critical area in AI Safety and outlining a comprehensive research agenda to mitigate future risks of collusion between generative AI systems.","['Collusion', 'AI Safety', 'Steganography', 'Large Language Models', 'Model Evaluation Framework', 'Multi-Agent Security', 'Security', 'Frontier Models', 'GenAI', 'AI Control']",[],"['Sumeet Ramesh Motwani', 'Mikhail Baranchuk', 'Martin Strohmeier', 'Vijay Bolina', 'Philip Torr', 'Lewis Hammond', 'Christian Schroeder de Witt']","['University of Oxford', 'Department of Computer Science', 'armasuisse Science & Technology', 'University of California, Davis', 'University of Oxford', 'University of Oxford', 'Department of Engineering Science, University of Oxford']",
https://openreview.net/forum?id=a560KLF3v5,Security,Unelicitable Backdoors via Cryptographic Transformer Circuits,"The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to properly evaluate ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security.","['Backdoor attacks', 'Transformers', 'handcrafting model parameters', 'cryptographic circuits']",[],"['Andis Draguns', 'Andrew Gritsevskiy', 'Sumeet Ramesh Motwani', 'Christian Schroeder de Witt']","['Contramont Research', 'Department of Computer Science, University of Wisconsin - Madison', 'University of Oxford', 'Department of Engineering Science, University of Oxford']",
https://openreview.net/forum?id=uoJQ9qadjY,Transparency & Explainability,Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios,"Complex visual reasoning and question answering (VQA) is a challenging task that requires compositional multi-step processing and higher-level reasoning capabilities beyond the immediate recognition and localization of objects and events. Here, we introduce a fully neural Iterative and Parallel Reasoning Mechanism (IPRM) that combines two distinct forms of computation -- iterative and parallel -- to better address complex VQA scenarios.  Specifically, IPRM's ""iterative"" computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g. when computing the query “determine the color of pen to the left of the child in red t-shirt sitting at the white table”). Meanwhile, its  ""parallel'' computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of  operations that are mutually independent (e.g. when counting individual colors for the query: ""determine the maximum occurring color amongst all t-shirts'""). We design IPRM as a lightweight and fully-differentiable neural module that can be conveniently applied to both transformer and non-transformer vision-language backbones. It notably outperforms prior task-specific methods and transformer-based attention modules across various image and video VQA benchmarks testing distinct complex reasoning capabilities such as compositional spatiotemporal reasoning (AGQA), situational reasoning (STAR), multi-hop reasoning generalization (CLEVR-Humans) and causal event linking (CLEVRER-Humans). Further, IPRM's internal computations can be visualized across reasoning steps, aiding interpretability and diagnosis of its errors.",['iterative and parallel computation; complex visual reasoning and question answering; neural network based reasoning architectures'],[],"['Shantanu Jaiswal', 'Debaditya Roy', 'Basura Fernando', 'Cheston Tan']","['Carnegie Mellon University', 'Institute of High Performance Computing,  A*STAR', 'Nanyang Technological University', 'A*STAR']",
https://openreview.net/forum?id=1cXdndzkxU,Fairness & Bias,An Adaptive Approach for Infinitely Many-armed Bandits under Generalized Rotting Constraints,"In this study, we consider the infinitely many-armed bandit problems in a rested rotting setting, where the mean reward of an arm may decrease with each pull, while otherwise, it remains unchanged. We explore two scenarios regarding the rotting of rewards: one in which the cumulative amount of rotting is bounded by $V_T$, referred to as the slow-rotting case, and the other in which the cumulative number of rotting instances is bounded by $S_T$, referred to as the abrupt-rotting case. To address the challenge posed by rotting rewards, we introduce an algorithm that utilizes UCB with an adaptive sliding window, designed to manage the bias and variance trade-off arising due to rotting rewards. Our proposed algorithm achieves tight regret bounds for both slow and abrupt rotting scenarios. Lastly, we demonstrate the performance of our algorithm using numerical experiments.","['bandits', 'rotting rewards', 'infinitely many arms']",[],"['Jung-hun Kim', 'Milan Vojnovic', 'Se-Young Yun']","['Seoul National University', 'London School of Economics', 'KAIST']",
https://openreview.net/forum?id=nY0BrZdqLt,Security,Time-Reversal Provides Unsupervised Feedback to LLMs,"Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.","['LLMs', 'Reranking', 'reverse LLMs', 'reverse scoring', 'defenses', 'generative models', 'sequence reversal']",[],"['Yerram Varun', 'Rahul Madhavan', 'Sravanti Addepalli', 'Arun Suggala', 'Karthikeyan Shanmugam', 'Prateek Jain']","['Research, Google', 'Management, Indian Institute of Management, Ahmedabad', 'Google', 'Google DeepMind, Google', 'Google Deepmind', 'Google']",
https://openreview.net/forum?id=Kcsj9FGnKR,Transparency & Explainability,DiffuLT: Diffusion for Long-tail Recognition Without External Knowledge,"This paper introduces a novel pipeline for long-tail (LT) recognition that diverges from conventional strategies. Instead, it leverages the long-tailed dataset itself to generate a balanced proxy dataset without utilizing external data or model. We deploy a diffusion model trained from scratch on only the long-tailed dataset to create this proxy and verify the effectiveness of the data produced. Our analysis identifies approximately-in-distribution (AID) samples, which slightly deviate from the real data distribution and incorporate a blend of class information, as the crucial samples for enhancing the generative model's performance in long-tail classification. We promote the generation of AID samples during the training of a generative model by utilizing a feature extractor to guide the process and filter out detrimental samples during generation. Our approach, termed Diffusion model for Long-Tail recognition (DiffuLT), represents a pioneer application of generative models in long-tail recognition. DiffuLT achieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, surpassing leading competitors by significant margins. Comprehensive ablations enhance the interpretability of our pipeline. Notably, the entire generative process is conducted without relying on external data or pre-trained model weights, which leads to its generalizability to real-world long-tailed scenarios.","['Long-tail learning; long-tail classification', 'diffusion model']",[],"['Jie Shao', 'Ke Zhu', 'Hanxiao Zhang', 'Jianxin Wu']","['Nanjing University', 'Nanjing University', 'Artificial Intelligence, nanjing university', 'School of Artificial Intelligence, Nanjing university']",
https://openreview.net/forum?id=LOH6qzI7T6,Security,Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space,"With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation. Existing LLMs do not have an inherent functionality to provide the users with an uncertainty/confidence metric for each response it generates, making it difficult to evaluate trustworthiness. Although several studies aim to develop uncertainty quantification methods for LLMs, they have fundamental limitations, such as being restricted to classification tasks, requiring additional training and data, considering only lexical instead of semantic information, and being prompt-wise but not response-wise. A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty/confidence information for each response from a probability distribution perspective in semantic space. It has no restriction on task types and is ""off-the-shelf"" for new models and tasks. Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and Mixtral-8x22B models, on four free-form question-answering benchmarks demonstrate the superior performance and robustness of semantic density compared to prior approaches.","['uncertainty quantification', 'large language models', 'trustworthy AI']",[],"['Xin Qiu', 'Risto Miikkulainen']","['Cognizant AI Labs, Cognizant Technology Solutions Corporation', 'The University of Texas, Austin']",
https://openreview.net/forum?id=SeefZa7Vmq,Privacy & Data Governance,Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need,"Traditional unlearnable strategies have been proposed to prevent unauthorized users from training on the 2D image data. With more 3D point cloud data containing sensitivity information, unauthorized usage of this new type data has also become a serious concern. To address this, we propose the first integral unlearnable framework for 3D point clouds including two processes: (i) we propose an unlearnable data protection scheme, involving a class-wise setting established by a category-adaptive allocation strategy and multi-transformations assigned to samples; (ii) we propose a data restoration scheme that utilizes class-wise inverse matrix transformation, thus enabling authorized-only training for unlearnable data. This restoration process is a practical issue overlooked in most existing unlearnable literature, i.e., even authorized users struggle to gain knowledge from 3D unlearnable data. Both theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks) demonstrate the effectiveness of our proposed unlearnable framework. Our code is available at https://github.com/CGCL-codes/UnlearnablePC.","['Unlearnable examples', '3D point clouds', 'deep neural networks']",[],"['Xianlong Wang', 'Minghui Li', 'Wei Liu', 'Hangtao Zhang', 'Shengshan Hu', 'Yechao Zhang', 'Ziqi Zhou', 'Hai Jin']","['Huazhong University of Science and Technology', 'Software of Engineering, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'School of CSE, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'CSE, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'School of Computer Science and Technology, Huazhong University of Science and Technology']",
https://openreview.net/forum?id=4bJufOS6No,Security,On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection,"Large numbers of synthesized videos from diffusion models pose threats to information security and authenticity, leading to an increasing demand for generated content detection. However, existing video-level detection algorithms primarily focus on detecting facial forgeries and often fail to identify diffusion-generated content with a diverse range of semantics. To advance the field of video forensics, we propose an innovative algorithm named Multi-Modal Detection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the profound perceptual and comprehensive abilities of Large Multi-modal Models (LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's multi-modal space, enhancing its ability to detect unseen forgery content. Besides, MM-Det leverages an In-and-Across Frame Attention (IAFA)  mechanism for feature augmentation in the spatio-temporal domain. A dynamic fusion strategy helps refine forgery representations for the fusion. Moreover, we construct a comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF), across a wide range of forgery videos. MM-Det achieves state-of-the-art performance in DVF, demonstrating the effectiveness of our algorithm. Both source code and DVF are available at https://github.com/SparkleXFantasy/MM-Det.",['Video Forensics，Multi-Modal Large Language Model'],[],"['Xiufeng Song', 'Xiao Guo', 'Jiache Zhang', 'Qirui Li', 'LEI BAI', 'Xiaoming Liu', 'Guangtao Zhai', 'Xiaohong Liu']","['Shanghai Jiaotong University', 'Michigan State University', 'UM-SJTU joint institute, Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'AI for Science, Shanghai AI Laboratory', 'Michigan State University', 'EE, Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=sEpSxteEKJ,Transparency & Explainability,Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction,"Dynamical systems theory (DST) is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS combined by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and Rössler systems, AL-RNNs derive, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems.","['recurrent neural networks', 'dynamical systems', 'chaos', 'attractors', 'interpretability']",[],"['Manuel Brenner', 'Christoph Jürgen Hemmer', 'Zahra Monfared', 'Daniel Durstewitz']","['Heidelberg University', 'Department for Physics and Astronomy, Ruprecht-Karls-Universität Heidelberg', 'IWR, Heidelberg University, Ruprecht-Karls-Universität Heidelberg', 'Centre for Theoretical & Computational Neuroscience, Heidelberg University']",
https://openreview.net/forum?id=yRuJqoWoCs,Fairness & Bias,$SE(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation,"Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation.","['$SE(3)$ Equivariance', 'Stereo Depth Estimation']",[],"['Yinshuang Xu', 'Dian Chen', 'Katherine Liu', 'Sergey Zakharov', 'Rares Andrei Ambrus', 'Kostas Daniilidis', 'Vitor Campagnolo Guizilini']","['Department of Computer and Information Science, School of Engineering and Applied Science, University of Pennsylvania', 'Toyota Research Institute', 'Robotics/Large Behavior Models, Computer Vision, Toyota Research Institute', 'Toyota Research Institute', 'Machine Learning, Toyota Research Institute', 'Archimedes Unit, Athena Research and Innovation Centre', 'Toyota Research Institute']",
https://openreview.net/forum?id=CrADAX7h23,Security,DAGER: Exact Gradient Inversion for Large Language Models,"Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).","['Federated Learning', 'Exact Gradient Inversion', 'Gradient Leakage', 'Privacy', 'Language Model', 'LLM', 'Attack']",[],"['Ivo Petrov', 'Dimitar Iliev Dimitrov', 'Maximilian Baader', 'Mark Niklas Mueller', 'Martin Vechev']","['INSAIT, Sofia University ""St. Kliment Ohridski""', 'INSAIT, Sofia University ""St Kliment Ohridski""', 'Department of Computer Science, ETH Zurich', 'LogicStar AI', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=motImXq3B1,Transparency & Explainability,P$^2$C$^2$Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics,"When solving partial differential equations (PDEs), classical numerical methods often require fine mesh grids and small time stepping to meet stability, consistency, and convergence conditions, leading to high computational cost. Recently, machine learning has been increasingly utilized to solve PDE problems, but they often encounter challenges related to interpretability, generalizability, and strong dependency on rich labeled data. Hence, we introduce a new PDE-Preserved Coarse Correction Network (P$^2$C$^2$Net) to efficiently solve spatiotemporal PDE problems on coarse mesh grids in small data regimes. The model consists of two synergistic modules: (1) a trainable PDE block that learns to update the coarse solution (i.e., the system state), based on a high-order numerical scheme with boundary condition encoding, and (2) a neural network block that consistently corrects the solution on the fly. In particular, we propose a learnable symmetric Conv filter, with weights shared over the entire model, to accurately estimate the spatial derivatives of PDE based on the neural-corrected system state. The resulting physics-encoded model is capable of handling limited training data (e.g., 3--5 trajectories) and accelerates the prediction of PDE solutions on coarse spatiotemporal grids while maintaining a high accuracy. P$^2$C$^2$Net achieves consistent state-of-the-art performance with over 50\% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion processes and turbulent flows.","['physics-informed learning', 'coarse model', 'spatiotemporal dynamics prediction']",[],"['Qi Wang', 'Pu Ren', 'Hao Zhou', 'Xin-Yang Liu', 'Zhiwen Deng', 'Yi Zhang', 'Ruizhi Chengze', 'Hongsheng Liu', 'Zidong Wang', 'Jian-Xun Wang', 'Ji-Rong Wen', 'Hao Sun', 'Yang Liu']","['Renmin University of China', 'Machine Learning and Analytics, Lawrence Berkeley National Lab', 'Renmin University of China', 'University of Notre Dame', 'ByteDance Inc.', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'CSI, Huawei Technologies Ltd.', 'Zhejiang University', 'University of Notre Dame', 'Gaoling School of Artificial Intelligence, Renmin University of China', '', 'School of Engineering Science, University of Chinese Academy of Sciences']",
https://openreview.net/forum?id=FeCWZviCeP,Transparency & Explainability,Hierarchical Programmatic Option Framework,"Deep reinforcement learning aims to learn deep neural network policies to solve large-scale decision-making problems. However, approximating policies using deep neural networks makes it difficult to interpret the learned decision-making process. To address this issue, prior works (Trivedi et al., 2021; Liu et al., 2023; Carvalho et al., 2024) proposed to use human-readable programs as policies to increase the interpretability of the decision-making pipeline. Nevertheless, programmatic policies generated by these methods struggle to effectively solve long and repetitive RL tasks and cannot generalize to even longer horizons during testing. To solve these problems, we propose the Hierarchical Programmatic Option framework (HIPO), which aims to solve long and repetitive RL problems with human-readable programs as options (low-level policies). Specifically, we propose a method that retrieves a set of effective, diverse, and compatible programs as options. Then, we learn a high-level policy to effectively reuse these programmatic options to solve reoccurring subtasks. Our proposed framework outperforms programmatic RL and deep RL baselines on various tasks. Ablation studies justify the effectiveness of our proposed search algorithm for retrieving a set of programmatic options.","['Reinforcement Learning', 'Programmatic Reinforcement Learning', 'Hierarchical Reinforcement Learning', 'Program Synthesis']",[],"['Yu-An Lin', 'Chen-Tao Lee', 'Chih-Han Yang', 'Guan-Ting Liu', 'Shao-Hua Sun']","['National Taiwan University', 'National Taiwan University', 'Computer Science and Information Engineering , National Taiwan University', 'Department of computer science and information engineering, National Taiwan University', 'National Taiwan University']",
https://openreview.net/forum?id=ISa7mMe7Vg,Security,Exploiting LLM Quantization,"Quantization leverages lower-precision weights to reduce the memory usage of large language models (LLMs) and is a key technique for enabling their deployment on commodity hardware. While LLM quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model.  We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an LLM community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.","['quantization', 'large language models', 'security', 'poisoning']",[],"['Kazuki Egashira', 'Mark Vero', 'Robin Staab', 'Jingxuan He', 'Martin Vechev']","['The University of Tokyo', 'Deparment of Computer Science, ETHZ-ETH Zurich', 'Computer Science, ETHZ - ETH Zurich', 'University of California, Berkeley', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=PTxRRUEpHq,Fairness & Bias,Gradient Methods for Online DR-Submodular Maximization with Stochastic Long-Term Constraints,"In this paper, we consider the problem of online monotone DR-submodular maximization subject to long-term stochastic constraints. Specifically, at each round $t\in [T]$, after committing an action $\mathbf{x}_t$, a random reward $f_t(\mathbf{x}_t)$ and an unbiased gradient estimate of the point $\widetilde{\nabla}f_t(\mathbf{x}_t)$ (semi-bandit feedback) are revealed. Meanwhile, a budget of $g_t(\mathbf{x}_t)$, which is linear and stochastic, is consumed of its total allotted budget $B_T$. We propose a gradient ascent based algorithm that achieves $\frac{1}{2}$-regret of $\mathcal{O}(\sqrt{T})$ with $\mathcal{O}(T^{3/4})$ constraint violation with high probability. Moreover, when first-order full-information feedback is available, we propose an algorithm that achieves $(1-1/e)$-regret of $\mathcal{O}(\sqrt{T})$ with $\mathcal{O}(T^{3/4})$ constraint violation. These algorithms significantly improve over the state-of-the-art in terms of query complexity.","['DR-submodular', 'long term constraint', 'gradient ascent']",[],"['Guanyu Nie', 'Vaneet Aggarwal', 'Christopher John Quinn']","['Computer Science, Iowa State University', 'Purdue University', 'Iowa State University']",
https://openreview.net/forum?id=qaC4sSztlF,Privacy & Data Governance,Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing,"Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.","['Diffusion based models', 'Safe generation', 'Concept Transfer', 'Representation editing']",[],"['Peiran Dong', 'Bingjie WANG', 'Song Guo', 'Junxiao Wang', 'Jie ZHANG', 'Zicong Hong']","['Department of Computing, Hong Kong Polytechnic University', 'DSAI, Hong Kong Polytechnic University', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'CIAT, Guangzhou University', 'Computer Science and Engineering, Hong Kong University of Science and Technology', 'Hong Kong Polytechnic University']",
https://openreview.net/forum?id=Es2Ey2tGmM,Fairness & Bias,Constrained Diffusion Models via Dual Training,"Diffusion models have attained prominence for their ability to synthesize a probability distribution for a given dataset via a diffusion process,  enabling the generation of new data points with high fidelity. However, diffusion processes are prone to generating samples that reflect biases in a training dataset. To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements. Specifically, we cast the training of diffusion models under requirements as a constrained distribution optimization problem that aims to reduce the distribution difference between original and generated data while obeying constraints on the distribution of generated data. We show that our constrained diffusion models generate new data from a mixture data distribution that achieves the optimal trade-off among objective and constraints. To train constrained diffusion models, we develop a dual training algorithm and characterize the optimality of the trained constrained diffusion model. We empirically demonstrate the effectiveness of our constrained models in two constrained generation tasks: (i) we consider a dataset with one or more underrepresented classes where we train the model with constraints to ensure fairly sampling from all classes during inference; (ii) we fine-tune a pre-trained diffusion model to sample from a new dataset while avoiding overfitting.","['Constrained diffusion model', 'constrained optimization', 'Lagrangian method', 'dual algorithm']",[],"['Shervin Khalafi', 'Dongsheng Ding', 'Alejandro Ribeiro']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']",
https://openreview.net/forum?id=JNDcFOczOf,Security,RA-PbRL: Provably Efficient Risk-Aware Preference-Based Reinforcement Learning,"Reinforcement Learning from Human Feedback (RLHF) has recently surged in popularity, particularly for aligning large language models and other AI systems with human intentions. At its core, RLHF can be viewed as a specialized instance of Preference-based Reinforcement Learning (PbRL), where the preferences specifically originate from human judgments rather than arbitrary evaluators. Despite this connection, most existing approaches in both RLHF and PbRL primarily focus on optimizing a mean reward objective, neglecting scenarios that necessitate risk-awareness, such as AI safety, healthcare, and autonomous driving. These scenarios often operate under a one-episode-reward setting, which makes conventional risk-sensitive objectives inapplicable. To address this, we explore and prove the applicability of two risk-aware objectives to PbRL : nested and static quantile risk objectives. We also introduce Risk-AwarePbRL (RA-PbRL), an algorithm designed to optimize both nested and static objectives. Additionally, we provide a theoretical analysis of the regret upper bounds, demonstrating that they are sublinear with respect to the number of episodes, and present empirical results to support our findings. Our code is available in https://github.com/aguilarjose11/PbRLNeurips.","['Reinforcement Learning Human Feedback', 'Risk-aware Reinforcement Learning']",[],"['Yujie Zhao', 'Jose Efraim Aguilar Escamilla', 'Weyl Lu', 'Huazheng Wang']","['CSE, University of California, San Diego', 'Eecs, Oregon State University', '', 'Oregon State University']",
https://openreview.net/forum?id=OYOkkqRLvj,Security,Amortized Eigendecomposition for Neural Networks,"Performing eigendecomposition during neural network training is essential for tasks such as dimensionality reduction, network compression, image denoising, and graph learning. However, eigendecomposition is computationally expensive as it is orders of magnitude slower than other neural network operations. To address this challenge, we propose a novel approach called ""amortized eigendecomposition"" that relaxes the exact eigendecomposition by introducing an additional loss term called eigen loss. Our approach offers significant speed improvements by replacing the computationally expensive eigendecomposition with a more affordable QR decomposition at each iteration. Theoretical analysis guarantees that the desired eigenpair is attained as optima of the eigen loss. Empirical studies on nuclear norm regularization, latent-space principal component analysis, and graphs adversarial learning demonstrate significant improvements in training efficiency while producing nearly identical outcomes to conventional approaches. This novel methodology promises to integrate eigendecomposition efficiently into neural network training, overcoming existing computational challenges and unlocking new potential for advanced deep learning applications.","['Eigendecomposition', 'SVD', 'Amortized optimization.']",[],"['Tianbo Li', 'Zekun Shi', 'Jiaxi Zhao', 'Min Lin']","['Sea AI Lab', 'School of Computing, National University of Singapore', 'National University of Singapore', 'Sea AI Lab']",
https://openreview.net/forum?id=zLU21oQjD5,Fairness & Bias,DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving,"Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries. Hypothesizing that difficult queries are crucial to learning complex reasoning, we propose *Difficulty-Aware Rejection Tuning* (`DART`), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples. Utilizing `DART`, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4. We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called `DART-Math`. In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, `DART-Math` outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.","['Large Language Models', 'Mathematical Reasoning', 'Synthetic Data']",[],"['Yuxuan Tong', 'Xiwen Zhang', 'Rui Wang', 'Ruidong Wu', 'Junxian He']","['', 'Helixon Research', 'Helixon', 'Helixon AI', 'Hong Kong University of Science and Technology']",
https://openreview.net/forum?id=rjSPDVdUaw,Fairness & Bias,Moving Off-the-Grid: Scene-Grounded Video Representations,"Current vision models typically maintain a fixed correspondence between their representation structure and image space. Each layer comprises a set of tokens arranged “on-the-grid,” which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present *Moving Off-the-Grid* (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move “off-the-grid” to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective—next frame prediction—trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG’s learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines.","['Self supervised learning', 'point tracking', 'representation learning']",[],"['Sjoerd van Steenkiste', 'Daniel Zoran', 'Yi Yang', 'Yulia Rubanova', 'Rishabh Kabra', 'Carl Doersch', 'Dilara Gokay', 'Joseph Heyward', 'Etienne Pot', 'Klaus Greff', 'Drew A. Hudson', 'Thomas Albert Keck', 'Joao Carreira', 'Alexey Dosovitskiy', 'Mehdi S. M. Sajjadi', 'Thomas Kipf']","['Google', 'Google DeepMind', 'Google DeepMind', 'Google DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'Foundational Research, Google', 'Google', 'Google', 'Google DeepMind', 'DeepMind', 'Google DeepMind', 'Google', 'Foundational Research, Google DeepMind', 'Google']",
https://openreview.net/forum?id=JEflV4nRlH,Security,What Makes and Breaks Safety Fine-tuning? A Mechanistic Study,"Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., “design”) versus the specific concepts the task is asked to be performed upon (e.g., a “cycle” vs. a “bomb”). Using this, we investigate three well-known safety fine-tuning methods—supervised safety fine-tuning, direct preference optimization, and unlearning—and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights’ null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.","['Mechanistic Interpretability', 'AI Safety', 'Safety fine tuning', 'Large Language Models']",[],"['Samyak Jain', 'Ekdeep Singh Lubana', 'Kemal Oksuz', 'Tom Joy', 'Philip Torr', 'Amartya Sanyal', 'Puneet K. Dokania']","['Research, Microsoft', 'Center for Brain Science, Harvard University, Harvard University', 'Five AI', 'Five AI', 'University of Oxford', 'Computer Science, Copenhagen University', 'Five AI Limited, Bosch']",
https://openreview.net/forum?id=5jYFoldunM,Security,On the Adversarial Robustness of Benjamini Hochberg,"The Benjamini-Hochberg (BH) procedure is widely used to control the false detection rate (FDR) in multiple testing. Applications of this control abound in drug discovery, forensics, anomaly detection, and, in particular, machine learning, ranging from nonparametric outlier detection to out-of-distribution detection and one-class classification methods. Considering this control could be relied upon in critical safety/security contexts, we investigate its adversarial robustness. More precisely, we study under what conditions BH does and does not exhibit adversarial robustness, we present a class of simple and easily implementable adversarial test-perturbation algorithms, and we perform computational experiments. With our algorithms, we demonstrate that there are conditions under which BH's control can be significantly broken with relatively few (even just one) test score perturbation(s), and provide non-asymptotic guarantees on the expected adversarial-adjustment to FDR. Our technical analysis involves a combinatorial reframing of the BH procedure as a ``balls into bins'' process, and drawing a connection to generalized ballot problems to facilitate an information-theoretic approach for deriving non-asymptotic lower bounds.","['multiple testing', 'p-values', 'false discovery rate', 'adversarial robust']",[],"['Louis Chen', 'Roberto Szechtman', 'Matan Seri']","['Operations Research, Naval Postgraduate School', 'Naval Postgraduate School', '']",
https://openreview.net/forum?id=bIa03mAtxQ,Transparency & Explainability,Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization,"The Mixture of Experts (MoE) paradigm provides a powerful way to decompose dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. However, a major challenge lies in the computational cost of scaling the number of experts high enough to achieve fine-grained specialization. In this paper, we propose the Multilinear Mixture of Experts (μMoE) layer to address this, focusing on vision models. μMoE layers enable scalable expert specialization by performing an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, μMoEs (1) avoid the restrictively high inference-time costs of dense MoEs, yet (2) do not inherit the training issues of the popular sparse MoEs' discrete (non-differentiable) expert routing. We present both qualitative and quantitative evidence that scaling μMoE layers when fine-tuning foundation models for vision tasks leads to more specialized experts at the class-level, further enabling manual bias correction in CelebA attribute classification. Finally, we show qualitative results demonstrating the expert specialism achieved when pre-training large GPT2 and MLP-Mixer models with parameter-matched μMoE blocks at every layer, maintaining comparable accuracy. Our code is available at: https://github.com/james-oldfield/muMoE.","['interpretability', 'mixture of experts']",[],"['James Oldfield', 'Markos Georgopoulos', 'Grigorios Chrysos', 'Christos Tzelepis', 'Yannis Panagakis', 'Mihalis Nicolaou', 'Jiankang Deng', 'Ioannis Patras']","['Queen Mary University London', 'Meta', 'University of Wisconsin - Madison', '', '', 'Computation-based Science and Technology Research Center, The Cyprus Institute', 'Department of Computing, Imperial College London', 'Queen Mary, University of London']",
https://openreview.net/forum?id=da0ZJatRCN,Security,Active Learning for Derivative-Based Global Sensitivity Analysis with Gaussian Processes,"We consider the problem of active learning for global sensitivity analysis of expensive black-box functions. Our aim is to efficiently learn the importance of different input variables, e.g., in vehicle safety experimentation, we study the impact of the thickness of various components on safety objectives. Since function evaluations are expensive, we use active learning to prioritize experimental resources where they yield the most value. We propose novel active learning acquisition functions that directly target key quantities of derivative-based global sensitivity measures (DGSMs) under Gaussian process surrogate models. We showcase the first application of active learning directly to DGSMs, and develop tractable uncertainty reduction and information gain acquisition functions for these measures. Through comprehensive evaluation on synthetic and real-world problems, our study demonstrates how these active learning acquisition strategies substantially enhance the sample efficiency of DGSM estimation, particularly with limited evaluation budgets. Our work paves the way for more efficient and accurate sensitivity analysis in various scientific and engineering applications.","['Global Sensitivity Analysis', 'Gaussian Processes', 'Bayesian Active Learning', 'Bayesian optimization']",[],"['Syrine Belakaria', 'Benjamin Letham', 'Jana Doppa', 'Barbara E Engelhardt', 'Stefano Ermon', 'Eytan Bakshy']","['Stanford University', '', '', 'Stanford University', 'Computer Science, Stanford University', 'Information, Meta']",
https://openreview.net/forum?id=9sP4oejtjB,Transparency & Explainability,Disentangling the Roles of Distinct Cell Classes with Cell-Type Dynamical Systems,"Latent dynamical systems have been widely used to characterize the dynamics of neural population activity in the brain. However, these models typically ignore the fact that the brain contains multiple cell types. This limits their ability to capture the functional roles of distinct cell classes, and to predict the effects of cell-specific perturbations on neural activity or behavior. To overcome these limitations, we introduce the `""cell-type dynamical systems"" (CTDS) model. This model extends latent linear dynamical systems to contain distinct latent variables for each cell class, with biologically inspired constraints on both dynamics and emissions. To illustrate our approach, we consider neural recordings with distinct excitatory (E) and inhibitory (I) populations.   The CTDS model defines separate latents for both cell types, and constrains the dynamics so that E (I) latents have a strictly positive (negative) effects on other latents. We applied CTDS to recordings from rat frontal orienting fields (FOF) and anterior dorsal striatum (ADS) during an auditory decision-making task. The model achieved higher accuracy than a standard linear dynamical system (LDS), and revealed that the animal's choice can be decoded from both E and I latents and thus is not restricted to a single cell-class. We also performed in-silico optogenetic perturbation experiments in the FOF and ADS, and found that CTDS was able to replicate the experimentally observed effects of different perturbations on behavior, whereas a standard LDS model---which does not differentiate between cell types---did not. Crucially, our model allowed us to understand the effects of these perturbations by revealing the dynamics of different cell-specific latents. Finally, CTDS can also be used to identify cell types for neurons whose class labels are unknown in electrophysiological recordings. These results illustrate the power of the CTDS model to provide more accurate and more biologically interpretable descriptions of neural population dynamics and their relationship to behavior.","['neuroscience', 'neural dynamics', 'animal decision making']",[],"['Aditi Jha', 'Diksha Gupta', 'Carlos D Brody', 'Jonathan W. Pillow']","['Princeton University', 'University College London, University of London', 'Princeton University', 'Princeton Neuroscience Institute, Princeton University']",
https://openreview.net/forum?id=3mCr7ZNdSw,Privacy & Data Governance,Privacy without Noisy Gradients: Slicing Mechanism for Generative Model Training,"Training generative models with differential privacy (DP)  typically involves injecting noise into gradient updates or adapting the discriminator's training procedure. As a result, such approaches often struggle with hyper-parameter tuning and convergence.  We consider the \emph{slicing privacy mechanism} that injects noise into random low-dimensional projections of the private data, and provide strong privacy guarantees for it. These noisy projections are used for training generative models. To enable optimizing generative models using this DP approach, we introduce the \emph{smoothed-sliced $f$-divergence} and show it enjoys statistical consistency.   Moreover, we present a kernel-based estimator for this divergence, circumventing the need for adversarial training.  Extensive numerical experiments demonstrate that our approach can generate synthetic data of higher quality compared with baselines. Beyond performance improvement, our method, by sidestepping the need for noisy gradients, offers data scientists the flexibility to adjust generator architecture and hyper-parameters, run the optimization over any number of epochs, and even restart the optimization process---all without incurring additional privacy costs.","['differential privacy', 'synthetic data generation', 'tabular data', 'GAN', 'f divergence', 'slicing']",[],"['Kristjan Greenewald', 'Yuancheng Yu', 'Hao Wang', 'Kai Xu']","['MIT-IBM Watson AI Lab, IBM Research', 'CS, University of Illinois at Urbana-Champaign', 'RedHat AI & MIT-IBM Watson AI Lab', 'MIT-IBM Watson AI Lab']",
https://openreview.net/forum?id=oPFjhl6DpR,Security,Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation,"Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29\% fewer samples than baselines, and reduces training time by 21--38\%.","['Safe Reinforcement Learning', 'Sample Manipulation', 'Efficient Learning']",[],"['Shangding Gu', 'Laixi Shi', 'Yuhao Ding', 'Alois Knoll', 'Costas Spanos', 'Adam Wierman', 'Ming Jin']","['EECS, University of California, Berkeley', 'California Institute of Technology', 'Cubist', 'Technical University Munich', 'University of California, Berkeley', 'Computing and Mathematical Sciences, California Institute of Technology', 'Virginia Tech']",
https://openreview.net/forum?id=A5pabdZp2F,Security,MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities,"Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin. To support accessibility and reproducibility, our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.","['Out-of-Distribution Detection', 'Multimodal Learning']",[],"['Hao Dong', 'Yue Zhao', 'Eleni Chatzi', 'Olga Fink']","['ETHZ - ETH Zurich', 'Department of Computer Science, University of Southern California', 'ETHZ - ETH Zurich', 'EPFL - EPF Lausanne']",
https://openreview.net/forum?id=AB6XpMzvqH,Fairness & Bias,Many-Shot In-Context Learning,"Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples – the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs. To mitigate this limitation, we explore two new settings: (1) ""Reinforced ICL"" that uses model-generated chain-of-thought rationales in place of human rationales, and (2) ""Unsupervised ICL"" where we remove rationales from the prompt altogether, and prompts the model only with domain-specific inputs. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. We demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to supervised fine-tuning. Finally, we reveal the limitations of next-token prediction loss as an indicator of downstream ICL performance.","['large language models', 'in-context learning', 'long-context models']",[],"['Rishabh Agarwal', 'Avi Singh', 'Lei M Zhang', 'Bernd Bohnet', 'Luis Rosias', 'Stephanie C.Y. Chan', 'Biao Zhang', 'Ankesh Anand', 'Zaheer Abbas', 'Azade Nova', 'John D Co-Reyes', 'Eric Chu', 'Feryal Behbahani', 'Aleksandra Faust', 'Hugo Larochelle']","['Computer Science, McGill University', 'Google', 'Google DeepMind', 'Google', 'DeepMind, Google', 'DeepMind', 'Google DeepMind', 'Mila, University of Montreal', 'DeepMind', 'Google Brain', 'University of California Berkeley', 'Massachusetts Institute of Technology', 'DeepMind', 'Google Brain', 'Université de Montréal']",
https://openreview.net/forum?id=lPDxPVS6ix,Security,SPEAR: Exact Gradient Inversion of Batches in Federated Learning,"Federated learning is a framework for collaborative machine learning where clients only share gradient updates and not their private data with a server. However, it was recently shown that gradient inversion attacks can reconstruct this data from the shared gradients. In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of $b=1$, with larger batches permitting only approximate reconstruction. In this work, we propose SPEAR, *the first algorithm reconstructing whole batches with $b >1$ exactly*. SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to $b \lesssim 25$ exactly while scaling to large networks. Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time.","['Federated Learning', 'Exact Gradient Inversion', 'Gradient Leakage', 'Privacy', 'Attack']",[],"['Dimitar Iliev Dimitrov', 'Maximilian Baader', 'Mark Niklas Mueller', 'Martin Vechev']","['INSAIT, Sofia University ""St Kliment Ohridski""', 'Department of Computer Science, ETH Zurich', 'LogicStar AI', 'Computer Science, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=HkC4OYee3Q,Security,SleeperNets: Universal Backdoor Poisoning Attacks Against  Reinforcement Learning Agents,"Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy -- guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop ""SleeperNets"" as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.","['Reinforcement Learning', 'Backdoor Attacks', 'Adversarial Machine Learning', 'Security', 'Poisoning Attacks', 'Reinforcement Learning Theory']",[],"['Ethan Rathbun', 'Christopher Amato', 'Alina Oprea']","['Khoury College of Computer Sciences, Northeastern University', 'Northeastern University', 'Computer Science, Northeastern University']",
https://openreview.net/forum?id=8jB6sGqvgQ,Security,Efficient Adversarial Training in LLMs with Continuous Attacks,"Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.","['Large Language Models', 'Adversarial Training', 'Robustness']",[],"['Sophie Xhonneux', 'Alessandro Sordoni', 'Stephan Günnemann', 'Gauthier Gidel', 'Leo Schwinn']","['Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'Microsoft', 'Technical University Munich', 'Computer Science and Operational Research, University of Montreal', 'Technical University of Munich']",
https://openreview.net/forum?id=1PcJ5Evta7,Security,BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment,"Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning on users' uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a ""backdoor trigger"". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the ""backdoor attack"", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.","['Fine-tuning based Jailbreak Attack', 'Backdoor Attack', 'Safety Alignment for Large Language Models']",[],"['Jiongxiao Wang', 'Jiazhao Li', 'Yiquan Li', 'Xiangyu Qi', 'Junjie Hu', 'Yixuan Li', 'Patrick McDaniel', 'Muhao Chen', 'Bo Li', 'Chaowei Xiao']","['iSchool, University of Wisconsin - Madison', 'Rufus, Amazon', 'University of Wisconsin - Madison', 'Princeton University', 'University of Wisconsin, Madison', 'University of Wisconsin, Madison', 'Computer Sciences, University of Wisconsin - Madison', 'University of California, Davis', 'CS, University of Illinois, Urbana Champaign', 'University of Wisconsin - Madison']",
https://openreview.net/forum?id=jXs6Cvpe7k,Security,Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks,"Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior. While some defenses have been proposed, they have not been adapted to newly proposed attacks and more challenging threat models. To address this, we propose an optimization-based objective for defending LLMs against jailbreaking attacks and an algorithm, Robust Prompt Optimization (RPO), to create robust system-level defenses. Our approach directly incorporates the adversary into the defensive objective and optimizes a lightweight and transferable suffix, enabling RPO to adapt to worst-case adaptive attacks. Our theoretical and experimental results show improved robustness to both jailbreaks seen during optimization and unknown jailbreaks, reducing the attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench, setting the state-of-the-art.","['large language models', 'AI safety', 'jailbreaking', 'red-teaming']",[],"['Andy Zhou', 'Bo Li', 'Haohan Wang']","['Department of Computer Science, University of Illinois at Urbana-Champaign', 'CS, University of Illinois, Urbana Champaign', 'University of Illinois at Urbana-Champaign']",
https://openreview.net/forum?id=AcBLtTKK5q,Security,Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters,"Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\sim$ $\times$ 19.88) and lower filtered-out rates ($\sim$ $\times$ 1/6) than baselines.","['Large Language Models', 'Jailbreak', 'Red-teaming', 'Safety']",[],"['Haibo Jin', 'Andy Zhou', 'Joe D. Menke', 'Haohan Wang']","['School of Information Sciences, University of Illinois at Urbana-Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'School of Information Sciences, University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']",
https://openreview.net/forum?id=hKVTwQQu76,Security,DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment,"Graph neural networks (GNNs) are recognized for their strong performance across various applications, with the backpropagation (BP) algorithm playing a central role in the development of most GNN models. However, despite its effectiveness, BP has limitations that challenge its biological plausibility and affect the efficiency, scalability and parallelism of training neural networks for graph-based tasks. While several non-backpropagation (non-BP) training algorithms, such as the direct feedback alignment (DFA), have been successfully applied to fully-connected and convolutional network components for handling Euclidean data, directly adapting these non-BP frameworks to manage non-Euclidean graph data in GNN models presents significant challenges. These challenges primarily arise from the violation of the independent and identically distributed (i.i.d.) assumption in graph data and the difficulty in accessing prediction errors for all samples (nodes) within the graph. To overcome these obstacles, in this paper we propose DFA-GNN, a novel forward learning framework tailored for GNNs with a case study of semi-supervised learning. The proposed method breaks the limitations of BP by using a dedicated forward training mechanism. Specifically, DFA-GNN extends the principles of DFA to adapt to graph data and unique architecture of GNNs, which incorporates the information of graph topology into the feedback links to accommodate the non-Euclidean characteristics of graph data. Additionally, for semi-supervised graph learning tasks, we developed a pseudo error generator that spreads residual errors from training data to create a pseudo error for each unlabeled node. These pseudo errors are then utilized to train GNNs using DFA. Extensive experiments on 10 public benchmarks reveal that our learning framework outperforms not only previous non-BP methods but also the standard BP methods, and it exhibits excellent robustness against various types of noise and attacks.","['Graph neural network', 'direct feedback alignment', 'graph learning', 'non-BP training approach']",[],"['Gongpei Zhao', 'Tao Wang', 'Congyan Lang', 'Yi Jin', 'Yidong Li', 'Haibin Ling']","['Beijing jiaotong univercity', 'Computer Science and Technology, Beijing Jiaotong University', 'School of Computer Science and Technology, Beijing jiaotong university', 'Beijing Jiaotong University', 'School of Computer Science and Technology, Beijing Jiaotong University', 'State University of New York, Stony Brook']",
https://openreview.net/forum?id=6gMnj9oc6d,Privacy & Data Governance,Scalable DP-SGD: Shuffling vs. Poisson Subsampling,"We provide new lower bounds on the privacy guarantee of _multi-epoch_ Adaptive Batch Linear Queries (ABLQ) mechanism with _shuffled batch sampling_, demonstrating substantial gaps when compared to _Poisson subsampling_; prior analysis was limited to a single epoch. Since the privacy analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by analyzing the ABLQ mechanism, this brings into serious question the common practice of implementing Shuffling based DP-SGD, but reporting privacy parameters as if Poisson subsampling was used. To understand the impact of this gap on the utility of trained machine learning models, we introduce a novel practical approach to implement Poisson subsampling _at scale_ using massively parallel computation, and efficiently train models with the same. We provide a comparison between the utility of models trained with Poisson subsampling based DP-SGD, and the optimistic estimates of utility when using shuffling, via our new lower bounds on the privacy guarantee of ABLQ with shuffling.","['DPSGD', 'Differential Privacy', 'Shuffling', 'Poisson subsampling']",[],"['Lynn Chua', 'Badih Ghazi', 'Pritish Kamath', 'Ravi Kumar', 'Pasin Manurangsi', 'Amer Sinha', 'Chiyuan Zhang']","['Google', 'Google', 'Google Research', 'Research, Google', 'Google', 'Research, Google', 'Google']",
https://openreview.net/forum?id=74B6qX62vW,Privacy & Data Governance,Sample-Efficient Private Learning of Mixtures of Gaussians,"We study the problem of learning mixtures of Gaussians with approximate differential privacy. We prove that roughly $kd^2 + k^{1.5} d^{1.75} + k^2 d$ samples suffice to learn a mixture of $k$ arbitrary $d$-dimensional Gaussians up to low total variation distance, with differential privacy. Our work improves over the previous best result (which required roughly $k^2 d^4$ samples) and is provably optimal when $d$ is much larger than $k^2$. Moreover, we give the first optimal bound for privately learning mixtures of $k$ univariate (i.e., $1$-dimensional) Gaussians. Importantly, we show that the sample complexity for learning mixtures of univariate Gaussians is linear in the number of components $k$, whereas the previous best sample complexity was quadratic in $k$. Our algorithms utilize various techniques, including the inverse sensitivity mechanism, sample compression for distributions, and methods for bounding volumes of sumsets.","['Differential Privacy', 'Density Estimation', 'Mixtures of Gaussians', 'Sample Complexity']",[],"['Hassan Ashtiani', 'Mahbod Majid', 'Shyam Narayanan']","['McMaster University', 'Carnegie Mellon University', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=T5Cerv7PT2,Security,Simplifying Constraint Inference with Inverse Reinforcement Learning,"Learning safe policies has presented a longstanding challenge for the reinforcement learning (RL) community. Various formulations of safe RL have been proposed; However, fundamentally, tabula rasa RL must learn safety constraints through experience, which is problematic for real-world applications. Imitation learning is often preferred in real-world settings because the experts' safety preferences are embedded in the data the agent imitates. However, imitation learning is limited in its extensibility to new tasks, which can only be learned by providing the agent with expert trajectories. For safety-critical applications with sub-optimal or inexact expert data, it would be preferable to learn only the safety aspects of the policy through imitation, while still allowing for task learning with  RL. The field of inverse constrained RL, which seeks to infer constraints from expert data, is a promising step in this direction. However, prior work in this area has relied on complex tri-level optimizations in order to infer safe behavior (constraints). This challenging optimization landscape leads to sub-optimal performance on several benchmark tasks. In this work, we present a simplified version of constraint inference that performs as well or better than prior work across a collection of continuous-control benchmarks. Moreover, besides improving performance, this simplified framework is easier to implement, tune, and more readily lends itself to various extensions, such as offline constraint inference.","['reinforcement learning', 'inverse reinforcement learning', 'safe reinforcement learning', 'constrained reinforcement learning']",[],"['Adriana Hugessen', 'Harley Wiltzer', 'Glen Berseth']","['Computer Science, Université de Montréal', 'School of Computer Science, McGill University, McGill University', 'University of Montreal, University of Montreal']",
https://openreview.net/forum?id=gVTkMsaaGI,Fairness & Bias,"Amortizing intractable inference in diffusion models for vision, language, and control","Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies *amortized* sampling of the posterior over data, $\mathbf{x}\sim p^{\rm post}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})$, in a model that consists of a diffusion generative model prior $p(\mathbf{x})$ and a black-box constraint or likelihood function $r(\mathbf{x})$. We state and prove the asymptotic correctness of a data-free learning objective, *relative trajectory balance*, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning. Code is available at [this link](https://github.com/GFNOrg/diffusion-finetuning).","['diffusion', 'inverse problems', 'conditional generation', 'language models', 'infilling', 'discrete diffusion', 'offline RL', 'planning', 'GFlowNet']",[],"['Siddarth Venkatraman', 'Moksh Jain', 'Luca Scimeca', 'Minsu Kim', 'Marcin Sendera', 'Mohsin Hasan', 'Luke Rowe', 'Sarthak Mittal', 'Pablo Lemos', 'Emmanuel Bengio', 'Alexandre Adam', 'Jarrid Rector-Brooks', 'Yoshua Bengio', 'Glen Berseth', 'Nikolay Malkin']","['Université de Montréal', 'Université de Montréal', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Korea Advanced Institute of Science and Technology', 'Mila - Quebec AI Institute', 'Computer Science, Mila - Quebec Artificial Intelligence Institute', 'Computer Science, Université de Montréal', 'University of Montreal', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Recursion', 'Université de Montréal', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'computer science and operations research, University of Montreal', 'University of Montreal, University of Montreal', '']",
https://openreview.net/forum?id=cQoAgPBARc,Fairness & Bias,Improving Deep Reinforcement Learning by Reducing the Chain Effect of Value and Policy Churn,"Deep neural networks provide Reinforcement Learning (RL) powerful function approximators to address large-scale decision-making problems. However, these approximators introduce challenges due to the non-stationary nature of RL training. One source of the challenges in RL is that output predictions can churn, leading to uncontrolled changes after each batch update for states not included in the batch. Although such a churn phenomenon exists in each step of network training, it remains under-explored on how churn occurs and impacts RL. In this work, we start by characterizing churn in a view of Generalized Policy Iteration with function approximation, and we discover a chain effect of churn that leads to a cycle where the churns in value estimation and policy improvement compound and bias the learning dynamics throughout the iteration. Further, we concretize the study and focus on the learning issues caused by the chain effect in different settings, including greedy action deviation in value-based methods, trust region violation in proximal policy optimization, and dual bias of policy value in actor-critic methods. We then propose a method to reduce the chain effect across different settings, called Churn Approximated ReductIoN (CHAIN), which can be easily plugged into most existing DRL algorithms. Our experiments demonstrate the effectiveness of our method in both reducing churn and improving learning performance across online and offline, value-based and policy-based RL settings.","['Reinforcement Learning', 'Deep Learning', 'Regularization and Optimization']",[],"['Hongyao Tang', 'Glen Berseth']","['Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'University of Montreal, University of Montreal']",
https://openreview.net/forum?id=sFaFDcVNbW,Security,GSGAN: Adversarial Learning for Hierarchical Generation of 3D Gaussian Splats,"Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a na\""ive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce GSGAN, a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (×100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability.","['3D GANs', 'Generative Adversarial Networks', '3D Gaussian Splatting', '3D Generative Models']",[],"['Sangeek Hyun', 'Jae-Pil Heo']","['Artificial Intelligence, Sungkyunkwan University', 'Sungkyunkwan University']",
https://openreview.net/forum?id=zApFYcLg6K,Fairness & Bias,On Differentially Private U Statistics,"We consider the problem of privately estimating a parameter $\mathbb{E}[h(X_1,\dots,X_k)]$, where $X_1$, $X_2$, $\dots$, $X_k$ are i.i.d. data from some distribution and $h$ is a permutation-invariant function. Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied in a black-box manner to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even $\Theta(1/n)$ rather than $O(1/n^2)$ in degenerate settings. To remedy this, we propose a new thresholding-based approach that reweights different subsets of the data using _local Hájek projections_. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics.","['Differential Privacy', 'Statistics', 'Mean Estimation']",[],"['Kamalika Chaudhuri', 'Po-Ling Loh', 'Shourya Pandey', 'Purnamrita Sarkar']","['UC San Diego, University of California, San Diego', 'University of Cambridge', 'Computer Science, The University of Texas at Austin', 'University of Texas, Austin']",
https://openreview.net/forum?id=TwrnhZfD6a,Security,Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning,"In many Deep Reinforcement Learning (RL) problems, decisions in a trained policy vary in significance for the expected safety and performance of the policy. Since RL policies are very complex, testing efforts should concentrate on states in which the agent's decisions have the highest impact on the expected outcome. In this paper, we propose a novel model-based method to rigorously compute a ranking of state importance across the entire state space. We then focus our testing efforts on the highest-ranked states. In this paper, we focus on testing for safety. However, the proposed methods can be easily adapted to test for performance. In each iteration, our testing framework computes optimistic and pessimistic safety estimates. These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space. Our approach divides the state space into safe and unsafe regions upon convergence, providing clear insights into the policy's weaknesses. Two important properties characterize our approach. (1) Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety. (2) Guaranteed Safety: Our approach can provide formal verification guarantees over the entire state space by sampling only a fraction of the policy. Any safety properties assured by the pessimistic estimate are formally proven to hold for the policy. We provide a detailed evaluation of our framework on several examples, showing that our method discovers unsafe policy behavior with low testing effort.","['Policy Verification', 'Probabilistic Model Checking', 'Deep Reinforcement Learning']",[],"['Stefan Pranger', 'Hana Chockler', 'Martin Tappler', 'Bettina Könighofer']","['Technische Universität Graz', ""Informatics, King's College London, University of London"", 'Technische Universität Wien', 'Technische Universität Graz']",
https://openreview.net/forum?id=rTxCIWsfsD,Security,Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions,"Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.","['Robust Offline Reinforcement Learning', 'Variational Bayesian Inference', 'Diverse Data Corruptions', 'Uncertainty']",[],"['Rui Yang', 'Jie Wang', 'Guoping Wu', 'Bin Li']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'School of Information Science and Technology, University of Science and Technology of China']",
https://openreview.net/forum?id=taI8M5DiXj,Fairness & Bias,When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding,"We consider the task of learning how to act in collaboration with a human expert based on observational data. The task is motivated by high-stake scenarios such as healthcare and welfare where algorithmic action recommendations are made to a human expert, opening the option of deferring making a recommendation in cases where the human might act better on their own.     This task is especially challenging when dealing with observational data, as using such data runs the risk of hidden confounders whose existence can lead to biased and harmful policies. However, unlike standard policy learning, the presence of a human expert can mitigate some of these risks. We build on the work of Mozannar and Sontag (2020) on consistent surrogate loss for learning with the option of deferral to an expert, where they solve a cost-sensitive supervised classification problem. Since we are solving a causal problem, where labels don’t exist, we use a causal model to learn costs which are robust to a bounded degree of hidden confounding.     We prove that our approach can take advantage of the strengths of both the model and the expert to obtain a better policy than either. We demonstrate our results by conducting experiments on synthetic and semi-synthetic data and show the advantages of our method compared to baselines.","['policy learning', 'causal inference', 'sensitivity analysis', 'human-algorithm collaboration']",[],"['Marah Ghoummaid', 'Uri Shalit']","['Technion - Israel Institute of Technology, Technion', 'Technion']",
https://openreview.net/forum?id=ZJBBeyEAyX,Security,OSLO: One-Shot Label-Only Membership Inference Attacks,"We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs), which accurately infer a given sample's membership in a target model's training set with high precision using just a single query, where the target model only returns the predicted hard label.    This is in contrast to state-of-the-art label-only attacks which require $\sim6000$ queries, yet get attack precisions lower than OSLO's.   OSLO leverages transfer-based black-box adversarial attacks. The core idea is that a member sample exhibits more resistance to adversarial perturbations than  a non-member. We compare OSLO against state-of-the-art label-only attacks and demonstrate that, despite requiring only one query, our method significantly outperforms previous attacks in terms of precision and true positive rate (TPR) under the same false positive rates (FPR). For example, compared to previous label-only MIAs, OSLO achieves a TPR that is at least 7$\times$ higher under a 1\% FPR and at least 22$\times$ higher under a 0.1\% FPR on CIFAR100 for a ResNet18 model. We evaluated multiple defense mechanisms against OSLO.","['membership inference attack', 'privacy', 'leakage']",[],"['Yuefeng Peng', 'Jaechul Roh', 'Subhransu Maji', 'Amir Houmansadr']","['', 'Computer Science, University of Massachusetts at Amherst', 'Computer Science, University of Massachusetts at Amherst', 'CS, University of Massachusetts, Amherst']",
https://openreview.net/forum?id=YOBGdVaYTS,Fairness & Bias,Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing,"Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate. In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks.  We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential (reasoning-based) solutions, which capture the underlying compositional primitives, or symmetric (memory-based) solutions, which simply memorize mappings without understanding the compositional structure. By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types. We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors. We validate our conclusions on various real-world datasets. Our findings provide valuable insights into the role of initialization scale in tuning the reasoning and memorizing ability and we propose the initialization rate $\gamma$ to be a convenient tunable hyper-parameter in common deep learning frameworks, where $1/d_{\mathrm{in}}^\gamma$ is the standard deviation of parameters of the layer with $d_{\mathrm{in}}$ input neurons.","['parameter initialization', 'transformer', 'compositional task', 'reasoning', 'memorizing']",[],"['Zhongwang Zhang', 'Pengxiao Lin', 'Zhiwei Wang', 'Yaoyu Zhang', 'Zhi-Qin John Xu']","['School of mathematical sciences, Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=v4dXL3LsGX,Fairness & Bias,Learning to Cooperate with Humans using Generative Agents,"Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world.  We show \emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method---Generative Agent Modeling for Multi-agent Adaptation (GAMMA)---on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.","['multi-agent reinforcement learning', 'human-AI cooperation']",[],"['Yancheng Liang', 'Daphne Chen', 'Abhishek Gupta', 'Simon Shaolei Du', 'Natasha Jaques']","['Department of Computer Science, University of Washington', 'Carnegie Mellon University', 'University of Washington', 'University of Washington', 'Computer Science and Engineering, University of Washington']",
https://openreview.net/forum?id=DpByqSbdhI,Security,UniMTS: Unified Pre-training for Motion Time Series,"Motion time series collected from low-power, always-on mobile and wearable devices such as smartphones and smartwatches offer significant insights into human behavioral patterns, with wide applications in healthcare, automation, IoT, and AR/XR. However, given security and privacy concerns, building large-scale motion time series datasets remains difficult, hindering the development of pre-trained models for human activity analysis. Typically, existing models are trained and tested on the same dataset, leading to poor generalizability across variations in device location, device mounting orientation, and human activity type. In this paper, we introduce UniMTS, the first unified pre-training procedure for motion time series that generalizes across diverse device latent factors and activities. Specifically, we employ a contrastive learning framework that aligns motion time series with text descriptions enriched by large language models. This helps the model learn the semantics of time series to generalize across activities. Given the absence of large-scale motion time series data, we derive and synthesize time series from existing motion skeleton data with all-joint coverage. We use spatio-temporal graph networks to capture the relationships across joints for generalization across different device locations. We further design rotation-invariant augmentation to make the model agnostic to changes in device mounting orientations. Our model shows exceptional generalizability across 18 motion time series classification benchmark datasets, outperforming the best baselines by 340% in the zero-shot setting, 16.3% in the few-shot setting, and 9.2% in the full-shot setting.","['motion time series classification', 'pre-training', 'contrastive learning', 'physics-based simulation', 'human activity recognition']",[],"['Xiyuan Zhang', 'Diyan Teng', 'Ranak Roy Chowdhury', 'Shuheng Li', 'Dezhi Hong', 'Rajesh K. Gupta', 'Jingbo Shang']","['AWS', 'Qualcomm Inc, QualComm', 'Amazon', 'University of California, San Diego', 'Amazon', 'University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=UuiZEOVtHx,Security,Safe and Efficient: A Primal-Dual Method for Offline Convex CMDPs under Partial Data Coverage,"Offline safe reinforcement learning (RL) aims to find an optimal policy using a pre-collected dataset when data collection is impractical or risky. We propose a novel linear programming (LP) based primal-dual algorithm for convex MDPs that incorporates ``uncertainty'' parameters to improve data efficiency while requiring only partial data coverage assumption. Our theoretical results achieve a sample complexity of $\mathcal{O}(1/(1-\gamma)\sqrt{n})$ under general function approximation, improving the current state-of-the-art by a factor of $1/(1-\gamma)$, where $n$ is the number of data samples in an offline dataset, and $\gamma$ is the discount factor. The numerical experiments validate our theoretical findings, demonstrating the practical efficacy of our approach in achieving improved safety and learning efficiency in safe offline settings.","['safe reinforcement learning', 'convex MDPs', 'offline', 'sample efficient.']",[],"['Haobo Zhang', 'Xiyue Peng', 'Honghao Wei', 'Xin Liu']","['ShanghaiTech University', 'ShanghaiTech University', 'Washington State University', 'ShanghaiTech University']",
https://openreview.net/forum?id=4czwwExZKQ,Fairness & Bias,ActSort: An active-learning accelerated cell sorting algorithm for large-scale calcium imaging datasets,"Recent advances in calcium imaging enable simultaneous recordings of up to a million neurons in behaving animals, producing datasets of unprecedented scales. Although individual neurons and their activity traces can be extracted from these videos with automated algorithms, the results often require human curation to remove false positives, a laborious process called \emph{cell sorting}. To address this challenge, we introduce ActSort, an active-learning algorithm for sorting large-scale datasets that integrates features engineered by domain experts together with data formats with minimal memory requirements. By strategically bringing outlier cell candidates near the decision boundary up for annotation, ActSort reduces human labor to about 1–3\% of cell candidates and improves curation accuracy by mitigating annotator bias. To facilitate the algorithm's widespread adoption among experimental neuroscientists, we created a user-friendly software and conducted a first-of-its-kind benchmarking study involving about 160,000 annotations.  Our tests validated ActSort's performance across different experimental conditions and datasets from multiple animals.  Overall, ActSort addresses a crucial bottleneck in processing large-scale calcium videos of neural activity and thereby facilitates systems neuroscience experiments at previously inaccessible scales. (\url{https://github.com/schnitzer-lab/ActSort-public})","['neuroscience', 'active learning', 'calcium imaging', 'cell sorting']",[],"['Yiqi Jiang', 'Hakki Orhun Akengin', 'Ji Zhou', 'Mehmet Anil Aslihak', 'Yang Li', 'Radoslaw Chrapkiewicz', 'Oscar Hernandez', 'Sadegh Ebrahimi', 'Omar Jaidar', 'Yanping Zhang', 'Hakan Inan', 'Christopher Miranda', 'Fatih Dinc', 'Marta Blanco-Pozo', 'Mark Schnitzer']","['Stanford University', 'Computer Science, Stanford University', 'Mechanical Engineering, Johns Hopkins University', 'Electrical and Electronics Engineering, Middle East Technical University', 'Biology Department, Stanford University', 'Cracking the Neural Code, Stanford University', 'Stanford University', 'Stanford University', 'Neurosurgery, Stanford University', 'Biology/CNC, Stanford University', 'Facebook', 'Stanford University', 'University of California, Santa Barbara', 'Biology, Stanford University', 'Stanford University']",
https://openreview.net/forum?id=DpP5F3UfKw,Transparency & Explainability,Divergences between Language Models and Human Brains,"Do machines and humans process language in similar ways? Recent research has hinted at the affirmative, showing that human neural activity can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using an LLM-based data-driven approach, we identify two domains that LMs do not capture well: social/emotional intelligence and physical commonsense. We validate these findings with human behavioral experiments and hypothesize that the gap is due to insufficient representations of social/emotional and physical knowledge in LMs. Our results show that fine-tuning LMs on these domains can improve their alignment with human brain responses.","['Natural Language Processing', 'NLP', 'Brain Imaging', 'Neuroimaging', 'Magnetoencephalography', 'MEG', 'Neuroscience', 'Cognitive Science', 'Interpretability', 'Deep Learning']",[],"['Yuchen Zhou', 'Emmy Liu', 'Graham Neubig', 'Michael J. Tarr', 'Leila Wehbe']","['Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'Psychology/Neuroscience, Carnegie Mellon University', 'Carnegie Mellon University']",
https://openreview.net/forum?id=YIB7REL8UC,Transparency & Explainability,Transformers Represent Belief State Geometry in their Residual Stream,"What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data- generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on. Our work provides a general framework connecting the structure of training data to the geometric structure of activations inside transformers.","['Interpretability', 'Computational Mechanics', 'Belief State', 'Features', 'Representation']",[],"['Adam Shai', 'Lucas Teixeira', 'Alexander Gietelink Oldenziel', 'Sarah Marzen', 'Paul M. Riechers']","['Stanford University', 'PIBBSS', 'Computer Science, University College London', 'W. M. Keck Science Department', 'Simplex']",
https://openreview.net/forum?id=6A29LUZhfv,Fairness & Bias,MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures,"Evaluating large language models (LLMs) is challenging. Traditional ground-truth- based benchmarks fail to capture the comprehensiveness and nuance of real-world queries, while LLM-as-judge benchmarks suffer from grading biases and limited query quantity. Both of them may also become contaminated over time. User- facing evaluation, such as Chatbot Arena, provides reliable signals but is costly and slow. In this work, we propose MixEval, a new paradigm for establishing efficient, gold-standard LLM evaluation by strategically mixing off-the-shelf bench- marks. It bridges (1) comprehensive and well-distributed real-world user queries and (2) efficient and fairly-graded ground-truth-based benchmarks, by matching queries mined from the web with similar queries from existing benchmarks. Based on MixEval, we further build MixEval-Hard, which offers more room for model improvement. Our benchmarks’ advantages lie in (1) a 0.96 model ranking correlation with Chatbot Arena arising from the highly impartial query distribution and grading mechanism, (2) fast, cheap, and reproducible execution (6% of the time and cost of MMLU), and (3) dynamic evaluation enabled by the rapid and stable data update pipeline. We provide extensive meta-evaluation and analysis for our and existing LLM benchmarks to deepen the community’s understanding of LLM evaluation and guide future research directions.","['LLM Evaluation', 'Approximating Human Preference', 'Dynamic Benchmarking', 'Benchmark Mixture', 'Web Query Detection']",[],"['Jinjie Ni', 'Fuzhao Xue', 'Xiang Yue', 'Yuntian Deng', 'Mahir Shah', 'Kabir Jain', 'Graham Neubig', 'Yang You']","['School of Computing, National University of Singapore', 'Computer Science , National University of Singapore', '', 'University of Waterloo', 'National University of Singapore', 'National University of Singapore', 'Carnegie Mellon University', 'Computer Science, National University of Singapore']",
https://openreview.net/forum?id=h6o6qXLmHZ,Transparency & Explainability,Dissect Black Box: Interpreting for Rule-Based Explanations in Unsupervised Anomaly Detection,"In high-stakes sectors such as network security, IoT security, accurately distinguishing between normal and anomalous data is critical due to the significant implications for operational success and safety in decision-making. The complexity is exacerbated by the presence of unlabeled data and the opaque nature of black-box anomaly detection models, which obscure the rationale behind their predictions. In this paper, we present a novel method to interpret the decision-making processes of these models, which are essential for detecting malicious activities without labeled attack data. We put forward the Segmentation Clustering Decision Tree (SCD-Tree), designed to dissect and understand the structure of normal data distributions. The SCD-Tree integrates predictions from the anomaly detection model into its splitting criteria, enhancing the clustering process with the model's insights into anomalies. To further refine these segments,  the Gaussian Boundary Delineation (GBD) algorithm is employed to define boundaries within each segmented distribution, effectively delineating normal from anomalous data points. At this point, this approach addresses the curse of dimensionality by segmenting high-dimensional data and ensures resilience to data drift and perturbations through flexible boundary fitting. We transform the intricate operations of anomaly detection into an interpretable rule's format, constructing a comprehensive set of rules for understanding.  Our method's evaluation on diverse datasets and models demonstrates superior explanation accuracy, fidelity, and robustness over existing method, proving its efficacy in environments where interpretability is paramount.","['Machine Learning', 'Anomaly Detection', 'Rule Extraction']",[],"['Yu Zhang', 'Ruoyu Li', 'Nengwu Wu', 'Qing Li', 'Xinhan Lin', 'Yang Hu', 'Tao Li', 'Yong Jiang']","['Tsinghua University, Tsinghua University', 'Shenzhen University', 'Hunan University of Science and Technology', 'Peng Cheng Laboratory', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University, Tsinghua University', 'College of Computer Science and Engineering, Hunan University of Science and Technology', 'Data and Information Science, Tsinghua University']",
https://openreview.net/forum?id=m0jZUvlKl7,Transparency & Explainability,AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties,"Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability. We leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection. Given an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version $\textit{should have looked like}$. A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations. We demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro.","['anomaly detection', 'anomaly explanation', 'anomaly repair', 'diffusion model']",[],"['Xiayan Ji', 'Anton Xue', 'Eric Wong', 'Oleg Sokolsky', 'Insup Lee']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']",
https://openreview.net/forum?id=4NQ24cHnOi,Privacy & Data Governance,"Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust","We give the first polynomial-time, differentially node-private, and robust algorithm for estimating the edge density of Erdős-Rényi random graphs and their generalization, inhomogeneous random graphs. We further prove information-theoretical lower bounds, showing that the error rate of our algorithm is optimal up to logarithmic factors. Previous algorithms incur either exponential running time or suboptimal error rates.  Two key ingredients of our algorithm are (1) a new sum-of-squares algorithm for robust edge density estimation, and (2) the reduction from privacy to robustness based on sum-of-squares exponential mechanisms due to Hopkins et al. (STOC 2023).","['differential privacy', 'robustness', 'random graph', 'sum of squares', 'average-case complexity']",[],"['Hongjie Chen', 'Jingqiu Ding', 'Yiding Hua', 'David Steurer']","['Department of Computer Science, ETHZ - ETH Zurich', 'ETHZ - ETH Zurich', 'ETH Zurich', '']",
https://openreview.net/forum?id=uLGyoBn7hm,Transparency & Explainability,Disentangled Representation Learning in Non-Markovian Causal Systems,"Considering various data modalities, such as images, videos, and text, humans perform causal reasoning using high-level causal variables, as opposed to operating at the low, pixel level from which the data comes.  In practice, most causal reasoning methods assume that the data is described as granular as the underlying causal generative factors, which is often violated in various AI tasks.  This mismatch translates into a lack of guarantees in various tasks such as generative modeling, decision-making, fairness, and generalizability, to cite a few.  In this paper, we acknowledge this issue and study the problem of causal disentangled representation learning from a combination of data gathered from various heterogeneous domains and assumptions in the form of a latent causal graph. To the best of our knowledge, the proposed work is the first to consider i) non-Markovian causal settings, where there may be unobserved confounding, ii) arbitrary distributions that arise from multiple domains, and iii) a relaxed version of disentanglement. Specifically, we introduce graphical criteria that allow for disentanglement under various conditions. Building on these results, we develop an algorithm that returns a causal disentanglement map, highlighting which latent variables can be disentangled given the combination of data and assumptions. The theory is corroborated by experiments.","['causal representation learning', 'disentanglement', 'nonlinear ICA']",[],"['Adam Li', 'Yushu Pan', 'Elias Bareinboim']","['Columbia University', 'Columbia University', 'Columbia University']",
https://openreview.net/forum?id=ZbjJE6Nq5k,Fairness & Bias,Normalization and effective learning rates in reinforcement learning,"Normalization layers have recently experienced a renaissance in the deep reinforcement learning and continual learning literature, with several works highlighting diverse benefits such as improving loss landscape conditioning and combatting overestimation bias. However, normalization brings with it a subtle but important side effect: an equivalence between growth in the norm of the network parameters and decay in the effective learning rate. This becomes problematic in continual learning settings, where the resulting learning rate schedule may decay to near zero too quickly relative to the timescale of the learning problem. We propose to make the learning rate schedule explicit with a simple re-parameterization which we call  Normalize-and-Project (NaP), which couples the insertion of normalization layers with weight projection, ensuring that the effective learning rate remains constant throughout training. This technique reveals itself as a powerful analytical tool to better understand learning rate schedules in deep reinforcement learning, and as a means of improving robustness to nonstationarity in synthetic plasticity loss benchmarks along with both the single-task and sequential variants of the Arcade Learning Environment. We also show that our approach can be easily applied to popular architectures such as ResNets and transformers while recovering and in some cases even slightly improving the performance of the base model in common stationary benchmarks.","['continual learning', 'reinforcement learning', 'optimization', 'plasticity']",[],"['Clare Lyle', 'Zeyu Zheng', 'Khimya Khetarpal', 'James Martens', 'Hado van Hasselt', 'Razvan Pascanu', 'Will Dabney']","['Google DeepMind', 'DeepMind', 'Google', 'DeepMind', 'DeepMind', 'Mila - Quebec Artificial Intelligence Institute', 'Google DeepMind']",
https://openreview.net/forum?id=hH4bPkOhhh,Transparency & Explainability,Identifying Selections for Unsupervised Subtask Discovery,"When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a *selection* mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at this [*link*](https://anonymous.4open.science/r/Identifying\_Selections\_for\_Unsupervised\_Subtask\_Discovery/README.md).","['Imitation Learning', 'Causality', 'Selection']",[],"['Yiwen Qiu', 'Yujia Zheng', 'Kun Zhang']","['CMU, Carnegie Mellon University', 'Carnegie Mellon University', 'Mohamed bin Zayed University of Artificial Intelligence']",
https://openreview.net/forum?id=FOfU3qhcIG,Transparency & Explainability,TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks,"While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs via context optimization. We introduce TuneTables, a parameter-efficient fine-tuning strategy for PFNs that compresses large datasets into a smaller learned context. We conduct extensive experiments on nineteen algorithms over 98 datasets and find that TuneTables achieves the best performance on average, outperforming boosted trees such as CatBoost, while optimizing fewer than 5\% of TabPFN's parameters. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective.","['tabular data', 'prior-data fitted networks']",[],"['Benjamin Feuer', 'Robin Tibor Schirrmeister', 'Valeriia Cherepanova', 'Chinmay Hegde', 'Frank Hutter', 'Micah Goldblum', 'Niv Cohen', 'Colin White']","['Computer Science and Engineering, New York University', 'Medical Center - University of Freiburg', 'Amazon', 'New York University', 'ELLIS Institute Tübingen & University of Freiburg', 'Electrical Engineering, Columbia University', 'Computer Science, New York University', 'Facebook']",
https://openreview.net/forum?id=FqWyzyErVT,Privacy & Data Governance,Federated Transformer: Multi-Party Vertical Federated Learning on Practical Fuzzily Linked Data,"Federated Learning (FL) is an evolving paradigm that enables multiple parties to collaboratively train models without sharing raw data. Among its variants, Vertical Federated Learning (VFL) is particularly relevant in real-world, cross-organizational collaborations, where distinct features of a shared instance group are contributed by different parties. In these scenarios, parties are often linked using fuzzy identifiers, leading to a common practice termed as _multi-party fuzzy VFL_. Existing models generally address either multi-party VFL or fuzzy VFL between two parties. Extending these models to practical multi-party fuzzy VFL typically results in significant performance degradation and increased costs for maintaining privacy. To overcome these limitations, we introduce the _Federated Transformer (FeT)_, a novel framework that supports multi-party VFL with fuzzy identifiers. FeT innovatively encodes these identifiers into data representations and employs a transformer architecture distributed across different parties, incorporating three new techniques to enhance performance. Furthermore, we have developed a multi-party privacy framework for VFL that integrates differential privacy with secure multi-party computation, effectively protecting local representations while minimizing associated utility costs. Our experiments demonstrate that the FeT surpasses the baseline models by up to 46\% in terms of accuracy when scaled to 50 parties. Additionally, in two-party fuzzy VFL settings, FeT also shows improved performance and privacy over cutting-edge VFL models.","['vertical federated learning', 'federated learning', 'transformer', 'record linkage', 'entity alignment', 'differential privacy', 'fuzzy alignment']",[],"['Zhaomin Wu', 'Junyi Hou', 'Yiqun Diao', 'Bingsheng He']","['National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'National University of Singapore']",
https://openreview.net/forum?id=0jld45XGgJ,Fairness & Bias,Neural collapse vs. low-rank bias: Is deep neural collapse really optimal?,"Deep neural networks (DNNs) exhibit a surprising structure in their final layer known as neural collapse (NC), and a growing body of works is currently investigated the propagation of neural collapse to earlier layers of DNNs -- a phenomenon called deep neural collapse (DNC). However, existing theoretical results are restricted to either linear models, the last two layers or binary classification. In contrast, we focus on non-linear models of arbitrary depth in multi-class classification and reveal a surprising qualitative shift. As soon as we go beyond two layers or two classes, DNC stops being optimal for the deep unconstrained features model (DUFM) -- the standard theoretical framework for the analysis of collapse. The main culprit is the low-rank bias of multi-layer regularization schemes. This bias leads to optimal solutions of even lower rank than the neural collapse. We support our theoretical findings with experiments on both DUFM and real data, which show the emergence of the low-rank structure in the solution found by gradient descent.","['neural collapse', 'deep neural collapse', 'unconstrained features model', 'deep unconstrained features model', 'low-rank bias']",[],"['Peter Súkeník', 'Christoph H. Lampert', 'Marco Mondelli']","['Institute of Science and Technology', 'Machine Learning and Computer Vision, Institute of Science and Technology Austria', 'Institute of Science and Technology Austria']",
https://openreview.net/forum?id=mxMvWwyBWe,Transparency & Explainability,Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions,"Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.  We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.","['fMRI', 'Encoding models', 'Neuroscience', 'Language neuroscience', 'Interpretability', 'Large language models', 'Explainability', 'Brain mapping']",[],"['Vinamra Benara', 'Chandan Singh', 'John Xavier Morris', 'Richard Antonello', 'Ion Stoica', 'Alexander Huth', 'Jianfeng Gao']","['University of California, Berkeley', 'Deep Learning Group, Microsoft Research', 'Cornell University', 'University of Texas, Austin', 'EECS, University of California, Berkeley', 'The University of Texas at Austin', 'Microsoft Research, Microsoft Research']",
https://openreview.net/forum?id=0og7nmvDbe,Transparency & Explainability,Confidence Regulation Neurons in Language Models,"Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an \textit{unembedding null space}, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token’s logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution. Finally, we present a detailed case study where entropy neurons actively manage confidence: the setting of induction, i.e. detecting and continuing repeated subsequences.","['LLMs', 'Interpretability', 'Mechanistic Interpretability']",[],"['Alessandro Stolfo', 'Ben Peng Wu', 'Wes Gurnee', 'Yonatan Belinkov', 'Xingyi Song', 'Mrinmaya Sachan', 'Neel Nanda']","['ETHZ - ETH Zurich', 'University of Sheffield', 'Massachusetts Institute of Technology', 'Technion, Technion', 'Computer Science, University of Sheffield', 'Swiss Federal Institute of Technology', 'Google DeepMind']",
https://openreview.net/forum?id=0zfUiSX5si,Fairness & Bias,AdaNovo: Towards Robust \emph{De Novo} Peptide Sequencing in Proteomics against Data Biases,"Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Despite the development of several deep learning methods for predicting amino acid sequences (peptides) responsible for generating the observed mass spectra, training data biases hinder further advancements of \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with Post-Translational Modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in unsatisfactory peptide sequencing performance. Secondly, various noise and missing peaks in mass spectra reduce the reliability of training data (Peptide-Spectrum Matches, PSMs). To address these challenges, we propose AdaNovo, a novel and domain knowledge-inspired framework that calculates Conditional Mutual Information (CMI) between the mass spectra and amino acids or peptides, using CMI for robust training against above biases. Extensive experiments indicate that AdaNovo outperforms previous competitors on the widely-used 9-species benchmark, meanwhile yielding 3.6\% - 9.4\% improvements in PTMs identification. The supplements contain the code.","['De Novo Peptide sequencing', 'Proteomics', 'Mass Spectrum']",[],"['Jun Xia', 'Shaorong Chen', 'Jingbo Zhou', 'Xiaojun Shan', 'Wenjie Du', 'Zhangyang Gao', 'Cheng Tan', 'Bozhen Hu', 'Jiangbin Zheng', 'Stan Z. Li']","['School of Engineering, Westlake University, China', 'Westlake University', 'School of Engineering, Westlake University', 'University of California, San Diego', 'University of Science and Technology of China, University of Science and Technology of China', 'School of Engineering, Westlake University, China', 'Zhejiang University & Westlake University', 'Engineering, Westlake University', 'Westlake University', 'AI Department, Westlake University']",
https://openreview.net/forum?id=9Nsa4lVZeD,Security,Stability and Generalization of Adversarial Training for Shallow Neural Networks with Smooth Activation,"Adversarial training has emerged as a popular approach for training models that are robust to inference-time adversarial attacks. However, our theoretical understanding of why and when it works remains limited. Prior work has offered generalization analysis of adversarial training, but they are either restricted to the Neural Tangent Kernel (NTK) regime or they make restrictive assumptions about data such as (noisy) linear separability or robust realizability. In this work, we study the stability and generalization of adversarial training for two-layer networks **without any data distribution assumptions** and **beyond the NTK regime**. Our findings suggest that for networks with *any given initialization* and *sufficiently large width*, the generalization bound can be effectively controlled via early stopping. We further improve the generalization bound by leveraging smoothing using Moreau’s envelope.","['Stability', 'adversarial training', 'neural networks', 'optimization and generalization guarantees', 'Moreau envelope', 'convexity', 'smoothness']",[],"['Kaibo Zhang', 'Yunjuan Wang', 'Raman Arora']","['Department of Computer Science, Whiting School of Engineering', 'Computer Science Department, Johns Hopkins University', 'Computer Science, Johns Hopkins University']",
https://openreview.net/forum?id=w2L3Ll1jbV,Security,Adversarially Robust Multi-task Representation Learning,"We study adversarially robust transfer learning, wherein, given labeled data on multiple (source) tasks, the goal is to train a model with small robust error on a previously unseen (target) task. In particular, we consider a multi-task representation learning (MTRL) setting, i.e., we assume that the source and target tasks admit a simple (linear) predictor on top of a shared representation (e.g., the final hidden layer of a  deep neural network). In this general setting, we provide rates on~the excess adversarial (transfer) risk for Lipschitz losses and smooth nonnegative losses. These rates show that learning a representation using adversarial training on diverse tasks  helps protect against inference-time attacks in data-scarce environments. Additionally, we provide novel rates for the single-task setting.","['Learning Theory', 'Multi-task and Transfer Learning', 'Adversarial Robustness']",[],"['Austin Watkins', 'Thanh Nguyen-Tang', 'Enayat Ullah', 'Raman Arora']","['Booz Allen Hamilton', 'Computer Science, Johns Hopkins University', 'Johns Hopkins University', 'Computer Science, Johns Hopkins University']",
https://openreview.net/forum?id=j14wStqZni,Privacy & Data Governance,Public-data Assisted Private Stochastic Optimization: Power and Limitations,"We study the limits and capability of public-data  assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has excess risk $\tilde{\Omega}\big(\min(\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} ) \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of public samples, ${n_{\text{priv}}}$ is the number of private samples, and $n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP supervised learning with \textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private supervised learning. For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\tilde{O}({n_{\text{priv}}}\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\tilde{O}\big(\frac{1}{\sqrt{{n_{\text{priv}}}}} + \frac{1}{\sqrt{{n_{\text{priv}}}\epsilon}}\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite \textit{fat-shattering dimension} with applications to neural networks and non-Euclidean geometries.","['Differential Privacy', 'Public Data', 'Stochastic Optimization', 'Generalized Linear Model']",[],"['Enayat Ullah', 'Michael Menart', 'Raef Bassily', 'Cristóbal A Guzmán', 'Raman Arora']","['Johns Hopkins University', 'Computer Science, University of Toronto', 'Computer Science and Engineering, Ohio State University', 'Pontificia Universidad Catolica de Chile', 'Computer Science, Johns Hopkins University']",
https://openreview.net/forum?id=181llen2gw,Fairness & Bias,A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks,"Recent advancements in Vision-Language Models (VLMs) have enabled complex multimodal tasks by processing text and image data simultaneously, significantly enhancing the field of artificial intelligence. However, these models often exhibit biases that can skew outputs towards societal stereotypes, thus necessitating debiasing strategies. Existing debiasing methods focus narrowly on specific modalities or tasks, and require extensive retraining. To address these limitations, this paper introduces Selective Feature Imputation for Debiasing (SFID), a novel methodology that integrates feature pruning and low confidence imputation (LCI) to effectively reduce biases in VLMs. SFID is versatile, maintaining the semantic integrity of outputs and costly effective by eliminating the need for retraining. Our experimental results demonstrate SFID's effectiveness across various VLMs tasks including zero-shot classification, text-to-image retrieval, image captioning, and text-to-image generation, by significantly reducing gender biases without compromising performance. This approach not only enhances the fairness of VLMs applications but also preserves their efficiency and utility across diverse scenarios.","['Vision-Language Model', 'Fairness', 'Debias']",[],"['Hoin Jung', 'Taeuk Jang', 'Xiaoqian Wang']","['Electrical and Computer Engineering, Purdue University', 'Amazon', 'Purdue University']",
https://openreview.net/forum?id=ohi00YhT3T,Transparency & Explainability,Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction,"Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a *Vision Transformer 3D*. This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.","['Neural decoding', 'Mind Reader', 'Visual Reconstruction', 'Multimodal Large Model', 'Concept Localization']",[],"['Guobin Shen', 'Dongcheng Zhao', 'Xiang He', 'Linghao Feng', 'Yiting Dong', 'Jihang Wang', 'Qian Zhang', 'Yi Zeng']","['BrainCog Lab, Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences, Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",
https://openreview.net/forum?id=BZLdXBjB8O,Transparency & Explainability,CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense,"Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39\% (+4.01\%) on CIFAR-10, 56.25\% (+3.13\%) on CIFAR-100, and 82.62\% (+4.93\%) on GTSRB (German Traffic Sign Recognition Benchmark). The code is available athttps://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff.","['Adversarial Defense', 'Diffusion Model', 'Causal']",[],"['Mingkun Zhang', 'Keping Bi', 'Wei Chen', 'Quanrun Chen', 'Jiafeng Guo', 'Xueqi Cheng']","[', Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Chinese Academy of Sciences', 'School of Statistics, University of International Business and Economics', 'Institute of Computing Technolgy, Chinese Academy of Sciences', 'network data science and technology, Institute of Computing Technology, Chinese Academy']",
https://openreview.net/forum?id=xymhWyiZOp,Security,On the Use of Anchoring for Training Vision Models,"Anchoring is a recent, architecture-agnostic principle for training deep neural networks that has been shown to significantly improve uncertainty estimation, calibration, and extrapolation capabilities. In this paper, we systematically explore anchoring as a general protocol for training vision models, providing fundamental insights into its training and inference processes and their implications for generalization and safety. Despite its promise, we identify a critical problem in anchored training that can lead to an increased risk of learning undesirable shortcuts, thereby limiting its generalization capabilities. To address this, we introduce a new anchored training protocol that employs a simple regularizer to mitigate this issue and significantly enhances generalization. We empirically evaluate our proposed approach across datasets and architectures of varying scales and complexities, demonstrating substantial performance gains in generalization and safety metrics compared to the standard training protocol. The open-source code is available at https://software.llnl.gov/anchoring.","['Anomaly Detection', 'OOD Generalization', 'ML Safety', 'Anchoring', 'Deep Neural Networks']",[],"['Vivek Narayanaswamy', 'Kowshik Thopalli', 'Rushil Anirudh', 'Yamen Mubarka', 'Wesam A. Sakla', 'Jayaraman J. Thiagarajan']","['Lawrence Livermore National Labs', 'Arizona State University', 'Amazon', 'Lawrence Livermore National Labs', 'Lawrence Livermore National Laboratory', 'SIML, Apple']",
https://openreview.net/forum?id=Y841BRW9rY,Security,AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases,"LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, resilience, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of $\ge$ 80% with minimal impact on benign performance ($\le$ 1%) with a poison rate < 0.1%. The code and data is available at https://github.com/BillChan226/AgentPoison.","['LLM Agent', 'LLM Red-teaming', 'Retrieval-Augmented Generation', 'Backdoor Poisoning', 'Trustworthy LLM']",[],"['Zhaorun Chen', 'Zhen Xiang', 'Chaowei Xiao', 'Dawn Song', 'Bo Li']","['Computer Science, University of Chicago', 'University of Georgia', 'University of Wisconsin - Madison', 'University of California Berkeley', 'CS, University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=ZeihWodDVh,Security,PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics,"Train-time data poisoning attacks threaten machine learning models by introducing adversarial examples during training, leading to misclassification. Current defense methods often reduce generalization performance, are attack-specific, and impose significant training overhead. To address this, we introduce a set of universal data purification methods using a stochastic transform, $\Psi(x)$, realized via iterative Langevin dynamics of Energy-Based Models (EBMs), Denoising Diffusion Probabilistic Models (DDPMs), or both. These approaches purify poisoned data with minimal impact on classifier generalization. Our specially trained EBMs and DDPMs provide state-of-the-art defense against various attacks (including Narcissus, Bullseye Polytope, Gradient Matching) on CIFAR-10, Tiny-ImageNet, and CINIC-10, without needing attack or classifier-specific information. We discuss performance trade-offs and show that our methods remain highly effective even with poisoned or distributionally shifted generative model training data.","['Energy-Based Models', 'Diffusion', 'Langevin dynamics', 'Poisons', 'robustness', 'defense', 'Backdoor']",[],"['Omead Pooladzandi', 'Sunay Gajanan Bhat', 'Jeffrey Jiang', 'Alexander Branch', 'Gregory Pottie']","['Electrical Engineering , Department of Electrical Engineering, California Institute of Technology', 'University of California, Los Angeles', 'Electrical and Computer Engineering, University of California, Los Angeles', 'ECE, University of California, Los Angeles', '']",
https://openreview.net/forum?id=pX71TM2MLh,Security,Data Free Backdoor Attacks,"Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class.  Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture. As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes.  In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture.  Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor.  Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions.  Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100\% attack success rates, and 3) bypasses six existing state-of-the-art defenses.  Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss. We will release our code upon paper acceptance.",['Data free backdoor attacks'],[],"['Bochuan Cao', 'Jinyuan Jia', 'Chuxuan Hu', 'Wenbo Guo', 'Zhen Xiang', 'Jinghui Chen', 'Bo Li', 'Dawn Song']","['Pennsylvania State University', 'College of IST, Pennsylvania State University', '', 'University of California, Santa Barbara', 'University of Georgia', 'Pennsylvania State University', 'CS, University of Illinois, Urbana Champaign', 'University of California Berkeley']",
https://openreview.net/forum?id=OtvNLTWYww,Fairness & Bias,A Theoretical Understanding of Self-Correction through In-context Alignment,"Going beyond mimicking limited human experiences, recent studies show initial evidence that, like humans, large language models (LLMs) are capable of improving their abilities purely by self-correction, i.e., correcting previous responses through self-examination, as seen in models like OpenAI o1. Nevertheless, little is known about how such capabilities arise. In this work, based on a simplified setup akin to an alignment task, we theoretically analyze self-correction from an in-context learning perspective, showing that when LLMs give relatively accurate self-examinations as rewards, they are capable of refining responses in an in-context way. Notably, going beyond previous theories on over-simplified linear transformers, our theoretical construction underpins the roles of several key designs of realistic transformers for self-correction: softmax attention, multi-head attention, and the MLP block. We validate these findings extensively on synthetic datasets. Inspired by these findings, we propose a simple self-correction strategy, Checking as Context (CaC), which finds novel applications in alleviating social bias and defending against LLM jailbreaks. We believe that these findings will inspire further research on understanding, exploiting, and enhancing self-correction for building better foundation models. Code is at https://github.com/yifeiwang77/Self-Correction.","['Self-correction', 'Theory', 'In-context Learning', 'Transformer', 'Language Model', 'Alignment']",[],"['Yifei Wang', 'Yuyang Wu', 'Zeming Wei', 'Stefanie Jegelka', 'Yisen Wang']","['Massachusetts Institute of Technology', 'Peking University', 'School of mathematical Science, Peking University', 'Computer Science, Technische Universität München', 'Peking University']",
https://openreview.net/forum?id=jdCMwF06c6,Security,LT-Defense: Searching-free Backdoor Defense via Exploiting the Long-tailed Effect,"Language models have shown vulnerability against backdoor attacks, threatening the security of services based on them. To mitigate the threat, existing solutions attempted to search for backdoor triggers, which can be time-consuming when handling a large search space. Looking into the attack process, we observe that poisoned data will create a long-tailed effect in the victim model, causing the decision boundary to shift towards the attack targets. Inspired by this observation, we introduce LT-Defense, the first searching-free backdoor defense via exploiting the long-tailed effect. Specifically, LT-Defense employs a small set of clean examples and two metrics to distinguish backdoor-related features in the target model. Upon detecting a backdoor model, LT-Defense additionally provides test-time backdoor freezing and attack target prediction. Extensive experiments demonstrate the effectiveness of LT-Defense in both detection accuracy and efficiency, e.g., in task-agnostic scenarios, LT-Defense achieves 98% accuracy across 1440 models with less than 1% of the time cost of state-of-the-art solutions.","['backdoor defense', 'natural language processing', 'deep long-tailed learning']",[],"['Yixiao Xu', 'Binxing Fang', 'Mohan Li', 'Keke Tang', 'Zhihong Tian']","['Beijing University of Posts and Telecommunications', 'Harbin Institute of Technology', 'Guangzhou University', 'Cyberspace Institute of Advanced Technology, Guangzhou University', 'CIAT, Guangzhou University']",
https://openreview.net/forum?id=FCsEvaMorw,Security,Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts,"As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.","['open-endedness', 'adversarial robustness', 'safety']",[],"['Mikayel Samvelyan', 'Sharath Chandra Raparthy', 'Andrei Lupu', 'Eric Hambro', 'Aram H. Markosyan', 'Manish Bhatt', 'Yuning Mao', 'Minqi Jiang', 'Jack Parker-Holder', 'Jakob Nicolaus Foerster', 'Tim Rocktäschel', 'Roberta Raileanu']","['Google DeepMind', 'Facebook', 'University of Oxford', 'Anthropic', 'Facebook', 'Facebook', 'Meta', 'University College London', 'Google DeepMind', 'University of Oxford, University of Oxford', 'Google DeepMind', 'Facebook']",
https://openreview.net/forum?id=8CBcdDQFDQ,Fairness & Bias,Stratified Prediction-Powered Inference for Effective Hybrid Evaluation of Language Models,"Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data.  PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate---but potentially biased---automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model). In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies. Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for parameters of any dimensionality that is based on stratified sampling. In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than  unstratified  approaches. Specifically, StratPPI  is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data.","['prediction powered inference', 'auto-raters']",[],"['Adam Fisch', 'Joshua Maynez', 'R. Alex Hofer', 'Bhuwan Dhingra', 'Amir Globerson', 'William W. Cohen']","['Massachusetts Institute of Technology', 'Google', 'Google DeepMind', 'Duke University', 'Tel Aviv University', 'Carnegie Mellon University']",
https://openreview.net/forum?id=SiALFXa0NN,Security,Provably Safe Neural Network Controllers via Differential Dynamic Logic,"While neural networks (NNs) have a large potential as autonomous controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs— especially when safety is needed for unbounded time horizons. One reason for this is the intractability of analyzing NNs, ODEs and hybrid systems. To this end, we introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first general approach that allows reusing control theory literature for NNCS verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of differential dynamic logic (dL). Based on a provably safe control envelope in dL, we derive a specification for the NN which is proven with NN verification tools. We show that a proof of the NN’s adherence to the specification is then mirrored by a dL proof on the infinite-time safety of the NNCS.  The NN verification properties resulting from hybrid systems typically contain nonlinear arithmetic over formulas with arbitrary logical structure while efficient NN verification tools merely support linear constraints. To overcome this divide, we present Mosaic: An efficient, sound and complete verification approach for polynomial real arithmetic properties on piece-wise linear NNs. Mosaic partitions complex NN verification queries into simple queries and lifts off-the-shelf linear constraint tools to the nonlinear setting in a completeness-preserving manner by combining approximation with exact reasoning for counterexample regions. In our evaluation we demonstrate the versatility of VerSAILLE and Mosaic: We prove infinite-time safety on the classical Vertical Airborne Collision Avoidance NNCS verification benchmark for some scenarios while (exhaustively) enumerating counterexample regions in unsafe scenarios. We also show that our approach significantly outperforms the State-of-the-Art tools in closed-loop NNV","['Cyber-Physical Systems', 'Neural Network Verification', 'Infinite-Time Horizon Safety', 'Differential Dynamic Logic', 'Safety Verification', 'neural network based control systems', 'Safety', 'Verification']",[],"['Samuel Teuber', 'Stefan Mitsch', 'Andre Platzer']","['Computer Science Department, Karlsruher Institut für Technologie', 'Carnegie Mellon University', 'Karlsruher Institut für Technologie']",
https://openreview.net/forum?id=FbXQrfkvtY,Fairness & Bias,Probing the Decision Boundaries of In-context Learning in Large Language Models,"In-context learning is an emergent paradigm in large language models (LLMs) that enables them to generalize to new tasks and domains by simply prompting these models with a few exemplars without explicit parameter updates. Many attempts have been made to understand in-context learning in LLMs as a function of model scale, pretraining data, and other factors. In this work, we propose a new mechanism to probe and understand in-context learning from the lens of decision boundaries for in-context binary classification. Decision boundaries are straightforward to visualize and provide important information about the qualitative behavior of the inductive biases of standard classifiers. To our surprise, we find that the decision boundaries learned by current LLMs in simple binary classification tasks are often irregularly non-smooth, regardless of task linearity. This paper investigates the factors influencing these decision boundaries and explores methods to enhance their generalizability. We assess various approaches, including training-free and fine-tuning methods for LLMs, the impact of model architecture, and the effectiveness of active prompting techniques for smoothing decision boundaries in a data-efficient manner. Our findings provide a deeper understanding of in-context learning dynamics and offer practical improvements for enhancing robustness and generalizability of in-context learning.",['in-context learning; Large language models; LLM decision boundary'],[],"['Siyan Zhao', 'Tung Nguyen', 'Aditya Grover']","['University of California, Los Angeles', 'Computer Science, University of California, Los Angeles', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=vBxeeH1X4y,Security,2D-OOB: Attributing Data Contribution Through Joint Valuation Framework,"Data valuation has emerged as a powerful framework for quantifying each datum's contribution to the training of a machine learning model. However, it is crucial to recognize that the quality of cells within a single data point can vary greatly in practice. For example, even in the case of an abnormal data point, not all cells are necessarily noisy. The single scalar score assigned by existing data valuation methods blurs the distinction between noisy and clean cells of a data point, making it challenging to interpret the data values. In this paper, we propose 2D-OOB, an out-of-bag estimation framework for jointly determining helpful (or detrimental) samples as well as the particular cells that drive them. Our comprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art performance across multiple use cases while being exponentially faster. Specifically, 2D-OOB shows promising results in detecting and rectifying fine-grained outliers at the cell level, and localizing backdoor triggers in data poisoning attacks.","['Data valuation', 'Cell-level attribution', 'Outlier detection']",[],"['Yifan Sun', 'Jingyan Shen', 'Yongchan Kwon']","['University of Illinois at Urbana-Champaign', 'Columbia University', '']",
https://openreview.net/forum?id=T0glCBw28a,Transparency & Explainability,The ALCHEmist: Automated Labeling 500x CHEaper than LLM Data Annotators,"Large pretrained models can be used as annotators, helping replace or augment crowdworkers and enabling distilling generalist models into smaller specialist models. Unfortunately, this comes at a cost: employing top-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are static and challenging to audit. To address these challenges, we propose a simple alternative: rather than directly querying labels from pretrained models, we task models to generate programs that can produce labels. These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less. Our system, $\textbf{Alchemist}$, obtains comparable to or better performance than large language model-based annotation in a range of tasks for a fraction of the cost: on average, improvements amount to a $\textbf{12.9}$% enhancement while the total labeling costs across all datasets are reduced by a factor of approximately $\textbf{500}\times$.",['Automated Data Labeling; Code Generation; Weak Supervision'],[],"['Tzu-Heng Huang', 'Catherine Cao', 'Vaishnavi Bhargava', 'Frederic Sala']","['Apple', 'University of Wisconsin - Madison', 'Department of Computer Science, University of Wisconsin - Madison', 'Computer Sciences, University of Wisconsin, Madison']",
https://openreview.net/forum?id=U4BC0GrFAz,Transparency & Explainability,Do causal predictors generalize better to new domains?,"We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features.  In addition, we show that recent causal machine learning methods for domain generalization do not perform better in our evaluation than standard predictors trained on the set of causal features. Likewise, causal discovery algorithms either fail to run or select causal variables that perform no better than our selection. Extensive robustness checks confirm that our findings are stable under variable misclassification.","['causality', 'domain generalization', 'tabular data']",[],"['Vivian Yvonne Nastl', 'Moritz Hardt']","['Max Planck Institute for Intelligent Systems', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute']",
https://openreview.net/forum?id=2zWbzx50mH,Transparency & Explainability,Compact Proofs of Model Performance via Mechanistic Interpretability,"We propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving accuracy lower bounds for a small transformer trained on Max-of-$K$, validating proof transferability across 151 random seeds and four values of $K$. We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless errors as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.","['mechanistic interpretability', 'verification', 'proof', 'guarantees', 'interpretability']",[],"['Jason Gross', 'Rajashree Agrawal', 'Thomas Kwa', 'Euan Ong', 'Chun Hei Yip', 'Alex Gibson', 'Soufiane Noubir', 'Lawrence Chan']","['Independent', 'Reed College', 'California Institute of Technology', 'University of Cambridge', 'Mathematics, University of Cambridge', 'Mathematics, University of Cambridge', 'Trinity College, University of Cambridge', 'University of California Berkeley']",
https://openreview.net/forum?id=0dtA21q83C,Fairness & Bias,DeNetDM: Debiasing by Network Depth Modulation,"Neural networks trained on biased datasets tend to inadvertently learn spurious correlations, hindering generalization. We formally prove that (1) samples that exhibit spurious correlations lie on a lower rank manifold relative to the ones that do not; and (2) the depth of a network acts as an implicit regularizer on the rank of the attribute subspace that is encoded in its representations. Leveraging these insights, we present DeNetDM, a novel debiasing method that uses network depth modulation as a way of developing robustness to spurious correlations. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Our method requires no bias annotations or explicit data augmentation while performing on par with approaches that require either or both. We demonstrate that DeNetDM outperforms existing debiasing techniques on both synthetic and real-world datasets by 5\%. The project page is available at https://vssilpa.github.io/denetdm/.","['Trustworthy Machine Learning', 'Debiasing', 'Robustness']",[],"['Silpa Vadakkeeveetil Sreelatha', 'Adarsh Kappiyath', 'Abhra Chaudhuri', 'Anjan Dutta']","['University of Surrey', '', 'Computing Research Group, Fujitsu Research of Europe', 'University of Surrey']",
https://openreview.net/forum?id=yDjojeIWO9,Security,Transferable Adversarial Attacks on SAM and Its Downstream Models,"The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage. This paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM.  In contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model. To enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations. Moreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM. Consequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability. Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models. Code is available at https://github.com/xiasong0501/GRAT.","['Segment anything', 'Security of large fundation model', 'Transfer-based adversarial attack', 'Fine-tuning']",[],"['Song Xia', 'Wenhan Yang', 'Yi Yu', 'Xun Lin', 'Henghui Ding', 'LINGYU DUAN', 'Xudong Jiang']","['Nanyang Technological University', 'Peng Cheng Laboratory', 'Interdisciplinary Programme, Nanyang Technological University', 'School of Computer Science and Engineering, Beihang University', 'Fudan University', 'Peking University', 'School of EEE, Nanyang Technological University']",
https://openreview.net/forum?id=QXkFC7D6p4,Privacy & Data Governance,FedGTST: Boosting Global Transferability of Federated Models via Statistics Tuning,"The performance of Transfer Learning (TL) significantly depends on effective pretraining, which not only requires extensive amounts of data but also substantial computational resources. As a result, in practice, it is challenging to successfully perform TL at the level of individual model developers. Federated Learning (FL) addresses these challenges by enabling collaboration among individual clients through an indirect expansion of the available dataset, distribution of the computation burden across different entities, and privacy-preserving communication mechanisms. Despite several attempts to devise effective transferable FL approaches, several important issues remain unsolved. First, existing methods in this setting primarily focus on optimizing transferability within their local client domains, thereby ignoring transferability over the global learning domain. Second, most approaches focus on analyzing indirect transferability metrics, which does not allow for accurate assessment of the final target loss and extent of transferability. To address these issues, we introduce two important FL features into the model. The first boosts transferability via an exchange protocol between the clients and the server that includes information about cross-client Jacobian (gradient) norms. The second feature promotes an increase of the average of the Jacobians of the clients at the server side, which is subsequently used as a local regularizer that reduces the cross-client Jacobian variance. A rigorous analysis of our transferable federated algorithm, termed FedGTST (Federated Global Transferability via Statistics Tuning), reveals that increasing the averaged Jacobian norm across clients and reducing its variance ensures tight control of the target loss. This insight leads to the first known upper bound on the target loss of transferable federated learning in terms of the source loss and source-target domain discrepancy. Extensive experimental results on datasets including MNIST → MNIST-M and CIFAR10 → SVHN suggest that FedGTST significantly outperforms other relevant baselines, such as FedSR. For example, on the second source-target dataset pair, we improve the accuracy of FedSR by 9.8% and that of FedIIR by 7.6% when the backbone used is LeNet.","['Federated Learning', 'Transfer Learning']",[],"['Evelyn Ma', 'Chao Pan', 'S. Rasoul Etesami', 'Han Zhao', 'Olgica Milenkovic']","['University of Illinois at Urbana-Champaign', 'Google', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=kQ9LgM2JQT,Fairness & Bias,QGFN: Controllable Greediness with Action Values,"Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, consistently biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.","['GFlowNets', 'generative models', 'molecule design']",[],"['Elaine Lau', 'Stephen Zhewen Lu', 'Ling Pan', 'Doina Precup', 'Emmanuel Bengio']","['McGill University', 'Mila - Quebec Artificial Intelligence Institute', 'Hong Kong University of Science and Technology', 'DeepMind', 'Recursion']",
https://openreview.net/forum?id=RRRyQMn6dv,Transparency & Explainability,CoSW: Conditional Sample Weighting for Smoke Segmentation with Label Noise,"Smoke segmentation is of great importance in precisely identifying the smoke location, enabling timely fire rescue and gas leak detection. However, due to the visual diversity and blurry edges of the non-grid smoke, noisy labels are almost inevitable in large-scale pixel-level smoke datasets. Noisy labels significantly impact the robustness of the model and may lead to serious accidents. Nevertheless, currently, there are no specific methods for addressing noisy labels in smoke segmentation. Smoke differs from regular objects as its transparency varies, causing inconsistent features in the noisy labels. In this paper, we propose a conditional sample weighting (CoSW). CoSW utilizes a multi-prototype framework, where prototypes serve as prior information to apply different weighting criteria to the different feature clusters. A novel regularized within-prototype entropy (RWE) is introduced to achieve CoSW and stable prototype update. The experiments show that our approach achieves SOTA performance on both real-world and synthetic noisy smoke segmentation datasets.",['Smoke Recognition; Smoke Segmentation; Industrial Applications'],[],"['Lujian Yao', 'Haitao Zhao', 'Zhongze Wang', 'Kaijie Zhao', 'Jingchao Peng']","['East China University of Science and Technology', 'East China University of Science and Technology', 'School of Information Science and Engineering, East China University of Science and Technology', 'East China University of Science and Technology', '']",
https://openreview.net/forum?id=V6qdb1AgsM,Privacy & Data Governance,Continual Counting with Gradual Privacy Expiration,"Differential privacy with gradual expiration models the setting where data items arrive in a stream and at a given time $t$ the privacy loss guaranteed for a data item seen at time $(t-d)$ is $\epsilon g(d)$, where $g$ is a monotonically non-decreasing function. We study the fundamental *continual (binary) counting* problem where each data item consists of a bit and the algorithm needs to output at each time step the sum of all the bits streamed so far. For a stream of length $T$ and privacy *without* expiration continual counting is possible with maximum  (over all time steps) additive error $O(\log^2(T)/\varepsilon)$ and the best known lower bound is $\Omega(\log(T)/\varepsilon)$; closing this gap is a challenging open problem.   We show that the situation is very different for privacy with gradual expiration by giving upper and lower bounds for a large set of expiration functions $g$. Specifically, our algorithm achieves an additive error of $O(\log(T)/\epsilon)$ for a large set of privacy expiration functions. We also give a lower bound that shows that if $C$ is the additive error of any $\epsilon$-DP algorithm for this problem, then the product of $C$ and the privacy expiration function after $2C$ steps must be $\Omega(\log(T)/\epsilon)$. Our algorithm matches this lower bound as its additive error is $O(\log(T)/\epsilon)$, even when $g(2C) = O(1)$.  Our empirical evaluation shows that we achieve a slowly growing privacy loss that has significantly smaller empirical privacy loss for large values of $d$ than a natural baseline algorithm.","['differential privacy', 'continual observation', 'privacy expiration']",[],"['Joel Daniel Andersson', 'Monika Henzinger', 'Rasmus Pagh', 'Teresa Anna Steiner', 'Jalaj Upadhyay']","['Computer Science, University of Copenhagen', 'Institute of Science and Technology', 'University of Copenhagen', 'Technical University of Denmark', 'Rutgers University']",
https://openreview.net/forum?id=fAlcxvrOEX,Security,AdjointDEIS: Efficient Gradients for Diffusion Models,"The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the *probability flow* ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call *AdjointDEIS*. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using *exponential integrators*. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released on our project page [https://zblasingame.github.io/AdjointDEIS/](https://zblasingame.github.io/AdjointDEIS/)","['continuous adjoint equations', 'neural differential equations', 'neural ODEs', 'adjoint sensitivity method', 'diffusion models', 'guided generation']",[],"['Zander W. Blasingame', 'Chen Liu']","['Electrical and Computer Engineering, Clarkson University', 'Electrical and Computer Engineering, Clarkson University']",
https://openreview.net/forum?id=WCc440cUhX,Transparency & Explainability,Understanding Transformers via N-Gram Statistics,"Transformer based large-language models (LLMs) display extreme proficiency with language yet a precise understanding of how they work remains elusive. One way of demystifying transformer predictions would be to describe how they depend on their context in terms of simple template functions. This paper takes a first step in this direction by considering families of functions (i.e. rules) formed out of simple N-gram based statistics of the training data. By studying how well these rulesets approximate transformer predictions, we obtain a variety of novel discoveries: a simple method to detect overfitting during training without using a holdout set, a quantitative measure of how transformers progress from learning simple to more complex statistical rules over the course of training, a model-variance criterion governing when transformer predictions tend to be described by N-gram rules, and insights into how well transformers can be approximated by N-gram rulesets in the limit where these rulesets become increasingly complex. In this latter direction, we find that for 79% and 68% of LLM next-token distributions on TinyStories and Wikipedia, respectively, their top-1 predictions agree with those provided by our N-gram rulesets.","['transformers', 'large-language models', 'ngrams', 'curriculum learning', 'interpretability']",[],['Timothy Nguyen'],['Google'],
https://openreview.net/forum?id=1Fc2Xa2cDK,Transparency & Explainability,Truth is Universal: Robust Detection of Lies in LLMs,"Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are  capable of ""lying"", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements.  In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, attaining 94\% accuracy in both distinguishing true from false factual statements and detecting lies generated in real-world scenarios.","['Interpretable AI', 'Explainable AI', 'Interpretability', 'AI Alignment', 'AI Safety', 'LLM Alignment', 'Truthful LLMs', 'Large Language Models', 'LLM', 'Mechanistic Interpretability', 'LLM Lie Detection']",[],"['Lennart Bürger', 'Fred A. Hamprecht', 'Boaz Nadler']","['Heidelberg University, Ruprecht-Karls-Universität Heidelberg', 'Heidelberg University', 'Weizmann Institute of Science']",
https://openreview.net/forum?id=v8X70gTodR,Transparency & Explainability,Analysing the Generalisation and Reliability of Steering Vectors,"Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to guide models' behaviour at scale.","['Interpretability', 'Causal Abstractions', 'Steering Vectors', 'Representation Engineering', 'Linear Representation Hypothesis', 'Contrastive Activation Addition']",[],"['Daniel Chee Hian Tan', 'David Chanin', 'Aengus Lynch', 'Brooks Paige', 'Dimitrios Kanoulas', 'Adrià Garriga-Alonso', 'Robert Kirk']","['Computer Science, University College London', 'Computer Science, University College London, University of London', 'University College London, University of London', 'University College London', 'University College London, University of London', 'FAR', 'UK AI Safety Institute']",
https://openreview.net/forum?id=1IU3P8VDbn,Transparency & Explainability,Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?,"Causal reasoning capability is critical in advancing large language models (LLMs) towards artificial general intelligence (AGI). While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark named CausalProbe 2024, whose corpus is fresh and nearly unseen for the studied LLMs. Empirical results show a significant performance drop on CausalProbe 2024 compared to earlier benchmarks, indicating that LLMs primarily engage in level-1 causal reasoning.To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. Inspired by this, we propose G$^2$-Reasoner, a LLM causal reasoning method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G$^2$-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and fictitious contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.","['trustworthy machine learning', 'LLM', 'causal reasoning']",[],"['Haoang Chi', 'He Li', 'Wenjing Yang', 'Feng Liu', 'Long Lan', 'Xiaoguang Ren', 'Tongliang Liu', 'Bo Han']","['National University of Defense Technology', 'Computer Science, National University of Defense Technology', 'National University of Defense Technology', 'Computing and Information Systems, University of Melbourne', 'college of computer science and technology, National University of Defense Technology', 'National University of Defense Technology', 'University of Sydney', 'Department of Computer Science, HKBU']",
https://openreview.net/forum?id=Cc0ckJlJF2,Security,Reward Machines for Deep RL in Noisy and Uncertain Environments,"Reward Machines provide an automaton-inspired structure for specifying instructions, safety constraints, and other temporally extended reward-worthy behaviour. By exposing the underlying structure of a reward function, they enable the decomposition of an RL task, leading to impressive gains in sample efficiency. Although Reward Machines and similar formal specifications have a rich history of application towards sequential decision-making problems, prior frameworks have traditionally ignored ambiguity and uncertainty when interpreting the domain-specific vocabulary forming the building blocks of the reward function. Such uncertainty critically arises in many real-world settings due to factors like partial observability or noisy sensors. In this work, we explore the use of Reward Machines for Deep RL in noisy and uncertain environments. We characterize this problem as a POMDP and propose a suite of RL algorithms that exploit task structure under uncertain interpretation of the domain-specific vocabulary. Through theory and experiments, we expose pitfalls in naive approaches to this problem while simultaneously demonstrating how task structure can be successfully leveraged under noisy interpretations of the vocabulary.","['Reward Machines', 'LTL', 'Linear Temporal Logic', 'Automata', 'RL', 'Reinforcement Learning', 'Formal Language']",[],"['Andrew C Li', 'Zizhao Chen', 'Toryn Q. Klassen', 'Pashootan Vaezipoor', 'Rodrigo Toro Icarte', 'Sheila A. McIlraith']","['Department of Computer Science, University of Toronto', 'Department of Computer Science, Cornell University', 'University of Toronto', 'Georgian', 'Pontificia Universidad Catolica de Chile', 'Department of Computer Science, University of Toronto']",
https://openreview.net/forum?id=348hfcprUs,Fairness & Bias,Fast Best-of-N Decoding via Speculative Rejection,"The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. In this work, we introduce Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient.","['alignment', 'large language models', 'rejection sampling', 'best-of-n', 'acceleration']",[],"['Hanshi Sun', 'Momin Haider', 'Ruiqi Zhang', 'Huitao Yang', 'Jiahao Qiu', 'Ming Yin', 'Mengdi Wang', 'Peter Bartlett', 'Andrea Zanette']","['Seed-MLSys, ByteDance Inc.', 'Computer Science, University of California, Santa Barbara', 'Department of Statistics, University of California, Berkeley', 'Fudan University', 'Princeton University', 'Princeton University', 'ECE, Princeton University', 'University of California - Berkeley', '']",
https://openreview.net/forum?id=bIFHHf2RoD,Fairness & Bias,CulturePark: Boosting Cross-cultural Understanding in Large Language Models,"Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures. Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media. However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale. Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection. CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures. It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs. Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs. We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education. Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on $41$ datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark.","['Culture bias', 'large language model', 'fine-tuning']",[],"['CHENG LI', 'Damien Teney', 'Linyi Yang', 'Qingsong Wen', 'Xing Xie', 'Jindong Wang']","['Microsoft', 'Idiap Research Institute', 'Southern University of Science and Technology', 'AI Research Institute, Squirrel Ai Learning', 'Microsoft Research Asia', 'Data Science, William & Mary']",
https://openreview.net/forum?id=sIsbOkQmBL,Fairness & Bias,CultureLLM: Incorporating Cultural Differences into Large Language Models,"Large language models (LLMs) have been observed to exhibit bias towards certain cultures due to the predominance of training data obtained from English corpora. Considering that multilingual cultural data is often expensive to procure, existing methodologies address this challenge through prompt engineering or culture-specific pre-training. However, these strategies may neglect the knowledge deficiency of low-resource cultures and necessitate substantial computing resources. In this paper, we propose CultureLLM, a cost-effective solution to integrate cultural differences into LLMs. CultureLLM employs the World Value Survey (WVS) as seed data and generates semantically equivalent training data through the proposed semantic data augmentation. Utilizing only $50$ seed samples from WVS with augmented data, we fine-tune culture-specific LLMs as well as a unified model (CultureLLM-One) for $9$ cultures, encompassing both rich and low-resource languages. Extensive experiments conducted on $60$ culture-related datasets reveal that CultureLLM significantly surpasses various counterparts such as GPT-3.5 (by $8.1$\%) and Gemini Pro (by $9.5$\%), demonstrating performance comparable to or exceeding that of GPT-4. Our human study indicates that the generated samples maintain semantic equivalence to the original samples, offering an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM.","['Culture bias', 'large language models', 'fairness']",[],"['CHENG LI', 'Mengzhuo Chen', 'Jindong Wang', 'Sunayana Sitaram', 'Xing Xie']","['Microsoft', 'University of the Chinese Academy of Sciences', 'Data Science, William & Mary', 'Microsoft', 'Microsoft Research Asia']",
https://openreview.net/forum?id=MQIET1VfoV,Fairness & Bias,Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance,"Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.","['Equivariant Graph Neural Networks', 'Reinforcement Learning', 'Multi-agent Reinforcement Learning', 'Symmetry', 'generalization', 'sample efficiency', 'MARL']",[],"['Joshua McClellan', 'Naveed Haghani', 'John Winder', 'Furong Huang', 'Pratap Tokekar']","['Johns Hopkins University Applied Physics Laboratory', 'University of Maryland, College Park', 'Johns Hopkins University Applied Physics Laboratory', 'Computer Science, University of Maryland', 'University of Maryland, College Park']",
https://openreview.net/forum?id=Bj2CpB9Dey,Transparency & Explainability,Tangent Space Causal Inference: Leveraging Vector Fields for Causal Discovery in Dynamical Systems,"Causal discovery with time series data remains a challenging yet increasingly important task across many scientific domains. Convergent cross mapping (CCM) and related methods have been proposed to study time series that are generated by dynamical systems, where traditional approaches like Granger causality are unreliable. However, CCM often yields inaccurate results depending upon the quality of the data. We propose the Tangent Space Causal Inference (TSCI) method for detecting causalities in dynamical systems. TSCI works by considering vector fields as explicit representations of the systems' dynamics and checks for the degree of synchronization between the learned vector fields. The TSCI approach is model-agnostic and can be used as a drop-in replacement for CCM and its generalizations. We first present a basic version of the TSCI algorithm, which is shown to be more effective than the basic CCM algorithm with very little additional computation. We additionally present augmented versions of TSCI that leverage the expressive power of latent variable models and deep learning. We validate our theory on standard systems, and we demonstrate improved causal inference performance across a number of benchmark tasks.","['causal discovery', 'convergent cross mapping', 'manifolds', 'dynamical systems', 'differential geometry']",[],"['Kurt Butler', 'Daniel Waxman', 'Petar Djuric']","['Electrical and Computer Engineering, State University of New York at Stony Brook', 'Electrical and Computer Engineering, State University of New York at Stony Brook', 'State University of New York at Stony Brook']",
https://openreview.net/forum?id=XZ4XSUTGRb,Fairness & Bias,Polyhedral Complex Derivation from Piecewise Trilinear Networks,"Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between the eikonal loss and the planarity of the hypersurfaces.","['Polyhedral Complex', 'Neural Radiance Fields', '3D Mesh']",[],['Jin-Hwa Kim'],['Seoul National University'],
https://openreview.net/forum?id=PLbFid00aU,Fairness & Bias,The Impact of Geometric Complexity on Neural Collapse in Transfer Learning,"Many of the recent advances in computer vision and language models can be attributed to the success of transfer learning via the pre-training of large foundation models. However, a theoretical framework which explains this empirical success is incomplete and remains an active area of research. Flatness of the loss surface and neural collapse have recently emerged as useful pre-training metrics which shed light on the implicit biases underlying pre-training. In this paper, we explore the geometric complexity of a model's learned representations as a fundamental mechanism that relates these two concepts. We show through experiments and theory that mechanisms which affect the geometric complexity of the pre-trained network also influence the neural collapse. Furthermore, we show how this effect of the geometric complexity generalizes to the neural collapse of new classes as well, thus encouraging better performance on downstream tasks, particularly in the few-shot setting.","['transfer learning', 'geometric complexity', 'neural collapse', 'implicit bias', 'flatness', 'generalization bounds']",[],"['Michael Munn', 'Benoit Dherin', 'Javier Gonzalvo']","['Google', '', 'Google']",
https://openreview.net/forum?id=3LKuC8rbyV,Privacy & Data Governance,Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning,"Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests.","['Machine unlearning', 'privacy', 'Langevin Monte Carlo', 'Langevin dynamic', 'gradient descent', 'differential privacy']",[],"['Eli Chien', 'Haoyu Peter Wang', 'Ziang Chen', 'Pan Li']","['Georgia Institute of Technology', 'ECE, Georgia Institute of Technology', 'Massachusetts Institute of Technology', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=h3k2NXu5bJ,Privacy & Data Governance,Certified Machine Unlearning via Noisy Stochastic Gradient Descent,"``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. We propose to leverage projected noisy stochastic gradient descent for unlearning and establish its first approximate unlearning guarantee under the convexity assumption. Our approach exhibits several benefits, including provable complexity saving compared to retraining, and supporting sequential and batch unlearning. Both of these benefits are closely related to our new results on the infinite Wasserstein distance tracking of the adjacent (un)learning processes. Extensive experiments show that our approach achieves a similar utility under the same privacy constraint while using $2\%$ and $10\%$ of the gradient computations compared with the state-of-the-art gradient-based approximate unlearning methods for mini-batch and full-batch settings, respectively.","['Machine unlearning', 'privacy', 'stochastic gradient descent']",[],"['Eli Chien', 'Haoyu Peter Wang', 'Ziang Chen', 'Pan Li']","['Georgia Institute of Technology', 'ECE, Georgia Institute of Technology', 'Massachusetts Institute of Technology', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=x2780VcMOI,Transparency & Explainability,A Polar coordinate system represents syntax in large language models,"Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a ''Structural Probe'' can find a subspace of neural activations, where syntactically-related words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the \emph{existence} but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a ''Polar Probe'' trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory.","['Natural Language Processing', 'Large Language Models', 'Interpretability', 'Syntax', 'Linguistics', 'Cognitive Science']",[],"['Pablo J. Diego Simon', ""Stéphane d'Ascoli"", 'Emmanuel Chemla', 'Yair Lakretz', 'Jean-Remi King']","['ETHZ - ETH Zurich', 'Facebook', 'CNRS', 'Ecole Normale Supérieure de Paris', 'CNRS']",
https://openreview.net/forum?id=1ONdF1JHyJ,Transparency & Explainability,Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via Diffusion Model,"Spatio-temporal (ST) prediction has garnered a De facto attention in earth sciences, such as meteorological prediction, human mobility perception. However, the scarcity of data coupled with the high expenses involved in sensor deployment results in notable data imbalances. Furthermore, models that are excessively customized and devoid of causal connections further undermine the generalizability and  interpretability. To this end, we establish a causal framework for ST predictions, termed CaPaint, which targets to identify causal regions in data and endow model with causal reasoning ability in a two-stage process. Going beyond this process, we utilize the back-door adjustment to specifically address the sub-regions identified as non-causal in the upstream phase. Specifically, we employ a novel image inpainting technique. By using a fine-tuned unconditional Diffusion Probabilistic Model (DDPM) as the generative prior, we in-fill the masks defined as environmental parts, offering the possibility of reliable extrapolation for potential data distributions. CaPaint overcomes the high complexity dilemma of optimal ST causal discovery models by reducing the data generation complexity from exponential to quasi-linear levels. Extensive experiments conducted on five real-world ST benchmarks demonstrate that integrating the CaPaint concept allows models to achieve improvements ranging from 4.3% to 77.3%. Moreover, compared to traditional mainstream ST augmenters, CaPaint underscores the potential of diffusion models in ST enhancement, offering a novel paradigm for this field. Our project is available at https://anonymous.4open.science/r/12345-DFCC.",['spatio-temporal data mining; causal inference;'],[],"['Yifan Duan', 'Jian Zhao', 'pengcheng', 'Junyuan Mao', 'Hao Wu', 'Jingyu Xu', 'shilong wang', 'Caoyuan Ma', 'Kai Wang', 'Kun Wang', 'Xuelong Li']","['Software Engineering, University of Science and Technology of China', 'School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwest Polytechnical University', 'Beijing Forestry University', 'Computer Science, University of Science and Technology of China', '', 'School of Computer Science, Wuhan University', 'Computer r  Science and Technology, University of Science and Technology of China', 'Computer Science, Wuhan University', 'soc, national university of singaore, National University of Singapore', 'Nanyang Technological University', 'China Telecom']",
https://openreview.net/forum?id=780uXnA4wN,Fairness & Bias,An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations,"Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector $\theta \in \mathbb{R}^n$, aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with $n$ is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator—the generator gradient estimator—for which the computation time remains stable in $n$. In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as $n$ increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.","['derivative estimation', 'gradient estimation', 'sensitivity analysis', 'stochastic differential equations', 'jump diffusions', 'neural SDEs']",[],"['Shengbo Wang', 'Jose Blanchet', 'Peter Glynn']","['Management Science and Engineering, Stanford University', 'Stanford University', 'Stanford University']",
https://openreview.net/forum?id=NGIIHlAEBt,Fairness & Bias,Understanding Bias in Large-Scale Visual Datasets,"A recent study has shown that large-scale visual datasets are very biased: they can be easily classified by modern neural networks. However, the concrete forms of bias among these datasets remain unclear. In this study, we propose a framework to identify the unique visual attributes distinguishing these datasets. Our approach applies various transformations to extract semantic, structural, boundary, color, and frequency information from datasets, and assess how much each type of information reflects their bias. We further decompose their semantic bias with object-level analysis, and leverage natural language methods to generate detailed, open-ended descriptions of each dataset's characteristics. Our work aims to help researchers understand the bias in existing large-scale pre-training datasets, and build more diverse and representative ones in the future. Our project page and code are available at boyazeng.github.io/understand_bias.","['dataset bias', 'visual datasets']",[],"['Boya Zeng', 'Yida Yin', 'Zhuang Liu']","['University of Pennsylvania', '', 'FAIR, Meta AI']",
https://openreview.net/forum?id=R0bnWrpIeN,Transparency & Explainability,CoSy: Evaluating Textual Explanations of Neurons,"A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While methods exist to connect neurons to human-understandable textual descriptions, evaluating the quality of these explanations is challenging due to the lack of a unified quantitative approach. We introduce CoSy (Concept Synthesis), a novel, architecture-agnostic framework for evaluating textual explanations of latent neurons. Given textual explanations, our proposed framework uses a generative model conditioned on textual input to create data points representing the explanations. By comparing the neuron's response to these generated data points and control data points, we can estimate the quality of the explanation. We validate our framework through sanity checks and benchmark various neuron description methods for Computer Vision tasks, revealing significant differences in quality.","['Explainable AI', 'Evaluation of Explainability Methods', 'Mechanistic Interpretability']",[],"['Laura Kopf', 'Philine Lou Bommer', 'Anna Hedström', 'Sebastian Lapuschkin', 'Marina MC Höhne', 'Kirill Bykov']","['Universität Potsdam', 'TU Berlin', 'J.P. Morgan Chase', 'Fraunhofer HHI', 'Computational Science, Universität Potsdam', 'Leibniz-Institut für Agrartechnik und Bioökonomie e.V. (ATB)']",
https://openreview.net/forum?id=zpw6NmhvKU,Transparency & Explainability,RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting,"The Rashomon effect is a mixed blessing in responsible machine learning. It enhances the prospects of finding models that perform well in accuracy while adhering to ethical standards, such as fairness or interpretability. Conversely, it poses a risk to the credibility of machine decisions through predictive multiplicity. While recent studies have explored the Rashomon effect across various machine learning algorithms, its impact on gradient boosting---an algorithm widely applied to tabular datasets---remains unclear. This paper addresses this gap by systematically analyzing the Rashomon effect and predictive multiplicity in gradient boosting algorithms. We provide rigorous theoretical derivations to examine the Rashomon effect in the context of gradient boosting and offer an information-theoretic characterization of the Rashomon set. Additionally, we introduce a novel inference technique called RashomonGB to efficiently inspect the Rashomon effect in practice. On more than 20 datasets, our empirical results show that RashomonGB outperforms existing baselines in terms of improving the estimation of predictive multiplicity metrics and model selection with group fairness constraints. Lastly, we propose a framework to mitigate predictive multiplicity in gradient boosting and empirically demonstrate its effectiveness.",['Rashomon effects; predictive multiplicity; gradient boosting; tabular data'],[],"['Hsiang Hsu', 'Ivan Brugere', 'Shubham Sharma', 'Freddy Lecue', 'Chun-Fu Chen']","['JP Morgan & Chase Bank', 'J.P. Morgan', 'J.P. Morgan Chase', 'INRIA', 'JPMorganChase, GTAR']",
https://openreview.net/forum?id=Cqr6E81iB7,Privacy & Data Governance,The Limits of Differential Privacy in Online Learning,"Differential privacy (DP) is a formal notion that restricts the privacy leakage of an algorithm when running on sensitive data, in which privacy-utility trade-off is one of the central problems in private data analysis. In this work, we investigate the fundamental limits of differential privacy in online learning algorithms and present evidence that separates three types of constraints: no DP, pure DP, and approximate DP. We first describe a hypothesis class that is online learnable under approximate DP but not online learnable under pure DP under the adaptive adversarial setting. This indicates that approximate DP must be adopted when dealing with adaptive adversaries. We then prove that any private online learner must make an infinite number of mistakes for almost all hypothesis classes. This essentially generalizes previous results and shows a strong separation between private and non-private settings since a finite mistake bound is always attainable (as long as the class is online learnable) when there is no privacy requirement.","['online learning', 'differential privacy']",[],"['Bo Li', 'Wei Wang', 'Peng Ye']","['', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology']",
https://openreview.net/forum?id=Y5DPSJzpra,Fairness & Bias,Harnessing small projectors and multiple views for efficient vision pretraining,"Recent progress in self-supervised (SSL) visual representation learning has led to the development of several different proposed frameworks that rely on augmentations of images but use different loss functions.  However, there are few theoretically grounded principles to guide practice, so practical implementation of each SSL framework requires several heuristics to achieve competitive performance. In this work, we build on recent analytical results to design practical recommendations for competitive and efficient SSL that are grounded in theory.  Specifically, recent theory tells us that existing SSL frameworks are actually minimizing the same idealized loss, which is to learn features that best match the data similarity kernel defined by the augmentations used. We show how this idealized loss can be reformulated to a functionally equivalent loss that is more efficient to compute. We study the implicit bias of using gradient descent to minimize our reformulated loss function, and find that using a stronger orthogonalization constraint with a reduced projector dimensionality should yield good representations. Furthermore, the theory tells us that approximating the reformulated loss should be improved by increasing the number of augmentations, and as such using multiple augmentations should lead to improved convergence. We empirically verify our findings on CIFAR, STL and Imagenet datasets, wherein we demonstrate an improved linear readout performance when training a ResNet-backbone using our theoretically grounded recommendations.   Remarkably, we also demonstrate that by leveraging these insights, we can reduce the pretraining dataset size by up to 2$\times$ while maintaining downstream accuracy simply by using more data augmentations.  Taken together, our work provides theoretically grounded recommendations that can be used to improve SSL convergence and efficiency.","['representation learning', 'self-supervised learning', 'data-augmentation', 'learning dynamics', 'sample efficient SSL', 'compute efficient SSL']",[],"['Arna Ghosh', 'Kumar Krishna Agrawal', 'Shagun Sodhani', 'Adam Oberman', 'Blake Aaron Richards']","['McGill University', 'University of California Berkeley, USA', 'Facebook', 'Mathematics and Statistics, McGill University', 'Paradigms of Intelligence, Google']",
https://openreview.net/forum?id=CeOwahuQic,Fairness & Bias,Can Large Language Model Agents Simulate Human Trust Behavior?,"Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in social science and role-playing applications. However, one fundamental question remains: can LLM agents really simulate human behavior? In this paper, we focus on one critical and elemental behavior in human interactions, trust, and investigate whether LLM agents can simulate human trust behavior. We first find that LLM agents generally exhibit trust behavior, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that GPT-4 agents manifest high behavioral alignment with humans in terms of trust behavior, indicating the feasibility of simulating human trust behavior with LLM agents. In addition,  we probe the biases of agent trust and  differences in agent trust towards other LLM agents and humans. We also explore the intrinsic properties of agent trust under conditions including external manipulations and advanced reasoning strategies. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans beyond value alignment. We further illustrate broader implications of our discoveries for applications where trust is paramount.","['LLM Agent', 'Human Simulation', 'Behavioral Alignment', 'Trust Games']",[],"['Chengxing Xie', 'Canyu Chen', 'Feiran Jia', 'Ziyu Ye', 'Shiyang Lai', 'Kai Shu', 'Jindong Gu', 'Adel Bibi', 'Ziniu Hu', 'David Jurgens', 'James Evans', 'Philip Torr', 'Bernard Ghanem', 'Guohao Li']","[""College of Computer Science and Technology, Xi'an University of Electronic Science and Technology"", 'Computer Science, Illinois Institute of Technology', 'Information Science and Technology, Pennsylvania State University', 'Google DeepMind', 'Sociology Department, University of Chicago', 'Emory University', '', 'Engineering Science, University of Oxford', 'xAI', 'Computer Science, University of Michigan - Ann Arbor', 'Sociology, University of Chicago', 'University of Oxford', 'Computer Science, King Abdullah University of Science and Technology', 'University of Oxford']",
https://openreview.net/forum?id=1L5vaNIoK5,Security,Diffusion Policy Attacker: Crafting Adversarial Attacks for Diffusion-based Policies,"Diffusion models have emerged as a promising approach for behavior cloning (BC), leveraging their exceptional ability to model multi-modal distributions. Diffusion policies (DP) have elevated BC performance to new heights, demonstrating robust efficacy across diverse tasks, coupled with their inherent flexibility and ease of implementation. Despite the increasing adoption of Diffusion Policies (DP) as a foundation for policy generation, the critical issue of safety remains largely unexplored. While previous attempts have targeted deep policy networks, DP used diffusion models as the policy network, making it ineffective to be attacked using previous methods because of its chained structure and randomness injected. In this paper, we undertake a comprehensive examination of DP safety concerns by introducing adversarial scenarios, encompassing offline and online attacks, global and patch-based attacks. We propose DP-Attacker, a suite of algorithms that can craft effective adversarial attacks across all aforementioned scenarios. We conduct attacks on pre-trained diffusion policies across various manipulation tasks. Through extensive experiments, we demonstrate that DP-Attacker has the capability to significantly decrease the success rate of DP for all scenarios. Particularly in offline scenarios, we exhibit the generation of highly transferable perturbations applicable to all frames. Furthermore, we illustrate the creation of adversarial physical patches that, when applied to the environment, effectively deceive the model. Video results are put in: https://sites.google.com/view/dp-attacker-videos/.","['Diffusion Model', 'Adversarial Attacks', 'Robot Learning']",[],"['Yipu Chen', 'Haotian Xue', 'Yongxin Chen']","['College of Computing, Georgia Institute of Technology', 'Georgia Institute of Technology', 'NVIDIA']",
https://openreview.net/forum?id=xNlQjS0dtO,Security,Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates,"Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. (2024) reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the “Pure Tuning, Safe Testing” (PTST) strategy --- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.","['safety prompt', 'AI alignment']",[],"['Kaifeng Lyu', 'Haoyu Zhao', 'Xinran Gu', 'Dingli Yu', 'Anirudh Goyal', 'Sanjeev Arora']","['Simons Institute, University of California, Berkeley', 'Princeton University', 'Tsinghua University, Tsinghua University', 'Microsoft Research, Microsoft', 'Google DeepMind', 'Princeton University']",
https://openreview.net/forum?id=HwO1mNluoL,Fairness & Bias,Mitigating Biases in Blackbox Feature Extractors for Image Classification Tasks,"In image classification, it is common to utilize a pretrained model to extract meaningful features of the input images, and then to train a classifier on top of it to make predictions for any downstream task. Trained on enormous amounts of data, these models have been shown to contain harmful biases which can hurt their performance when adapted for a downstream classification task. Further, very often they may be blackbox, either due to scale, or because of unavailability of model weights or architecture. Thus, during a downstream task, we cannot debias such models by updating the weights of the feature encoder, as only the classifier can be finetuned. In this regard, we investigate the suitability of some existing debiasing techniques and thereby motivate the need for more focused research towards this problem setting. Furthermore, we propose a simple method consisting of a clustering-based adaptive margin loss with a blackbox feature encoder, with no knowledge of the bias attribute. Our experiments demonstrate the effectiveness of our method across multiple benchmarks.",['bias;fairness;spurious correlation'],[],"['Abhipsa Basu', 'Saswat Subhajyoti Mallick', 'Venkatesh Babu Radhakrishnan']","['Computational and Data Sciences, Indian Institute of Science', '', 'Indian Institute of Science']",
https://openreview.net/forum?id=85tu7K06i3,Security,Looks Too Good To Be True: An Information-Theoretic Analysis of Hallucinations in Generative Restoration Models,"The pursuit of high perceptual quality in image restoration has driven the development of revolutionary generative models, capable of producing results often visually indistinguishable from real data. However, as their perceptual quality continues to improve, these models also exhibit a growing tendency to generate hallucinations – realistic-looking details that do not exist in the ground truth images. Hallucinations in these models create uncertainty about their reliability, raising major concerns about their practical application. This paper investigates this phenomenon through the lens of information theory, revealing a fundamental tradeoff between uncertainty and perception. We rigorously analyze the relationship between these two factors, proving that the global minimal uncertainty in generative models grows in tandem with perception.  In particular, we define the inherent uncertainty of the restoration problem and show that attaining perfect perceptual quality entails at least twice this uncertainty. Additionally, we establish a relation between distortion, uncertainty and perception, through which we prove the aforementioned uncertainly-perception tradeoff induces the well-known perception-distortion tradeoff. We demonstrate our theoretical findings through experiments with super-resolution and inpainting algorithms. This work uncovers fundamental limitations of generative models in achieving both high perceptual quality and reliable predictions for image restoration.  Thus, we aim to raise awareness among practitioners about this inherent tradeoff, empowering them to make informed decisions and potentially prioritize safety over perceptual performance.","['uncertainty', 'hallucinations', 'perception', 'tradeoff', 'distortion', 'restoration tasks', 'inverse problems.']",[],"['Regev Cohen', 'Idan Kligvasser', 'Ehud Rivlin', 'Daniel Freedman']","['Google', 'Verily (Google Life Sciences)', 'Technion, Technion', 'Verily']",
https://openreview.net/forum?id=xZKXGvLB0c,Transparency & Explainability,Causal vs. Anticausal merging of predictors,"We study the differences arising from merging predictors in the causal and anticausal directions using the same data. In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors. We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect. We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction. Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.","['Causality', 'Merging of predictors', 'Causal vs Anticausal', 'Maximum Entropy']",[],"['Sergio Hernan Garrido Mejia', 'Patrick Blöbaum', 'Bernhard Schölkopf', 'Dominik Janzing']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', '', '', 'Amazon']",
https://openreview.net/forum?id=bioHNTRnQk,Fairness & Bias,Model Collapse Demystified: The Case of Regression,"The era of proliferation of large language and image generation models begs the question of what happens if models are trained on the synthesized outputs of other models. The phenomenon of ""model collapse"" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e. the model collapses. In this work, we investigate this phenomenon within the context of high-dimensional regression with Gaussian data, considering both low- and high-dimensional asymptotics. We derive analytical formulas that quantitatively describe this phenomenon in both under-parameterized and over-parameterized regimes. We show how test error increases linearly in the number of model iterations in terms of all problem hyperparameters (covariance spectrum, regularization, label noise level, dataset size) and further isolate how model collapse affects both bias and variance terms in our setup. We show that even in the noise-free case, catastrophic (exponentially fast) model-collapse can happen in the over-parametrized regime. In the special case of polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.","['Model Collapse', 'Kernel Ridge Regression', 'High dimensional asymptotics', 'Synthetic Data', 'Scaling Laws']",[],"['Elvis Dohmatob', 'Yunzhen Feng', 'Julia Kempe']","['Facebook', 'Stanford University', '']",
https://openreview.net/forum?id=y6qhVtFG77,Transparency & Explainability,NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping,"Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.","['EEG-to-fMRI synthesis', 'EEG', 'fMRI']",[],"['Yamin Li', 'Ange Lou', 'Ziyuan Xu', 'SHENGCHAO ZHANG', 'Shiyu Wang', 'Dario J. Englot', 'Soheil Kolouri', 'Daniel Moyer', 'Roza G Bayrak', 'Catie Chang']","['Computer Science, Vanderbilt University', 'Vanderbilt University', 'Computer Science, , Vanderbilt University', 'Vanderbilt University', 'Engineering, Vanderbilt University', 'Vanderbilt University', 'Computer Science, Vanderbilt University', 'Computer Science, Vanderbilt University', 'Electrical and Computer Engineering, Vanderbilt University', 'Vanderbilt University']",
https://openreview.net/forum?id=SoM3vngOH5,Fairness & Bias,Tree of Attacks: Jailbreaking Black-Box LLMs Automatically,"While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed *jailbreaks*. In this work, we present *Tree of Attacks with Pruning*  (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art *guardrails*, e.g., LlamaGuard.","['LLMs', 'black-box jailbreaks', 'red teaming', 'alignment']",[],"['Anay Mehrotra', 'Manolis Zampetakis', 'Paul Kassianik', 'Blaine Nelson', 'Hyrum S Anderson', 'Yaron Singer', 'Amin Karbasi']","['Yale University', 'Yale University', 'Cisco', 'Robust Intelligence', 'Robust Intelligence', 'Harvard University', 'Yale University']",
https://openreview.net/forum?id=BdGFgKrlHl,Fairness & Bias,Addressing Bias in Online Selection with Limited Budget of Comparisons,"Consider a hiring process with candidates coming from different universities. It is easy to order candidates with the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments, leading to a potentially high total cost for the hiring organization. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate? We model the above problem as a multicolor secretary problem, allowing comparisons between candidates from distinct groups at a fixed cost. Our study explores how the allocated budget enhances the success probability of online selection algorithms.","['the secretary problem', 'online selection', 'online algorithms', 'decision-making under uncertainty']",[],"['Ziyad Benomar', 'Evgenii Chzhen', 'Nicolas Schreuder', 'Vianney Perchet']","['', 'CNRS/University Paris-Saclay', 'CNRS', 'Ensae ParisTech']",
https://openreview.net/forum?id=mkw6x0OExg,Transparency & Explainability,Explanations that reveal all through the deﬁnition of encoding,"Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a deﬁnition of encoding that identiﬁes this extra predictive power via conditional dependence and show that the deﬁnition ﬁts existing examples of encoding. This deﬁnition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a “what you see is what you get” property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to show that despite prompting an LLM to produce non-encoding explanations for a sentiment analysis task, the LLM-generated explanations encode.","['feature attributions', 'model explanations', 'evaluating explanations', 'encoding the prediction', 'interpretability']",[],"['Aahlad Manas Puli', 'Nhi Nguyen', 'Rajesh Ranganath']","['Data Science, New York University', 'Computer Science, New York University', 'New York University']",
https://openreview.net/forum?id=QUYLbzwtTV,Fairness & Bias,Bias in Motion: Theoretical Insights into the Dynamics of Bias in SGD Training,"Machine learning systems often acquire biases by leveraging undesired features in the data, impacting accuracy variably across different sub-populations of the data. However, our current understanding of bias formation mostly focuses on the initial and final stages of learning, leaving a gap in knowledge regarding the transient dynamics. To address this gap, this paper explores the evolution of bias in a teacher-student setup that models different data sub-populations with a Gaussian-mixture model. We provide an analytical description of the stochastic gradient descent dynamics of a linear classifier in this setup, which we prove to be exact in high dimension. Notably, our analysis identifies different properties of the sub-populations that drive bias at different timescales and hence shows a shifting preference of our classifier during training. By applying our general solution to fairness and robustness, we delineate how and when heterogeneous data and spurious features can generate and amplify bias. We empirically validate our results in more complex scenarios by training deeper networks on synthetic and real data, i.e. using CIFAR10, MNIST, and CelebA datasets.","['learning dynamics', 'online learning', 'stochastic gradient descent', 'analytical model', 'fairness', 'spurious correlation']",[],"['Anchit Jain', 'Rozhin Nobahari', 'Aristide Baratin', 'Stefano Sarao Mannelli']","['Department of Engineering, University of Cambridge', 'University of Montreal', '', 'Chalmers University of Technology']",
https://openreview.net/forum?id=5lLb7aXRN9,Transparency & Explainability,Unconditional stability of a recurrent neural circuit implementing divisive normalization,"Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of ""oscillatory recurrent gated neural integrator circuits'' (ORGaNICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and that has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGaNICs circuit when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGaNICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGaNICs outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.","['Recurrent Networks', 'Theoretical Neuroscience', 'Dynamical Systems', 'Normalization']",[],"['Shivang Rawat', 'David Heeger', 'Stefano Martiniani']","['Courant Institute of Mathematical Sciences, New York University', 'New York University', 'New York University']",
https://openreview.net/forum?id=01s5ODIHKd,Security,FreqMark: Invisible Image Watermarking via Frequency Based Optimization in Latent Space,"Invisible watermarking is essential for safeguarding digital content, enabling copyright protection and content authentication.  However, existing watermarking methods fall short in robustness against regeneration attacks. In this paper, we propose a novel method called FreqMark that involves unconstrained optimization of the image latent frequency space obtained after VAE encoding. Specifically, FreqMark embeds the watermark by optimizing the latent frequency space of the images and then extracts the watermark through a pre-trained image encoder. This optimization allows a flexible trade-off between image quality with watermark robustness and effectively resists regeneration attacks. Experimental results demonstrate that FreqMark offers significant advantages in image quality and robustness, permits flexible selection of the encoding bit number, and achieves a bit accuracy exceeding 90\% when encoding a 48-bit hidden message under various attack scenarios.","['deep watermarking', 'latent frequency optimization']",[],"['YiYang Guo', 'Ruizhe Li', 'Mude Hui', 'Hanzhong Allan Guo', 'Chen Zhang', 'Chuangjian Cai', 'Le Wan', 'Shangfei Wang']","['Computer Science and Technology, University of Science and Technology of China', '', 'CSE Dept, University of California, Santa Cruz', 'University of Hong Kong', 'University of Science and Technology of China', 'Biomedical Engineering, Medical School, Tsinghua University, Tsinghua University', 'Jilin University', 'School of Computer Science and Technology, University of Science and Technology of China']",
https://openreview.net/forum?id=AFnSMlye5K,Transparency & Explainability,Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis,"The success of machine learning models relies heavily on effectively representing high-dimensional data. However, ensuring data representations capture human-understandable concepts remains difficult, often requiring the incorporation of prior knowledge and decomposition of data into multiple subspaces. Traditional linear methods fall short in modeling more than one space, while more expressive deep learning approaches lack interpretability. Here, we introduce Supervised Independent Subspace Principal Component Analysis ($\texttt{sisPCA}$), a PCA extension designed for multi-subspace learning. Leveraging the Hilbert-Schmidt Independence Criterion (HSIC), $\texttt{sisPCA}$ incorporates supervision and simultaneously ensures subspace disentanglement. We demonstrate $\texttt{sisPCA}$'s connections with autoencoders and regularized linear regression and showcase its ability to identify and separate hidden data structures through extensive applications, including breast cancer diagnosis from image features, learning aging-associated DNA methylation changes, and single-cell analysis of malaria infection. Our results reveal distinct functional pathways associated with malaria colonization, underscoring the essentiality of explainable representation in high-dimensional data analysis.","['Principal Component Analysis (PCA)', 'Disentanglement Representation Learning', 'Computational Biology']",[],"['Jiayu Su', 'David A. Knowles', 'Raul Rabadan']","['Department of Systems Biology, Columbia University', 'Columbia University', '']",
https://openreview.net/forum?id=fYfliutfHX,Security,Learning predictable and robust neural representations by straightening image sequences,"Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations.  Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that follow straighter temporal trajectories than their initial photoreceptor encoding, which allows for prediction by linear extrapolation. Inspired by these experimental findings, we develop a self-supervised learning (SSL) objective that explicitly quantifies and promotes straightening. We demonstrate the power of this objective in training deep feedforward neural networks on smoothly-rendered synthetic image sequences that mimic commonly-occurring properties of natural videos. The learned model contains neural embeddings that are predictive, but also factorize the geometric, photometric, and semantic attributes of objects. The representations also prove more robust to noise and adversarial attacks compared to previous SSL methods that optimize for invariance to random augmentations. Moreover, these beneficial properties can be transferred to other training procedures by using the straightening objective as a regularizer, suggesting a broader utility for straightening as a principle for robust unsupervised learning.",['straightening; prediction; self-supervised learning; robustness'],[],"['Xueyan Niu', 'Cristina Savin', 'Eero P Simoncelli']","['Center for Neural Science, New York University', 'Center for Data Science , New York University', 'New York University']",
https://openreview.net/forum?id=ylceJ2xIw5,Fairness & Bias,Fair Wasserstein Coresets,"Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).","['Algorithmic Fairness', 'Nonconvex Optimization', 'Coresets']",[],"['Zikai Xiong', 'Niccolo Dalmasso', 'Shubham Sharma', 'Freddy Lecue', 'Daniele Magazzeni', 'Vamsi K. Potluru', 'Tucker Balch', 'Manuela Veloso']","['Massachusetts Institute of Technology', 'J.P. Morgan Chase', 'J.P. Morgan Chase', 'INRIA', ""King's College London"", 'AI Research, J.P. Morgan Chase', 'J.P. Morgan Chase', 'School of Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=VZQmIoDGBG,Security,SafeWorld: Geo-Diverse Safety Alignment,"In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlooks the geo-diversity of cultural and legal standards across the world. To reveal the chal5 lenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs’ ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria effectively. To enhance LLMs’ alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.","['LLM', 'Geo-Diverse', 'Cultural Norm', 'Public Policy', 'Safety']",[],"['Da Yin', 'Haoyi Qiu', 'Kung-Hsiang Huang', 'Kai-Wei Chang', 'Nanyun Peng']","['University of California, Los Angeles', 'UCLA Computer Science Department, University of California, Los Angeles', 'SalesForce.com', 'University of California, Los Angeles', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=TA5zPfH8iI,Transparency & Explainability,B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable,"B-cos Networks have been shown to be effective for obtaining highly human interpretable explanations of model decisions by architecturally enforcing stronger alignment between inputs and weight. B-cos variants of convolutional networks (CNNs) and vision transformers (ViTs), which primarily replace linear layers with B-cos transformations, perform competitively to their respective standard variants while also yielding explanations that are faithful by design. However, it has so far been necessary to train these models from scratch, which is increasingly infeasible in the era of large, pre-trained foundation models. In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose ‘B-cosification’, a novel approach to transform existing pre-trained models to become inherently interpretable. We perform a thorough study of design choices to perform this conversion, both for convolutional neural networks and vision transformers. We find that B-cosification can yield models that are on par with B-cos models trained from scratch in terms of interpretability, while often outperforming them in terms of classification performance at a fraction of the training cost. Subsequently, we apply B-cosification to a pretrained CLIP model, and show that, even with limited data and compute cost, we obtain a B-cosified version that is highly interpretable and competitive on zero shot performance across a variety of datasets. We release our code and pre-trained model weights at https://github.com/shrebox/B-cosification.","['B-cos networks', 'Explainability', 'Inherent Interpretability', 'CLIP']",[],"['Shreyash Arya', 'Sukrut Rao', 'Moritz Böhle', 'Bernt Schiele']","['Computer Vision and Machine Learning, Saarland Informatics Campus, Max-Planck Institute', 'Computer Vision and Machine Learning, Max Planck Institute for Informatics, Saarland Informatics Campus', 'Kyutai', 'Max Planck Institute for Informatics, Saarland Informatics Campus']",
https://openreview.net/forum?id=tNhwg9U767,Fairness & Bias,Microstructures and Accuracy of Graph Recall by Large Language Models,"Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from --- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.","['large language models', 'graph recall', 'human cognition', 'sociology', 'network motif']",[],"['Yanbang Wang', 'Hejie Cui', 'Jon Kleinberg']","['Department of Computer Science, Cornell University', 'Stanford University', 'Cornell University']",
https://openreview.net/forum?id=nXXwYsARXB,Transparency & Explainability,A hierarchical decomposition for explaining ML performance discrepancies,"Machine learning (ML) algorithms can often differ in performance across domains. Understanding why their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Aggregate decompositions express the total performance gap as the gap due to a shift in the feature distribution $p(X)$ plus the gap due to a shift in the outcome's conditional  distribution $p(Y|X)$. While this coarse explanation is helpful for guiding root cause analyses, it provides limited details and can only suggest coarse fixes involving all variables in an ML system. Detailed decompositions quantify the importance of each variable to each term in the aggregate decomposition, which can provide a deeper understanding and suggest more targeted interventions. Although parametric methods exist for conducting a full hierarchical decomposition of an algorithm's performance gap at the aggregate and detailed levels, current nonparametric methods only cover parts of the hierarchy; many also require knowledge of the entire causal graph. We introduce a nonparametric hierarchical framework for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge. Furthermore, we derive debiased, computationally-efficient estimators and statistical inference procedures to construct confidence intervals for the explanations.","['explainability', 'distribution shift', 'double machine learning']",[],"['Harvineet Singh', 'Fan Xia', 'Adarsh Subbaswamy', 'Alexej Gossmann', 'Jean Feng']","['University of California, San Francisco', 'University of California, San Francisco', 'U.S. Food and Drug Administration', '', 'University of California, San Francisco']",
https://openreview.net/forum?id=9uMJeCUeKk,Security,"Ask, Attend, Attack: An Effective Decision-Based Black-Box Targeted Attack for Image-to-Text Models","While image-to-text models have demonstrated significant advancements in various vision-language tasks, they remain susceptible to adversarial attacks. Existing white-box attacks on image-to-text models require access to the architecture, gradients, and parameters of the target model, resulting in low practicality. Although the recently proposed gray-box attacks have improved practicality, they suffer from semantic loss during the training process, which limits their targeted attack performance. To advance adversarial attacks of image-to-text models, this paper focuses on a challenging scenario: decision-based black-box targeted attacks where the attackers only have access to the final output text and aim to perform targeted attacks. Specifically, we formulate the decision-based black-box targeted attack as a large-scale optimization problem. To efficiently solve the optimization problem, a three-stage process \textit{Ask, Attend, Attack}, called \textit{AAA}, is proposed to coordinate with the solver. \textit{Ask} guides attackers to create target texts that satisfy the specific semantics. \textit{Attend} identifies the crucial regions of the image for attacking, thus reducing the search space for the subsequent \textit{Attack}. \textit{Attack} uses an evolutionary algorithm to attack the crucial regions, where the attacks are semantically related to the target texts of \textit{Ask}, thus achieving targeted attacks without semantic loss. Experimental results on transformer-based and CNN+RNN-based image-to-text models confirmed the effectiveness of our proposed \textit{AAA}.","['Black-box adversarial attack', 'Image-to-text model', 'Attention', 'Differential evolution']",[],"['Qingyuan Zeng', 'Zhenzhong Wang', 'Yiu-ming Cheung', 'Min Jiang']","['Institute of Artificial Intelligence, Xiamen University', '', '', 'Department of Artificial Intelligence, Xiamen University']",
https://openreview.net/forum?id=cbkJBYIkID,Security,Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor,"Data-poisoning backdoor attacks are serious security threats to machine learning models, where an adversary can manipulate the training dataset to inject backdoors into models. In this paper, we focus on in-training backdoor defense, aiming to train a clean model even when the dataset may be potentially poisoned. Unlike most existing methods that primarily detect and remove/unlearn suspicious samples to mitigate malicious backdoor attacks, we propose a novel defense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB leverages the “home field” advantage of defenders by proactively injecting a defensive backdoor into the model during training. Taking advantage of controlling the training process, the defensive backdoor is designed to suppress the malicious backdoor effectively while remaining secret to attackers. In addition, we introduce a reversible mapping to determine the defensive target label. During inference, PDB embeds a defensive trigger in the inputs and reverses the model’s prediction, suppressing malicious backdoor and ensuring the model's utility on the original task. Experimental results across various datasets and models demonstrate that our approach achieves state-of-the-art defense performance against a wide range of backdoor attacks. The code is available at https://github.com/shawkui/Proactive_Defensive_Backdoor.","['Adversarial Machine Learning', 'Backdoor Attack', 'Backdoor Defense', 'AI Security']",[],"['Shaokui Wei', 'Hongyuan Zha', 'Baoyuan Wu']","['SDS, The Chinese University of Hong Kong, Shenzhen', 'The Chinese University of Hong Kong, Shenzhen', 'The Chinese University of Hong Kong, Shenzhen']",
https://openreview.net/forum?id=axW8xvQPkF,Fairness & Bias,Fairness in Social Influence Maximization via Optimal Transport,"We study fairness in social influence maximization, whereby one seeks to select seeds that spread a given information throughout a network, ensuring balanced outreach among different communities (e.g. demographic groups). In the literature, fairness is often quantified in terms of the expected outreach within individual communities. In this paper, we demonstrate that such fairness metrics can be misleading since they overlook the stochastic nature of information diffusion processes. When information diffusion occurs in a probabilistic manner, multiple outreach scenarios can occur. As such, outcomes such as “In 50% of the cases, no one in group 1 gets the information, while everyone in group 2 does, and in the other 50%, it is the opposite”, which always results in largely unfair outcomes, are classified as fair by a variety of fairness metrics in the literature. We tackle this problem by designing a new fairness metric, mutual fairness, that captures variability in outreach through optimal transport theory. We propose a new seed- selection algorithm that optimizes both outreach and mutual fairness, and we show its efficacy on several real datasets. We find that our algorithm increases fairness with only a minor decrease (and at times, even an increase) in efficiency.","['Fairness', 'social influence maximization', 'optimal transport']",[],"['Shubham Chowdhary', 'Giulia De Pasquale', 'Nicolas Lanzetti', 'Ana-Andreea Stoica', 'Florian Dorfler']","['Department of Mathematics, Automatic Control Lab, ETHZ - ETH Zurich', 'Eindhoven University of Technology', 'ETHZ - ETH Zurich', '', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=3f8i9GlBzu,Transparency & Explainability,Can Transformers Smell Like Humans?,"The human brain encodes stimuli from the environment into representations that form a sensory perception of the world. Despite recent advances in understanding visual and auditory perception, olfactory perception remains an under-explored topic in the machine learning community due to the lack of large-scale datasets annotated with labels of human olfactory perception. In this work, we ask the question of whether pre-trained transformer models of chemical structures encode representations that are aligned with human olfactory perception, i.e., can transformers smell like humans? We demonstrate that representations encoded from transformers pre-trained on general chemical structures are highly aligned with human olfactory perception.  We use multiple datasets and different types of perceptual representations to show that the representations encoded by transformer models are able to predict: (i) labels associated with odorants‌‌ provided by experts; (ii) continuous ratings provided by human participants with respect to pre-defined descriptors; and (iii) similarity ratings between odorants provided by human participants. Finally, we evaluate the extent to which this alignment is associated with physicochemical features of odorants known to be relevant for olfactory decoding.","['Representational Alignment', 'Olfactory', 'Transformers', 'Representation Learning', 'Neuroscience']",[],"['Farzaneh Taleb', 'Miguel Vasco', 'Antonio H. Ribeiro', 'Mårten Björkman', 'Danica Kragic']","['KTH Royal Institute of Technology', 'RPL, KTH Royal Institute of Technology', 'Department of Information Technology, Uppsala University', 'EECS, KTH Royal Institute of Technology, Stockholm, Sweden', 'KTH']",
https://openreview.net/forum?id=HYa3eu8scG,Transparency & Explainability,Training for Stable Explanation for Free,"To foster trust in machine learning models, explanations must be faithful and stable for consistent insights. Existing relevant works rely on the $\ell_p$ distance for stability assessment, which diverges from human perception. Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race. To address these challenges, we introduce a novel metric to assess the stability of top-$k$ salient features. We introduce R2ET which trains for stable explanation by efficient and effective regularizer, and analyze R2ET by multi-objective optimization to prove numerical and statistical stability of explanations. Moreover, theoretical connections between R2ET and certified robustness justify R2ET's stability in all attacks. Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods. The code can be found at https://github.com/ccha005/R2ET.","['accountable machine learning', 'stability', 'transparency', 'interpretability']",[],"['Chao Chen', 'Chenghua Guo', 'Rufeng Chen', 'Guixiang Ma', 'Ming Zeng', 'Xiangwen Liao', 'Xi Zhang', 'Sihong Xie']","['', 'Beijing University of Posts and Telecommunications', 'The Hong Kong University of Science and Technology', 'Intel Labs, Intel', 'Carnegie Mellon University', 'Fuzhou University', 'Beijing University of Posts and Telecommunications', 'HKUST-GZ']",
https://openreview.net/forum?id=uuQQwrjMzb,Fairness & Bias,Adaptive Labeling for Efficient Out-of-distribution Model Evaluation,"Datasets often suffer severe selection bias; clinical labels are only available on patients for whom doctors ordered medical exams. To assess model performance outside the support of available data, we present a computational framework for adaptive labeling, providing cost-efficient model evaluations under severe distribution shifts. We formulate the problem as a Markov Decision Process over states defined by posterior beliefs on model performance. Each batch of new labels incurs a “state transition” to sharper beliefs, and we choose batches to minimize uncertainty on model performance at the end of the label collection process. Instead of relying on high-variance REINFORCE policy gradient estimators that do not scale, our adaptive labeling policy is optimized using path-wise policy gradients computed by auto-differentiating through simulated roll-outs. Our framework is agnostic to different uncertainty quantification approaches and highlights the virtue of planning in adaptive labeling. On synthetic and real datasets, we empirically demonstrate even a one-step lookahead policy substantially outperforms active learning-inspired heuristics.","['Model Evaluation', 'Uncertainty Quantification', 'Markov Decision Process', 'Policy Gradient', 'Auto-differentiation']",[],"['Daksh Mittal', 'Yuanzhe Ma', 'Shalmali Joshi', 'Hongseok Namkoong']","['Decision, Risk and Operations (Columbia Business School), Columbia University', 'Columbia University', 'Columbia University', 'LinkedIn']",
https://openreview.net/forum?id=1067784F6e,Security,Data Distribution Valuation,"Data valuation is a class of techniques for quantitatively assessing the value of data for applications like pricing in data marketplaces. Existing data valuation methods define a value for a discrete dataset. However, in many use cases, users are interested in not only the value of the dataset, but that of the distribution from which the dataset was sampled. For example, consider a buyer trying to evaluate whether to purchase data from different vendors. The buyer may observe (and compare) only a small preview sample from each vendor, to decide which vendor's data distribution is most useful to the buyer and purchase. The core question is how should we compare the values of data distributions from their samples? Under a Huber characterization of the data heterogeneity across vendors, we propose a maximum mean discrepancy (MMD)-based valuation method which enables theoretically principled and actionable policies for comparing data distributions from samples. We empirically demonstrate that our method is sample-efficient and effective in identifying valuable data distributions against several existing baselines, on multiple real-world datasets (e.g., network intrusion detection, credit card fraud detection) and downstream applications (classification, regression).","['Data distribution valuation', 'Huber model', 'Maximum mean discrepancy']",[],"['Xinyi Xu', 'Shuaiqi Wang', 'Chuan-Sheng Foo', 'Bryan Kian Hsiang Low', 'Giulia Fanti']","['Department of Computer Science, National University of Singapore', 'CMU, Carnegie Mellon University', 'Centre for Frontier AI Research, A*STAR', 'National University of Singapore', 'Electrical and Computer Engineering, CMU, Carnegie Mellon University']",
https://openreview.net/forum?id=Luxk3z1tSG,Security,Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach,"Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications. However, the scarcity and quality limitations of graph data present challenges to their training process in practical settings. To facilitate the development of effective GNNs, companies and researchers often seek external collaboration. Yet, directly sharing data raises privacy concerns, motivating data owners to train GNNs on their private graphs and share the trained models. Unfortunately, these models may still inadvertently disclose sensitive properties of their training graphs (\textit{e.g.}, average default rate in a transaction network), leading to severe consequences for data owners.  In this work, we study graph property inference attack to identify the risk of sensitive property information leakage from shared models. Existing approaches typically train numerous shadow models for developing such attack, which is computationally intensive and impractical. To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques. Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks. To enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model's error. Subsequently, we propose a novel selection mechanism to ensure that the retained approximated models achieve high diversity and low error. Extensive experiments across six real-world scenarios demonstrate our method's substantial improvement, with average increases of 2.7\% in attack accuracy and 4.1\% in ROC-AUC, while being 6.5$\times$ faster compared to the best baseline.","['graph neural network', 'property inference attack', 'efficiency']",[],"['Hanyang Yuan', 'Jiarong Xu', 'Renhong Huang', 'Mingli Song', 'Chunping Wang', 'Yang Yang']","['Zhejiang University', 'School of Management, Fudan University', 'Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Finvolution Group', 'CS&T, Zhejiang University']",
https://openreview.net/forum?id=x4HMnqs6IE,Privacy & Data Governance,$\text{ID}^3$: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition,"Synthetic face recognition (SFR) aims to generate synthetic face datasets that mimic the distribution of real face data, which allows for training face recognition models in a privacy-preserving manner. Despite the remarkable potential of diffusion models in image generation, current diffusion-based SFR models struggle with generalization to real-world faces. To address this limitation, we outline three key objectives for SFR: (1) promoting diversity across identities (inter-class diversity), (2) ensuring diversity within each identity by injecting various facial attributes (intra-class diversity), and (3) maintaining identity consistency within each identity group (intra-class identity preservation). Inspired by these goals, we introduce a diffusion-fueled SFR model termed $\text{ID}^3$. $\text{ID}^3$ employs an ID-preserving loss to generate diverse yet identity-consistent facial appearances. Theoretically, we show that minimizing this loss is equivalent to maximizing the lower bound of an adjusted conditional log-likelihood over ID-preserving data. This equivalence motivates an ID-preserving sampling algorithm, which operates over an adjusted gradient vector field, enabling the generation of fake face recognition datasets that approximate the distribution of real-world faces. Extensive experiments across five challenging benchmarks validate the advantages of $\text{ID}^3$.",['synthetic face recognition'],[],"['Jianqing Xu', 'Shen Li', 'Jiaying Wu', 'Miao Xiong', 'Ailin Deng', 'Jiazhen Ji', 'Yuge Huang', 'Guodong Mu', 'Wenjie Feng', 'Shouhong Ding', 'Bryan Hooi']","['', 'IDS, national university of singaore, National University of Singapore', 'National University of Singapore', 'Institute of Data Science, National University of Singapore', 'National University of Singapore', 'Tencent Youtu Lab', 'Tencent Youtu Lab', 'Tencent Youtu Lab', 'Institute of Data Science, National Universiasty of Singapore', 'Tencent Youtu Lab', 'Computer Science, National University of Singapore']",
https://openreview.net/forum?id=x2zY4hZcmg,Security,Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning,"Among approaches for provably safe reinforcement learning, Model Predictive Shielding (MPS) has proven effective at complex tasks in continuous, high-dimensional state spaces, by leveraging a *backup policy* to ensure safety when the learned policy attempts to take risky actions. However, while MPS can ensure safety both during and after training, it often hinders task progress due to the conservative and task-oblivious nature of backup policies. This paper introduces *Dynamic Model Predictive Shielding* (DMPS), which optimizes reinforcement learning objectives while maintaining provable safety. DMPS employs a local planner to dynamically select safe recovery actions that maximize both short-term progress as well as long-term rewards. Crucially,  the planner and the neural policy  play a synergistic role in DMPS. When planning recovery actions for ensuring safety,  the planner utilizes the neural policy to estimate long-term rewards, allowing it to *observe* beyond its short-term planning horizon.  Conversely, the neural policy under training learns from the recovery plans proposed by the planner, converging to policies that are both *high-performing* and *safe* in practice. This approach guarantees safety during and after training, with bounded recovery regret that decreases exponentially with planning horizon depth. Experimental results demonstrate that DMPS converges to policies that rarely require shield interventions after training and achieve higher rewards compared to several state-of-the-art baselines.","['Safe Reinforcement Learning', 'Model Predictive Shielding', 'Planning', 'MCTS']",[],"['Arko Banerjee', 'Kia Rahmani', 'Joydeep Biswas', 'Isil Dillig']","['College of Natural Sciences, University of Texas at Austin', 'Computer Science, University of Texas at Austin', 'Computer Science, The University of Texas at Austin', 'University of Texas, Austin']",
https://openreview.net/forum?id=j2wCrWmgMX,Security,Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities,"Uncertainty quantification in Large Language Models (LLMs) is crucial for applications where safety and reliability are important. In particular, uncertainty can be used to improve the trustworthiness of LLMs by detecting factually incorrect model responses, commonly called hallucinations. Critically, one should seek to capture the model's semantic uncertainty, i.e., the uncertainty over the meanings of LLM outputs, rather than uncertainty over lexical or syntactic variations that do not affect answer correctness. To address this problem, we propose Kernel Language Entropy (KLE), a novel method for uncertainty estimation in white- and black-box LLMs. KLE defines positive semidefinite unit trace kernels to encode the semantic similarities of LLM outputs and quantifies uncertainty using the von Neumann entropy. It considers pairwise semantic dependencies between answers (or semantic clusters), providing more fine-grained uncertainty estimates than previous methods based on hard clustering of answers. We theoretically prove that KLE generalizes the previous state-of-the-art method called semantic entropy and empirically demonstrate that it improves uncertainty quantification performance across multiple natural language generation datasets and LLM architectures.","['uncertainty quantification', 'LLMs']",[],"['Alexander V Nikitin', 'Jannik Kossen', 'Yarin Gal', 'Pekka Marttinen']","['Aalto University', 'Facebook', 'University of Oxford', 'Department of Computer Science, Aalto University']",
https://openreview.net/forum?id=l04i6dPMxK,Fairness & Bias,Bandits with Abstention under Expert Advice,"We study the classic problem of prediction with expert advice under bandit feedback.  Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the CBA (Confidence-rated Bandits with Abstentions) algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. Our problem can be construed as the aggregation of confidence-rated predictors, with the learner having the option to abstain from play. We are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists, we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). We discuss how CBA can be applied to the problem of adversarial contextual bandits with the option of abstaining from selecting any action. We are able to leverage a wide range of inductive biases, outperforming previous approaches both theoretically and in preliminary experimental analysis. Additionally, we achieve a reduction in runtime from quadratic to almost linear in the number of contexts for the specific case of metric space contexts.","['Multi-armed bandits', 'Expert advice', 'Abstention', 'Contextual bandits']",[],"['Stephen Pasteris', 'Alberto Rumi', 'Maximilian Thiessen', 'Shota Saito', 'Atsushi Miyauchi', 'Fabio Vitale', 'Mark Herbster']","['Alan Turing Institute', 'University of Milan', 'TU Wien', 'Indeed', 'CENTAI Institute', '', 'Computer Science, University College London']",
https://openreview.net/forum?id=CMc0jMY0Wr,Privacy & Data Governance,Optimal Private and Communication Constraint Distributed Goodness-of-Fit Testing for Discrete Distributions in the Large Sample Regime,"We study distributed goodness-of-fit testing for discrete distribution under bandwidth and differential privacy constraints. Information constraint distributed goodness-of-fit testing is a problem that has received considerable attention recently. The important case of discrete distributions is theoretically well understood in the classical case where all data is available in one ""central"" location. In a federated setting, however, data is distributed across multiple ""locations"" (e.g. servers) and cannot readily be shared due to e.g. bandwidth or privacy constraints that each server needs to satisfy. We show how recently derived results for goodness-of-fit testing for the mean of a multivariate Gaussian model extend to the discrete distributions, by leveraging Le Cam's theory of statistical equivalence. In doing so, we derive matching minimax upper- and lower-bounds for the goodness-of-fit testing for discrete distributions under bandwidth or privacy constraints in the regime where number of samples held locally are large.","['hypothesis testing', 'federated learning', 'distributed inference', 'goodness-of-fit', 'differential privacy', 'communication constraint']",[],['Lasse Vuursteen'],"['Department of Statistics, The Wharton School, University of Pennsylvania']",
https://openreview.net/forum?id=G2dYZJO4BE,Security,Achievable distributional robustness when the robust risk is only partially identified,"In safety-critical applications, machine learning models should generalize well under worst-case distribution shifts, that is, have a small robust risk. Invariance-based algorithms can provably take advantage of structural assumptions on the shifts when the training distributions are heterogeneous enough to identify the robust risk. However, in practice, such identifiability conditions are rarely satisfied – a scenario so far underexplored in the theoretical literature. In this paper, we aim to fill the gap and propose to study the more general setting of partially identifiable robustness. In particular, we define a new risk measure, the identifiable robust risk, and its corresponding (population) minimax quantity that is an algorithm-independent measure for the best achievable robustness under partial identifiability. We introduce these concepts broadly, and then study them within the framework of linear structural causal models for concreteness of the presentation. We use the introduced minimax quantity to show how previous approaches provably achieve suboptimal robustness in the partially identifiable case. We confirm our findings through empirical simulations and real-world experiments and demonstrate how the test error of existing robustness methods grows increasingly suboptimal as the proportion of previously unseen test directions increases.","['distributional robustness', 'domain generalization', 'causal inference', 'partial identification']",[],"['Julia Kostin', 'Nicola Gnecco', 'Fanny Yang']","['D-INFK, ETHZ - ETH Zurich', 'University College London, University of London', 'Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=4Yj7L9Kt7t,Privacy & Data Governance,Taming Heavy-Tailed Losses in Adversarial Bandits and the Best-of-Both-Worlds Setting,"In this paper, we study the multi-armed bandit problem in the best-of-both-worlds (BOBW) setting with heavy-tailed losses, where the losses can be negative and unbounded but have $(1+v)$-th raw moments bounded by $u^{1+v}$ for some known $u>0$ and $v\in(0,1]$. Specifically, we consider the BOBW setting where the underlying environment could be either (oblivious) adversarial (i.e., the loss distribution can change arbitrarily over time) or stochastic (i.e., the loss distribution is fixed over time) and is unknown to the decision-maker a prior, and propose an algorithm that achieves a $T^{\frac{1}{1+v}}$-type worst-case (pseudo-)regret in the adversarial regime and a $\log T$-type gap-dependent regret in the stochastic regime, where $T$ is the time horizon. Compared to the state-of-the-art results, our algorithm offers stronger \emph{high-probability} regret guarantees rather than expected regret guarantees, and more importantly, relaxes a strong technical assumption on the loss distribution. This assumption is needed even for the weaker expected regret obtained in the literature and is generally hard to verify in practice. As a byproduct, relaxing this assumption leads to the first near-optimal regret result for heavy-tailed bandits with Huber contamination in the adversarial regime, in contrast to all previous works focused on the (easier) stochastic regime. Our result also implies a high-probability BOBW regret guarantee when the bounded true losses are protected with pure Local Differential Privacy (LDP), while the existing work ensures the (weaker) \emph{approximate} LDP with the regret bounds in expectation only.","['bandits', 'regret minimization', 'the best of both worlds', 'heavy tails', 'online learning']",[],"['Duo Cheng', 'Xingyu Zhou', 'Bo Ji']","['Virginia Polytechnic Institute and State University', 'ECE, Wayne State University', 'Virginia Tech']",
https://openreview.net/forum?id=lV1wGHKd5x,Transparency & Explainability,Listenable Maps for Zero-Shot Audio Classifiers,"Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Zero-Shot Audio Classifiers), which, to the best of our knowledge, is the first decoder-based post-hoc explanation method for explaining the decisions of zero-shot audio classifiers. The proposed method utilizes a novel loss function that aims to closely reproduce the original similarity patterns between text-and-audio pairs in the generated explanations. We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context. Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.","['Zero shot audio classifiers', 'Posthoc explanations']",[],"['Francesco Paissan', 'Luca Della Libera', 'Mirco Ravanelli', 'Cem Subakan']","['Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Mila - Quebec Artificial Intelligence Institute', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'Universite Laval, Université Laval']",
https://openreview.net/forum?id=TMlGQw7EbC,Transparency & Explainability,Markov Equivalence and Consistency in Differentiable Structure Learning,"Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers (without resorting to approximations such as Gumbel-Softmax), thus paving the way for differentiable structure learning under general models and losses. Open-source code is available at \url{https://github.com/duntrain/dagrad}.","['structure learning', 'graphical models', 'causality', 'non-convex optimization', 'regularization']",[],"['Chang Deng', 'Kevin Bello', 'Pradeep Kumar Ravikumar', 'Bryon Aragam']","['University of Chicago', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University', 'University of Chicago']",
https://openreview.net/forum?id=J0Itri0UiN,Fairness & Bias,Counterfactual Fairness by Combining Factual and Counterfactual Predictions,"In high-stakes domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns.  This work focuses on Counterfactual Fairness (CF), which posits that an ML model's outcome on any individual should remain unchanged if they had belonged to a different demographic group. Previous works have proposed methods that guarantee CF.  Notwithstanding, their effects on the model's predictive performance remain largely unclear. To fill this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner.  We first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one with a minimal loss of performance. By analyzing the excess risk incurred by perfect CF, we quantify this inherent trade-off.  Further analysis on our method's performance with access to only incomplete causal knowledge is also conducted.  Built upon this, we propose a practical algorithm that can be applied in such scenarios.  Experiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.","['Counterfactual Fairness', 'Fairness', 'Trustworthy ML']",[],"['Zeyu Zhou', 'Tianci Liu', 'Ruqi Bai', 'Jing Gao', 'Murat Kocaoglu', 'David I. Inouye']","['Purdue University', 'ECE, Purdue University', 'Electrical and Computer Engineering, Purdue University', 'Purdue University', 'Purdue University', 'Purdue University']",
https://openreview.net/forum?id=VqxODXhU4k,Transparency & Explainability,Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients,"Instrumental variables (IVs) provide a powerful strategy for identifying causal effects in the presence of unobservable confounders. Within the nonparametric setting (NPIV), recent methods have been based on nonlinear generalizations of Two-Stage Least Squares and on minimax formulations derived from moment conditions or duality. In a novel direction, we show how to formulate a functional stochastic gradient descent algorithm to tackle NPIV regression by directly minimizing the populational risk. We provide theoretical support in the form of bounds on the excess risk, and conduct numerical experiments showcasing our method's superior stability and competitive performance relative to current state-of-the-art alternatives. This algorithm enables flexible estimator choices, such as neural networks or kernel based methods, as well as non-quadratic loss functions, which may be suitable for structural equations beyond the setting of continuous outcomes and additive noise. Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature.","['Nonparametric Instrumental Variables', 'Stochastic Gradients', 'RKHS', 'Binary response', 'Deep Learning', 'Causality']",[],"['Yuri Fonseca', 'Caio Peixoto', 'Yuri Saporito']","['Columbia University', 'School of Applied Mathematics, Fundação Getúlio Vargas (FGV)', 'FGV EMAp']",
https://openreview.net/forum?id=lKnl4CLhhS,Privacy & Data Governance,Efficient and Private Marginal Reconstruction with Local Non-Negativity,"Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people. Many differentially private algorithms for query release and synthetic data contain steps that reconstruct answers to queries from answers to other queries that have been measured privately.  Reconstruction is an important subproblem for such mechanisms to economize the privacy budget, minimize error on reconstructed answers, and allow for scalability to high-dimensional datasets. In this paper, we introduce a principled and efficient postprocessing method ReM (Residuals-to-Marginals) for reconstructing answers to marginal queries. Our method builds on recent work on efficient mechanisms for marginal query release, based on making measurements using a residual query basis that admits efficient pseudoinversion, which is an important primitive used in reconstruction. An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers.  We demonstrate the utility of ReM and GReM-LNN by applying them to improve existing private query answering mechanisms.","['differential privacy', 'query release', 'synthetic data']",[],"['Brett Mullins', 'Miguel Fuentes', 'Yingtai Xiao', 'Daniel Kifer', 'Cameron N Musco', 'Daniel Sheldon']","['University of Massachusetts at Amherst', 'University of Massachusetts at Amherst', 'ByteDance Inc.', 'Computer Science & Engineering, Pennsylvania State University', 'University of Massachusetts, Amherst', 'University of Massachusetts at Amherst']",
https://openreview.net/forum?id=1e3MOwHSIX,Fairness & Bias,MAGNET: Improving the Multilingual Fairness of Language Models with Adaptive Gradient-Based Tokenization,"In multilingual settings, non-Latin scripts and low-resource languages are usually disadvantaged in terms of language models’ utility, efficiency, and cost. Specifically, previous studies have reported multiple modeling biases that the current tokenization algorithms introduce to non-Latin script languages, the main one being over-segmentation. In this work, we propose MAGNET— multilingual adaptive gradient-based tokenization—to reduce over-segmentation via adaptive gradient-based subword tokenization. MAGNET learns to predict segment boundaries between byte tokens in a sequence via sub-modules within the model, which act as internal boundary predictors (tokenizers). Previous gradient-based tokenization methods aimed for uniform compression across sequences by integrating a single boundary predictor during training and optimizing it end-to-end through stochastic reparameterization alongside the next token prediction objective. However, this approach still results in over-segmentation for non-Latin script languages in multilingual settings. In contrast, MAGNET offers a customizable architecture where byte-level sequences are routed through language-script-specific predictors, each optimized for its respective language script. This modularity enforces equitable segmentation granularity across different language scripts compared to previous methods. Through extensive experiments, we demonstrate that in addition to reducing segmentation disparities, MAGNET also enables faster language modeling and improves downstream utility.","['tokenization', 'multilingual LMs', 'over-segmentation', 'fariness']",[],"['Orevaoghene Ahia', 'Sachin Kumar', 'Hila Gonen', 'Valentin Hofmann', 'Tomasz Limisiewicz', 'Yulia Tsvetkov', 'Noah A. Smith']","['', 'Computer Science and Engineering, Ohio State University, Columbus', 'University of Washington', 'Allen Institute for Artificial Intelligence', 'Charles University', 'Department of Computer Science, University of Washington', 'Computer Science & Engineering, University of Washington']",
https://openreview.net/forum?id=Y1fPxGevQj,Transparency & Explainability,xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology,"Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology.","['computational pathology', 'multiple instance learning', 'explainable AI', 'layer-wise relevance propagation', 'knowledge discovery']",[],"['Julius Hense', 'Mina Jamshidi Idaji', 'Oliver Eberle', 'Thomas Schnake', 'Jonas Dippel', 'Laure Ciernik', 'Oliver Buchstab', 'Andreas Mock', 'Frederick Klauschen', 'Klaus Robert Muller']","['Machine Learning Group, Technische Universität Berlin', 'Machine learning group, Technische Universität Berlin', 'Technische Universität Berlin', 'Technische Universität Berlin', 'Aignostics GmbH', 'Technische Universität Berlin', 'Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', 'Google']",
https://openreview.net/forum?id=wl44W8xpc7,Fairness & Bias,Learning Infinitesimal Generators of Continuous Symmetries from Data,"Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at \url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.","['Symmetry Discovery', 'Geometric Deep Learning', 'Lie Point Symmetry', 'Neural PDE Solver']",[],"['Gyeonghoon Ko', 'Hyunsu Kim', 'Juho Lee']","['Graduate School of AI, Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science & Technology']",
https://openreview.net/forum?id=aVh9KRZdRk,Transparency & Explainability,Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks,"Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a  x + b  y \text{ mod } p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is *transient*, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.","['In-Context Learning', 'Grokking', 'Modular Arithmetic', 'Interpretability']",[],"['Tianyu He', 'Darshil Doshi', 'Aritra Das', 'Andrey Gromov']","['Physics, University of Maryland, College Park', 'Physics, University of Maryland, College Park', 'University of Maryland, College Park', 'Facebook']",
https://openreview.net/forum?id=yvUHnBkCzd,Privacy & Data Governance,Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning,"Federated learning is renowned for its efficacy in distributed model training, ensuring that users, called clients, retain data privacy by not disclosing their data to the central server that orchestrates collaborations. Most previous work on federated learning assumes that clients possess static batches of training data. However, clients may also need to make real-time predictions on streaming data in non-stationary environments. In such dynamic environments, employing pre-trained models may be inefficient, as they struggle to adapt to the constantly evolving data streams. To address this challenge, clients can fine-tune models online, leveraging their observed data to enhance performance. Despite the potential benefits of client participation in federated online model fine-tuning, existing analyses have not conclusively demonstrated its superiority over local model fine-tuning. To bridge this gap, the present paper develops a novel personalized federated learning algorithm, wherein each client constructs a personalized model by combining a locally fine-tuned model with multiple federated models learned by the server over time. Theoretical analysis and experiments on real datasets corroborate the effectiveness of this approach for real-time predictions and federated model fine-tuning.","['Federated Learning', 'Personalized Models', 'Real-time Predictions']",[],"['Pouya M. Ghari', 'Yanning Shen']","['University of California, Irvine', 'University of California, Irvine']",
https://openreview.net/forum?id=V0JvwCQlJe,Fairness & Bias,FairWire: Fair Graph Generation,"Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for their deployment in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel fairness regularizer that offers a versatile use. Faced with the bias amplification in graph generation models brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model. Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs.","['Trustworthy ML', 'Learning over Graphs', 'Fair ML over graphs', 'Fair Graph Generation']",[],"['Oyku Deniz Kose', 'Yanning Shen']","['University of California, Irvine', 'University of California, Irvine']",
https://openreview.net/forum?id=SXy1nVGyO7,Fairness & Bias,On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution,"The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of _hybrid deep generative models (hybrid-DGMs)_  that integrates known physics-based mathematical expressions in neural generative models. To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component. The identifiability of these hybrid-DGMs, however, has not yet been theoretically probed or established. How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs? What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability? This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs. On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs.",['hybrid modeling; identifiability; meta-learning'],[],"['Yubo Ye', 'Maryam Toloubidokhti', 'Sumeet Vadhavkar', 'Xiajun Jiang', 'Huafeng Liu', 'Linwei Wang']","['Zhejiang University', 'Rochester Institute of Technology', 'Computing and Information Sciences , Rochester Institute of Technology', 'University of Memphis', 'Optical Department, Zhejiang University', 'Rochester Institute of Technology']",
https://openreview.net/forum?id=nv2Qt5cj1a,Security,Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",['Membership inference attack; Large Vision-Language Models'],[],"['Zhan Li', 'Yongtao Wu', 'Yihang Chen', 'Francesco Tonin', 'Elias Abad Rocamora', 'Volkan Cevher']","['IC, EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'University of California, Los Angeles', 'EPFL - EPF Lausanne', 'EPFL - EPF Lausanne', 'EPFL - EPF Lausanne']",
https://openreview.net/forum?id=pU0z2sNM1M,Transparency & Explainability,Causal Dependence Plots,"To use artificial intelligence and machine learning models wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how a model's predicted outcome depends on changes in a given predictor *along with consequent causal changes in other predictor variables*. Crucially, this differs from standard methods based on independence or holding other predictors constant, such as regression coefficients or Partial Dependence Plots (PDPs). Our explanatory framework generalizes PDPs, including them as a special case, as well as a variety of other interpretive plots that show, for example, the total, direct, and indirect effects of causal mediation. We demonstrate with simulations and real data experiments how CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to applications like scientific machine learning and algorithmic fairness.","['Interpretable machine learning', 'interpretability', 'explainable AI', 'explainability', 'causality', 'partial dependence plots', 'total dependence plots', 'model agnostic explanations']",[],"['Joshua R. Loftus', 'Lucius E.J. Bynum', 'Sakina Hansen']","['London School of Economics', '', 'London School of Economics and Political Science, University of London']",
https://openreview.net/forum?id=2UJLv3KPGO,Fairness & Bias,Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions,"As machine learning (ML) models are increasingly used in social domains to make consequential decisions about humans, they often have the power to reshape data distributions. Humans, as strategic agents, continuously adapt their behaviors in response to the learning system. As populations change dynamically, ML systems may need frequent updates to ensure high performance. However, acquiring high-quality *human-annotated* samples can be highly challenging and even infeasible in social domains. A common practice to address this issue is using the model itself to annotate unlabeled data samples. This paper investigates the long-term impacts when ML models are retrained with *model-annotated* samples when they incorporate human strategic responses. We first formalize the interactions between strategic agents and the model and then analyze how they evolve under such dynamic interactions. We find that agents are increasingly likely to receive positive decisions as the model gets retrained, whereas the proportion of agents with positive labels may decrease over time. We thus propose a *refined retraining process* to stabilize the dynamics. Last, we examine how algorithmic fairness can be affected by these retraining processes and find that enforcing common fairness constraints at every round may not benefit the disadvantaged group in the long run. Experiments on (semi-)synthetic and real data validate the theoretical findings.","['Strategic Classification', 'Long-term Fairness']",[],"['Tian Xie', 'Xueru Zhang']","['CSE, Ohio State University, Columbus', 'Ohio State University']",
https://openreview.net/forum?id=dBE8KHdMFs,Fairness & Bias,ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence,"Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can process data without the limitation of time intervals. They have advantages in learning and understanding the evolution of complex real dynamics. Many previous works have focused on NODEs in concise forms, while numerous physical systems taking straightforward forms in fact belong to their more complex quasi-classes, thus appealing to a class of general NODEs with high scalability and flexibility to model those systems. This however may result in intricate nonlinear properties. In this paper, we introduce ControlSynth Neural ODEs (CSODEs). We show that despite their highly nonlinear nature, convergence can be guaranteed via tractable linear inequalities. In the composition of CSODEs, we introduce an extra control term for learning the potential simultaneous capture of dynamics at different scales, which could be particularly useful for partial differential equation-formulated systems. Finally, we compare several representative NNs with CSODEs on important physical dynamics under the inductive biases of CSODEs, and illustrate that CSODEs have better learning and predictive abilities in these settings.","['Neural ODEs', 'Differential Equations', 'Dynamical Systems', 'Deep Learning']",[],"['Wenjie Mei', 'Dongzhe Zheng', 'Shihua Li']","['Southeast University', 'University of Cambridge', 'School of Automation, Southeast University']",
https://openreview.net/forum?id=a3cauWMXNV,Fairness & Bias,Fair GLASSO: Estimating Fair Graphical Models with Unbiased Statistical Behavior,"We propose estimating Gaussian graphical models (GGMs) that are fair with respect to sensitive nodal attributes. Many real-world models exhibit unfair discriminatory behavior due to biases in data. Such discrimination is known to be exacerbated when data is equipped with pairwise relationships encoded in a graph. Additionally, the effect of biased data on graphical models is largely underexplored. We thus introduce fairness for graphical models in the form of two bias metrics to promote balance in statistical similarities across nodal groups with different sensitive attributes. Leveraging these metrics, we present Fair GLASSO, a regularized graphical lasso approach to obtain sparse Gaussian precision matrices with unbiased statistical dependencies across groups. We also propose an efficient proximal gradient algorithm to obtain the estimates. Theoretically, we express the tradeoff between fair and accurate estimated precision matrices. Critically, this includes demonstrating when accuracy can be preserved in the presence of a fairness regularizer. On top of this, we study the complexity of Fair GLASSO and demonstrate that our algorithm enjoys a fast convergence rate. Our empirical validation includes synthetic and real-world simulations that illustrate the value and effectiveness of our proposed optimization problem and iterative algorithm.","['Graphical model', 'fairness', 'graph learning', 'graphical lasso']",[],"['Madeline Navarro', 'Samuel Rey', 'Andrei Buciulea', 'Antonio Marques', 'Santiago Segarra']","['Department of Electrical and Computer Engineering, Rice University', 'Universidad Rey Juan Carlos', 'Universidad Rey Juan Carlos', 'King Juan Carlos University', 'Rice University']",
https://openreview.net/forum?id=1mAaewThcz,Fairness & Bias,Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks,"Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., privileging celebrities and other high-degree actors in social networks during social and content recommendation. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias.","['graph learning', 'fairness', 'degree']",[],"['Arjun Subramonian', 'Jian Kang', 'Yizhou Sun']","['Computer Science, University of California, Los Angeles', 'University of Rochester', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=vjCFnYTg67,Security,Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature,"Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.","['Large language model', 'Text Provenance', 'Spoofing attacks']",[],"['Tong Zhou', 'Xuandong Zhao', 'Xiaolin Xu', 'Shaolei Ren']","['Northeastern University', 'University of California, Berkeley', '', '']",
https://openreview.net/forum?id=MvjLRFntW6,Transparency & Explainability,A Concept-Based Explainability Framework for Large Multimodal Models,"Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as ``multi-modal concepts''.  We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually. Our implementation is publicly available: https://github.com/mshukor/xl-vlms.","['Large multimodal models', 'interpretability', 'concept extraction', 'multimodal concepts', 'multimodal representations', 'vision-language models', 'vision-language interpretations']",[],"['Jayneel Parekh', 'Pegah KHAYATAN', 'Mustafa Shukor', 'Alasdair Newson', 'Matthieu Cord']","['ISIR, Sorbonne Université', 'ISIR, MLIA, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Télécom ParisTech', 'Sorbonne Université']",
https://openreview.net/forum?id=ZJ2ONmSgCS,Security,DiffHammer: Rethinking the Robustness of Diffusion-Based Adversarial Purification,"Diffusion-based purification has demonstrated impressive robustness as an adversarial defense. However, concerns exist about whether this robustness arises from insufficient evaluation. Our research shows that EOT-based attacks face gradient dilemmas due to global gradient averaging, resulting in ineffective evaluations. Additionally, 1-evaluation underestimates resubmit risks in stochastic defenses. To address these issues, we propose an effective and efficient attack named DiffHammer. This method bypasses the gradient dilemma through selective attacks on vulnerable purifications, incorporating $N$-evaluation into loops and using gradient grafting for comprehensive and efficient evaluations. Our experiments validate that DiffHammer achieves effective results within 10-30 iterations, outperforming other methods. This calls into question the reliability of diffusion-based purification after mitigating the gradient dilemma and scrutinizing its resubmit risk.","['adaptive adversarial attack', 'adversarial purification', 'diffusion']",[],"['Kaibo Wang', 'Xiaowen Fu', 'Yuxuan Han', 'Yang Xiang']","['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology', '', '']",
https://openreview.net/forum?id=KqbLzSIXkm,Fairness & Bias,DiMSUM: Diffusion Mamba - A Scalable and Unified Spatial-Frequency Method for Image Generation,"We introduce a novel state-space architecture for diffusion models, effectively harnessing spatial and frequency information to enhance the inductive bias towards local features in input images for image generation tasks. While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data. Our method demonstrates that integrating wavelet transformation into Mamba enhances the local structure awareness of visual inputs and better captures long-range relations of frequencies by disentangling them into wavelet subbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation. Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships. Through extensive experiments on standard benchmarks, our method demonstrates superior results compared to DiT and DIFFUSSM, achieving faster training convergence and delivering high-quality outputs. The codes and pretrained models are released at https://github.com/VinAIResearch/DiMSUM.git.","['Diffusion models', 'Mamba', 'Wavelet transformation']",[],"['Hao Phung', 'Quan Dao', 'Trung Tuan Dao', 'Hoang Phan', 'Dimitris N. Metaxas', 'Anh Tuan Tran']","['Computer Science, Cornell University', 'Computer Science, Rutgers University', 'VinAI Research', 'New York University', 'Computer Science, Rutgers University', 'VinAI Research']",
https://openreview.net/forum?id=IbIB8SBKFV,Security,Improving Alignment and Robustness with Circuit Breakers,"AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with ""circuit breakers."" Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image ""hijacks"" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.","['alignment', 'adversarial robustness', 'adversarial attacks', 'harmfulness', 'security', 'reliability', 'ML safety', 'AI safety']",[],"['Andy Zou', 'Long Phan', 'Justin Wang', 'Derek Duenas', 'Maxwell Lin', 'Maksym Andriushchenko', 'J Zico Kolter', 'Matt Fredrikson', 'Dan Hendrycks']","['CMU, Carnegie Mellon University', 'Center for AI Safety', 'Carnegie Mellon University', 'CMU, Carnegie Mellon University', 'University of California, Berkeley', 'EPFL - EPF Lausanne', 'Machine Learning, Carnegie Mellon University', 'Gray Swan AI', 'UC Berkeley']",
https://openreview.net/forum?id=1wxFznQWhp,Transparency & Explainability,Delving into the Reversal Curse: How Far Can Large Language Models Generalize?,"While large language models (LLMs) showcase unprecedented capabilities, they also exhibit certain inherent limitations when facing seemingly trivial tasks.  A prime example is the recently debated ""reversal curse"", which surfaces when models, having been trained on the fact ""A is B"", struggle to generalize this knowledge to infer that ""B is A"". In this paper, we examine the manifestation of the reversal curse across various tasks and delve into both the generalization abilities and the problem-solving mechanisms of LLMs. This investigation leads to a series of significant insights: (1) LLMs are able to generalize to ""B is A"" when both A and B are presented in the context as in the case of a multiple-choice question. (2) This generalization ability is highly correlated to the structure of the fact ""A is B"" in the training documents. For example, this generalization only applies to biographies structured in ""[Name] is [Description]"" but not to ""[Description] is [Name]"". (3) We propose and verify the hypothesis that LLMs possess an inherent bias in fact recalling during knowledge application, which explains and underscores the importance of the document structure to successful learning. (4) The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone. Based on these intriguing findings, our work not only presents a novel perspective for interpreting LLMs' generalization abilities from their intrinsic working mechanism but also provides new insights for the development of more effective learning methods for LLMs.","['Large Language Models', 'Interpretability', 'Reversal Curse', 'Knowledge Injection']",[],"['Zhengkai Lin', 'Zhihang Fu', 'Kai Liu', 'Liang Xie', 'Binbin Lin', 'Wenxiao Wang', 'Deng Cai', 'Yue Wu', 'Jieping Ye']","['Zhejiang University', 'Alibaba Group', 'National University of Singapore', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Alibaba Group', 'Alibaba Group']",
https://openreview.net/forum?id=28bFUt6rUY,Fairness & Bias,EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models,"Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.","['Text-to-Image Generation', 'Large Vision-Language Models', 'Diffusion Models']",[],"['Rui Zhao', 'Hangjie Yuan', 'Yujie Wei', 'Shiwei Zhang', 'Yuchao Gu', 'Lingmin Ran', 'Xiang Wang', 'Jay Zhangjie Wu', 'David Junhao Zhang', 'Yingya Zhang', 'Mike Zheng Shou']","['National University of Singapore', 'DAMO Academy, Alibaba Group', 'Fudan University', 'Tongyi Lab, Alibaba Group', 'Electrical and Computer Engineering, National University of Singapore', 'Electrical and Computing Engineering, national university of singaore, National University of Singapore', 'School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'National University of Singapore', 'School of Computing, National University of Singapore', 'Alibaba Group', 'National University of Singapore']",
https://openreview.net/forum?id=2Inwtjvyx8,Security,Revisiting Adversarial Patches for Designing Camera-Agnostic Attacks against Person Detection,"Physical adversarial attacks can deceive deep neural networks (DNNs), leading to erroneous predictions in real-world scenarios. To uncover potential security risks, attacking the safety-critical task of person detection has garnered significant attention. However, we observe that existing attack methods overlook the pivotal role of the camera, involving capturing real-world scenes and converting them into digital images, in the physical adversarial attack workflow. This oversight leads to instability and challenges in reproducing these attacks. In this work, we revisit patch-based attacks against person detectors and introduce a camera-agnostic physical adversarial attack to mitigate this limitation. Specifically, we construct a differentiable camera Image Signal Processing (ISP) proxy network to compensate for the physical-to-digital transition gap. Furthermore, the camera ISP proxy network serves as a defense module, forming an adversarial optimization framework with the attack module. The attack module optimizes adversarial patches to maximize effectiveness, while the defense module optimizes the conditional parameters of the camera ISP proxy network to minimize attack effectiveness. These modules engage in an adversarial game, enhancing cross-camera stability. Experimental results demonstrate that our proposed Camera-Agnostic Patch (CAP) attack effectively conceals persons from detectors across various imaging hardware, including two distinct cameras and four smartphones.","['Physical Adversarial Attack', 'Person Detection', 'Adversarial Patch', 'Camera ISP']",[],"['Hui Wei', 'Zhixiang Wang', 'Kewei Zhang', 'Jiaqi Hou', 'Yuanwei Liu', 'Hao Tang', 'Zheng Wang']","['Wuhan University', 'CyberAgent AI Lab', 'School of Computer Science, Wuhan University', 'Wuhan University', 'Wuhan University', 'Peking University', 'Wuhan University']",
https://openreview.net/forum?id=2TktDpGqNM,Transparency & Explainability,Overcoming Common Flaws in the Evaluation of Selective Classification Systems,"Selective Classification, wherein models can reject low-confidence predictions, promises reliable translation of machine-learning based classification systems to real-world scenarios such as clinical diagnostics. While current evaluation of these systems typically assumes fixed working points based on pre-defined rejection thresholds, methodological progress requires benchmarking the general performance of systems akin to the $\mathrm{AUROC}$ in standard classification. In this work, we define 5 requirements for multi-threshold metrics in selective classification regarding task alignment, interpretability, and flexibility, and show how current approaches fail to meet them. We propose the Area under the Generalized Risk Coverage curve ($\mathrm{AUGRC}$), which meets all requirements and can be directly interpreted as the average risk of undetected failures. We empirically demonstrate the relevance of $\mathrm{AUGRC}$ on a comprehensive benchmark spanning 6 data sets and 13 confidence scoring functions. We find that the proposed metric substantially changes metric rankings on 5 out of the 6 data sets.","['Selective Classification', 'Method evaluation', 'Failure Detection']",[],"['Jeremias Traub', 'Till J. Bungert', 'Carsten T. Lüth', 'Michael Baumgartner', 'Klaus Maier-Hein', 'Lena Maier-hein', 'Paul F Jaeger']","['', 'Deutsches Krebsforschungszentrum', 'Ruprecht-Karls-Universität Heidelberg', 'Siemens Healthineers', 'German Cancer Research Center', 'IMSY , Deutsches Krebsforschungszentrum', 'Google']",
https://openreview.net/forum?id=2bdSnxeQcW,Fairness & Bias,Exclusively Penalized Q-learning for Offline Reinforcement Learning,"Constraint-based offline reinforcement learning (RL) involves policy constraints or imposing penalties on the value function to mitigate overestimation errors caused by distributional shift. This paper focuses on a limitation in existing offline RL methods with penalized value function, indicating the potential for underestimation bias due to unnecessary bias introduced in the value function. To address this concern, we propose Exclusively Penalized Q-learning (EPQ), which reduces estimation bias in the value function by selectively penalizing states that are prone to inducing estimation errors. Numerical results show that our method significantly reduces underestimation bias and improves performance in various offline control tasks compared to other offline RL methods.","['Deep reinforcement learning', 'offline RL', 'Q-learning', 'overestimation reduction']",[],"['Junghyuk Yeom', 'Yonghyeon Jo', 'Jeongmo Kim', 'Sanghyeon Lee', 'Seungyul Han']","['Artificiall Intelligence, Ulsan National Institute of Science and Technology', 'Graduate School of Artificial Intelligence, Ulsan National Institute of Science and Technology', 'Artificial Intelligence, Ulsan National Institute of Science and Technology', 'Ulsan National Institute of Science and Technology', 'Ulsan National Institute of Science and Technology']",
https://openreview.net/forum?id=MDgn9aazo0,Transparency & Explainability,From Similarity to Superiority: Channel Clustering for Time Series Forecasting,"Time series forecasting has attracted significant attention in recent decades.     Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy.     There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adaptable \textbf{C}hannel \textbf{C}lustering \textbf{M}odule (CCM). CCM dynamically groups channels characterized by intrinsic similarities and leverages cluster information instead of individual channel identities, combining the best of CD and CI worlds. Extensive experiments on real-world datasets demonstrate that CCM can (1) boost the performance of CI and CD models by an average margin of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2) enable zero-shot forecasting with mainstream time series forecasting models; (3) uncover intrinsic time series patterns among channels and improve interpretability of complex time series models.","['Deep Learning', 'Time Series Forecasting']",[],"['Jialin Chen', 'Jan Eric Lenssen', 'Aosong Feng', 'Weihua Hu', 'Matthias Fey', 'Leandros Tassiulas', 'Jure Leskovec', 'Rex Ying']","['Yale University', 'D2: Computer Vision and Machine Learning, Saarland Informatics Campus, Max-Planck Institute', 'Yale University', 'Stanford University', 'TU Dortmund University', 'Yale University', 'Department of Computer Science, Stanford University', 'Department of Computer Science, Yale University']",
https://openreview.net/forum?id=2vywag2lVC,Security,Last-Iterate Global Convergence of Policy Gradients for Constrained Reinforcement Learning,"*Constrained Reinforcement Learning* (CRL) tackles sequential decision-making problems where agents are required to achieve goals by maximizing the expected return while meeting domain-specific constraints, which are often formulated on expected costs. In this setting, *policy-based* methods are widely used since they come with several advantages when dealing with continuous-control problems. These methods search in the policy space with an *action-based* or *parameter-based* exploration strategy, depending on whether they learn directly the parameters of a stochastic policy or those of a stochastic hyperpolicy. In this paper, we propose a general framework for addressing CRL problems via *gradient-based primal-dual* algorithms, relying on an alternate ascent/descent scheme with dual-variable regularization. We introduce an exploration-agnostic algorithm, called C-PG, which exhibits global last-iterate convergence guarantees under (weak) gradient domination assumptions, improving and generalizing existing results. Then, we design C-PGAE and C-PGPE, the action-based and the parameter-based versions of C-PG, respectively, and we illustrate how they naturally extend to constraints defined in terms of *risk measures* over the costs, as it is often requested in safety-critical scenarios. Finally, we numerically validate our algorithms on constrained control problems, and compare them with state-of-the-art baselines, demonstrating their effectiveness.","['Constrained Reinforcement Learning', 'Last-Iterate Convergence', 'Global Convergence', 'Policy Gradients']",[],"['Alessandro Montenegro', 'Marco Mussi', 'Matteo Papini', 'Alberto Maria Metelli']","['Politecnico di Milano', 'Politecnico di Milano', 'Polytechnic Institute of Milan', 'Politecnico di Milano']",
https://openreview.net/forum?id=2wfd3pti8v,Fairness & Bias,Automated Efficient Estimation using Monte Carlo Efficient Influence Functions,"Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. We introduce \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and functionals that previously required rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we present a novel capstone example using MC-EIF for optimal portfolio selection.","['Efficient influence function', 'semiparametric statistics', 'double robustness', 'automatic differentiation']",[],"['Raj Agrawal', 'Sam Witty', 'Andy Zane', 'Eli Bingham']","['Massachusetts Institute of Technology', 'Basis Research Institute', 'Basis', 'Broad Institute']",
https://openreview.net/forum?id=3A5VgiH5Pw,Transparency & Explainability,Towards Multi-dimensional Explanation Alignment for Medical Classification,"The lack of interpretability in the field of medical image analysis has significant ethical and legal implications. Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, and issues related to efficiency. To address these limitations, we propose a novel framework called Med-MICN (Medical Multi-dimensional Interpretable Concept Network). Med-MICN provides interpretability alignment for various angles, including neural symbolic reasoning, concept semantics, and saliency maps, which are superior to current interpretable methods. Its advantages include high prediction accuracy, interpretability across multiple dimensions, and automation through an end-to-end concept labeling process that reduces the need for extensive human training effort when working with new datasets. To demonstrate the effectiveness and interpretability of Med-MICN, we apply it to four benchmark datasets and compare it with baselines. The results clearly demonstrate the superior performance and interpretability of our Med-MICN.","['Explainable Medical Image', 'Interpretable ML', 'Explainable AI']",[],"['Lijie Hu', 'Songning Lai', 'Wenshuo Chen', 'Hongru Xiao', 'Hongbin Lin', 'Lu Yu', 'Jingfeng Zhang', 'Di Wang']","['KAUST', 'The Hong Kong University of Science and Technology', 'Shandong University', 'College of Civil Engineering, Tongji University', 'AI Thrust, HKUST(GZ)', 'Ant Group', 'AIP, RIKEN', '']",
https://openreview.net/forum?id=3XLQp2Xx3J,Security,GS-Hider: Hiding Messages into 3D Gaussian Splatting,"3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis. Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets. Steganography, as a crucial technique for encrypted transmission and copyright protection, has been extensively studied. However, it still lacks profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS possesses two distinct features: 1) explicit 3D representation; and 2) real-time rendering speeds. These characteristics result in the 3DGS point cloud files being public and transparent, with each Gaussian point having a clear physical significance. Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task. To solve the above-mentioned issue, we first propose a steganography framework for 3DGS, dubbed GS-Hider, which can embed 3D scenes and images into original GS point clouds in an invisible manner and accurately extract the hidden messages. Specifically, we design a coupled secured feature attribute to replace the original 3DGS's spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility. Our project is available at: https://xuanyuzhang21.github.io/project/gshider.","['3D Steganography', '3D Gaussian Splatting', 'Copyright Protection']",[],"['Xuanyu Zhang', 'Jiarui Meng', 'Runyi Li', 'Zhipei Xu', 'Yongbing Zhang', 'Jian Zhang']","['School of Electronic and Computer Engineering, Peking University', 'Peking University', 'School of Electronic and Computer Engineering, Peking University', 'Peking University', 'Department of Computer Science and Technology, Harbin Institute of Technology', 'Peking University']",
https://openreview.net/forum?id=3csuL7TVpV,Security,Decoding-Time Language Model Alignment with Multiple Objectives,"Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives.  Here, we propose $\textbf{multi-objective decoding~(MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weighting over different objectives. We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8\% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned  LMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0\% and achieves 7.9--33.3\% improvement across three other metrics ($\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT).","['multi-objective alignment', 'decoding-time algorithms', 'RLHF']",[],"['Ruizhe Shi', 'Yifang Chen', 'Yushi Hu', 'Alisa Liu', 'Hannaneh Hajishirzi', 'Noah A. Smith', 'Simon Shaolei Du']","['IIIS, Tsinghua University', 'Department of Computer Science, University of Washington', 'University of Washington', 'University of Washington', 'CS, Allen Institute for Artificial Intelligence', 'Computer Science & Engineering, University of Washington', 'University of Washington']",
https://openreview.net/forum?id=3kDWoqs2X2,Fairness & Bias,Fearless Stochasticity in Expectation Propagation,"Expectation propagation (EP) is a family of algorithms for performing approximate inference in probabilistic models. The updates of EP involve the evaluation of moments—expectations of certain functions—which can be estimated from Monte Carlo (MC) samples. However, the updates are not robust to MC noise when performed naively, and various prior works have attempted to address this issue in different ways. In this work, we provide a novel perspective on the moment-matching updates of EP; namely, that they perform natural-gradient-based optimisation of a variational objective. We use this insight to motivate two new EP variants, with updates that are particularly well-suited to MC estimation. They remain stable and are most sample-efficient when estimated with just a single sample. These new variants combine the benefits of their predecessors and address key weaknesses. In particular, they are easier to tune, offer an improved speed-accuracy trade-off, and do not rely on the use of debiasing estimators. We demonstrate their efficacy on a variety of probabilistic inference tasks.","['expectation propagation', 'natural gradients', 'probabilistic methods', 'markov chain monte carlo', 'variational inference']",[],"['Jonathan So', 'Richard E. Turner']","['University of Cambridge', 'Alan Turing Institute']",
https://openreview.net/forum?id=3vJbgcjgvd,Fairness & Bias,Higher-Order Causal Message Passing for Experimentation with Complex Interference,"Accurate estimation of treatment effects is essential for decision-making across various scientific fields. This task, however, becomes challenging in areas like social sciences and online marketplaces, where treating one experimental unit can influence outcomes for others through direct or indirect interactions. Such interference can lead to biased treatment effect estimates, particularly when the structure of these interactions is unknown. We address this challenge by introducing a new class of estimators based on causal message-passing, specifically designed for settings with pervasive, unknown interference. Our estimator draws on information from the sample mean and variance of unit outcomes and treatments over time, enabling efficient use of observed data to estimate the evolution of the system state. Concretely, we construct non-linear features from the moments of unit outcomes and treatments and then learn a function that maps these features to future mean and variance of unit outcomes. This allows for the estimation of the treatment effect over time. Extensive simulations across multiple domains, using synthetic and real network data, demonstrate the efficacy of our approach in estimating total treatment effect dynamics, even in cases where interference exhibits non-monotonic behavior in the probability of treatment.","['Causal Inference', 'Network Interference', 'Dynamic Treatment Effect', 'Randomized Experiment', 'Approximate Message Passing']",[],"['Mohsen Bayati', 'Yuwei Luo', 'William Overman', 'Sadegh Shirani', 'Ruoxuan Xiong']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Emory University']",
https://openreview.net/forum?id=444LAH3MhG,Fairness & Bias,Boundary Matters: A Bi-Level Active Finetuning Method,"The pretraining-finetuning paradigm has gained widespread adoption in vision tasks and other fields. However, the finetuning phase still requires high-quality annotated samples. To overcome this challenge, the concept of active finetuning has emerged, aiming to select the most appropriate samples for model finetuning within a limited budget. Existing active learning methods  struggle in this scenario due to their inherent bias in batch selection. Meanwhile, the recent active finetuning approach focuses solely on global distribution alignment but neglects the contributions of samples to local boundaries. Therefore, we propose a Bi-Level Active Finetuning framework (BiLAF) to select the samples for annotation in one shot, encompassing two stages: core sample selection for global diversity and boundary sample selection for local decision uncertainty. Without the need of ground-truth labels, our method can successfully identify pseudo-class centers, apply a novel denoising technique, and iteratively select boundary samples  with designed evaluation metric. Extensive experiments provide  qualitative and quantitative evidence of our method's superior efficacy, consistently outperforming the existing baselines.",['Active Finetuning'],[],"['Han Lu', 'Yichen Xie', 'Xiaokang Yang', 'Junchi Yan']","['Shanghai Jiao Tong University', 'University of California, Berkeley', 'SEIEE, Shanghai Jiao Tong University, China', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=4Ktifp48WD,Fairness & Bias,Differentially Private Optimization with Sparse Gradients,"Motivated by applications of large embedding models, we study differentially private (DP) optimization problems under sparsity of _individual_ gradients.  We start with  new near-optimal bounds for the classic mean estimation problem but with sparse data, improving upon existing algorithms particularly for the high-dimensional regime. The corresponding lower bounds are based on a novel block-diagonal construction that is combined with existing DP mean estimation lower bounds. Next, we obtain pure- and approximate-DP algorithms with almost optimal rates  for stochastic convex optimization with sparse gradients; the former represents the first nearly dimension-independent rates for this problem. Furthermore, by introducing novel analyses of bias reduction in mean estimation and randomly-stopped biased SGD we  obtain nearly dimension-independent rates for near-stationary points for the empirical risk in nonconvex settings under approximate-DP.","['Differential Privacy', 'Stochastic Optimization', 'Convex Optimization', 'Sparsity']",[],"['Badih Ghazi', 'Cristóbal A Guzmán', 'Pritish Kamath', 'Ravi Kumar', 'Pasin Manurangsi']","['Google', 'Pontificia Universidad Catolica de Chile', 'Google Research', 'Research, Google', 'Google']",
https://openreview.net/forum?id=4D7haH4pdR,Fairness & Bias,Bias Detection via Signaling,"We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent's beliefs directly, we take an approach inspired by *information design*. Specifically, we measure an agent's bias by designing a *signaling scheme* and observing the actions they take in response to different signals, assuming that they are maximizing their own expected utility; our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.","['Information design', 'algorithmic fairness', 'Bayesian inference']",[],"['Yiling Chen', 'Tao Lin', 'Ariel D. Procaccia', 'Aaditya Ramdas', 'Itai Shapira']","['Harvard University', 'School of Engineering and Applied Sciences, Harvard University', 'Harvard University', 'Carnegie Mellon University', 'Harvard University, Harvard University']",
https://openreview.net/forum?id=4I2aEav51N,Fairness & Bias,Instance-Specific Asymmetric Sensitivity in Differential Privacy,"We provide a new algorithmic framework for differentially private estimation of general functions that adapts to the hardness of the underlying dataset. We build upon previous work that gives a paradigm for selecting an output through the exponential mechanism based upon closeness of the inverse to the underlying dataset, termed the inverse sensitivity mechanism. Our framework will slightly modify the closeness metric and instead give a simple and efficient application of the sparse vector technique. While the inverse sensitivity mechanism was shown to be instance optimal, it was only with respect to a class of unbiased mechanisms such that the most likely outcome matches the underlying data. We break this assumption in order to more naturally navigate the bias-variance tradeoff, which will also critically allow for extending our method to unbounded data. In consideration of this tradeoff, we provide theoretical guarantees and empirical validation that our technique will be particularly effective when the distances to the underlying dataset are asymmetric. This asymmetry is inherent to a range of important problems including fundamental statistics such as variance, as well as commonly used machine learning performance metrics for both classification and regression tasks. We efficiently instantiate our method in $O(n)$ time for these problems and empirically show that our techniques will give substantially improved differentially private estimations.","['Differential Privacy', 'Statistics', 'Machine Learning']",[],['David Durfee'],['Anonym'],
https://openreview.net/forum?id=4TlUE0ufiz,Security,Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity,"Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning. We propose introspective planning, a systematic approach that guides LLMs to refine their own uncertainty in alignment with inherent task ambiguity. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a new safe mobile manipulation benchmark, indicate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Additionally, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests.","['Large Language Models', 'Conformal Prediction', 'Uncertainty Quantification', 'Foundation Models for Decision Making']",[],"['Kaiqu Liang', 'Zixu Zhang', 'Jaime Fernández Fisac']","['Princeton University', 'Electrical and Computer Engineering, Princeton University', 'Princeton University']",
https://openreview.net/forum?id=4mzGiMooXM,Fairness & Bias,"Magnet: We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function","Text-to-image diffusion models particularly Stable Diffusion, have revolutionized the field of computer vision. However, the synthesis quality often deteriorates when asked to generate images that faithfully represent complex prompts involving multiple attributes and objects. While previous studies suggest that blended text embeddings lead to improper attribute binding, few have explored this in depth. In this work, we critically examine the limitations of the CLIP text encoder in understanding attributes and investigate how this affects diffusion models. We discern a phenomenon of attribute bias in the text space and highlight a contextual issue in padding embeddings that entangle different concepts. We propose Magnet, a novel training-free approach to tackle the attribute binding problem. We introduce positive and negative binding vectors to enhance disentanglement, further with a neighbor strategy to increase accuracy. Extensive experiments show that Magnet significantly improves synthesis quality and binding accuracy with negligible computational cost, enabling the generation of unconventional and unnatural concepts.","['Image Generation', 'Diffusion Models', 'Vision-Language Models', 'Attribute Binding']",[],"['Chenyi Zhuang', 'Ying Hu', 'Pan Gao']","['Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics, Tsinghua University']",
https://openreview.net/forum?id=5BXXoJh0Vr,Transparency & Explainability,CausalStock: Deep End-to-end Causal Discovery for News-driven Multi-stock Movement Prediction,"There are two issues in news-driven multi-stock movement prediction tasks that are not well solved in the existing works. On the one hand, ""relation discovery"" is a pivotal part when leveraging the price information of other stocks to achieve accurate stock movement prediction. Given that stock relations are often unidirectional, such as the ""supplier-consumer"" relationship, causal relations are more appropriate to capture the impact between stocks. On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty. With these two issues in mind, we propose a novel framework called CausalStock for news-driven multi-stock movement prediction, which discovers the temporal causal relations between stocks. We design a lag-dependent temporal causal discovery mechanism to model the temporal causal graph distribution. Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements. Additionally, we propose a Denoised News Encoder by taking advantage of the excellent text evaluation ability of large language models (LLMs) to extract useful information from massive news data. The experiment results show that CausalStock outperforms the strong baselines for both news-driven multi-stock movement prediction and multi-stock movement prediction tasks on six real-world datasets collected from the US, China, Japan, and UK markets. Moreover, getting benefit from the causal relations, CausalStock could offer a clear prediction mechanism with good explainability.","['Causal discovery', 'Stock movement prediction', 'Text mining']",[],"['Shuqi Li', 'Yuebo Sun', 'Yuxin Lin', 'Xin Gao', 'Shuo Shang', 'Rui Yan']","['Gaoling School of Artificial Intelligence, Renmin University of China', 'Renmin University of China', 'Columbia University', 'King Abdullah University of Science and Technology', 'CS, University of Electronic Science and Technology of China', 'Renmin University of China']",
https://openreview.net/forum?id=5FHzrRGOKR,Transparency & Explainability,Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning,"Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms. Our code is publicly available on GitHub.","['federated learning', 'explainable AI', 'counterfactuals', 'secure aggregation']",[],"['Dario Fenoglio', 'Gabriele Dominici', 'Pietro Barbiero', 'Alberto Tonda', 'Martin Gjoreski', 'Marc Langheinrich']","['Informatics, Universita della Svizzera Italiana', 'Universita della Svizzera Italiana', 'International Business Machines', 'UMR 518 MIA-PS, INRAE, Université Paris-Saclay', 'Università della svizzera italiana', 'Informatics, Universita della Svizzera Italiana']",
https://openreview.net/forum?id=5H4l37IsZ8,Fairness & Bias,Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning,"Exemplar-Free Class Incremental Learning (EFCIL) tackles the problem of training a model on a sequence of tasks without access to past data. Existing state-of-the-art methods represent classes as Gaussian distributions in the feature extractor's latent space, enabling Bayes classification or training the classifier by replaying pseudo features. However, we identify two critical issues that compromise their efficacy when the feature extractor is updated on incremental tasks. First, they do not consider that classes' covariance matrices change and must be adapted after each task. Second, they are susceptible to a task-recency bias caused by dimensionality collapse occurring during training. In this work, we propose AdaGauss - a novel method that adapts covariance matrices from task to task and mitigates the task-recency bias owing to the additional anti-collapse loss function. AdaGauss yields state-of-the-art results on popular EFCIL benchmarks and datasets when training from scratch or starting from a pre-trained backbone.","['continual learning', 'exemplar free', 'exemplar free class incremental learning', 'class incremental learning', 'exemplar-free']",[],"['Grzegorz Rypeść', 'Sebastian Cygert', 'Tomasz Trzcinski', 'Bartłomiej Twardowski']","['Warsaw University of Technology', 'IDEAS NCBR', 'Institute of Computer Science, Warsaw University of Technology', 'IDEAS NCBR']",
https://openreview.net/forum?id=5SUP6vUVkP,Transparency & Explainability,Conditional Density Estimation with Histogram Trees,"Conditional density estimation (CDE) goes beyond regression by modeling the full conditional distribution, providing a richer understanding of the data than just the conditional mean in regression. This makes CDE particularly useful in critical application domains. However, interpretable CDE methods are understudied. Current methods typically employ kernel-based approaches, using kernel functions directly for kernel density estimation or as basis functions in linear models. In contrast, despite their conceptual simplicity and visualization suitability, tree-based methods---which are arguably more comprehensible---have been largely overlooked for CDE tasks. Thus, we propose the Conditional Density Tree (CDTree), a fully non-parametric model consisting of a decision tree in which each leaf is formed by a histogram model. Specifically, we formalize the problem of learning a CDTree using the minimum description length (MDL) principle, which eliminates the need for tuning the hyperparameter for regularization. Next, we propose an iterative algorithm that, although greedily, searches the optimal histogram for every possible node split. Our experiments demonstrate that, in comparison to existing interpretable CDE methods, CDTrees are both more accurate (as measured by the log-loss) and more robust against irrelevant features. Further, our approach leads to smaller tree sizes than existing tree-based models, which benefits interpretability.","['Conditional density estimation', 'MDL principle', 'Decision Tree', 'Histogram']",[],"['Lincen Yang', 'Matthijs van Leeuwen']","['Leiden University, Leiden University', 'Leiden University, Leiden University']",
https://openreview.net/forum?id=5IFeCNA7zR,Fairness & Bias,DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph,"The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity. Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs.","['Dynamic Evaluation', 'Large Language Model']",[],"['Zhehao Zhang', 'Jiaao Chen', 'Diyi Yang']","['Dartmouth College', 'Georgia Institute of Technology', 'Stanford University']",
https://openreview.net/forum?id=5a27EE8LxX,Security,Toxicity Detection for Free,"Current LLMs are generally aligned to follow safety requirements and tend to refuse toxic prompts. However, LLMs can fail to refuse toxic prompts or be overcautious and refuse benign examples. In addition, state-of-the-art toxicity detectors have low TPRs at low FPR, incurring high costs in real-world applications where toxic examples are rare. In this paper, we introduce Moderation Using LLM Introspection (MULI), which detects toxic prompts using the information extracted directly from LLMs themselves. We found we can distinguish between benign and toxic prompts from the distribution of the first response token's logits. Using this idea, we build a robust detector of toxic prompts using a sparse logistic regression model on the first response token logits. Our scheme outperforms SOTA detectors under multiple metrics.","['LLM', 'toxicity detection']",[],"['Zhanhao Hu', 'Julien Piet', 'Geng Zhao', 'Jiantao Jiao', 'David Wagner']","['University of California, Berkeley', 'Electrical Engineering & Computer Science Department, University of California, Berkeley', '', 'University of California Berkeley', 'University of California Berkeley']",
https://openreview.net/forum?id=5pJfDlaSxV,Security,Verifiably Robust Conformal Prediction,"Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\ell_2$-bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\ell_1$, $\ell_2$, and $\ell_\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.","['Conformal Prediction', 'Adversarial Attacks', 'Distribution Shift', 'Formal Verification']",[],"['Linus Jeary', 'Tom Kuipers', 'Mehran Hosseini', 'Nicola Paoletti']","[""King's College London, University of London"", ""Department of Informatics, King's College London, University of London"", ""Department of Informatics, King's College London, University of London"", ""King's College London, University of London""]",
https://openreview.net/forum?id=6KThdqFgmA,Fairness & Bias,Fair and Welfare-Efficient Constrained Multi-Matchings under Uncertainty,"We study fair allocation of constrained resources, where a market designer optimizes overall welfare while maintaining group fairness. In many large-scale settings, utilities are not known in advance, but are instead observed after realizing the allocation. We therefore estimate agent utilities using machine learning. Optimizing over estimates requires trading-off between mean utilities and their predictive variances. We discuss these trade-offs under two paradigms for preference modeling – in the stochastic optimization regime, the market designer has access to a probability distribution over utilities, and in the robust optimization regime they have access to an uncertainty set containing the true utilities with high probability. We discuss utilitarian and egalitarian welfare objectives, and we explore how to optimize for them under stochastic and robust paradigms. We demonstrate the efficacy of our approaches on three publicly available conference reviewer assignment datasets. The approaches presented enable scalable constrained resource allocation under uncertainty for many combinations of objectives and preference models.","['Resource allocation', 'robust optimization', 'CVaR', 'constrained allocation']",[],"['Elita Lobo', 'Justin Payan', 'Cyrus Cousins', 'Yair Zick']","['University of New Hampshire', 'Machine Learning, CMU, Carnegie Mellon University', 'Philosophy, Duke University', 'Computer Science, University of Massachusetts, Amherst']",
https://openreview.net/forum?id=6LVxO1C819,Security,HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate Federated Learning,"Data heterogeneity among Federated Learning (FL) users poses a significant challenge, resulting in reduced global model performance. The community has designed various techniques to tackle this issue, among which Knowledge Distillation (KD)-based techniques are common.     While these techniques effectively improve performance under high heterogeneity, they inadvertently cause higher accuracy degradation under model poisoning attacks (known as \emph{attack amplification}). This paper presents a case study to reveal this critical vulnerability in KD-based FL systems. We show why KD causes this issue through empirical evidence and use it as motivation to design a hybrid distillation technique. We introduce a novel algorithm, Hybrid Knowledge Distillation for Robust and Accurate FL (HYDRA-FL), which reduces the impact of attacks in attack scenarios by offloading some of the KD loss to a shallow layer via an auxiliary classifier. We model HYDRA-FL as a generic framework and adapt it to two KD-based FL algorithms, FedNTD and MOON. Using these two as case studies, we demonstrate that our technique outperforms baselines in attack settings while maintaining comparable performance in benign settings.","['Federated Learning', 'Knowledge Distillation', 'Poisoning Attacks']",[],"['Momin Ahmad Khan', 'Yasra Chandio', 'Fatima M. Anwar']","['ECE, University of Massachusetts at Amherst', '', 'Electrical and Computer Engineering Department, University of Massachusetts at Amherst']",
https://openreview.net/forum?id=6OK8Qy9yVu,Privacy & Data Governance,Why Go Full? Elevating Federated Learning Through Partial Network Updates,"Federated learning is a distributed machine learning paradigm designed to protect user data privacy, which has been successfully implemented across various scenarios. In traditional federated learning, the entire parameter set of local models is updated and averaged in each training round. Although this full network update method maximizes knowledge acquisition and sharing for each model layer, it prevents the layers of the global model from cooperating effectively to complete the tasks of each client, a challenge we refer to as layer mismatch. This mismatch problem recurs after every parameter averaging, consequently slowing down model convergence and degrading overall performance. To address the layer mismatch issue, we introduce the FedPart method, which restricts model updates to either a single layer or a few layers during each communication round. Furthermore, to maintain the efficiency of knowledge acquisition and sharing, we develop several strategies to select trainable layers in each round, including sequential updating and multi-round cycle training. Through both theoretical analysis and experiments, our findings demonstrate that the FedPart method significantly surpasses conventional full network update strategies in terms of convergence speed and accuracy, while also reducing communication and computational overheads.","['Federated Learning', 'Partial Network Updates', 'Convergence Efficiency', 'Computational and Communicational Overhead Reduction']",[],"['Haolin Wang', 'Xuefeng Liu', 'Jianwei Niu', 'Wenkai Guo', 'Shaojie Tang']","['Beihang University', 'Behang University', 'School of Computer Science and Engineering, Beihang University', 'Beijing University of Aeronautics and Astronautics', '']",
https://openreview.net/forum?id=6U8iV9HVpS,Security,Robust Neural Contextual Bandit against Adversarial Corruptions,"Contextual bandit algorithms aim to identify the optimal arm with the highest reward among a set of candidates, based on the accessible contextual information. Among these algorithms, neural contextual bandit methods have shown generally superior performances against linear and kernel ones, due to the representation power of neural networks. However, similar to other neural network applications, neural bandit algorithms can be vulnerable to adversarial attacks or corruptions on the received labels (i.e., arm rewards), which can lead to unexpected performance degradation without proper treatments. As a result, it is necessary to improve the robustness of neural bandit models against potential reward corruptions. In this work, we propose a novel neural contextual bandit algorithm named R-NeuralUCB, which utilizes a novel context-aware Gradient Descent (GD) training strategy to improve the robustness against adversarial reward corruptions. Under over-parameterized neural network settings, we provide regret analysis for R-NeuralUCB to quantify  reward corruption impacts, without the commonly adopted arm separateness assumption in existing neural bandit works. We also conduct experiments against baselines on real data sets under different scenarios, in order to demonstrate the effectiveness of our proposed R-NeuralUCB.","['Contextual Bandits', 'Neural Networks', 'Adversarial Corruption']",[],"['Yunzhe Qi', 'Yikun Ban', 'Arindam Banerjee', 'Jingrui He']","['University of Illinois at Urbana-Champaign', '', 'University of Illinois, Urbana Champaign', 'School of Information Sciences, University of Illinois at Urbana-Champaign']",
https://openreview.net/forum?id=6aJrEC28hR,Fairness & Bias,Graph neural networks and non-commuting operators,"Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph. They are built from layers of graph convolutions which serve as a powerful inductive bias for describing the flow of information among the vertices. Often, more than one data modality is available. This work considers a setting in which several graphs have the same vertex set and a common vertex-level learning task. This generalizes standard GNN models to GNNs with several graph operators that do not commute. We may call this model graph-tuple neural networks (GtNN).   In this work, we develop the mathematical theory to address the stability and transferability of GtNNs using properties of non-commuting non-expansive operators. We develop a limit theory of graphon-tuple neural networks and use it to prove a universal transferability theorem that guarantees that all graph-tuple neural networks are transferable on convergent graph-tuple sequences. In particular, there is no non-transferable energy under the convergence we consider here. Our theoretical results extend well-known transferability theorems for GNNs to the case of several simultaneous graphs (GtNNs) and provide a strict improvement on what is currently known even in the GNN case.  We illustrate our theoretical results with simple experiments on synthetic and real-world data. To this end, we derive a training procedure that provably enforces the stability of the resulting model.","['graph neural networks', 'trasferability', 'stability', 'non-commuting operators', 'graphons']",[],"['Mauricio Velasco', ""Kaiying O'Hare"", 'Bernardo Rychtenberg', 'Soledad Villar']","['Universidad Católica del Uruguay', 'AMS, Johns Hopkins University', 'Montevideo, Universidad Católica del Uruguay', 'Johns Hopkins University']",
https://openreview.net/forum?id=6SRPizFuaE,Privacy & Data Governance,Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains,"Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance,  we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and preserve local data privacy. The $\alpha$-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity. Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches.","['Federated Prototype Learning', 'Domain Heterogeneity']",[],"['Lei Wang', 'Jieming Bian', 'Letian Zhang', 'Chen Chen', 'Jie Xu']","['ECE, University of Florida', 'University of Florida', '', 'Computer Science, University of Central Florida', 'Electrical and Computer Engineering, University of Florida']",
https://openreview.net/forum?id=6lx34fpanw,Fairness & Bias,Improving Generalization in Federated Learning with Model-Data Mutual Information Regularization: A Posterior Inference Approach,"Most of existing federated learning (FL) formulation is treated as a point-estimate of models, inherently prone to overfitting on scarce client-side data with overconfident decisions. Though Bayesian inference can alleviate this issue, a direct posterior inference at clients may result in biased local posterior estimates due to data heterogeneity, leading to a sub-optimal global posterior. From an information-theoretic perspective, we propose FedMDMI, a federated posterior inference framework based on model-data mutual information (MI). Specifically, a global model-data MI term is introduced as regularization to enforce the global model to learn essential information from the heterogeneous local data, alleviating the bias caused by data heterogeneity and hence enhancing generalization. To make this global MI tractable, we decompose it into local MI terms at the clients, converting the global objective with MI regularization into several locally optimizable objectives based on local data. For these local objectives, we further show that the optimal local posterior is a Gibbs posterior, which can be efficiently sampled with stochastic gradient Langevin dynamics methods. Finally, at the server, we approximate sampling from the global Gibbs posterior by simply averaging samples from the local posteriors. Theoretical analysis provides a generalization bound for FL w.r.t. the model-data MI, which, at different levels of regularization, represents a federated version of the bias-variance trade-off. Experimental results demonstrate a better generalization behavior with better calibrated uncertainty estimates of FedMDMI.","['Federated learning', 'posterior inference', 'model-data mutual information']",[],"['Hao Zhang', 'Chenglin Li', 'Nuowen Kan', 'Ziyang Zheng', 'Wenrui Dai', 'Junni Zou', 'Hongkai Xiong']","['Shanghai Jiao Tong University', 'Department of Electronic Engineering, Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Department of Computer Science & Engineering, Shanghai Jiao Tong University', 'Department of Computer Science and Engineering, Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=44WWOW4GPF,Fairness & Bias,Learning symmetries via weight-sharing with doubly stochastic tensors,"Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group $G$. In this work, we propose to *learn* such a weight-sharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries.","['symmetry discovery', 'weight-sharing']",[],"['Putri A Van der Linden', 'Alejandro García Castellanos', 'Sharvaree Vadgama', 'Thijs P. Kuipers', 'Erik J Bekkers']","['University of Amsterdam, University of Amsterdam', 'IvI, University of Amsterdam', 'Computer Science , University of Amsterdam', 'Biomedical Engineering and Physics, University of Amsterdam', 'University of Amsterdam']",
https://openreview.net/forum?id=792txRlKit,Security,DataStealing: Steal Data from Diffusion Models in Federated Learning with Multiple Trojans,"Federated Learning (FL) is commonly used to collaboratively train models with privacy preservation. In this paper, we found out that the popular diffusion models have introduced a new vulnerability to FL, which brings serious privacy threats. Despite stringent data management measures, attackers can steal massive private data from local clients through multiple Trojans, which control generative behaviors with multiple triggers. We refer to the new task as ${\bf\textit{DataStealing}}$ and demonstrate that the attacker can achieve the purpose based on our proposed Combinatorial Triggers (ComboTs) in a vanilla FL system. However, advanced distance-based FL defenses are still effective in filtering the malicious update according to the distances between each local update. Hence, we propose an Adaptive Scale Critical Parameters (AdaSCP) attack to circumvent the defenses and seamlessly incorporate malicious updates into the global model. Specifically, AdaSCP evaluates the importance of parameters with the gradients in dominant timesteps of the diffusion model. Subsequently, it adaptively seeks the optimal scale factor and magnifies critical parameter updates before uploading to the server. As a result, the malicious update becomes similar to the benign update, making it difficult for distance-based defenses to identify. Extensive experiments reveal the risk of leaking thousands of images in training diffusion models with FL. Moreover, these experiments demonstrate the effectiveness of AdaSCP in defeating advanced distance-based defenses. We hope this work will attract more attention from the FL community to the critical privacy security issues of Diffusion Models. Code: https://github.com/yuangan/DataStealing.","['Federated Learning', 'Diffusion Models', 'DataStealing', 'Multiple Trojans', 'Adaptive Scale']",[],"['Yuan Gan', 'Jiaxu Miao', 'Yi Yang']","['College of Computer Science and Technology, Zhejiang University', 'Computer Science, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University']",
https://openreview.net/forum?id=7PORYhql4V,Transparency & Explainability,Great Minds Think Alike: The Universal Convergence Trend of Input Salience,"Uncertainty is introduced in optimized DNNs through stochastic algorithms, forming specific distributions. Training models can be seen as random sampling from this distribution of optimized models. In this work, we study the distribution of optimized DNNs as a family of functions by leveraging a pointwise approach. We focus on the input saliency maps, as the input gradient field is decisive to the models' mathematical essence. Our investigation of saliency maps reveals a counter-intuitive trend: two stochastically optimized models tend to resemble each other more as either of their capacities increases. Therefore, we hypothesize several properties of these distributions, suggesting that (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean directions of the input salience. And (2) the distributions of optimized models follow a convergence trend to their shared population mean as the capacity increases. Furthermore, we also propose semi-parametric distributions based on the Saw distribution to model the convergence trend, satisfying all the counter-intuitive observations. Our experiments shed light on the significant implications of our hypotheses in various application domains, including black-box attacks, deep ensembles, etc. These findings not only enhance our understanding of DNN behaviors but also offer valuable insights for their practical application in diverse areas of deep learning.","['explainable artificial intelligence', 'saliency maps', 'model distributions']",[],"['Yipei Wang', 'Jeffrey Mark Siskind', 'Xiaoqian Wang']","['ECE, Purdue University', 'Purdue University', 'Purdue University']",
https://openreview.net/forum?id=7U5MwUS3Rw,Fairness & Bias,Towards Harmless Rawlsian Fairness Regardless of Demographic Prior,"Due to privacy and security concerns, recent advancements in group fairness advocate for model training regardless of demographic information. However, most methods still require prior knowledge of demographics. In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely _harmless Rawlsian fairness_. We ascertain that such a fairness requirement with no prior demographic information essential promotes training losses to exhibit a Dirac delta distribution. To this end, we propose a simple but effective method named VFair to minimize the variance of training losses inside the optimal set of empirical losses. This problem is then optimized by a tailored dynamic update approach that operates in both loss and gradient dimensions, directing the model towards relatively fairer solutions while preserving its intact utility. Our experimental findings indicate that regression tasks, which are relatively unexplored from literature, can achieve significant fairness improvement through VFair regardless of any prior, whereas classification tasks usually do not because of their quantized utility measurements. The implementation of our method is publicly available at https://github.com/wxqpxw/VFair.","['Harmless fairness', 'demographics-free', 'reducing variance of losses']",[],"['Xuanqian Wang', 'Jing Li', 'Ivor Tsang', 'Yew-Soon Ong']","['School of Computer Science and Engineering, Beijing University of Aeronautics and Astronautics', '', 'A*STAR', 'College of Computing and Data Science, Nanyang Technological University']",
https://openreview.net/forum?id=7UyBKTFrtd,Transparency & Explainability,Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE),"CLIP embeddings have demonstrated remarkable performance across a wide range of multimodal applications. However, these high-dimensional, dense vector representations are not easily interpretable,  limiting our understanding of the rich structure of CLIP and its use in downstream applications that require transparency.  In this work, we show that the semantic structure of CLIP's latent space can be leveraged to provide interpretability, allowing for the decomposition of representations into semantic concepts.  We formulate this problem as one of sparse recovery and propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, \method is task-agnostic and can be used, without training,  to explain and even replace traditional dense CLIP representations, maintaining high downstream performance while significantly improving their interpretability. We also demonstrate significant use cases of \method representations including detecting spurious correlations and model editing. Code is provided at https://github.com/AI4LIFE-GROUP/SpLiCE.","['Interpretable Machine Learning', 'Dictionary Learning', 'Representation Learning', 'Multimodal Models', 'Interpretability', 'CLIP']",[],"['Usha Bhalla', 'Alex Oesterling', 'Suraj Srinivas', 'Flavio Calmon', 'Himabindu Lakkaraju']","['', 'School of Engineering and Applied Sciences, Harvard University', 'Robert Bosch LLC', 'SEAS, Harvard University', 'Harvard University']",
https://openreview.net/forum?id=7WvwzuYkUq,Fairness & Bias,Progressive Entropic Optimal Transport Solvers,"Optimal transport (OT) has profoundly impacted machine learning by providing theoretical and computational tools to realign datasets. In this context, given two large point clouds of sizes $n$ and $m$ in $\mathbb{R}^d$, entropic OT (EOT) solvers have emerged as the most reliable tool to either solve the Kantorovich problem and output a $n\times m$ coupling matrix, or to solve the Monge problem and learn a vector-valued push-forward map.  While the robustness of EOT couplings/maps makes them a go-to choice in practical applications, EOT solvers remain difficult to tune because of a small but influential set of hyperparameters, notably the omnipresent entropic regularization strength $\varepsilon$. Setting $\varepsilon$ can be difficult, as it simultaneously impacts various performance metrics, such as compute speed, statistical performance, generalization, and bias. In this work, we propose a new class of EOT solvers (ProgOT), that can estimate both plans and transport maps. We take advantage of several opportunities to optimize the computation of EOT solutions by *dividing* mass displacement using a time discretization, borrowing inspiration from dynamic OT formulations, and *conquering* each of these steps using EOT with properly scheduled parameters. We provide experimental evidence demonstrating that ProgOT is a faster and more robust alternative to *standard solvers* when computing couplings at large scales, even outperforming neural network-based approaches. We also prove statistical consistency of our approach for estimating OT maps.","['Optimal Transport', 'Entropy Regularization']",[],"['Parnian Kassraie', 'Aram-Alexandre Pooladian', 'Michal Klein', 'James Thornton', 'Jonathan Niles-Weed', 'marco cuturi']","['Swiss Federal Institute of Technology', 'Center for Data Science, New York University', 'Apple', 'Google DeepMind', 'New York University', 'Apple']",
https://openreview.net/forum?id=7hy5fy2OC6,Security,Invisible Image Watermarks Are Provably Removable Using Generative AI,"Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks.  The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image.  This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack. Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality. However, watermarks that keep the image semantically similar can be an alternative defense against our attacks. Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at https://github.com/XuandongZhao/WatermarkAttacker","['Image Watermark', 'AI Safety']",[],"['Xuandong Zhao', 'Kexun Zhang', 'Zihao Su', 'Saastha Vasan', 'Ilya Grishchenko', 'Christopher Kruegel', 'Giovanni Vigna', 'Yu-Xiang Wang', 'Lei Li']","['University of California, Berkeley', 'Carnegie Mellon University', 'Computer Science, University of California, Santa Barbara', 'Computer Science, University of California Santa Barbara', '', '', ', University of California, Santa Barbara', 'University of California, San Diego', 'Language Technology Institute, School of Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=7t6aq0Fa9D,Fairness & Bias,"FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model","Topic models have been evolving rapidly over the years, from conventional to recent neural models. However, existing topic models generally struggle with either effectiveness, efficiency, or stability, highly impeding their practical applications. In this paper, we propose FASTopic, a fast, adaptive, stable, and transferable topic model. FASTopic follows a new paradigm: Dual Semantic-relation Reconstruction (DSR). Instead of previous conventional, VAE-based, or clustering-based methods, DSR directly models the semantic relations among document embeddings from a pretrained Transformer and learnable topic and word embeddings. By reconstructing through these semantic relations, DSR discovers latent topics. This brings about a neat and efficient topic modeling framework. We further propose a novel Embedding Transport Plan (ETP) method. Rather than early straightforward approaches, ETP explicitly regularizes the semantic relations as optimal transport plans. This addresses the relation bias issue and thus leads to effective topic modeling. Extensive experiments on benchmark datasets demonstrate that our FASTopic shows superior effectiveness, efficiency, adaptivity, stability, and transferability, compared to state-of-the-art baselines across various scenarios.","['topic model', 'topic modeling', 'document embedding']",[],"['Xiaobao Wu', 'Thong Thanh Nguyen', 'Delvin Ce Zhang', 'William Yang Wang', 'Anh Tuan Luu']","['Nanyang Technological University', 'National University of Singapore', 'The Pennsylvania State University', 'Computer Science, UC Santa Barbara', 'Nanyang Technological University']",
https://openreview.net/forum?id=7txPaUpUnc,Transparency & Explainability,Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning,"Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e_sae.","['Mechnastic Interpretability', 'Interpretability', 'Explainability', 'Transparency', 'Sparse Coding', 'Causal mediation analysis', 'High dimensional data analysis']",[],"['Dan Braun', 'Jordan Taylor', 'Nicholas Goldowsky-Dill', 'Lee Sharkey']","['AI, Apollo Research', 'School of Mathematics and Physics, University of Queensland', 'Apollo Research', 'Apollo Research']",
https://openreview.net/forum?id=7t9eDEY2GT,Security,Flipping-based Policy for Chance-Constrained Markov Decision Processes,"Safe reinforcement learning (RL) is a promising approach for many real-world decision-making problems where ensuring safety is a critical necessity. In safe RL research, while expected cumulative safety constraints (ECSCs) are typically the first choices, chance constraints are often more pragmatic for incorporating safety under uncertainties. This paper proposes a \textit{flipping-based policy} for Chance-Constrained Markov Decision Processes (CCMDPs). The flipping-based policy selects the next action by tossing a potentially distorted coin between two action candidates. The probability of the flip and the two action candidates vary depending on the state. We establish a Bellman equation for CCMDPs and further prove the existence of a flipping-based policy within the optimal solution sets. Since solving the problem with joint chance constraints is challenging in practice, we then prove that joint chance constraints can be approximated into Expected Cumulative Safety Constraints (ECSCs) and that there exists a flipping-based policy in the optimal solution sets for constrained MDPs with ECSCs. As a specific instance of practical implementations, we present a framework for adapting constrained policy optimization to train a flipping-based policy. This framework can be applied to other safe RL algorithms. We demonstrate that the flipping-based policy can improve the performance of the existing safe RL algorithms under the same limits of safety constraints on Safety Gym benchmarks.","['Reinforcement learning', 'Chance constraints', 'Stochastic policy']",[],"['Xun Shen', 'Shuo Jiang', 'Akifumi Wachi', 'Kazumune Hashimoto', 'Sebastien Gros']","['Osaka University', 'Osaka University, Tokyo Institute of Technology', 'LY Corporation', 'Osaka University', 'Norwegian Institute of Technology']",
https://openreview.net/forum?id=82Ndsr4OS6,Security,Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning,"We propose WSAC (Weighted Safe Actor-Critic), a novel algorithm for Safe Offline Reinforcement Learning (RL) under functional approximation, which can robustly optimize policies to improve upon an arbitrary reference policy with limited data coverage. WSAC is designed as a two-player Stackelberg game to optimize a refined objective function. The actor optimizes the policy against two adversarially trained value critics with small importance-weighted Bellman errors, which focus on scenarios where the actor's performance is inferior to the reference policy. In theory, we demonstrate that when the actor employs a no-regret optimization oracle, WSAC achieves a number of guarantees: $(i)$ For the first time in the safe offline  RL setting, we establish that WSAC can produce a policy that outperforms {\bf any} reference policy while maintaining the same level of safety, which is critical to designing a safe algorithm for offline RL. $(ii)$ WSAC achieves the optimal statistical convergence rate of $1/\sqrt{N}$ to the reference policy, where $N$ is the size of the offline dataset. $(iii)$ We theoretically show that WSAC guarantees a safe policy improvement across a broad range of hyperparameters that control the degree of pessimism, indicating its practical robustness. Additionally, we offer a practical version of WSAC and compare it with existing state-of-the-art safe offline RL algorithms in several continuous control environments. WSAC outperforms all baselines across a range of tasks, supporting the theoretical results.","['RL', 'safe-RL', 'Offline RL']",[],"['Honghao Wei', 'Xiyue Peng', 'Arnob Ghosh', 'Xin Liu']","['Washington State University', 'ShanghaiTech University', 'New Jersey Institute of Technology', 'ShanghaiTech University']",
https://openreview.net/forum?id=8aAaYEwNR4,Fairness & Bias,EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas,"One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification only in pure natural language processing benchmarks can be insufficient. Since emotions often influence human decisions, this paper examines LLM alignment in complex strategic and ethical environments, providing an in-depth analysis of the drawbacks of our psychology and the emotional impact on decision-making in humans and LLMs. We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in various strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. Our game-theoretic analysis revealed that LLMs are susceptible to emotional biases influenced by model size, alignment strategies, and primary pretraining language. Notably, these biases often diverge from typical human emotional responses, occasionally leading to unexpected drops in cooperation rates, even under positive emotional influence. Such behavior complicates the alignment of multiagent systems, emphasizing the need for benchmarks that can rigorously evaluate the degree of emotional alignment. Our framework provides a foundational basis for developing such benchmarks.","['LLM', 'Ethics', 'Emotions', 'Game Theory']",[],"['Mikhail Mozikov', 'Nikita Severin', 'Valeria Bodishtianu', 'Maria Glushanina', 'Ivan Nasonov', 'Daniil Orekhov', 'Vladislav Pekhotin', 'Ivan Makovetskiy', 'Mikhail Baklashkin', 'Vasily Lavrentyev', 'Akim Tsvigun', 'Denis Turdakov', 'Tatiana Shavrina', 'Andrey Savchenko', 'Ilya Makarov']","['Moscow State Institute of Steel and Alloys', 'Higher School of Economics', '', 'St. Petersburg State University', 'Physics, Moscow State University, Lomonosov Moscow State University', 'Higher School of Economics', 'ITKN, Moscow State Institute of Steel and Alloys', 'Moscow State Institute of Steel and Alloys', 'Applied Mathematics & Computer Science, Moscow Institute of Physics and Technology', 'AI Talent Hub, ITMO University', 'Semrush', 'Lomonosov Moscow State University', 'Artificial Intelligence Research Institute', 'AI Lab, Sber AI Lab', 'ISPRAS']",
https://openreview.net/forum?id=8ugOlbjJpp,Privacy & Data Governance,Private Algorithms for Stochastic Saddle Points and Variational Inequalities: Beyond Euclidean Geometry,"In this work, we conduct a systematic study of stochastic saddle point problems (SSP) and stochastic variational inequalities (SVI) under the constraint of $(\epsilon,\delta)$-differential privacy (DP) in both Euclidean and non-Euclidean setups. We first consider Lipschitz convex-concave SSPs in the $\ell_p/\ell_q$ setup, $p,q\in[1,2]$. That is, we consider the case where the primal problem has an $\ell_p$-setup (i.e., the primal parameter is constrained to an $\ell_p$ bounded domain and the loss is $\ell_p$-Lipschitz with respect to the primal parameter) and the dual problem has an $\ell_q$ setup. Here, we obtain a bound of $\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ on the strong SP-gap, where $n$ is the number of samples and $d$ is the dimension. This rate is nearly optimal for any $p,q\in[1,2]$. Without additional assumptions, such as smoothness or linearity requirements, prior work under DP has only obtained this rate when $p=q=2$ (i.e., only in the Euclidean setup). Further, existing algorithms have each only been shown to work for specific settings of $p$ and $q$ and under certain assumptions on the loss and the feasible set, whereas we provide a general algorithm for DP SSPs whenever $p,q\in[1,2]$. Our result is obtained via a novel analysis of the recursive regularization algorithm. In particular, we develop new tools for analyzing generalization, which may be of independent interest. Next, we turn our attention towards SVIs with a monotone, bounded and Lipschitz operator and consider $\ell_p$-setups, $p\in[1,2]$. Here, we provide the first analysis which obtains a bound on the strong VI-gap of $\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$. For $p-1=\Omega(1)$, this rate is near optimal due to existing lower bounds. To obtain this result, we develop a modified version of recursive regularization. Our analysis builds on the techniques we develop for SSPs as well as employing additional novel components which handle difficulties arising from adapting the recursive regularization framework to SVIs.","['Differential Privacy', 'Stochastic Saddle Point Problem', 'Stochastic Variational Inequality', 'Strong Gap', 'Stochastic Minimax Optimization', 'Algorithmic Stability']",[],"['Raef Bassily', 'Cristóbal A Guzmán', 'Michael Menart']","['Computer Science and Engineering, Ohio State University', 'Pontificia Universidad Catolica de Chile', 'Computer Science, University of Toronto']",
https://openreview.net/forum?id=8jyCRGXOr5,Transparency & Explainability,Efficient Sketches for Training Data Attribution and Studying the Loss Landscape,"The study of modern machine learning models often necessitates storing vast quantities of gradients or Hessian vector products (HVPs). Traditional sketching methods struggle to scale under these memory constraints.  We present a novel  framework for scalable gradient and HVP sketching, tailored for modern hardware.  We provide theoretical guarantees and demonstrate the power of our methods in applications like training data attribution, Hessian spectrum analysis, and intrinsic dimension computation for pre-trained language models. Our work sheds new light on the behavior of pre-trained language models, challenging assumptions about their intrinsic dimensionality and Hessian properties.","['Deep Learning', 'Language Models', 'Interpretability', 'Training Data Attribution', 'Loss Landscape']",[],['Andrea Schioppa'],[''],
https://openreview.net/forum?id=8puv3c9CPg,Transparency & Explainability,Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects,"Though vision transformers (ViTs) have achieved state-of-the-art performance in a variety of settings, they exhibit surprising failures when performing tasks involving visual relations. This begs the question: how do ViTs attempt to perform tasks that require computing visual relations between objects? Prior efforts to interpret ViTs tend to focus on characterizing relevant low-level visual features. In contrast, we adopt methods from mechanistic interpretability to study the higher-level visual algorithms that ViTs use to perform abstract visual reasoning. We present a case study of a fundamental, yet surprisingly difficult, relational reasoning task: judging whether two visual entities are the same or different. We find that pretrained ViTs fine-tuned on this task often exhibit two qualitatively different stages of processing despite having no obvious inductive biases to do so: 1) a perceptual stage wherein local object features are extracted and stored in a disentangled representation, and 2) a relational stage wherein object representations are compared. In the second stage, we find evidence that ViTs can learn to represent somewhat abstract visual relations, a capability that has long been considered out of reach for artificial neural networks. Finally, we demonstrate that failures at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks. By understanding ViTs in terms of discrete processing stages, one can more precisely diagnose and rectify shortcomings of existing and future models.","['visual reasoning', 'mechanistic interpretability', 'transformers', 'cognitive science']",[],"['Michael A. Lepori', 'Alexa R. Tartaglini', 'Wai Keen Vong', 'Thomas Serre', 'Brenden Lake', 'Ellie Pavlick']","['Brown University', 'Computer Science, Stanford University', 'New York University', 'Brown University', 'New York University', 'Brown University']",
https://openreview.net/forum?id=4NJBV6Wp0h,Fairness & Bias,LLM Evaluators Recognize and Favor Their Own Generations,"Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others’ while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By finetuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.","['LLMs', 'evaluations', 'benchmarking', 'situational-awareness']",[],"['Arjun Panickssery', 'Samuel R. Bowman', 'Shi Feng']","['Dept of Computer Science, University of Illinois at Urbana-Champaign', '', 'George Washington University']",
https://openreview.net/forum?id=99rOAM7Jfm,Privacy & Data Governance,Noise-Aware Differentially Private Regression via Meta-Learning,"Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions. While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance. One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data. In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al. (2013), yielding the DPConvCNP. DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions. We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.","['differential privacy', 'meta-learning', 'neural processes', 'Gaussian processes', 'sim-to-real', 'probabilistic regression']",[],"['Ossi Räisä', 'Stratis Markou', 'Matthew Ashman', 'Wessel P Bruinsma', 'Marlon Tobaben', 'Antti Honkela', 'Richard E. Turner']","['Department of Computer Science, University of Helsinki', 'University of Cambridge', 'Engineering, University of Cambridge', 'Microsoft Research', 'University of Helsinki', 'University of Helsinki', 'Alan Turing Institute']",
https://openreview.net/forum?id=93ktalFvnJ,Privacy & Data Governance,Boosting Alignment for Post-Unlearning Text-to-Image Generative Models,"Large-scale generative models have shown impressive image-generation capabilities, propelled by massive data. However, this often inadvertently leads to the generation of harmful or inappropriate content and raises copyright concerns. Driven by these concerns, machine unlearning has become crucial to effectively purge undesirable knowledge from models. While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives. To address these challenges, we propose a framework that seeks an optimal model update at each unlearning iteration, ensuring monotonic improvement on both objectives. We further derive the characterization of such an update.   In addition, we design procedures to strategically diversify the unlearning and remaining datasets to boost performance improvement. Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models' original trained states, thus outperforming state-of-the-art baselines.","['unlearning', 'diffusion models', 'stable diffusion']",[],"['Myeongseob Ko', 'Henry Li', 'Zhun Wang', 'Jonathan Patsenker', 'Jiachen T. Wang', 'Qinbin Li', 'Ming Jin', 'Dawn Song', 'Ruoxi Jia']","['Virginia Polytechnic Institute and State University', 'Applied Mathematics, Yale University', '', 'Applied Mathematics, Yale University', 'Electrical and Computer Engineering, Princeton University', 'Huazhong University of Science and Technology', 'Virginia Tech', 'University of California Berkeley', 'Virginia Tech']",
https://openreview.net/forum?id=9OHXQybMZB,Fairness & Bias,Aligning Model Properties via Conformal Risk Control,"AI model alignment is crucial due to inadvertent biases in training data and the underspecified machine learning pipeline, where models with excellent test metrics may not meet end-user requirements. While post-training alignment via human feedback shows promise, these methods are often limited to generative AI settings where humans can interpret and provide feedback on model outputs. In traditional non-generative settings with numerical or categorical outputs, detecting misalignment through single-sample outputs remains challenging, and enforcing alignment during training requires repeating costly training processes. In this paper we consider an alternative strategy. We propose interpreting model alignment through property testing, defining an aligned model $f$ as one belonging to a subset $\mathcal{P}$ of functions that exhibit specific desired behaviors. We focus on post-processing a pre-trained model $f$ to better align with $\mathcal{P}$ using conformal risk control. Specifically, we develop a general procedure for converting queries for testing a given property $\mathcal{P}$ to a collection of loss functions suitable for use in a conformal risk control algorithm. We prove a probabilistic guarantee that the resulting conformal interval around $f$ contains a function approximately satisfying $\mathcal{P}$. We exhibit applications of our methodology on a collection of supervised learning datasets for (shape-constrained) properties such as monotonicity and concavity. The general procedure is flexible and can be applied to a wide range of desired properties. Finally, we prove that pre-trained models will always require alignment techniques even as model sizes or training data increase, as long as the training data contains even small biases.","['Alignment', 'Conformal Prediction', 'Conformal Risk Control', 'Property Testing']",[],"['William Overman', 'Jacqueline Jil Vallon', 'Mohsen Bayati']","['Stanford University', 'Management Science and Engineering, Stanford University', 'Stanford University']",
https://openreview.net/forum?id=vJLTcCBZVT,Fairness & Bias,Improving Subgroup Robustness via Data Selection,"Machine learning models can often fail on subgroups that are underrepresented during training. While dataset balancing can improve performance on underperforming groups, it requires access to training group annotations and can end up removing large portions of the dataset. In this paper, we introduce Data Debiasing with Datamodels (D3M), a debiasing approach which isolates and removes specific training examples that drive the model's failures on minority groups. Our approach enables us to efficiently train debiased classifiers while removing only a small number of examples, and does not require training group annotations or additional hyperparameter tuning.","['group robustness', 'fairness', 'data attribution', 'machine learning']",[],"['Saachi Jain', 'Kimia Hamidieh', 'Kristian Georgiev', 'Andrew Ilyas', 'Marzyeh Ghassemi', 'Aleksander Madry']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Stein Fellow (Statistics), Stanford University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=9zQl27mqWE,Fairness & Bias,Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes,"The training dynamics of linear networks are well studied in two distinct setups: the lazy regime and balanced/active regime, depending on the initialization and width of the network. We provide a surprisingly simple unifying formula for the evolution of the learned matrix that contains as special cases both lazy and balanced regimes but also a mixed regime in between the two. In the mixed regime, a part of the network is lazy while the other is balanced. More precisely the network is lazy along singular values that are below a certain threshold and balanced along those that are above the same threshold. At initialization, all singular values are lazy, allowing for the network to align itself with the task, so that later in time, when some of the singular value cross the threshold and become active they will converge rapidly (convergence in the balanced regime is notoriously difficult in the absence of alignment). The mixed regime is the `best of both worlds': it converges from any random initialization (in contrast to balanced dynamics which require special initialization), and has a low rank bias (absent in the lazy dynamics). This allows us to prove an almost complete phase diagram of training behavior as a function of the variance at initialization and the width, for a MSE training task.","['Linear Networks', 'Lazy Regime', 'Active Regime', 'Training Dynamics', 'Phase Diagram']",[],"['Zhenfeng Tu', 'Santiago Aranguri', 'Arthur Jacot']","['Courant Institute of Mathematical Sciences, New York University', 'Courant Institute, New York University', 'Courant Institute, NYU, New York University']",
https://openreview.net/forum?id=A0HSmrwtLH,Transparency & Explainability,Testing Semantic Importance via Betting,"Recent works have extended notions of feature importance to semantic concepts that are inherently interpretable to the users interacting with a black-box predictive model. Yet, precise statistical guarantees such as false positive rate and false discovery rate control are needed to communicate findings transparently, and to avoid unintended consequences in real-world scenarios. In this paper, we formalize the global (i.e., over a population) and local (i.e., for a sample) statistical importance of semantic concepts for the predictions of opaque models by means of conditional independence, which allows for rigorous testing. We use recent ideas of sequential kernelized independence testing to induce a rank of importance across concepts, and we showcase the effectiveness and flexibility of our framework on synthetic datasets as well as on image classification using several vision-language models.","['Explainability', 'Semantic Concepts', 'Sequential Testing', 'Conditional Independence Testing']",[],"['Jacopo Teneggi', 'Jeremias Sulam']","['Johns Hopkins University', 'Biomedical Engineering, Johns Hopkins University']",
https://openreview.net/forum?id=41lovPOCo5,Fairness & Bias,TableRAG: Million-Token Table Understanding with Language Models,"Recent advancements in language models (LMs) have notably enhanced their ability to reason with tabular data, primarily through program-aided mechanisms that manipulate and analyze tables. However, these methods often require the entire table as input, leading to scalability challenges due to the positional bias or context length constraints. In response to these challenges, we introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework specifically designed for LM-based table understanding. TableRAG leverages query expansion combined with schema and cell retrieval to pinpoint crucial information  before providing it to the LMs. This enables more efficient data encoding and precise retrieval, significantly reducing prompt lengths and mitigating information loss. We have developed two new million-token benchmarks from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's effectiveness at scale. Our results demonstrate that TableRAG's retrieval design achieves the highest retrieval quality, leading to the new state-of-the-art performance on large-scale table understanding.","['large language model', 'large scale', 'tabular reasoning', 'retrieval', 'LLM']",[],"['Si-An Chen', 'Lesly Miculicich', 'Julian Martin Eisenschlos', 'Zifeng Wang', 'Zilong Wang', 'Yanfei Chen', 'Yasuhisa Fujii', 'Hsuan-Tien Lin', 'Chen-Yu Lee', 'Tomas Pfister']","['Computer Science and Information Engineering, National Taiwan University', 'Google', 'Google DeepMind', 'Google', 'University of California, San Diego', 'Google', 'Google', 'National Taiwan University', 'Google', 'Google']",
https://openreview.net/forum?id=AWFryOJaGi,Fairness & Bias,Lambda: Learning Matchable Prior For Entity Alignment with Unlabeled Dangling Cases,"We investigate the entity alignment (EA) problem with unlabeled dangling cases, meaning that partial entities have no counterparts in the other knowledge graph (KG), yet these entities are unlabeled. The problem arises when the source and target graphs are of different scales, and it is much cheaper to label the matchable pairs than the dangling entities. To address this challenge, we propose the framework \textit{Lambda} for dangling detection and entity alignment. Lambda features a GNN-based encoder called KEESA with a spectral contrastive learning loss for EA and a positive-unlabeled learning algorithm called iPULE for dangling detection. Our dangling detection module offers theoretical guarantees of unbiasedness, uniform deviation bounds, and convergence. Experimental results demonstrate that each component contributes to overall performances that are superior to baselines, even when baselines additionally exploit 30\% of dangling entities labeled for training.","['Knowledge Graph', 'Entity Alignment', 'Positive-Unlabeled Learning', 'Dangling Cases']",[],"['Hang Yin', 'Liyao Xiang', 'Dong Ding', 'Yuheng He', 'Yihan Wu', 'Pengzhi Chu', 'Xinbing Wang', 'Chenghu Zhou']","['Shanghai Jiaotong University', 'John Hopcroft Center for Computer Science, Shanghai Jiao Tong University', 'Jiont Institute, Shanghai Jiaotong University', 'School of Sensing Science and engineering, Shanghai Jiaotong University', 'School of Mathmatical Science, Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'IGSNRR, Chinese Academy of Sciences, Beijing, China']",
https://openreview.net/forum?id=AhlaBDHMQh,Transparency & Explainability,Learning Identifiable Factorized Causal Representations of Cellular Responses,"The study of cells and their responses to genetic or chemical perturbations promises to accelerate the discovery of therapeutics targets. However, designing adequate and insightful models for such data is difficult because the response of a cell to perturbations essentially depends on contextual covariates (e.g., genetic background or type of the cell). There is therefore a need for models that can identify interactions between drugs and contextual covariates. This is crucial for discovering therapeutics targets, as such interactions may reveal drugs that affect certain cell types but not others. We tackle this problem with a novel Factorized Causal Representation (FCR) learning method, an identifiable deep generative model that reveals causal structure in single-cell perturbation data from several cell lines. FCR learns multiple cellular representations that are disentangled, comprised of covariate-specific (Z_x), treatment-specific (Z_t) and interaction-specific (Z_tx) representations. Based on recent advances of non-linear ICA theory, we prove the component-wise identifiability of Z_tx and block-wise identifiability of Z_t and Z_x. Then, we present our implementation of FCR, and empirically demonstrate that FCR outperforms state-of-the-art baselines in various tasks across four single-cell datasets.","['Single Cell', 'Computational Biology', 'Causality']",[],"['Haiyi Mao', 'Romain Lopez', 'Kai Liu', 'Jan-Christian Huetter', 'David Richmond', 'Panayiotis V. Benos', 'Lin Qiu']","['School of Medicine, University of Pittsburgh', 'New York University', 'SES AI', 'Genentech', '', '', 'Genentech Inc.']",
https://openreview.net/forum?id=Ai76ATrb2y,Transparency & Explainability,Auditing Privacy Mechanisms via Label Inference Attacks,"We propose reconstruction advantage measures to audit label privatization mechanisms. A reconstruction advantage measure quantifies the increase in an attacker's ability to infer the true label of an unlabeled example when provided with a private version of the labels in a dataset (e.g., aggregate of labels from different users or noisy labels output by randomized response), compared to an attacker that only observes the feature vectors, but may have prior knowledge of the correlation between features and labels. We consider two such auditing measures: one additive, and on multiplicative. These cover previous approaches taken in the literature on empirical auditing and differential privacy. These measures allow us to place a variety of proposed privatization schemes---some differentially private, some not---on the same footing. We analyze these measures theoretically under a distributional model which, we claim, encapsulates reasonable adversarial settings. We also quantify their behavior empirically on real and simulated  prediction tasks. Across a range of experimental settings, we find that differentially private schemes dominate or match the privacy-utility tradeoff of more heuristic approaches.","['label inference', 'label reconstruction', 'differential privacy', 'learning from label proportions']",[],"['Robert Istvan Busa-Fekete', 'Travis Dick', 'Claudio Gentile', 'Andres Munoz medina', 'Adam Smith', 'Marika Swanberg']","['Google Research', 'Google', 'Google Research, Google', 'Google', 'Computer Science, Boston University', 'Computer Science, Boston University']",
https://openreview.net/forum?id=AqcPvWwktK,Fairness & Bias,Semi-supervised Multi-label Learning with Balanced Binary Angular Margin Loss,"Semi-supervised multi-label learning (SSMLL) refers to inducing classifiers using a small number of samples with multiple labels and many unlabeled samples. The prevalent solution of SSMLL involves forming pseudo-labels for unlabeled samples and inducing classifiers using both labeled and pseudo-labeled samples in a self-training manner. Unfortunately, with the commonly used binary type of loss and negative sampling, we have empirically found that learning with labeled and pseudo-labeled samples can result in the variance bias problem between the feature distributions of positive and negative samples for each label. To alleviate this problem, we aim to balance the variance bias between positive and negative samples from the perspective of the feature angle distribution for each label. Specifically, we extend the traditional binary angular margin loss to a balanced extension with feature angle distribution transformations under the Gaussian assumption, where the distributions are iteratively updated during classifier training. We also suggest an efficient prototype-based negative sampling method to maintain high-quality negative samples for each label. With this insight, we propose a novel SSMLL method, namely Semi-Supervised Multi-Label Learning with Balanced Binary Angular Margin loss (S$^2$ML$^2$-BBAM). To evaluate the effectiveness of S$^2$ML$^2$-BBAM, we compare it with existing competitors on benchmark datasets. The experimental results validate that S$^2$ML$^2$-BBAM can achieve very competitive performance.","['Semi-supervised Multi-label Learning', 'Variance Bias', 'Balanced Binary Angular Margin Loss', 'Self-training']",[],"['Ximing Li', 'Silong Liang', 'Changchun Li', 'pengfei wang', 'Fangming Gu']","['College of Computer Science and Technology, Jilin University', 'College of Computer Science and Technology, Jilin University', 'College of Computer Science and Technology, Jilin University', '', 'College of Computer Science and Technology, Jilin University']",
https://openreview.net/forum?id=AoEeBqP8AD,Fairness & Bias,Unsupervised Anomaly Detection in The Presence of Missing Values,"Anomaly detection methods typically require fully observed data for model training and inference and cannot handle incomplete data, while the missing data problem is pervasive in science and engineering, leading to challenges in many important applications such as abnormal user detection in recommendation systems and novel or anomalous cell detection in bioinformatics, where the missing rates can be higher than 30\% or even 80\%. In this work, first, we construct and evaluate a straightforward strategy, ''impute-then-detect'', via combining state-of-the-art imputation methods with unsupervised anomaly detection methods, where the training data are composed of normal samples only. We observe that such two-stage methods frequently yield imputation bias from normal data, namely, the imputation methods are inclined to make incomplete samples ''normal"", where the fundamental reason is that the imputation models learned only on normal data and cannot generalize well to abnormal data in the inference stage. To address this challenge, we propose an end-to-end method that integrates data imputation with anomaly detection into a unified optimization problem. The proposed model learns to generate well-designed pseudo-abnormal samples to mitigate the imputation bias and ensure the discrimination ability of both the imputation and detection processes. Furthermore, we provide theoretical guarantees for the effectiveness of the proposed method, proving that the proposed method can correctly detect anomalies with high probability. Experimental results on datasets with manually constructed missing values and inherent missing values demonstrate that our proposed method effectively mitigates the imputation bias and surpasses the baseline methods significantly. The source code of our method is available at https://github.com/jicongfan/ImAD-Anomaly-Detection-With-Missing-Data.","['missing data', 'anomaly detection', 'deep learning']",[],"['Feng Xiao', 'Jicong Fan']","['The Chinese University of Hong Kong,Shenzhen', 'The Chinese University of Hong Kong, Shenzhen']",
https://openreview.net/forum?id=AvWB40qXZh,Transparency & Explainability,NeuMA: Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics,"While humans effortlessly discern intrinsic dynamics and adapt to new scenarios, modern AI systems often struggle. Current methods for visual grounding of dynamics either use pure neural-network-based simulators (black box), which may violate physical laws, or traditional physical simulators (white box), which rely on expert-defined equations that may not fully capture actual dynamics. We propose the Neural Material Adaptor (NeuMA), which integrates existing physical laws with learned corrections, facilitating accurate learning of actual dynamics while maintaining the generalizability and interpretability of physical priors. Additionally, we propose Particle-GS, a particle-driven 3D Gaussian Splatting variant that bridges simulation and observed images, allowing back-propagate image gradients to optimize the simulator. Comprehensive experiments on various dynamics in terms of grounded particle accuracy, dynamic rendering quality, and generalization ability demonstrate that NeuMA can accurately capture intrinsic dynamics. Project Page: https://xjay18.github.io/projects/neuma.html.","['Intuitive Physics', 'Differentiable Renderer', 'Neural Simulation']",[],"['Junyi Cao', 'Shanyan Guan', 'Yanhao Ge', 'Wei Li', 'Xiaokang Yang', 'Chao Ma']","['Shanghai Jiaotong University', 'vivo', 'Future Imaging Area', '', 'SEIEE, Shanghai Jiao Tong University, China', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=Apq6corvfZ,Privacy & Data Governance,Instance-Optimal Private Density Estimation in the Wasserstein Distance,"Estimating the density of a distribution from samples is a fundamental problem in statistics. In many practical settings, the Wasserstein distance is an appropriate error metric for density estimation. For example, when estimating population densities in a geographic region, a small Wasserstein distance means that the estimate is able to capture roughly where the population mass is. In this work we study differentially private density estimation in the Wasserstein distance. We design and analyze instance-optimal algorithms for this problem that can adapt to easy instances.  For distributions $P$ over $\mathbb{R}$, we consider a strong notion of instance-optimality: an algorithm that uniformly achieves the instance-optimal estimation rate is competitive with an algorithm that is told that the distribution is either $P$ or $Q_P$ for some distribution $Q_P$ whose probability density function (pdf) is within a factor of 2 of the pdf of $P$. For distributions over $\mathbb{R}^2$, we use a slightly different notion of instance optimality. We say that an algorithm is instance-optimal if it is competitive with an algorithm that is given a constant multiplicative approximation of the density of the distribution. We characterize the instance-optimal estimation rates in both these settings and show that they are uniformly achievable (up to polylogarithmic factors). Our approach for $\mathbb{R}^2$ extends to arbitrary metric spaces as it goes via hierarchically separated trees. As a special case our results lead to instance-optimal learning in TV distance for discrete distributions.","['Differential Privacy', 'Density Estimation', 'Instance Optimality', 'Wasserstein Distance']",[],"['Vitaly Feldman', 'Audra McMillan', 'Satchit Sivakumar', 'Kunal Talwar']","['Apple AI Research', 'Apple', 'Boston University', 'Apple']",
https://openreview.net/forum?id=B29BlRe26Z,Fairness & Bias,SLowcalSGD : Slow Query Points Improve Local-SGD for Stochastic Convex Optimization,"We consider  distributed learning scenarios where $M$ machines interact with a parameter server along several communication rounds in order to minimize a joint objective function.  Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD.  Key to our approach is a slow querying technique  that we customize to the distributed setting, which in turn enables a better mitigation of the bias caused by local updates.",['Stochastic Convex Optimization'],[],"['Tehila Dahan', 'Kfir Yehuda Levy']","['Electrical & Computer Engineering, Technion - Israel Institute of Technology, Technion', 'Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=B4k2TecKT2,Fairness & Bias,Towards Accurate and Fair Cognitive Diagnosis via Monotonic Data Augmentation,"Intelligent education stands as a prominent application of machine learning. Within this domain, cognitive diagnosis (CD) is a key research focus that aims to diagnose students' proficiency levels in specific knowledge concepts. As a crucial task within the field of education, cognitive diagnosis encompasses two fundamental requirements: accuracy and fairness. Existing studies have achieved significant success by primarily utilizing observed historical logs of student-exercise interactions. However, real-world scenarios often present a challenge, where a substantial number of students engage with a limited number of exercises. This data sparsity issue can lead to both inaccurate and unfair diagnoses. To this end, we introduce a monotonic data augmentation framework, CMCD, to tackle the data sparsity issue and thereby achieve accurate and fair CD results. Specifically, CMCD integrates the monotonicity assumption, a fundamental educational principle in CD, to establish two constraints for data augmentation. These constraints are general and can be applied to the majority of CD backbones. Furthermore, we provide theoretical analysis to guarantee the accuracy and convergence speed of CMCD. Finally, extensive experiments on real-world datasets showcase the efficacy of our framework in addressing the data sparsity issue with accurate and fair CD results.","['Cognitive Diagnosis', 'Intelligent Education', 'Monotonicity Assumption']",[],"['Zheng Zhang', 'Wei Song', 'Qi Liu', 'Qingyang Mao', 'Yiyan Wang', 'Weibo Gao', 'Zhenya Huang', 'Shijin Wang', 'Enhong Chen']","['', 'University of Science and Technology of China', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China', 'University of Science and Technology of China', 'Beijing Normal University', '', 'School of Computer Science and Technology, University of Science and Technology of China', 'State Key Laboratory of Cognitive Intelligence', 'School of Computer Science and Technology, University of Science and Technology of China']",
https://openreview.net/forum?id=BOhnXyIPWW,Privacy & Data Governance,Locally Private and Robust Multi-Armed Bandits,"We study the interplay between local differential privacy (LDP) and robustness to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs). We consider two different practical settings: LDP-then-Corruption (LTC) where each user's locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user's raw data may be corrupted such that the LDP mechanism will only be applied to the corrupted data. To start with, we present the first tight characterization of the mean estimation error in high probability under both LTC and CTL settings. Leveraging this new result, we then present an almost tight characterization (up to log factor) of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings, respectively. Our theoretical results in both settings are also corroborated by a set of systematic simulations. One key message in this paper is that LTC is a more difficult setting that leads to a worse performance guarantee compared to the CTL setting (in the minimax sense). Our sharp understanding of LTC and CTL also naturally allows us to give the first tight performance bounds for the most practical setting where corruption could happen both before and after the LDP mechanism.  As an important by-product, we also give the first correct and tight regret bound for locally private and heavy-tailed online MABs, i.e., without Huber corruption, by identifying a fundamental flaw in the state-of-the-art.","['Local Differential Privacy', 'Robustness', 'Huber Corruption', 'Multi-Armed Bandits']",[],"['Xingyu Zhou', 'WEI ZHANG']","['ECE, Wayne State University', 'Texas A&M University - College Station']",
https://openreview.net/forum?id=BAjjINf0Oh,Privacy & Data Governance,Oracle-Efficient Differentially Private Learning with Public Data,"Due to statistical lower bounds on the learnability of many function classes under privacy constraints, there has been recent interest in leveraging public data to improve the performance of private learning algorithms. In this model, algorithms must always guarantee differential privacy with respect to the private samples while also ensuring learning guarantees when the private data distribution is sufficiently close to that of the public data.  Previous work has demonstrated that when sufficient public, unlabelled data is available, private learning can be made statistically tractable, but the resulting algorithms have all been computationally inefficient.  In this work, we present the first computationally efficient, algorithms to provably leverage public data to learn privately whenever a function class is learnable non-privately, where our notion of computational efficiency is with respect to the number of calls to an optimization oracle for the function class.  In addition to this general result, we provide specialized algorithms with improved sample complexities in the special cases when the function class is convex or when the task is binary classification.","['Oracle Efficiency', 'Differential Privacy', 'PAC learning']",[],"['Adam Block', 'Mark Bun', 'Rathin Desai', 'Abhishek Shetty', 'Steven Wu']","['Computer Science, Columbia University', 'Computer Science, Boston University', 'Boston University, Boston University', 'University of California Berkeley', 'School of Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=BQh1SGvROG,Security,AdanCA: Neural Cellular Automata As Adaptors For More Robust Vision Transformer,"Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions. Although such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy input. In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning. Using our analysis of AdaNCA placement and robustness improvement, we also develop an algorithm for identifying the most effective insertion points for AdaNCA. With less than a 3% increase in parameters, AdaNCA contributes to more than 10% absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across eight robustness benchmarks and four ViT architectures that AdaNCA, as a plug-and-play module, consistently improves the robustness of ViTs.","['Neural Cellular Automata', 'Vision Transformer', 'Adversarial Robustness', 'Out-of-distribution generalization']",[],"['Yitao Xu', 'Tong Zhang', 'Sabine Susstrunk']","['EPFL - EPF Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'School of Computer and Communication Sciences, EPFL - EPF Lausanne']",
https://openreview.net/forum?id=C3JCwbMXbU,Fairness & Bias,Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler,"In Open-Set Domain Generalization (OSDG), the model is exposed to both new variations of data appearance (domains) and open-set conditions, where both known and novel categories are present at test time. The challenges of this task arise from the dual need to generalize across diverse domains and accurately quantify category novelty, which is critical for applications in dynamic environments. Recently, meta-learning techniques have demonstrated superior results in OSDG, effectively orchestrating the meta-train and -test tasks by employing varied random categories and predefined domain partition strategies. These approaches prioritize a well-designed training schedule over traditional methods that focus primarily on data augmentation and the enhancement of discriminative feature learning.  The prevailing meta-learning models in OSDG typically utilize a predefined sequential domain scheduler to structure data partitions. However, a crucial aspect that remains inadequately explored is the influence brought by strategies of domain schedulers during training.  In this paper, we observe that an adaptive domain scheduler benefits more in OSDG compared with prefixed sequential and random domain schedulers. We propose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve an adaptive domain scheduler. This method strategically sequences domains by assessing their reliabilities in utilizing a follower network, trained with confidence scores learned in an evidential manner, regularized by max rebiasing discrepancy, and optimized in a bilevel manner. We verify our approach on three OSDG benchmarks, i.e., PACS, DigitsDG, and OfficeHome. The results show that our method substantially improves OSDG performance and achieves more discriminative embeddings for both the seen and unseen categories, underscoring the advantage of a judicious domain scheduler for the generalizability to unseen domains and unseen categories. The source code is publicly available at https://github.com/KPeng9510/EBiL-HaDS.","['open set domain generalization', 'image classification']",[],"['Kunyu Peng', 'Di Wen', 'Kailun Yang', 'Ao Luo', 'Yufan Chen', 'Jia Fu', 'M. Saquib Sarfraz', 'Alina Roitberg', 'Rainer Stiefelhagen']","['IAR, Karlsruher Institut für Technologie', 'Institute for Anthropomatics and Robotics, Karlsruher Institut für Technologie', 'Hunan University', 'KDDI Research, Inc.', 'Informatik, Karlsruhe Institute for Technology', 'Computer Science, KTH Royal Institute of Technology', 'Autonomous Systems, Mercedes-Benz Tech innovation', 'Universität Stuttgart', 'Informatics, Karlsruhe Institute of Technology']",
https://openreview.net/forum?id=BgZcuEsYU8,Fairness & Bias,Causal Inference in the Closed-Loop: Marginal Structural Models for Sequential Excursion Effects,"Optogenetics is widely used to study the effects of neural circuit manipulation on behavior. However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed. To fill this gap, we introduce a nonparametric causal inference framework for analyzing ""closed-loop"" designs, which use dynamic policies that assign treatment based on covariates. In this setting, standard methods can introduce bias and occlude causal effects. Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes. In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling. Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points. From another view, our work extends ""excursion effect"" methods---popular in the mobile health literature---to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations. We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects. We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses.","['marginal structural models', 'optogenetics', 'excursion effects', 'neuroscience', 'dynamic treatment regimes', 'micro-randomized trials', 'sequentially randomized experiments']",[],"['Alexander W. Levis', 'Gabriel Loewinger', 'Francisco Pereira']","['Statistics & Data Science, Carnegie Mellon University', 'Machine Learning Team, National Institutes of Health', 'National Institute of Mental Health']",
https://openreview.net/forum?id=C0EhyoPpTN,Transparency & Explainability,Inferring stochastic low-rank recurrent neural networks from neural data,"A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability.","['Low-rank RNNs', 'dynamical systems', 'variational inference', 'sequential monte carlo', 'neural data']",[],"['Matthijs Pals', 'A Erdem Sağtekin', 'Felix C Pei', 'Manuel Gloeckler', 'Jakob H. Macke']","['Excellence Cluster Machine Learning, Eberhard-Karls-Universität Tübingen', 'Machine Learning in Science, Eberhard-Karls-Universität Tübingen', 'Georgia Institute of Technology', 'Machine Learning in Science, University of Tuebingen', 'University of Tuebingen']",
https://openreview.net/forum?id=jTyjwRpLZ5,Fairness & Bias,Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity,"Optimization of convex functions under stochastic zeroth-order feedback has been a major and challenging question in online learning. In this work, we consider the problem of optimizing second-order smooth and strongly convex functions where the algorithm is only accessible to noisy evaluations of the objective function it queries.  We provide the first tight characterization for the rate of the minimax simple regret by developing matching upper and lower bounds.  We propose an algorithm that features a combination of a bootstrapping stage and a mirror-descent stage.  Our main technical innovation consists of a sharp characterization for the spherical-sampling gradient estimator under higher-order smoothness conditions, which allows the algorithm to optimally balance the bias-variance tradeoff,  and a new iterative method for the bootstrapping stage, which maintains the performance for unbounded Hessian.","['Stochastic Optimization', 'Sample Complexity', 'Optimality']",[],"['Qian Yu', 'Yining Wang', 'Baihe Huang', 'Qi Lei', 'Jason D. Lee']","['University of California, Santa Barbara', 'University of Texas at Dallas', 'University of California, Berkeley', 'New York University', 'Princeton University']",
https://openreview.net/forum?id=XlvUz9F50g,Fairness & Bias,A Simple and Adaptive Learning Rate for FTRL in Online Learning with Minimax Regret of $\Theta(T^{2/3})$ and its Application to Best-of-Both-Worlds,"Follow-the-Regularized-Leader (FTRL) is a powerful framework for various online learning problems. By designing its regularizer and learning rate to be adaptive to past observations, FTRL is known to work adaptively to various properties of an underlying environment. However, most existing adaptive learning rates are for online learning problems with a minimax regret of $\Theta(\sqrt{T})$ for the number of rounds $T$, and there are only a few studies on adaptive learning rates for problems with a minimax regret of $\Theta(T^{2/3})$, which include several important problems dealing with indirect feedback. To address this limitation, we establish a new adaptive learning rate framework for problems with a minimax regret of $\Theta(T^{2/3})$. Our learning rate is designed by matching the stability, penalty, and bias terms that naturally appear in regret upper bounds for problems with a minimax regret of $\Theta(T^{2/3})$. As applications of this framework, we consider three major problems with a minimax regret of $\Theta(T^{2/3})$: partial monitoring, graph bandits, and multi-armed bandits with paid observations. We show that FTRL with our learning rate and the Tsallis entropy regularizer improves existing Best-of-Both-Worlds (BOBW) regret upper bounds, which achieve simultaneous optimality in the stochastic and adversarial regimes. The resulting learning rate is surprisingly simple compared to the existing learning rates for BOBW algorithms for problems with a minimax regret of $\Theta(T^{2/3})$.","['follow-the-regularized-leader', 'adaptive learning rate', 'best-of-both-worlds algorithm', 'partial monitoring', 'graph bandits', 'online learning']",[],"['Taira Tsuchiya', 'Shinji Ito']","['RIKEN', 'The University of Tokyo']",
https://openreview.net/forum?id=ChnJ3W4HFG,Fairness & Bias,Enhancing Robustness of Last Layer Two-Stage Fair Model Corrections,"Last-layer retraining methods have emerged as an efficient framework for correcting existing base models. Within this framework, several methods have been proposed to deal with correcting models for subgroup fairness with and without group membership information. Importantly, prior work has demonstrated that many methods are susceptible to noisy labels. To this end, we propose a drop-in correction for label noise in last-layer retraining, and demonstrate that it achieves state-of-the-art worst-group accuracy for a broad range of symmetric label noise and across a wide variety of datasets exhibiting spurious correlations. Our proposed approach uses label spreading on a latent nearest neighbors graph and has minimal computational overhead compared to existing methods.","['fairness', 'robustness', 'worst-group accuracy', 'subgroup', 'subpopulation']",[],"['Nathan Stromberg', 'Rohan Ayyagari', 'Sanmi Koyejo', 'Richard Nock', 'Lalitha Sankar']","['Arizona State University', 'Arizona State University', 'Virtue AI', 'Google Research', 'Arizona State University']",
https://openreview.net/forum?id=CgGjT8EG8A,Fairness & Bias,Universal Exact Compression of Differentially Private Mechanisms,"To reduce the communication cost of differential privacy mechanisms, we introduce a novel construction, called Poisson private representation (PPR), designed to compress and simulate any local randomizer while ensuring local differential privacy. Unlike previous simulation-based local differential privacy mechanisms, PPR exactly preserves the joint distribution of the data and the output of the original local randomizer. Hence, the PPR-compressed privacy mechanism retains all desirable statistical properties of the original privacy mechanism such as unbiasedness and Gaussianity. Moreover, PPR achieves a compression size within a logarithmic gap from the theoretical lower bound. Using the PPR, we give a new order-wise trade-off between communication, accuracy, central and local differential privacy for distributed mean estimation. Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.","['Differential Privacy', 'Channel Simulation', 'Federated Learning', 'Communication', 'Poisson Process']",[],"['Yanxiao Liu', 'Wei-Ning Chen', 'Ayfer Ozgur', 'Cheuk Ting Li']","['Department of Information Engineering, The Chinese University of Hong Kong', 'Stanford University', 'Stanford University', 'Department of Information Engineering, The Chinese University of Hong Kong']",
https://openreview.net/forum?id=Cw7Agrr8GJ,Transparency & Explainability,Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning,"Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.","['Large Language Model', 'Temporal Knowledge Graph', 'Knowledge Graph Reasoning']",[],"['Jiapu Wang', 'Kai Sun', 'LINHAO LUO', 'Wei Wei', 'Yongli Hu', 'Alan Wee-Chung Liew', 'Shirui Pan', 'Baocai Yin']","['Beijing University of Technology', 'Beijing University Of Technology', 'Monash University', '', 'School of Information Science and Technology, Beijing University of Technology', 'Griffith University', 'Griffith University', 'School of Information Science and Technology, Beijing University of Technology']",
https://openreview.net/forum?id=Fr9d1UMc37,Security,LLM Dataset Inference: Did you train on my dataset?,"The proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs).  We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful.  However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values $< 0.1$, without any false positives.","['LLM', 'dataset inference', 'membership inference', 'copyright']",[],"['Pratyush Maini', 'Hengrui Jia', 'Nicolas Papernot', 'Adam Dziedzic']","['Carnegie Mellon University', 'University of  Toronto', 'University of Toronto', '']",
https://openreview.net/forum?id=D4yRz3s7UL,Security,DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms,"Vision transformers have shown remarkable advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification mechanisms have been proposed to address this issue. These mechanisms employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model’s efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector – carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system’s resources, while maintaining its stealthiness. Our evaluation demonstrates the attack’s effectiveness on three token sparsification mechanisms and examines the attack’s transferability between them and its effect on the GPU resources. To mitigate the impact of the attack, we propose various countermeasures.","['Adversarial Attack', 'Vision Transformers', 'Token Sparsification']",[],"['Oryan Yehezkel', 'Alon Zolfi', 'Amit Baras', 'Yuval Elovici', 'Asaf Shabtai']","['Ben Gurion University of the Negev', 'Software and Information Systems Engineering, Ben Gurion University of the Negev', 'Ben-Gurion University of the Negev', 'Information and Software Engineering, Ben Gurion University of the Negev, Technion', '']",
https://openreview.net/forum?id=DexM7d1H6e,Fairness & Bias,Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding,"With the emergence of large pre-trained multimodal video models, multiple benchmarks have been proposed to evaluate model capabilities. However, most of the benchmarks are human-centric, with evaluation data and tasks centered around human applications. Animals are an integral part of the natural world, and animal-centric video understanding is crucial for animal welfare and conservation efforts. Yet, existing benchmarks overlook evaluations focused on animals, limiting the application of the models. To address this limitation, our work established an animal-centric benchmark, namely Animal-Bench, to allow for a comprehensive evaluation of model capabilities in real-world contexts, overcoming agent-bias in previous benchmarks. Animal-Bench includes 13 tasks encompassing both common tasks shared with humans and special tasks relevant to animal conservation, spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To generate this benchmark, we defined a task system centered on animals and proposed an automated pipeline for animal-centric data processing. To further validate the robustness of models against real-world challenges, we utilized a video editing approach to simulate realistic scenarios like weather changes and shooting parameters due to animal movements. We evaluated 8 current multimodal video models on our benchmark and found considerable room for improvement. We hope our work provides insights for the community and opens up new avenues for research in multimodal video models. Our data and code will be released at https://github.com/PRIS-CV/Animal-Bench.","['Multimodal video model', 'Evaluation benchmark', 'Robustness testing']",[],"['Yinuo Jing', 'Ruxu Zhang', 'Kongming Liang', 'Yongxiang Li', 'Zhongjiang He', 'Zhanyu Ma', 'Jun Guo']","['Beijing University of Posts and Telecommunications', 'lut', 'Beijing University of Posts and Telecommunications', '', 'Institute of AI, ChinaTelecom', 'Beijing University of Post and Telecommunication', 'Beijing University of Posts and Telecommunications']",
https://openreview.net/forum?id=Dr7UarlhVE,Privacy & Data Governance,Exactly Minimax-Optimal Locally Differentially Private Sampling,"The sampling problem under local differential privacy has recently been studied with potential applications to generative models, but a fundamental analysis of its privacy-utility trade-off (PUT) remains incomplete. In this work, we define the fundamental PUT of private sampling in the minimax sense, using the $f$-divergence between original and sampling distributions as the utility measure. We characterize the exact PUT for both finite and continuous data spaces under some mild conditions on the data distributions, and  propose sampling mechanisms that are universally optimal for all $f$-divergences. Our numerical experiments demonstrate the superiority of our mechanisms over baselines, in terms of theoretical utilities for finite data space and of empirical utilities for continuous data space.","['Local differential privacy', 'Sampling', 'Privacy-utility trade-off']",[],"['Hyun-Young Park', 'Shahab Asoodeh', 'Si-Hyeon Lee']","['Electrical Engineering, Korea Advanced Institute of Science & Technology', 'McMaster University', 'Korea Advanced Institute of Science & Technology']",
https://openreview.net/forum?id=E2odGznGim,Security,Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack,"Deep neural networks face persistent challenges in defending against backdoor attacks, leading to an ongoing battle between attacks and defenses. While existing backdoor defense strategies have shown promising performance on reducing attack success rates, can we confidently claim that the backdoor threat has truly been eliminated from the model? To address it, we re-investigate the characteristics of the backdoored models after defense (denoted as defense models). Surprisingly, we find that the original backdoors still exist in defense models derived from existing post-training defense strategies, and the backdoor existence is measured by a novel metric called backdoor existence coefficient. It implies that the backdoors just lie dormant rather than being eliminated. To further verify this finding, we empirically show that these dormant backdoors can be easily re-activated during inference stage, by manipulating the original trigger with well-designed tiny perturbation using universal adversarial attack. More practically, we extend our backdoor re-activation to black-box scenario, where the defense model can only be queried by the adversary during inference stage, and develop two effective methods, i.e., query-based and transfer-based backdoor re-activation attacks. The effectiveness of the proposed methods are verified on both image classification and multimodal contrastive learning (i.e., CLIP) tasks. In conclusion, this work uncovers a critical vulnerability that has never been explored in existing defense strategies, emphasizing the urgency of designing more robust and advanced backdoor defense mechanisms in the future.",['Backdoor Learning; Backdoor Attack; Backdoor Defense; Adversarial Machine Learning'],[],"['Mingli Zhu', 'Siyuan Liang', 'Baoyuan Wu']","['The Chinese University of Hong Kong(Shen Zhen)', 'National University of Singapore', 'The Chinese University of Hong Kong, Shenzhen']",
https://openreview.net/forum?id=EHXyeImux0,Security,Data Mixture Inference Attack: BPE Tokenizers Reveal Training Data Compositions,"The pretraining data of today's strongest language models remains opaque, even when their parameters are open-sourced. In particular, little is known about the proportions of different domains, languages, or code represented in the data. While a long line of membership inference attacks aim to identify training examples on an instance level, they do not extend easily to *global* statistics about the corpus. In this work, we tackle a task which we call *data mixture inference*, which aims to uncover the distributional make-up of the pretraining data. We introduce a novel attack based on a previously overlooked source of information — byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered vocabulary learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first token is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest (e.g., different natural languages), we formulate a linear program that solves for the relative proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data. In controlled experiments, we show that our attack can recover mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released alongside recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o is much more multilingual than its predecessors, training on 10x more non-English data than GPT-3.5, Llama 3 and Claude are trained on predominantly code, and many recent models are trained on 7-16% books. We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs.","['tokenizers', 'distribution inference', 'security']",[],"['Jonathan Hayase', 'Alisa Liu', 'Yejin Choi', 'Sewoong Oh', 'Noah A. Smith']","['Paul G. Allen School of Computer Science & Engineering, University of Washington', 'University of Washington', 'Computer Science, Computer Science Department, Stanford University', 'Allen school, University of Washington', 'Computer Science & Engineering, University of Washington']",
https://openreview.net/forum?id=EVw8Jh5Et9,Privacy & Data Governance,Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning,"Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks. To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks. However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation. Few recent efforts to simultaneously tackle both challenges offen depend on impractical assumption of non-colluding two-server setups that disrupt FL's topology, or three-party computation which introduces scalability issues, complicating deployment and application. To overcome this dilemma, this paper introduce a Dual Defense Federated learning (DDFed) framework. DDFed simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology. DDFed initially leverages cutting-edge fully homomorphic encryption (FHE) to securely aggregate model updates, without the impractical requirement for non-colluding two-server setups and ensures strong privacy protection. Additionally, we proposes a unique two-phase anomaly detection mechanism for encrypted model updates, featuring secure similarity computation and feedback-driven collaborative selection, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection process. We conducted extensive experiments on various model poisoning attacks and FL scenarios, including both cross-device and cross-silo FL. Experiments on publicly available datasets demonstrate that DDFed successfully protects model privacy and effectively defends against model poisoning threats.","['federated learning', 'privacy preservation', 'secure aggregation', 'model poisoning attack']",[],"['Runhua Xu', 'Shiqi Gao', 'Chao Li', 'James Joshi', 'Jianxin Li']","['Beijing University of Aeronautics and Astronautics', 'Beijing University of Aeronautics and Astronautics', 'Department of Computer Science and Technology, Beijing Jiaotong University', 'University of Pittsburgh', 'School of Computer Science and Engineering, Beihang University']",
https://openreview.net/forum?id=EWxNEnFjKR,Fairness & Bias,Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution,"Detecting and rejecting unknown out-of-distribution (OOD) samples is critical for deployed neural networks to void unreliable predictions. In real-world scenarios, however, the efficacy of existing OOD detection methods is often impeded by the inherent imbalance of in-distribution (ID) data, which causes significant performance decline. Through statistical observations, we have identified two common challenges faced by different OOD detectors: misidentifying tail class ID samples as OOD, while erroneously predicting OOD samples as head class from ID. To explain this phenomenon, we introduce a generalized statistical framework, termed ImOOD, to formulate the OOD detection problem on imbalanced data distribution. Consequently, the theoretical analysis reveals that there exists a class-aware bias item between balanced and imbalanced OOD detection, which contributes to the performance gap. Building upon this finding, we present a unified training-time regularization technique to mitigate the bias and boost imbalanced OOD detectors across architecture designs. Our theoretically grounded method translates into consistent improvements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks against several state-of-the-art OOD detection ap- proaches. Code is available at https://github.com/alibaba/imood.","['Out-of-Distribution Detection', 'Imbalanced Recognition']",[],"['Kai Liu', 'Zhihang Fu', 'Sheng Jin', 'Chao Chen', 'Ze Chen', 'Rongxin Jiang', 'Fan Zhou', 'Yaowu Chen', 'Jieping Ye']","['National University of Singapore', 'Alibaba Group', 'ByteDance Inc.', 'Alibaba Group', 'Alibaba Group', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Alibaba Group']",
https://openreview.net/forum?id=NTkYSWnVjl,Security,Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection,"It is well known that query-based attacks tend to have relatively higher success rates in adversarial black-box attacks. While research on black-box attacks is actively being conducted, relatively few studies have focused on pixel attacks that target only a limited number of pixels. In image classification, query-based pixel attacks often rely on patches, which heavily depend on randomness and neglect the fact that scattered pixels are more suitable for adversarial attacks. Moreover, to the best of our knowledge, query-based pixel attacks have not been explored in the field of object detection. To address these issues, we propose a novel pixel-based black-box attack called Remember and Forget Pixel Attack using Reinforcement Learning(RFPAR), consisting of two main components: the Remember and Forget processes. RFPAR mitigates randomness and avoids patch dependency by leveraging rewards generated through a one-step RL algorithm to perturb pixels. RFPAR effectively creates perturbed images that minimize the confidence scores while adhering to limited pixel constraints. Furthermore, we advance our proposed attack beyond image classification to object detection, where RFPAR reduces the confidence scores of detected objects to avoid detection. Experiments on the ImageNet-1K dataset for classification show that RFPAR outperformed state-of-the-art query-based pixel attacks. For object detection, using the MSCOCO dataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAP reduction to state-of-the-art query-based attack while requiring fewer query. Further experiments on the Argoverse dataset using YOLOv8 confirm that RFPAR effectively removed objects on a larger scale dataset. Our code is available at https://github.com/KAU-QuantumAILab/RFPAR.","['Adversarial attack', 'Query-based pixel attack', 'Image classification', 'Object detection', 'RL']",[],"['Dongsu Song', 'Daehwa Ko', 'Jay Hoon Jung']","['Korea Aerospace University', 'Department of software, Korea Aerospace University', '']",
https://openreview.net/forum?id=F8aSOovlEP,Transparency & Explainability,MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning,"Video causal reasoning aims to achieve a high-level understanding of video content from a causal perspective. However, current video reasoning tasks are limited in scope, primarily executed in a question-answering paradigm and focusing on short videos containing only a single event and simple causal relationships, lacking comprehensive and structured causality analysis for videos with multiple events. To fill this gap, we introduce a new task and dataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal relationships between events distributed chronologically across long videos. Given visual segments and textual descriptions of events, MECD requires identifying the causal associations between these events to derive a comprehensive, structured event-level video causal diagram explaining why and how the final result event occurred. To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient mask-based event prediction model to perform an Event Granger Test, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked. Furthermore, we integrate causal inference techniques such as front-door adjustment and counterfactual inference to address challenges in MECD like causality confounding and illusory causality. Experiments validate the effectiveness of our framework in providing causal relationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively.",['Video understanding; Video reasoning; Causal discovery; Causal inference'],[],"['Tieyuan Chen', 'Huabin Liu', 'Tianyao He', 'Yihang Chen', 'Chaofan Gan', 'Xiao Ma', 'Cheng Zhong', 'Yang Zhang', 'Yingxue Wang', 'Hui Lin', 'Weiyao Lin']","['Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'School of Electronic Information and Electrical Engineering, Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'SIEEE, Shanghai Jiaotong University', 'Lenovo Group Limited', 'Lenovo Research', 'Lenovo Research, Lenovo Research, AI Lab', 'Beijing Institute of Technology', 'National Engineering Laboratory for Public Security Risk Perception and Control by Big Data, Electronic Science Research Institute of China Electronics Technology Group Corporation', 'School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University']",
https://openreview.net/forum?id=FMrNus3d0n,Security,GuardT2I: Defending Text-to-Image Models from Adversarial Prompts,"Recent advancements in Text-to-Image models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work contents, despite existing countermeasures such as Not-Safe-For-Work classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I a novel moderation framework that adopts a generative approach to enhance Text-to-Image models’ robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a large language model to conditionally transform text guidance embeddings within the Text-to-Image models into natural language for effective adversarial prompt detection, without compromising the models’ inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.  Our framework is available at https://github.com/cure-lab/GuardT2I.",['Text-to-Image generation'],[],"['Yijun Yang', 'Ruiyuan Gao', 'Xiao Yang', 'Jianyuan Zhong', 'Qiang Xu']","['CS, Tsinghua University', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Tsinghua University, Tsinghua University', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'The Chinese University of Hong Kong']",
https://openreview.net/forum?id=FNtsZLwkGr,Transparency & Explainability,Pruning neural network models for gene regulatory dynamics using data and domain knowledge,"The practical utility of machine learning models in the sciences often hinges on their interpretability. It is common to assess a model's merit for scientific discovery, and thus novel insights, by how well it aligns with already available domain knowledge - a dimension that is currently largely disregarded in the comparison of neural network models. While pruning can simplify deep neural network architectures and excels in identifying sparse models, as we show in the context of gene regulatory network inference, state-of-the-art techniques struggle with biologically meaningful structure learning. To address this issue, we propose DASH, a generalizable framework that guides network pruning by using domain-specific structural information in model fitting and leads to sparser, better interpretable models that are more robust to noise. Using both synthetic data with ground truth information, as well as real-world gene expression data, we show that DASH, using knowledge about gene interaction partners within the putative regulatory network, outperforms general pruning methods by a large margin and yields deeper insights into the biological systems being studied.","['neural network pruning', 'sparsification', 'domain knowledge', 'gene regulation']",[],"['Intekhab Hossain', 'Jonas Fischer', 'Rebekka Burkholz', 'John Quackenbush']","['Harvard University, Harvard University', '', 'Helmholtz Center CISPA for Information Security', ""Brigham and Women's Hospital""]",
https://openreview.net/forum?id=FZ45kf5pIA,Security,Edit Distance Robust Watermarks via Indexing Pseudorandom Codes,"Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn, & Zamir (2023) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.   Our main result is a watermarking scheme which achieves both (a) and (b) when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our codes have the additional benefit of relying on weaker computational assumptions than used in previous work.   Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.","['Watermarking', 'pseudorandom codes', 'language models']",[],"['Noah Golowich', 'Ankur Moitra']","['EECS, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=FY6vPtITtE,Fairness & Bias,The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks,"The Neural Tangent Kernel (NTK) viewpoint is widely employed to analyze the training dynamics of overparameterized Physics-Informed Neural Networks (PINNs). However, unlike the case of linear Partial Differential Equations (PDEs), we show how the NTK perspective falls short in the nonlinear scenario. Specifically, we establish that the NTK yields a random matrix at initialization that is not constant during training, contrary to conventional belief. Another significant difference from the linear regime is that, even in the idealistic infinite-width limit, the Hessian does not vanish and hence it cannot be disregarded during training. This motivates the adoption of second-order optimization methods. We explore the convergence guarantees of such methods in both linear and nonlinear cases, addressing challenges such as spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we highlight the benefits of second-order methods in benchmark test cases.","['Physics-Informed Neural Networks', 'Neural Tangent Kernel', 'Nonlinear PDEs', 'Second-order optimization']",[],"['Andrea Bonfanti', 'Giuseppe Bruno', 'Cristina Cipriani']","['FG-431, Basque Center for Applied Mathematics', 'IMSV, Universität Bern', 'Technische Universität München']",
https://openreview.net/forum?id=FfFcDNDNol,Security,When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search,"Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to ""fool"" LLMs into responding to harmful questions. Early-stage jailbreaking attacks require access to model internals or significant human efforts.  More advanced attacks utilize genetic algorithms for automatic and black-box attacks. However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks. In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL). We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms. Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm. Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs.  We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs. We further validate the key design choices of RLbreaker via a comprehensive ablation study.",['jailbreaking attack; LLM security; deep reinforcement learning'],[],"['Xuan Chen', 'Yuzhou Nie', 'Wenbo Guo', 'Xiangyu Zhang']","['', 'Computer Science, University of California, Santa Barbara', 'University of California, Santa Barbara', 'Computer Science, Purdue University']",
https://openreview.net/forum?id=Fp3JVz5XE7,Privacy & Data Governance,Federated Black-Box Adaptation for Semantic Segmentation,"Federated Learning (FL) is a form of distributed learning that allows multiple institutions or clients to collaboratively learn a global model to solve a task. This allows the model to utilize the information from every institute while preserving data privacy. However, recent studies show that the promise of protecting the privacy of data is not upheld by existing methods and that it is possible to recreate the training data from the different institutions. This is done by utilizing gradients transferred between the clients and the global server during training or by knowing the model architecture at the client end. In this paper, we propose a federated learning framework for semantic segmentation without knowing the model architecture nor transferring gradients between the client and the server, thus enabling better privacy preservation. We propose \textit{BlackFed} - a black-box adaptation of neural networks that utilizes zero order optimization (ZOO) to update the client model weights and first order optimization (FOO) to update the server weights. We evaluate our approach on several computer vision and medical imaging datasets to demonstrate its effectiveness. To the best of our knowledge, this work is one of the first works in employing federated learning for segmentation, devoid of gradients or model information exchange. Code: https://github.com/JayParanjape/blackfed/tree/master","['Federated Learning', 'Blackbox Learning', 'Split Networks', 'Segmentation']",[],"['Jay Nitin Paranjape', 'Shameema Sikder', 'S. Swaroop Vedula', 'Vishal M. Patel']","['Electrical and Computer Engineering, Johns Hopkins University', 'Ophthalmology, Johns Hopkins University', 'CS, Johns Hopkins', 'ECE, Johns Hopkins University']",
https://openreview.net/forum?id=FsA0OSsdzJ,Transparency & Explainability,Structured Learning of Compositional Sequential Interventions,"We consider sequential treatment regimes where each unit is exposed to combinations of interventions over time. When interventions are described by qualitative labels, such as ""close schools for a month due to a pandemic"" or ""promote this podcast to this user during this week"", it is unclear which appropriate structural assumptions allow us to generalize behavioral predictions to previously unseen combinations of interventions. Standard black-box approaches mapping sequences of categorical variables to outputs are applicable, but they rely on poorly understood assumptions on how reliable generalization can be obtained, and may underperform under sparse sequences, temporal variability, and large action spaces. To approach that,  we pose an explicit model for composition, that is, how the effect of sequential interventions can be isolated into modules, clarifying which data conditions allow for the identification of their combined effect at different units and time steps. We show the identification properties of our compositional model, inspired by advances in causal matrix factorization methods. Our focus is on predictive models for novel compositions of interventions instead of matrix completion tasks and causal effect estimation. We compare our approach to flexible but generic black-box models to illustrate how structure aids prediction in sparse data conditions.","['Causality', 'sequential data']",[],"['Jialin Yu', 'Andreas Koukorinis', 'Nicolò Colombo', 'Yuchen Zhu', 'Ricardo Silva']","['Statistical Science, University College London, University of London', 'University College London', 'Royal Holloway, University of London', 'Computer Science, University College London', 'University College London']",
https://openreview.net/forum?id=GNhrGRCerd,Security,Trap-MID: Trapdoor-based Defense against Model Inversion Attacks,"Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models. While existing defenses often rely on regularization techniques to reduce information leakage, they remain vulnerable to recent attacks. In this paper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to mislead MI attacks. A trapdoor is integrated into the model to predict a specific label when the input is injected with the corresponding trigger. Consequently, this trapdoor information serves as the ""shortcut"" for MI attacks, leading them to extract trapdoor triggers rather than private data. We provide theoretical insights into the impacts of trapdoor's effectiveness and naturalness on deceiving MI attacks. In addition, empirical experiments demonstrate the state-of-the-art defense performance of Trap-MID against various MI attacks without the requirements for extra data or large computational overhead. Our source code is publicly available at https://github.com/ntuaislab/Trap-MID.","['model inversion attacks', 'privacy', 'defense', 'trapdoor', 'backdoor']",[],"['Zhen-Ting Liu', 'Shang-Tse Chen']","['National Taiwan University', 'Department of Computer Science and Information Engineering, National Taiwan University']",
https://openreview.net/forum?id=GRmQjLzaPM,Security,BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction,"Simulating realistic behaviors of traffic agents is pivotal for efficiently validating the safety of autonomous driving systems. Existing data-driven simulators primarily use an encoder-decoder architecture to encode the historical trajectories before decoding the future. However, the heterogeneity between encoders and decoders complicates the models, and the manual separation of historical and future trajectories leads to low data utilization. Given these limitations, we propose BehaviorGPT, a homogeneous and fully autoregressive Transformer designed to simulate the sequential behavior of multiple agents. Crucially, our approach discards the traditional separation between ""history"" and ""future"" by modeling each time step as the ""current"" one for motion generation, leading to a simpler, more parameter- and data-efficient agent simulator. We further introduce the Next-Patch Prediction Paradigm (NP3) to mitigate the negative effects of autoregressive modeling, in which models are trained to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. Despite having merely 3M model parameters, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a realism score of 0.7473 and a minADE score of 1.4147, demonstrating its exceptional performance in traffic agent simulation.","['Multi-Agent Systems', 'Transformers', 'Generative Models', 'Autonomous Driving']",[],"['Zikang Zhou', 'Haibo HU', 'Xinhong Chen', 'Jianping Wang', 'Nan Guan', 'Kui Wu', 'Yung-Hui Li', 'Yu-Kai Huang', 'Chun Jason Xue']","['Computer Science, City University of Hong Kong', 'Computer Science, City University of Hong Kong', 'City University of Hong Kong', 'Computer Science , City University of Hong Kong', 'City University of Hong Kong', 'University of Victoria', 'AI Research Center, Hon Hai Research Institute', 'Electrical and Computer Engineering, Carnegie Mellon University', 'cs, Mohamed bin Zayed University of Artificial Intelligence']",
https://openreview.net/forum?id=GZnsqBwHAG,Security,Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models,"Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as “safety basin”: random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt’s critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide new insights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape.",['LLM safety; LLM Landscape; LLM Finetuning; Attack and Defense'],[],"['ShengYun Peng', 'Pin-Yu Chen', 'Matthew Daniel Hull', 'Duen Horng Chau']","['Computer Science, Georgia Institute of Technology', 'International Business Machines', 'Georgia Institute of Technology', 'College of Computing, Georgia Institute of Technology']",
https://openreview.net/forum?id=ELnxXc8pik,Fairness & Bias,Hierarchy-Agnostic Unsupervised Segmentation: Parsing Semantic Image Structure,"Unsupervised semantic segmentation aims to discover groupings within images, capturing objects' view-invariance without external supervision. Moreover, this task is inherently ambiguous due to the varying levels of semantic granularity. Existing methods often bypass this ambiguity using dataset-specific priors. In our research, we address this ambiguity head-on and provide a universal tool for pixel-level semantic parsing of images guided by the latent representations encoded in self-supervised models.  We introduce a novel algebraic approach that recursively decomposes an image into nested subgraphs, dynamically estimating their count and ensuring clear separation. The innovative approach identifies scene-specific primitives and constructs a hierarchy-agnostic tree of semantic regions from the image pixels.  The model captures fine and coarse semantic details, producing a nuanced and unbiased segmentation.  We present a new metric for estimating the quality of the semantic segmentation of discovered elements on different levels of the hierarchy. The metric validates the intrinsic nature of the compositional relations among parts, objects, and scenes in a hierarchy-agnostic domain. Our results prove the power of this methodology, uncovering semantic regions without prior definitions and scaling effectively across various datasets. This robust framework for unsupervised image segmentation proves more accurate semantic hierarchical relationships between scene elements than traditional algorithms. The experiments underscore its potential for broad applicability in image analysis tasks, showcasing its ability to deliver a detailed and unbiased segmentation that surpasses existing unsupervised methods.","['Unsupervised Hierarchical Segmentation', 'Spectral Clustering', 'Self-Supervised Feature Extraction', 'Semantic Region Tree']",[],"['Simone Rossetti', 'fiora pirri']","['DIAG, DeepPlants Srl', 'DeepPlants srl']",
https://openreview.net/forum?id=GhqdnLZMAz,Transparency & Explainability,Improving Decision Sparsity,"Sparsity is a central aspect of interpretability in machine learning. Typically, sparsity is measured in terms of the size of a model globally, such as the number of variables it uses. However, this notion of sparsity is not particularly relevant for decision making; someone subjected to a decision does not care about variables that do not contribute to the decision. In this work, we dramatically expand a notion of *decision sparsity* called the *Sparse Explanation Value* (SEV) so that its explanations are more meaningful. SEV considers movement along a hypercube towards a reference point. By allowing flexibility in that reference and by considering how distances along the hypercube translate to distances in feature space, we can derive sparser and more meaningful explanations for various types of function classes. We present cluster-based SEV and its variant tree-based SEV, introduce a method that improves credibility of explanations, and propose algorithms that optimize decision sparsity in machine learning models.","['sparse explanation', 'decision sparsity']",[],"['Yiyang Sun', 'Tong Wang', 'Cynthia Rudin']","['Computer Science, Duke University', 'Yale University', '']",
https://openreview.net/forum?id=GgV6UczIWM,Fairness & Bias,A distributional simplicity bias in the learning dynamics of transformers,"The remarkable capability of over-parameterised neural networks to generalise effectively has been explained by invoking a ``simplicity bias'': neural networks prevent overfitting by initially learning simple classifiers before progressing to more complex, non-linear functions. While simplicity biases have been described theoretically and experimentally in feed-forward networks for supervised learning, the extent to which they also explain the remarkable success of transformers trained with self-supervised techniques remains unclear. In our study, we demonstrate that transformers, trained on natural language data, also display a simplicity bias. Specifically, they sequentially learn many-body interactions among input tokens, reaching a saturation point in the prediction error for low-degree interactions while continuing to learn high-degree interactions.  To conduct this analysis, we develop a procedure to generate \textit{clones} of a given natural language data set, which rigorously capture the interactions between tokens up to a specified order. This approach opens up the possibilities of studying how interactions of different orders in the data affect learning, in natural language processing and beyond.","['Transformers', 'Natural Language Processing', 'Sequential Learning', 'Simplicity Bias']",[],"['Riccardo Rende', 'Federica Gerace', 'Alessandro Laio', 'Sebastian Goldt']","['Theoretical and Scientific Data Science, International Higher School for Advanced Studies Trieste', 'International Higher School for Advanced Studies Trieste', 'SISSA/ISAS', 'SISSA']",
https://openreview.net/forum?id=GruuYVTGXV,Fairness & Bias,Dual Critic Reinforcement Learning under Partial Observability,"Partial observability in environments poses significant challenges that impede the formation of effective policies in reinforcement learning. Prior research has shown that borrowing the complete state information can enhance sample efficiency. This strategy, however, frequently encounters unstable learning with high variance in practical applications due to the over-reliance on complete information. This paper introduces DCRL, a Dual Critic Reinforcement Learning framework designed to adaptively harness full-state information during training to reduce variance for optimized online performance. In particular, DCRL incorporates two distinct critics: an oracle critic with access to complete state information and a standard critic functioning within the partially observable context. It innovates a synergistic strategy to meld the strengths of the oracle critic for efficiency improvement and the standard critic for variance reduction, featuring a novel mechanism for seamless transition and weighting between them. We theoretically prove that DCRL mitigates the learning variance while maintaining unbiasedness. Extensive experimental analyses across the Box2D and Box3D environments have verified DCRL's superior performance. The source code is available in the supplementary.","['Reinforcement Learning', 'Partial Observability', 'POMDP']",[],"['Jinqiu Li', 'Enmin Zhao', 'Tong Wei', 'Junliang Xing', 'Shiming Xiang']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'JD.com', 'Department of Computer Science and Technology, Tsinghua University, Tsinghua University', 'Department of Computer Science and Technology, Tsinghua University, Tsinghua University', 'National Laboratory of Pattern Recognition, Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",
https://openreview.net/forum?id=GtEmIzLZmR,Transparency & Explainability,Achievable Fairness on Your Data With Utility Guarantees,"In machine learning fairness, training models that minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off inherently depends on dataset characteristics such as dataset imbalances or biases and therefore, using a uniform fairness requirement across diverse datasets remains questionable. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Crucially, we introduce a novel methodology for quantifying uncertainty in our estimates, thereby providing practitioners with a robust framework for auditing model fairness while avoiding false conclusions due to estimation errors. Our experiments spanning tabular (e.g., Adult), image (CelebA), and language (Jigsaw) datasets underscore that our approach not only reliably quantifies the optimum achievable trade-offs across various data modalities but also helps detect suboptimality in SOTA fairness methods.","['fairness', 'dataset', 'accuracy-fairness trade-off', 'uncertainty quantification']",[],"['Muhammad Faaiz Taufiq', 'Jean-Francois Ton', 'Yang Liu']","['AI Lab, ByteDance Inc.', 'R&D, Bytedance', 'Computer Science and Engineering, University of California, Santa Cruz']",
https://openreview.net/forum?id=H2ATO32ilj,Security,ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users,"Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.","['Red-teaming', 'Text-to-image Model']",[],"['Guanlin Li', 'Kangjie Chen', 'Shudong Zhang', 'Jie Zhang', 'Tianwei Zhang']","['Nanyang Technological University', 'CCDS, Nanyang Technological University', 'Huawei Technologies Ltd.', 'CFAR, A*STAR', 'Nanyang Technological University']",
https://openreview.net/forum?id=HCTikT7LS4,Security,Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach,"Deep reinforcement learning agents achieve state-of-the-art performance in a wide range of simulated control tasks. However, successful applications to real-world problems remain limited. One reason for this dichotomy is because the learnt policies are not robust to observation noise or adversarial attacks. In this paper, we investigate the robustness of deep RL policies to a single small state perturbation in deterministic continuous control tasks. We demonstrate that RL policies can be deterministically chaotic, as small perturbations to the system state have a large impact on subsequent state and reward trajectories. This unstable non-linear behaviour has two consequences: first, inaccuracies in sensor readings, or adversarial attacks, can cause significant performance degradation; second, even policies that show robust performance in terms of rewards may have unpredictable behaviour in practice. These two facets of chaos in RL policies drastically restrict the application of deep RL to real-world problems. To address this issue, we propose an improvement on the successful Dreamer V3 architecture, implementing Maximal Lyapunov Exponent regularisation. This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks and thereby improving the suitability of deep reinforcement learning for real-world applications.","['Reinforcement Learning', 'Robust Reinforcement Learning', 'Stable Reinforcement Learning', 'Lyapunov Exponents', 'Chaos Theory']",[],"['Rory Young', 'Nicolas Pugeault']","['University of Glasgow', 'School of Computing Science, University of Glasgow']",
https://openreview.net/forum?id=HFS800reZK,Fairness & Bias,Learning Representations for Hierarchies with Minimal Support,"When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph.  In this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.","['graph embeddings', 'representation learning']",[],"['Benjamin Rozonoyer', 'Michael Boratko', 'Dhruvesh Patel', 'Wenlong Zhao', 'Shib Sankar Dasgupta', 'Hung Le', 'Andrew McCallum']","['UMass, Department of Computer Science', 'Google', 'College of Information and Computer Sciences, College of Information and Computer Science, University of Massachusetts, Amherst', 'College of Information and Computer Sciences, University of Massachusetts at Amherst', 'University of Massachusetts, Amherst', 'Department of Computer Science, University of Massachusetts at Amherst', 'Department of Computer Science, University of Massachusetts Amherst']",
https://openreview.net/forum?id=HPvIf4w5Dd,Fairness & Bias,Finding good policies in average-reward Markov Decision Processes without prior knowledge,"We revisit the identification of an $\varepsilon$-optimal policy in average-reward Markov Decision Processes (MDP). In such MDPs, two measures of complexity have appeared in the literature: the diameter, $D$, and the optimal bias span, $H$, which satisfy $H\leq D$.   Prior work have studied the complexity of $\varepsilon$-optimal policy identification only when a generative model is available. In this case, it is known that there exists an MDP with $D \simeq H$ for which the sample complexity to output an $\varepsilon$-optimal policy is $\Omega(SAD/\varepsilon^2)$ where $S$ and $A$ are the sizes of the state and action spaces. Recently, an algorithm with a sample complexity of order $SAH/\varepsilon^2$ has been proposed, but it requires the knowledge of $H$. We first show that the sample complexity required to estimate $H$ is not bounded by any function of $S,A$ and $H$, ruling out the possibility to easily make the previous algorithm agnostic to $H$. By relying instead on a diameter estimation procedure, we propose the first algorithm for $(\varepsilon,\delta)$-PAC policy identification that does not need any form of prior knowledge on the MDP. Its sample complexity scales in $SAD/\varepsilon^2$ in the regime of small $\varepsilon$, which is near-optimal. In the online setting, our first contribution is a lower bound which implies that a sample complexity polynomial in $H$ cannot be achieved in this setting. Then, we propose an online algorithm with a sample complexity in $SAD^2/\varepsilon^2$, as well as a novel approach based on a data-dependent stopping rule that we believe is promising to further reduce this bound.","['sample complexity', 'Markov decision process', 'best policy identification', 'average reward']",[],"['Adrienne Tuynman', 'Rémy Degenne', 'Emilie Kaufmann']","['INRIA', 'INRIA', 'CNRS']",
https://openreview.net/forum?id=HcifdQZFZV,Security,Safe LoRA: The Silver Lining of Reducing Safety Risks when Finetuning Large Language Models,"While large language models (LLMs) such as Llama-2 or GPT-4 have shown impressive zero-shot performance, fine-tuning is still necessary to enhance their performance for customized datasets, domain-specific tasks, or other private needs. However, fine-tuning all parameters of LLMs requires significant hardware resources, which can be impractical for typical users. Therefore, parameter-efficient fine-tuning such as LoRA have emerged, allowing users to fine-tune LLMs without the need for considerable computing resources, with little performance degradation compared to fine-tuning all parameters. Unfortunately, recent studies indicate that fine-tuning can increase the risk to the safety of LLMs, even when data does not contain malicious content. To address this challenge, we propose $\textsf{Safe LoRA}$, a simple one-liner patch to the original LoRA implementation by introducing the projection of LoRA weights from selected layers to the safety-aligned subspace, effectively reducing the safety risks in LLM fine-tuning while maintaining utility. It is worth noting that $\textsf{Safe LoRA}$ is a training-free and data-free approach, as it only requires the knowledge of the weights from the base and aligned LLMs. Our extensive experiments demonstrate that when fine-tuning on purely malicious data, $\textsf{Safe LoRA}$ retains similar safety performance as the original aligned model. Moreover, when the fine-tuning dataset contains a mixture of both benign and malicious data,  $\textsf{Safe LoRA}$ mitigates the negative effect made by malicious data while preserving performance on downstream tasks.  Our codes are available at https://github.com/IBM/SafeLoRA.","['Larage Language Models', 'safety']",[],"['Chia-Yi Hsu', 'Yu-Lin Tsai', 'Chih-Hsun Lin', 'Pin-Yu Chen', 'Chia-Mu Yu', 'Chun-Ying Huang']","['Computer Science , National Yang Ming Chiao Tung University', 'Arete Honors Program, National Yang Ming Chiao Tung University', 'National Chiao Tung University', 'International Business Machines', 'Electronics and Electrical Engineering, National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University']",
https://openreview.net/forum?id=IGhpUd496D,Security,Provable Editing of Deep Neural Networks using Parametric Linear Relaxation,"Ensuring that a DNN satisfies a desired property is critical when deploying DNNs in safety-critical applications. There are efficient methods that can verify whether a DNN satisfies a property, as seen in the annual DNN verification competition (VNN-COMP). However, the problem of provably editing a DNN to satisfy a property remains challenging. We present PREPARED, the first efficient technique for provable editing of DNNs. Given a DNN $\mathcal{N}$ with parameters $\theta$, input polytope $P$, and output polytope $Q$, PREPARED finds new parameters $\theta'$ such that $\forall \mathrm{x} \in P . \mathcal{N}(\mathrm{x}; \theta') \in Q$ while minimizing the changes $\lVert{\theta' - \theta}\rVert$. Given a DNN and a property it violates from the VNN-COMP benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds. PREPARED is efficient because it relaxes the NP-hard provable editing problem to solving a linear program. The key contribution is the novel notion of Parametric Linear Relaxation, which enables PREPARED to construct tight output bounds of the DNN that are parameterized by the new parameters $\theta'$. We demonstrate that PREPARED is more efficient and effective compared to prior DNN editing approaches i) using the VNN-COMP benchmarks, ii) by editing CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and iii) by training a DNN to model a geodynamics process and satisfy physics constraints.","['Provable editing', 'provable repair', 'provable training', 'trustworthiness', 'linear programming', 'local robustness', 'verification']",[],"['Zhe Tao', 'Aditya Thakur']","['Department of Computer Science, University of California, Davis', 'Computer Science, University of California, Davis']",
https://openreview.net/forum?id=I3IuclVLFZ,Fairness & Bias,FedLPA: One-shot Federated Learning with Layer-Wise Posterior Aggregation,"Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing communication overhead, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with layer-wise posterior aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any private label information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics.",['One-shot Federated Learning'],[],"['Xiang Liu', 'Liangxi Liu', 'Feiyang Ye', 'Yunheng Shen', 'Xia Li', 'Linshan Jiang', 'Jialin Li']","['National University of Singapore', 'SUSTech', 'University of Technology Sydney', '', '', 'National University of Singapore', 'School of Computing, National University of Singapore']",
https://openreview.net/forum?id=KsLX5pFpOs,Fairness & Bias,Proportional Fairness in Clustering: A Social Choice Perspective,"We study the proportional clustering problem of Chen et al. (ICML'19) and relate it to the area of multiwinner voting in computational social choice. We show that any clustering satisfying a weak proportionality notion of Brill and Peters (EC'23) simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al., but also to individual fairness (Jung et al., FORC'20) and the ``core'' (Li et al., ICML'21). In fact, we show that any approximation to proportional fairness is also an approximation to individual fairness and vice versa. Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters imply approximations to these stronger guarantees.","['clustering', 'fair clustering', 'proportional clustering', 'social choice', 'fairness', 'multiwinner voting']",[],"['Leon Kellerhals', 'Jannik Peters']","['Algorithmics and Computational Complexity, Technische Universität Berlin', 'National University of Singapore']",
https://openreview.net/forum?id=I96GFYalFO,Fairness & Bias,FedSSP: Federated Graph Learning with Spectral Knowledge and Personalized Preference,"Personalized Federated Graph Learning (pFGL) facilitates the decentralized training of Graph Neural Networks (GNNs) without compromising privacy while accommodating personalized requirements for non-IID participants. In cross-domain scenarios, structural heterogeneity poses significant challenges for pFGL. Nevertheless, previous pFGL methods incorrectly share non-generic knowledge globally and fail to tailor personalized solutions locally under domain structural shift. We innovatively reveal that the spectral nature of graphs can well reflect inherent domain structural shifts. Correspondingly, our method overcomes it by sharing generic spectral knowledge. Moreover, we indicate the biased message-passing schemes for graph structures and propose the personalized preference module. Combining both strategies, we propose our pFGL framework $\textbf{FedSSP}$ which $\textbf{S}$hares generic $\textbf{S}$pectral knowledge while satisfying graph $\textbf{P}$references. Furthermore, We perform extensive experiments on cross-dataset and cross-domain settings to demonstrate the superiority of our framework. The code is available at https://github.com/OakleyTan/FedSSP.",['Federated Learning; Graph Learning'],[],"['Zihan Tan', 'Guancheng Wan', 'Wenke Huang', 'Mang Ye']","['School of Computer Science, Wuhan University', 'Emory University, Emory University', 'Wuhan University, Wuhan University', 'Wuhan University']",
https://openreview.net/forum?id=I90ypQpLgL,Fairness & Bias,Fair Online Bilateral Trade,"In online bilateral trade, a platform posts prices to incoming pairs of buyers and sellers that have private valuations for a certain good. If the price is lower than the buyers' valuation and higher than the sellers' valuation, then a trade takes place. Previous work focused on the platform perspective, with the goal of setting prices maximizing the *gain from trade* (the sum of sellers' and buyers' utilities). Gain from trade is, however, potentially unfair to traders, as they may receive highly uneven shares of the total utility. In this work we enforce fairness by rewarding the platform with the _fair gain from trade_, defined as the minimum between sellers' and buyers' utilities. After showing that any no-regret learning algorithm designed to maximize the sum of the utilities may fail badly with fair gain from trade, we present our main contribution: a complete characterization of the regret regimes for fair gain from trade when, after each interaction, the platform only learns whether each trader accepted the current price. Specifically, we prove the following regret bounds: $\Theta(\ln T)$ in the deterministic setting, $\Omega(T)$ in the stochastic setting, and $\tilde{\Theta}(T^{2/3})$ in the stochastic setting when sellers' and buyers' valuations are independent of each other. We conclude by providing tight regret bounds when, after each interaction, the platform is allowed to observe the true traders' valuations.","['Regret minimization', 'online learning', 'two-sided markets', 'fairness']",[],"['François Bachoc', 'Nicolò Cesa-Bianchi', 'Tommaso Cesari', 'Roberto Colomboni']","['Institut de Mathématiques de Toulouse', 'Computer Science, University of Milan', 'University of Ottawa', 'DEIB, Polytechnic University of Milan (POLIMI)']",
https://openreview.net/forum?id=IGAN7RldcF,Transparency & Explainability,Unveiling User Satisfaction and Creator Productivity Trade-Offs in Recommendation Platforms,"On User-Generated Content (UGC) platforms, recommendation algorithms significantly impact creators' motivation to produce content as they compete for algorithmically allocated user traffic. This phenomenon subtly shapes the volume and diversity of the content pool, which is crucial for the platform's sustainability. In this work, we demonstrate, both theoretically and empirically, that a purely relevance-driven policy with low exploration strength boosts short-term user satisfaction but undermines the long-term richness of the content pool. In contrast, a more aggressive exploration policy may slightly compromise user satisfaction but promote higher content creation volume. Our findings reveal a fundamental trade-off between immediate user satisfaction and overall content production on UGC platforms. Building on this finding, we propose an efficient optimization method to identify the optimal exploration strength, balancing user and creator engagement. Our model can serve as a pre-deployment audit tool for recommendation algorithms on UGC platforms, helping to align their immediate objectives with sustainable, long-term goals.","['Mechanism Design', 'Content Recommendation Platform', 'Content Creator Competition Game', 'Creator Productivity']",[],"['Fan Yao', 'Yiming Liao', 'Jingzhou Liu', 'Shaoliang Nie', 'Qifan Wang', 'Haifeng Xu', 'Hongning Wang']","['University of Chicago', 'Meta', 'Meta', 'GenAI, Meta Inc', 'Meta AI', 'University of Chicago', 'Department of Computer Science and Technology, Tsinghua University']",
https://openreview.net/forum?id=IoRT7EhFap,Fairness & Bias,Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning,"Deep neural networks (DNNs) have showcased their remarkable precision in approximating smooth functions. However, they suffer from the {\it spectral bias}, wherein DNNs typically exhibit a tendency to prioritize the learning of lower-frequency components of a function, struggling to effectively capture its high-frequency features. This paper is to address this issue. Notice that a function having only low frequency components may be well-represented by a shallow neural network (SNN), a network having only a few layers. By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data. We implement the proposed idea by exploiting the multi-grade deep learning (MGDL) model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN (with trainable parameters) composed with the SNNs (with fixed parameters) trained in the preceding grades as features. We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features. Our study reveals that MGDL excels at representing functions containing high-frequency information. Specifically, the neural networks learned in each grade adeptly capture some low-frequency information, allowing their compositions with SNNs learned in the previous grades effectively representing the high-frequency features. Our experimental results underscore the efficacy of MGDL in addressing the spectral bias inherent in DNNs. By leveraging MGDL, we offer insights into overcoming spectral bias limitation of DNNs, thereby enhancing the performance and applicability of deep learning models in tasks requiring the representation of high-frequency information. This study confirms that the proposed method offers a promising solution to address the spectral bias of DNNs. The code is available on GitHub: \href{https://github.com/Ronglong-Fang/AddressingSpectralBiasviaMGDL}{\texttt{Addressing Spectral Bias via MGDL}}.","['deep neural network', 'spectral bias', 'multi-grade deep learning']",[],"['Ronglong Fang', 'Yuesheng Xu']","['Mathematics and Statistics, Old Dominion University', 'Department of Mathematics and Statistics, Old Dominion University']",
https://openreview.net/forum?id=Iq2IAWozNr,Fairness & Bias,Smoke and Mirrors in Causal Downstream Tasks,"Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where the outcome of interest is recorded in high-dimensional observations in a Randomized Controlled Trial (RCT). Despite being the simplest possible causal setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded ISTAnt, the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences.","['AI for Science', 'Randomized Controlled Trial', 'Representation Learning']",[],"['Riccardo Cadei', 'Lukas Lindorfer', 'Sylvia Cremer', 'Cordelia Schmid', 'Francesco Locatello']","['Institute of Science and Technology', 'Institute of Science and Technology Austria, Institute of Science and Technology Austria', 'ISTA (Institute of Science and Technology Austria)', 'Google', 'Institute of Science and Technology']",
https://openreview.net/forum?id=J6NByZlLNj,Security,WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks,"Due to the increasing popularity of Artificial Intelligence (AI), more and more backdoor attacks are designed to mislead Deep Neural Network (DNN) predictions by manipulating training samples or processes.  Although backdoor attacks have been investigated in various scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily identified by existing backdoor detection algorithms. To overcome this weakness,  this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains high-frequency image features through Discrete Wavelet Transform (DWT) to generate highly stealthy backdoor triggers. By introducing an asymmetric frequency obfuscation method, our approach adds an adaptive residual to the training and inference stages to improve the impact of triggers, thus further enhancing the effectiveness of WaveAttack. Comprehensive experimental results show that, WaveAttack can not only achieve higher effectiveness than state-of-the-art backdoor attack methods, but also outperform them in the fidelity of images (i.e.,  by up to 28.27\% improvement in PSNR, 1.61\% improvement in SSIM, and 70.59\% reduction in IS).  Our code is available at https://github.com/BililiCode/WaveAttack.",['backdoor attack;'],[],"['Jun Xia', 'Zhihao Yue', 'Yingbo Zhou', 'Zhiwei Ling', 'Yiyu Shi', 'Xian Wei', 'Mingsong Chen']","['University of Notre Dame', 'East China Normal University', 'Software Engineering Institute, East China Normal University', 'Zhejiang University', 'University of Notre Dame', 'SEI, East China Normal University', 'Software Engineering Institute, East China Normal University']",
https://openreview.net/forum?id=JhqyeppMiD,Privacy & Data Governance,Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models,"Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs’ susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs’ text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker’s intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.","['Data Poisoning Attacks', 'AI Security', 'Vision-Language Models', 'Multimodality']",[],"['Yuancheng Xu', 'Jiarui Yao', 'Manli Shu', 'Yanchao Sun', 'Zichu Wu', 'Ning Yu', 'Tom Goldstein', 'Furong Huang']","['Computer Science, University of Maryland, College Park', 'University of Illinois at Urbana-Champaign', 'Salesforce Research', 'Apple AI/ML', 'Computer Science, University of Waterloo', '', 'Computer Science, University of Maryland, College Park', 'Computer Science, University of Maryland']",
https://openreview.net/forum?id=Jm2aK3sDJD,Transparency & Explainability,VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance,"Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models and pre-trained Vision-Language Models to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boosted performance. Our method leverages off-the-shelf open-domain grounded object detectors to provide visually grounded concept annotation, which largely enhances the faithfulness of concept prediction while further improving the model performance. In addition, we propose a new metric called Number of Effective Concepts (NEC) to control the information leakage and provide better interpretability. Extensive evaluations across five standard benchmarks show that our method, VLG-CBM, outperforms existing methods by at least 4.27\% and up to 51.09\% on *Accuracy at NEC=5* (denoted as ANEC-5), and by at least 0.45\% and up to 29.78\% on *average accuracy* (denoted as ANEC-avg), while preserving both faithfulness and interpretability of the learned concepts as demonstrated in extensive experiments.","['Interpretable machine learning', 'concept bottleneck models']",[],"['Divyansh Srivastava', 'Ge Yan', 'Tsui-Wei Weng']","['University of California, San Diego', 'Computer Science and Engineering, University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=JxlQ2pbyzS,Transparency & Explainability,Collaborative Cognitive Diagnosis with Disentangled Representation Learning for Learner Modeling,"Learners sharing similar implicit cognitive states often display comparable observable problem-solving performances. Leveraging collaborative connections among such similar learners proves valuable in comprehending human learning. Motivated by the success of collaborative modeling in various domains, such as recommender systems, we aim to investigate how collaborative signals among learners contribute to the diagnosis of human cognitive states (i.e., knowledge proficiency) in the context of intelligent education. The primary challenges lie in identifying implicit collaborative connections and disentangling the entangled cognitive factors of learners for improved explainability and controllability in learner Cognitive Diagnosis (CD). However, there has been no work on CD capable of simultaneously modeling collaborative and disentangled cognitive states. To address this gap, we present Coral, a $\underline{Co}$llabo$\underline{ra}$tive cognitive diagnosis model with disentang$\underline{l}$ed representation learning. Specifically, Coral first introduces a disentangled state encoder to achieve the initial disentanglement of learners' states. Subsequently, a meticulously designed collaborative representation learning procedure captures collaborative signals. It dynamically constructs a collaborative graph of learners by iteratively searching for optimal neighbors in a context-aware manner. Using the constructed graph, collaborative information is extracted through node representation learning. Finally, a decoding process aligns the initial cognitive states and collaborative states, achieving co-disentanglement with practice performance reconstructions. Extensive experiments demonstrate the superior performance of Coral, showcasing significant improvements over state-of-the-art methods across several real-world datasets. Our code is available at https://github.com/bigdata-ustc/Coral.","['Learner Modeling', 'Intelligent Education System', 'Cognitive Diagnosis', 'Disentangled Representation Learning']",[],"['Weibo Gao', 'Qi Liu', 'Linan Yue', 'Fangzhou Yao', 'Hao Wang', 'Yin Gu', 'Zheng Zhang']","['', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China', 'University of Science and Technology of China, University of Science and Technology of China', '', 'School of Computer Science and Technology, University of Science and Technology of China', 'University of Science and Technology of China', '']",
https://openreview.net/forum?id=KEe4IUp20I,Fairness & Bias,SpaceByte: Towards Deleting Tokenization from Large Language Modeling,"Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.","['byte level language model', 'model architecture', 'tokenization', 'efficient pretraining']",[],['Kevin Slagle'],"['Electrical and Computer Engineering, Rice University']",
https://openreview.net/forum?id=KYHVBsEHuC,Security,DiffuPac: Contextual Mimicry in Adversarial Packets Generation via Diffusion Model,"In domains of cybersecurity, recent advancements in Machine Learning (ML) and Deep Learning (DL) have significantly enhanced Network Intrusion Detection Systems (NIDS), improving the effectiveness of cybersecurity operations. However, attackers have also leveraged ML/DL to develop sophisticated models that generate adversarial packets capable of evading NIDS detection. Consequently, defenders must study and analyze these models to prepare for the evasion attacks that exploit NIDS detection mechanisms. Unfortunately, conventional generation models often rely on unrealistic assumptions about attackers' knowledge of NIDS components, making them impractical for real-world scenarios. To address this issue, we present DiffuPac, a first-of-its-kind generation model designed to generate adversarial packets that evade detection without relying on specific NIDS components. DiffuPac integrates a pre-trained Bidirectional Encoder Representations from Transformers (BERT) with diffusion model, which, through its capability for conditional denoising and classifier-free guidance, effectively addresses the real-world constraint of limited attacker knowledge. By concatenating malicious packets with contextually relevant normal packets and applying targeted noising only to the malicious packets, DiffuPac seamlessly blends adversarial packets into genuine network traffic. Through evaluations on real-world datasets, we demonstrate that DiffuPac achieves strong evasion capabilities against sophisticated NIDS, outperforming conventional methods by an average of 6.69 percentage points, while preserving the functionality and practicality of the generated adversarial packets.","['Network Intrusion Detection System', 'Adversarial Machine Learning', 'Cybersecurity', 'Adversarial Sample Generation']",[],"['Abdullah Bin Jasni', 'Akiko Manada', 'Kohei Watabe']","['Electrical, Electronic and Information, Graduate School of Engineering, Nagaoka University of Technology', 'Nagaoka University of Technology', 'Graduate School of Science and Engineering, Saitama University']",
https://openreview.net/forum?id=KSyTvgoSrX,Privacy & Data Governance,Banded Square Root Matrix Factorization for Differentially Private Model Training,"Current state-of-the-art methods for differentially private model training are based on matrix factorization techniques. However, these methods suffer from high computational overhead because they require numerically solving a demanding optimization problem to determine an approximately optimal factorization prior to the actual model training. In this work, we present a new matrix factorization approach, BSR, which overcomes this computational bottleneck. By exploiting properties of the standard matrix square root, BSR allows to efficiently handle also large-scale problems. For the key scenario of stochastic gradient descent with momentum and weight decay, we even derive analytical expressions for BSR that render the computational overhead negligible. We prove bounds on the approximation quality that hold both in the centralized and in the federated learning setting. Our numerical experiments demonstrate that models trained using BSR perform on par with the best existing methods, while completely avoiding their computational overhead.","['Differential Privacy', 'Machine Learning', 'Federated Learning']",[],"['Kalinin Nikita', 'Christoph H. Lampert']","['Computer Science, Institute of Science and Technology', 'Machine Learning and Computer Vision, Institute of Science and Technology Austria']",
https://openreview.net/forum?id=KYHma7hzjr,Transparency & Explainability,Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?,"Recently, interpretable machine learning has re-explored concept bottleneck models (CBM). An advantage of this model class is the user's ability to intervene on predicted concept values, affecting the downstream output. In this work, we introduce a method to perform such concept-based interventions on *pretrained* neural networks, which are not interpretable by design, only given a small validation set with concept labels. Furthermore, we formalise the notion of *intervenability* as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black boxes. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We focus on backbone architectures of varying complexity, from simple, fully connected neural nets to Stable Diffusion. We demonstrate that the proposed fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the practical utility of our techniques, we apply them to deep chest X-ray classifiers and show that fine-tuned black boxes are more intervenable than CBMs. Lastly, we establish that our methods are still effective under vision-language-model-based concept annotations, alleviating the need for a human-annotated validation set.","['interpretability', 'explainability', 'concepts', 'concept bottleneck models', 'model interventions', 'healthcare']",[],"['Sonia Laguna', 'Ričards Marcinkevičs', 'Moritz Vandenhirtz', 'Julia E Vogt']","['Department of Computer Science, ETHZ - ETH Zurich', 'Department of Computer Science, Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Computer Sience, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=KppBAWJbry,Security,Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models,"It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a re-evaluation of safety protocols in the use of open-source pre-trained models.","['Privacy', 'Membership inference attack']",[],"['Yuxin Wen', 'Leo Marchyok', 'Sanghyun Hong', 'Jonas Geiping', 'Tom Goldstein', 'Nicholas Carlini']","['Department of Computer Science, University of Maryland, College Park', 'Electrical Engineering and Computer Science, Oregon State University', 'Oregon State University', 'ELLIS Institute Tübingen', 'Computer Science, University of Maryland, College Park', 'Google']",
https://openreview.net/forum?id=Kl13lipxTW,Security,BackTime: Backdoor Attacks on Multivariate Time Series Forecasting,"Multivariate Time Series (MTS) forecasting is a fundamental task with numerous real-world applications, such as transportation, climate, and epidemiology. While a myriad of powerful deep learning models have been developed for this task, few works have explored the robustness of MTS forecasting models to malicious attacks, which is crucial for their trustworthy employment in high-stake scenarios. To address this gap, we dive deep into the backdoor attacks on MTS forecasting models and propose an effective attack method named BackTime. By subtly injecting a few \textit{stealthy triggers} into the MTS data, BackTime can alter the predictions of the forecasting model according to the attacker's intent. Specifically, BackTime first identifies vulnerable timestamps in the data for poisoning, and then adaptively synthesizes stealthy and effective triggers by solving a bi-level optimization problem with a GNN-based trigger generator. Extensive experiments across multiple datasets and state-of-the-art MTS forecasting models demonstrate the effectiveness, versatility, and stealthiness of BackTime attacks.",['backdoor attack; multivariate time series forecasting'],[],"['Xiao Lin', 'Zhining Liu', 'Dongqi Fu', 'Ruizhong Qiu', 'Hanghang Tong']","['University of Illinois Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'Meta AI, Meta', 'Siebel School of Computing and Data Science, University of Illinois Urbana-Champaign', 'computer science, University of Illinois at Urbana-Champaign']",
https://openreview.net/forum?id=KyVBzkConO,Security,Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models,"As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors, as defined in Goldwasser et al. [2022], in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of indistinguishability obfuscation. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence of the backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.","['backdoors', 'white-box undetectable', 'obfuscation', 'theory']",[],"['Alkis Kalavasis', 'Amin Karbasi', 'Argyris Oikonomou', 'Katerina Sotiraki', 'Grigoris Velegkas', 'Manolis Zampetakis']","['Yale University', 'Yale University', 'Central Applied Science, Meta, Yale University', 'Computer Science, Yale University', 'Computer Science, Yale University', 'Yale University']",
https://openreview.net/forum?id=L4RwA0qyUd,Fairness & Bias,Proximal Causal Inference With Text Data,"Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data. These approaches, however, assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is sometimes infeasible due to data privacy or annotation costs. In this work, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that uses two instances of pre-treatment text data, infers two proxies using two zero-shot models on the separate instances, and applies these proxies in the proximal g-formula. We prove, under certain assumptions about the instances of text and accuracy of the zero-shot predictions, that our method of inferring text-based proxies satisfies identification conditions of the proximal g-formula while other seemingly reasonable proposals do not. To address untestable assumptions associated with our method and the proximal g-formula, we further propose an odds ratio falsification heuristic that flags when to proceed with downstream effect estimation using the inferred proxies. We evaluate our method in synthetic and semi-synthetic settings---the latter with real-world clinical notes from MIMIC-III and open large language models for zero-shot prediction---and find that our method produces estimates with low bias. We believe that this text-based design of proxies allows for the use of proximal causal inference in a wider range of scenarios, particularly those for which obtaining suitable proxies from structured data is difficult.","['proximal causal inference', 'directed acyclic graphs', 'electronic health records', 'causal inference']",[],"['Jacob M. Chen', 'Rohit Bhattacharya', 'Katherine A. Keith']","['Johns Hopkins University', 'Williams College', 'Williams College']",
https://openreview.net/forum?id=LONd7ACEjy,Security,Cross-Modality Perturbation Synergy Attack for Person Re-identification,"In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. For instance, infrared images are typically grayscale, unlike visible images that contain color information. Existing attack methods have primarily focused on the characteristics of the visible image modality, overlooking the features of other modalities and the variations in data distribution among different modalities. This oversight can potentially undermine the effectiveness of these methods in image retrieval across diverse modalities. This study represents the first exploration into the security of cross-modality ReID models and proposes a universal perturbation attack specifically designed for cross-modality ReID. This attack optimizes perturbations by leveraging gradients from diverse modality data, thereby disrupting the discriminator and reinforcing the differences between modalities. We conducted experiments on three widely used cross-modality datasets, namely RegDB, SYSU, and LLCM. The results not only demonstrate the effectiveness of our method but also provide insights for future improvements in the robustness of cross-modality ReID systems.",['Computer Vision; Adversarial Attack；Person Re-identification'],[],"['Yunpeng Gong', 'Zhun Zhong', 'Yansong Qu', 'Zhiming Luo', 'Rongrong Ji', 'Min Jiang']","['AI department, Xiamen University', 'University of Nottingham', 'Xiamen University', 'Department of Artificial Intelligence, Xiamen University', 'Department of Artificial Intelligence, Xiamen University', 'Department of Artificial Intelligence, Xiamen University']",
https://openreview.net/forum?id=LI5KmimXbM,Transparency & Explainability,Association Pattern-aware Fusion for Biological Entity Relationship Prediction,"Deep learning-based methods significantly advance the exploration of associations among triple-wise biological entities (e.g., drug-target protein-adverse reaction), thereby facilitating drug discovery and safeguarding human health. However, existing researches only focus on entity-centric information mapping and aggregation, neglecting the crucial role of potential association patterns among different entities. To address the above limitation, we propose a novel association pattern-aware fusion method for biological entity relationship prediction, which effectively integrates the related association pattern information into entity representation learning. Additionally, to enhance the missing information of the low-order message passing, we devise a bind-relation module that considers the strong bind of low-order entity associations. Extensive experiments conducted on three biological datasets quantitatively demonstrate that the proposed method achieves about 4%-23% hit@1 improvements compared with state-of-the-art baselines. Furthermore, the interpretability of association patterns is elucidated in detail, thus revealing the intrinsic biological mechanisms and promoting it to be deployed in real-world scenarios. Our data and code are available at https://github.com/hry98kki/PatternBERP.","['association pattern', 'deep learning', 'interpretable machine learning', 'biological network']",[],"['Lingxiang Jia', 'Yuchen Ying', 'Zunlei Feng', 'Zipeng Zhong', 'Shaolun Yao', 'Jiacong Hu', 'Mingjiang Duan', 'Xingen Wang', 'Jie Song', 'Mingli Song']","['', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Computer Science, Zhejiang University', 'Computer Science & Technology, Zhejiang University', 'School of Software Technology, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University']",
https://openreview.net/forum?id=LJNqVIKSCr,Fairness & Bias,Double-Ended Synthesis Planning with Goal-Constrained Bidirectional Search,"Computer-aided synthesis planning (CASP) algorithms have demonstrated expert-level abilities in planning retrosynthetic routes to molecules of low to moderate complexity. However, current search methods assume the sufficiency of reaching arbitrary building blocks, failing to address the common real-world constraint where using specific molecules is desired. To this end, we present a formulation of synthesis planning with starting material constraints. Under this formulation, we propose Double-Ended Synthesis Planning ($\texttt{DESP}$), a novel CASP algorithm under a _bidirectional graph search_ scheme that interleaves expansions from the target and from the goal starting materials to ensure constraint satisfiability. The search algorithm is guided by a goal-conditioned cost network learned offline from a partially observed hypergraph of valid chemical reactions. We demonstrate the utility of $\texttt{DESP}$ in improving solve rates and reducing the number of search expansions by biasing synthesis planning towards expert goals on multiple new benchmarks. $\texttt{DESP}$ can make use of existing one-step retrosynthesis models, and we anticipate its performance to scale as these one-step model capabilities improve.","['Retrosynthesis', 'synthesis planning', 'chemistry', 'bidirectional search']",[],"['Kevin Yu', 'Jihye Roh', 'Ziang Li', 'Wenhao Gao', 'Runzhong Wang', 'Connor W. Coley']","['Computational Science and Engineering, Massachusetts Institute of Technology', 'Chemical Engineering, Massachusetts Institute of Technology', 'Georgia Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=LSqDcfX3xU,Security,On provable privacy vulnerabilities of graph representations,"Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of similarity-based edge reconstruction attacks (SERA), furnishing a non-asymptotic analysis of their reconstruction capacities. Moreover, we present empirical corroboration indicating that such attacks can perfectly reconstruct sparse graphs as graph size increases. Conversely, we establish that sparsity is a critical factor for SERA's effectiveness, as demonstrated through analysis and experiments on (dense) stochastic block models. Finally, we explore the resilience of private graph representations produced via noisy aggregation (NAG) mechanism against SERA. Through theoretical analysis and empirical assessments, we affirm the mitigation of SERA using NAG . In parallel, we also empirically delineate instances wherein SERA demonstrates both efficacy and deficiency in its capacity to function as an instrument for elucidating the trade-off between privacy and utility.","['privacy', 'graph representation learning', 'non-asymptotic analysis', 'privacy attacks']",[],"['Ruofan Wu', 'Guanhua Fang', 'Mingyang Zhang', 'Qiying Pan', 'Tengfei LIU', 'Weiqiang Wang']","['Coupang', 'Fudan University', 'Ant Group', 'Shanghai Jiaotong University', '', 'Ant Group']",
https://openreview.net/forum?id=LQR22jM5l3,Transparency & Explainability,Mind the Graph When Balancing Data for Fairness or Robustness,"Failures of fairness or robustness in machine learning predictive settings can be due to undesired dependencies between covariates, outcomes and auxiliary factors of variation. A common strategy to mitigate these failures is data balancing, which attempts to remove those undesired dependencies. In this work, we define conditions on the training distribution for data balancing to lead to fair or robust models. Our results display that in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task, leading to multiple failure modes and even interference with other mitigation techniques such as regularization. Overall, our results highlight the importance of taking the causal graph into account before performing data balancing.","['Fairness', 'Robustness', 'Data balancing', 'Causality', 'Fine-tuning']",[],"['Jessica Schrouff', 'Alexis Bellot', 'Amal Rannen-Triki', 'Alan Malek', 'Isabela Albuquerque', 'Arthur Gretton', ""Alexander Nicholas D'Amour"", 'Silvia Chiappa']","['Google DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'DeepMind', 'Google', 'Google', 'Google DeepMind']",
https://openreview.net/forum?id=LjnDqVcrE9,Transparency & Explainability,ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models,"In this work, we propose a training-free method to inject visual prompts into Multimodal Large Language Models (MLLMs) through learnable latent variable optimization. We observe that attention, as the core module of MLLMs, connects text prompt tokens and visual tokens, ultimately determining the final results. Our approach involves adjusting visual tokens from the MLP output during inference, controlling the attention response to ensure text prompt tokens attend to visual tokens in referring regions. We optimize a learnable latent variable based on an energy function, enhancing the strength of referring regions in the attention map. This enables detailed region description and reasoning without the need for substantial training costs or model retraining. Our method offers a promising direction for integrating referring abilities into MLLMs, and supports referring with box, mask, scribble and point. The results demonstrate that our method exhibits out-of-domain generalization and interpretability.",['Training-Free；Visual Prompt；Multimodal Large Language Models'],[],"['Mingrui Wu', 'Xinyue Cai', 'Jiayi Ji', 'Jiale Li', 'Oucheng Huang', 'Gen Luo', 'Hao Fei', 'GUANNAN JIANG', 'Xiaoshuai Sun', 'Rongrong Ji']","['Xiamen University', 'Xiamen University', 'Xiamen University', 'College of Information, Xiamen University', 'Xiamen University', 'Xiamen University', 'National University of Singapore', 'IMD, Contemporary Amperex Technology Co., Limited', 'Xiamen University', 'Department of Artificial Intelligence, Xiamen University']",
https://openreview.net/forum?id=LuqrIkGuru,Security,Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections,"Despite the remarkable capabilities demonstrated by Graph Neural Networks (GNNs) in graph-related tasks, recent research has revealed the fairness vulnerabilities in GNNs when facing malicious adversarial attacks. However, all existing fairness attacks require manipulating the connectivity between existing nodes, which may be prohibited in reality. To this end, we introduce a Node Injection-based Fairness Attack (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting. In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes’ feature matrix to further ensure the effectiveness of fairness attacks. Comprehensive experiments on three real-world datasets consistently demonstrate that NIFA can significantly undermine the fairness of mainstream GNNs, even including fairness-aware GNNs, by injecting merely 1% of nodes. We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms. Our code and data are released at: https://github.com/CGCL-codes/NIFA.","['Fairness', 'Graph Neural Network', 'Attack']",[],"['Zihan Luo', 'Hong Huang', 'Yongkang Zhou', 'Jiping Zhang', 'Nuo Chen', 'Hai Jin']","['Computer Science and Technology, Huazhong University of Science and Technology', 'School of Computer Science, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'School of Computer Science and Technology, Huazhong University of Science and Technology']",
https://openreview.net/forum?id=hOcsUrOY0D,Privacy & Data Governance,Attack-Aware Noise Calibration for Differential Privacy,"Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale to satisfy a given privacy budget ε. This privacy budget is in turn interpreted in terms of operational attack risks, such as accuracy, sensitivity, and specificity of inference attacks aimed to recover information about the training data records. We show that first calibrating the noise scale to a privacy budget ε, and then translating ε to attack risk leads to overly conservative risk assessments and unnecessarily low utility. Instead, we propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the step of choosing ε. For a given notion of attack risk, our approach significantly decreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than ε, when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy.","['differential privacy', 'DP-SGD']",[],"['Bogdan Kulynych', 'Juan Felipe Gomez', 'Georgios Kaissis', 'Flavio Calmon', 'Carmela Troncoso']","['CHUV - University Hospital Lausanne', 'Harvard University', '', 'SEAS, Harvard University', 'Swiss Federal Institute of Technology Lausanne']",
https://openreview.net/forum?id=M1PRU0x1Iz,Security,FedAvP: Augment Local Data via Shared Policy in Federated Learning,"Federated Learning (FL) allows multiple clients to collaboratively train models without directly sharing their private data. While various data augmentation techniques have been actively studied in the FL environment, most of these methods share input-level or feature-level data information over communication, posing potential privacy leakage. In response to this challenge, we introduce a federated data augmentation algorithm named FedAvP that shares only the augmentation policies, not the data-related information.  For data security and efficient policy search, we interpret the policy loss as a meta update loss in standard FL algorithms and utilize the first-order gradient information to further enhance privacy and reduce communication costs. Moreover, we propose a meta-learning method to search for adaptive personalized policies tailored to heterogeneous clients. Our approach outperforms existing best performing augmentation policy search methods and federated data augmentation methods, in the benchmarks for heterogeneous FL.","['federated learning', 'data augmentation']",[],"['Minui Hong', 'Junhyeog Yun', 'Insu Jeon', 'Gunhee Kim']","['Seoul National University', '', 'Seoul National University', 'Department of Computer Science and Engineering, Seoul National University']",
https://openreview.net/forum?id=M2QREVHK1V,Fairness & Bias,Perceptual Fairness in Image Restoration,"Fairness in image restoration tasks is the desire to treat different sub-groups of images equally well. Existing definitions of fairness in image restoration are highly restrictive. They consider a reconstruction to be a correct outcome for a group (e.g., women) *only* if it falls within the group's set of ground truth images (e.g., natural images of women); otherwise, it is considered *entirely* incorrect. Consequently, such definitions are prone to controversy, as errors in image restoration can manifest in various ways. In this work we offer an alternative approach towards fairness in image restoration, by considering the *Group Perceptual Index* (GPI), which we define as the statistical distance between the distribution of the group's ground truth images and the distribution of their reconstructions. We assess the fairness of an algorithm by comparing the GPI of different groups, and say that it achieves perfect *Perceptual Fairness* (PF) if the GPIs of all groups are identical. We motivate and theoretically study our new notion of fairness, draw its connection to previous ones, and demonstrate its utility on state-of-the-art face image restoration algorithms.","['fairness', 'bias', 'inverse problems', 'image restoration', 'image processing', 'machine learning', 'computer vision', 'responsible AI', 'super-resolution', 'deblurring', 'denoising']",[],"['Guy Ohayon', 'Michael Elad', 'Tomer Michaeli']","['Computer Science, Technion - Israel Institute of Technology', 'Computer Science Department, Technion - Israel Institute of Technology', 'Electrical and Computer Engineering, Technion, Technion']",
https://openreview.net/forum?id=M20p6tq9Hq,Transparency & Explainability,Identifiability Guarantees for Causal Disentanglement from Purely Observational Data,"Causal disentanglement aims to learn about latent causal factors behind data, hold- ing the promise to augment existing representation learning methods in terms of interpretability and extrapolation. Recent advances establish identifiability results assuming that interventions on (single) latent factors are available; however, it re- mains debatable whether such assumptions are reasonable due to the inherent nature of intervening on latent variables. Accordingly, we reconsider the fundamentals and ask what can be learned using just observational data.  We provide a precise characterization of latent factors that can be identified in nonlinear causal models with additive Gaussian noise and linear mixing, without any interventions or graphical restrictions. In particular, we show that the causal variables can be identified up to a _layer_-wise transformation and that further disen- tanglement is not possible. We transform these theoretical results into a practical algorithm consisting of solving a quadratic program over the score estimation of the observed data. We provide simulation results to support our theoretical guarantees and demonstrate that our algorithm can derive meaningful causal representations from purely observational data.","['causality', 'disentanglement', 'identifiability theory']",[],"['Ryan Welch', 'Jiaqi Zhang', 'Caroline Uhler']","['Electrical Engineering and Computer Science, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Electrical Engineering & Computer Science, Massachusetts Institute of Technology']",
https://openreview.net/forum?id=MNg331t8Tj,Fairness & Bias,Advancing Fine-Grained Classification by Structure and Subject Preserving Augmentation,"Fine-grained visual classification (FGVC) involves classifying closely related subcategories. This task is inherently difficult due to the subtle differences between classes and the high intra-class variance. Moreover, FGVC datasets are typically small and challenging to gather, thus highlighting a significant need for effective data augmentation. Recent advancements in text-to-image diffusion models have introduced new possibilities for data augmentation in image classification. While these models have been used to generate training data for classification tasks, their effectiveness in full-dataset training of FGVC models remains under-explored. Recent techniques that rely on text-to-image generation or Img2Img methods, such as SDEdit, often struggle to generate images that accurately represent the class while modifying them to a degree that significantly increases the dataset's diversity. To address these challenges, we present SaSPA: Structure and Subject Preserving Augmentation. Contrary to recent methods, our method does not use real images as guidance, thereby increasing generation flexibility and promoting greater diversity. To ensure accurate class representation, we employ conditioning mechanisms, specifically by conditioning on image edges and subject representation. We conduct extensive experiments and benchmark SaSPA against both traditional and generative data augmentation techniques. SaSPA consistently outperforms all established baselines across multiple settings, including full dataset training and contextual bias. Additionally, our results reveal interesting patterns in using synthetic data for FGVC models; for instance, we find a relationship between the amount of real data used and the optimal proportion of synthetic data.","['Fine-grained Visual Classification', 'Data Augmentation', 'Synthetic Data', 'Diffusion Models', 'Image Classification']",[],"['Eyal Michaeli', 'Ohad Fried']","['Reichman University', 'Computer Science, Reichman University']",
https://openreview.net/forum?id=MP7j58lbWO,Fairness & Bias,Probing Social Bias in Labor Market Text Generation by ChatGPT: A Masked Language Model Approach,"As generative large language models (LLMs) such as ChatGPT gain widespread adoption in various domains, their potential to propagate and amplify social biases, particularly in high-stakes areas such as the labor market, has become a pressing concern. AI algorithms are not only widely used in the selection of job applicants, individual job seekers may also make use of generative LLMs to help develop their job application materials. Against this backdrop, this research builds on a novel experimental design to examine social biases within ChatGPT-generated job applications in response to real job advertisements. By simulating the process of job application creation, we examine the language patterns and biases that emerge when the model is prompted with diverse job postings. Notably, we present a novel bias evaluation framework based on Masked Language Models to quantitatively assess social bias based on validated inventories of social cues/words, enabling a systematic analysis of the language used. Our findings show that the increasing adoption of generative AI, not only by employers but also increasingly by individual job seekers, can reinforce and exacerbate gender and social inequalities in the labor market through the use of biased and gendered language.","['social bias', 'LLM', 'NLP', 'sociology', 'labor market']",[],"['Lei Ding', 'Yang Hu', 'Nicole Denier', 'Enze Shi', 'Junxi Zhang', 'Qirui Hu', 'Karen D. Hughes', 'Linglong Kong', 'Bei Jiang']","['University of Alberta', 'Lancaster University', 'Sociology, University of Alberta', 'University of Alberta', 'Concordia University', 'Department of statistics and Data Science, Tsinghua University', 'SEM / Sociology, University of Alberta', 'University of Alberta', 'Mathematical and Statistical Sciences, University of Alberta']",
https://openreview.net/forum?id=MXRO5kukST,Fairness & Bias,SAND: Smooth imputation of sparse and noisy functional data with Transformer networks,"Although the transformer architecture has come to dominate other models for text and image data, its application to irregularly-spaced longitudinal data has been limited. We introduce a variant of the transformer that enables it to more smoothly impute such functional data. We augment the vanilla transformer with a simple module we call SAND (self-attention on derivatives), which naturally encourages smoothness by modeling the sub-derivative of the imputed curve. On the theoretical front, we prove the number of hidden nodes required by a network with SAND to achieve an $\epsilon$ prediction error bound for functional imputation. Extensive experiments over various types of functional data demonstrate that transformers with SAND produce better imputations than both their standard counterparts as well as transformers augmented with alternative approaches to encode the inductive bias of smoothness. SAND also outperforms standard statistical methods for functional imputation like kernel smoothing and PACE.","['functional data', 'sparse functional data', 'imputation', 'Transformer', 'self-attention']",[],"['Ju-Sheng Hong', 'Junwen Yao', 'Jonas Mueller', 'Jane-Ling Wang']","['Mathematics, University of California, Davis', 'Waymo', 'Cleanlab', 'University of California, Davis']",
https://openreview.net/forum?id=MRO2QhydPF,Fairness & Bias,Reinforcement Learning with Adaptive Regularization for Safe Control of Critical Systems,"Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Regularization (RL-AR), an algorithm that enables safe RL exploration by combining the RL policy with a policy regularizer that hard-codes the safety constraints. RL-AR performs policy combination via a ""focus module,"" which determines the appropriate combination depending on the state—relying more on the safe policy regularizer for less-exploited states while allowing unbiased convergence for well-exploited states. In a series of critical control applications, we demonstrate that RL-AR not only ensures safety during training but also achieves a return competitive with the standards of model-free RL that disregards safety.","['Reinforcement Learning', 'Safe Crticial System Control', 'Policy Regularization']",[],"['Haozhe Tian', 'Homayoun Hamedmoghadam', 'Robert Noel Shorten', 'Pietro Ferraro']","['Dyson School of Deisgn Engineering, Imperial College London', 'Imperial College London', 'Imperial College London', 'Imperial-X , Imperial College London']",
https://openreview.net/forum?id=MfGRUVFtn9,Security,Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness,"The security threat of backdoor attacks is a central concern for deep neural networks (DNNs). Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense. Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy. However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect. In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (*i.e.*, larger gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning. Then, we propose an effective two-stage defense method. In the first stage, an efficient *Neuron Weight Change (NWC)-based Backdoor Reinitialization* is proposed based on observation 1). In the second stage, based on observation 2), we design an *Activeness-Aware Fine-Tuning* to replace the vanilla fine-tuning. Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches. The code is available at https://github.com/linweiii/TSBD.git.",['Deep Neural Network; AI security; Backdoor Defense'],[],"['Weilin Lin', 'Li Liu', 'Shaokui Wei', 'Jianze Li', 'Hui Xiong']","['Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou)', 'The Hong Kong University of Science and Technology (Guangzhou)', 'SDS, The Chinese University of Hong Kong, Shenzhen', 'Shenzhen Research Institute of Big Data', 'Hong Kong University of Science and Technology (Guangzhou)']",
https://openreview.net/forum?id=MelYGfpy4x,Transparency & Explainability,Robust group and simultaneous inferences for high-dimensional single index model,"The high-dimensional single index model (SIM), which assumes that the response is independent of the predictors given a linear combination of predictors, has drawn attention due to its flexibility and interpretability, but its efficiency is adversely affected by outlying observations and heavy-tailed distributions. This paper introduces a robust procedure by recasting the SIM into a pseudo-linear model with transformed responses. It relaxes the distributional conditions on random errors from sub-Gaussian to more general distributions and thus it is robust with substantial efficiency gain for heavy-tailed random errors. Under this paradigm, we provide asymptotically honest group inference procedures based on the idea of orthogonalization, which enjoys the feature that it does not require the zero and nonzero coefficients to be well-separated. Asymptotic null distribution and bootstrap implementation are both established. Moreover, we develop a multiple testing procedure for determining if the individual coefficients are relevant simultaneously, and show that it is able to control the false discovery rate asymptotically. Numerical results indicate that the new procedures can be highly competitive among existing methods, especially for heavy-tailed errors.","['FDR control', 'high-dimensional inference', 'honest test', 'outliers', 'robustness']",[],"['Weichao Yang', 'Hongwei Shi', 'Xu Guo', 'Changliang Zou']","['School of Statistics, Beijing Normal University', 'Department of Mathematical and Statistical Sciences, University of Alberta', 'School of Statistics, Beijing Normal University', 'Nankai University']",
https://openreview.net/forum?id=MdmzAezNHq,Privacy & Data Governance,"Differential Privacy in Scalable General Kernel Learning via $K$-means Nystr{\""o}m Random Features","As the volume of data invested in statistical learning increases and concerns regarding privacy grow, the privacy leakage issue has drawn significant attention. Differential privacy has emerged as a widely accepted concept capable of mitigating privacy concerns, and numerous differentially private (DP) versions of machine learning algorithms have been developed. However, existing works on DP kernel learning algorithms have exhibited practical limitations, including scalability, restricted choice of kernels, or dependence on test data availability. We propose DP scalable kernel empirical risk minimization (ERM) algorithms and a DP kernel mean embedding (KME) release algorithm suitable for general kernels. Our approaches address the shortcomings of previous algorithms by employing Nyström methods, classical techniques in non-private scalable kernel learning. These methods provide data-dependent low-rank approximations of the kernel matrix for general kernels in a DP manner. We present excess empirical risk bounds and computational complexities for the scalable kernel DP ERM, KME algorithms, contrasting them with established methodologies. Furthermore, we develop a private data-generating algorithm capable of learning diverse kernel models. We conduct experiments to demonstrate the performance of our algorithms, comparing them with existing methods to highlight their superiority.","['differential privacy', 'kernel learning', 'kernel mean embedding', 'kernel empirical risk minimization']",[],"['Bonwoo Lee', 'Jeongyoun Ahn', 'Cheolwoo Park']","['Korea Advanced Institute of Science & Technology', '', 'Korea Advanced Institute of Science and Technology']",
https://openreview.net/forum?id=Mrs9a1XQAp,Security,Beyond Slow Signs in High-fidelity Model Extraction,"Deep neural networks, costly to train and rich in intellectual property value, are increasingly threatened by model extraction attacks that compromise their confiden- tiality. Previous attacks have succeeded in reverse-engineering model parameters up to a precision of float64 for models trained on random data with at most three hidden layers using cryptanalytical techniques. However, the process was identified to be very time consuming and not feasible for larger and deeper models trained on standard benchmarks. Our study evaluates the feasibility of parameter extraction methods of Carlini et al. [1] further enhanced by Canales-Martínez et al. [2] for models trained on standard benchmarks. We introduce a unified codebase that integrates previous methods and reveal that computational tools can significantly influence performance. We develop further optimisations to the end-to-end attack and improve the efficiency of extracting weight signs by up to 14.8 times com- pared to former methods through the identification of easier and harder to extract neurons. Contrary to prior assumptions, we identify extraction of weights, not extraction of weight signs, as the critical bottleneck. With our improvements, a 16,721 parameter model with 2 hidden layers trained on MNIST is extracted within only 98 minutes compared to at least 150 minutes previously. Finally, addressing methodological deficiencies observed in previous studies, we propose new ways of robust benchmarking for future model extraction attacks.","['model extraction', 'cryptanalytic extraction']",[],"['Hanna Foerster', 'Robert D. Mullins', 'Ilia Shumailov', 'Jamie Hayes']","['Department of Computer Science, University of Cambridge', 'Department of Computer Science and Technology, University of Cambridge', 'Google DeepMind', 'Google DeepMind']",
https://openreview.net/forum?id=MwFeh4RqvA,Transparency & Explainability,Generating compositional scenes via Text-to-image RGBA Instance Generation,"Text-to-image diffusion generative models can generate high quality images at the cost of tedious prompt engineering. Controllability can be improved by introducing layout conditioning, however existing methods lack layout editing ability and fine-grained control over object attributes. The concept of multi-layer generation holds great potential to address these limitations, however generating image instances concurrently to scene composition limits control over fine-grained object attributes, relative positioning in 3D space and scene manipulation abilities. In this work, we propose a novel multi-stage generation paradigm that is designed for fine-grained control, flexibility and interactivity. To ensure control over instance attributes, we devise a novel training paradigm to adapt a diffusion model to generate isolated scene components as RGBA images with transparency information. To build complex images, we employ these pre-generated instances and introduce a multi-layer composite generation process that smoothly assembles components in realistic scenes. Our experiments show that our RGBA diffusion model is capable of generating diverse and high quality instances with precise control over object attributes. Through multi-layer composition, we demonstrate that our approach allows to build and manipulate images from highly complex prompts with fine-grained control over object appearance and location, granting a higher degree of control than competing methods.","['RGBA generation', 'scene composition', 'diffusion models']",[],"['Alessandro Fontanella', 'Petru-Daniel Tudosiu', 'Yongxin Yang', 'Shifeng Zhang', 'Sarah Parisot']","['University of Edinburgh, University of Edinburgh', 'Leonardo.ai', 'Queen Mary University of London', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.']",
https://openreview.net/forum?id=Nf4MHF1pi5,Fairness & Bias,Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,"Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.","['LLM-based Agents', 'Backdoor Attack']",[],"['Wenkai Yang', 'Xiaohan Bi', 'Yankai Lin', 'Sishuo Chen', 'Jie Zhou', 'Xu Sun']","['Gaoling School of Artificial Intelligence , Renmin University of China', 'Peking University', 'Renmin University of China', 'Alibaba Group', 'WeChat AI, WeChat AI, Tencent Inc.', 'dept of computer science, Peking University']",
https://openreview.net/forum?id=NmlnmLYMZ4,Fairness & Bias,When does perceptual alignment benefit vision representations?,"Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception.  While vision representations have previously benefited from human preference alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability in standard computer vision tasks. We finetune state-of-the-art models on a dataset of human similarity judgments for synthetic image triplets and evaluate them across diverse computer vision tasks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, semantic segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can make them better representation learners.","['representation learning', 'alignment', 'perception', 'transfer learning', 'computer vision', 'foundation model']",[],"['Shobhita Sundaram', 'Stephanie Fu', 'Lukas Muttenthaler', 'Netanel Yakir Tamir', 'Lucy Chai', 'Simon Kornblith', 'Trevor Darrell', 'Phillip Isola']","['EECS, Massachusetts Institute of Technology', 'University of California, Berkeley', 'TU Berlin', 'Apple', 'Massachusetts Institute of Technology', 'Pretraining, Anthropic', 'EECS, Electrical Engineering & Computer Science Department', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=4i9xuPEu9w,Transparency & Explainability,BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning,"Offline model-based reinforcement learning (MBRL) enhances data efficiency by utilizing pre-collected datasets to learn models and policies, especially in scenarios where exploration is costly or infeasible. Nevertheless, its performance often suffers from the objective mismatch between model and policy learning, resulting in inferior performance despite accurate model predictions. This paper first identifies the primary source of this mismatch comes from the underlying confounders present in offline data for MBRL. Subsequently, we introduce **B**ilin**E**ar **CAUS**al r**E**presentation (BECAUSE), an algorithm to capture causal representation for both states and actions to reduce the influence of the distribution shift, thus mitigating the objective mismatch problem. Comprehensive evaluations on 18 tasks that vary in data quality and environment context demonstrate the superior performance of BECAUSE over existing offline RL algorithms. We show the generalizability and robustness of BECAUSE under fewer samples or larger numbers of confounders. Additionally, we offer theoretical analysis of BECAUSE to prove its error bound and sample efficiency when integrating causal representation into offline MBRL. See more details in our project page: [https://sites.google.com/view/be-cause](https://sites.google.com/view/be-cause).","['Model-based RL', 'Causal Reasoning', 'Offline RL']",[],"['Haohong Lin', 'Wenhao Ding', 'Jian Chen', 'Laixi Shi', 'Jiacheng Zhu', 'Bo Li', 'Ding Zhao']","['Control Science and Engineering, Carnegie Mellon University', 'NVIDIA Research', 'CMU, Carnegie Mellon University', 'California Institute of Technology', 'CSAIL, Massachusetts Institute of Technology', 'CS, University of Illinois, Urbana Champaign', 'Carnegie Mellon University']",
https://openreview.net/forum?id=2mqiTiJKrx,Fairness & Bias,Adaptive Experimentation When You Can't Experiment,"This paper introduces the confounded pure exploration transductive linear bandit (CPET-LB) problem.  As a motivating example,  often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects.  Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment.  Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such encouragement designs.  We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded.  Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.","['Experimental Design', 'multi-armed bandit']",[],"['Yao Zhao', 'Kwang-Sung Jun', 'Tanner Fiez', 'Lalit K Jain']","['computer science, University of Arizona', 'University of Arizona', 'Amazon', 'University of Washington']",
https://openreview.net/forum?id=O1fp9nVraj,Security,On scalable oversight with weak LLMs judging strong LLMs,"Scalable oversight protocols aim to enable humans to accurately supervise superhuman AI.  In this paper we study debate, where two AI's compete to convince a judge; consultancy,  where a single AI tries to convince a judge that asks questions; and compare to a baseline of direct question-answering, where the judge just answers outright without the AI. We use large language models (LLMs) as both AI agents and as stand-ins for human judges, taking the judge models to be weaker than agent models.  We benchmark on a diverse range of asymmetries between judges and agents, extending previous work on a single extractive QA task with information asymmetry, to also include mathematics, coding, logic and multimodal reasoning asymmetries.  We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed. Previous work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy. Further, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies.","['alignment', 'safety', 'scalable oversight', 'debate', 'LLM']",[],"['Zachary Kenton', 'Noah Yamamoto Siegel', 'Janos Kramar', 'Jonah Brown-Cohen', 'Samuel Albanie', 'Jannis Bulian', 'Rishabh Agarwal', 'David Lindner', 'Yunhao Tang', 'Noah Goodman', 'Rohin Shah']","['DeepMind', 'University College London, University of London', 'DeepMind', 'Google DeepMind', '', 'Google DeepMind', 'Computer Science, McGill University', '', 'DeepMind', 'Stanford University', 'DeepMind']",
https://openreview.net/forum?id=NnoAj91HZX,Fairness & Bias,No-Regret M${}^{\natural}$-Concave Function Maximization: Stochastic Bandit Algorithms and NP-Hardness of Adversarial Full-Information Setting,"M${}^{\natural}$-concave functions, a.k.a. gross substitute valuation functions, play a fundamental role in many fields, including discrete mathematics and economics. In practice, perfect knowledge of M${}^{\natural}$-concave functions is often unavailable a priori, and we can optimize them only interactively based on some feedback. Motivated by such situations, we study online M${}^{\natural}$-concave function maximization problems, which are interactive versions of the problem studied by Murota and Shioura (1999). For the stochastic bandit setting, we present $O(T^{-1/2})$-simple regret and $O(T^{2/3})$-regret algorithms under $T$ times access to unbiased noisy value oracles of M${}^{\natural}$-concave functions.  A key to proving these results is the robustness of the greedy algorithm to local errors in M${}^{\natural}$-concave function maximization, which is one of our main technical results. While we obtain those positive results for the stochastic setting, another main result of our work is an impossibility in the adversarial setting. We prove that, even with full-information feedback, no algorithms that run in polynomial time per round can achieve $O(T^{1-c})$ regret for any constant $c > 0$ unless $\mathsf{P} = \mathsf{NP}$.  Our proof is based on a reduction from the matroid intersection problem for three matroids, which would be a novel idea in the context of online learning.","['online learning', 'discrete convex analysis', 'combinatorial bandit']",[],"['Taihei Oki', 'Shinsaku Sakaue']","['The University of Tokyo', 'The University of Tokyo']",
https://openreview.net/forum?id=NtNTfRTjE8,Security,Breaking Semantic Artifacts for Generalized AI-generated Image Detection,"With the continuous evolution of AI-generated images, the generalized detection of them has become a crucial aspect of AI security.  Existing detectors have focused on cross-generator generalization, while it remains unexplored whether these detectors can generalize across different image scenes, e.g., images from different datasets with different semantics. In this paper, we reveal that existing detectors suffer from substantial Accuracy drops in such cross-scene generalization. In particular, we attribute their failures to ''semantic artifacts'' in both real and generated images, to which detectors may overfit. To break such ''semantic artifacts'', we propose a simple yet effective approach based on conducting an image patch shuffle and then training an end-to-end patch-based classifier. We conduct a comprehensive open-world evaluation on 31 test sets, covering 7 Generative Adversarial Networks, 18 (variants of) Diffusion Models, and another 6 CNN-based generative models. The results demonstrate that our approach outperforms previous approaches by 2.08\% (absolute) on average regarding cross-scene detection Accuracy. We also notice the superiority of our approach in open-world generalization, with an average Accuracy improvement of 10.59\% (absolute) across all test sets. Our code is available at *https://github.com/Zig-HS/FakeImageDetection*.",['AI Security; Deepfake Detection; AI-generated Image; Diffusion Model;'],[],"['Chende Zheng', 'Chenhao Lin', 'Zhengyu Zhao', 'Hang Wang', 'Xu Guo', 'Shuai Liu', 'Chao Shen']","[""Electronic and Information Engineering, Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", '', ""School of Cyber Science and Engineering, Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", 'Xi’an Jiaotong University']",
https://openreview.net/forum?id=O9RZAEp34l,Transparency & Explainability,Abrupt Learning in Transformers: A Case Study on Matrix Completion,"Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values. To understand this phenomenon in depth, we formulate the low-rank matrix completion problem as a masked language modeling (MLM) task, and show that it is possible to train a BERT model to solve this task to low error. Furthermore, the loss curve shows a plateau early in training followed by a sudden drop to near-optimal values, despite no changes in the training procedure or hyper-parameters. To gain interpretability insights into this sudden drop, we examine the model's predictions, attention heads, and hidden states before and after this transition. Concretely, we observe that (a) the model transitions from simply copying the masked input to accurately predicting the masked entries; (b) the attention heads transition to interpretable patterns relevant to the task; and (c) the embeddings and hidden states encode information relevant to the problem. We also analyze the training dynamics of individual model components to understand the sudden drop in loss.","['Science of language models', 'matrix completion', 'BERT', 'phase transition', 'interpretability']",[],"['Pulkit Gopalani', 'Ekdeep Singh Lubana', 'Wei Hu']","['', 'Center for Brain Science, Harvard University, Harvard University', 'University of Michigan - Ann Arbor']",
https://openreview.net/forum?id=O8yHsRLwPl,Fairness & Bias,Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity,"We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method—Shadowheart SGD—that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.","['nonconvex optimization', 'parallel methods', 'asynchronous methods', 'lower bounds', 'compressed communication']",[],"['Alexander Tyurin', 'Marta Pozzi', 'Ivan Ilin', 'Peter Richtárik']","['Skolkovo Institute of Science and Technology', 'University of Pavia', 'GenAI Center of Excellence, King Abdullah University of Science and Technology', 'King Abdullah University of Science and Technology (KAUST)']",
https://openreview.net/forum?id=8KkBxzn0km,Fairness & Bias,Saliency-driven Experience Replay for Continual Learning,"We present Saliency-driven Experience Replay - SER - a biologically-plausible approach based on replicating human visual saliency to enhance classification models in continual learning settings. Inspired by neurophysiological evidence that the primary visual cortex does not contribute to object manifold untangling for categorization and that primordial saliency biases are still embedded in the modern brain, we propose to employ auxiliary saliency prediction features as a modulation signal to drive and stabilize the learning of a sequence of non-i.i.d. classification tasks. Experimental results confirm that SER effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings. Moreover, we show that saliency-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to adversarial attacks than baseline methods. Code is available at: https://github.com/perceivelab/SER","['Continual Learning', 'Transfer Learning', 'Saliency Prediction']",[],"['Giovanni Bellitto', 'Federica Proietto Salanitri', 'Matteo Pennisi', 'Matteo Boschini', 'Lorenzo Bonicelli', 'Angelo Porrello', 'Simone Calderara', 'Simone Palazzo', 'Concetto Spampinato']","['University of Catania', 'Department of Electrical, Electronic and Computer Engineering, University of Catania', '', 'University of Modena and Reggio Emilia', 'Department of , University of Modena and Reggio Emilia', 'Department of Engineering ""Enzo Ferrari"", University of Modena and Reggio Emilia, AimageLab', 'University of Modena and Reggio Emilia', 'University of Catania', 'University of Catania']",
https://openreview.net/forum?id=ObUjBHBx8O,Fairness & Bias,Mitigating Spurious Correlations via Disagreement Probability,"Models trained with empirical risk minimization (ERM) are prone to be biased towards spurious correlations between target labels and bias attributes, which leads to poor performance on data groups lacking spurious correlations. It is particularly challenging to address this problem when access to bias labels is not permitted. To mitigate the effect of spurious correlations without bias labels, we first introduce a novel training objective designed to robustly enhance model performance across all data samples, irrespective of the presence of spurious correlations. From this objective, we then derive a debiasing method, Disagreement Probability based Resampling for debiasing (DPR), which does not require bias labels. DPR leverages the disagreement between the target label and the prediction of a biased model to identify bias-conflicting samples—those without spurious correlations—and upsamples them according to the disagreement probability. Empirical evaluations on multiple benchmarks demonstrate that DPR achieves state-of-the-art performance over existing baselines that do not use bias labels. Furthermore, we provide a theoretical analysis that details how DPR reduces dependency on spurious correlations.","['Debiasing', 'Spurious correlation', 'Group robustness']",[],"['Hyeonggeun Han', 'Sehwan Kim', 'Hyungjun Joo', 'Sangwoo Hong', 'Jungwoo Lee']","['Seoul National University', 'Electrical and Computer Engineering, Seoul National University', 'Seoul National University', 'Electrical and Computer Engineering, Seoul National University', 'Department of Electrical and Computer Engineering, Seoul National University']",
https://openreview.net/forum?id=OONojmx3wH,Fairness & Bias,When is Multicalibration Post-Processing Necessary?,"Calibration is a well-studied property of predictors which guarantees meaningful uncertainty estimates. Multicalibration is a related notion --- originating in algorithmic fairness --- which requires predictors to be simultaneously calibrated over a potentially complex and overlapping collection of protected subpopulations (such as groups defined by ethnicity, race, or income). We conduct the first comprehensive study evaluating the usefulness of multicalibration post-processing across a broad set of tabular, image, and language datasets for models spanning from simple decision trees to 90 million parameter fine-tuned LLMs. Our findings can be summarized as follows: (1) models which are calibrated out of the box tend to be relatively multicalibrated without any additional post-processing; (2) multicalibration can help inherently uncalibrated models and also large vision and language models; and (3) traditional calibration measures may sometimes provide multicalibration implicitly. More generally, we also distill many independent observations which may be useful for practical and effective applications of multicalibration post-processing in real-world contexts.","['multicalibration', 'calibration', 'fairness']",[],"['Dutch Hansen', 'Siddartha Devic', 'Preetum Nakkiran', 'Vatsal Sharan']","['Mathematics, University of Southern California', 'University of Southern California', 'Apple', 'University of Southern California']",
https://openreview.net/forum?id=P3v3x7HnV0,Fairness & Bias,QueST: Self-Supervised Skill Abstractions for Learning Continuous Control,"Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a promising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture sharable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST’s architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io.","['Behavior Clonning', 'Action Tokenization', 'Self Supervised Skill Abstraction', 'Few-shot Imitation Learning']",[],"['Atharva Mete', 'Haotian Xue', 'Albert Wilcox', 'Yongxin Chen', 'Animesh Garg']","['Interactive Computing, Georgia Institute of Technology', 'Georgia Institute of Technology', 'Computer Science, Georgia Institute of Technology', 'NVIDIA', 'Computer Science, Georgia Institute of Technology']",
https://openreview.net/forum?id=PH7sdEanXP,Fairness & Bias,"Scaling Laws in Linear Regression: Compute, Parameters, and Data","Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.  We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.","['learning theory', 'scaling law', 'deep learning', 'SGD', 'optimization']",[],"['Licong Lin', 'Jingfeng Wu', 'Sham M. Kakade', 'Peter Bartlett', 'Jason D. Lee']","['University of California, Berkeley', 'University of California, Berkeley', 'Harvard University', 'University of California - Berkeley', 'Princeton University']",
https://openreview.net/forum?id=OycU0bAus6,Fairness & Bias,DenoiseRep: Denoising Model for Representation Learning,"The denoising model has been proven a powerful generative model but has little exploration of discriminative tasks. Representation learning is important in discriminative tasks, which is defined as *""learning representations (or features) of the data that make it easier to extract useful information when building classifiers or other predictors""*. In this paper, we propose a novel Denoising Model for Representation Learning (*DenoiseRep*) to improve feature discrimination with joint feature extraction and denoising. *DenoiseRep* views each embedding layer in a backbone as a denoising layer, processing the cascaded embedding layers as if we are recursively denoise features step-by-step. This unifies the frameworks of feature extraction and denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step. After that, *DenoiseRep* fuses the parameters of feature extraction and denoising layers, and *theoretically demonstrates* its equivalence before and after the fusion, thus making feature denoising computation-free. *DenoiseRep* is a label-free algorithm that incrementally improves features but also complementary to the label if available. Experimental results on various discriminative vision tasks, including re-identification (Market-1501, DukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet, UB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation (ADE20K) show stability and impressive improvements. We also validate its effectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda) architectures.","['Diffusion Model', 'Representation Learning', 'Generative Model', 'Discriminative Models']",[],"['zhengrui Xu', ""Guan'an Wang"", 'Xiaowen Huang', 'Jitao Sang']","['Beijing Jiaotong University', 'Peking University', 'Beijing Jiaotong University', '']",
https://openreview.net/forum?id=PAWQvrForJ,Security,Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration,"Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA).  Specifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs. Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding.  Comprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA","['Large Language Models', 'Membership Inference Attacks', 'Privacy and Security']",[],"['Wenjie Fu', 'Huandong Wang', 'Chen Gao', 'Guanghua Liu', 'Yong Li', 'Tao Jiang']","['Huazhong University of Science and Technology', 'Department of Electronic Engineering, Tsinghua University', 'BNRist, Tsinghua University', 'Huazhong University of Science and Technology', 'Tsinghua University, Tsinghua University, Tsinghua University', 'Huazhong University of Science and Technology, Tsinghua University']",
https://openreview.net/forum?id=P6aJ7BqYlc,Privacy & Data Governance,GACL: Exemplar-Free Generalized Analytic Continual Learning,"Class incremental learning (CIL) trains a network on sequential tasks with separated categories in each task but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution. Existing attempts for the GCIL either have poor performance or invade data privacy by saving exemplars. In this paper, we propose a new exemplar-free GCIL technique named generalized analytic continual learning (GACL). The GACL adopts analytic learning (a gradient-free training technique) and delivers an analytical  (i.e., closed-form) solution to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, thereby attaining a weight-invariant property, a rare yet valuable property supporting an equivalence between incremental learning and its joint training. Such an equivalence is crucial in GCIL settings as data distributions among different tasks no longer pose challenges to adopting our GACL. Theoretically, this equivalence property is validated through matrix analysis tools. Empirically, we conduct extensive experiments where, compared with existing GCIL methods, our GACL exhibits a consistently leading performance across various datasets and GCIL settings. Source code is available at https://github.com/CHEN-YIZHU/GACL.","['Class incremental learning', 'closed-form solution', 'exemplar-free', 'continual learning', 'online continual learning', 'generalized continual learning']",[],"['Huiping Zhuang', 'Yizhu Chen', 'Di Fang', 'Run He', 'Kai Tong', 'Hongxin Wei', 'Ziqian Zeng', 'Cen Chen']","['Shien-Ming Wu School of Intelligent Engineering, South China University of Technology', 'Future Technology, South China University of Technology', 'Shien-Ming Wu School of Intelligent Engineering, South China University of Technology', 'Shien-Ming Wu School of Intelligent Engineering, South China University of Technology', 'Shien-Ming Wu School of Intelligent Engineering, South China University of Technology', 'Southern University of Science and Technology', 'School of Intelligent Engineering, South China University of Technology', 'Future Technology, South China University of Technology']",
https://openreview.net/forum?id=PThi9hf9UT,Fairness & Bias,Mutual Information Estimation via $f$-Divergence and Data Derangements,"Estimating mutual information accurately is pivotal across diverse applications, from machine learning to communications and biology, enabling us to gain insights into the inner mechanisms of complex systems. Yet, dealing with high-dimensional data presents a formidable challenge, due to its size and the presence of intricate relationships. Recently proposed neural methods employing variational lower bounds on the mutual information have gained prominence. However, these approaches suffer from either high bias or high variance, as the sample size and the structure of the loss function directly influence the training process. In this paper, we propose a novel class of discriminative mutual information estimators based on the variational representation of the $f$-divergence. We investigate the impact of the permutation function used to obtain the marginal training samples and present a novel architectural solution based on derangements. The proposed estimator is flexible since it exhibits an excellent bias/variance trade-off. The comparison with state-of-the-art neural estimators, through extensive experimentation within established reference scenarios, shows that our approach offers higher accuracy and lower complexity.","['mutual information', 'variational divergence', 'f-divergence', 'neural estimators', 'permutation', 'derangement']",[],"['Nunzio Alexandro Letizia', 'Nicola Novello', 'Andrea M Tonello']","['Alpen-Adria Universität Klagenfurt', '', 'Alpen-Adria Universität Klagenfurt']",
https://openreview.net/forum?id=PZCiWtQjAw,Transparency & Explainability,Continual Audio-Visual Sound Separation,"In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep ($\textbf{Cont}$inual $\textbf{A}$udio-$\textbf{V}$isual Sound $\textbf{Sep}$aration). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024.","['Audio-Visual Learning', 'Sound Separation', 'Continual Learning']",[],"['Weiguo Pian', 'Yiyang Nan', 'Shijian Deng', 'Shentong Mo', 'Yunhui Guo', 'Yapeng Tian']","['', 'Computer Science, Brown University', '', 'CMU, Carnegie Mellon University', 'Computer Science, University of Texas at Dallas', 'Computer Science, University of Texas at Dallas']",
https://openreview.net/forum?id=PgTHgLUFi3,Fairness & Bias,On Sampling Strategies for Spectral Model Sharding,"The problem of heterogeneous clients in federated learning has recently drawn a lot of attention. Spectral model sharding, i.e., partitioning the model parameters into low-rank matrices based on the singular value decomposition, has been one of the proposed solutions for more efficient on-device training in such settings. In this work we present two sampling strategies for such sharding, obtained as solutions to specific optimization problems. The first produces unbiased estimators of the original weights, while the second aims to minimize the squared approximation error. We discuss how both of these estimators can be incorporated in the federated learning loop and practical considerations that arise during local training. Empirically, we demonstrate that both of these methods can lead to improved performance in various commonly used datasets.","['federated learning', 'singular vector decomposition', 'heterogeneous devices']",[],"['Denis Korzhenkov', 'Christos Louizos']","['Qualcomm Inc, QualComm', '']",
https://openreview.net/forum?id=PmLty7tODm,Transparency & Explainability,Interpretable Mesomorphic Networks for Tabular Data,"Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design. In this paper, we propose a new class of interpretable neural networks for tabular data that are both deep and linear at the same time (i.e. mesomorphic). We optimize deep hypernetworks to generate explainable linear models on a per-instance basis. As a result, our models retain the accuracy of black-box deep networks while offering free-lunch explainability for tabular data by design. Through extensive experiments, we demonstrate that our explainable deep networks have comparable performance to state-of-the-art classifiers on tabular data and outperform current existing methods that are explainable by design.","['explainability', 'deep neural networks', 'tabular data', 'hypernetwork', 'interpretability', 'explainable benchmark', 'xai.']",[],"['Arlind Kadra', 'Sebastian Pineda Arango', 'Josif Grabocka']","['Universität Freiburg', 'Universität Freiburg', 'University of Technology Nuremberg']",
https://openreview.net/forum?id=Pojt9RWIjJ,Fairness & Bias,From Transparent to Opaque: Rethinking Neural Implicit Surfaces with $\alpha$-NeuS,"Traditional 3D shape reconstruction techniques from multi-view images, such as structure from motion and multi-view stereo, face challenges in reconstructing transparent objects. Recent advances in neural radiance fields and its variants primarily address opaque or transparent objects, encountering difficulties to reconstruct both transparent and opaque objects simultaneously. This paper introduces $\alpha$-NeuS$\textemdash$an extension of NeuS$\textemdash$that proves NeuS is unbiased for materials from fully transparent to fully opaque. We find that transparent and opaque surfaces align with the non-negative local minima and the zero iso-surface, respectively, in the learned distance field of NeuS. Traditional iso-surfacing extraction algorithms, such as marching cubes, which rely on fixed iso-values, are ill-suited for such data. We develop a method to extract the transparent and opaque surface simultaneously based on DCUDF. To validate our approach, we construct a benchmark that includes both real-world and synthetic scenes, demonstrating its practical utility and effectiveness. Our data and code are publicly available at https://github.com/728388808/alpha-NeuS.","['NeuS', 'Transparent Modeling']",[],"['Haoran Zhang', 'Junkai Deng', 'Xuhui Chen', 'Fei Hou', 'Wencheng Wang', 'Hong Qin', 'Chen Qian', 'Ying He']","['State Key Laboratory of Computer Science, Institute of Software, Institute of Software, Chinese Academy of Sciences', '', 'Chinese Academy of Sciences, Chinese Academy of Sciences', 'Institute of Software, Chinese Academy of Sciences', 'Institute of Software, Chinese Academy of Sciences', 'Department of Computer Science, Stony Brook University (State University of New York, Stony Brook)', 'Department of Computer Sicence and Technology, Tsinghua University, Tsinghua University', '']",
https://openreview.net/forum?id=QAbhLBF72K,Privacy & Data Governance,What makes unlearning hard and what to do about it,"Machine unlearning is the problem of removing the effect of a subset of training data (the ``forget set'') from a trained model without damaging the model's utility e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data. With unlearning research still being at its infancy, many fundamental open questions exist:  Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem?  How do these characteristics affect different state-of-the-art algorithms? With this paper, we present the first investigation aiming to answer these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets. Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set.  We find that RUM substantially improves top-performing unlearning algorithms.  Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art.",['machine unlearning'],[],"['Kairan Zhao', 'Meghdad Kurmanji', 'George-Octavian Bărbulescu', 'Eleni Triantafillou', 'Peter Triantafillou']","['Department of Computer Science, University of Warwick', 'Computer Science, University of Cambridge', 'Department of Computer Science, University of Warwick', 'Google', 'University of Warwick']",
https://openreview.net/forum?id=QB6CvDqa6b,Security,An Offline Adaptation Framework for Constrained Multi-Objective Reinforcement Learning,"In recent years, significant progress has been made in multi-objective reinforcement learning (RL) research, which aims to balance multiple objectives by incorporating preferences for each objective. In most existing studies, specific preferences must be provided during deployment to indicate the desired policies explicitly. However, designing these preferences depends heavily on human prior knowledge, which is typically obtained through extensive observation of high-performing demonstrations with expected behaviors. In this work, we propose a simple yet effective offline adaptation framework for multi-objective RL problems without assuming handcrafted target preferences, but only given several demonstrations to implicitly indicate the preferences of expected policies. Additionally, we demonstrate that our framework can naturally be extended to meet constraints on safety-critical objectives by utilizing safe demonstrations, even when the safety thresholds are unknown. Empirical results on offline multi-objective and safe tasks demonstrate the capability of our framework to infer policies that align with real preferences while meeting the constraints implied by the provided demonstrations.","['Multi-Objective RL', 'safe RL', 'offline RL']",[],"['Qian Lin', 'Zongkai Liu', 'Danying Mo', 'Chao Yu']","['Computer science, SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY', 'Sun Yat-Sen University', 'Sun Yat-sen University']",
https://openreview.net/forum?id=QVtwpT5Dmg,Security,Rule Based Rewards for Language Model Safety,"Reinforcement learning based fine-tuning of large language models (LLMs) on human preferences has been shown to enhance both their capabilities and safety behavior.   However, in cases related to safety, without precise instructions to human annotators, the data collected may cause the model to become overly cautious, or to respond in an undesirable style, such as being judgmental.   Additionally, as model capabilities and usage patterns evolve, there may be a costly need to add or relabel data to modify safety behavior.    We propose a novel preference modeling approach that utilizes AI feedback and only requires a small amount of human data.    Our method, Rule Based Rewards (RBR), uses a collection of rules for desired or undesired behaviors (e.g. refusals should not be judgmental) along with a LLM grader.   In contrast to prior methods using AI feedback, our method uses fine-grained, composable, LLM-graded few-shot prompts as reward directly in RL training, resulting in greater control, accuracy and ease of updating.   We show that RBRs are an effective training method, achieving an F1 score of 97.1, compared to a human-feedback baseline of 91.7, resulting in much higher safety-behavior accuracy through better balancing usefulness and safety.","['Large Language Model', 'LLM', 'RLHF', 'RLAIF', 'Safety', 'RBR', 'refusal', 'alignment']",[],"['Tong Mu', 'Alec Helyar', 'Johannes Heidecke', 'Joshua Achiam', 'Andrea Vallone', 'Ian D Kivlichan', 'Molly Lin', 'Alex Beutel', 'John Schulman', 'Lilian Weng']","['OpenAI', 'Research, OpenAI', 'OpenAI', 'University of California Berkeley', 'University of California, Santa Barbara', 'OpenAI', 'OpenAI', '', 'OpenAI', 'OpenAI']",
https://openreview.net/forum?id=QgaGs7peYe,Security,Predicting Future Actions of Reinforcement Learning Agents,"As reinforcement learning agents become increasingly deployed in real-world scenarios, predicting future agent actions and events during deployment is important for facilitating better human-agent interaction and preventing catastrophic outcomes. This paper experimentally evaluates and compares the effectiveness of future action and event prediction for three types of RL agents: explicitly planning, implicitly planning, and non-planning. We employ two approaches: the inner state approach, which involves predicting based on the inner computations of the agents (e.g., plans or neuron activations), and a simulation-based approach, which involves unrolling the agent in a learned world model. Our results show that the plans of explicitly planning agents are significantly more informative for prediction than the neuron activations of the other types. Furthermore, using internal plans proves more robust to model quality compared to simulation-based approaches when predicting actions, while the results for event prediction are more mixed. These findings highlight the benefits of leveraging inner states and simulations to predict future agent actions and events, thereby improving interaction and safety in real-world deployments.","['Safe reinforcement learning', 'deep reinforcement learning', 'agent predictability']",[],"['Stephen Chung', 'Scott Niekum', 'David Krueger']","['University of Cambridge', 'College of Information and Computer Sciences, University of Massachusetts at Amherst', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal']",
https://openreview.net/forum?id=QiCJomIW3l,Transparency & Explainability,Toward Dynamic Non-Line-of-Sight Imaging with Mamba Enforced Temporal Consistency,"Dynamic reconstruction in confocal non-line-of-sight imaging encounters great challenges since the dense raster-scanning manner limits the practical frame rate. A fewer pioneer works reconstruct high-resolution volumes from the under-scanning transient measurements but overlook temporal consistency among transient frames. To fully exploit multi-frame information, we propose the first spatial-temporal Mamba (ST-Mamba) based method tailored for dynamic reconstruction of transient videos. Our method capitalizes on neighbouring transient frames to aggregate the target 3D hidden volume. Specifically, the interleaved features extracted from the input transient frames are fed to the proposed ST-Mamba blocks, which leverage the time-resolving causality in transient measurement. The cross ST-Mamba blocks are then devised to integrate the adjacent transient features. The target high-resolution transient frame is subsequently recovered by the transient spreading module. After transient fusion and recovery, a physical-based network is employed to reconstruct the hidden volume. To tackle the substantial noise inherent in transient videos, we propose a wave-based loss function to impose constraints within the phasor field. Besides, we introduce a new dataset, comprising synthetic videos for training and real-world videos for evaluation. Extensive experiments showcase the superior performance of our method on both synthetic data and real world data captured by different imaging setups. The code and data are available at https://github.com/Depth2World/Dynamic_NLOS.","['dynamic', 'non-line-of-sight imaging', 'spatial-temporal Mamba']",[],"['Yue Li', 'Yi Sun', 'Shida Sun', 'Juntian Ye', 'Yueyi Zhang', 'Feihu Xu', 'Zhiwei Xiong']","['', 'University of Science and Technology of China', '', 'University of Science and Technology of China', 'Hesai Technology', 'University of Science and Technology of China', 'USTC']",
https://openreview.net/forum?id=QvqLdeSLWA,Fairness & Bias,Suppress Content Shift: Better Diffusion Features via Off-the-Shelf Generation Techniques,"Diffusion models are powerful generative models, and this capability can also be applied to discrimination. The inner activations of a pre-trained diffusion model can serve as features for discriminative tasks, namely, diffusion feature. We discover that diffusion feature has been hindered by a hidden yet universal phenomenon that we call content shift. To be specific, there are content differences between features and the input image, such as the exact shape of a certain object. We locate the cause of content shift as one inherent characteristic of diffusion models, which suggests the broad existence of this phenomenon in diffusion feature. Further empirical study also indicates that its negative impact is not negligible even when content shift is not visually perceivable. Hence, we propose to suppress content shift to enhance the overall quality of diffusion features. Specifically, content shift is related to the information drift during the process of recovering an image from the noisy input, pointing out the possibility of turning off-the-shelf generation techniques into tools for content shift suppression. We further propose a practical guideline named GATE to efficiently evaluate the potential benefit of a technique and provide an implementation of our methodology. Despite the simplicity, the proposed approach has achieved superior results on various tasks and datasets, validating its potential as a generic booster for diffusion features. Our code is available at https://github.com/Darkbblue/diffusion-content-shift.","['Diffusion Models', 'Representation Learning', 'Model Property']",[],"['Benyuan Meng', 'Qianqian Xu', 'Zitai Wang', 'Zhiyong Yang', 'Xiaochun Cao', 'Qingming Huang']","['University of the Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'the Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academic of Sciences', 'School of Cyber Science and Technology, SUN YAT-SEN UNIVERSITY', 'University of Chinese Academy of Sciences']",
https://openreview.net/forum?id=RE5LSV8QYH,Transparency & Explainability,Qualitative Mechanism Independence,"We define what it means for a joint probability distribution to be compatible with aset of independent causal mechanisms, at a qualitative level—or, more precisely with a directed hypergraph $\mathcal A$, which is the qualitative structure of a probabilistic dependency graph (PDG). When A represents a qualitative Bayesian network, QIM-compatibility with $\mathcal A$ reduces to satisfying the appropriate conditional independencies. But giving semantics to hypergraphs using QIM-compatibility lets us do much more. For one thing, we can capture functional dependencies. For another, we can capture important aspects of causality using compatibility: we can use compatibility to understand cyclic causal graphs, and to demonstrate structural compatibility, we must essentially produce a causal model. Finally, compatibility has deep connections to information theory. Applying compatibility to cyclic structures helps to clarify a longstanding conceptual issue in information theory.","['causality', 'information theory', 'directed hypergraphs', 'qualitative structures']",[],"['Oliver Ethan Richardson', 'Spencer J Peters', 'Joseph Halpern']","['Informatique, Mila - Quebec Artificial Intelligence Institute', 'Cornell University', 'Cornell University']",
https://openreview.net/forum?id=RDsDvSHGkA,Security,Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel Orthogonal Learner,"Estimating causal quantities from observational data is crucial for understanding the safety and effectiveness of medical treatments. However, to make reliable inferences, medical practitioners require not only estimating averaged causal quantities, such as the conditional average treatment effect, but also understanding the randomness of the treatment effect as a random variable. This randomness is referred to as aleatoric uncertainty and is necessary for understanding the probability of benefit from treatment or quantiles of the treatment effect. Yet, the aleatoric uncertainty of the treatment effect has received surprisingly little attention in the causal machine learning community. To fill this gap, we aim to quantify the aleatoric uncertainty of the treatment effect at the covariate-conditional level, namely, the conditional distribution of the treatment effect (CDTE). Unlike average causal quantities, the CDTE is not point identifiable without strong additional assumptions. As a remedy, we employ partial identification to obtain sharp bounds on the CDTE and thereby quantify the aleatoric uncertainty of the treatment effect. We then develop a novel, orthogonal learner for the bounds on the CDTE, which we call AU-learner. We further show that our AU-learner has several strengths in that it satisfies Neyman-orthogonality and, thus, quasi-oracle efficiency. Finally, we propose a fully-parametric deep learning instantiation of our AU-learner.","['causal inference', 'treatment effect estimation']",[],"['Valentyn Melnychuk', 'Stefan Feuerriegel', 'Mihaela van der Schaar']","['Ludwig-Maximilians-Universität München', 'LMU Munich', 'University of Cambridge']",
https://openreview.net/forum?id=RPChapuXlC,Security,Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack,"Recent studies show that Large Language Models (LLMs) with safety alignment can be jail-broken by fine-tuning on a dataset mixed with harmful data. For the first time in the literature, we show that the jail-break effect can be mitigated by separating two states in the fine-tuning stage to respectively optimize over the alignment and user datasets. Unfortunately, our subsequent study shows that this simple Bi-State Optimization (BSO) solution experiences convergence instability when steps invested in its alignment state is too small, leading to downgraded alignment performance. By statistical analysis, we show that the \textit{excess drift} towards the switching iterates of the two states could be a probable reason for the instability. To remedy this issue, we propose \textbf{L}azy(\textbf{i}) \textbf{s}afety \textbf{a}lignment (\textbf{Lisa}), which introduces a proximal term to constraint the drift of each state. Theoretically, the benefit of the proximal term is supported by the convergence analysis, wherein we show that a sufficient large proximal factor is necessary to guarantee Lisa's convergence.   Empirically, our results on four downstream fine-tuning tasks show that Lisa with a proximal term can significantly increase alignment performance while maintaining the LLM's accuracy on the user tasks.  Code is available at https://github.com/git-disl/Lisa.","['Large language model', 'safety alignment', 'harmful finetuning attack']",[],"['Tiansheng Huang', 'Sihao Hu', 'Fatih Ilhan', 'Selim Furkan Tekin', 'Ling Liu']","['Georgia Institute of Technology', 'College of Computing, Georgia Institute of Technology', 'Computer Science, Georgia Institute of Technology', 'Computer Science, College of Computing, Georgia Institute of Technology', '']",
https://openreview.net/forum?id=RMdnTnffou,Transparency & Explainability,Coarse-to-Fine Concept Bottleneck Models,"Deep learning algorithms have recently gained significant attention due to their impressive performance. However, their high complexity and un-interpretable mode of operation hinders their confident deployment in real-world safety-critical tasks. This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly interpretable decision making process with respect to human understandable concepts, on two levels of granularity. To this end, we propose a novel two-level concept discovery formulation leveraging: (i) recent advances in vision-language models, and (ii) an innovative formulation for coarse-to-fine concept selection via data-driven and sparsity inducing Bayesian arguments. Within this framework, concept information does not solely rely on the similarity between the whole image and general unstructured concepts; instead, we introduce the notion of concept hierarchy to uncover and exploit more granular concept information residing in patch-specific regions of the image scene. As we experimentally show, the proposed construction not only outperforms recent CBM approaches, but also yields a principled framework towards interpetability.","['Interpretability', 'Explainability', 'Concept Bottleneck Models', 'Sparsity', 'Multimodal Models', 'Concepts', 'Textual Descriptions', 'Bayesian', 'Masking']",[],"['Konstantinos P. Panousis', 'Dino Ienco', 'Diego Marcos']","['INRIA', 'MathNum, National Institute for Agriculture, Environment and Food', 'INRIA']",
https://openreview.net/forum?id=RHQbxlhzhm,Transparency & Explainability,FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models,"Survival analysis is an important research topic with applications in healthcare, business, and manufacturing. One essential tool in this area is the Cox proportional hazards (CPH) model, which is widely used for its interpretability, flexibility, and predictive performance. However, for modern data science challenges such as high dimensionality (both $n$ and $p$) and high feature correlations, current algorithms to train the CPH model have drawbacks, preventing us from using the CPH model at its full potential. The root cause is that the current algorithms, based on the Newton method, have trouble converging due to vanishing second order derivatives when outside the local region of the minimizer. To circumvent this problem, we propose new optimization methods by constructing and minimizing surrogate functions that exploit hidden mathematical structures of the CPH model. Our new methods are easy to implement and ensure monotonic loss decrease and global convergence. Empirically, we verify the computational efficiency of our methods. As a direct application, we show how our optimization methods can be used to solve the cardinality-constrained CPH problem, producing very sparse high-quality models that were not previously practical to construct. We list several extensions that our breakthrough enables, including optimization opportunities, theoretical questions on CPH's mathematical structure, as well as other CPH-related applications.","['survival analysis', 'cox proportional hazards model', 'optimization', 'first-order method', 'sparse learning']",[],"['Jiachang Liu', 'Rui Zhang', 'Cynthia Rudin']","['Cornell University', 'Duke University', '']",
https://openreview.net/forum?id=RaNct2xkyI,Security,Feature-Level Adversarial Attacks and Ranking Disruption for Visible-Infrared Person Re-identification,"Visible-infrared person re-identification (VIReID) is widely used in fields such as video surveillance and intelligent transportation, imposing higher demands on model security. In practice, the adversarial attacks based on VIReID aim to disrupt output ranking and quantify the security risks of models. Although numerous studies have been emerged on adversarial attacks and defenses in fields such as face recognition, person re-identification, and pedestrian detection, there is currently a lack of research on the security of VIReID systems. To this end, we propose to explore the vulnerabilities of VIReID systems and prevent potential serious losses due to insecurity. Compared to research on single-modality ReID, adversarial feature alignment and modality differences need to be particularly emphasized. Thus, we advocate for feature-level adversarial attacks to disrupt the output rankings of VIReID systems. To obtain adversarial features, we introduce \textit{Universal Adversarial Perturbations} (UAP) to simulate common disturbances in real-world environments. Additionally, we employ a \textit{Frequency-Spatial Attention Module} (FSAM), integrating frequency information extraction and spatial focusing mechanisms, and further emphasize important regional features from different domains on the shared features. This ensures that adversarial features maintain consistency within the feature space. Finally, we employ an \textit{Auxiliary Quadruple Adversarial Loss} to amplify the differences between modalities, thereby improving the distinction and recognition of features between visible and infrared images, which causes the system to output incorrect rankings. Extensive experiments on two VIReID benchmarks (i.e., SYSU-MM01, RegDB) and different systems validate the effectiveness of our method.","['Visible-Infrared Person Re-identification', 'Universal Adversarial Perturbation', 'Frequency-Spatial Attention Module', 'Auxiliary Quadruple Adversarial loss']",[],"['Xi Yang', 'Huanling Liu', 'De Cheng', 'Nannan Wang', 'Xinbo Gao']","['State Key Laboratory of Integrated Services Networks, Xidian University', ""Xi'an University of Electronic Science and Technology"", 'School of Telecommunication and Engineering, Xidian University', 'State Key Lab of ISN, Xidian University', 'Computer Science and Technology, Chongqing University of Post and Telecommunications']",
https://openreview.net/forum?id=RlZgnEZsOH,Security,HuRef: HUman-REadable Fingerprint for Large Language Models,"Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without interfering with training or exposing model parameters to the public. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining,  with negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF,  which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model.  Due to the potential risk of information leakage, we cannot publish invariant terms directly. Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image. In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners. To ensure the published fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP). Experimental results across various LLMs demonstrate the effectiveness of our method. The code is available at https://github.com/LUMIA-Group/HuRef.","['Model Identification', 'Fingerprinting', 'Large Language Models (LLMs)']",[],"['Boyi Zeng', 'Lizheng Wang', 'Yuncong Hu', 'Yi Xu', 'Chenghu Zhou', 'Xinbing Wang', 'Yu Yu', 'Zhouhan Lin']","['Shanghai Jiaotong University', '', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'IGSNRR, Chinese Academy of Sciences, Beijing, China', 'Shanghai Jiao Tong University', 'Computer Science, Shanghai Jiaotong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=Rsb32EBmbj,Security,Exploring Adversarial Robustness of Deep State Space Models,"Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments. Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures. However, its effectiveness in improving the AR of SSMs remains unclear. While many enhancements in SSM components, such as integrating Attention mechanisms and expanding to data-dependent SSM parameterizations, have brought significant gains in Standard Training (ST) settings, their potential benefits in AT remain unexplored. To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance. We observe that pure SSM structures struggle to benefit from AT, whereas incorporating Attention yields a markedly better trade-off between robustness and generalization for SSMs in AT compared to other components. Nonetheless, the integration of Attention also leads to Robust Overfitting (RO) issues. To understand these phenomena, we empirically and theoretically  analyze the output error of SSMs under AP. We find that fixed-parameterized SSMs have output error bounds strictly related to their parameters, limiting their AT benefits, while input-dependent SSMs may face the problem of error explosion. Furthermore, we show that the Attention component effectively scales the output error of SSMs during training, enabling them to benefit more from AT, but at the cost of introducing RO due to its high model complexity. Inspired by this, we propose a simple and effective Adaptive Scaling (AdS) mechanism that brings AT performance close to Attention-integrated SSMs without introducing the issue of RO.",['Adversarial Robustness; Robustness Exploration; State Space Models'],[],"['Biqing Qi', 'Yiang Luo', 'Junqi Gao', 'Pengfei Li', 'Kai Tian', 'Zhiyuan Ma', 'Bowen Zhou']","['Shanghai Artificial Intelligence Laboratory', 'Harbin Institute of Technology', 'Department of Mathematics, Harbin Institute of Technology', 'School of mathematics, Harbin Institute of Technology', 'Tsinghua University', 'Department of Electronic Engineering, Tsinghua University, Tsinghua University', 'Department of Electron kneincering, Tsinghua University']",
https://openreview.net/forum?id=S3HvA808gk,Fairness & Bias,A Closer Look at AUROC and AUPRC under Class Imbalance,"In machine learning (ML), a widespread claim is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for tasks with class imbalance. This paper refutes this notion on two fronts. First, we theoretically characterize the behavior of AUROC and AUPRC in the presence of model mistakes, establishing clearly that AUPRC is not generally superior in cases of class imbalance. We further show that AUPRC can be a harmful metric as it can unduly favor model improvements in subpopulations with more frequent positive labels, heightening algorithmic disparities. Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets. Prompted by these insights, we conduct a review of over 1.5 million scientific papers to understand the origin of this invalid claim, finding that it is often made without citation, misattributed to papers that do not argue this point, and aggressively over-generalized from source arguments. Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.","['AUROC', 'AUPRC', 'Area under the receiver operating characteristic', 'Area under the precision recall curve', 'evaluation', 'fairness', 'disparities', 'bias']",[],"['Matthew B.A. McDermott', 'Haoran Zhang', 'Lasse Hyldig Hansen', 'Giovanni Angelotti', 'Jack Gallifant']","['Harvard University', 'Massachusetts Institute of Technology', 'Cognitive Science, Aarhus University', 'Dalle Molle Institute for Artificial Intelligence', ""AI in Medicine Group, Brigham and Women's Hospital, Harvard University""]",
https://openreview.net/forum?id=SRWs2wxNs7,Fairness & Bias,U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers,"Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention and bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.",['Diffusion Transformers; U-Net; Architecture'],[],"['Yuchuan Tian', 'Zhijun Tu', 'Hanting Chen', 'Jie Hu', 'Chao Xu', 'Yunhe Wang']","['Peking University', ""Huawei Noah's Ark Lab"", 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Peking University', ""Huawei Noah's Ark Lab""]",
https://openreview.net/forum?id=STrpbhrvt3,Transparency & Explainability,A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis,"While deep networks have achieved broad success in analyzing natural images, when applied to medical scans, they often fail in unexcepted situations. We investigate this challenge and focus on model sensitivity to domain shifts, such as data sampled from different hospitals or data confounded by demographic variables such as sex, race, etc, in the context of chest X-rays and skin lesion images. A key finding we show empirically is that existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings. Taking inspiration from medical training, we propose giving deep networks a prior grounded in explicit medical knowledge communicated in natural language. To this end, we introduce Knowledge-enhanced Bottlenecks (KnoBo), a class of concept bottleneck models that incorporates knowledge priors that constrain it to reason with clinically relevant factors found in medical textbooks or PubMed. KnoBo uses retrieval-augmented language models to design an appropriate concept space paired with an automatic training procedure for recognizing the concept. We evaluate different resources of knowledge and recognition architectures on a broad range of domain shifts across 20 datasets. In our comprehensive evaluation with two imaging modalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average. Finally, evaluations reveal that PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance.","['Robustness', 'Interpretability', 'Domain Generalization', 'Knowledge Prior', 'Medical Images']",[],"['Yue Yang', 'Mona Gandhi', 'Yufei Wang', 'Yifan Wu', 'Michael S Yao', 'Chris Callison-Burch', 'James Gee', 'Mark Yatskar']","['Computer and Information Science, University of Pennsylvania, University of Pennsylvania', 'Computer Science, Ohio State University, Columbus', 'Computer and Information Science, University of Pittsburgh', 'University of Pennsylvania, University of Pennsylvania', 'Department of Bioengineering, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania, University of Pennsylvania', '']",
https://openreview.net/forum?id=Sk2duBGvrK,Fairness & Bias,Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure,"In this work, we study the generalizability of diffusion models by looking into the hidden properties of the learned score functions, which are essentially a series of deep denoisers trained on various noise levels. We observe that as diffusion models transition from memorization to generalization, their corresponding nonlinear diffusion denoisers exhibit increasing linearity. This discovery leads us to investigate the linear counterparts of the nonlinear diffusion models, which are a series of linear models trained to match the function mappings of the nonlinear diffusion denoisers. Surprisingly, these linear denoisers are approximately the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset. This finding implies that diffusion models have the inductive bias towards capturing and utilizing the Gaussian structure (covariance information) of the training dataset for data generation. We empirically demonstrate that this inductive bias is a unique property of diffusion models in the generalization regime, which becomes increasingly evident when the model's capacity is relatively small compared to the training dataset size. In the case that the model is highly overparameterized, this inductive bias emerges during the initial training phases before the model fully memorizes its training data. Our study provides crucial insights into understanding the notable strong generalization phenomenon recently observed in real-world diffusion models.","['diffusion models', 'inductive bias', 'generalization', 'memorization']",[],"['Xiang Li', 'Yixiang Dai', 'Qing Qu']","['University of Michigan - Ann Arbor', 'EECS, University of Michigan - Ann Arbor', 'EECS, University of Michigan']",
https://openreview.net/forum?id=SvmJJJS0q1,Transparency & Explainability,Detecting and Measuring Confounding Using Causal Mechanism Shifts,"Detecting and measuring confounding effects from data is a key challenge in causal inference. Existing methods frequently assume causal sufficiency, disregarding the presence of unobserved confounding variables. Causal sufficiency is both unrealistic and empirically untestable. Additionally, existing methods make strong parametric assumptions about the underlying causal generative process to guarantee the identifiability of confounding variables. Relaxing the causal sufficiency and parametric assumptions and leveraging recent advancements in causal discovery and confounding analysis with non-i.i.d. data, we propose a comprehensive approach for detecting and measuring confounding. We consider various definitions of confounding and introduce tailored methodologies to achieve three objectives: (i) detecting and measuring confounding among a set of variables, (ii) separating observed and unobserved confounding effects, and (iii) understanding the relative strengths of confounding bias between different sets of variables. We present useful properties of a confounding measure and present measures that satisfy those properties. Our empirical results support the usefulness of the proposed measures.","['Causality', 'Confounding', 'Mechanisms', 'Measure']",[],"['Abbavaram Gowtham Reddy', 'Vineeth N. Balasubramanian']","['CISPA Helmholtz Center for Information Security', 'Computer Science and Engineering, Artificial Intelligence, Indian Institute of Technology Hyderabad']",
https://openreview.net/forum?id=SpPAB1tmlC,Fairness & Bias,Unveiling Encoder-Free Vision-Language Models,"Existing vision-language models (VLMs) mostly rely on vision encoders to extract visual features followed by large language models (LLMs) for visual-language tasks. However, the vision encoders set a strong inductive bias in abstracting visual representation, e.g., resolution, aspect ratio, and semantic priors, which could impede the flexibility and efficiency of the VLMs. Training pure VLMs that accept the seamless vision and language inputs, i.e., without vision encoders, remains challenging and rarely explored. Empirical observations reveal that direct training without encoders results in slow convergence and large performance gaps. In this work, we bridge the gap between encoder-based and encoder-free models, and present a simple yet effective training recipe towards pure VLMs. Specifically, we unveil the key aspects of training encoder-free VLMs efficiently via thorough experiments: (1) Bridging vision-language representation inside one unified decoder; (2) Enhancing visual recognition capability via extra supervision. With these strategies, we launch EVE, an encoder-free vision-language model that can be trained and forwarded efficiently. Notably, solely utilizing 35M publicly accessible data, EVE can impressively rival the encoder-based VLMs of similar capacities across multiple vision-language benchmarks. It significantly outperforms the counterpart Fuyu-8B with mysterious training procedures and undisclosed training data. We believe that EVE provides a transparent and efficient route for developing pure decoder-only architecture across modalities.","['Large Vision-Language Model', 'Encoder-Free Multi-Modality Model', 'Pure Decoder-only Architecture']",[],"['Haiwen Diao', 'Yufeng Cui', 'Xiaotong Li', 'Yueze Wang', 'Huchuan Lu', 'Xinlong Wang']","['Ganjingzi, Dalian University of Technology', 'Beijing Academy of Artificial Intelligence', 'Peking University', 'Beijing Academy of Artificial Intelligence', 'School of Information and Communication Engineering, Dalian University of Technology', 'Beijing Academy of Artificial Intelligence']",
https://openreview.net/forum?id=T07OHxcEYP,Privacy & Data Governance,Differentially Private Reinforcement Learning with Self-Play,"We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses.  The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL.","['differential privacy', 'multi-agent reinforcement learning', 'trajectory-wise privacy protection']",[],"['Dan Qiao', 'Yu-Xiang Wang']","['UC San Diego, University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=SrFbgIjb53,Security,MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability,"Large Language Models (LLMs) are increasingly deployed in various applications. As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions. Many defense strategies have been developed to enhance the safety of LLMs. However, our research finds that existing defense strategies lead LLMs to predominantly adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. To solve this problem, we introduce the MoGU framework, designed to enhance LLMs' safety while preserving their usability. Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution. When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless. Conversely, for benign instructions, the router prioritizes the usable LLM, facilitating usable and helpful responses. On various open-sourced LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework. Besides, our analysis provides key insights into the effectiveness of MoGU and verifies that our designed routing mechanism can effectively balance the contribution of each variant by assigning weights. Our work released the safer Llama2, Vicuna, Falcon, Dolphin, and Baichuan2.","['Enhancing Safety', 'Preseving Usability', 'Large Language Models']",[],"['Yanrui Du', 'Sendong Zhao', 'Danyang Zhao', 'Ming Ma', 'Yuhan Chen', 'Liangyu Huo', 'Qing Yang', 'Dongliang Xu', 'Bing Qin']","['Harbin Institute of Technology', 'Harbin Institute of Technology', 'computer science, Harbin Institute of Technology', 'faculty of computing, Harbin Institute of Technology', '', 'Du Xiaoman Technology(BeiJing)', 'Du Xiaoman Technology(BeiJing)', 'DuXiaoman Technology', 'Harbin Institute of Technology']",
https://openreview.net/forum?id=T826pwZLci,Privacy & Data Governance,Federated Online Prediction from Experts with Differential Privacy: Separations and Regret Speed-ups,"We study the problems of differentially private federated online prediction from experts against both *stochastic adversaries* and *oblivious adversaries*. We aim to minimize the average regret on $m$ clients working in parallel over time horizon $T$ with explicit differential privacy (DP) guarantees. With stochastic adversaries, we propose a **Fed-DP-OPE-Stoch** algorithm that achieves $\sqrt{m}$-fold speed-up of the per-client regret compared to the single-player counterparts under both pure DP and approximate DP constraints, while maintaining logarithmic communication costs. With oblivious adversaries, we establish non-trivial lower bounds indicating that *collaboration among clients does not lead to regret speed-up with general oblivious adversaries*. We then consider a special case of the oblivious adversaries setting, where there exists a low-loss expert. We design a new algorithm **Fed-SVT** and show that it achieves an $m$-fold regret speed-up under both pure DP and approximate DP constraints over the single-player counterparts. Our lower bound indicates that Fed-SVT is nearly optimal up to logarithmic factors. Experiments demonstrate the effectiveness of our proposed algorithms. To the best of our knowledge, this is the first work examining the differentially private online prediction from experts in the federated setting.","['online learning', 'federated learning', 'differential privacy']",[],"['Fengyu Gao', 'Ruiquan Huang', 'Jing Yang']","['University of Virginia', 'Electrical Engineering, Pennsylvania State University', 'University of Virginia, Charlottesville']",
https://openreview.net/forum?id=TGmwp9jJXl,Fairness & Bias,From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach,"We investigate learning the eigenfunctions of evolution operators for time-reversal invariant stochastic processes, a prime example being the Langevin equation used in molecular dynamics. Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation. To overcome this bottleneck, data are collected via biased simulations that explore the state space more rapidly. We propose a framework for learning from biased simulations rooted in the infinitesimal generator of the process {and the associated resolvent operator}. We contrast our approach to more common ones based on the transfer operator, showing that it can provably learn the spectral properties of the unbiased system from biased data.  In experiments, we highlight the advantages of our method over transfer operator approaches  and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues. Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach recovers relevant information about the transition mechanism.","['Molecular dynamics', 'stochastic differential equations']",[],"['Timothée Devergne', 'Vladimir R Kostic', 'Michele Parrinello', 'Massimiliano Pontil']","['Università degli Studi di Genova, Istituto Italiano di Tecnologia', 'Istituto Italiano di Tecnologia', 'Istituto Italiano di Tecnologia, Genova', 'Università degli Studi di Genova, Istituto Italiano di Tecnologia']",
https://openreview.net/forum?id=TBVLQjdFcA,Fairness & Bias,Generated and Pseudo Content guided Prototype Refinement for Few-shot Point Cloud Segmentation,"Few-shot 3D point cloud semantic segmentation aims to segment query point clouds with only a few annotated support point clouds. Existing prototype-based methods learn prototypes from the 3D support set to guide the segmentation of query point clouds. However, they encounter the challenge of low prototype quality due to constrained semantic information in the 3D support set and class information bias between support and query sets. To address these issues, in this paper, we propose a novel framework called Generated and Pseudo Content guided Prototype Refinement (GPCPR), which explicitly leverages LLM-generated content and reliable query context to enhance prototype quality. GPCPR achieves prototype refinement through two core components: LLM-driven Generated Content-guided Prototype Refinement (GCPR) and Pseudo Query Context-guided Prototype Refinement (PCPR). Specifically, GCPR integrates diverse and differentiated class descriptions generated by large language models to enrich prototypes with comprehensive semantic knowledge. PCPR further aggregates reliable class-specific pseudo-query context to mitigate class information bias and generate more suitable query-specific prototypes. Furthermore, we introduce a dual-distillation regularization term, enabling knowledge transfer between early-stage entities (prototypes or pseudo predictions) and their deeper counterparts to enhance refinement. Extensive experiments demonstrate the superiority of our method, surpassing the state-of-the-art methods by up to 12.10% and 13.75% mIoU on S3DIS and ScanNet, respectively.",['3D point cloud segmentation; few shot learning; Large Language Model'],[],"['Lili Wei', 'Congyan Lang', 'Ziyi Chen', 'Tao Wang', 'Yidong Li', 'Jun Liu']","['School of Computer Science & Technology, Beijing Jiaotong university', 'School of Computer Science and Technology, Beijing jiaotong university', 'School of Computer and Information Technology, Beijing Jiaotong University', 'Computer Science and Technology, Beijing Jiaotong University', 'School of Computer Science and Technology, Beijing Jiaotong University', 'Lancaster University']",
https://openreview.net/forum?id=TIhiFqGOYC,Transparency & Explainability,Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,"Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with several simple questions supported by a generic fact, LLMs often struggle to abstract and apply the generic fact to provide consistent and precise answers, revealing a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts. The code is available at https://github.com/Waste-Wood/MeanLearn.","['Abstract Reasoning', 'Large Language Models', 'Question Answering']",[],"['Kai Xiong', 'Xiao Ding', 'Ting Liu', 'Bing Qin', 'Dongliang Xu', 'Qing Yang', 'Hongtao Liu', 'Yixin Cao']","['Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'Harbin Institute of Technology', 'DuXiaoman Technology', 'Du Xiaoman Technology(BeiJing)', 'Du Xiaoman Financial', 'Fudan University']",
https://openreview.net/forum?id=TNEmAgwoXR,Security,Confident Natural Policy Gradient for Local Planning in  $q_\pi$-realizable Constrained MDPs,"The constrained Markov decision process (CMDP) framework emerges as an important reinforcement learning approach for imposing safety or other critical objectives while maximizing cumulative reward. However, the current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains under investigation, particularly when function approximation is applied to the value functions. In this paper, we address the learning problem given linear function approximation with $q_{\pi}$-realizability, where the value functions of all policies are linearly representable with a known feature map, a setting known to be more general and challenging than other linear settings. Utilizing a local-access model, we propose a novel primal-dual algorithm that, after $\tilde{O}(\text{poly}(d) \epsilon^{-3})$ iterations, outputs with high probability a policy that strictly satisfies the constraints while nearly optimizing the value with respect to a reward function. Here, $d$ is the feature dimension and $\epsilon > 0$ is a given  error. The algorithm relies on a carefully crafted off-policy evaluation procedure to evaluate the policy using historical data, which informs policy updates through policy gradients and conserves samples. To our knowledge, this is the first result achieving polynomial sample complexity for CMDP in the $q_{\pi}$-realizable setting.","['reinforcement learning', 'constrained MDP', 'sample complexity', 'q-pi realizability', 'local planning']",[],"['Tian Tian', 'Lin Yang', 'Csaba Szepesvari']","['University of Alberta', 'University of California, Los Angeles', 'University of Alberta']",
https://openreview.net/forum?id=TeQvz5AlI8,Security,DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in Frequency Domain,"To protect deep neural networks (DNNs) from adversarial attacks, adversarial training (AT) is developed by incorporating adversarial examples (AEs) into model training. Recent studies show that adversarial attacks disproportionately impact the patterns within the phase of the sample's frequency spectrum---typically containing crucial semantic information---more than those in the amplitude, resulting in the model's erroneous categorization of AEs. We find that, by mixing the amplitude of training samples' frequency spectrum with those of distractor images for AT, the model can be guided to focus on phase patterns unaffected by adversarial perturbations. As a result, the model's robustness can be improved. Unfortunately, it is still challenging to select appropriate distractor images, which should mix the amplitude without affecting the phase patterns. To this end, in this paper, we propose an optimized **Adversarial Amplitude Generator (AAG)** to achieve a better tradeoff between improving the model's robustness and retaining phase patterns. Based on this generator, together with an efficient AE production procedure, we design a new **Dual Adversarial Training (DAT)** strategy. Experiments on various datasets show that our proposed DAT leads to significantly improved robustness against diverse adversarial attacks. The source code is available at https://github.com/Feng-peng-Li/DAT.","['Deep learning', 'Model robustness']",[],"['Fengpeng Li', 'Kemou Li', 'Haiwei Wu', 'Jinyu Tian', 'Jiantao Zhou']","['Faculty of Science and Technology, University of Macau', 'University of Macau', '', 'Macau University of Science and Technology', 'Computer and Information Science, University of Macau']",
https://openreview.net/forum?id=TbPv0qFnHO,Fairness & Bias,Beyond Euclidean: Dual-Space Representation Learning for Weakly Supervised Video Violence Detection,"While numerous Video Violence Detection (VVD) methods have focused on representation learning in Euclidean space, they struggle to learn sufficiently discriminative features, leading to weaknesses in recognizing normal events that are visually similar to violent events (i.e., ambiguous violence). In contrast, hyperbolic representation learning, renowned for its ability to model hierarchical and complex relationships between events, has the potential to amplify the discrimination between visually similar events. Inspired by these, we develop a novel Dual-Space Representation Learning (DSRL) method for weakly supervised VVD to utilize the strength of both Euclidean and hyperbolic geometries, capturing the visual features of events while also exploring the intrinsic relations between events, thereby enhancing the discriminative capacity of the features. DSRL employs a novel information aggregation strategy to progressively learn event context in hyperbolic spaces, which selects aggregation nodes through layer-sensitive hyperbolic association degrees constrained by hyperbolic Dirichlet energy. Furthermore, DSRL attempts to break the cyber-balkanization of different spaces, utilizing cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features for final violence detection.  Comprehensive experiments demonstrate the effectiveness of our proposed DSRL.","['weakly supervised video violence detection', 'ambiguous violence', 'hyperbolic graph convolutional network', 'dual-space interaction']",[],"['Jiaxu Leng', 'Zhanjie Wu', 'Mingpi Tan', 'Yiran Liu', 'Ji Gan', 'Haosheng Chen', 'Xinbo Gao']","['Chongqing University of Post and Telecommunications, Chongqing University of Post and Telecommunications', 'Chongqing University of Post and Telecommunications', 'Chongqing University of Post and Telecommunications', 'Computer Science and Technology, Chongqing University of Post and Telecommunications', 'School of Computer Science and Technology, Chongqing University of Post and Telecommunications', 'School of Computer Science and Technology, Chongqing University of Post and Telecommunications', 'Computer Science and Technology, Chongqing University of Post and Telecommunications']",
https://openreview.net/forum?id=Tnl2K6Iz9j,Fairness & Bias,Dynamic Service Fee Pricing under Strategic Behavior: Actions as Instruments and Phase Transition,"We study a dynamic pricing problem for third-party platform service fees under strategic, far-sighted customers. In each time period, the platform sets a service fee based on historical data, observes the resulting transaction quantities, and collects revenue. The platform also monitors equilibrium prices influenced by both demand and supply. The objective is to maximize total revenue over a time horizon $T$. Our problem incorporates three practical challenges: (a) initially, the platform lacks knowledge of the demand side beforehand, necessitating a balance between exploring (learning the demand curve) and exploiting (maximizing revenue) simultaneously; (b) since only equilibrium prices and quantities are observable, traditional Ordinary Least Squares (OLS) estimators would be biased and inconsistent; (c) buyers are rational and strategic, seeking to maximize their consumer surplus and potentially misrepresenting their preferences. To address these challenges, we propose novel algorithmic solutions. Our approach involves: (i) a carefully designed active randomness injection to balance exploration and exploitation effectively; (ii) using non-i.i.d. actions as instrumental variables (IV) to consistently estimate demand; (iii) a low-switching cost design that promotes nearly truthful buyer behavior. We show an expected regret bound of $\tilde{\mathcal{O}} (\sqrt{T}\wedge\sigma_S^{-2})$ and demonstrate its optimality, up to logarithmic factors, with respect to both the time horizon $T$ and the randomness in supply $\sigma_S$. Despite its simplicity, our model offers valuable insights into the use of actions as estimation instruments, the benefits of low-switching pricing policies in mitigating strategic buyer behavior, and the role of supply randomness in facilitating exploration which leads to a phase transition of policy performance.","['online learning', 'online platform', 'dynamic pricing', 'strategic behavior', 'instrumental variable', 'phase transition']",[],"['Rui Ai', 'David Simchi-Levi', 'Feng Zhu']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=TuCQdBo4NC,Security,FEEL-SNN: Robust Spiking Neural Networks with Frequency Encoding and Evolutionary Leak Factor,"Currently, researchers think that the inherent robustness of spiking neural networks (SNNs) stems from their biologically plausible spiking neurons, and are dedicated to developing more bio-inspired models to defend attacks. However, most work relies solely on experimental analysis and lacks theoretical support, and the direct-encoding method and fixed membrane potential leak factor they used in spiking neurons are simplified simulations of those in the biological nervous system, which makes it difficult to ensure generalizability across all datasets and networks. Contrarily, the biological nervous system can stay reliable even in a highly complex noise environment, one of the reasons is selective visual attention and non-fixed membrane potential leaks in biological neurons.  This biological finding has inspired us to design a highly robust SNN model that closely mimics the biological nervous system. In our study, we first present a unified theoretical framework for SNN robustness constraint, which suggests that improving the encoding method and evolution of the membrane potential leak factor in spiking neurons can improve SNN robustness. Subsequently, we propose a robust SNN (FEEL-SNN) with Frequency Encoding (FE) and Evolutionary Leak factor (EL) to defend against different noises, mimicking the selective visual attention mechanism and non-fixed leak observed in biological systems.  Experimental results confirm the efficacy of both our FE, EL, and FEEL methods, either in isolation or in conjunction with established robust enhancement algorithms, for enhancing the robustness of SNNs.","['spiking neural network', 'robustness', 'attack']",[],"['Mengting Xu', 'De Ma', 'Huajin Tang', 'Qian Zheng', 'Gang Pan']","['Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Computer Science and Technology, Zhejiang University', '']",
https://openreview.net/forum?id=TzxSrNJE0T,Fairness & Bias,Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation,"Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train deep neural networks and generative models. Most theoretical results assume that it is possible to obtain unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias of the gradient estimator.  In particular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential moving average variant of Adam, with biased gradients, converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) and applications to several learning frameworks that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.","['Stochastic Optimization', 'Adaptive Stochastic Approximation', 'Monte Carlo Methods', 'Variational Autoenconders']",[],"['Sobihan Surendran', 'Adeline Fermanian', 'Antoine Godichon-Baggioni', 'Sylvain Le Corff']","['Sorbonne Université - Faculté des Sciences (Paris VI)', 'Califrais', 'Sorbonne Université - Faculté des Sciences (Paris VI)', 'Sorbonne Université, LPSM']",
https://openreview.net/forum?id=U4KldRgoph,Fairness & Bias,Enhancing Graph Transformers with Hierarchical Distance Structural Encoding,"Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current methods often fall short in capturing longer ranges, hierarchical structures, or community structures, which are common in various graphs such as molecules, social networks, and citation networks. This paper presents a Hierarchical Distance Structural Encoding (HDSE) method to model node distances in a graph, focusing on its multi-level, hierarchical nature. We introduce a novel framework to seamlessly integrate HDSE into the attention mechanism of existing graph transformers, allowing for simultaneous application with other positional encodings. To apply graph transformers with HDSE to large-scale graphs, we further propose a high-level HDSE that effectively biases the linear transformers towards graph hierarchies. We theoretically prove the superiority of HDSE in terms of expressivity and generalization. Empirically, we demonstrate that graph transformers with HDSE excel in graph classification, regression on 7 graph-level datasets, and node classification on 11 large-scale graphs.","['Graph Transformers', 'Graph Neural Networks', 'Graph Classification', 'Node Classification', 'Large Graphs', 'Scalability']",[],"['Yuankai Luo', 'Hongkang Li', 'Lei Shi', 'Xiao-Ming Wu']","['Hong Kong Polytechnic University', 'Rensselaer Polytechnic Institute', 'Beijing University of Aeronautics and Astronautics', 'Hong Kong Polytechnic University']",
https://openreview.net/forum?id=U9MzoDOKZu,Fairness & Bias,Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement,"A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.","['decision transformer', 'offline meta reinforcement learning', 'world model']",[],"['Zhi Wang', 'Li Zhang', 'Wenhao Wu', 'Yuanheng Zhu', 'Dongbin Zhao', 'Chunlin Chen']","['', 'nanjing university', 'School of Management and Engineering, nanjing university', '', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'School of Management and Engineering, Nanjing University']",
https://openreview.net/forum?id=U3Rgdb4li9,Transparency & Explainability,Targeted Sequential Indirect Experiment Design,"Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are high-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.","['causality', 'experiment design', 'instrumental variables', 'indirect experiments']",[],"['Elisabeth Ailer', 'Niclas Dern', 'Jason Hartford', 'Niki Kilbertus']","['Helmholtz Zentrum', 'Technische Universität München', 'Computer Science, University of Manchester', 'Technische Universität München']",
https://openreview.net/forum?id=U6oQEzSp8z,Transparency & Explainability,An eye for an ear: zero-shot audio description leveraging an image captioner with audio-visual token distribution matching,"Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.","['Multimodal representation learning', 'Audio Captioning', 'Image Captioning', 'Audio-Visual', 'Large Language Model']",[],"['Hugo Malard', 'Michel Olvera', 'Stéphane Lathuilière', 'Slim Essid']","['Télécom ParisTech', '', 'INRIA', 'IDS, Télécom ParisTech']",
https://openreview.net/forum?id=UBpPOqrBKE,Privacy & Data Governance,Federated Graph Learning for Cross-Domain Recommendation,"Cross-domain recommendation (CDR) offers a promising solution to the data sparsity problem by enabling knowledge transfer across source and target domains. However, many recent CDR models overlook crucial issues such as privacy as well as the risk of negative transfer (which negatively impact model performance), especially in multi-domain settings. To address these challenges, we propose FedGCDR, a novel federated graph learning framework that securely and effectively leverages positive knowledge from multiple source domains. First, we design a positive knowledge transfer module that ensures privacy during inter-domain knowledge transmission. This module employs differential privacy-based knowledge extraction combined with a feature mapping mechanism, transforming source domain embeddings from federated graph attention networks into reliable domain knowledge. Second, we design a knowledge activation module to filter out potential harmful or conflicting knowledge from source domains, addressing the issues of negative transfer. This module enhances target domain training by expanding the graph of the target domain to generate reliable domain attentions and fine-tunes the target model for improved negative knowledge filtering and more accurate predictions. We conduct extensive experiments on 16 popular domains of the Amazon dataset, demonstrating that FedGCDR significantly outperforms state-of-the-art methods.",['federated learning; cross-domain recommendation; negative transfer; GNN'],[],"['Ziqi Yang', 'Zhaopeng Peng', 'Zihui Wang', 'Jianzhong Qi', 'Chaochao Chen', 'Weike Pan', 'Chenglu Wen', 'Cheng Wang', 'Xiaoliang Fan']","['Xiamen University, Xiamen University', 'School of Informatics, Xiamen University', 'School of Information, Xiamen University', 'University of Melbourne', 'Zhejiang University', '', 'Artificial Intelligence Department, Xiamen University', 'School of Informatics, Xiamen University', 'Computer Science, Xiamen University']",
https://openreview.net/forum?id=UDi51I8K1p,Transparency & Explainability,Exploring the trade-off between deep-learning and explainable models for brain-machine interfaces,"People with brain or spinal cord-related paralysis often need to rely on others for basic tasks, limiting their independence. A potential solution is brain-machine interfaces (BMIs), which could allow them to voluntarily control external devices (e.g., robotic arm) by decoding brain activity to movement commands. In the past decade, deep-learning decoders have achieved state-of-the-art results in most BMI applications, ranging from speech production to finger control. However, the 'black-box' nature of deep-learning decoders could lead to unexpected behaviors, resulting in major safety concerns in real-world physical control scenarios. In these applications, explainable but lower-performing decoders, such as the Kalman filter (KF), remain the norm. In this study, we designed a BMI decoder based on KalmanNet, an extension of the KF that augments its operation with recurrent neural networks to compute the Kalman gain. This results in a varying “trust” that shifts between inputs and dynamics. We used this algorithm to predict finger movements from the brain activity of two monkeys. We compared KalmanNet results offline (pre-recorded data, $n=13$ days) and online (real-time predictions, $n=5$ days) with a simple KF and two recent deep-learning algorithms: tcFNN (non-ReFIT version) and LSTM. KalmanNet achieved comparable or better results than other deep learning models in offline and online modes, relying on the dynamical model for stopping while depending more on neural inputs for initiating movements. We further validated this mechanism by implementing a heteroscedastic KF that used the same strategy, and it also approached state-of-the-art performance while remaining in the explainable domain of standard KFs. However, we also see two downsides to KalmanNet. KalmanNet shares the limited generalization ability of existing deep-learning decoders, and its usage of the KF as an inductive bias limits its performance in the presence of unseen noise distributions. Despite this trade-off, our analysis successfully integrates traditional controls and modern deep-learning approaches to motivate high-performing yet still explainable BMI designs.","['brain-machine interfaces', 'neural decoders', 'safety', 'kalman filter', 'real-time processing']",[],"['Luis Hernan Cubillos', 'Guy Revach', 'Matthew Mender', 'Joseph T Costello', 'Hisham Temmar', 'Aren Hite', 'Diksha Anoop Kumar Zutshi', 'Dylan Michael Wallace', 'Xiaoyong Ni', 'Madison M. Kelberman', 'Matt Willsey', 'Ruud Van Sloun', 'Nir Shlezinger', 'Parag Ganapati Patil', 'Anne Draelos', 'Cynthia Chestek']","['Robotics, University of Michigan - Ann Arbor', 'Electrical Engineering, ETH Zürich', 'University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor', 'Biomedical Engineering, University of Michigan - Ann Arbor', 'Biomedical Engineering, University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor', 'Campus Biotech Geneva, EPFL - EPF Lausanne', 'Biomedical Engineering, University of Michigan - Ann Arbor', 'Stanford University', 'Eindhoven University of Technology', 'ECE, Ben Gurion University of the Negev', 'University of Michigan - Ann Arbor', 'Biomedical Engineering and Computational Medicine & Bioinformatics, University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor']",
https://openreview.net/forum?id=UMPedMhKWm,Security,Rapid Plug-in Defenders,"In the realm of daily services, the deployment of deep neural networks underscores the paramount importance of their reliability. However, the vulnerability of these networks to adversarial attacks, primarily evasion-based, poses a concerning threat to their functionality. Common methods for enhancing robustness involve heavy adversarial training or leveraging learned knowledge from clean data, both necessitating substantial computational resources. This inherent time-intensive nature severely limits the agility of large foundational models to swiftly counter adversarial perturbations. To address this challenge, this paper focuses on the \textbf{Ra}pid \textbf{P}lug-\textbf{i}n \textbf{D}efender (\textbf{RaPiD}) problem, aiming to rapidly counter adversarial perturbations without altering the deployed model. Drawing inspiration from the generalization and the universal computation ability of pre-trained transformer models, we propose a novel method termed \textbf{CeTaD} (\textbf{C}onsidering Pr\textbf{e}-trained \textbf{T}ransformers \textbf{a}s \textbf{D}efenders) for RaPiD, optimized for efficient computation. \textbf{CeTaD} strategically fine-tunes the normalization layer parameters within the defender using a limited set of clean and adversarial examples. Our evaluation centers on assessing \textbf{CeTaD}'s effectiveness, transferability, and the impact of different components in scenarios involving one-shot adversarial examples. The proposed method is capable of rapidly adapting to various attacks and different application scenarios without altering the target model and clean training data. We also explore the influence of varying training data conditions on \textbf{CeTaD}'s performance. Notably, \textbf{CeTaD} exhibits adaptability across differentiable service models and proves the potential of continuous learning.","['Rapid Plug-in Defenders', 'Few-shot Adversarial Training', 'Adversarial Examples and Defenses']",[],"['Kai Wu', 'Yujian Betterest Li', 'Jian Lou', 'Xiaoyu Zhang', 'Handing Wang', 'Jing Liu']","['', '', 'Sun Yat-Sen University', 'School of Cyber Engineering, Xidian University', 'School of Artificial Intelligence, Xidian University', 'Guangzhou Institute of Technology, Xidian University, China']",
https://openreview.net/forum?id=UTrIEHobXI,Security,Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images,"Single-view 3D reconstruction methods like Triplane Gaussian Splatting (TGS) have enabled high-quality 3D model generation from just a single image input within seconds. However, this capability raises concerns about potential misuse, where malicious users could exploit TGS to create unauthorized 3D models from copyrighted images. To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed ``geometry cloaks'', into images before supplying them to TGS. These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional adversarial attacks that simply degrade output quality, our method forces TGS to fail the 3D reconstruction in a specific way - by generating an identifiable customized pattern that acts as a watermark. This watermark allows copyright holders to assert ownership over any attempted 3D reconstructions made from their protected images. Extensive experiments have verified the effectiveness of our geometry cloak.","['Copyright protection', 'Image cloaking', 'single-view image to 3D model']",[],"['Qi Song', 'Ziyuan Luo', 'Ka Chun Cheung', 'Simon See', 'Renjie Wan']","['Hong Kong Baptist University', '', 'NVIDIA', 'NVAITC, NVIDIA', 'Department of Computer Science, Hong Kong Baptist University']",
https://openreview.net/forum?id=UVjuYBSbCN,Fairness & Bias,Toward a Well-Calibrated Discrimination via Survival Outcome-Aware Contrastive Learning,"Previous deep learning approaches for survival analysis have primarily relied on ranking losses to improve discrimination performance, which often comes at the expense of calibration performance.  To address such an issue, we propose a novel contrastive learning approach specifically designed to enhance discrimination without sacrificing calibration.  Our method employs weighted sampling within a contrastive learning framework, assigning lower penalties to samples with similar survival outcomes. This aligns well with the assumption that patients with similar event times share similar clinical statuses. Consequently, when augmented with the commonly used negative log-likelihood loss, our approach significantly improves discrimination performance without directly manipulating the model outputs, thereby achieving better calibration. Experiments on multiple real-world clinical datasets demonstrate that our method outperforms state-of-the-art deep survival models in both discrimination and calibration. Through comprehensive ablation studies, we further validate the effectiveness of our approach through quantitative and qualitative analyses.","['Survival Analysis', 'Contrastive Learning', 'Deep Learning', 'Healthcare']",[],"['Dongjoon Lee', 'Hyeryn Park', 'Changhee Lee']","['Chung-Ang University', 'Chung-Ang University', 'Korea University']",
https://openreview.net/forum?id=UdXE5V2d0O,Security,Direct Unlearning Optimization for Robust and Safe Text-to-Image Models,"Recent advancements in text-to-image (T2I) models have greatly benefited from large-scale datasets, but they also pose significant risks due to the potential generation of unsafe content. To mitigate this issue, researchers proposed unlearning techniques that attempt to induce the model to unlearn potentially harmful prompts. However, these methods are easily bypassed by adversarial attacks, making them unreliable for ensuring the safety of generated images. In this paper, we propose Direct Unlearning Optimization (DUO), a novel framework for removing NSFW content from T2I models while preserving their performance on unrelated topics. DUO employs a preference optimization approach using curated paired image data, ensuring that the model learns to remove unsafe visual concepts while retain unrelated features. Furthermore, we introduce an output-preserving regularization term to maintain the model's generative capabilities on safe content. Extensive experiments demonstrate that DUO can robustly defend against various state-of-the-art red teaming methods without significant performance degradation on unrelated topics, as measured by FID and CLIP scores. Our work contributes to the development of safer and more reliable T2I models, paving the way for their responsible deployment in both closed-source and open-source scenarios.","['diffusion models', 'unlearning', 'safety']",[],"['Yong-Hyun Park', 'Sangdoo Yun', 'Jin-Hwa Kim', 'Junho Kim', 'Geonhui Jang', 'Yonghyun Jeong', 'Junghyo Jo', 'Gayoung Lee']","['NAVER', 'NAVER', 'Seoul National University', 'AI Lab, NAVER', 'School of Industrial and Management Engineering, Korea University', 'NAVER', 'Seoul National University', 'NAVER']",
https://openreview.net/forum?id=5atraF1tbg,Transparency & Explainability,PANORAMIA: Privacy Auditing of Machine Learning Models without Retraining,"We present PANORAMIA, a privacy leakage measurement framework for machine learning models that relies on membership inference attacks using generated data as non-members. By relying on generated non-member data, PANORAMIA eliminates the common dependency of privacy measurement tools on in-distribution non-member data. As a result, PANORAMIA does not modify the model, training data, or training process, and only requires access to a subset of the training data. We evaluate PANORAMIA on ML models for image and tabular data classification, as well as on large-scale language models.","['privacy', 'auditing', 'machine learning', 'differential privacy', 'membership inference attack']",[],"['Mishaal Kazmi', 'Hadrien Lautraite', 'Alireza Akbari', 'Qiaoyue Tang', 'Mauricio Soroco', 'Tao Wang', 'Sébastien Gambs', 'Mathias Lécuyer']","['University of British Columbia', 'Université du Québec à Montréal', 'Simon Fraser University', 'University of British Columbia', 'Computing Science, Simon Fraser University', 'Simon Fraser University', 'Computer Science, Université du Québec à Montréal', 'Computer Science, University of British Columbia']",
https://openreview.net/forum?id=Ugr0yPzY71,Security,Faster Repeated Evasion Attacks in Tree Ensembles,"Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.","['ML', 'Tree Ensembles', 'Verification', 'Evasion Attacks', 'Adversarial Attacks']",[],"['Lorenzo Cascioli', 'Laurens Devos', 'Ondrej Kuzelka', 'Jesse Davis']","['KU Leuven', 'KU Leuven', 'CS Department, Czech Technical University in Prague', 'Department of Computer Science, KU Leuven']",
https://openreview.net/forum?id=UkauUrTbxx,Security,ProTransformer: Robustify Transformers via Plug-and-Play Paradigm,"Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5\%, 28.3\%, 16.1\%, and 11.4\% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8\% and 17.8\%, respectively, and enhancing Vicuna by an average of 10.4\% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains.","['Transformers', 'Adversarial Robustness', 'Large Language Models']",[],"['Zhichao Hou', 'Weizhi Gao', 'Yuchen Shen', 'Feiyi Wang', 'Xiaorui Liu']","['Computer Science, North Carolina State University', 'Computer Science Department, North Carolina State University', 'Carnegie Mellon University', 'Oak Ridge National Laboratory', 'Computer Science, North Carolina State University']",
https://openreview.net/forum?id=ybMrn4tdn0,Transparency & Explainability,Auditing Local Explanations is Hard,"In sensitive contexts, providers of machine learning algorithms are increasingly required to give explanations for their algorithms' decisions. However, explanation receivers might not trust the provider, who potentially could output misleading or manipulated explanations. In this work, we investigate an auditing framework in which a third-party auditor or a collective of users attempts to sanity-check explanations: they can query model decisions and the corresponding local explanations, pool all the information received, and then check for basic consistency properties. We prove upper and lower bounds on the amount of queries that are needed for an auditor to succeed within this framework. Our results show that successful auditing requires a potentially exorbitant number of queries -- particularly in high dimensional cases. Our analysis also reveals that a key property is the ``locality'' of the provided explanations --- a quantity that so far has not been paid much attention to in the explainability literature. Looking forward, our results suggest that for complex high-dimensional settings, merely providing a pointwise prediction and explanation could be insufficient, as there is no way for the users to verify that the provided explanations are not completely made-up.","['explanation', 'auditing', 'trust', 'regulation']",[],"['Robi Bhattacharjee', 'Ulrike von Luxburg']","['Computer Science, Eberhard-Karls-Universität Tübingen', 'University of Tuebingen']",
https://openreview.net/forum?id=UwvjJZWjPT,Fairness & Bias,Inductive biases of multi-task learning and finetuning: multiple regimes of feature reuse,"Neural networks are often trained on multiple tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In particular, it is common practice to pretrain neural networks on a large auxiliary task before finetuning on a downstream task with fewer samples. Despite the prevalence of this approach, the inductive biases that arise from learning multiple tasks are poorly characterized. In this work, we address this gap. We describe novel implicit regularization penalties associated with MTL and PT+FT in diagonal linear networks and single-hidden-layer ReLU networks. These penalties indicate that MTL and PT+FT induce the network to reuse features in different ways. 1) Both MTL and PT+FT exhibit biases towards feature reuse between tasks, and towards sparsity in the set of learned features. We show a ""conservation law"" that implies a direct tradeoff between these two biases. 2) PT+FT exhibits a novel ""nested feature selection"" regime, not described by either the ""lazy"" or ""rich"" regimes identified in prior work, which biases it to *rely on a sparse subset* of the features learned during pretraining. This regime is much narrower for MTL. 3) PT+FT (but not MTL) in ReLU networks benefits from features that are correlated between the auxiliary and main task. We confirm these findings empirically with teacher-student models, and introduce a technique -- weight rescaling following pretraining -- that can elicit the nested feature selection regime. Finally, we validate our theory in deep neural networks trained on image classification. We find that weight rescaling improves performance when it causes models to display signatures of nested feature selection. Our results suggest that nested feature selection may be an important inductive bias for finetuning neural networks.","['multi-task learning', 'implicit regularization', 'finetuning', 'pretraining', 'implicit bias']",[],"['Samuel Lippl', 'Jack Lindsey']","['Columbia University', 'Columbia University']",
https://openreview.net/forum?id=UvbpbEhGaw,Fairness & Bias,Self-Supervised Alignment with Mutual Information: Learning to Follow Principles without Preference Labels,"When prompting a language model (LM), users often expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles (i.e., a constitution) into a model is resource-intensive, technically challenging, and generally requires human preference labels or examples. We introduce SAMI, an iterative algorithm that finetunes a pretrained language model (without requiring preference labels or demonstrations) to increase the conditional mutual information between constitutions and self-generated responses given queries from a dataset. On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%. Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue. SAMI requires a model that writes the principles. To avoid dependence on strong models for writing principles, we align a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct), achieving a 65% win rate on summarization. Finally, we investigate whether SAMI generalizes to diverse summarization principles (e.g., ""summaries should be scientific"") and scales to stronger models (llama3-70b), finding that it achieves win rates of up to 68% for learned and 67% for held-out principles compared to the base model. Our results show that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight.","['alignment', 'contrastive learning', 'constitutional ai', 'self-improvement']",[],"['Jan-Philipp Fränken', 'Eric Zelikman', 'Rafael Rafailov', 'Kanishk Gandhi', 'Tobias Gerstenberg', 'Noah Goodman']","['Stanford University', 'Stanford University', 'Computer Science, Stanford University', 'Computer Science, Stanford University', 'Stanford University', 'Stanford University']",
https://openreview.net/forum?id=Uymv9ThB50,Security,Uncovering Safety Risks of Large Language Models through Concept Activation Vector,"Despite careful safety alignment, current large language models (LLMs) remain vulnerable to various attacks. To further unveil the safety risks of LLMs, we introduce a Safety Concept Activation Vector (SCAV) framework, which effectively guides the attacks by accurately interpreting LLMs' safety mechanisms. We then develop an SCAV-guided attack method that can generate both attack prompts and embedding-level attacks with automatically selected perturbation hyperparameters. Both automatic and human evaluations demonstrate that our attack method significantly improves the attack success rate and response quality while requiring less training data. Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known. Our experiments further uncover the safety risks present in current LLMs. For example, in our evaluation of seven open-source LLMs, we observe an average attack success rate of 99.14%, based on the classic keyword-matching criterion. Finally, we provide insights into the safety mechanism of LLMs. The code is available at https://github.com/SproutNan/AI-Safety_SCAV.","['large language model', 'responsible AI', 'AI safety', 'concept-based model explanation']",[],"['Zhihao Xu', 'Ruixuan HUANG', 'Changyu Chen', 'Xiting Wang']","['Gaoling School of Artificial Intelligence, Renmin University of China', 'Hong Kong University of Science and Technology', 'Renmin University of China', 'Renmin University of China']",
https://openreview.net/forum?id=VMiLdBkCJM,Fairness & Bias,Towards Combating Frequency Simplicity-biased Learning for Domain Generalization,"Domain generalization methods aim to learn transferable knowledge from source domains that can generalize well to unseen target domains.  Recent studies show that neural networks frequently suffer from a simplicity-biased learning behavior which leads to over-reliance on specific frequency sets, namely as frequency shortcuts, instead of semantic information, resulting in poor generalization performance.  Despite previous data augmentation techniques successfully enhancing generalization performances, they intend to apply more frequency shortcuts, thereby causing hallucinations of generalization improvement. In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective. Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain.  Intuitively, as frequency shortcuts are hidden in the dominant and highly dependent frequencies of dataset structure, dynamically perturbating the over-reliance frequency components could prevent the application of frequency shortcuts. To this end, we propose two effective data augmentation modules designed to collaboratively and adaptively adjust the frequency characteristic of the dataset, aiming to dynamically influence the learning behavior of the model and ultimately serving as a strategy to mitigate shortcut learning. Our code will be made publicly available.",['Frequency Shortcut;Domain Generalization'],[],"['Xilin He', 'Jingyu Hu', 'Qinliang Lin', 'Cheng Luo', 'Weicheng Xie', 'Siyang Song', 'Muhammad Haris Khan', 'Linlin Shen']","['The Chinese University of Hong Kong', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Shenzhen University', 'Department of Computer Science, University of Exeter', 'Computer Vision, Mohamed Bin Zayed University of Artificial Intelligence', 'Shenzhen University']",
https://openreview.net/forum?id=Vhh7ONtfvV,Transparency & Explainability,Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP,"Recent work has explored how individual components of the CLIP-ViT model contribute to the final representation by leveraging the shared image-text representation space of CLIP.  These components, such as attention heads and MLPs, have been shown to capture distinct image features like shape, color or texture. However, understanding the role of these components in arbitrary vision transformers (ViTs) is challenging. To this end, we introduce a general framework which can identify the roles of various components in ViTs beyond CLIP. Specifically, we (a) automate the decomposition of the final representation into contributions from different model components, and (b) linearly map these contributions to CLIP space to interpret them via text. Additionally, we introduce a novel scoring function to rank components by their importance with respect to specific features. Applying our framework to various ViT variants (e.g. DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the roles of different components concerning particular image features. These insights facilitate applications such as image retrieval using text descriptions or reference images, visualizing token importance heatmaps, and mitigating spurious correlations. We release our [code](https://github.com/SriramB-98/vit-decompose) to reproduce the experiments in the paper.","['vision', 'interpretability', 'explainability']",[],"['Sriram Balasubramanian', 'Samyadeep Basu', 'Soheil Feizi']","['University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'University of Maryland, College Park']",
https://openreview.net/forum?id=VrVx83BkQX,Security,Stepwise Alignment for Constrained Language Model Policy Optimization,"Safety and trustworthiness are indispensable requirements for real-world applications of AI systems using large language models (LLMs). This paper formulates human value alignment as an optimization problem of the language model policy to maximize reward under a safety constraint, and then proposes an algorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One key idea behind SACPO, supported by theory, is that the optimal policy incorporating reward and safety can be directly obtained from a reward-aligned policy. Building on this key idea, SACPO aligns LLMs step-wise with each metric while leveraging simple yet powerful alignment algorithms such as direct preference optimization (DPO). SACPO offers several advantages, including simplicity, stability, computational efficiency, and flexibility of algorithms and datasets. Under mild assumptions, our theoretical analysis provides the upper bounds on optimality and safety constraint violation. Our experimental results show that SACPO can fine-tune Alpaca-7B better than the state-of-the-art method in terms of both helpfulness and harmlessness.","['AI Alignment', 'Large Language Models', 'AI Safety', 'Safe RL']",[],"['Akifumi Wachi', 'Thien Q. Tran', 'Rei Sato', 'Takumi Tanabe', 'Youhei Akimoto']","['LY Corporation', 'LINE Yahoo Corp', 'LY Corporation', 'LY Corporation', 'RIKEN AIP']",
https://openreview.net/forum?id=WILLwyVmP8,Transparency & Explainability,Interpretable Concept-Based Memory Reasoning,"The lack of transparency in the decision-making processes of deep learning systems presents a significant challenge in modern artificial intelligence (AI), as it impairs users’ ability to rely on and verify these systems. To address this challenge, Concept Bottleneck Models (CBMs) have made significant progress by incorporating human-interpretable concepts into deep learning architectures. This approach allows predictions to be traced back to specific concept patterns that users can understand and potentially intervene on. However, existing CBMs’ task predictors are not fully interpretable, preventing a thorough analysis and any form of formal verification of their decision-making process prior to deployment, thereby raising significant reliability concerns. To bridge this gap, we introduce Concept-based Memory Reasoner (CMR), a novel CBM designed to provide a human-understandable and provably-verifiable task prediction process. Our approach is to model each task prediction as a neural selection mechanism over a memory of learnable logic rules, followed by a symbolic evaluation of the selected rule. The presence of an explicit memory and the symbolic evaluation allow domain experts to inspect and formally verify the validity of certain global properties of interest for the task prediction process. Experimental results demonstrate that CMR achieves better accuracy-interpretability trade-offs to state-of-the-art CBMs, discovers logic rules consistent with ground truths, allows for rule interventions, and allows pre-deployment verification.","['Concept-based models', 'explainable AI', 'neurosymbolic']",[],"['David Debot', 'Pietro Barbiero', 'Francesco Giannini', 'Gabriele Ciravegna', 'Michelangelo Diligenti', 'Giuseppe Marra']","['Computer Science, KU Leuven', 'International Business Machines', 'School of Education Pisa', 'Polytechnic Institute of Turin', 'Department of Information Engineering and Mathematical Sciences', 'KU Leuven']",
https://openreview.net/forum?id=WJ04ZX8txM,Transparency & Explainability,Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers,"Large Language Models (LLMs) have the capacity to store and recall facts. Through experimentation with open-source models, we observe that this ability to retrieve facts can be easily manipulated by changing contexts, even without altering their factual meanings. These findings highlight that LLMs might behave like an associative memory model where certain tokens in the contexts serve as clues to retrieving facts. We mathematically explore this property by studying how transformers, the building blocks of LLMs, can complete such memory tasks. We study a simple latent concept association problem with a one-layer transformer and we show theoretically and empirically that the transformer gathers information using self-attention and uses the value matrix for associative memory.","['Transformer', 'Associative Memory', 'Large Language Models', 'Interpretability', 'Fact retrieval']",[],"['Yibo Jiang', 'Goutham Rajendran', 'Pradeep Kumar Ravikumar', 'Bryon Aragam']","['University of Chicago', 'Meta GenAI', 'Carnegie Mellon University', 'University of Chicago']",
https://openreview.net/forum?id=WPPC7FHtaM,Transparency & Explainability,IPO: Interpretable Prompt Optimization for Vision-Language Models,"Pre-trained vision-language models like CLIP have remarkably adapted to various downstream tasks. Nonetheless, their performance heavily depends on the specificity of the input text prompts, which requires skillful prompt template engineering. Instead, current approaches to prompt optimization learn the prompts through gradient descent, where the prompts are treated as adjustable parameters. However, these methods tend to lead to overfitting of the base classes seen during training and produce prompts that are no longer understandable by humans. This paper introduces a simple but interpretable prompt optimizer (IPO), that utilizes large language models (LLMs) to generate textual prompts dynamically. We introduce a Prompt Optimization Prompt that not only guides LLMs in creating effective prompts but also stores past prompts with their performance metrics, providing rich in-context information. Additionally, we incorporate a large multimodal model (LMM) to condition on visual content by generating image descriptions, which enhance the interaction between textual and visual modalities. This allows for the creation of dataset-specific prompts that improve generalization performance, while maintaining human comprehension. Extensive testing across 11 datasets reveals that IPO not only improves the accuracy of existing gradient-descent-based prompt learning methods but also considerably enhances the interpretability of the generated prompts. By leveraging the strengths of LLMs, our approach ensures that the prompts remain human-understandable, thereby facilitating better transparency and oversight for vision-language models.","['Prompt learning', 'Large language model', 'Interpretable Prompt Optimization']",[],"['Yingjun Du', 'Wenfang Sun', 'Cees G. M. Snoek']","['IvI, University of Amsterdam', 'Westlake University', 'University of Amsterdam']",
https://openreview.net/forum?id=WSsht66fbC,Security,Safety through feedback in Constrained RL,"In safety-critical RL settings, the inclusion of an additional cost function is often favoured over the arduous task of modifying the reward function to ensure the agent's safe behaviour. However, designing or evaluating such a cost function can be prohibitively expensive. For instance, in the domain of self-driving, designing a cost function that encompasses all unsafe behaviours (e.g., aggressive lane changes, risky overtakes) is inherently complex, it must also consider all the actors present in the scene making it expensive to evaluate. In such scenarios, the cost function can be learned from feedback collected offline in between training rounds. This feedback can be system generated or elicited from a human observing the training process. Previous approaches have not been able to scale to complex environments and are constrained to receiving feedback at the state level which can be expensive to collect. To this end, we introduce an approach that scales to more complex domains and extends beyond state-level feedback, thus, reducing the burden on the evaluator. Inferring the cost function in such settings poses challenges, particularly in assigning credit to individual states based on trajectory-level feedback. To address this, we propose a surrogate objective that transforms the problem into a state-level supervised classification task with noisy labels, which can be solved efficiently. Additionally, it is often infeasible to collect feedback for every trajectory generated by the agent, hence, two fundamental questions arise: (1) Which trajectories should be presented to the human? and (2) How many trajectories are necessary for effective learning? To address these questions, we introduce a \textit{novelty-based sampling} mechanism that selectively involves the evaluator only when the the agent encounters a \textit{novel} trajectory, and discontinues querying once the trajectories are no longer \textit{novel}. We showcase the efficiency of our method through experimentation on several benchmark Safety Gymnasium environments and realistic self-driving scenarios. Our method demonstrates near-optimal performance, comparable to when the cost function is known, by relying solely on trajectory-level feedback across multiple domains. This highlights both the effectiveness and scalability of our approach. The code to replicate these results can be found at \href{https://github.com/shshnkreddy/RLSF}{https://github.com/shshnkreddy/RLSF}","['Constrained RL', 'Cost Inference', 'Human Feedback']",[],"['Shashank Reddy Chirra', 'Pradeep Varakantham', 'Praveen Paruchuri']","['Singapore Management University', 'School of Computing and Information Systems, Singapore Management University', 'IIIT Hyderabad']",
https://openreview.net/forum?id=Wl2optQcng,Fairness & Bias,Personalized Federated Learning via Feature Distribution Adaptation,"Federated learning (FL) is a distributed learning framework that leverages commonalities between distributed client datasets to train a global model. Under heterogeneous clients, however, FL can fail to produce stable training results. Personalized federated learning (PFL) seeks to address this by learning individual models tailored to each client. One approach is to decompose model training into shared representation learning and personalized classifier training. Nonetheless, previous works struggle to navigate the bias-variance trade-off in classifier learning, relying solely on limited local datasets or introducing costly techniques to improve generalization. In this work, we frame representation learning as a generative modeling task, where representations are trained with a classifier based on the global feature distribution. We then propose an algorithm, pFedFDA, that efficiently generates personalized models by adapting global generative classifiers to their local feature distributions. Through extensive computer vision benchmarks, we demonstrate that our method can adjust to complex distribution shifts with significant improvements over current state-of-the-art in data-scarce settings.","['Federated Learning', 'Data Heterogeneity', 'Personalization']",[],"['Connor Mclaughlin', 'Lili Su']","['Northeastern University', 'Northeastern University']",
https://openreview.net/forum?id=WhE4C4fLbE,Fairness & Bias,CHASE: Learning Convex Hull Adaptive Shift for Skeleton-based Multi-Entity Action Recognition,"Skeleton-based multi-entity action recognition is a challenging task aiming to identify interactive actions or group activities involving multiple diverse entities. Existing models for individuals often fall short in this task due to the inherent distribution discrepancies among entity skeletons, leading to suboptimal backbone optimization. To this end, we introduce a Convex Hull Adaptive Shift based multi-Entity action recognition method (CHASE), which mitigates inter-entity distribution gaps and unbiases subsequent backbones. Specifically, CHASE comprises a learnable parameterized network and an auxiliary objective. The parameterized network achieves plausible, sample-adaptive repositioning of skeleton sequences through two key components. First, the Implicit Convex Hull Constrained Adaptive Shift ensures that the new origin of the coordinate system is within the skeleton convex hull. Second, the Coefficient Learning Block provides a lightweight parameterization of the mapping from skeleton sequences to their specific coefficients in convex combinations. Moreover, to guide the optimization of this network for discrepancy minimization, we propose the Mini-batch Pair-wise Maximum Mean Discrepancy as the additional objective. CHASE operates as a sample-adaptive normalization method to mitigate inter-entity distribution discrepancies, thereby reducing data bias and improving the subsequent classifier's multi-entity action recognition performance. Extensive experiments on six datasets, including NTU Mutual 11/26, H2O, Assembly101, Collective Activity and Volleyball, consistently verify our approach by seamlessly adapting to single-entity backbones and boosting their performance in multi-entity scenarios. Our code is publicly available at https://github.com/Necolizer/CHASE .","['Action Recognition', 'Skeletons', 'Multi-Entity Actions', 'Convex Hull']",[],"['Yuhang Wen', 'Mengyuan Liu', 'Songtao Wu', 'Beichen Ding']","['School of Intelligent Systems Engineering, SUN YAT-SEN UNIVERSITY', 'Peking University', 'Sony R&D Center China, Sony (China) Limited', 'School of Advanced Manufacturing, Sun Yat-Sen University']",
https://openreview.net/forum?id=WvWS8goWyR,Fairness & Bias,Fairness-Aware Estimation of Graphical Models,"This paper examines the issue of fairness in the estimation of graphical models (GMs), particularly Gaussian, Covariance, and Ising models. These models play a vital role in understanding complex relationships in high-dimensional data. However, standard GMs can result in biased outcomes, especially when the underlying data involves sensitive characteristics or protected groups. To address this, we introduce a comprehensive framework designed to reduce bias in the estimation of GMs related to protected attributes. Our approach involves the integration of the pairwise graph disparity error and a tailored loss function into a nonsmooth multi-objective optimization problem, striving to achieve fairness across different sensitive groups while maintaining the effectiveness of the GMs. Experimental evaluations on synthetic and real-world datasets demonstrate that our framework effectively mitigates bias without undermining GMs' performance.","['Fairness', 'Graphical Model', 'Optimization']",[],"['Zhuoping Zhou', 'Davoud Ataee Tarzanagh', 'Bojian Hou', 'Qi Long', 'Li Shen']","['University of Pennsylvania, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'Biostatistics, Epidemiology and Informatics, University of Pennsylvania']",
https://openreview.net/forum?id=X2G7LA7Av9,Security,Can Simple Averaging Defeat Modern Watermarks?,"Digital watermarking techniques are crucial for copyright protection and source identification of images, especially in the era of generative AI models. However, many existing watermarking methods, particularly content-agnostic approaches that embed fixed patterns regardless of image content, are vulnerable to steganalysis attacks that can extract and remove the watermark with minimal perceptual distortion. In this work, we categorise watermarking algorithms into content-adaptive and content-agnostic ones, and demonstrate how averaging a collection of watermarked images could reveal the underlying watermark pattern. We then leverage this extracted pattern for effective watermark removal under both greybox and blackbox settings, even when the collection of images contains multiple watermark patterns. For some algorithms like Tree-Ring watermarks, the extracted pattern can also forge convincing watermarks on clean images. Our quantitative and qualitative evaluations across twelve watermarking methods highlight the threat posed by steganalysis to content-agnostic watermarks and the importance of designing watermarking techniques resilient to such analytical attacks. We propose security guidelines calling for using content-adaptive watermarking strategies and performing security evaluation against steganalysis. We also suggest multi-key assignments as potential mitigations against steganalysis vulnerabilities. Github page: \url{https://github.com/showlab/watermark-steganalysis}.","['Watermark', 'Security']",[],"['Pei Yang', 'Hai Ci', 'Yiren Song', 'Mike Zheng Shou']","['School of Computing, national university of singaore, National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'National University of Singapore']",
https://openreview.net/forum?id=X64IJvdftR,Security,Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing,"Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing interval bound propagation and Lipschitz-bounds not only offer conservative certification bounds but also are restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. The implementation will be publicly available upon the acceptance of this work. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing.",['trustworthy machine learning; certified robustness; randomized smoothing'],[],"['Weizhi Gao', 'Zhichao Hou', 'Han Xu', 'Xiaorui Liu']","['Computer Science Department, North Carolina State University', 'Computer Science, North Carolina State University', 'University of Arizona', 'Computer Science, North Carolina State University']",
https://openreview.net/forum?id=XHCYZNmqnv,Security,Detecting Brittle Decisions for Free: Leveraging Margin Consistency in Deep Robust Classifiers,"Despite extensive research on adversarial training strategies to improve robustness, the decisions of even the most robust deep learning models can still be quite sensitive to imperceptible perturbations, creating serious risks when deploying them for high-stakes real-world applications. While detecting such cases may be critical, evaluating a model's vulnerability at a per-instance level using adversarial attacks is computationally too intensive and unsuitable for real-time deployment scenarios. The input space margin is the exact score to detect non-robust samples and is intractable for deep neural networks. This paper introduces the concept of margin consistency -- a property that links the input space margins and the logit margins in robust models -- for efficient detection of vulnerable samples. First, we establish that margin consistency is a necessary and sufficient condition to use a model's logit margin as a score for identifying non-robust samples. Next, through comprehensive empirical analysis of various robustly trained models on CIFAR10 and CIFAR100 datasets, we show that they indicate high margin consistency with a strong correlation between their input space margins and the logit margins. Then, we show that we can effectively use the logit margin to confidently detect brittle decisions with such models. Finally, we address cases where the model is not sufficiently margin-consistent by learning a pseudo-margin from the feature representation. Our findings highlight the potential of leveraging deep representations to efficiently assess adversarial vulnerability in deployment scenarios.","['adversarial robustness', 'empirical robustness estimation', 'classification', 'vulnerability detection']",[],"['Jonas Ngnawe', 'Sabyasachi Sahoo', 'Yann Batiste Pequignot', 'Frederic Precioso', 'Christian Gagné']","['Université Laval', 'Université Laval', 'Universite Laval, Université Laval', ""Universite Cote d'Azur"", 'Electrucal Engineering and Computer Engineering, Université Laval']",
https://openreview.net/forum?id=XNGsx3WCU9,Fairness & Bias,Visual Data Diagnosis and Debiasing with Concept Graphs,"The widespread success of deep learning models today is owed to the curation of extensive datasets significant in size and complexity. However, such models frequently pick up inherent biases in the data during the training process, leading to unreliable predictions. Diagnosing and debiasing datasets is thus a necessity to ensure reliable model performance. In this paper, we present ConBias, a novel framework for diagnosing and mitigating Concept co-occurrence Biases in visual datasets. ConBias represents visual datasets as knowledge graphs of concepts, enabling meticulous analysis of spurious concept co-occurrences to uncover concept imbalances across the whole dataset. Moreover, we show that by employing a novel clique-based concept balancing strategy, we can mitigate these imbalances, leading to enhanced performance on downstream tasks. Extensive experiments show that data augmentation based on a balanced concept distribution augmented by ConBias improves generalization performance across multiple datasets compared to state-of-the-art methods.","['responsible ai', 'bias', 'generative ai', 'fairness']",[],"['Rwiddhi Chakraborty', 'Yinong Oliver Wang', 'Jialu Gao', 'Runkai Zheng', 'Cheng Zhang', 'Fernando De la Torre']","['University of Tromsø', 'Robotics Institute, Carnegie Mellon University', 'Robotics Institute, CMU, Carnegie Mellon University', 'Robotics Institute, CMU, Carnegie Mellon University', 'Texas A&M University - College Station', 'Robotics Institute, School of Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=XOVks7JHQA,Fairness & Bias,Linear Uncertainty Quantification of Graphical Model Inference,"Uncertainty Quantification (UQ) is vital for decision makers as it offers insights into the potential reliability of data and model, enabling more informed and risk-aware decision-making.  Graphical models, capable of representing data with complex dependencies, are widely used across domains. Existing sampling-based UQ methods are unbiased but cannot guarantee convergence and are time-consuming on large-scale graphs.  There are fast UQ methods for graphical models with closed-form solutions and convergence guarantee but with uncertainty underestimation. We propose *LinUProp*, a UQ method that utilizes a novel linear propagation of uncertainty to model uncertainty among related nodes additively instead of multiplicatively, to offer linear scalability, guaranteed convergence, and closed-form solutions without underestimating uncertainty. Theoretically, we decompose the expected prediction error of the graphical model and prove that the uncertainty computed by *LinUProp* is the *generalized variance component* of the decomposition. Experimentally, we demonstrate that *LinUProp* is consistent with the sampling-based method but with linear scalability and fast convergence. Moreover, *LinUProp* outperforms competitors in uncertainty-based active learning on four real-world graph datasets, achieving higher accuracy with a lower labeling budget.","['graphical models', 'belief propagation', 'uncertainty quantification']",[],"['Chenghua Guo', 'Han Yu', 'Jiaxin Liu', 'Chao Chen', 'Qi Li', 'Sihong Xie', 'Xi Zhang']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Computer Science, Lehigh University', '', 'Iowa State University', 'HKUST-GZ', 'Beijing University of Posts and Telecommunications']",
https://openreview.net/forum?id=XUL75cvHL5,Fairness & Bias,The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize,"In this work, we investigate stochastic approximation (SA) with Markovian data and nonlinear updates under constant stepsize $\alpha>0$. Existing work has primarily focused on either i.i.d. data or linear update rules. We take a new perspective and carefully examine the simultaneous presence of Markovian dependency of data and nonlinear update rules, delineating how the interplay between these two structures leads to complications that are not captured by prior techniques. By leveraging the smoothness and recurrence properties of the SA updates, we develop a fine-grained analysis of the correlation between the SA iterates $\theta_k$ and Markovian data $x_k$. This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process $(x_k, \theta_k)$. Furthermore, we present a precise characterization of the asymptotic bias of the SA iterates, given by $\mathbb{E}[\theta_\infty]-\theta^\ast=\alpha(b_\textup{m}+b_\textup{n}+b_\textup{c})+\mathcal{O}(\alpha^{3/2})$. Here, $b_\textup{m}$ is associated with the Markovian noise, $b_\textup{n}$ is tied to the nonlinearity of the SA operator, and notably, $b_\textup{c}$ represents a multiplicative interaction between the Markovian noise and the nonlinearity of the operator, which is absent in previous works. As a by-product of our analysis, we derive finite-time bounds on higher moment $\mathbb{E}[||\theta_k-\theta^\ast||^{2p}]$ and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem.","['stochastic approximation', 'nonlinearity', 'Markov chain', 'weak convergence', 'Wasserstein metric', 'asymptotic bias', 'Richardson-Romberg extrapolation', 'Polyak-Ruppert averaging']",[],"['Dongyan Lucy Huo', 'Yixuan Zhang', 'Yudong Chen', 'Qiaomin Xie']","['Cornell University', 'University of Wisconsin - Madison', 'Department of Computer Sciences, University of Wisconsin - Madison', 'University of Wisconsin - Madison']",
https://openreview.net/forum?id=Y6RV6z98Pk,Security,SampDetox: Black-box Backdoor Defense via Perturbation-based Sample Detoxification,"The advancement of Machine Learning has enabled the widespread deployment of Machine Learning as a Service (MLaaS) applications. However, the untrustworthy nature of third-party ML services poses backdoor threats. Existing defenses in MLaaS are limited by their reliance on training samples or white-box model analysis, highlighting the need for a black-box backdoor purification method. In our paper, we attempt to use diffusion models for purification by introducing noise in a forward diffusion process to destroy backdoors and recover clean samples through a reverse generative process. However, since a higher noise also destroys the semantics of the original samples, it still results in a low restoration performance. To investigate the effectiveness of noise in eliminating different types of backdoors, we conducted a preliminary study, which demonstrates that backdoors with low visibility can be easily destroyed by lightweight noise and those with high visibility need to be destroyed by high noise but can be easily detected. Based on the study, we propose SampDetox, which strategically combines lightweight and high noise. SampDetox applies weak noise to eliminate low-visibility backdoors and compares the structural similarity between the recovered and original samples to localize high-visibility backdoors. Intensive noise is then applied to these localized areas, destroying the high-visibility backdoors while preserving global semantic information. As a result, detoxified samples can be used for inference, even by poisoned models. Comprehensive experiments demonstrate the effectiveness of SampDetox in defending against various state-of-the-art backdoor attacks.","['Backdoor defense', 'black-box']",[],"['Yanxin Yang', 'Chentao Jia', 'DengKe Yan', 'Ming Hu', 'Tianlin Li', 'Xiaofei Xie', 'Xian Wei', 'Mingsong Chen']","['Software Engineering, East China Normal University', 'Software Engineering, East China Normal University', 'East China Normal University', 'SCIS, Singapore Management University', 'Nanyang Technological University', 'School of Computing and Information Systems, Singapore Management University', 'SEI, East China Normal University', 'Software Engineering Institute, East China Normal University']",
https://openreview.net/forum?id=YNRYWZHmKY,Fairness & Bias,A Cat Is A Cat (Not A Dog!): Unraveling Information Mix-ups in Text-to-Image Encoders through Causal Analysis and Embedding Optimization,"This paper analyzes the impact of causal manner in the text encoder of text-to-image (T2I) diffusion models, which can lead to information bias and loss. Previous works have focused on addressing the issues through the denoising process. However, there is no research discussing how text embedding contributes to T2I models, especially when generating more than one object. In this paper, we share a comprehensive analysis of text embedding: i) how text embedding contributes to the generated images and ii) why information gets lost and biases towards the first-mentioned object. Accordingly, we propose a simple but effective text embedding balance optimization method, which is training-free, with an improvement of 125.42\% on information balance in stable diffusion. Furthermore, we propose a new automatic evaluation metric that quantifies information loss more accurately than existing methods, achieving 81\% concordance with human assessments. This metric effectively measures the presence and accuracy of objects, addressing the limitations of current distribution scores like CLIP's text-image similarities.","['Causal manner', 'Embedding optimization', 'Information mix-ups', 'Information loss', 'Text-to-image generative model']",[],"['Chieh-Yun Chen', 'Chiang Tseng', 'Li-Wu Tsao', 'Hong-Han Shuai']","['Georgia Institute of Technology', 'Institute of Electrical and Computer Engineering, National Yang Ming Chiao Tung University', 'National Yang Ming Chiao Tung University', 'ECE, National Yang Ming Chiao Tung University']",
https://openreview.net/forum?id=YNx7ai4zTs,Privacy & Data Governance,Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models,"Machine unlearning (MU) empowers individuals with the `right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii)  Joint training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.","['multimodal', 'machine unlearning']",[],"['Jiaqi Li', 'Qianshan Wei', 'Chuanyi Zhang', 'Guilin Qi', 'Miaozeng Du', 'Yongrui Chen', 'Sheng Bi', 'Fan Liu']","['Cyber Science and Engineering, Southeast University', 'School of Calculating Apparatus and Technology, Southeast University', 'College of Artificial Intelligence and Automation, Hohai University', 'Southeast University', 'Southeast University', 'Southeast University', 'School of Law, Southeast University', 'Hohai University']",
https://openreview.net/forum?id=YwpL0BVxts,Security,"United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories","In recent years, deep neural networks (DNNs) have witnessed extensive applications, and protecting their intellectual property (IP) is thus crucial. As a non-invasive way for model IP protection, model fingerprinting has become popular. However, existing single-point based fingerprinting methods are highly sensitive to the changes in the decision boundary, and may suffer from the misjudgment of the resemblance of sparse fingerprinting, yielding high false positives of innocent models. In this paper, we propose ADV-TRA, a more robust fingerprinting scheme that utilizes adversarial trajectories to verify the ownership of DNN models. Benefited from the intrinsic progressively adversarial level, the trajectory is capable of tolerating greater degree of alteration in decision boundaries. We further design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes. Such a design enables a more unique and reliable fingerprinting with relatively low querying costs. Experiments on three datasets against four types of removal attacks show that ADV-TRA exhibits superior performance in distinguishing between infringing and innocent models, outperforming the state-of-the-art comparisons.","['Deep neural network', 'intellectual property protection', 'model fingerprinting', 'adversarial sample.']",[],"['Tianlong Xu', 'Chen Wang', 'Gaoyang Liu', 'Yang Yang', 'Kai Peng', 'Wei Liu']","['Huazhong University of Science and Technology', 'School of Electronic Information and Communications, Huazhong University of Science and Technology', 'School of Computing Science, Simon Fraser University', 'School of Artificial Intelligence, Hubei University', 'Huazhong University of Science and Technology', 'School of Eectronic Information and Communication Engineering, Huazhong University of Science and Technology']",
https://openreview.net/forum?id=YxyYTcv3hp,Fairness & Bias,Ferrari: Federated Feature Unlearning via Optimizing Feature Sensitivity,"The advent of Federated Learning (FL) highlights the practical necessity for the ’right to be forgotten’ for all clients, allowing them to request data deletion from the machine learning model’s service provider. This necessity has spurred a growing demand for Federated Unlearning (FU). Feature unlearning has gained considerable attention due to its applications in unlearning sensitive, backdoor, and biased features. Existing methods employ the influence function to achieve feature unlearning, which is impractical for FL as it necessitates the participation of other clients, if not all, in the unlearning process. Furthermore, current research lacks an evaluation of the effectiveness of feature unlearning. To address these limitations, we define feature sensitivity in evaluating feature unlearning according to Lipschitz continuity. This metric characterizes the model output’s rate of change or sensitivity to perturbations in the input feature. We then propose an effective federated feature unlearning framework called Ferrari, which minimizes feature sensitivity. Extensive experimental results and theoretical analysis demonstrate the effectiveness of Ferrari across various feature unlearning scenarios, including sensitive, backdoor, and biased features. The code is publicly available at https://github.com/OngWinKent/Federated-Feature-Unlearning","['Machine Unlearning', 'Federated Unlearning', 'Feature Unlearning', 'Lipschitz Continuity']",[],"['Hanlin Gu', 'Win Kent Ong', 'Chee Seng Chan', 'Lixin Fan']","['AI, webank', 'Department of Artificial Intelligence, Universiti Malaya', '', 'WeBank']",
https://openreview.net/forum?id=ZEVDMQ6Mu5,Privacy & Data Governance,Curvature Clues: Decoding Deep Learning Privacy with Input Loss Curvature,"In this paper, we explore the properties of loss curvature with respect to input data in deep neural networks. Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input. We investigate how input loss curvature varies between train and test sets, and its implications for train-test distinguishability. We develop a theoretical framework that derives an upper bound on the train-test distinguishability based on privacy and the size of the training set. This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature. We validate our theoretical findings through experiments in computer vision classification tasks, demonstrating that input loss curvature surpasses existing methods in membership inference effectiveness. Our analysis highlights how the performance of membership inference attack (MIA) methods varies with the size of the training set, showing that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet. These findings not only advance our understanding of deep neural network behavior but also improve the ability to test privacy-preserving techniques in machine learning.","['Input Loss Curvature', 'Differential Privacy', 'Membership Inference']",[],"['Deepak Ravikumar', 'Efstathia Soufleri', 'Kaushik Roy']","['', 'Athena Research and Innovation Centre', 'ECE, Purdue University']",
https://openreview.net/forum?id=ZGN8dOhpi6,Fairness & Bias,A Pairwise Pseudo-likelihood Approach for Matrix Completion with Informative Missingness,"While several recent matrix completion methods are developed to deal with non-uniform observation probabilities across matrix entries, very few allow the missingness to depend on the mostly unobserved matrix measurements, which is generally ill-posed. We aim to tackle a subclass of these ill-posed settings, characterized by a flexible separable observation probability assumption that can depend on the matrix measurements. We propose a regularized pairwise pseudo-likelihood approach for matrix completion and prove that the proposed estimator can asymptotically recover the low-rank parameter matrix up to an identifiable equivalence class of a constant shift and scaling, at a near-optimal asymptotic convergence rate of the standard well-posed (non-informative missing) setting, while effectively mitigating the impact of informative missingness. The efficacy of our method is validated via numerical experiments, positioning it as a robust tool for matrix completion to mitigate data bias.",['Identifiability; missing not at random; U-statistics'],[],"['Jiangyuan Li', 'Jiayi Wang', 'Raymond K. W. Wong', 'Kwun Chuen Gary Chan']","['Google', 'Department of Statistics, University of Texas at Dallas', '', 'Department of Biostatistics, University of Washington']",
https://openreview.net/forum?id=ZViYPzh9Wq,Transparency & Explainability,Reconstructing the Image Stitching Pipeline: Integrating Fusion and Rectangling into a Unified Inpainting Model,"Deep learning-based image stitching pipelines are typically divided into three cascading stages: registration, fusion, and rectangling. Each stage requires its own network training and is tightly coupled to the others, leading to error propagation and posing significant challenges to parameter tuning and system stability. This paper proposes the Simple and Robust Stitcher (SRStitcher), which  revolutionizes the image stitching pipeline by simplifying the fusion and rectangling stages into a unified inpainting model, requiring no model training or fine-tuning. We reformulate the problem definitions of the fusion and rectangling stages and demonstrate that they can be effectively integrated into an inpainting task. Furthermore, we design the weighted masks to guide the reverse process in a pre-trained large-scale diffusion model, implementing this integrated inpainting task in a single inference. Through extensive experimentation, we verify the interpretability and generalization capabilities of this unified model, demonstrating that SRStitcher outperforms state-of-the-art methods in both performance and stability.","['Image Stitching', 'Image Fusion', 'Image Rectangling', 'Diffusion Model']",[],"['Xieziqi', 'Weidong Zhao', 'XianhuiLiu', 'Jian Zhao', 'Ning Jia']","['College of Electronics and Information Engineering, Tongji University', 'CEIE, Tongji University', 'School of Electronic and Information Engineering, Tongji University', 'CS, Tongji University', 'college of  Electronic and Information Engineering, Tongji University']",
https://openreview.net/forum?id=ZtTWKr51yH,Security,Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data,"State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints. To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms. This new attack does not require parameter tuning and further degrades the accuracy, up to 81\% points compared to the previous gradient attacks. Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack.  We demonstrate the effectiveness of our attacks on five architectures and four critical use cases. Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1\% points and 21.9\% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA. Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.","['machine learning', 'security', 'adversarial attacks', 'tabular data', 'threat models', 'constrained machine learning']",[],"['Thibault Simonetto', 'Salah GHAMIZI', 'Maxime Cordy']","['University Of Luxembourg', 'Luxembourg Institute of Science and Technology', 'University of Luxemburg']",
https://openreview.net/forum?id=a1wf2N967T,Transparency & Explainability,Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models,"Disentangled representation learning (DRL) aims to identify and decompose underlying factors behind observations, thus facilitating data perception and generation. However, current DRL approaches often rely on the unrealistic assumption that semantic factors are statistically independent. In reality, these factors may exhibit correlations, which off-the-shelf solutions have yet to properly address. To tackle this challenge, we introduce a bidirectional weighted graph-based framework, to learn factorized attributes and their interrelations within complex data. Specifically, we propose a $\beta$-VAE based module to extract factors as the initial nodes of the graph, and leverage the multimodal large language model (MLLM) to discover and rank latent correlations, thereby updating the weighted edges. By integrating these complementary modules, our model successfully achieves fine-grained, practical and unsupervised disentanglement. Experiments demonstrate our method's superior performance in disentanglement and reconstruction. Furthermore, the model inherits enhanced interpretability and generalizability from MLLMs.","['Disentangled representation learning', 'Interpretable and explainable AI', 'Multimodal large language model', 'Computer Vision']",[],"['Baao Xie', 'Qiuyu Chen', 'Yunnan Wang', 'Zequn Zhang', 'Xin Jin', 'Wenjun Zeng']","['Computer Science, Ningbo Institute of Digital Twin', 'Computer Science, Shanghai Jiaotong University', 'Ant Research', 'School of Information Science and Technology, University of Science and Technology of China', 'Eastern Institute of Technology, Ningbo', 'Computer Science, Eastern Institute for Advanced Study']",
https://openreview.net/forum?id=aBpxukZS37,Transparency & Explainability,Diffusion PID: Interpreting Diffusion via Partial Information Decomposition,"Text-to-image diffusion models have made significant progress in generating naturalistic images from textual inputs, and demonstrate the capacity to learn and represent complex visual-semantic relationships. While these diffusion models have achieved remarkable success, the underlying mechanisms driving their performance are not yet fully accounted for, with many unanswered questions surrounding what they learn, how they represent visual-semantic relationships, and why they sometimes fail to generalize. Our work presents Diffusion Partial Information Decomposition (DiffusionPID), a novel technique that applies information-theoretic principles to decompose the input text prompt into its elementary components, enabling a detailed examination of how individual tokens and their interactions shape the generated image. We introduce a formal approach to analyze the uniqueness, redundancy, and synergy terms by applying PID to the denoising model at both the image and pixel level. This approach enables us to characterize how individual tokens and their interactions affect the model output. We first present a fine-grained analysis of characteristics utilized by the model to uniquely localize specific concepts, we then apply our approach in bias analysis and show it can recover gender and ethnicity biases. Finally, we use our method to visually characterize word ambiguity and similarity from the model’s perspective and illustrate the efficacy of our method for prompt intervention. Our results show that PID is a potent tool for evaluating and diagnosing text-to-image diffusion models. Link to project page: https://rbz-99.github.io/Diffusion-PID/.","['Diffusion', 'Interpretability', 'Information Decomposition', 'Mutual Information', 'Bias']",[],"['Shaurya Rajat Dewan', 'Rushikesh Zawar', 'Prakanshul Saxena', 'Yingshan Chang', 'Andrew Luo', 'Yonatan Bisk']","['Robotics Institute, School of Computer Science, CMU, Carnegie Mellon University', 'Robotics Institute, Carnegie Mellon University', 'Robotics Institute, CMU, Carnegie Mellon University', 'Language Technologies Institute, Carnegie Mellon University', 'University of Hong Kong', 'Meta']",
https://openreview.net/forum?id=aCcHVnwNlf,Privacy & Data Governance,On Differentially Private Subspace Estimation in a Distribution-Free Setting,"Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying for the high ambient dimension.  On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that has a polynomial dependency on the dimension. However, their bounds do not rule out the possibility to reduce the number of points for ""easy"" instances. Yet, providing a measure that captures how much a given dataset is ""easy"" for this task turns out to be challenging, and was not properly addressed in prior works.  Inspired by the work of Singhal and Steinke (NeurIPS 2021), we provide the first measures that quantify ""easiness"" as a function of multiplicative singular-value gaps in the input dataset, and support them with new upper and lower bounds. In particular, our results determine the first types of gaps that are sufficient and necessary for estimating a subspace with an amount of points that is independent of the dimension. Furthermore, we realize our upper bounds using a practical algorithm and demonstrate its advantage in high-dimensional regimes compared to prior approaches.","['Differential Privacy', 'Private Subspace Estimation', 'Private PCA', 'Fingerprinting Codes']",[],['Eliad Tsfadia'],['Georgetown University'],
https://openreview.net/forum?id=aLzA7MSc6Y,Fairness & Bias,Symmetric Linear Bandits with Hidden Symmetry,"High-dimensional linear bandits with low-dimensional structure have received considerable attention in recent studies due to their practical significance. The most common structure in the literature is sparsity. However, it may not be available in practice. Symmetry, where the reward is invariant under certain groups of transformations on the set of arms, is another important inductive bias in the high-dimensional case that covers many standard structures, including sparsity. In this work, we study high-dimensional symmetric linear bandits where the symmetry is hidden from the learner, and the correct symmetry needs to be learned in an online setting. We examine the structure of a collection of hidden symmetry and provide a method based on model selection within the collection of low-dimensional subspaces. Our algorithm achieves a regret bound of $ O(d_0^{2/3} T^{2/3} \log(d))$, where $d$ is the ambient dimension which is potentially very large, and $d_0$ is the dimension of the true low-dimensional subspace such that $d_0 \ll d$. With an extra assumption on well-separated models, we can further improve the regret to $ O(d_0 \sqrt{T\log(d)} )$.","['Bandit theory', 'group theory', 'symmetry', 'sparsity.']",[],"['Nam Phuong Tran', 'The-Anh Ta', 'Debmalya Mandal', 'Long Tran-Thanh']","['Department of Computer Science, University of Warwick', 'Data61, CSIRO', 'University of Warwick', 'The university of Warwick']",
https://openreview.net/forum?id=aTNT3FuVBG,Fairness & Bias,SureMap: Simultaneous mean estimation for single-task and multi-task disaggregated evaluation,"Disaggregated evaluation&mdash;estimation of performance of a machine learning model on different subpopulations&mdash;is a core task when assessing performance and group-fairness of AI systems. A key challenge is that evaluation data is scarce, and subpopulations arising from intersections of attributes (e.g., race, sex, age) are often tiny. Today, it is common for multiple clients to procure the same AI model from a model developer, and the task of disaggregated evaluation is faced by each customer individually.  This gives rise to what we call the *multi-task disaggregated evaluation problem*, wherein multiple clients seek to conduct a disaggregated evaluation of a given model in their own data setting (task).  In this work we develop a disaggregated evaluation method called **SureMap** that has high estimation accuracy for both multi-task *and* single-task disaggregated evaluations of blackbox models.  SureMap's efficiency gains come from (1) transforming the problem into structured simultaneous Gaussian mean estimation and (2) incorporating external data, e.g., from the AI system creator or from their other clients.  Our method combines *maximum a posteriori* (MAP) estimation using a well-chosen prior together with cross-validation-free tuning via Stein's unbiased risk estimate (SURE). We evaluate SureMap on disaggregated evaluation tasks in multiple domains, observing significant accuracy improvements over several strong competitors.","['fairness', 'evaluation', 'multi-task', 'transfer']",[],"['Mikhail Khodak', 'Lester Mackey', 'Alexandra Chouldechova', 'Miroslav Dudík']","['', 'Microsoft Research New England', 'Microsoft', 'Microsoft']",
https://openreview.net/forum?id=aSkckaNxnO,Fairness & Bias,Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control,"As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality. In this work, we address this challenge through Sparse Activation Control. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint modules that are closely related to specific tasks within the model, i.e. attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factualness, and bias concurrently.",['large language models'],[],"['Yuxin Xiao', 'Chaoqun Wan', 'Yonggang Zhang', 'Wenxiao Wang', 'Binbin Lin', 'Xiaofei He', 'Xu Shen', 'Jieping Ye']","['Computer Science, Zhejiang University', 'Alibaba Group', 'Hong Kong Baptist University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Alibaba Group', 'Alibaba Group']",
https://openreview.net/forum?id=asYYSzL4N5,Security,BAN: Detecting Backdoors Activated by Adversarial Neuron Noise,"Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community.  Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios. State-of-the-art backdoor inversion recovers a mask in the feature space to locate prominent backdoor features, where benign and backdoor features can be disentangled.  However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features. To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information. In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models. Experimental results demonstrate our defense, BAN, is 1.37$\times$ (on CIFAR-10) and 5.11$\times$ (on ImageNet200) more efficient with an average 9.99\% higher detect success rate than the state-of-the-art defense BTI DBF. Our code and trained models are publicly available at https://github.com/xiaoyunxxy/ban.","['backdoor detection', 'backdoor trigger inversion', 'AI Security']",[],"['Xiaoyun Xu', 'Zhuoran Liu', 'Stefanos Koffas', 'Shujian Yu', 'Stjepan Picek']","['Radboud University', 'Radboud University', 'Cyber-security, Delft University of Technology', 'Vrije Universiteit Amsterdam', 'Radboud University Nijmegen']",
https://openreview.net/forum?id=aon7bwYBiq,Privacy & Data Governance,Differentially Private Graph Diffusion with Applications in Personalized PageRanks,"Graph diffusion, which iteratively propagates real-valued substances among the graph, is used in numerous graph/network-involved applications. However, releasing diffusion vectors may reveal sensitive linking information in the data such as transaction information in financial network data. However, protecting the privacy of graph data is challenging due to its interconnected nature.     This work proposes a novel graph diffusion framework with edge-level different privacy guarantees by using noisy diffusion iterates.     The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes. Our privacy loss analysis is based on Privacy Amplification by Iteration (PABI), which to our best knowledge, is the first effort that analyzes PABI with Laplace noise and provides relevant applications.     We also introduce a novel $\infty$-Wasserstein distance tracking method, which tightens the analysis of privacy leakage and makes PABI more applicable in practice.      We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks. Experiments on real-world network data demonstrate the superiority of our method under stringent privacy conditions.","['differential privacy', 'graph diffusion', 'personalize pagerank']",[],"['Rongzhe Wei', 'Eli Chien', 'Pan Li']","['ECE, Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=bCqIx5Q8qX,Security,MALT Powers Up Adversarial Attacks,"Current adversarial attacks for multi-class classifiers choose potential adversarial target classes naively based on the classifier's confidence levels. We present a novel adversarial targeting method, \textit{MALT - Mesoscopic Almost Linearity Targeting}, based on local almost linearity assumptions. Our attack wins over the current state of the art AutoAttack on the standard benchmark datasets CIFAR-100 and Imagenet and for different robust models. In particular, our attack uses a \emph{five times faster} attack strategy than AutoAttack's while successfully matching AutoAttack's successes and attacking additional samples that were previously out of reach. We additionally prove formally and demonstrate empirically that our targeting method, although inspired by linear predictors, also applies to non-linear models.","['Adversarial Examples', 'Robustness', 'Neural Networks', 'Classification', 'Adversarial Attacks']",[],"['Odelia Melamed', 'Gilad Yehudai', 'Adi Shamir']","['Weizmann Institute, Technion', 'New York University', 'WeiThe Weizmann Institute of Sciencezmann Institute, Technion']",
https://openreview.net/forum?id=bnZZedw9CM,Fairness & Bias,Decoupled Kullback-Leibler Divergence Loss,"In this paper, we delve deeper into the Kullback–Leibler (KL) Divergence loss and mathematically prove that it is equivalent to the Decoupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error ($\mathbf{w}$MSE) loss and 2) a Cross-Entropy loss incorporating soft labels.  Thanks to the decomposed formulation of DKL loss, we have identified two areas for improvement.  Firstly, we address the limitation of KL/DKL in scenarios like knowledge distillation by breaking its asymmetric optimization property. This modification ensures that the $\mathbf{w}$MSE component is always effective during training, providing extra constructive cues. Secondly, we introduce class-wise global information into KL/DKL to mitigate bias from individual samples. With these two enhancements, we derive the Improved Kullback–Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training, and knowledge distillation tasks. The proposed approach achieves new state-of-the-art adversarial robustness on the public leaderboard --- \textit{RobustBench} and competitive performance on knowledge distillation, demonstrating the substantial practical merits. Our code is available at https://github.com/jiequancui/DKL.",['Adversarial Training; Knowledge Distillation; Kullback-Leibler Divergence Loss; Long-tail Recognition'],[],"['Jiequan Cui', 'Zhuotao Tian', 'Zhisheng Zhong', 'XIAOJUAN QI', 'Bei Yu', 'Hanwang Zhang']","['CCDS, Nanyang Technological University', 'Harbin Institute of Technology (Shenzhen)', 'The Chinese University of Hong Kong', 'Department of Eletrical and Electronic Engineering, University of Hong Kong', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'computer science, Nanyang Technological University']",
https://openreview.net/forum?id=bzPmjmiaz8,Fairness & Bias,Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing,"Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with \emph{all} (factual) statements in persona documents. Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation. This paper presents a pioneering exploration to quantify PRP faithfulness evaluation as a fine-grained and explainable criterion, which also serves as a reliable reference for faithfulness optimization. Our criterion first discriminates persona statements into \emph{active} and \emph{passive} constraints by identifying the query-statement relevance. Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active constraints and (b) not contradicted by passive constraints. We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of statement-to-response natural language inference (NLI) scores weighted by constraint-query relevance scores.  In practice, we build the APC scoring system by symbolically distilling small NLI and relevance discriminators (300M parameters) from GPT-4 for efficiency, and both show high consistency with GPT-4's discrimination. We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation. As the APC score could faithfully reflect the PRP quality, we further leverage it as a reward system in direct preference optimization (DPO) for better AI characters.  Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations. We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques. We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion.  Finally, we provide comprehensive analyses and case studies to support the effectiveness of APC and APC-based DPO.","['Persona-driven Role-playing', 'Global Faithfulness Control', 'Evaluation Metric', 'Direct Preference Optimization']",[],"['Letian Peng', 'Jingbo Shang']","['University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=c37x7CXZ2Y,Fairness & Bias,Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data,"Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics.  Since the treatments of interest often cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding. One strategy to overcome these limitations is to leverage instrumental variables (IVs) as latent quasi-experiments, such as randomized intent-to-treat assignments or randomized product recommendations. This approach, on the other hand, can suffer from low compliance, i.e., IV weakness. Some subgroups may even exhibit zero compliance, meaning we cannot instrument for their CATEs at all. In this paper, we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups. We propose a two-stage framework that first learns \textit{biased} CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates. We characterize the convergence rates of our method and validate its effectiveness through a simulation study. Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.","['Causal inference', 'heterogeneous treatment effects', 'weak instrumental variables', 'unobserved confounding', 'data combination']",[],"['Miruna Oprescu', 'Nathan Kallus']","['Computer Science, Cornell University', 'Machine Learning & Inference Research, Netflix']",
https://openreview.net/forum?id=cCDMXXiamP,Transparency & Explainability,Nearly Tight Black-Box Auditing of Differentially Private Machine Learning,"This paper presents an auditing procedure for the Differentially Private Stochastic Gradient Descent (DP-SGD) algorithm in the black-box threat model that is substantially tighter than prior work. The main intuition is to craft worst-case initial model parameters, as DP-SGD's privacy analysis is agnostic to the choice of the initial model parameters. For models trained on MNIST and CIFAR-10 at theoretical $\varepsilon=10.0$, our auditing procedure yields empirical estimates of $\varepsilon_{emp} = 7.21$ and $6.95$, respectively, on a 1,000-record sample and $\varepsilon_{emp} = 6.48$ and $4.96$ on the full datasets. By contrast, previous audits were only (relatively) tight in stronger white-box models, where the adversary can access the model's inner parameters and insert arbitrary gradients. Overall, our auditing procedure can offer valuable insight into how the privacy analysis of DP-SGD could be improved and detect bugs and DP violations in real-world implementations. The source code needed to reproduce our experiments is available from https://github.com/spalabucr/bb-audit-dpsgd.","['Privacy Auditing', 'Differential Privacy', 'DP-SGD']",[],"['Meenatchi Sundaram Muthu Selva Annamalai', 'Emiliano De Cristofaro']","['Computer Science and Engineering, University College London, University of London', 'University of California, Riverside']",
https://openreview.net/forum?id=kS9dciADtY,Fairness & Bias,Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection,"Zero-Shot Temporal Action Detection (ZSTAD) aims to classify and localize action segments in untrimmed videos for unseen action categories. Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals. In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process. Our simple approach results in superior performance compared to previous methods. Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts. To address this issue, we propose Text-infused attention and Foreground-aware Action Detection (Ti-FAD), which enhances the ability to focus on text-related sub-actions and distinguish relevant action segments from the background. Our extensive experiments demonstrate that Ti-FAD outperforms the state-of-the-art methods on ZSTAD benchmarks by a large margin:  41.2\% (+ 11.0\%) on THUMOS14 and 32.0\% (+ 5.4\%) on ActivityNet v1.3. Code is available at: https://github.com/YearangLee/Ti-FAD.","['Temporal Action Detection', 'Vision and Language', 'Zero-Shot Learning']",[],"['Yearang Lee', 'Ho-Joong Kim', 'Seong-Whan Lee']","['Department of Artificial Intelligence, Korea University', 'Department of Artificial Intelligence, Korea University', 'Korea University']",
https://openreview.net/forum?id=cPzjN7KABv,Privacy & Data Governance,Private Geometric Median,"In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_1,\dots,x_n$ in $\mathbb{R}^d$, the goal is to find a point $\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\sum_{i=1}^{n} \lVert|\theta - x_i\rVert_2$. Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$, and the excess risk of the algorithm depends linearly on $R$. In this paper, we ask: can we design an efficient and private algorithm with an excess error guarantee that scales with the (unknown) radius containing the majority of the datapoints? Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints. Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP. We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity.","['Differential Privacy', 'Differentially Private Convex Optimization', 'Geometric Median']",[],"['Mahdi Haghifam', 'Thomas Steinke', 'Jonathan Ullman']","['Northeastern University', 'Google DeepMind', 'Computer Science, University of Michigan - Ann Arbor']",
https://openreview.net/forum?id=cRLFvSOrzt,Privacy & Data Governance,Credit Attribution and Stable Compression,"Credit attribution is crucial across various fields. In academic research, proper citation acknowledges prior work and establishes original contributions. Similarly, in generative models, such as those trained on existing artworks or music, it is important to ensure that any generated content influenced by these works appropriately credits the original creators.  We study credit attribution by machine learning algorithms. We propose new definitions--relaxations of Differential Privacy--that weaken the stability guarantees for a designated subset of $k$ datapoints. These $k$ datapoints can be used non-stably with permission from their owners, potentially in exchange for compensation. Meanwhile, the remaining datapoints are guaranteed to have no significant influence on the algorithm's output.  Our framework extends well-studied notions of stability, including Differential Privacy ($k = 0$), differentially private learning with public data (where the $k$ public datapoints are fixed in advance), and stable sample compression (where the $k$ datapoints are selected adaptively by the algorithm). We examine the expressive power of these stability notions within the PAC learning framework, provide a comprehensive characterization of learnability for algorithms adhering to these principles, and propose directions and questions for future research.","['Credit Attribution', 'Algorithmic Stability', 'Stable Sample Compression']",[],"['Roi Livni', 'Shay Moran', 'Kobbi Nissim', 'Chirag Pabbaraju']","['Electrical Engineering, Tel Aviv University', 'Technion, Technion', 'Computer Science, Georgetown University', 'Computer Science, Stanford University']",
https://openreview.net/forum?id=cOw65A9FGf,Security,Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models,"Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR). This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model’s robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a 9.58% enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. Our code is available at https://github.com/zhyblue424/TGA-ZSR.","['adversarial attack', 'zero-shot robustness', 'pre-trained vision-language models']",[],"['Lu Yu', 'Haiyang Zhang', 'Changsheng Xu']","['Computer Science and Engineering, Tianjin University of Technology', 'Tianjin University of Technology', 'NLPR, Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",
https://openreview.net/forum?id=cYZibc2gKf,Fairness & Bias,Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation,"Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for *off-policy evaluation* (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators -- which include existing OPE methods as special cases -- that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call *abstract reward processes* (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.","['Off-Policy Evaluation', 'State Abstraction', 'Importance Sampling']",[],"['Shreyas Chaudhari', 'Ameet Deshpande', 'Bruno Castro da Silva', 'Philip S. Thomas']","['University of Massachusetts at Amherst', 'Princeton University', 'University of Massachusetts, Amherst', 'College of Information and Computer Sciences, College of Information and Computer Science, University of Massachusetts, Amherst']",
https://openreview.net/forum?id=cV4fcjcwmz,Security,Extracting Training Data from Molecular Pre-trained Models,"Graph Neural Networks (GNNs) have significantly advanced the field of drug discovery, enhancing the speed and efficiency of molecular identification. However, training these GNNs demands vast amounts of molecular data, which has spurred the emergence of collaborative model-sharing initiatives. These initiatives facilitate the sharing of molecular pre-trained models among organizations without exposing proprietary training data. Despite the benefits, these molecular pre-trained models may still pose privacy risks. For example, malicious adversaries could perform data extraction attack to recover private training data, thereby threatening commercial secrets and collaborative trust. This work, for the first time, explores the risks of extracting private training molecular data from molecular pre-trained models. This task is nontrivial as the molecular pre-trained models are non-generative and exhibit a diversity of model architectures, which differs significantly from language and image models. To address these issues, we introduce a molecule generation approach and propose a novel, model-independent scoring function for selecting promising molecules. To efficiently reduce the search space of potential molecules, we further introduce a Molecule Extraction Policy Network for molecule extraction. Our experiments demonstrate that even with only query access to molecular pre-trained models, there is a considerable risk of extracting training data, challenging the assumption that model sharing alone provides adequate protection against data extraction attacks. Our codes are publicly available at: \url{https://github.com/renH2/Molextract}.","['Graph Data Extraction Attack', 'Graph Pre-training']",[],"['Renhong Huang', 'Jiarong Xu', 'Zhiming Yang', 'Xiang Si', 'Xin Jiang', 'Hanyang Yuan', 'Chunping Wang', 'Yang Yang']","['Zhejiang University', 'School of Management, Fudan University', 'Fudan University', 'Fudan University', 'Cornell University', 'Zhejiang University', 'Finvolution Group', 'CS&T, Zhejiang University']",
https://openreview.net/forum?id=zLBlin2zvW,Fairness & Bias,Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders,"Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.","['Mechanistic Interpretability', 'Sparse Autoencoders', 'Science of Deep Learning']",[],"['Senthooran Rajamanoharan', 'Arthur Conmy', 'Lewis Smith', 'Tom Lieberum', 'Vikrant Varma', 'Janos Kramar', 'Rohin Shah', 'Neel Nanda']","['Google DeepMind', 'Google DeepMind', 'Google', 'Google', 'Google DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind']",
https://openreview.net/forum?id=cesWi7mMLY,Fairness & Bias,Long-Tailed Out-of-Distribution Detection via Normalized Outlier Distribution Adaptation,"One key challenge in Out-of-Distribution (OOD) detection is the absence of ground-truth OOD samples during training. One principled approach to address this issue is to use samples from external datasets as outliers ($\textit{i.e.}$, pseudo OOD samples) to train OOD detectors.   However, we find empirically that the outlier samples often present a distribution shift compared to the true OOD samples, especially in Long-Tailed Recognition (LTR) scenarios, where ID classes are heavily imbalanced, $\textit{i.e.}$, the true OOD samples exhibit very different probability distribution to the head and tailed ID classes from the outliers.   In this work, we propose a novel approach, namely $\textit{normalized outlier distribution adaptation}$ (AdaptOD), to tackle this distribution shift problem.   One of its key components is $\textit{dynamic outlier distribution adaptation}$ that effectively adapts a vanilla outlier distribution based on the outlier samples to the true OOD distribution by utilizing the OOD knowledge in the predicted OOD samples during inference.   Further, to obtain a more reliable set of predicted OOD samples on long-tailed ID data, a novel $\textit{dual-normalized energy loss}$ is introduced in AdaptOD, which leverages class- and sample-wise normalized energy to enforce a more balanced prediction energy on imbalanced ID samples. This helps avoid bias toward the head samples and learn a substantially better vanilla outlier distribution than existing energy losses during training. It also eliminates the need of manually tuning the sensitive margin hyperparameters in energy losses.   Empirical results on three popular benchmarks for OOD detection in LTR show the superior performance of AdaptOD over state-of-the-art methods. Code is available at https://github.com/mala-lab/AdaptOD.",['out-of-distribution detection'],[],"['Wenjun Miao', 'Guansong Pang', 'Jin Zheng', 'Xiao Bai']","['Beihang University', 'Singapore Management University', 'School of Computer Science and Engineering, BeiHang University', 'Beijing University of Aeronautics and Astronautics']",
https://openreview.net/forum?id=cw5mgd71jW,Security,Many-shot Jailbreaking,"We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This attack is newly feasible with the larger context windows recently deployed by language model providers like Google DeepMind, OpenAI and Anthropic. We find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. We demonstrate the success of this attack on the most widely used state-of-the-art closed-weight models, and across various tasks. Our results suggest very long contexts present a rich new attack surface for LLMs.","['large language models', 'long context', 'robustness', 'jailbreaks', 'in-context learning']",[],"['Cem Anil', 'Esin DURMUS', 'Nina Rimsky', 'Mrinank Sharma', 'Joe Benton', 'Sandipan Kundu', 'Joshua Batson', 'Meg Tong', 'Jesse Mu', 'Daniel J Ford', 'Francesco Mosconi', 'Rajashree Agrawal', 'Rylan Schaeffer', 'Naomi Bashkansky', 'Samuel Svenningsen', 'Mike Lambert', 'Ansh Radhakrishnan', 'Carson Denison', 'Evan J Hubinger', 'Yuntao Bai', 'Trenton Bricken', 'Timothy Maxwell', 'Nicholas Schiefer', 'James Sully', 'Alex Tamkin', 'Tamera Lanham', 'Karina Nguyen', 'Tomasz Korbak', 'Jared Kaplan', 'Deep Ganguli', 'Samuel R. Bowman', 'Ethan Perez', 'Roger Baker Grosse', 'David Duvenaud']","['Research, Anthropic', 'Anthropic', 'Anthropic', 'University of Oxford', 'Anthropic', '', '', 'Anthropic', 'Anthropic', 'Research, Anthropic PBC', 'Anthropic PBC', 'Reed College', 'Computer Science Department, Stanford University', 'Harvard University', 'Constellation', 'Research, Anthropic', 'Yale University', 'Anthropic', 'Machine Intelligence Research Institute', 'Finetuning, Anthropic']",
https://openreview.net/forum?id=cyv0LkIaoH,Fairness & Bias,Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences,"The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impacting the training of future generated models.     Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step.     However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users.     In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an implicit preference optimization mechanism. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.","['retraining', 'curating', 'generative model', 'self-consuming']",[],"['Damien Ferbach', 'Quentin Bertrand', 'Joey Bose', 'Gauthier Gidel']","['Mila - Quebec Artificial Intelligence Institute', 'INRIA', 'Computer Science, University of Oxford', 'Computer Science and Operational Research, University of Montreal']",
https://openreview.net/forum?id=d75qCZb7TX,Fairness & Bias,Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models,"The predominant *de facto* paradigm of testing ML models relies on either using only held-out data to compute aggregate evaluation metrics or by assessing the performance on different subgroups.  However, such *data-only testing* methods operate under the restrictive assumption that the available empirical data is the sole input for testing ML models, disregarding valuable contextual information that could guide model testing. In this paper, we challenge the go-to approach of *data-only testing* and introduce *Context-Aware Testing* (CAT) which uses context as an inductive bias to guide the search for meaningful model failures. We instantiate the first CAT system, *SMART Testing*, which employs large language models to hypothesize relevant and likely failures, which are evaluated on data using a *self-falsification mechanism*. Through empirical evaluations in diverse settings, we show that SMART automatically identifies more relevant and impactful failures than alternatives, demonstrating the potential of CAT as a testing paradigm.","['model testing', 'tabular data', 'large language models']",[],"['Paulius Rauba', 'Nabeel Seedat', 'Max Ruiz Luyten', 'Mihaela van der Schaar']","['Department of Applied Mathematics and Theoretical Physics, University of Cambridge', 'University of Cambridge', 'Mathematics, University of Cambridge', 'University of Cambridge']",
https://openreview.net/forum?id=dA7hUm4css,Security,One-Shot Safety Alignment for Large Language Models via Optimal Dualization,"The growing safety concerns surrounding large language models raise an urgent need to align them with diverse human preferences to simultaneously enhance their helpfulness and safety. A promising approach is to enforce safety constraints through Reinforcement Learning from Human Feedback (RLHF). For such constrained RLHF, typical Lagrangian-based primal-dual policy optimization methods are computationally expensive and often unstable. This paper presents a perspective of dualization that  reduces constrained alignment to an equivalent unconstrained alignment problem. We do so by pre-optimizing a smooth and convex dual function that has a closed form. This shortcut eliminates the need for cumbersome primal-dual policy iterations, greatly reducing the computational burden and improving training stability. Our strategy leads to two practical algorithms in model-based and preference-based settings (MoCAN and PeCAN, respectively). A broad range of experiments demonstrate the effectiveness and merits of our algorithms.","['Large Language Models', 'Alignment', 'RLHF', 'Safety', 'Constraints']",[],"['Xinmeng Huang', 'Shuo Li', 'Edgar Dobriban', 'Osbert Bastani', 'Hamed Hassani', 'Dongsheng Ding']","['University of Pennsylvania', 'Computer and Information Science, University of Pennsylvania', 'The Wharton School, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']",
https://openreview.net/forum?id=dHIKahbV6G,Fairness & Bias,UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models,"Pre-trained vision-language models (e.g., CLIP) have shown powerful zero-shot transfer capabilities. But they still struggle with domain shifts and typically require labeled data to adapt to downstream tasks, which could be costly. In this work, we aim to  leverage unlabeled data that naturally spans multiple domains to enhance the transferability of vision-language models.  Under this unsupervised multi-domain setting, we have identified inherent model bias within CLIP, notably  in its visual and text encoders. Specifically, we observe that CLIP’s visual encoder tends to prioritize  encoding domain over discriminative category information, meanwhile its text encoder exhibits a preference for domain-relevant classes. To mitigate this model bias, we propose a training-free and label-free feature calibration method, Unsupervised Multi-domain Feature Calibration (UMFC). UMFC estimates image-level biases from domain-specific features and text-level biases from the direction of domain transition. These biases are subsequently   subtracted from original image and text features separately, to render them domain-invariant. We evaluate our method on multiple settings including transductive learning and test-time adaptation. Extensive experiments show that our method outperforms CLIP and performs on par with the state-of-the-arts that need additional annotations or optimization. Our code is available at https://github.com/GIT-LJc/UMFC.","['model calibration', 'test-time adaptation', 'CLIP', 'multi-domain']",[],"['Jiachen Liang', 'RuiBing Hou', 'Minyang Hu', 'Hong Chang', 'Shiguang Shan', 'Xilin Chen']","['ICT, , Chinese Academy of Sciences', ', Chinese Academy of Sciences', ', Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology']",
https://openreview.net/forum?id=dIktpSgK4F,Transparency & Explainability,Dissecting Query-Key Interaction in Vision Transformers,"Self-attention in vision transformers is often thought to perform perceptual grouping where tokens attend to other tokens with similar embeddings, which could correspond to semantically similar features of an object. However, attending to dissimilar tokens can be beneficial by providing contextual information. We propose to analyze the query-key interaction by the singular value decomposition of the interaction matrix (i.e. ${\textbf{W}_q}^\top\textbf{W}_k$). We find that in many ViTs, especially those with classification training objectives, early layers attend more to similar tokens, while late layers show increased attention to dissimilar tokens, providing evidence corresponding to perceptual grouping and contextualization, respectively. Many of these interactions between features represented by singular vectors are interpretable and semantic, such as attention between relevant objects, between parts of an object, or between the foreground and background. This offers a novel perspective on interpreting the attention mechanism, which contributes to understanding how transformer models utilize context and salient features when processing images.","['Vision transformer', 'Interpretability', 'Contextualization', 'Self-attention']",[],"['Xu Pan', 'Aaron Philip', 'Ziqian Xie', 'Odelia Schwartz']","['Center for Brain Science, Harvard University, Harvard University', 'Michigan State University', '', 'Computer Science, University of Miami, University of Miami']",
https://openreview.net/forum?id=dkpmfIydrF,Privacy & Data Governance,Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models,"Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs’ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at https://github.com/OPTML-Group/AdvUnlearn.  Warning: This paper contains model outputs that may be offensive in nature.","['diffusion model', 'machine unlearning', 'adversarial training']",[],"['Yimeng Zhang', 'Xin Chen', 'Jinghan Jia', 'Yihua Zhang', 'Chongyu Fan', 'Jiancheng Liu', 'Mingyi Hong', 'Ke Ding', 'Sijia Liu']","['ByteDance Inc.', 'Intel Corp', 'Michigan State University', 'Computer Science and Engineering, Michigan State University', 'Computer Science and Engineering, Michigan State University', 'Computer Science and Engineering, Michigan State University', 'AGI, Amazon', 'Intel', 'CSE, Michigan State University']",
https://openreview.net/forum?id=dheDf5EpBT,Privacy & Data Governance,Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement,"Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction. Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at [Unified-Unlearning-w-Remain-Geometry](https://github.com/K1nght/Unified-Unlearning-w-Remain-Geometry).","['Machine unlearning', 'steepest descent', 'diffusion model']",[],"['Zhehao Huang', 'Xinwen Cheng', 'JingHao Zheng', 'Haoran Wang', 'Zhengbao He', 'Tao Li', 'Xiaolin Huang']","['Institute of Image Processing and Pattern Recognition, Shanghai Jiaotong University', 'Automation Department, Shanghai Jiaotong University', 'Department of Automation, Shanghai Jiaotong University', 'School of Mathematical Sciences, Shanghai Jiaotong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=dwYekpbmYG,Transparency & Explainability,Free Lunch in Pathology Foundation Model: Task-specific Model Adaptation with Concept-Guided Feature Enhancement,"Whole slide image (WSI) analysis is gaining prominence within the medical imaging field. Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks. However, these foundation models are usually designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types. In this work, we present Concept Anchor-guided Task-specific Feature Enhancement (CATE), an adaptable paradigm that can boost the expressivity and discriminativeness of pathology foundation models for specific downstream tasks. Based on a set of task-specific concepts derived from the pathology vision-language model with expert-designed prompts, we introduce two interconnected modules to dynamically calibrate the generic image features extracted by foundation models for certain tasks or cancer types. Specifically, we design a Concept-guided Information Bottleneck module to enhance task-relevant characteristics by maximizing the mutual information between image features and concept anchors while suppressing superfluous information. Moreover, a Concept-Feature Interference module is proposed to utilize the similarity between calibrated features and concept anchors to further generate discriminative task-specific features. The extensive experiments on public WSI datasets demonstrate that CATE significantly enhances the performance and generalizability of MIL models. Additionally, heatmap and umap visualization results also reveal the effectiveness and interpretability of CATE.","['Computational Pathology', 'Whole Slide Image', 'Vision Language Model', 'Mutiple Instance Learning']",[],"['Yanyan Huang', 'Weiqin Zhao', 'Yihang Chen', 'Yu Fu', 'Lequan Yu']","['University of Hong Kong', 'Division of Statistics and Actuarial Science School of Computing and Data Science, University of Hong Kong', 'Department of Statistics and Actuarial Science, University of Hong Kong', 'School of Information Science and Engineering, Lanzhou University', 'The University of Hong Kong']",
https://openreview.net/forum?id=dz6ex9Ee0Q,Fairness & Bias,Robust Graph Neural Networks via Unbiased Aggregation,"The adversarial robustness of Graph Neural Networks (GNNs) has been questioned due to the false sense of security uncovered by strong adaptive attacks despite the existence of numerous defenses. In this work, we delve into the robustness analysis of representative robust GNNs and provide a unified robust estimation point of view to understand their robustness and limitations. Our novel analysis of estimation bias motivates the design of a  robust and unbiased graph signal estimator.  We then develop an efficient Quasi-Newton Iterative Reweighted Least Squares algorithm to solve the estimation problem, which is unfolded as robust unbiased aggregation layers in GNNs with theoretical guarantees. Our comprehensive experiments confirm the strong robustness of our proposed model under various scenarios, and the ablation study provides a deep understanding of its advantages.","['Adversarial Robustness', 'Graph Neural Networks', 'Robust Estimation']",[],"['Zhichao Hou', 'Ruiqi Feng', 'Tyler Derr', 'Xiaorui Liu']","['Computer Science, North Carolina State University', 'Engineering School, Westlake University', 'Vanderbilt University', 'Computer Science, North Carolina State University']",
https://openreview.net/forum?id=dxxj4S06YL,Fairness & Bias,Fair Secretaries with Unfair Predictions,"Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality. The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous. A serious concern with algorithms that use predictions is that  these predictions can be biased and, as a result, cause the algorithm to make decisions that are deemed unfair. We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting---the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least $\max\{\Omega (1) , 1 - O(\varepsilon)\}$ times the optimal value, where $\varepsilon$ is the prediction error. We show how to preserve this promise while also guaranteeing to accept the best candidate with probability $\Omega(1)$.  Our algorithm and analysis are based on a new ``pegging'' idea that diverges from existing works and simplifies/unifies some of their results. Finally, we extend to the $k$-secretary problem and complement our theoretical analysis with experiments.","['Secretary problem', 'fairness', 'algorithms with predictions', 'online algorithms']",[],"['Eric Balkanski', 'Will Ma', 'Andreas Maggiori']","['Columbia University', 'Columbia University', 'Data Science Institute, Columbia University']",
https://openreview.net/forum?id=e6KrSouGHJ,Security,Attack-Resilient Image Watermarking Using Stable Diffusion,"Watermarking images is critical for tracking image provenance and proving ownership. With the advent of generative models, such as stable diffusion, that can create fake but realistic images, watermarking has become particularly important to make human-created images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate above 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. We hypothesize that the reciprocating denoising process in diffusion models may inherently enhance the robustness of the watermark when faced with strong attacks and validate the hypothesis. Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion-based attack methods. ZoDiac is open-sourced and available at https://github.com/zhanglijun95/ZoDiac.","['Computer vision', 'Generative AI', 'Image Watermarking']",[],"['Lijun Zhang', 'Xiao Liu', 'Antoni Viros i Martin', 'Cindy Xiong Bearfield', 'Yuriy Brun', 'Hui Guan']","['Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts at Amherst', 'Hybrid Cloud, International Business Machines', 'Georgia Institute of Technology', 'University of Massachusetts Amherst', 'University of Massachusetts, Amherst']",
https://openreview.net/forum?id=e5icsXBD8Q,Privacy & Data Governance,Large Language Model Unlearning via Embedding-Corrupted Prompts,"Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present \textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at \textit{nearly zero side effects} in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases. We have made our code publicly available at \url{https://github.com/chrisliu298/llm-unlearn-eco}.","['machine unlearning', 'safety', 'alignment', 'large language model unlearning']",[],"['Chris Yuhao Liu', 'Yaxuan Wang', 'Jeffrey Flanigan', 'Yang Liu']","['Department of Computer Science and Engineering, University of California, Santa Cruz', 'Computer Science and Engineering, University of California, Santa Cruz', 'University of California, Santa Cruz', 'Computer Science and Engineering, University of California, Santa Cruz']",
https://openreview.net/forum?id=eKVugi5zr0,Fairness & Bias,RoME: A Robust Mixed-Effects Bandit Algorithm for Optimizing Mobile Health Interventions,"Mobile health leverages personalized and contextually tailored interventions optimized through bandit and reinforcement learning algorithms. In practice, however, challenges such as participant heterogeneity, nonstationarity, and nonlinear relationships hinder algorithm performance. We propose RoME, a **Ro**bust **M**ixed-**E**ffects contextual bandit algorithm that simultaneously addresses these challenges via (1) modeling the differential reward with user- and time-specific random effects, (2) network cohesion penalties, and (3) debiased machine learning for flexible estimation of baseline rewards. We establish a high-probability regret bound that depends solely on the dimension of the differential-reward model, enabling us to achieve robust regret bounds even when the baseline reward is highly complex. We demonstrate the superior performance of the RoME algorithm in a simulation and two off-policy evaluation studies.","['Bandit Algorithms', 'Causal Inference', 'Supervised Learning', 'mHealth', 'Mixed-effects Modeling']",[],"['Easton Knight Huch', 'Jieru Shi', 'Madeline R Abbott', 'Jessica R Golbus', 'Alexander Moreno', 'Walter H. Dempsey']","['University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor', '', 'University of Michigan - Ann Arbor', 'Alector', 'University of Chicago']",
https://openreview.net/forum?id=eU87jJyEK5,Fairness & Bias,SpeAr: A Spectral Approach for Zero-Shot Node Classification,"Zero-shot node classification is a vital task in the field of graph data processing, aiming to identify nodes of classes unseen during the training process. Prediction bias is one of the primary challenges in zero-shot node classification, referring to the model's propensity to misclassify nodes of unseen classes as seen classes. However, most methods introduce external knowledge to mitigate the bias, inadequately leveraging the inherent cluster information within the unlabeled nodes. To address this issue, we employ spectral analysis coupled with learnable class prototypes to discover the implicit cluster structures within the graph, providing a more comprehensive understanding of classes. In this paper, we propose a spectral approach for zero-shot node classification (SpeAr). Specifically, we establish an approximate relationship between minimizing the spectral contrastive loss and performing spectral decomposition on the graph, thereby enabling effective node characterization through loss minimization. Subsequently, the class prototypes are iteratively refined based on the learned node representations, initialized with the semantic vectors. Finally, extensive experiments verify the effectiveness of the SpeAr, which can further alleviate the bias problem.","['zero-shot node classification', 'graph neural networks', 'class prototypes', 'spectral contrastive loss']",[],"['Ting Guo', 'Da Wang', 'Jiye Liang', 'Kaihan Zhang', 'Jianchao Zeng']","['North University of China', 'School of Computer and Information Technology, Shanxi University', 'School of Computer and Information Technology, Shanxi University', 'North University of China', 'North University of China']",
https://openreview.net/forum?id=eSes1Mic9d,Transparency & Explainability,Who's asking? User personas and the mechanics of latent misalignment,"Studies show that safety-tuned models may nevertheless divulge harmful information. In this work, we show that whether they do so depends significantly on who they are talking to, which we refer to as *user persona*. In fact, we find manipulating user persona to be more effective for eliciting harmful content than certain more direct attempts to control model refusal. We study both natural language prompting and activation steering as intervention methods and show that activation steering is significantly more effective at bypassing safety filters. We shed light on the mechanics of this phenomenon by showing that even when model generations are safe, harmful content can persist in hidden representations and can be extracted by decoding from earlier layers.  We also show we can predict a persona’s effect on refusal given only the geometry of its steering vector. Finally, we show that certain user personas induce the model to form more charitable interpretations of otherwise dangerous queries.","['safety', 'interpretability', 'explainability', 'NLP', 'alignment', 'activation engineering', 'jailbreaking']",[],"['Asma Ghandeharioun', 'Ann Yuan', 'Marius Guerard', 'Emily Reif', 'Michael A. Lepori', 'Lucas Dixon']","['Google', 'Google', 'Google', 'Department of Computer Science, University of Washington', 'Brown University', 'Research, Google']",
https://openreview.net/forum?id=eWiGn0Fcdx,Transparency & Explainability,Exploring Token Pruning in Vision State Space Models,"State Space Models (SSMs) have the advantage of keeping linear computational complexity compared to attention modules in transformers, and have been applied to vision tasks as a new type of powerful vision foundation model. Inspired by the observations that the final prediction in vision transformers (ViTs) is only based on a subset of most informative tokens, we take the novel step of enhancing the efficiency of SSM-based vision models through token-based pruning. However, direct applications of existing token pruning techniques designed for ViTs fail to deliver good performance, even with extensive fine-tuning. To address this issue, we revisit the unique computational characteristics of SSMs and discover that naive application disrupts the sequential token positions. This insight motivates us to design a novel and general token pruning method specifically for SSM-based vision models. We first introduce a pruning-aware hidden state alignment method to stabilize the neighborhood of remaining tokens for performance enhancement. Besides, based on our detailed analysis, we propose a token importance evaluation method adapted for SSM models, to guide the token pruning. With efficient implementation and practical acceleration methods, our method brings actual speedup. Extensive experiments demonstrate that our approach can achieve significant computation reduction with minimal impact on performance across different tasks. Notably, we achieve 81.7\% accuracy on ImageNet with a 41.6\% reduction in the FLOPs for pruned PlainMamba-L3. Furthermore, our work provides deeper insights into understanding the behavior of SSM-based vision models for future research.","['State Space Models', 'Token pruning', 'Efficiency', 'Interpretability']",[],"['Zheng Zhan', 'Zhenglun Kong', 'Yifan Gong', 'Yushu Wu', 'Zichong Meng', 'Hangyu Zheng', 'Xuan Shen', 'Stratis Ioannidis', 'Wei Niu', 'Pu Zhao', 'Yanzhi Wang']","['Northeastern University', 'Department of Biomedical Informatics, Harvard Medical School, Harvard University', 'Northeastern University', 'College of Engineering, Northeastern University', 'Khoury College of Computer Science, Northeastern University', 'University of Georgia', 'ECE, Northeastern University', 'Northeastern University', 'School of Computing, University of Georgia', 'Northeastern University', 'ECE, Northeastern University']",
https://openreview.net/forum?id=ejIzdt50ek,Fairness & Bias,Stochastic Optimization Schemes for Performative Prediction with Nonconvex Loss,"This paper studies a risk minimization problem with decision dependent data distribution. The problem pertains to the performative prediction setting in which a trained model can affect the outcome estimated by the model. Such dependency creates a feedback loop that influences the stability of optimization algorithms such as stochastic gradient descent (SGD). We present the first study on performative prediction with smooth but possibly non-convex loss. We analyze a greedy deployment scheme with SGD (SGD-GD). Note that in the literature, SGD-GD is often studied with strongly convex loss. We first propose the definition of stationary performative stable (SPS) solutions through relaxing the popular performative stable condition. We then prove that SGD-GD converges to a biased SPS solution in expectation. We consider two conditions of sensitivity on the distribution shifts: (i) the sensitivity is characterized by Wasserstein-1 distance and the loss is Lipschitz w.r.t.~data samples, or (ii) the sensitivity is characterized by total variation (TV) divergence and the loss is bounded. In both conditions, the bias levels are proportional to the stochastic gradient's variance and sensitivity level.  Our analysis is extended to a lazy deployment scheme where models are deployed once per several SGD updates, and we show that it converges to an SPS solution with reduced bias. Numerical experiments corroborate our theories.","['Non-convex optimization', 'Performative prediction', 'Stochastic optimization algorithm']",[],"['Qiang LI', 'Hoi To Wai']","['Chinese University of Hong Kong', 'The Chinese University of Hong Kong']",
https://openreview.net/forum?id=etPAH4xSUn,Fairness & Bias,In-Context Symmetries: Self-Supervised Learning through Contextual World Models,"At the core of self-supervised learning for vision is the idea of learning invariant or equivariant representations with respect to a set of data transformations. This approach, however, introduces strong inductive biases, which can render the representations fragile in downstream tasks that do not conform to these symmetries. In this work, drawing insights from world models, we propose to instead learn a general representation that can adapt to be invariant or equivariant to different transformations by paying attention to context --- a memory module that tracks task-specific states, actions and future states. Here, the action is the transformation, while the current and future states respectively represent the input's representation before and after the transformation.  Our proposed algorithm, Contextual Self Supervised Learning (ContextSSL), learns equivariance to all transformations (as opposed to invariance). In this way, the model can learn to encode all relevant features as general representations while having the versatility to tail down to task-wise symmetries when given a few examples as the context. Empirically, we demonstrate significant performance gains over existing methods on equivariance-related tasks, supported by both qualitative and quantitative evaluations.",['Self-Supervised Learning; Context; Equivariance'],[],"['Sharut Gupta', 'Chenyu Wang', 'Yifei Wang', 'Tommi Jaakkola', 'Stefanie Jegelka']","['Massachusetts Institute of Technology', '', 'Massachusetts Institute of Technology', '', 'Computer Science, Technische Universität München']",
https://openreview.net/forum?id=f70e6YYFHF,Transparency & Explainability,The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More,"Today's best language models still struggle with ""hallucinations"", factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The *reversal curse*, where models cannot recall information when probed in a different order than was encountered during training, exemplifies limitations in information retrieval.  To better understand these limitations, we reframe the reversal curse as a *factorization curse* --- a failure of models to learn the same joint distribution under different factorizations. We more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing *WikiReversal*, a realistic testbed based on Wikipedia knowledge graphs. Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens.  Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.","['Reversal curse', 'reliability', 'safety', 'language models', 'interpretability', 'learning objectives']",[],"['Ouail Kitouni', 'Niklas Nolte', 'Adina Williams', 'Michael Rabbat', 'Diane Bouchacourt', 'Mark Ibrahim']","['Massachusetts Institute of Technology', 'Facebook', 'FAIR (Meta Platforms Inc.)', 'Facebook', 'Facebook AI Research', 'Facebook AI Research (FAIR) Meta']",
https://openreview.net/forum?id=fEvUEBbEjb,Security,TARP-VP: Towards Evaluation of Transferred  Adversarial Robustness and Privacy on Label  Mapping Visual Prompting Models,"Adversarial robustness and privacy of deep learning (DL) models are two widely studied topics in AI security. Adversarial training (AT) is  an effective approach to improve the robustness of DL models against adversarial attacks. However, while models with AT demonstrate enhanced robustness, they become more susceptible to membership inference attacks (MIAs), thus increasing the risk of privacy leakage. This indicates a negative trade-off between adversarial robustness and privacy in general deep learning models. Visual prompting is a novel model reprogramming (MR) technique used for fine-tuning pre-trained models, achieving good performance in vision tasks, especially when combined with the label mapping technique. However, the performance of label-mapping-based visual prompting (LM-VP) under adversarial attacks and MIAs lacks evaluation. In this work, we regard the MR of LM-VP as a unified entity, referred to as the LM-VP model, and take a step toward jointly evaluating the adversarial robustness and privacy of LM-VP models. Experimental results show that  the choice of pre-trained models significantly affects the white-box adversarial robustness of LM-VP, and standard AT even substantially degrades its performance. In contrast, transfer AT-trained LM-VP achieves a good trade-off between transferred adversarial robustness and privacy, a finding that has been consistently validated across various pre-trained models.",['Visual Prompting; Adversarial Robustness; Membership Inference Attacks'],[],"['Zhen Chen', 'Yi Zhang', 'Fu Wang', 'Xingyu Zhao', 'Xiaowei Huang', 'Wenjie Ruan']","['University of Liverpool', 'Warwick Manufacturing Group, University of Warwick', 'University of Exeter', 'WMG, University of Warwick', 'University of Liverpool', '']",
https://openreview.net/forum?id=fMWrTAe5Iy,Fairness & Bias,R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction,"3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R$^2$-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown \emph{integration bias} in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency. Crucially, it delivers high-quality results in 4 minutes, which is 12$\times$ faster than NeRF-based methods and on par with traditional algorithms.","['3D Gaussian Splatting', '3D Reconstruction', 'CT Reconstruction', 'Tomographic Reconstruction']",[],"['Ruyi Zha', 'Tao Jun Lin', 'Yuanhao Cai', 'Jiwen Cao', 'Yanhao Zhang', 'Hongdong Li']","['School of Computing, Australian National University', '', 'Computer Science, Johns Hopkins University', 'ANU College of Engineering, Computing and Cybernetics, Australian National University', 'Robotics Institute, University of Technology Sydney', 'Australian National University']",
https://openreview.net/forum?id=fMdrBucZnj,Fairness & Bias,Expected Probabilistic Hierarchies,"Hierarchical clustering has usually been addressed by discrete optimization using heuristics or continuous optimization of relaxed scores for hierarchies. In this work, we propose to optimize expected scores under a probabilistic model over hierarchies. (1) We show theoretically that the global optimal values of the expected Dasgupta cost and Tree-Sampling divergence (TSD), two unsupervised metrics for hierarchical clustering, are equal to the optimal values of their discrete counterparts contrary to some relaxed scores. (2) We propose Expected Probabilistic Hierarchies (EPH), a probabilistic model to learn hierarchies in data by optimizing expected scores. EPH uses differentiable hierarchy sampling enabling end-to-end gradient descent based optimization, and an unbiased subgraph sampling approach to scale to large datasets. (3) We evaluate EPH on synthetic and real-world datasets including vector and graph datasets. EPH outperforms all other approaches quantitatively and provides meaningful hierarchies in qualitative evaluations.","['hierarchical clustering', 'graph clustering', 'clustering', 'unsupervised learning', 'probabilistic models']",[],"['Marcel Kollovieh', 'Bertrand Charpentier', 'Daniel Zügner', 'Stephan Günnemann']","['Department of Informatics, Technische Universität München', 'Pruna AI', 'Microsoft', 'Technical University Munich']",
https://openreview.net/forum?id=fpxRpPbF1t,Transparency & Explainability,Differentiable Modal Synthesis for Physical Modeling of Planar String Sound and Motion Simulation,"While significant advancements have been made in music generation and differentiable sound synthesis within machine learning and computer audition, the simulation of instrument vibration guided by physical laws has been underexplored. To address this gap, we introduce a novel model for simulating the spatio-temporal motion of nonlinear strings, integrating modal synthesis and spectral modeling within a neural network framework. Our model leverages mechanical properties and fundamental frequencies as inputs, outputting string states across time and space that solve the partial differential equation characterizing the nonlinear string. Empirical evaluations demonstrate that the proposed architecture achieves superior accuracy in string motion simulation compared to existing baseline architectures. The code and demo are available online.","['Differentiable Audio Signal Processing', 'Physical Modeling', 'Musical Sound Synthesis', 'Physical Simulation']",[],"['Jin Woo Lee', 'Jaehyun Park', 'Min Jun Choi', 'Kyogu Lee']","['Meta', 'Seoul National University', 'Department of Intelligence and Information, Seoul National University', 'Seoul National University']",
https://openreview.net/forum?id=h0rbjHyWoa,Security,Generalize or Detect? Towards Robust Semantic Segmentation Under Multiple Distribution Shifts,"In open-world scenarios, where both novel classes and domains may exist, an ideal segmentation model should detect anomaly classes for safety and generalize to new domains. However, existing methods often struggle to distinguish between domain-level and semantic-level distribution shifts, leading to poor OOD detection or domain generalization performance. In this work, we aim to equip the model to generalize effectively to covariate-shift regions while precisely identifying semantic-shift regions. To achieve this, we design a novel generative augmentation method to produce coherent images that incorporate both anomaly (or novel) objects and various covariate shifts at both image and object levels. Furthermore, we introduce a training strategy that recalibrates uncertainty specifically for semantic shifts and enhances the feature extractor to align features associated with domain shifts. We validate the effectiveness of our method across benchmarks featuring both semantic and domain shifts. Our method achieves state-of-the-art performance across all benchmarks for both OOD detection and domain generalization. Code is available at https://github.com/gaozhitong/MultiShiftSeg.","['Anomaly Segmentation', 'Out-of-distribution Detection', 'Domain Generalization', 'Semantic Segmentation']",[],"['Zhitong Gao', 'Bingnan Li', 'Mathieu Salzmann', 'Xuming He']","['VILab, EPFL', 'Computer Science and Engineering , University of California, San Diego', 'Swiss Data Science Center', 'ShanghaiTech University']",
https://openreview.net/forum?id=hGgkdFF2hR,Transparency & Explainability,Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling,"Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning. A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset. [Forrow et al. 2019] introduced a factored coupling for the k-Wasserstein barycenter problem, which [Scetbon et al. 2021] adapted to solve the primal low-rank OT problem. We derive an alternative parameterization of the low-rank problem based on the _latent coupling_ (LC) factorization previously introduced by [Lin et al. 2021] generalizing [Forrow et al. 2019]. The LC factorization has multiple  advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability. We leverage these advantages to derive a new algorithm _Factor Relaxation with Latent Coupling_ (FRLC), which uses _coordinate_ mirror descent to compute the LC factorization. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity. We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications -- including graph clustering and spatial transcriptomics --  while demonstrating its interpretability.","['Optimal Transport', 'Sinkhorn', 'Low-Rank', 'Matrix Factorization']",[],"['Peter Halmos', 'Xinhao Liu', 'Julian Gold', 'Benjamin Raphael']","['Princeton University', 'Princeton University', 'Center for Statistics and Machine Learning, Princeton University', 'Princeton University']",
https://openreview.net/forum?id=hRKsahifqj,Fairness & Bias,Autoregressive Policy Optimization for Constrained Allocation Tasks,"Allocation tasks represent a class of problems where a limited amount of resources must be allocated to a set of entities at each time step. Prominent examples of this task include portfolio optimization or distributing computational workloads across servers. Allocation tasks are typically bound by linear constraints describing practical requirements that have to be strictly fulfilled at all times. In portfolio optimization, for example, investors may be obligated to allocate less than 30\% of the funds into a certain industrial sector in any investment period.  Such constraints restrict the action space of allowed allocations in intricate ways, which makes learning a policy that avoids constraint violations difficult. In this paper, we propose a new method for constrained allocation tasks based on an autoregressive process to sequentially sample allocations for each entity. In addition, we introduce a novel de-biasing mechanism to counter the initial bias caused by sequential sampling. We demonstrate the superior performance of our approach compared to a variety of Constrained Reinforcement Learning (CRL) methods on three distinct constrained allocation tasks: portfolio optimization, computational workload distribution, and a synthetic allocation benchmark. Our code is available at: https://github.com/niklasdbs/paspo","['Reinforcement learning', 'Constraint Reinforcement Learning', 'Allocation Tasks']",[],"['David Winkel', 'Niklas Alexander Strauß', 'Maximilian Bernhard', 'Zongyue Li', 'Thomas Seidl', 'Matthias Schubert']","['Institute for Informatics, Ludwig-Maximilians-Universität München', 'Ludwig-Maximilians-Universität München', '', 'Ludwig-Maximilians-Universität München', 'Computer Science, Ludwig-Maximilians-Universität München', 'Institut für Informatik, LMU Munich']",
https://openreview.net/forum?id=JCyBN5syv3,Security,SimGen: Simulator-conditioned Driving Scene Generation,"Controllable synthetic data generation can substantially lower the annotation cost of training data. Prior works use diffusion models to generate driving images conditioned on the 3D object layout. However, those models are trained on small-scale datasets like nuScenes, which lack appearance and layout diversity. Moreover, overfitting often happens, where the trained models can only generate images based on the layout data from the validation set of the same dataset. In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world. It uses a novel cascade diffusion pipeline to address challenging sim-to-real gaps and multi-condition conflicts. A driving video dataset DIVA is collected to enhance the generative diversity of SimGen, which contains over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator. SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator. We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation.","['Autonomous Driving', 'Generative Models', 'Simulators']",[],"['Yunsong Zhou', 'Michael Simon', 'Zhenghao Peng', 'Sicheng Mo', 'Hongzi Zhu', 'Minyi Guo', 'Bolei Zhou']","['Computer Science, Shanghai Jiaotong University', 'Computer Science, University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles', 'Shanghai Jiao Tong University', 'Department of Computer Science, Shanghai Jiaotong University', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=hYMxyeyEc5,Security,Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning,"Real-world data deviating from the independent and identically distributed (\textit{i.i.d.}) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms. Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation. However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces. Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning. Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.","['Out-of-Distribution Detection', 'Mathematical Reasoning', 'Generative Language Models']",[],"['Yiming Wang', 'Pei Zhang', 'Baosong Yang', 'Derek F. Wong', 'Zhuosheng Zhang', 'Rui Wang']","['Shanghai Jiao Tong University', 'Alibaba Group', 'Alibaba Group', 'Computer and Information Science, University of Macau', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=hgdh4foghu,Security,Policy-shaped prediction: avoiding distractions in model-based reinforcement learning,"Model-based reinforcement learning (MBRL) is a promising route to sample-efficient policy optimization. However, a known vulnerability of reconstruction-based MBRL consists of scenarios in which detailed aspects of the world are highly predictable, but irrelevant to learning a good policy. Such scenarios can lead the model to exhaust its capacity on meaningless content, at the cost of neglecting important environment dynamics. While existing approaches attempt to solve this problem, we highlight its continuing impact on leading MBRL methods ---including DreamerV3 and DreamerPro--- with a novel environment where background distractions are intricate, predictable, and useless for planning future actions. To address this challenge we develop a method for focusing the capacity of the world model through a synergy of a pretrained segmentation model, a task-aware reconstruction loss, and adversarial learning. Our method outperforms a variety of other approaches designed to reduce the impact of distractors, and is an advance towards robust model-based reinforcement learning.","['machine learning', 'model based reinforcement learning', 'reinforcement learning', 'segment anything model']",[],"['Miles Richard Hutson', 'Isaac Kauvar', 'Nick Haber']","['Stanford University', 'Stanford University', 'Stanford University']",
https://openreview.net/forum?id=hjhpCJfbFG,Transparency & Explainability,Interpretable Image Classification with Adaptive Prototype-based Vision Transformers,"We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.'' In our model, a prototype consists of **parts**, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.","['deep learning', 'interpretability', 'prototype-based neural network', 'case-based reasoning']",[],"['Chiyu Ma', 'Jon Donnelly', 'Wenjun Liu', 'Soroush Vosoughi', 'Cynthia Rudin', 'Chaofan Chen']","['Dartmouth College', 'Computer Science, Duke University', 'Dartmouth College', 'Dartmouth College', '', 'School of Computing and Information Science, University of Maine']",
https://openreview.net/forum?id=i4gqCM1r3z,Privacy & Data Governance,Reconstruction Attacks on Machine Unlearning: Simple Models are Vulnerable,"Machine unlearning is motivated by principles of data autonomy. The premise is that a person can request to have their data's influence removed from deployed models, and those models should be updated as if they were retrained without the person's data. We show that these updates expose individuals to high-accuracy reconstruction attacks which allow the attacker to recover their data in its entirety, even when the original models are so simple that privacy risk might not otherwise have been a concern. We show how to mount a near-perfect attack on the deleted data point from linear regression models. We then generalize our attack to other loss functions and architectures,  and empirically demonstrate the effectiveness of our attacks across a wide range of datasets (capturing both tabular and image data). Our work highlights that privacy risk is significant even for extremely simple model classes when individuals can request deletion of their data from the model.","['machine unlearning', 'reconstruction attack']",[],"['Martin Andres Bertran', 'Shuai Tang', 'Michael Kearns', 'Jamie Heather Morgenstern', 'Aaron Roth', 'Steven Wu']","['Duke University', 'Jump Trading', 'University of Pennsylvania', 'University of Washington, Seattle', 'Amazon', 'School of Computer Science, Carnegie Mellon University']",
https://openreview.net/forum?id=i816TeqgVh,Fairness & Bias,SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions,"Unsupervised skill discovery carries the promise that an intelligent agent can learn reusable skills through autonomous, reward-free interactions with environments. Existing unsupervised skill discovery methods learn skills by encouraging distinguishable behaviors that cover diverse states. However, in complex environments with many state factors (e.g., household environments with many objects), learning skills that cover all possible states is impossible, and naively encouraging state diversity often leads to simple skills that are not ideal for solving downstream tasks. This work introduces Skill Discovery from Local Dependencies (SkiLD), which leverages state factorization as a natural inductive bias to guide the skill learning process. The key intuition guiding SkiLD is that skills that induce \textbf{diverse interactions} between state factors are often more valuable for solving downstream tasks. To this end, SkiLD develops a novel skill learning objective that explicitly encourages the mastering of skills that effectively induce different interactions within an environment. We evaluate SkiLD in several domains with challenging, long-horizon sparse reward tasks including a realistic simulated household robot domain, where SkiLD successfully learns skills with clear semantic meaning and shows superior performance compared to existing unsupervised reinforcement learning methods that only maximize state coverage.",['unsupervised skill discovery'],[],"['Zizhao Wang', 'Jiaheng Hu', 'Caleb Chuck', 'Stephen Chen', 'Roberto Martín-Martín', 'Amy Zhang', 'Scott Niekum', 'Peter Stone']","['University of Texas at Austin', 'University of Texas at Austin', 'University of Texas, Austin', '', 'Consumer Robotics, Amazon', 'University of Texas at Austin', 'College of Information and Computer Sciences, University of Massachusetts at Amherst', 'Sony AI']",
https://openreview.net/forum?id=iO7viYaAt7,Security,Expectation Alignment: Handling Reward Misspecification in the Presence of Expectation Mismatch,"Detecting and handling misspecified objectives, such as reward functions, has been widely recognized as one of the central challenges within the domain of Artificial Intelligence (AI) safety research. However, even with the recognition of the importance of this problem, we are unaware of any works that attempt to provide a clear definition for what constitutes (a) misspecified objectives and (b) successfully resolving such misspecifications. In this work, we use the theory of mind, i.e., the human user's beliefs about the AI agent, as a basis to develop a formal explanatory framework, called Expectation Alignment (EAL), to understand the objective misspecification and its causes. Our EAL framework not only acts as an explanatory framework for existing works but also provides us with concrete insights into the limitations of existing methods to handle reward misspecification and novel solution strategies. We use these insights to propose a new interactive algorithm that uses the specified reward to infer potential user expectations about the system behavior. We show how one can efficiently implement this algorithm by mapping the inference problem into linear programs. We evaluate our method on a set of standard Markov Decision Process (MDP) benchmarks.",['Reward Design'],[],"['Malek Mechergui', 'Sarath Sreedharan']","['Colorado State University', 'Colorado State University']",
https://openreview.net/forum?id=iWlqbNE8P7,Fairness & Bias,Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling,"Learning complex physical dynamics purely from data is challenging due to the intrinsic properties of systems to be satisfied. Incorporating physics-informed priors, such as in Hamiltonian Neural Networks (HNNs), achieves high-precision modeling for energy-conservative systems. However, real-world systems often deviate from strict energy conservation and follow different physical priors. To address this, we present a framework that achieves high-precision modeling for a wide range of dynamical systems from the numerical aspect, by enforcing Time-Reversal Symmetry (TRS) via a novel regularization term. It helps preserve energies for conservative systems while serving as a strong inductive bias for non-conservative, reversible systems. While TRS is a domain-specific physical prior, we present the first theoretical proof that TRS loss can universally improve modeling accuracy by minimizing higher-order Taylor terms in ODE integration, which is numerically beneficial to various systems regardless of their properties, even for irreversible systems. By integrating the TRS loss within neural ordinary differential equation models, the proposed model TREAT demonstrates superior performance on diverse physical systems. It achieves a significant 11.5% MSE improvement in a challenging chaotic triple-pendulum scenario, underscoring TREAT’s broad applicability and effectiveness.",['Physics-informed Neural Networks;  Dynamical Systems ; Graph Neural Networks'],[],"['Zijie Huang', 'Wanjia Zhao', 'Jingdong Gao', 'Ziniu Hu', 'Xiao Luo', 'Yadi Cao', 'Yuanzhou Chen', 'Yizhou Sun', 'Wei Wang']","['Computer Science, University of California, Los Angeles', 'Stanford University', 'University of California, Los Angeles', 'xAI', 'Department of Computer Science, University of California, Los Angeles', 'University of California, San Diego', 'Computer Science, University of California, Los Angeles', 'Computer Science, University of California, Los Angeles', 'Computer Science, University of California, Los Angeles']",
https://openreview.net/forum?id=iSjqTQ5S1f,Transparency & Explainability,Stochastic Concept Bottleneck Models,"Concept Bottleneck Models (CBMs) have emerged as a promising interpretable method whose final prediction is based on intermediate, human-understandable concepts rather than the raw input. Through time-consuming manual interventions, a user can correct wrongly predicted concept values to enhance the model's downstream performance. We propose *Stochastic Concept Bottleneck Models* (SCBMs), a novel approach that models concept dependencies. In SCBMs, a single-concept intervention affects all correlated concepts, thereby improving intervention effectiveness. Unlike previous approaches that model the concept relations via an autoregressive structure, we introduce an explicit, distributional parameterization that allows SCBMs to retain the CBMs' efficient training and inference procedure.  Additionally, we leverage the parameterization to derive an effective intervention strategy based on the confidence region. We show empirically on synthetic tabular and natural image datasets that our approach improves intervention effectiveness significantly. Notably, we showcase the versatility and usability of SCBMs by examining a setting with CLIP-inferred concepts, alleviating the need for manual concept annotations.","['Concept Bottleneck Models', 'Interventions', 'Interpretability', 'Concepts']",[],"['Moritz Vandenhirtz', 'Sonia Laguna', 'Ričards Marcinkevičs', 'Julia E Vogt']","['ETHZ - ETH Zurich', 'Department of Computer Science, ETHZ - ETH Zurich', 'Department of Computer Science, Swiss Federal Institute of Technology', 'Computer Sience, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=ibKpPabHVn,Security,DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection,"Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the ""swap property"" that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop ""Deep Dependency Regularized Knockoff (DeepDRK),"" a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian.","['Feature Selection', 'Deep Learning', 'Model-X Knockoff', 'FDR Control', 'Boosting Power']",[],"['Hongyu Shen', 'Yici Yan', 'Zhizhen Zhao']","['Amazon', 'University of Illinois at Urbana-Champaign', 'Electrical and computer engineering, University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=iykao97YXf,Transparency & Explainability,Reinforcement Learning with LTL and $\omega$-Regular Objectives via Optimality-Preserving Translation to Average Rewards,"Linear temporal logic (LTL) and, more generally, $\omega$-regular objectives are alternatives to the traditional discount sum and average reward objectives in reinforcement learning (RL), offering the advantage of greater comprehensibility and hence explainability. In this work, we study the relationship between these objectives. Our main result is that each RL problem for $\omega$-regular objectives can be reduced to a limit-average reward problem in an optimality-preserving fashion, via (finite-memory) reward machines. Furthermore, we demonstrate the efficacy of this approach by showing that optimal policies for limit-average problems can be found asymptotically by solving a sequence of discount-sum problems approximately. Consequently, we resolve an open problem: optimal policies for LTL and $\omega$-regular objectives can be learned asymptotically.","['Reinforcement Learning', 'LTL', 'limit average rewards', 'translation']",[],"['Xuan-Bach Le', 'Dominik Wagner', 'Leon Witzman', 'Alexander Rabinovich', 'Luke Ong']","['College of Computing and Data Science, Nanyang Technological University', 'Computing and Data Science, Nanyang Technological University', 'Nanyang Technological University', 'Tel Aviv University', 'Department of Computer Science, Nanyang Technological University']",
https://openreview.net/forum?id=ioAlzcELTf,Fairness & Bias,Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling,"Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which generalizes weather forecasts to finer-grained temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, effectively generalizes forecasts across multiple time scales, including 30-minute, which is even smaller than the dataset's temporal resolution.","['weather forecast', 'physics-AI hybrid model', 'partial differential equation', 'machine learning']",[],"['Wanghan Xu', 'Fenghua Ling', 'Wenlong Zhang', 'Tao Han', 'Hao Chen', 'Wanli Ouyang', 'LEI BAI']","['School of Electronic Information and Electrical Engineering, Shanghai Jiaotong University', 'Shanghai Artificial Intelligence Laboratory', 'AI4Science, Shanghai Artificial Intelligence Laboratory', 'Department of Computer Science and Engineering, Hong Kong University of Science and Technology', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai AI Lab', 'AI for Science, Shanghai AI Laboratory']",
https://openreview.net/forum?id=j6Zsoj544N,Privacy & Data Governance,Does Worst-Performing Agent Lead the Pack? Analyzing Agent Dynamics in Unified Distributed SGD,"Distributed learning is essential to train machine learning algorithms across *heterogeneous* agents while maintaining data privacy. We conduct an asymptotic analysis of Unified Distributed SGD (UD-SGD), exploring a variety of communication patterns, including decentralized SGD and local SGD within Federated Learning (FL), as well as the increasing communication interval in the FL setting. In this study, we assess how different sampling strategies, such as *i.i.d.* sampling, shuffling, and Markovian sampling, affect the convergence speed of UD-SGD by considering the impact of agent dynamics on the limiting covariance matrix as described in the Central Limit Theorem (CLT). Our findings not only support existing theories on linear speedup and asymptotic network independence, but also theoretically and empirically show how efficient sampling strategies employed by individual agents contribute to overall convergence in UD-SGD. Simulations reveal that a few agents using highly efficient sampling can achieve or surpass the performance of the majority employing moderately improved strategies, providing new insights beyond traditional analyses focusing on the worst-performing agent.","['Distributed Optimization', 'Agent Dynamics', 'Federated Learning', 'Central Limit Theorem', 'Efficient Sampling']",[],"['Jie Hu', 'Yi-Ting Ma', 'Do Young Eun']","['Department of Electrical and Computer Engineering, North Carolina State University', 'North Carolina State University', 'North Carolina State University']",
https://openreview.net/forum?id=jCMYIUwprx,Security,INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness,"Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes (+10% absolute improvements in all models).","['code generation', 'safety', 'helpfulness', 'code security', 'large language model', 'critic', 'autonomous agent']",[],"['Hung Le', 'Doyen Sahoo', 'Yingbo Zhou', 'Caiming Xiong', 'Silvio Savarese']","['Amazon', 'SalesForce.com', 'Salesforce Research', 'Salesforce Research', 'Salesforce']",
https://openreview.net/forum?id=jLUbLxa4XV,Security,Certified Adversarial Robustness via Randomized $\alpha$-Smoothing for Regression Models,"Certified adversarial robustness of large-scale deep networks has progressed substantially after the introduction of randomized smoothing. Deep net classifiers are now provably robust in their predictions against a large class of threat models, including $\ell_1$, $\ell_2$, and $\ell_\infty$ norm-bounded attacks. Certified robustness analysis by randomized smoothing has not been performed for deep regression networks where the output variable is continuous and unbounded. In this paper, we extend the existing results for randomized smoothing into regression models using powerful tools from robust statistics, in particular, $\alpha$-trimming filter as the smoothing function. Adjusting the hyperparameter $\alpha$ achieves a smooth trade-off between desired certified robustness and utility. For the first time, we propose a benchmark for certified robust regression in visual positioning systems using the Cambridge Landmarks dataset where robustness analysis is essential for autonomous navigation of AI agents and self-driving cars. Code is publicly available at \url{https://github.com/arekavandi/Certified_adv_RRegression/}.","['Certified robustness', 'alpha-trimming', 'Probabilistic certificates', 'Robust regression', 'Randomized smoothing']",[],"['Aref Miri Rekavandi', 'Farhad Farokhi', 'Olga Ohrimenko', 'Benjamin I. P. Rubinstein']","['', 'The University of Melbourne', '', 'The University of Melbourne']",
https://openreview.net/forum?id=jWGGEDYORs,Transparency & Explainability,DARNet: Dual Attention Refinement Network with Spatiotemporal Construction for Auditory Attention Detection,"At a cocktail party, humans exhibit an impressive ability to direct their attention. The auditory attention detection (AAD) approach seeks to identify the attended speaker by analyzing brain signals, such as EEG signals.  However, current AAD algorithms overlook the spatial distribution information within EEG signals and lack the ability to capture long-range latent dependencies, limiting the model's ability to decode brain activity. To address these issues, this paper proposes a dual attention refinement network with spatiotemporal construction for AAD, named DARNet, which consists of the spatiotemporal construction module, dual attention refinement module, and feature fusion \& classifier module. Specifically, the spatiotemporal construction module aims to construct more expressive spatiotemporal feature representations, by capturing the spatial distribution characteristics of EEG signals. The dual attention refinement module aims to extract different levels of temporal patterns in EEG signals and enhance the model's ability to capture long-range latent dependencies. The feature fusion \& classifier module aims to aggregate temporal patterns and dependencies from different levels and obtain the final classification results. The experimental results indicate that DARNet achieved excellent classification performance, particularly under short decision windows. While maintaining excellent classification performance, DARNet significantly reduces the number of required parameters. Compared to the state-of-the-art models, DARNet reduces the parameter count by 91\%. Code is available at: https://github.com/fchest/DARNet.git.","['auditory attention decoding (AAD)', 'electroencephalography (EEG)', 'brain-computer interface (BCI)']",[],"['Sheng Yan', 'Cunhang Fan', 'Hongyu Zhang', 'Xiaoke Yang', 'Jianhua Tao', 'Zhao Lv']","['School of Computer Science and Technology, Anhui University', 'School of Computer Science and Technology, Anhui University, Hefei 230601, China', 'Anhui University', 'Anhui University', '', 'School of Computer Science and Technology, Anhui University, Hefei 230601, China']",
https://openreview.net/forum?id=jV6z08u7y0,Fairness & Bias,The Implicit Bias of Gradient Descent toward Collaboration between Layers: A Dynamic Analysis of Multilayer Perceptions,"The implicit bias of gradient descent has long been considered the primary mechanism explaining the superior generalization of over-parameterized neural networks without overfitting, even when the training error is zero. However, the implicit bias toward adversarial robustness has rarely been considered in the research community, although it is crucial for the trustworthiness of machine learning models. To fill this gap, in this paper, we explore whether consecutive layers collaborate to strengthen adversarial robustness during gradient descent. By quantifying this collaboration between layers using our proposed concept, co-correlation, we demonstrate a monotonically increasing trend in co-correlation, which implies a decreasing trend in adversarial robustness during gradient descent. Additionally, we observe different behaviours between narrow and wide neural networks during gradient descent. We conducted extensive experiments that verified our proposed theorems.","['Implicit Bias', 'Adversarial Robustness', 'Dynamic Analysis']",[],"['Zheng Wang', 'Geyong Min', 'Wenjie Ruan']","['WMG (Warwick Manufacturing Group), The University of Warwick', 'Department of Computer Science, University of Exeter', '']",
https://openreview.net/forum?id=jgpWXnXdME,Transparency & Explainability,Advection Augmented Convolutional Neural Networks,"Many problems in physical sciences are characterized by the prediction of space-time sequences. Such problems range from weather prediction to the analysis of disease propagation and video prediction. Modern techniques for the solution of these problems typically combine Convolution Neural Networks (CNN) architecture with a time prediction mechanism. However, oftentimes, such approaches underperform in the long-range propagation of information and lack explainability. In this work, we introduce a physically inspired architecture for the solution of such problems. Namely, we propose to augment CNNs with advection by designing a novel semi-Lagrangian push operator. We show that the proposed operator allows for the non-local transformation of information compared with standard convolutional kernels. We then complement it with Reaction and Diffusion neural components to form a network that mimics the Reaction-Advection-Diffusion equation, in high dimensions. We demonstrate the effectiveness of our network on a number of spatio-temporal datasets that show their merit. Our code is available at https://github.com/Siddharth-Rout/deepADRnet.","['Reaction-Advection-Diffusion System', 'Partial Differential Equation', 'Semi-Lagrangian Scheme', 'Spatio-temporal Prediction']",[],"['Niloufar Zakariaei', 'Siddharth Rout', 'Eldad Haber', 'Moshe Eliasof']","['University of British Columbia', 'University of British Columbia', 'EOAS, University of British Columbia', 'University of Cambridge']",
https://openreview.net/forum?id=js74ZCddxG,Security,RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation,"Federated learning (FL) allows multiple devices to train a model collaboratively without sharing their data. Despite its benefits, FL is vulnerable to privacy leakage and poisoning attacks. To address the privacy concern, secure aggregation (SecAgg) is often used to obtain the aggregation of gradients on sever without inspecting individual user updates. Unfortunately, existing defense strategies against poisoning attacks rely on the analysis of local updates in plaintext, making them incompatible with SecAgg. To reconcile the conflicts, we propose a robust federated learning framework against poisoning attacks (RFLPA) based on SecAgg protocol. Our framework computes the cosine similarity between local updates and server updates to conduct robust aggregation. Furthermore, we leverage verifiable packed Shamir secret sharing to achieve reduced communication cost of $O(M+N)$ per user, and design a novel dot product aggregation algorithm to resolve the issue of increased information leakage. Our experimental results show that RFLPA significantly reduces communication and computation overhead by over $75\%$ compared to the state-of-the-art secret sharing method, BREA, while maintaining competitive accuracy.","['federated learning', 'poisoning attack', 'privacy protection']",[],"['Peihua Mai', 'Ran Yan', 'Yan Pang']","['National University of Singapore', 'National University of Singapore', 'Analytics and Operations, National University of Singapore']",
https://openreview.net/forum?id=k9SH68MvJs,Security,Diffusion-Reward Adversarial Imitation Learning,"Imitation learning aims to learn a policy from observing expert demonstrations without access to reward signals from environments. Generative adversarial imitation learning (GAIL) formulates imitation learning as adversarial learning, employing a generator policy learning to imitate expert behaviors and discriminator learning to distinguish the expert demonstrations from agent trajectories. Despite its encouraging results, GAIL training is often brittle and unstable. Inspired by the recent dominance of diffusion models in generative modeling, we propose Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more robust and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator, and design diffusion rewards based on the classifier’s output for policy learning. Extensive experiments are conducted in navigation, manipulation, and locomotion, verifying DRAIL’s effectiveness compared to prior imitation learning methods. Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL. Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more robust and smoother rewards. Project page: https://nturobotlearninglab.github.io/DRAIL/","['Imitation Learning', 'Adversarial Imitation Learning', 'Diffusion Model']",[],"['Chun-Mao Lai', 'Hsiang-Chun Wang', 'Ping-Chun Hsieh', 'Yu-Chiang Frank Wang', 'Min-Hung Chen', 'Shao-Hua Sun']","['National Taiwan University', 'Graduate Institute of Communication Engineering, National Taiwan University', 'National Chiao Tung University', 'NVIDIA', 'NVIDIA', 'National Taiwan University']",
https://openreview.net/forum?id=k8AYft5ED1,Security,Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation,"Adversarial Collaborative Filtering (ACF), which typically applies adversarial perturbations at user and item embeddings through adversarial training, is widely recognized as an effective strategy for enhancing the robustness of Collaborative Filtering (CF) recommender systems against poisoning attacks. Besides, numerous studies have empirically shown that ACF can also improve recommendation performance compared to traditional CF. Despite these empirical successes, the theoretical understanding of ACF's effectiveness in terms of both performance and robustness remains unclear. To bridge this gap, in this paper, we first theoretically show that ACF can achieve a lower recommendation error compared to traditional CF with the same training epochs in both clean and poisoned data contexts. Furthermore, by establishing bounds for reductions in recommendation error during ACF's optimization process, we find that applying personalized magnitudes of perturbation for different users based on their embedding scales can further improve ACF's effectiveness. Building on these theoretical understandings, we propose Personalized Magnitude Adversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.","['Adversarial Collaborative Filtering', 'Robust Recommender System', 'Poisoning Attacks']",[],"['Kaike Zhang', 'Qi Cao', 'Yunfan Wu', 'Fei Sun', 'Huawei Shen', 'Xueqi Cheng']","['Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences, China', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'network data science and technology, Institute of Computing Technology, Chinese Academy']",
https://openreview.net/forum?id=kVr3L73pNH,Privacy & Data Governance,Data Attribution for Text-to-Image Models by Unlearning Synthesized Images,"The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. Influence is defined such that, for a given output, if a model is retrained from scratch without the most influential images, the model would fail to reproduce the same output. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining models from scratch. In our work, we propose an efficient data attribution method by simulating unlearning the synthesized image. We achieve this by increasing the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. We then identify training images with significant loss deviations after the unlearning process and label these as influential. We evaluate our method with a computationally intensive but ""gold-standard"" retraining from scratch and demonstrate our method's advantages over previous methods.","['Data Attribution', 'Influence Estimation', 'Text-to-Image models', 'Machine Unlearning']",[],"['Sheng-Yu Wang', 'Aaron Hertzmann', 'Alexei A Efros', 'Jun-Yan Zhu', 'Richard Zhang']","['Robotics Institute, CMU, Carnegie Mellon University', 'Adobe', 'University of California Berkeley', 'School of Computer Science, Carnegie Mellon University', 'Adobe Systems']",
https://openreview.net/forum?id=kPGNE4CrTq,Transparency & Explainability,Solving Sparse \& High-Dimensional-Output Regression via Compression,"Multi-Output Regression (MOR) has been widely used in scientific data analysis for decision-making. Unlike traditional regression models, MOR aims to simultaneously predict multiple real-valued outputs given an input. However, the increasing dimensionality of the outputs poses significant challenges regarding interpretability and computational scalability for modern MOR applications. As a first step to address these challenges, this paper proposes a Sparse \& High-dimensional-Output REgression (SHORE) model by incorporating additional sparsity requirements to resolve the output interpretability, and then designs a computationally efficient two-stage optimization framework capable of solving SHORE with provable accuracy via compression on outputs. Theoretically, we show that the proposed framework is computationally scalable while maintaining the same order of training loss and prediction loss before-and-after compression under arbitrary or relatively weak sample set conditions. Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework.","['multi-output regression', 'sparsity', 'compression', 'non-convex optimization']",[],"['Renyuan Li', 'Zhehui Chen', 'Guanyi Wang']","['ISE, National University of Singapore', 'Google', 'National University of Singapore']",
https://openreview.net/forum?id=kRwQCAIA7z,Privacy & Data Governance,Dimension-free Private Mean Estimation for Anisotropic Distributions,"We present differentially private algorithms for high-dimensional mean estimation. Previous private estimators on distributions over $\mathbb{R}^d$ suffer from a curse of dimensionality, as they require $\Omega(d^{1/2})$ samples to achieve non-trivial error, even in cases where $O(1)$ samples suffice without privacy. This rate is  unavoidable when the distribution is isotropic, namely, when the covariance is a multiple of the identity matrix. Yet, real-world data is often highly anisotropic, with signals concentrated on a small number of principal components. We develop estimators that are appropriate for such signals---our estimators are $(\varepsilon,\delta)$-differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions.  Given $n$ samples from a distribution with known covariance-proxy $\Sigma$ and unknown mean $\mu$, we present an estimator $\hat{\mu}$ that achieves error, $\|\hat{\mu}-\mu\|_2\leq \alpha$, as long as $n\gtrsim \text{tr}(\Sigma)/\alpha^2+ \text{tr}(\Sigma^{1/2})/(\alpha\varepsilon)$. We show that this is the optimal sample complexity for this task up to logarithmic factors. Moreover, for the case of unknown covariance, we present an algorithm whose sample complexity has improved dependence on the dimension, from $d^{1/2}$ to $d^{1/4}$.","['differential privacy', 'mean estimation', 'anisotropic', 'covariance-adaptive error']",[],"['Yuval Dagan', 'Michael Jordan', 'Xuelin Yang', 'Lydia Zakynthinou', 'Nikita Zhivotovskiy']","['Massachusetts Institute of Technology', 'University of California, Berkeley', '', 'Simons Institute for the Theory of Computing, University of California, Berkeley', 'Statistics, University of California, Berkeley']",
https://openreview.net/forum?id=lG1VEQJvUH,Fairness & Bias,Unitary Convolutions for Learning on Graphs and Groups,"Data with geometric structure is ubiquitous in machine learning often arising from fundamental symmetries in a domain, such as permutation-invariance in graphs and translation-invariance in images. Group-convolutional architectures, which encode symmetries as inductive bias, have shown great success in applications, but can suffer from instabilities as their depth increases and often struggle to learn long range dependencies in data. For instance, graph neural networks experience instability due to the convergence of node representations (over-smoothing), which can occur after only a few iterations of message-passing, reducing their effectiveness in downstream tasks. Here, we propose and study unitary group convolutions, which allow for deeper networks that are more stable during training. The main focus of the paper are graph neural networks, where we show that unitary graph convolutions provably avoid over-smoothing. Our experimental results confirm that unitary graph convolutional networks achieve competitive performance on benchmark datasets compared to state-of-the-art graph neural networks. We complement our analysis of the graph domain with the study of general unitary convolutions and analyze their role in enhancing stability in general group convolutional architectures.","['graph neural networks', 'geometric deep learning', 'learning stability', 'unitary', 'orthogonal']",[],"['Bobak Kiani', 'Lukas Fesser', 'Melanie Weber']","['Massachusetts Institute of Technology', '', '']",
https://openreview.net/forum?id=l8XnqbQYBK,Fairness & Bias,Toward Conditional Distribution Calibration in Survival Prediction,"Survival prediction often involves estimating the time-to-event distribution from censored datasets. Previous approaches have focused on enhancing discrimination and marginal calibration. In this paper, we highlight the significance of *conditional calibration* for real-world applications – especially its role in individual decision-making. We propose a method based on conformal prediction that uses the model’s predicted individual survival probability at that instance’s observed time. This method effectively improves the model’s marginal and conditional calibration, without compromising discrimination. We provide asymptotic theoretical guarantees for both marginal and conditional calibration and test it extensively across 15 diverse real-world datasets, demonstrating the method’s practical effectiveness and versatility in various settings.",['survival analysis; calibration; conformal prediction; censorship; discrimination'],[],"['Shi-ang Qi', 'Yakun Yu', 'Russell Greiner']","['Computing Science, University of Alberta', 'University of Alberta', 'University of Alberta']",
https://openreview.net/forum?id=l6xVqzm72i,Fairness & Bias,MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space,"Recent advances in low light image enhancement have been dominated by Retinex-based learning framework, leveraging convolutional neural networks (CNNs) and Transformers. However, the vanilla Retinex theory primarily addresses global illumination degradation and neglects local issues such as noise and blur in dark conditions. Moreover, CNNs and Transformers struggle to capture global degradation due to their limited receptive fields. While state space models (SSMs) have shown promise in the long-sequence modeling, they face challenges in combining local invariants and global context in visual data. In this paper, we introduce MambaLLIE, an implicit Retinex-aware low light enhancer featuring a global-then-local state space design. We first propose a Local-Enhanced State Space Module (LESSM) that incorporates an augmented local bias within a 2D selective scan mechanism, enhancing the original SSMs by preserving local 2D dependency. Additionally, an Implicit Retinex-aware Selective Kernel module (IRSK) dynamically selects features using spatially-varying operations, adapting to varying inputs through an adaptive kernel selection process. Our Global-then-Local State Space Block (GLSSB) integrates LESSM and IRSK with layer normalization (LN) as its core. This design enables MambaLLIE to achieve comprehensive global long-range modeling and flexible local feature aggregation. Extensive experiments demonstrate that MambaLLIE significantly outperforms state-of-the-art CNN and Transformer-based methods. Our code is available at https://github.com/wengjiangwei/MambaLLIE.","['Low Light Enhancement', 'State Space Models', 'Feature Control']",[],"['Jiangwei Weng', 'Zhiqiang Yan', 'Ying Tai', 'Jianjun Qian', 'Jian Yang', 'Jun Li']","['1School of Computer Science and Engineering, Nanjing University of Science and Technology', 'School of Computing, National University of Singapore', 'Intelligence science and technology, Nanjing University', 'Nanjing University of Science and Technology, Nanjing University of Science and Techonology', 'Department of Computer Science, Nanjing University of Science and Technology', 'Nanjing University of Science and Technology']",
https://openreview.net/forum?id=lflwtGE6Vf,Fairness & Bias,(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning,"Federated Learning (FL) is a distributed machine learning framework that trains accurate global models while preserving clients' privacy-sensitive data. However, most FL approaches assume that clients possess labeled data, which is often not the case in practice. Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not. However, a significant performance gap exists between Centralized Semi-Supervised Learning (SSL) and FSSL. This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data. We propose $(FL)^2$, a robust training method for unlabeled clients using sharpness-aware consistency regularization. We show that regularizing the original pseudo-labeling loss is suboptimal, and hence we carefully select unlabeled samples for regularization. We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client. Our experiments on three benchmark datasets demonstrate that our approach significantly improves performance and bridges the gap with SSL, particularly in scenarios with scarce labeled data.","['Federated Learning', 'Semi-Supervised Learning', 'Federated Semi-Supervised Learning']",[],"['Seungjoo Lee', 'Thanh-Long V. Le', 'Jaemin Shin', 'Sung-Ju Lee']","['KAIST, Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science & Technology']",
https://openreview.net/forum?id=luQiVmnviX,Fairness & Bias,UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation,"Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness—sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs.","['LLM Bias', 'In-Context Learning', 'Attention and FFN Manipulation', 'Prompt Brittleness']",[],"['Hanzhang Zhou', 'Zijian Feng', 'Zixiao Zhu', 'Junlang Qian', 'Kezhi Mao']","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']",
https://openreview.net/forum?id=lvcWA24dxB,Fairness & Bias,MotionCraft: Physics-Based Zero-Shot Video Generation,"Generating videos with realistic and physically plausible motion is one of the main recent challenges in computer vision.  While diffusion models are achieving compelling results in image generation, video diffusion models are limited by heavy training and huge models, resulting in videos that are still biased to the training dataset. In this work we propose MotionCraft, a new zero-shot video generator to craft physics-based and realistic videos. MotionCraft is able to warp the noise latent space of an image diffusion model, such as Stable Diffusion, by applying an optical flow derived from a physics simulation. We show that warping the noise latent space results in coherent application of the desired motion while allowing the model to generate missing elements consistent with the scene evolution, which would otherwise result in artefacts or missing content if the flow was applied in the pixel space. We compare our method with the state-of-the-art Text2Video-Zero reporting qualitative and quantitative improvements, demonstrating the effectiveness of our approach to generate videos with finely-prescribed complex motion dynamics.","['Zero-shot video generation', 'diffusion model', 'physics-based video generation']",[],"['Antonio Montanaro', 'Luca Savant Aira', 'Emanuele Aiello', 'Diego Valsesia', 'Enrico Magli']","['Politecnico di Torino', 'DET, Polytechnic Institute of Turin', 'DET, Politecnico di Torino', 'Politecnico di Torino', 'Politecnico di Torino']",
https://openreview.net/forum?id=m0DS4OOmSY,Security,Should We Really Edit Language Models? On the Evaluation of Edited Language Models,"Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models.  Current methods mainly focus on reliability, generalization, and locality,  with many methods excelling across these criteria.  Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored.  In this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings. (1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits. When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged.  (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing.  (3) Language model with large scale is more resistant to editing compared to small model. (4) The safety of the edited model, is significantly weakened, even for those safety-aligned models. Our findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods.","['Model Editing', 'Language Models', 'Language Model Evaluation']",[],"['Qi Li', 'Xiang Liu', 'Zhenheng Tang', 'Peijie Dong', 'Zeyu Li', 'Xinglin Pan', 'Xiaowen Chu']","['', 'DSA, Hong Kong University of Science and Technology (Guang Zhou))', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guang Zhou)', 'INFO, The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Data Science and Analytics, Hong Kong University of Science and Technology (Guangzhou)']",
https://openreview.net/forum?id=mAdGQ1Hh3L,Fairness & Bias,START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation,"Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains. Existing DG methods primarily rely on convolutional neural networks (CNNs), which inherently learn texture biases due to their limited receptive fields, making them prone to overfitting source domains. While some works have introduced transformer-based methods (ViTs) for DG to leverage the global receptive field, these methods incur high computational costs due to the quadratic complexity of self-attention. Recently, advanced state space models (SSMs), represented by Mamba, have shown promising results in supervised learning tasks by achieving linear complexity in sequence length during training and fast RNN-like computation during inference. Inspired by this, we investigate the generalization ability of the Mamba model under domain shifts and find that input-dependent matrices within SSMs could accumulate and amplify domain-specific features, thus hindering model generalization. To address this issue, we propose a novel SSM-based architecture with saliency-based token-aware transformation (namely START), which achieves state-of-the-art (SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains. Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity. Our code is available at https://github.com/lingeringlight/START.","['Domain generalization', 'state space models']",[],"['Jintao Guo', 'Lei Qi', 'Yinghuan Shi', 'Yang Gao']","['Department of Computer Science and Technology, Nanjing University', 'School of Computer Science, Southeast University', 'School of Computer Science, Nanjing University', 'School of Intelligence Science and Technology, Nanjing University']",
https://openreview.net/forum?id=mGz3Jux9wS,Fairness & Bias,Long-tailed Object Detection Pretraining: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction,"Pre-training plays a vital role in various vision tasks, such as object recognition and detection. Commonly used pre-training methods, which typically rely on randomized approaches like uniform or Gaussian distributions to initialize model parameters, often fall short when confronted with long-tailed distributions, especially in detection tasks. This is largely due to extreme data imbalance and the issue of simplicity bias. In this paper, we introduce a novel pre-training framework for object detection, called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL). Our method builds on a Holistic-Local Contrastive Learning mechanism, which aligns pre-training with object detection by capturing both global contextual semantics and detailed local patterns. To tackle the imbalance inherent in long-tailed data, we design a dynamic rebalancing strategy that adjusts the sampling of underrepresented instances throughout the pre-training process, ensuring better representation of tail classes. Moreover, Dual Reconstruction addresses simplicity bias by enforcing a reconstruction task aligned with the self-consistency principle, specifically benefiting underrepresented tail classes. Experiments on COCO and LVIS v1.0 datasets demonstrate the effectiveness of our method, particularly in improving the mAP/AP scores for tail classes.","['Long-tailed object detection', 'pretraining']",[],"['Chen-Long Duan', 'Yong Li', 'Xiu-Shen Wei', 'Lin Zhao']","['Nanjing University of Science and Technology', 'School of Computer Science and Engineering,, Nanjing University of Science and Technology', '', 'Nanjing University of Science and Technology']",
https://openreview.net/forum?id=mH1xtt2bJE,Fairness & Bias,MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts,"Leveraging the model’s outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground-truth labels. Despite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift. In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption. Our findings motivate our proposed method \method{} that \textbf{(1)}~applies a data-dependent normalization on the logits to reduce prediction bias, and \textbf{(2)} takes the $L_p$ norm of the matrix of normalized logits as the estimation score. Our theoretical analysis highlights the connection between the provided score and the model's uncertainty.  We conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that \method{} achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts. The code is available at https://github.com/Renchunzi-Xie/MaNo.","['Unsupervised Learning', 'Distribution Shifts', 'Unsupervised Accuracy Estimation', 'Generalization', 'Deep Learning']",[],"['RENCHUNZI XIE', 'Ambroise Odonnat', 'Vasilii Feofanov', 'Weijian Deng', 'Jianfeng Zhang', 'Bo An']","['', ""Noah's Ark Lab, Huawei Technologies Ltd."", ""Huawei Noah's Ark Lab"", 'Australian National University', 'Huawei Technologies Ltd.', 'School of Computer Science and Engineering, Nanyang Technological University']",
https://openreview.net/forum?id=mVfRrMfGdY,Privacy & Data Governance,Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification,"Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy. This is traditionally formalized via mechanism-agnostic subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters. We propose the first general framework for deriving mechanism-specific guarantees, which leverage additional information beyond these parameters to more tightly characterize the subsampled mechanism's privacy. Such guarantees are of particular importance for privacy accounting, i.e., tracking privacy over multiple iterations. Overall, our framework based on conditional optimal transport lets us derive existing and novel guarantees for approximate DP, accounting with Renyi DP, and accounting with dominating pairs in a unified, principled manner. As an application, we analyze how subsampling affects the privacy of groups of multiple users. Our tight mechanism-specific bounds outperform tight mechanism-agnostic bounds and classic group privacy results.","['Differential Privacy', 'Privacy-Preserving Machine Learning', 'Amplification', 'Subsampling', 'Accounting']",[],"['Jan Schuchardt', 'Mihail Stoian', 'Arthur Kosmala', 'Stephan Günnemann']","['Department of Informatics, Technical University Munich', 'Department Engineering, University of Technology Nuremberg', 'School of Computation, Information and Technology, Technische Universität München', 'Technical University Munich']",
https://openreview.net/forum?id=mdWz5koY5p,Transparency & Explainability,RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space,"Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies.  Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return. In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss. Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).","['Multi-agent Reinforcement Learning', 'Decision Tree', 'Explainable AI']",[],"['Jingdi Chen', 'Hanhan Zhou', 'Yongsheng Mei', 'Carlee Joe-Wong', 'Gina Adam', 'Nathaniel D. Bastian', 'Tian Lan']","['Electrical and Computer Engineering, Carnegie Mellon University', 'Amazon', 'Electrical & Computer Engineering, George Washington University', 'Carnegie Mellon University', 'George Washington University', 'United States Military Academy', 'George Washington University']",
https://openreview.net/forum?id=mP084aMFsd,Transparency & Explainability,A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences,"In modern telecommunication networks, faults manifest as alarms, generating thousands of events daily. Network operators need an efficient method to identify the root causes of these alarms to mitigate potential losses. This task is challenging due to the increasing scale of telecommunication networks and the interconnected nature of devices, where one fault can trigger a cascade of alarms across multiple devices within a topological network. Recent years have seen a growing focus on causal approaches to addressing this problem, emphasizing the importance of learning a Granger causal graph from topological event sequences. Such causal graphs delineate the relations among alarms and can significantly aid engineers in identifying and rectifying faults. However, existing methods either ignore the topological relationships among devices or suffer from relatively low scalability and efficiency, failing to deliver high-quality responses in a timely manner. To this end, this paper proposes $S^2GCSL$, a simple yet scalable Granger causal structural learning approach for topological event sequences. $S^2GCSL$ utilizes a linear kernel to model activation interactions among various event types within a topological network, and employs gradient descent to efficiently optimize the likelihood function. Notably, it can seamlessly incorporate expert knowledge as constraints within the optimization process, which enhances the interpretability of the outcomes. Extensive experimental results on both large-scale synthetic and real-world problems verify the scalability and efficacy of $S^2GCSL$.","['Telecommunication Network Fault Diagnosis', 'Topological Hawkes Processes', 'Causal structure learning', 'Event Sequences', 'Scalability']",[],"['Mingjia Li', 'Shuo Liu', 'Hong Qian', 'Aimin Zhou']","['East China Normal University', 'Computer Science and Technology, National University of Singapore', 'East China Normal University', 'East China Normal University']",
https://openreview.net/forum?id=me1MpmENpw,Fairness & Bias,Semantics and Spatiality of Emergent Communication,"When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.","['emergent communication', ""Lewis' games""]",[],"['Rotem Ben Zion', 'Boaz Carmeli', 'Orr Paradise', 'Yonatan Belinkov']","['Data and Decisions Science, Technion - Israel Institute of Technology, Technion - Israel Institute of Technology', 'Technion - Israel Institute of Technology', 'EECS, University of California, Berkeley', 'Technion, Technion']",
https://openreview.net/forum?id=mwN1bbD5DQ,Fairness & Bias,Learning De-Biased Representations for Remote-Sensing Imagery,"Remote sensing (RS) imagery, which requires specialized satellites to collect and is difficult to annotate, suffers from data scarcity and class imbalance in certain spectrums. Due to their data scarcity, training large-scale RS models from scratch is unrealistic, and the alternative is to transfer pre-trained models by fine-tuning or a more data-efficient method LoRA. Due to class imbalance, transferred models exhibit strong bias, where features of the major class dominate over those of the minor class. In this paper, we propose debLoRA, a generic training approach that works with any LoRA variants to yield debiased features. It is an unsupervised learning approach that can diversify minor class features based on the shared attributes with major classes, where the attributes are obtained by a simple step of clustering. To evaluate it, we conduct extensive experiments in two transfer learning scenarios in the RS domain: from natural to optical RS images, and from optical RS to multi-spectrum RS images. We perform object classification and oriented object detection tasks on the optical RS dataset DOTA and the SAR dataset FUSRS. Results show that our debLoRA consistently surpasses prior arts across these RS adaptation settings, yielding up to 3.3 and 4.7 percentage points gains on the tail classes for natural $\to$ optical RS and optical RS $\to$ multi-spectrum RS adaptations, respectively, while preserving the performance on head classes, substantiating its efficacy and adaptability","['Adaptation', 'Long-tailed learning', 'Remote Sensing']",[],"['Zichen Tian', 'Zhaozheng Chen', 'Qianru Sun']","['', 'Singapore Management University', 'School of Computing and Information Systems, Singapore Management University']",
https://openreview.net/forum?id=n01yLUy7Mj,Transparency & Explainability,Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge,"Contrastive Language-Image Pretraining (CLIP) performs zero-shot image classification by mapping images and textual class representation into a shared embedding space, then retrieving the class closest to the image. This work provides a new approach for interpreting CLIP models for image classification from the lens of mutual knowledge between the two modalities. Specifically, we ask: what concepts do both vision and language CLIP encoders learn in common that influence the joint embedding space, causing points to be closer or further apart? We answer this question via an approach of textual concept-based explanations, showing their effectiveness, and perform an analysis encompassing a pool of 13 CLIP models varying in architecture, size and pretraining datasets. We explore those different aspects in relation to mutual knowledge, and analyze zero-shot predictions. Our approach demonstrates an effective and human-friendly way of understanding zero-shot classification decisions with CLIP.","['CLIP', 'vision-language', 'explainability', 'interpretability', 'mutual knowledge']",[],"['Fawaz Sammani', 'Nikos Deligiannis']","['Max-Planck Institute for Informatics, Saarland Informatics Campus, Max-Planck Institute', 'IMEC']",
https://openreview.net/forum?id=nLSLbJgL7f,Fairness & Bias,To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation,"Accuracy is a commonly adopted performance metric in various classification tasks, which measures the proportion of correctly classified samples among all samples. It assumes equal importance for all classes, hence equal severity for misclassifications. However, in the task of emotional classification, due to the psychological similarities between emotions, misclassifying a certain emotion into one class may be more severe than another, e.g., misclassifying 'excitement' as 'anger' apparently is more severe than as 'awe'. Albeit high meaningful for many applications, metrics capable of measuring these cases of misclassifications in visual emotion recognition tasks have yet to be explored. In this paper, based on Mikel's emotion wheel from psychology, we propose a novel approach for evaluating the performance in visual emotion recognition, which takes into account the distance on the emotion wheel between different emotions to mimic the psychological nuances of emotions. Experimental results in semi-supervised learning on emotion recognition and user study have shown that our proposed metrics is more effective than the accuracy to assess the performance and conforms to the cognitive laws of human emotions. The code is available at https://github.com/ZhaoChenxi-nku/ECC.","['visual emotion recognition', 'evaluation measure']",[],"['Chenxi Zhao', 'Jinglei Shi', 'Liqiang Nie', 'Jufeng Yang']","['TianJin, Nankai University', 'School of Computer Science, Nankai University', 'Harbin Institute of Technology (Shenzhen)', 'College of Computer Science, Nankai University']",
https://openreview.net/forum?id=nRdST1qifJ,Security,Fight Back Against Jailbreaking via Prompt Adversarial Tuning,"While Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to jailbreaking attacks. Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs. However, how to achieve intrinsic robustness through prompt optimization remains an open problem. In this paper, motivated by adversarial training paradigms for achieving reliable robustness, we propose an approach named **Prompt Adversarial Tuning (PAT)** that trains a prompt control attached to the user prompt as a guard prefix. To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts. Comprehensive experiments show that our method is effective against both grey-box and black-box attacks, reducing the success rate of advanced attacks to nearly 0, while maintaining the model's utility on the benign task and incurring only negligible computational overhead, charting a new perspective for future explorations in LLM security. Our code is available at https://github.com/PKU-ML/PAT.","['Large Language Model', 'Jailbreak Defense', 'Prompt Tuning']",[],"['Yichuan Mo', 'Yuji Wang', 'Zeming Wei', 'Yisen Wang']","['School of Intelligence Science and Technology, Peking University', 'Shanghai Jiaotong University', 'School of mathematical Science, Peking University', 'Peking University']",
https://openreview.net/forum?id=nWMqQHzI3W,Security,SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions,"Neural Control Barrier Functions (NCBFs) have shown significant promise in enforcing safety constraints on nonlinear autonomous systems. State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewise-linear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost. In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV). Our framework consists of two components, namely (i) an NCBF synthesis algorithm that introduces a novel regularizer to reduce the number of activation regions at the safety boundary, and (ii) a verification algorithm that exploits tight over-approximations of the safety conditions to reduce the cost of verifying each piecewise-linear segment. Our simulations show that SEEV significantly improves verification efficiency while maintaining the CBF quality across various benchmark systems and neural network structures. Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.","['Safe Control', 'Barrier Functions', 'Control Barrier Functions', 'Neural Networks']",[],"['Hongchao Zhang', 'Zhizhen Qin', 'Sicun Gao', 'Andrew Clark']","['Washington University, Saint Louis', 'University of California, San Diego', 'University of California, San Diego', 'Washington University, Saint Louis']",
https://openreview.net/forum?id=nxumYwxJPB,Security,"If You Want to Be Robust, Be Wary of Initialization","Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model’s robustness. We introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model’s vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks. Extensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\% compared to alternative initialization approaches.","['Adversarial Robustness', 'Graph Neural Networks']",[],"['Sofiane ENNADIR', 'Johannes F. Lutzeyer', 'Michalis Vazirgiannis', 'El houcine Bergou']","['EECS, KTH Royal Institute of Technology', 'Ecole Polytechique', 'Ecole Polytechnique, France', 'KAUST']",
https://openreview.net/forum?id=o7DOGbZeyP,Security,LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate,"High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning — ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.  We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) — on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.","['vision transformers', 'position encoding', 'computer vision']",[],"['Anthony Fuller', 'Daniel Kyrollos', 'Yousef Yassin', 'James R Green']","['Carleton University', 'Systems and Computer, Carleton University', 'Faculty of Engineering and Design, Carleton University', 'Carleton University']",
https://openreview.net/forum?id=o4coDIby7e,Transparency & Explainability,Measuring Goal-Directedness,"We define maximum entropy goal-directedness (MEG), a formal measure of goal- directedness in causal models and Markov decision processes, and give algorithms for computing it. Measuring goal-directedness is important, as it is a critical element of many concerns about harm from AI. It is also of philosophical interest, as goal-directedness is a key aspect of agency. MEG is based on an adaptation of the maximum causal entropy framework used in inverse reinforcement learning. It can measure goal-directedness with respect to a known utility function, a hypothesis class of utility functions, or a set of random variables. We prove that MEG satisfies several desiderata and demonstrate our algorithms with small-scale experiments.","['Causality', 'Graphical Models', 'Maximum Causal Entropy', 'Agency']",[],"['Matt MacDermott', 'James Fox', 'Francesco Belardinelli', 'Tom Everitt']","['Mila - Quebec Artificial Intelligence Institute', 'Department of Computer Science', 'Imperial College London', 'DeepMind']",
https://openreview.net/forum?id=oMHpejyGdx,Security,Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models,"Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using “prompt-specific methods” to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts. In this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution. This approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability. Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of our method in comparison to existing techniques.","['Adversarial perturbations', 'customized diffusion models', 'privacy protection', 'prompt distribution modeling']",[],"['Cong Wan', 'Yuhang He', 'Xiang Song', 'Yihong Gong']","[""Xi'an Jiaotong University"", ""College of Artificial Intelligence, Xi'an Jiaotong University"", ""School of Software Engineering, Xi'an Jiaotong University"", ""School of Software  Engineering, Xi'an Jiaotong University""]",
https://openreview.net/forum?id=oTzydUKWpq,Transparency & Explainability,Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level,"Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats. Text-attributed graphs (TAGs), where nodes are associated with textual features, are crucial due to their prevalence in real-world applications and are commonly used to evaluate these vulnerabilities. However, existing research only focuses on embedding-level GIAs, which inject node embeddings rather than actual textual content, limiting their applicability and simplifying detection. In this paper, we pioneer the exploration of GIAs at the text level, presenting three novel attack designs that inject textual content into the graph. Through theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength.  Among the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability.  Despite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors.  These insights underscore the necessity for further research into the potential and practical significance of text-level GIAs.","['Graph Neural Networks', 'Graph Adversarial Attack', 'Graph Injection Attack']",[],"['Runlin Lei', 'Yuwei Hu', 'Yuchen Ren', 'Zhewei Wei']","['Renmin University of China, Renmin University of China', 'Gaoling School of Artificial Intelligence, Renmin University of China', '', 'Gaoling School of Artificial Intelligence, Renmin University of China']",
https://openreview.net/forum?id=MPidsCd9e7,Privacy & Data Governance,Adversarially Robust Dense-Sparse Tradeoffs via Heavy-Hitters,"In the adversarial streaming model, the input is a sequence of adaptive updates that defines an underlying dataset and the goal is to approximate, collect, or compute some statistic while using space sublinear in the size of the dataset. In 2022, Ben-Eliezer, Eden, and Onak showed a dense-sparse trade-off technique that elegantly combined sparse recovery with known techniques using differential privacy and sketch switching to achieve adversarially robust algorithms for $L_p$ estimation and other algorithms on turnstile streams. However, there has been no progress since, either in terms of achievability or impossibility. In this work, we first give improved algorithms for adversarially robust $L_p$-heavy hitters, utilizing deterministic turnstile heavy-hitter algorithms with better tradeoffs. We then utilize our heavy-hitter algorithm to reduce the problem to estimating the frequency moment of the tail vector. We give a new algorithm for this problem in the classical streaming setting, which achieves additive error and uses space independent in the size of the tail. We then leverage these ingredients to give an improved algorithm for adversarially robust $L_p$ estimation on turnstile streams. We believe that our results serve as an important conceptual message, demonstrating that there is no inherent barrier at the previous state-of-the-art.","['streaming algorithms', 'adversarial robustness', 'heavy-hitters', 'norm estimation', 'differential privacy']",[],"['David Woodruff', 'Samson Zhou']","['Carnegie Mellon University', 'Texas A&M University - College Station']",
https://openreview.net/forum?id=oX6aIl9f0Y,Privacy & Data Governance,Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions,"We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a $k^{\text{th}}$-moment bound on the Lipschitz constants of sample functions, rather than a uniform bound. We propose a new reduction-based approach that enables us to obtain the first optimal rates (up to logarithmic factors) in the heavy-tailed setting, achieving error $G_2 \cdot \frac 1 {\sqrt n} + G_k \cdot (\frac{\sqrt d}{n\epsilon})^{1 - \frac 1 k}$ under $(\epsilon, \delta)$-approximate differential privacy, up to a mild $\textup{polylog}(\frac{1}{\delta})$ factor, where $G_2^2$ and $G_k^k$ are the $2^{\text{nd}}$ and $k^{\text{th}}$ moment bounds on sample Lipschitz constants, nearly-matching a lower bound of [LR23]. We then give a suite of private algorithms for DP-SCO with heavy-tailed gradients improving our basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models.","['Stochastic Convex Optimization', 'Heavy-Tailed Distributions', 'Differential Privacy']",[],"['Hilal Asi', 'Daogao Liu', 'Kevin Tian']","['Apple', 'University of Washington, Seattle', 'University of Texas at Austin']",
https://openreview.net/forum?id=ogaeChzbKu,Security,RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees,"Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, referred to as RAW. As a departure from existing encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework also provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of adversarial attacks targeting watermark removal.  Experiments on a diverse range of images generated by state-of-the-art diffusion models demonstrate substantially improved watermark encoding speed and watermark detection performance, under adversarial attacks, while maintaining image quality. Our code is publicly available [here](https://github.com/jeremyxianx/RAWatermark).","['Copyright Protection', 'Privacy']",[],"['Xun Xian', 'Ganghua Wang', 'Xuan Bi', 'Jayanth Srinivasa', 'Ashish Kundu', 'Mingyi Hong', 'Jie Ding']","['College of Science & Engineering, University of Minnesota, Minneapolis', 'Data Science, University of Chicago', 'University of Minnesota - Twin Cities', 'Cisco', '', 'AGI, Amazon', 'University of Minnesota - Twin Cities']",
https://openreview.net/forum?id=opt72TYzwZ,Transparency & Explainability,Optimal ablation for interpretability,"Interpretability studies often involve tracing the flow of information through machine learning models to identify specific model components that perform relevant computations for tasks of interest. Prior work quantifies the importance of a model component on a particular task by measuring the impact of performing ablation on that component, or simulating model inference with the component disabled.  We propose a new method, optimal ablation (OA), and show that OA-based component importance has theoretical and empirical advantages over measuring importance via other ablation methods. We also show that OA-based component importance can benefit several downstream interpretability tasks, including circuit discovery, localization of factual recall, and latent prediction.","['mechanistic intepretability', 'model internals', 'ablation', 'activation patching', 'automatic circuit discovery', 'causal tracing', 'tuned lens']",[],"['Maximilian Li', 'Lucas Janson']","['Harvard University', 'Statistics, Harvard University']",
https://openreview.net/forum?id=p3tSEFMwpG,Fairness & Bias,Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data,"While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance. Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts. To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass. Specifically, it learns to approximate Bayesian inference on synthetic datasets drawn from a prior that specifies the model's inductive bias. This prior is based on structural causal models (SCM), which gradually shift over time. To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters. The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning. Comprehensive evaluations across 18 synthetic and real-world datasets demonstrate large performance improvements over a wide range of baselines, such as XGB, CatBoost, TabPFN, and applicable methods featured in the Wild-Time benchmark. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration. This approach could serve as significant groundwork for further research on out-of-distribution prediction.","['Temporal Distribution Shifts', 'In-Context Learning', 'Bayesian Inference', 'Prior-Data Fitted Networks', 'Temporal Domain Generalization', 'Structural Causal Models', 'TabPFN', 'Tabular Data Modeling', 'Out-Of-Distribution Generalization', 'Domain Generalization', 'Meta-Learning', 'Concept Drift']",[],"['Kai Helli', 'David Schnurr', 'Noah Hollmann', 'Samuel Müller', 'Frank Hutter']","['TUM School of Computation, Information and Technology (CIT), Technische Universität München', 'D-INFK, ETHZ - ETH Zurich', 'Albert-Ludwigs-Universität Freiburg', 'University of Freiburg, Universität Freiburg', 'ELLIS Institute Tübingen & University of Freiburg']",
https://openreview.net/forum?id=pJlFURyTG5,Security,Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning,"A challenging problem in seeking to bring multi-agent reinforcement learning (MARL) techniques into real-world applications, such as autonomous driving and drone swarms, is how to control multiple agents safely and cooperatively to accomplish tasks. Most existing safe MARL methods learn the centralized value function by introducing a global state to guide safety cooperation. However, the global coupling arising from agents’ safety constraints and the exponential growth of the state-action space size limit their applicability in instant communication or computing resource-constrained systems and larger multi-agent systems. In this paper, we develop a novel scalable and theoretically-justified multi-agent constrained policy optimization method. This method utilizes the rigorous bounds of the trust region method and the bounds of the truncated advantage function to provide a new local policy optimization objective for each agent. Also, we prove that the safety constraints and the joint policy improvement can be met when each agent adopts a sequential update scheme to optimize a $\kappa$-hop policy. Then, we propose a practical algorithm called Scalable MAPPO-Lagrangian (Scal-MAPPO-L). The proposed method’s effectiveness is verified on a collection of benchmark tasks, and the results support our theory that decentralized training with local interactions can still improve reward performance and satisfy safe constraints.","['Multi-agent reinforcement learning', 'policy optimization', 'safe learning', 'scalable method']",[],"['Lijun Zhang', 'Lin Li', 'Wei Wei', 'Huizhong Song', 'Yaodong Yang', 'Jiye Liang']","['School of Computer and Information Technology, Shanxi University', 'Shanxi University', 'School of Computer and Information Technology, shanxi university', 'Shanxi University', 'Peking University', 'School of Computer and Information Technology, Shanxi University']",
https://openreview.net/forum?id=pH3XAQME6c,Security,Refusal in Language Models Is Mediated by a Single Direction,"Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities. This interpretable rank-one weight edit results in an effective jailbreak technique that is simpler and more efficient than fine-tuning. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.","['mechanistic interpretability', 'refusal', 'jailbreaks', 'language models', 'steering vectors', 'representation engineering']",[],"['Andy Arditi', 'Oscar Balcells Obeso', 'Aaquib Syed', 'Daniel Paleka', 'Nina Rimsky', 'Wes Gurnee', 'Neel Nanda']","['Computer Science, University of Chicago', 'ETHZ - ETH Zurich', 'University of Maryland, College Park', 'Department of Computer Science, ETHZ - ETH Zurich', 'Anthropic', 'Massachusetts Institute of Technology', 'Google DeepMind']",
https://openreview.net/forum?id=pGEY8JQ3qx,Fairness & Bias,Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs,"We study the sample complexity of learning an $\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\widetilde{O}\left(SA\frac{\mathsf{H}}{\varepsilon^2} \right)$, where $\mathsf{H}$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,\mathsf{H}$, and $\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We also initiate the study of sample complexity in general (multichain) average-reward MDPs. We argue a new transient time parameter $\mathsf{B}$ is necessary, establish an $\widetilde{O}\left(SA\frac{\mathsf{B} + \mathsf{H}}{\varepsilon^2} \right)$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To optimally analyze this reduction, we develop improved bounds for $\gamma$-discounted MDPs, showing that $\widetilde{O}\left(SA\frac{\mathsf{H}}{(1-\gamma)^2\varepsilon^2} \right)$ and $\widetilde{O}\left(SA\frac{\mathsf{B} + \mathsf{H}}{(1-\gamma)^2\varepsilon^2} \right)$ samples suffice to learn $\varepsilon$-optimal policies in weakly communicating and in general MDPs, respectively. Both these results circumvent the well-known minimax lower bound of $\widetilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\varepsilon^2} \right)$ for $\gamma$-discounted MDPs, and establish a quadratic rather than cubic horizon dependence for a fixed MDP instance.","['reinforcement learning theory', 'average reward', 'sample complexity']",[],"['Matthew Zurek', 'Yudong Chen']","['', 'Department of Computer Sciences, University of Wisconsin - Madison']",
https://openreview.net/forum?id=paYwtPBpyZ,Fairness & Bias,Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Generation,"Proteins are essential for almost all biological processes and derive their diverse functions from complex $3 \rm D$ structures, which are in turn determined by their amino acid sequences.  In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow++, a novel sequence-conditioned $\text{SE}(3)$-equivariant flow matching model for protein structure generation. FoldFlow++ presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase  diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow++ at scale on a new dataset  that is an order of magnitude  larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow++ to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow++ outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow++ makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.","['Proteins', 'Flow Matching', 'Generative Models']",[],"['Guillaume Huguet', 'James Vuckovic', 'Kilian FATRAS', 'Eric Thibodeau-Laufer', 'Pablo Lemos', 'Riashat Islam', 'Cheng-Hao Liu', 'Jarrid Rector-Brooks', 'Tara Akhound-Sadegh', 'Michael M. Bronstein', 'Alexander Tong', 'Joey Bose']","['University of Montreal', 'Microsoft', 'McGill University', 'dreamfold, Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'Saudi Data and AI Authority, Saudi Data and AI Authority', 'Dreamfold', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'McGill University', 'University of Oxford', 'Mila', 'Computer Science, University of Oxford']",
https://openreview.net/forum?id=pqD7ckR8AF,Security,SuperDeepFool: a new fast and accurate minimal adversarial attack,"Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal $\ell_{2}$ adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal $\ell_{2}$ adversarial perturbations.","['Deep Learning', 'Adversarial Attacks', 'Robustness', 'Interpretable AI', 'ML Security']",[],"['Alireza Abdolahpourrostam', 'Mahed Abroshan', 'Seyed-Mohsen Moosavi-Dezfooli']","['IC, EPFL - EPF Lausanne', 'Imperial College London', 'Apple Inc.']",
https://openreview.net/forum?id=piOzFx9whU,Transparency & Explainability,Wasserstein Distributionally Robust Optimization through the Lens of Structural Causal Models and Individual Fairness,"In recent years, Wasserstein Distributionally Robust Optimization (DRO) has garnered substantial interest for its efficacy in data-driven decision-making under distributional uncertainty. However, limited research has explored the application of DRO to address individual fairness concerns, particularly when considering causal structures and discrete sensitive attributes in learning problems. To address this gap, we first formulate the DRO problem from the perspectives of causality and individual fairness. We then present the DRO dual formulation as an efficient tool to convert the main problem into a more tractable and computationally efficient form. Next, we characterize the closed form of the approximate worst-case loss quantity as a regularizer, eliminating the max-step in the Min-Max DRO problem. We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization. Finally, by removing the assumption of a known structural causal model, we provide finite sample error bounds when designing DRO with empirical distributions and estimated causal structures to ensure efficiency and robust learning.","['Wasserstein Distributionally Robust Optimization', 'Individual Fairness', 'Structural Causal Model', 'Regularized Optimization']",[],"['Ahmad Reza Ehyaei', 'Golnoosh Farnadi', 'Samira Samadi']","['Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'School of Computer Science, McGill University', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute']",
https://openreview.net/forum?id=plH8gW7tPQ,Transparency & Explainability,Algorithmic Capabilities of Random Transformers,"Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained.","['transformer', 'deep learning', 'interpretability', 'capability', 'emergence', 'randomness', 'language models']",[],"['Ziqian Zhong', 'Jacob Andreas']","['Computer Science, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=pqYceEa87j,Fairness & Bias,Spectral Editing of Activations for Large Language Model Alignment,"Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.","['Large Language Model', 'Alignment', 'Spectral Decomposition', 'Representation Engineering', 'Model Editing']",[],"['Yifu QIU', 'Zheng Zhao', 'Yftah Ziser', 'Anna Korhonen', 'Edoardo Ponti', 'Shay B Cohen']","['University of Edinburgh, University of Edinburgh', 'University of Edinburgh, University of Edinburgh', 'Research, NVIDIA', 'University of Cambridge', 'Informatics, University of Edinburgh', 'University of Edinburgh']",
https://openreview.net/forum?id=psG4LXlDNs,Security,Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process,"We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{1}{\Delta\cdot\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\Delta$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.","['constrained MDP', 'reinforcement learning', 'online linear programming']",[],"['Jiashuo Jiang', 'Yinyu Ye']","['Hong Kong University of Science and Technology', 'Stanford University']",
https://openreview.net/forum?id=pwKkNSuuEs,Transparency & Explainability,Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification,"In time-series analysis, many recent works seek to provide a unified view and representation for time-series across multiple domains, leading to the development of foundation models for time-series data. Despite diverse modeling techniques, existing models are black boxes and fail to provide insights and explanations about their representations. In this paper, we present VQShape, a pre-trained, generalizable, and interpretable model for time-series representation learning and classification. By introducing a novel representation for time-series data, we forge a connection between the latent space of VQShape and shape-level features. Using vector quantization, we show that time-series from different domains can be described using a unified set of low-dimensional codes, where each code can be represented as an abstracted shape in the time domain. On classification tasks, we show that the representations of VQShape can be utilized to build interpretable classifiers, achieving comparable performance to specialist models. Additionally, in zero-shot learning, VQShape and its codebook can generalize to previously unseen datasets and domains that are not included in the pre-training process. The code and pre-trained weights are available at https://github.com/YunshiWen/VQShape.","['Time-series', 'Interpretability', 'Self-supervised Learning', 'Pre-trained Model']",[],"['Yunshi Wen', 'Tengfei Ma', 'Tsui-Wei Weng', 'Lam M. Nguyen', 'Anak Agung Julius']","['ECSE, Rensselaer Polytechnic Institute', 'State University of New York at Stony Brook', 'University of California, San Diego', 'IBM Research, Thomas J. Watson Research Center', 'Rensselaer Polytechnic Institute']",
https://openreview.net/forum?id=qAP6RyYIJc,Security,Stealth edits to large language models,"We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining. Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious *stealth attacks*. This metric is fundamental to predicting the success of a variety of editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these as *stealth editing* methods, because they directly update a model's weights to specify its response to specific known hallucinating prompts without affecting other model behaviour. By carefully applying our theoretical insights, we are able to introduce a new *jet-pack* network block which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. We also reveal the vulnerability of language models to stealth attacks: a small change to a model's weights which fixes its response to a single attacker-chosen prompt. Stealth attacks are computationally simple, do not require access to or knowledge of the model's training data, and therefore represent a potent yet previously unrecognised threat to redistributed foundation models. Extensive experimental results illustrate and support our methods and their theoretical underpinnings. Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.","['large language models', 'stealth attacks', 'memory editing']",[],"['Oliver Sutton', 'Qinghua Zhou', 'Wei Wang', 'Desmond Higham', 'Alexander N. Gorban', 'Alexander Bastounis', 'Ivan Y Tyukin']","[""King's College London, University of London"", '', 'School of Computing and Mathematical Sciences, University of Leicester', 'School of Mathematics, University of Edinburgh', 'School of Computing and Mathematical Sciences, University of Leicester: Leicester, Leicestershire, GB', 'University of Leicester', ""King's College London, University of London""]",
https://openreview.net/forum?id=qDuqp1nZZ6,Privacy & Data Governance,Differentially Private Equivalence Testing for Continuous Distributions and Applications,"We present the first algorithm for testing equivalence  between two continuous distributions using differential privacy (DP). Our algorithm is a private version of the algorithm of Diakonikolas et al.  The algorithm of Diakonikolas et al uses the data itself to repeatedly discretize the real line so that --- when the two distributions are far apart in ${\cal A}_k$-norm --- one of the discretized distributions exhibits large $L_2$-norm difference; and upon repeated sampling such large gap would be detected. Designing its private analogue poses two difficulties. First, our DP algorithm can not resample new datapoints as a change to a single datapoint may lead to a very large change in the descretization of the real line. In contrast, the (sorted) index of the discretization point changes only by $1$ between neighboring instances, and so we use a novel algorithm that set the discretization points using random Bernoulli noise, resulting in only a few buckets being affected under the right coupling. Second, our algorithm, which doesn't resample data, requires we also revisit the utility analysis of the original algorithm and prove its correctness w.r.t. the original sorted data; a problem we tackle using sampling a subset of Poisson-drawn size from each discretized bin. Lastly, since any distribution can be reduced to a continuous distribution, our algorithm is successfully carried to multiple other families of distributions and thus has numerous applications.","['Differential Privacy', 'Equivalence Tester', 'Continuous Distributions']",[],"['Or Sheffet', 'Daniel Omer']","['University of Alberta', 'Mathematics, Bar-Ilan University']",
https://openreview.net/forum?id=qXidsICaja,Transparency & Explainability,Expert-level protocol translation for self-driving labs,"Recent development in Artificial Intelligence (AI) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, we propose a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories.","['Self-Driving Laboratories', 'Domain-Specific Language', 'Structural Representation', 'Knowledge Externalization']",[],"['Yu-Zhe Shi', 'Fanxu Meng', 'Haofei Hou', 'Zhangqian Bi', 'Qiao Xu', 'Lecheng Ruan', 'Qining Wang']","['Hong Kong University of Science and Technology', 'College of  Engineering, Peking University', 'College of Engineering, Peking University', 'Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', '', 'Peking University']",
https://openreview.net/forum?id=qZFshkbWDo,Security,"Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense","Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as they allow attackers to manipulate model predictions with backdoor triggers. To address these security vulnerabilities, various backdoor purification methods have been proposed to purify compromised models. Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs. However, \textit{Does achieving a low ASR through current safety purification methods truly eliminate learned backdoor features from the pretraining phase?} In this paper, we provide an affirmative answer to this question by thoroughly investigating the \textit{Post-Purification Robustness} of current backdoor purification methods. We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples. Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models. We find the failure to achieve satisfactory post-purification robustness stems from the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. To improve the post-purification robustness, we propose a straightforward tuning defense, Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates. Extensive experiments demonstrate that PAM significantly improves post-purification robustness while maintaining a good clean accuracy and low ASR. Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.","['Backdoor Safety', 'Safety Tuning', 'Superficial Safety']",[],"['Rui Min', 'Zeyu Qin', 'Nevin L. Zhang', 'Li Shen', 'Minhao Cheng']","['Hong Kong University of Science and Technology', 'CSE, The Hong Kong University of Science and Technology', 'Department of Computer Science and Engineering, The Hong Kong University of Science and Technology', 'Sun Yat-Sen University', 'College of Information Sciences and Technology, Pennsylvania State University']",
https://openreview.net/forum?id=r8YntmAd0g,Privacy & Data Governance,DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction,"Privacy is a growing concern in modern deep-learning systems and applications. Differentially private (DP) training prevents the leakage of sensitive information in the collected training data from the trained machine learning models. DP optimizers, including DP stochastic gradient descent (DPSGD) and its variants, privatize the training procedure by gradient clipping and *DP noise* injection. However, in practice, DP models trained using DPSGD and its variants often suffer from significant model performance degradation. Such degradation prevents the application of DP optimization in many key tasks, such as foundation model pretraining. In this paper, we provide a novel *signal processing perspective* to the design and analysis of DP optimizers. We show that a ''frequency domain'' operation called *low-pass filtering* can be used to effectively reduce the impact of DP noise.  More specifically, by defining the ''frequency domain'' for both the gradient and differential privacy (DP) noise, we have developed a new component, called DOPPLER. This component is designed for DP algorithms and works by effectively amplifying the gradient while suppressing DP noise within this frequency domain. As a result, it maintains privacy guarantees and enhances the quality of the DP-protected model. Our experiments show that the proposed DP optimizers with a low-pass filter outperform their counterparts without the filter on various models and datasets. Both theoretical and practical evidence suggest that the  DOPPLER is effective in closing the gap between DP and non-DP training.","['differential privacy', 'optimization', 'low-pass filter', 'signal processing']",[],"['Xinwei Zhang', 'Zhiqi Bu', 'Mingyi Hong', 'Meisam Razaviyayn']","['University of Southern California', 'Amazon', 'AGI, Amazon', 'University of Southern California']",
https://openreview.net/forum?id=rIOl7KbSkv,Security,No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices,"Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating the misuse of such AI-generated content. However, we show that common design choices in LLM watermarking schemes make the resulting systems  surprisingly susceptible to attack---leading to fundamental trade-offs in robustness, utility, and usability.  To navigate these trade-offs, we rigorously study a set of simple yet effective attacks on common watermarking systems, and propose  guidelines and defenses for LLM watermarking in practice.","['watermarking', 'large language models', 'security', 'privacy']",[],"['Qi Pang', 'Shengyuan Hu', 'Wenting Zheng', 'Virginia Smith']","['', 'Machine Learning Department, Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']",
https://openreview.net/forum?id=rXGxbDJadh,Security,Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor,"Vision-and-Language Navigation (VLN) requires an agent to dynamically explore environments following natural language. The VLN agent, closely integrated into daily lives, poses a substantial threat to the security of privacy and property upon the occurrence of malicious behavior. However, this serious issue has long been overlooked. In this paper, we pioneer the exploration of an object-aware backdoored VLN, achieved by implanting object-aware backdoors during the training phase.  Tailored to the unique VLN nature of cross-modality and continuous decision-making, we propose a novel backdoored VLN paradigm: IPR Backdoor.  This enables the agent to act in abnormal behavior once encountering the object triggers during language-guided navigation in unseen environments, thereby executing an attack on the target scene. Experiments demonstrate the effectiveness of our method in both physical and digital spaces across different VLN agents, as well as its robustness to various visual and textual variations. Additionally, our method also well ensures navigation performance in normal scenarios with remarkable stealthiness.","['Vision-and-Language Navigation', 'Multimodal', 'Continous Decision-Making', 'Backdoor Attack']",[],"['Keji He', 'Kehan Chen', 'Jiawang Bai', 'Yan Huang', 'Qi Wu', 'Shu-Tao Xia', 'Liang Wang']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of the Chinese Academy of Sciences', 'Tsinghua University, Tsinghua University', 'NLPR, Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'ComputerScience, The University of Adelaide', 'Division of Information Science and Technology, Shenzhen International Graduate School, Tsinghua University', 'NLPR, Institute of Automation， CAS，China']",
https://openreview.net/forum?id=rbtnRsiXSN,Security,DeMo: Decoupling Motion Forecasting into  Directional Intentions and Dynamic States,"Accurate motion forecasting for traffic agents is crucial for ensuring the safety and efficiency of autonomous driving systems in dynamically changing environments. Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories. While straightforward and effective, the absence of detailed representation of future trajectories may yield suboptimal outcomes, given that the agent states dynamically evolve over time. To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time. By leveraging this format, we separately optimize the multi-modality and dynamic evolutionary properties of trajectories. Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories. To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths. Extensive experiments on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo achieves state-of-the-art performance in motion forecasting. In addition, we will make our code and models publicly available.","['Motion Forecasting', 'Autonomous Driving', 'Mamba', 'Attention']",[],"['Bozhou Zhang', 'Nan Song', 'Li Zhang']","['Fudan University', 'Fudan University', 'Fudan University']",
https://openreview.net/forum?id=re2jPCnzkA,Transparency & Explainability,MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs,"Providing functionality through articulation and interaction with objects is a key objective in 3D generation. We introduce MIDGArD (Modular Interpretable Diffusion over Graphs for Articulated Designs), a novel diffusion-based framework for articulated 3D asset generation. MIDGArD improves over foundational work in the field by enhancing quality, consistency, and controllability in the generation process. This is achieved through MIDGArD's modular approach that separates the problem into two primary components: structure generation and shape generation. The structure generation module of MIDGArD aims at producing coherent articulation features from noisy or incomplete inputs. It acts on the object's structural and kinematic attributes, represented as features of a graph that are being progressively denoised to issue coherent and interpretable articulation solutions. This denoised graph then serves as an advanced conditioning mechanism for the shape generation module, a 3D generative model that populates each link of the articulated structure with consistent 3D meshes. Experiments show the superiority of MIDGArD on the quality, consistency, and interpretability of the generated assets. Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics.","['3D articulated objects', 'diffusion models', 'generative models']",[],"['Quentin Leboutet', 'Nina Wiedemann', 'zhipeng cai', 'Michael Paulitsch', 'Kai Yuan']","['Intel', 'Intel', 'Intel Labs, Intel', 'Intel', 'Intel']",
https://openreview.net/forum?id=rkuVYosT2c,Fairness & Bias,Distributed Least Squares in Small Space via Sketching and Bias Reduction,"Matrix sketching is a powerful tool for reducing the size of large data matrices. Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression. We show that these limitations can be circumvented in the distributed setting by designing sketching methods that minimize the bias of the estimator, rather than its error. In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data. This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches. Our key novelty is a new bias analysis for sketched least squares, giving a sharp characterization of its dependence on the sketch sparsity. The techniques include new higher moment restricted Bai-Silverstein inequalities, which are of independent interest to the non-asymptotic analysis of deterministic equivalents for random matrices that arise from sketching.","['Matrix Sketching', 'Least squares', 'Randomized Linear Algebra', 'Random Matrix Theory']",[],"['Sachin Garg', 'Kevin Tan', 'Michal Derezinski']","['University of Michigan - Ann Arbor', 'Statistics and Data Science, Wharton Statistics Department, The Wharton School', 'University of Michigan - Ann Arbor']",
https://openreview.net/forum?id=re0ly2Ylcu,Fairness & Bias,Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context,"When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Although several empirical studies have investigated the rationality and social behavior performance of LLMs, their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics theories, to evaluate the decision-making behaviors of LLMs. With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs. Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics.","['Large Language Model (LLM)', 'LLM Reasoning', 'Fairness', 'Ethical AI', 'AI Decision-Making', 'LLM Prompting']",[],"['Jingru Jia', 'Zehua Yuan', 'Junhao Pan', 'Paul E McNamara', 'Deming Chen']","['University of Illinois at Urbana-Champaign', 'Electrical and Computer Engineering, University of Illinois at Urbana Champaign', 'Computer Engineering, University of Illinois at Urbana Champaign', 'Agricultural & Consumer Economics, University of Illinois at Urbana-Champaign', '']",
https://openreview.net/forum?id=rYjYwuM6yH,Transparency & Explainability,"3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability","Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with <0.1% trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.","['parameter-efficient finetuning', 'orthogonal finetuning', 'batching', 'interpretability']",[],"['Baohao Liao', 'Christof Monz']","['University of Amsterdam', 'Informatics Institue, University of Amsterdam, University of Amsterdam']",
https://openreview.net/forum?id=rpZWSDjc4N,Transparency & Explainability,FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors,"LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability. Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors. In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors. FFAM employs non-negative matrix factorization to generate concept activation maps and subsequently aggregates these maps to obtain a global visual explanation. To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object. Additionally, we introduce a voxel upsampling strategy to align the scale between the activation map and input point cloud. We qualitatively and quantitatively analyze FFAM with multiple detectors on several datasets. Experimental results validate the high-quality visual explanations produced by FFAM. The code is available at \url{https://anonymous.4open.science/r/FFAM-B9AF}.","['Explainable artificial intelligence', 'visual explanation', '3D object detection']",[],"['Shuai Liu', 'Boyang Li', 'Zhiyu Fang', 'Mingyue Cui', 'Kai Huang']","['SUN YAT-SEN UNIVERSITY', 'School of Computer Science and Engineering, SUN YAT-SEN UNIVERSITY', 'SUN YAT-SEN UNIVERSITY', 'School of Computer Science and Engineering, Sun Yat-Sen University', 'School of Computer Science and Engineering, SUN YAT-SEN UNIVERSITY']",
https://openreview.net/forum?id=sNz7tptCH6,Security,Boosting the Transferability of Adversarial Attack on Vision Transformer with Adaptive Token Tuning,"Vision transformers (ViTs) perform exceptionally well in various computer vision tasks but remain vulnerable to adversarial attacks. Recent studies have shown that the transferability of adversarial examples exists for CNNs, and the same holds true for ViTs. However, existing ViT attacks aggressively regularize the largest token gradients to exact zero within each layer of the surrogate model, overlooking the interactions between layers, which limits their transferability in attacking black-box models. Therefore, in this paper, we focus on boosting the transferability of adversarial attacks on ViTs through adaptive token tuning (ATT). Specifically, we propose three optimization strategies: an adaptive gradient re-scaling strategy to reduce the overall variance of token gradients, a self-paced patch out strategy to enhance the diversity of input tokens, and a hybrid token gradient truncation strategy to weaken the effectiveness of attention mechanism. We demonstrate that scaling correction of gradient changes using gradient variance across different layers can produce highly transferable adversarial examples. In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability. On the other hand, using feature importance as a guidance to discard a subset of perturbation patches in each iteration, along with combining self-paced learning and progressively more sampled attacks, significantly enhances the transferability over attacks that use all perturbation patches. Extensive experiments conducted on ViTs, undefended CNNs, and defended CNNs validate the superiority of our proposed ATT attack method. On average, our approach improves the attack performance by 10.1% compared to state-of-the-art transfer-based attacks. Notably, we achieve the best attack performance with an average of 58.3% on three defended CNNs. Code is available at https://github.com/MisterRpeng/ATT.","['Adversarial Attack', 'Adversarial Transferability', 'Black-box Attack']",[],"['Di Ming', 'Peng Ren', 'Yunlong Wang', 'Xin Feng']","['Chongqing University of Technology', 'Banan district, Chongqing University of Technology', 'IQVIA', 'Chongqing University of Technology']",
https://openreview.net/forum?id=s4Wx2qXhv9,Security,Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness,"Randomized smoothing is a popular certified defense against adversarial attacks. In its essence, we need to solve a problem of  statistical estimation which is usually very time-consuming since we need to perform numerous (usually $10^5$) forward passes of the classifier for every point to be certified. In this paper, we review the statistical estimation problems for randomized smoothing to find out if the computational burden is necessary.  In particular, we consider the (standard) task of adversarial robustness where we need to decide if a point is robust at a certain radius or not using as few samples as possible while maintaining statistical guarantees.  We present estimation procedures employing confidence sequences enjoying the same statistical guarantees as the standard methods, with the optimal sample complexities for the estimation task and empirically demonstrate their good performance. Additionally, we provide a randomized version of Clopper-Pearson confidence intervals resulting in strictly stronger certificates.","['randomized smoothing', 'adversarial robustness', 'confidence interval', 'confidence sequence']",[],['Vaclav Voracek'],['University of Tuebingen'],
https://openreview.net/forum?id=satH8Evs2y,Security,Beware of Road Markings: A New Adversarial Patch Attack to Monocular Depth Estimation,"Monocular Depth Estimation (MDE) enables the prediction of scene depths from a single RGB image, having been widely integrated into production-grade autonomous driving systems, e.g., Tesla Autopilot. Current adversarial attacks to MDE models focus on attaching an optimized adversarial patch to a designated obstacle. Although effective, this approach presents two inherent limitations: its reliance on specific obstacles and its limited malicious impact. In contrast, we propose a pioneering attack to MDE models that \textit{decouples obstacles from patches physically and deploys optimized patches on roads}, thereby extending the attack scope to arbitrary traffic participants. This approach is inspired by our groundbreaking discovery: \textit{various MDE models with different architectures, trained for autonomous driving, heavily rely on road regions} when predicting depths for different obstacles. Based on this discovery, we design the Adversarial Road Marking (AdvRM) attack, which camouflages patches as ordinary road markings and deploys them on roads, thereby posing a continuous threat within the environment. Experimental results from both dataset simulations and real-world scenarios demonstrate that AdvRM is effective, stealthy, and robust against various MDE models, achieving about 1.507 of Mean Relative Shift Ratio (MRSR) over 8 MDE models. The code is available at \url{https://github.com/a-c-a-c/AdvRM.git}","['monocular depth estimation', 'adversarial patch', 'road dependence']",[],"['Hangcheng Liu', 'Zhenhu Wu', 'Hao Wang', 'XINGSHUO HAN', 'Shangwei Guo', 'Tao Xiang', 'Tianwei Zhang']","['School of Computer Science and Engineering, Nanyang Technological University', 'Beijing University of Posts and Telecommunications', 'College of Computer Science, Chongqing University', 'Nanyang Technological University', 'Chongqing University', 'Colllege of Computer Science, Chongqing University', 'Nanyang Technological University']",
https://openreview.net/forum?id=sntv8Ac3U2,Transparency & Explainability,Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis,"Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes.    To overcome these limitations, this work introduces a new family of  $\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image. We also show that training FG-DMs from scratch on MM-CelebA-HQ, Cityscapes, ADE20K, and COCO produce images of high quality (FID) and diversity (LPIPS).",['Image Synthesis; Controllable 2D/3D Synthesis; Diffusion'],[],"['Deepak Sridhar', 'Abhishek Peri', 'Rohith Reddy Rachala', 'Nuno Vasconcelos']","['ECE, University of California, San Diego', 'University of California, San Diego', 'ECE, University of California, San Diego', 'Electrical and Computer Engineering, University of California, San Diego']",
https://openreview.net/forum?id=seYXqfGT0q,Fairness & Bias,Prototypical Hash Encoding for On-the-Fly Fine-Grained Category Discovery,"In this paper, we study a practical yet challenging task, On-the-fly Category Discovery (OCD), aiming to online discover the newly-coming stream data that belong to both known and unknown classes, by leveraging only known category knowledge contained in labeled data. Previous OCD methods employ the hash-based technique to represent old/new categories by hash codes for instance-wise inference. However, directly mapping features into low-dimensional hash space not only inevitably damages the ability to distinguish classes and but also causes ``high sensitivity'' issue, especially for fine-grained classes, leading to inferior performance. To address these drawbacks, we propose a novel Prototypical Hash Encoding (PHE) framework consisting of Category-aware Prototype Generation (CPG) and Discriminative Category Encoding (DCE) to mitigate the sensitivity of hash code while preserving rich discriminative information contained in high-dimension feature space, in a two-stage projection fashion. CPG enables the model to fully capture the intra-category diversity by representing each category with multiple prototypes. DCE boosts the discrimination ability of hash code with the guidance of the generated category prototypes and the constraint of minimum separation distance. By jointly optimizing CPG and DCE, we demonstrate that these two components are mutually beneficial towards an effective OCD. Extensive experiments show the significant superiority of our PHE over previous methods, e.g. obtaining an improvement of +5.3% in ALL ACC averaged on all datasets. Moreover, due to the nature of the interpretable prototypes, we visually analyze the underlying mechanism of how PHE helps group certain samples into either known or unknown categories. Code is available at https://github.com/HaiyangZheng/PHE.","['Category Discovery', 'On-the-fly', 'Deep Hash']",[],"['Haiyang Zheng', 'Nan Pu', 'Wenjing Li', 'Nicu Sebe', 'Zhun Zhong']","['Information Science, University of Trento', 'Department of information engineering and computer science, University of Trento', 'Department of Computer Science, University of Science and Technology of China', 'Computer Science and Information Engineering, University of Trento', 'University of Nottingham']",
https://openreview.net/forum?id=XrK4JK2jBr,Transparency & Explainability,Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems,"Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety. Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human’s actions to act independently rather than in a synergistic fashion. To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates. Our 50-subject study provides several findings that we summarize into guidelines. While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and that black-box approaches are easier to train and result in better HMT performance highlighting a tradeoff between explainability and interactivity versus ease-of-training. Together, these findings present three important future research directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.","['Human-Machine Teaming', 'Adaptive AI']",[],"['Rohan R Paleja', 'Michael Joseph Munje', 'Kimberlee Chestnut Chang', 'Reed Jensen', 'Matthew Gombolay']","['MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Computer Science, University of Texas at Austin', 'Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Interactive Computing, Georgia Institute of Technology']",
https://openreview.net/forum?id=tKuLgnDWWN,Privacy & Data Governance,SILENCE: Protecting privacy in offloaded speech understanding on resource-constrained devices,"Speech serves as a ubiquitous input interface for embedded mobile devices.  Cloud-based solutions, while offering powerful speech understanding services, raise significant concerns regarding user privacy.  To address this, disentanglement-based encoders have been proposed to remove sensitive information from speech signals without compromising the speech understanding functionality.  However, these encoders demand high memory usage and computation complexity, making them impractical for resource-constrained wimpy devices. Our solution is based on a key observation that speech understanding hinges on long-term dependency knowledge of the entire utterance, in contrast to privacy-sensitive elements that are short-term dependent.  Exploiting this observation, we propose SILENCE, a lightweight system that selectively obscuring short-term details, without damaging the long-term dependent speech understanding performance. The crucial part of SILENCE is a differential mask generator derived from interpretable learning to  automatically configure the masking process. We have implemented SILENCE on the STM32H7 microcontroller and evaluate its efficacy under different attacking scenarios.  Our results demonstrate that SILENCE offers speech understanding performance and privacy protection capacity comparable to existing encoders, while achieving up to 53.3$\times$ speedup and 134.1$\times$ reduction in memory footprint.","['spoken language understanding', 'resource-constrained devices', 'privacy-preserving']",[],"['DONGQI CAI', 'Shangguang Wang', 'Zeling Zhang', 'Felix Xiaozhu Lin', 'Mengwei Xu']","['Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'Beijing University of Posts and Telecommunications', 'University of Virginia, Charlottesville', 'Beijing University of Posts and Telecommunications']",
https://openreview.net/forum?id=tdZLKY9usl,Security,Reimagining Mutual Information for Enhanced Defense against Data Leakage in Collaborative Inference,"Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus protecting user's data. Nevertheless, prior research has shown that collaborative inference still results in the exposure of input and predictions from edge devices. To defend against such data leakage in collaborative inference, we introduce InfoScissors, a defense strategy designed to reduce the mutual information between a model's intermediate outcomes and the device's input and predictions. We evaluate our defense on several datasets in the context of diverse attacks. Besides the empirical comparison, we provide a theoretical analysis of the inadequacies of recent defense strategies that also utilize mutual information, particularly focusing on those based on the Variational Information Bottleneck (VIB) approach. We illustrate the superiority of our method and offer a theoretical analysis of it.",['Collaborative inference'],[],"['Lin Duan', 'Jingwei Sun', 'Jinyuan Jia', 'Yiran Chen', 'Maria Gorlatova']","['Duke University', 'Electrical and Computer Engineering, Duke University', 'College of IST, Pennsylvania State University', 'Duke University', '']",
https://openreview.net/forum?id=tu1oC7zHGW,Fairness & Bias,Unveiling the Tapestry of Consistency in Large Vision-Language Models,"Large vision-language models (LVLMs) have recently achieved rapid progress, exhibiting great perception and reasoning abilities concerning visual information. However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point. This inconsistency of answers between different solution spaces is prevalent in LVLMs and erodes trust. To this end, we provide a multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when the solution space of a prompt revolves around a knowledge point. Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers.  (2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption. (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency. Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption. We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain.","['Consistency', 'ConBench', 'Large Vision-Language Models', 'Analysis']",[],"['Yuan Zhang', 'Fei xiao', 'Tao Huang', 'Chun-Kai Fan', 'Hongyuan Dong', 'Jiawen Li', 'Jiacong Wang', 'Kuan Cheng', 'Shanghang Zhang', 'Haoyuan Guo']","['Peking University', 'ByteDance Inc.', 'School of Computer Science, The University of Sydney', 'School of Computer Science, Peking University', 'ByteDance Inc.', '', 'school of artificial intelligence, University of the Chinese Academy of Sciences', 'Peking University', 'Peking University', 'ByteDance Inc.']",
https://openreview.net/forum?id=ttUXtV2YrA,Transparency & Explainability,Revisiting the Integration of Convolution and Attention for Vision Backbone,"Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel \textbf{at different granularity levels} instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus  enabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named \textit{GLMix}: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (e.g., 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at \url{https://github.com/rayleizhu/GLMix}.","['convolution', 'attention', 'vision backbone']",[],"['Lei Zhu', 'Xinjiang Wang', 'Wayne Zhang', 'Rynson W. H. Lau']","['City University of Hong Kong', 'SenseTime Group', 'SenseTime Research', 'Department of Computer Science, City University of Hong Kong']",
https://openreview.net/forum?id=twpPD9UMUN,Fairness & Bias,"Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering","Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, *MUSIC-AVQA-R*, crafted in two steps: rephrasing questions within the test split of a public dataset (*MUSIC-AVQA*) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at <https://github.com/reml-group/MUSIC-AVQA-R>.","['audio-visual question answering', 'bias elimination', 'debiasing', 'multimodality learning']",[],"['Jie Ma', 'Min Hu', 'Pinghui Wang', 'Wangchun Sun', 'Lingyun Song', 'Hongbin Pei', 'Jun Liu', 'Youtian Du']","[""School of Cyber Science and Engineering, Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""Computer Science, Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", 'School of Computer Science, Northwest Polytechnical University', '', ""Xi'an Jiaotong University"", ""Automation, Xi'an Jiaotong University""]",
https://openreview.net/forum?id=tyPcIETPWM,Transparency & Explainability,Conditional Outcome Equivalence: A Quantile Alternative to CATE,"The conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE). This ability to provide information over multiple quantiles of the response makes the CQTE especially valuable in cases where the effect of a treatment is not well-modelled by a location shift, even conditionally on the covariates. Nevertheless, the estimation of the CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself. This is in stark contrast to the CATE where it is possible to obtain high-quality estimates  which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth. Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make. We combine the desirable properties of the CATE and CQTE by considering a new estimand, the conditional quantile comparator (CQC). The CQC not only retains information about the whole treatment distribution, similar to the CQTE, but also having more natural examples of smoothness and is able to leverage simplicity in an auxiliary estimand. We provide finite sample bounds on the error of our estimator, demonstrating its ability to exploit simplicity. We validate our theory in numerical simulations which show that our method produces more accurate estimates than baselines. Finally, we apply our methodology to a study on the effect of employment incentives on earnings across different age groups. We see that our method is able to reveal heterogeneity of the effect across different quantiles.","['Heteregenous Treatment Effect', 'Conditional Average Treatment Effect', 'Conditional Quantile Treatment Effect', 'Quantile Regression']",[],"['Josh Givens', 'Henry Reeve', 'Song Liu', 'Katarzyna Reluga']","['Mathematics, University of Bristol', 'University of Bristol', 'University of Bristol', 'School of Mathematics, University of Bristol']",
https://openreview.net/forum?id=u9ShP64FJV,Security,Protecting Your LLMs with Information Bottleneck,"The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content. Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts. To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer. Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM. Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed.  Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.","['Defense', 'Information Bottleneck', 'Jailbreaking', 'Large Language Models']",[],"['Zichuan Liu', 'Zefan Wang', 'Linjie Xu', 'Jinyu Wang', 'Lei Song', 'Tianchun Wang', 'Chunlin Chen', 'Wei Cheng', 'Jiang Bian']","['Microsoft Research', 'Tsinghua University', 'School of Electronic Engineering and Computer Science', 'Microsoft', 'Microsoft', 'Pennsylvania State University', 'School of Management and Engineering, Nanjing University', 'Data Science, NEC-Labs', 'Microsoft Research, Microsoft']",
https://openreview.net/forum?id=u1Z3HWz4VJ,Security,RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations for Universal Robustness,"Most existing works focus on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, these AT models' multiple-norm robustness (union accuracy) is still low, which is crucial since in the real-world an adversary is not necessarily bounded by a single norm. The tradeoffs among robustness against multiple $l_p$ perturbations and accuracy/robustness make obtaining good union and clean accuracy challenging. We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts. We connect natural training (NT) with AT via gradient projection, to incorporate useful information from NT into AT, where we empirically and theoretically show it moderates the accuracy/robustness tradeoff. We propose a novel training framework \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. \textbf{RAMP} can be easily adapted for robust fine-tuning and full AT. For robust fine-tuning, \textbf{RAMP} obtains a union accuracy up to $53.3\%$ on CIFAR-10, and $29.1\%$ on ImageNet. For training from scratch, \textbf{RAMP} achieves a union accuracy of $44.6\%$ and good clean accuracy of $81.2\%$ on ResNet-18 against AutoAttack on CIFAR-10. Beyond multi-norm robustness \textbf{RAMP}-trained models achieve superior \textit{universal robustness}, effectively generalizing against a range of unseen adversaries and natural corruptions.","['Adversarial Robustness', 'Pre-training and Fine-tuning', 'Distribution Shifts']",[],"['Enyi Jiang', 'Gagandeep Singh']","['University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign']",
https://openreview.net/forum?id=ucxQrked0d,Fairness & Bias,Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning,"Training offline RL models using visual inputs poses two significant challenges, *i.e.*, the overfitting problem in representation learning and the overestimation bias for expected future rewards. Recent work has attempted to alleviate the overestimation bias by encouraging conservative behaviors. This paper, in contrast, tries to build more flexible constraints for value estimation without impeding the exploration of potential advantages. The key idea is to leverage off-the-shelf RL simulators, which can be easily interacted with in an online manner, as the “*test bed*” for offline policies. To enable effective online-to-offline knowledge transfer, we introduce CoWorld, a model-based RL approach that mitigates cross-domain discrepancies in state and reward spaces. Experimental results demonstrate the effectiveness of CoWorld, outperforming existing RL approaches by large margins.","['World models', 'reinforcement learning', 'visual control']",[],"['Qi Wang', 'Junming Yang', 'Yunbo Wang', 'Xin Jin', 'Wenjun Zeng', 'Xiaokang Yang']","['Shanghai Jiao Tong University', 'Southeast University', 'Shanghai Jiao Tong University', 'Eastern Institute of Technology, Ningbo', 'Computer Science, Eastern Institute for Advanced Study', 'SEIEE, Shanghai Jiao Tong University, China']",
https://openreview.net/forum?id=v1kpc060aC,Fairness & Bias,Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML,"We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous compute resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.","['stochastic convex optimization', 'byzantine robust learning', 'online convex optimization']",[],"['Tehila Dahan', 'Kfir Yehuda Levy']","['Electrical & Computer Engineering, Technion - Israel Institute of Technology, Technion', 'Technion - Israel Institute of Technology, Technion']",
https://openreview.net/forum?id=vBGMbFgvsX,Fairness & Bias,Going Beyond Heuristics by Imposing Policy Improvement as a Constraint,"In many reinforcement learning (RL) applications, incorporating heuristic rewards alongside the task reward is crucial for achieving desirable performance. Heuristics encode prior human knowledge about how a task should be done, providing valuable hints for RL algorithms. However, such hints may not be optimal, limiting the performance of learned policies.  The currently established way of using heuristics is to modify the heuristic reward in a manner that ensures that the optimal policy learned with it remains the same as the optimal policy for the task reward (i.e., optimal policy invariance).  However, these methods often fail in practical scenarios with limited training data. We found that while optimal policy invariance ensures convergence to the best policy based on task rewards, it doesn't guarantee better performance than policies trained with biased heuristics under a finite data regime, which is impractical. In this paper, we introduce a new principle tailored for finite data settings. Instead of enforcing optimal policy invariance, we train a policy that combines task and heuristic rewards and ensures it outperforms the heuristic-trained policy. As such, we prevent policies from merely exploiting heuristic rewards without improving the task reward. Our experiments on robotic locomotion, helicopter control, and manipulation tasks demonstrate that our method consistently outperforms the heuristic policy, regardless of the heuristic rewards' quality. Code is available at https://github.com/Improbable-AI/hepo.",['Deep reinforcement learning'],[],"['Chi-Chang Lee', 'Zhang-Wei Hong', 'Pulkit Agrawal']","['National Taiwan University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=vAOgaPvgYr,Security,OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step,"Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations. However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in *a single autoregressive step*, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100\% accuracy on single arithmetic operations ($+,-,\times,\div,\sin{},\cos{},\log{},\exp{},\sqrt{}$), outperforming GPT 4o with and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models. Code is available at https://github.com/druidowm/OccamLLM.","['LLM', 'Language Model', 'Arithmetic', 'OccamNet', 'Llama']",[],"['Owen M Dugan', 'Donato M. Jiménez Benetó', 'Charlotte Loh', 'Zhuo Chen', 'Rumen Dangovski', 'Marin Soljacic']","['Massachusetts Institute of Technology', 'Universitat Politècnica de Catalunya', 'Massachusetts Institute of Technology', '', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=vH7GcaDhAo,Fairness & Bias,RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions,"We propose a method for metric-scale monocular depth estimation. Inferring depth from a single image is an ill-posed problem due to the loss of scale from perspective projection during the image formation process. Any scale chosen is a bias, typically stemming from training on a dataset; hence, existing works have instead opted to use relative (normalized, inverse) depth. Our goal is to recover metric-scaled depth maps through a linear transformation. The crux of our method lies in the observation that certain objects (e.g., cars, trees, street signs) are typically found or associated with certain types of scenes (e.g., outdoor). We explore whether language descriptions can be used to transform relative depth predictions to those in metric scale. Our method, RSA , takes as input a text caption describing objects present in an image and outputs the parameters of a linear transformation which can be applied globally to a relative depth map to yield metric-scaled depth predictions. We demonstrate our method on recent general-purpose monocular depth models on indoors (NYUv2, VOID) and outdoors (KITTI). When trained on multiple datasets, RSA can serve as a general alignment module in zero-shot settings. Our method improves over common practices in aligning relative to metric depth and results in predictions that are comparable to an upper bound of fitting relative depth to ground truth via a linear transformation. Code is available at: https://github.com/Adonis-galaxy/RSA.","['Monocular Depth Estimation', 'Vision-Language Model', 'Multimodal Learning']",[],"['Ziyao Zeng', 'Yangchao Wu', 'Hyoungseob Park', 'Daniel Wang', 'Fengyu Yang', 'Stefano Soatto', 'Dong Lao', 'Byung-Woo Hong', 'Alex Wong']","['Computer Science, Yale University', 'University of California, Los Angeles', 'Computer Science, Yale University', 'Computer Science, Yale University', 'Yale University', 'Amazon Web Services', 'Computer Science, University of California, Los Angeles', 'Artificial Intelligence, Chung-Ang University', 'Computer Science, Yale University']",
https://openreview.net/forum?id=vI1WqFn15v,Security,Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes,"Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the **Refusal Loss** of LLMs and then proposes a method called **Gradient Cuff** to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.","['Large Language Models', 'Jailbreak Detection', 'AI Alignment and Safety']",[],"['Xiaomeng Hu', 'Pin-Yu Chen', 'Tsung-Yi Ho']","['Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'International Business Machines', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong']",
https://openreview.net/forum?id=vWSll6M9pj,Transparency & Explainability,"Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs","Research in auditory, visual, and audiovisual speech recognition (ASR, VSR, and AVSR, respectively) has traditionally been conducted independently. Even recent self-supervised studies addressing two or all three tasks simultaneously tend to yield separate models, leading to disjoint inference pipelines with increased memory requirements and redundancies. This paper proposes unified training strategies for these systems. We demonstrate that training a single model for all three tasks enhances VSR and AVSR performance, overcoming typical optimisation challenges when training from scratch. Moreover, we introduce a greedy pseudo-labelling approach to more effectively leverage unlabelled samples, addressing shortcomings in related self-supervised methods. Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside our semi-supervised approach. Despite using a single model for all tasks, our unified approach achieves state-of-the-art performance on LRS3 for ASR, VSR, and AVSR compared to recent methods. Code will be made publicly available.","['Speech recognition', 'lipreading', 'self-supervised learning', 'semi-supervised learning']",[],"['Alexandros Haliassos', 'Rodrigo Mira', 'Honglie Chen', 'Zoe Landgraf', 'Stavros Petridis', 'Maja Pantic']","['Facebook', 'Meta AI, Meta', 'Facebook', 'Facebook', 'Facebook', 'Facebook']",
https://openreview.net/forum?id=vwgWbCxeAQ,Transparency & Explainability,Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective,"Foundational Vision-Language models such as CLIP have exhibited impressive generalization in downstream tasks. However, CLIP suffers from a two-level misalignment issue, i.e., task misalignment and data misalignment, when adapting to specific tasks. Soft prompt tuning has mitigated the task misalignment, yet the data misalignment remains a challenge. To analyze the impacts of the data misalignment, we revisit the pre-training and adaptation processes of CLIP and develop a structural causal model. We discover that while we expect to capture task-relevant information for downstream tasks accurately, the task-irrelevant knowledge impacts the prediction results and hampers the modeling of the true relationships between the images and the predicted classes. As task-irrelevant knowledge is unobservable, we leverage the front-door adjustment and propose Causality-Guided Semantic Decoupling and Classification (CDC) to mitigate the interference of task-irrelevant knowledge. Specifically, we decouple semantics contained in the data of downstream tasks and perform classification based on each semantic. Furthermore, we employ the Dempster-Shafer evidence theory to evaluate the uncertainty of each prediction generated by diverse semantics. Experiments conducted in multiple different settings have consistently demonstrated the effectiveness of CDC.","['causal', 'adaptation', 'foundational models']",[],"['Yanan Zhang', 'Jiangmeng Li', 'Lixiang Liu', 'Wenwen Qiang']","['University of the Chinese Academy of Sciences', 'National Key Laboratory of Space Integrated Information System, Institute of Software, Chinese Academy of Sciences', 'University of the Chinese Academy of Sciences', 'Institute of Software Chinese Academy of Sciences']",
https://openreview.net/forum?id=vvpewjtnvm,Privacy & Data Governance,Low Precision Local Training is Enough for Federated Learning,"Federated Learning (FL) is a prevalent machine learning paradigm designed to address challenges posed by heterogeneous client data while preserving data privacy.      Unlike distributed training, it typically orchestrates resource-constrained edge devices to communicate via a low-bandwidth communication network with a central server.  This urges the development of more computation and communication efficient training algorithms. In this paper, we propose an efficient FL paradigm,  where the local  models in the clients  are trained with  low-precision operations and communicated  with the server in low precision format, while only the model aggregation in the server is performed with  high-precision computation. We surprisingly find that   high precision models can be recovered from the  low precision local models with proper aggregation in the server.       In this way, both the workload in the client-side and the communication cost can be significantly reduced.   We theoretically show that our proposed  paradigm can converge to the optimal solution as the training goes on, which demonstrates that low precision local training is enough for FL.  Our paradigm can be integrated with existing FL algorithms flexibly. Experiments across extensive benchmarks are conducted to showcase the effectiveness of our proposed method. Notably, the models trained by our method with the precision as low as 8 bits are  comparable  to those from the  full precision training. As a by-product, we show that low precision local training can relieve the over-fitting issue in local training, which under heterogeneous client data  can cause the client models drift further away from each other and lead to the failure in  model aggregation. Code is released at https://github.com/digbangbang/LPT-FL.","['Federated Learning', 'Low Precision Training']",[],"['Zhiwei Li', 'Yiqiu LI', 'Binbin Lin', 'Zhongming Jin', 'WEIZHONG ZHANG']","['Shanghai, Fudan University', 'Fudan University', 'Zhejiang University', 'Apsara Lab, Alibaba Cloud Computing', 'Fudan University']",
https://openreview.net/forum?id=wJAF8TGVUG,Fairness & Bias,S-MolSearch: 3D Semi-supervised Contrastive Learning for Bioactive Molecule Search,"Virtual Screening is an essential technique in the early phases of drug discovery, aimed at identifying promising drug candidates from vast molecular libraries.  Recently, ligand-based virtual screening has garnered significant attention due to its efficacy in conducting extensive database screenings without relying on specific protein-binding site information. Obtaining binding affinity data for complexes is highly expensive, resulting in a limited amount of available data that covers a relatively small chemical space. Moreover, these datasets contain a significant amount of inconsistent noise. It is challenging to identify an inductive bias that consistently maintains the integrity of molecular activity during data augmentation. To tackle these challenges, we propose S-MolSearch, the first framework to our knowledge, that leverages molecular 3D information and affinity information in semi-supervised contrastive learning for ligand-based virtual screening.  % S-MolSearch processes both labeled and unlabeled data, trains molecular structural encoders, and generates soft labels for unlabeled data, drawing on the principles of inverse optimal transport. Drawing on the principles of inverse optimal transport, S-MolSearch efficiently processes both labeled and unlabeled data, training molecular structural encoders while generating soft labels for the unlabeled data. This design allows S-MolSearch to adaptively utilize unlabeled data within the learning process. Empirically, S-MolSearch demonstrates superior performance on widely-used benchmarks LIT-PCBA and DUD-E. It surpasses both structure-based and ligand-based virtual screening methods for AUROC, BEDROC and EF.",['semi-supervised learning;  3D molecule search; contrastive learning'],[],"['Gengmo Zhou', 'Zhen Wang', 'Feng Yu', 'Guolin Ke', 'Zhewei Wei', 'Zhifeng Gao']","['Renmin University of China', 'DP Technology', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'DP Technology', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'DP Technology']",
https://openreview.net/forum?id=wiMaws0FWB,Fairness & Bias,Implicit Bias of Mirror Flow on Separable Data,"We examine the continuous-time counterpart of mirror descent, namely mirror flow, on classification problems which are linearly separable. Such problems are minimised ‘at infinity’ and have many possible solutions; we study which solution is preferred by the algorithm depending on the mirror potential. For exponential tailed losses and under mild assumptions on the potential, we show that the iterates converge in direction towards a $\phi_\infty$-maximum margin classifier. The function $\phi_\infty$ is the horizon function of the mirror potential and characterises its shape ‘at infinity’. When the potential is separable, a simple formula allows to compute this function. We analyse several examples of potentials and provide numerical experiments highlighting our results.","['Implicit bias', 'Mirror descent', 'Classification']",[],"['Scott Pesme', 'Radu-Alexandru Dragomir', 'Nicolas Flammarion']","['INRIA', 'Télécom Paris', 'IC, Swiss Federal Institute of Technology Lausanne']",
https://openreview.net/forum?id=wbE0QCBWji,Security,Constructing Semantics-Aware Adversarial Examples with a Probabilistic Perspective,"We propose a probabilistic perspective on adversarial examples, allowing us to embed subjective understanding of semantics as a distribution into the process of generating adversarial examples, in a principled manner. Despite significant pixel-level modifications compared to traditional adversarial attacks, our method preserves the overall semantics of the image, making the changes difficult for humans to detect. This extensive pixel-level modification enhances our method's ability to deceive classifiers designed to defend against adversarial attacks. Our empirical findings indicate that the proposed methods achieve higher success rates in circumventing adversarial defense mechanisms, while remaining difficult for human observers to detect.","['Adversarial Examples', 'Probabilistic Generative Models', 'Diffusion Models', 'Energy-based Models']",[],"['Andi Zhang', 'Mingtian Zhang', 'Damon Wischik']","['Centre for AI Foundamentals, University of Manchester', 'University College London', 'University of Cambridge']",
https://openreview.net/forum?id=wkwGedn19x,Transparency & Explainability,Scaling White-Box Transformers for Vision,"CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address.  Specifically, we propose CRATE-$\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE. Through extensive experiments, we demonstrate that CRATE-$\alpha$ can effectively scale with larger model sizes and datasets.  For example, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images.","['white-box deep neural networks', 'representation learning', 'transformer', 'sparse coding', 'scaling']",[],"['Jinrui Yang', 'Xianhang Li', 'Druv Pai', 'Yuyin Zhou', 'Yi Ma', 'Yaodong Yu', 'Cihang Xie']","['University of California, Santa Cruz', 'University of California, Santa Cruz', 'EECS, Electrical Engineering & Computer Science Department, University of California, Berkeley', 'University of California, Santa Cruz', 'Computer Science, University of Hong Kong', 'Electrical Engineering & Computer Science Department, University of California Berkeley', 'University of California, Santa Cruz']",
https://openreview.net/forum?id=ww62xltEfB,Fairness & Bias,A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness,"While neural networks can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood. To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them. Especially, throughout the past few years, the concept of *bi-Lipschitzness* has been proved as a beneficial inductive bias in many areas. However, due to its complexity, the design and control of bi-Lipschitz architectures are falling behind, and a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants along with solid theoretical analysis is lacking. In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality. Its desirable properties are illustrated with concrete experiments to illustrate its broad range of applications.","['bi-Lipschitzness', 'theoretical guarantee', 'tight control', 'direct parameterization', 'inductive bias', 'convex neural network', 'Legendre-Fenchel transformation']",[],"['Yuri Kinoshita', 'Taro Toyoizumi']","['Graduate School of Information Science and Technology, The University of Tokyo', 'RIKEN']",
https://openreview.net/forum?id=wsqDJHPUHN,Security,On the Ability of Developers' Training Data Preservation of Learnware,"The learnware paradigm aims to enable users to leverage numerous existing well-trained models instead of building machine learning models from scratch. In this paradigm, developers worldwide can submit their well-trained models spontaneously into a learnware dock system, and the system helps developers generate specification for each model to form a learnware. As the key component, a specification should characterize the capabilities of the model, enabling it to be adequately identified and reused, while preserving the developer's original data. Recently, the RKME (Reduced Kernel Mean Embedding) specification was proposed and most commonly utilized. This paper provides a theoretical analysis of RKME specification about its preservation ability for developer's training data. By modeling it as a geometric problem on manifolds and utilizing tools from geometric analysis, we prove that the RKME specification is able to disclose none of the developer's original data and possesses robust defense against common inference attacks, while preserving sufficient information for effective learnware identification.","['Learnware', 'Model Specification', 'Reduced Kernel Mean Embedding', 'Data Preservation', 'Synthetic Data', 'Learnware Dock System']",[],"['Hao-Yi Lei', 'Zhi-Hao Tan', 'Zhi-Hua Zhou']","['Nanjing University', 'School of Artificial Intelligence, Nanjing University', 'School of Artificial Intelligence, Nanjing University']",
https://openreview.net/forum?id=xM5m7J6Lbl,Transparency & Explainability,Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies,"While autonomous agents often surpass humans in their ability to handle vast and complex data, their potential misalignment (i.e., lack of transparency regarding their true objective) has thus far hindered their use in critical applications such as social decision processes. More importantly, existing alignment methods provide no formal guarantees on the safety of such models. Drawing from utility and social choice theory, we provide a novel quantitative definition of alignment in the context of social decision-making. Building on this definition, we introduce probably approximately aligned (i.e., near-optimal) policies, and we derive a sufficient condition for their existence. Lastly, recognizing the practical difficulty of satisfying this condition, we introduce the relaxed concept of safe (i.e., nondestructive) policies, and we propose a simple yet robust method to safeguard the black-box policy of any autonomous agent, ensuring all its actions are verifiably safe for the society.","['Alignment', 'Planning', 'Social Choice', 'AI Safety']",[],"['Frédéric Berdoz', 'Roger Wattenhofer']","['ETHZ - ETH Zurich', 'ITET, Swiss Federal Institute of Technology']",
https://openreview.net/forum?id=xRQxan3WkM,Fairness & Bias,The Implicit Bias of Adam on Separable Data,"Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\ell_\infty$-margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective.","['Adam', 'implicit bias']",[],"['Chenyang Zhang', 'Difan Zou', 'Yuan Cao']","['Department of Statistics and Actuarial Science, The Univeristy of Hongkong', 'University of Hong Kong', 'University of Hong Kong']",
https://openreview.net/forum?id=xSziO6gQgG,Fairness & Bias,Implicit Optimization Bias of Next-token Prediction in Linear Models,"We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization  across \emph{distinct} contexts, each tied with a \emph{sparse}  conditional probability distribution across a finite vocabulary of tokens, we introduce ``NTP-separability conditions'' that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits' differences of in-support tokens to their log-odds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings.","['entropy', 'gradient descent', 'SVM', 'next-token prediction', 'linear models', 'separability', 'language models', 'word embeddings']",[],['Christos Thrampoulidis'],['University of British Columbia'],
https://openreview.net/forum?id=xUoNgR1Byy,Transparency & Explainability,Interpreting Learned Feedback Patterns in Large Language Models,"Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term **Learned Feedback Pattern** (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the **safety** and **alignment** of LLMs.","['Interpretability', 'Reward Models', 'Safety']",[],"['Luke Marks', 'Amir Abdullah', 'Clement Neo', 'Rauno Arike', 'David Krueger', 'Philip Torr', 'Fazl Barez']","['Interpretability, Apart Lab', 'AI Platform, Cynch AI', 'Nanyang Technological University', 'University of Amsterdam', 'Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal', 'University of Oxford', '']",
https://openreview.net/forum?id=xeviQPXTMU,Security,FedGMark: Certifiably Robust Watermarking for Federated Graph Learning,"Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.","['Watermark', 'Federated Graph Learning']",[],"['Yuxin Yang', 'Qiang Li', 'Yuan Hong', 'Binghui Wang']","['College of Computer Science and Technology, Jilin University', 'Computer Science, Jilin University', 'University of Connecticut', 'Illinois Institute of Technology']",
https://openreview.net/forum?id=xabStWAUtr,Fairness & Bias,Co-occurrence is not Factual Association in Language Models,"Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.","['language model', 'knowledge learning', 'reasoning']",[],"['Xiao Zhang', 'Miao Li', 'Ji Wu']","['Tsinghua University, Tsinghua University', 'Department of EE, Tsinghua University', 'Tsinghua University, Tsinghua University']",
https://openreview.net/forum?id=xcF2VbyZts,Transparency & Explainability,SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization,"Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT.","['Social Relation Reasoning', 'Large Language Models', 'Foundation Models', 'Prompt Optimization']",[],"['Wanhua Li', 'Zibin Meng', 'Jiawei Zhou', 'Donglai Wei', 'Chuang Gan', 'Hanspeter Pfister']","['Harvard University', '', 'State University of New York at Stony Brook', 'Boston College', 'University of Massachusetts at Amherst', 'Harvard University']",
https://openreview.net/forum?id=y9zIRxshzj,Transparency & Explainability,Causal Discovery from Event Sequences by Local Cause-Effect Attribution,"Sequences of events, such as crashes in the stock market or outages in a network, contain strong temporal dependencies, whose understanding is crucial to react to and influence future events. In this paper, we study the problem of discovering the underlying causal structure from event sequences. To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays. We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects.  We base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity. As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction. To discover causal graphs, we introduce the Cascade algorithm, which adds edges in topological order. Extensive evaluation shows that Cascade outperforms existing methods in settings with instantaneous effects, noise, and multiple colliders, and discovers insightful causal graphs on real-world data.","['causality', 'causal discovery', 'event sequences']",[],"['Joscha Cüppers', 'Sascha Xu', 'Ahmed Musa', 'Jilles Vreeken']","['EDA Group, CISPA Helmholtz Center for Information Security', 'CS, CISPA, saarland university, saarland informatics campus', 'Technische Universität Dresden', 'CISPA Helmholtz Center for Information Security']",
https://openreview.net/forum?id=yBHbeSpwYS,Fairness & Bias,In Pursuit of Causal Label Correlations for Multi-label Image Recognition,"Multi-label image recognition aims to predict all objects present in an input image. A common belief is that modeling the correlations between objects is beneficial for multi-label recognition. However, this belief has been recently challenged as label correlations may mislead the classifier in testing, due to the possible contextual bias in training. Accordingly, a few of recent works not only discarded label correlation modeling, but also advocated to remove contextual information for multi-label image recognition. This work explicitly explores label correlations for multi-label image recognition based on a principled causal intervention approach. With causal intervention, we pursue causal label correlations and suppress spurious label correlations, as the former tend to convey useful contextual cues while the later may mislead the classifier. Specifically, we decouple label-specific features with a Transformer decoder attached to the backbone network, and model the confounders which may give rise to spurious correlations by clustering spatial features of all training images. Based on label-specific features and confounders, we employ a cross-attention module to implement causal intervention, quantifying the causal correlations from all object categories to each predicted object category. Finally, we obtain image labels by combining the predictions from decoupled features and causal label correlations. Extensive experiments clearly validate the effectiveness of our approach for multi-label image recognition in both common and cross-dataset settings.","['Multi-label', 'Label correlation', 'Causal intervention']",[],"['Zhao-Min Chen', 'Xin Jin', 'YisuGe', 'Sixian Chan']","['Wenzhou University', 'Intelligence Vision Lab, Samsung R&D Institute China-Nanjing (SRC-N)', 'Wenzhou University', 'the College of Computer Science and Technology, Zhejiang University of Technology']",
https://openreview.net/forum?id=yAAQWBMGiT,Fairness & Bias,Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning,"We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks.","['Data selection', 'Finetuning', 'Sketching', 'Johnson-Lindenstrauss transform']",[],"['Yijun Dong', 'Hoang Phan', 'Xiang Pan', 'Qi Lei']","['Courant Institute, New York University', 'New York University', 'New York University', 'New York University']",
https://openreview.net/forum?id=y6JotynERr,Privacy & Data Governance,Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration,"Federated Learning (FL) has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL algorithms lack support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes---from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each device. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype's ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive task arithmetic knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous device prototypes with varying capacities. Comprehensive evaluations of our method across both computer vision (CV) and natural language processing (NLP) tasks demonstrate that TAKFL achieves state-of-the-art results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Our code is released at https://github.com/MMorafah/TAKFL and the project website is available at https://mmorafah.github.io/takflpage .","['Federated Learning', 'Heterogeneous Device Prototypes', 'Knowledge Distillation', 'Task Arithmetic', 'Machine Learning']",[],"['Mahdi Morafah', 'Vyacheslav Kungurtsev', 'Hojin Matthew Chang', 'Chen Chen', 'Bill Lin']","['', 'Computer Science, Czech Technical Univeresity in Prague, Czech Technical University of Prague', 'Jacobs School of Engineering, University of California, San Diego', 'Computer Science, University of Central Florida', 'ECE, University of California, San Diego']",
https://openreview.net/forum?id=y9huwsnGRJ,Transparency & Explainability,"Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving","Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement. Project page: https://pjlab-adg.github.io/LeapAD","['Autonomous Driving', 'Dual-process System', 'Knowledge-Driven', 'Vision Language Model']",[],"['Jianbiao Mei', 'Yukai Ma', 'Xuemeng Yang', 'Licheng Wen', 'Xinyu Cai', 'Xin Li', 'Daocheng Fu', 'Bo Zhang', 'Pinlong Cai', 'Min Dou', 'Botian Shi', 'Liang He', 'Yong Liu', 'Yu Qiao']","['Zhejiang University', 'Zhejiang University', 'Shanghai AI Lab', 'Shanghai AI Lab', 'Autonomous Driving, Shanghai AI Lab', 'Shanghai Jiaotong University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Artificial Intelligence Laboratory', 'Frontier Discovery Center, Shanghai Artificial Intelligence Laboratory', 'Shanghai AI Laboratory', 'Shanghai AI Lab', '', 'Institute of cyber systems and control, Zhejiang University', '']",
https://openreview.net/forum?id=yBrxziByeG,Fairness & Bias,Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model,"Existing multi-modal image fusion methods fail to address the compound degradations presented in source images, resulting in fusion images plagued by noise, color bias, improper exposure, etc. Additionally, these methods often overlook the specificity of foreground objects, weakening the salience of the objects of interest within the fused images. To address these challenges, this study proposes a novel interactive multi-modal image fusion framework based on the text-modulated diffusion model, called Text-DiFuse. First, this framework integrates feature-level information integration into the diffusion process, allowing adaptive degradation removal and multi-modal information fusion. This is the first attempt to deeply and explicitly embed information fusion within the diffusion process, effectively addressing compound degradation in image fusion. Second, by embedding the combination of the text and zero-shot location model into the diffusion fusion process, a text-controlled fusion re-modulation strategy is developed. This enables user-customized text control to improve fusion performance and highlight foreground objects in the fused images. Extensive experiments on diverse public datasets show that our Text-DiFuse achieves state-of-the-art fusion performance across various scenarios with complex degradation. Moreover, the semantic segmentation experiment validates the significant enhancement in semantic performance achieved by our text-controlled fusion re-modulation strategy. The code is publicly available at https://github.com/Leiii-Cao/Text-DiFuse.","['Image fusion', 'multi-modal fusion', 'text', 'diffusion']",[],"['Hao Zhang', 'Lei Cao', 'Jiayi Ma']","['Wuhan University', 'Wuhan University', 'Electronic Information School, Wuhan University']",
https://openreview.net/forum?id=yS9xU6ANiA,Transparency & Explainability,Exogenous Matching: Learning Good Proposals for Tractable Counterfactual Estimation,"We propose an importance sampling method for tractable and efficient estimation of counterfactual expressions in general settings, named Exogenous Matching. By minimizing a common upper bound of counterfactual estimators, we transform the variance minimization problem into a conditional distribution learning problem, enabling its integration with existing conditional distribution modeling approaches. We validate the theoretical results through experiments under various types and settings of Structural Causal Models (SCMs) and demonstrate the outperformance on counterfactual estimation tasks compared to other existing importance sampling methods. We also explore the impact of injecting structural prior knowledge (counterfactual Markov boundaries) on the results. Finally, we apply this method to identifiable proxy SCMs and demonstrate the unbiasedness of the estimates, empirically illustrating the applicability of the method to practical scenarios.","['causality', 'causal inference', 'counterfactual estimation', 'importance sampling', 'normalizing flows']",[],"['Yikang Chen', 'Dehui du', 'Lili Tian']","['East China Normal University', 'software engineering institute, East China Normal University', 'East China Normal University']",
https://openreview.net/forum?id=yUckuDjAE0,Security,Learning Bregman Divergences with Application to Robustness,"We propose a novel and general method to learn Bregman divergences from raw high-dimensional data that measure similarity between images in pixel space. As a prototypical application, we learn divergences that consider real-world corruptions of images (e.g., blur) as close to the original and noisy perturbations as far, even if in $L^p$-distance the opposite holds. We also show that the learned Bregman divergence excels on datasets of human perceptual similarity judgment, suggesting its utility in a range of applications. We then define adversarial attacks by replacing the projected gradient descent (PGD) with the mirror descent associated with the learned Bregman divergence, and use them to improve the state-of-the-art in robustness through adversarial training for common image corruptions. In particular, for the contrast corruption that was found problematic in prior work we achieve an accuracy that exceeds the $L^p$- and the LPIPS-based adversarially trained neural networks by a margin of 27.16\% on the CIFAR-10-C corruption data set.","['Bregman divergence', 'similarity and distance learning', 'mirror descent', 'corruption robustness.']",[],"['Mohamed-Hicham LEGHETTAS', 'Markus Püschel']","['Department of Computer Science, ETHZ - ETH Zurich', 'Department of Computer Science, ETHZ - ETH Zurich']",
https://openreview.net/forum?id=yVzWlFhpRW,Security,Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking,"Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization ( PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.","['Reinforcement Learning', 'Policy Gradient', 'Action Masking', 'Robotics', 'Continuous Actions']",[],"['Roland Stolz', 'Hanna Krasowski', 'Jakob Thumm', 'Michael Eichelbeck', 'Philipp Gassert', 'Matthias Althoff']","['Professorship for Cyberphysical Systems, Technische Universität München', 'University of California, Berkeley', 'Informatics, Technische Universität München', 'Technische Universität München', 'Technical University Munich', 'Technische Universität München']",
https://openreview.net/forum?id=yltJAlwtW9,Fairness & Bias,Information-theoretic Generalization Analysis for Expected Calibration Error,"While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited. In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, uniform mass and uniform width binning. Our analysis establishes upper bounds on the bias, achieving an improved convergence rate. Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias. We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data. Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.","['information thery', 'information-theoretic generalization error analysis', 'generalization error', 'expected calibration error', 'calibration error', 'binning']",[],"['Futoshi Futami', 'Masahiro Fujisawa']","['Osaka University', 'RIKEN']",
https://openreview.net/forum?id=yzviAnpvU6,Fairness & Bias,ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization,"Gradient estimation is critical in zeroth-order optimization methods, which aims to obtain the descent direction by sampling update directions and querying function evaluations. Extensive research has been conducted including smoothing and linear interpolation. The former methods smooth the objective function, causing a biased gradient estimation, while the latter often enjoys more accurate estimates, at the cost of large amounts of samples and queries at each iteration to update variables. This paper resorts to the linear interpolation strategy and proposes to reduce the complexity of gradient estimation by reusing queries in the prior iterations while maintaining the sample size unchanged. Specifically, we model the gradient estimation as a quadratically constrained linear program problem and manage to derive the analytical solution. It innovatively decouples the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed, significantly reducing the computation complexity. Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency. Our code is available at https://github.com/Thinklab-SJTU/ReLIZO.git.",['Zero-order Optimization; Linear Interpolation; Reusing Strategy;'],[],"['Xiaoxing Wang', 'Xiaohan Qin', 'Xiaokang Yang', 'Junchi Yan']","['IAAR, IAAR', 'Department of Computer Science and Engineering, State University of New York at Buffalo', 'SEIEE, Shanghai Jiao Tong University, China', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=zNIhPZnqhh,Transparency & Explainability,Continuous Spatiotemporal Events Decoupling through Spike-based Bayesian Computation,"Numerous studies have demonstrated that the cognitive processes of the human brain can be modeled using the Bayesian theorem for probabilistic inference of the external world. Spiking neural networks (SNNs), capable of performing Bayesian computation with greater physiological interpretability, offer a novel approach to distributed information processing in the cortex. However, applying these models to real-world scenarios to harness the advantages of brain-like computation remains a challenge.  Recently, bio-inspired sensors with high dynamic range and ultra-high temporal resolution have been widely used in extreme vision scenarios. Event streams, generated by various types of motion, represent spatiotemporal data. Inferring motion targets from these streams without prior knowledge remains a difficult task. The Bayesian inference-based Expectation-Maximization (EM) framework has proven effective for motion segmentation in event streams, allowing for decoupling without prior information about the motion or its source.  This work demonstrates that Bayesian computation based on spiking neural networks can decouple event streams of different motions. The Winner-Take-All (WTA) circuits in the constructed network implement an equivalent E-step, while STDP achieves an equivalent optimization in M-step. Through theoretical analysis and experiments, we show that STDP-based learning can maximize the contrast of warped events under mixed motion models. Experimental results show that the constructed spiking network can effectively segment the motion contained in event streams.","['Bayesian Computation', 'Spiking Neural Network', 'Event Cameras', 'Motion Segmentation', 'Winner-Take-All', 'Spike-timing-dependent plasticity']",[],"['Yajing Zheng', 'Jiyuan Zhang', 'Zhaofei Yu', 'Tiejun Huang']","['Imaging Neuroscience, University College London, University of London', 'School of Computer Science, Peking University', 'Peking University', 'School of Computer Science, Peking University']",
https://openreview.net/forum?id=ztwl4ubnXV,Fairness & Bias,OxonFair: A Flexible Toolkit for Algorithmic Fairness,"We present OxonFair, a new open source toolkit for enforcing fairness in binary classification. Compared to existing toolkits: (i) We support NLP and Computer Vision classification as well as standard tabular problems. (ii) We support enforcing fairness on validation data, making us robust to a wide range of overfitting challenges. (iii) Our approach can optimize any measure based on True Positives, False Positive, False Negatives, and True Negatives. This makes it easily extensible and much more expressive than existing toolkits. It supports all 9 and all 10 of the decision-based group metrics of two popular review articles. (iv) We jointly optimize a performance objective alongside fairness constraints. This minimizes degradation while enforcing fairness, and even improves the performance of inadequately tuned unfair baselines. OxonFair is compatible with standard ML toolkits, including sklearn, Autogluon, and PyTorch and is available  at https://github.com/oxfordinternetinstitute/oxonfair.","['Fairness Toolkit', 'Algorithmic Fairness', 'Trustworthy AI']",[],"['Eoin D. Delaney', 'Zihao Fu', 'Sandra Wachter', 'Brent Mittelstadt', 'Chris Russell']","['Oxford Internet Institute, University of Oxford', 'University of Oxford', 'Oxford Internet Institute, University of Oxford', 'Oxford Internet Institute, University of Oxford', 'Oxford Internet Institute, University of Oxford']",
https://openreview.net/forum?id=ziehA15y8k,Security,Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning,"Adversarial attacks against graph neural networks (GNNs) through perturbations of the graph structure are increasingly common in social network tasks like rumor detection. Social media platforms capture diverse attack sequence samples through both machine and manual screening processes. Investigating effective ways to leverage these adversarial samples to enhance robustness is imperative. We improve the maximum entropy inverse reinforcement learning (IRL) method with the mixture-of-experts approach to address multi-source graph adversarial attacks. This method reconstructs the attack policy, integrating various attack models and providing feature-level explanations, subsequently generating additional adversarial samples to fortify the robustness of detection models. We develop precise sample guidance and a bidirectional update mechanism to reduce the deviation caused by imprecise feature representation and negative sampling within the large action space of social graphs, while also accelerating policy learning. We take rumor detector as an example targeted GNN model on real-world rumor datasets. By utilizing a small subset of samples generated by various graph adversarial attack methods, we reconstruct the attack policy, closely approximating the performance of the original attack method. We validate that samples generated by the learned policy enhance model robustness through adversarial training and data augmentation.","['graph adverisarial attack', 'reinforcement learning', 'inverse reinforcement learning', 'graph neural networks']",[],"['Yuefei Lyu', 'Chaozhuo Li', 'Sihong Xie', 'Xi Zhang']","['Beijing University of Posts and Telecommunications', 'Computer Science, Beijing University of Posts and Telecommunications', 'HKUST-GZ', 'Beijing University of Posts and Telecommunications']",
https://openreview.net/forum?id=zn6s6VQYb0,Fairness & Bias,GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction,"Graph-structured data is integral to many applications, prompting the development of various graph representation methods. Graph autoencoders (GAEs), in particular, reconstruct graph structures from node embeddings. Current GAE models primarily utilize self-correlation to represent graph structures and focus on node-level tasks, often overlooking multi-graph scenarios. Our theoretical analysis indicates that self-correlation generally falls short in accurately representing specific graph features such as islands, symmetrical structures, and directional edges, particularly in smaller or multiple graph contexts.To address these limitations, we introduce a cross-correlation mechanism that significantly enhances the GAE representational capabilities. Additionally, we propose the GraphCroc, a new GAE that supports flexible encoder architectures tailored for various downstream tasks and ensures robust structural reconstruction, through a mirrored encoding-decoding process. This model also tackles the challenge of representation bias during optimization by implementing a loss-balancing strategy. Both theoretical analysis and numerical evaluations demonstrate that our methodology significantly outperforms existing self-correlation-based GAEs in graph structure reconstruction.","['Graph Neural Network', 'Graph Representation', 'Auto-Encoder']",[],"['Shijin Duan', 'Ruyi Ding', 'Jiaxing He', 'Aidong Adam Ding', 'Yunsi Fei', 'Xiaolin Xu']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', '']",
https://openreview.net/forum?id=zv4UISZzp5,Fairness & Bias,IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation,"As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities.  Our data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains. To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2. The results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works. We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.","['LLM', 'Data Generalization', 'Discrimination Indexes']",[],"['Fan Lin', 'Shuyi Xie', 'Yong Dai', 'Wenlin Yao', 'TianJiao Lang', 'Yu Zhang']","['School of Computer Science and Engineering, Southeast University', 'TEG, Tencent Big Data', 'AI Lab, Tencent AI Lab', 'Amazon', 'TEG, Tencent Big Data', '']",
https://openreview.net/forum?id=mZHbkbYWTp,Fairness & Bias,Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise,"A pivotal brain computation relies on the ability to sustain perception-action loops. Stochastic optimal control theory offers a mathematical framework to explain these processes at the algorithmic level through optimality principles. However, incorporating a realistic noise model of the sensorimotor system — accounting for multiplicative noise in feedback and motor output, as well as internal noise in estimation — makes the problem challenging. Currently, the algorithm that is commonly used is the one proposed in the seminal study in (Todorov, 2005). After discovering some pitfalls in the original derivation, i.e., unbiased estimation does not hold, we improve the algorithm by proposing an efficient gradient descent-based optimization that minimizes the cost-to-go while only imposing linearity of the control law. The optimal solution is obtained by iteratively propagating in closed form the sufficient statistics to compute the expected cost and then minimizing this cost with respect to the filter and control gains. We demonstrate that this approach results in a significantly lower overall cost than current state-of-the-art solutions, particularly in the presence of internal noise, though the improvement is present in other circumstances as well, with theoretical explanations for this enhanced performance. Providing the optimal control law is key for inverse control inference, especially in explaining behavioral data under rationality assumptions.","['control theory', 'stochastic optimal control', 'sensorimotor system', 'multiplicative and internal noise', 'motor control']",[],"['Francesco Damiani', 'Akiyuki Anzai', 'Jan Drugowitsch', 'Gregory C DeAngelis', 'Rubén Moreno-Bote']","['Department of Information and Communications Technologies, Universitat Pompeu Fabra', 'Center for Visual Science, University of Rochester', 'Harvard University', 'University of Rochester', 'Universitat Pompeu Fabra']",
https://openreview.net/forum?id=4lGPSbGe11,Fairness & Bias,Is Cross-validation the Gold Standard to Estimate Out-of-sample Model Performance?,"Cross-Validation (CV) is the default choice for estimate the out-of-sample performance of machine learning models. Despite its wide usage, their statistical benefits have remained half-understood, especially in challenging nonparametric regimes. In this paper we fill in this gap and show that, in terms of estimating the out-of-sample performances, for a wide spectrum of models, CV does not statistically outperform the simple ``plug-in'' approach where one reuses training data for testing evaluation. Specifically, in terms of both the asymptotic bias and coverage accuracy of the associated interval for out-of-sample evaluation, $K$-fold CV provably cannot outperform plug-in regardless of the rate at which the parametric or nonparametric models converge. Leave-one-out CV can have a smaller bias as compared to plug-in; however, this bias improvement is negligible compared to the variability of the evaluation, and in some important cases leave-one-out again does not outperform plug-in once this variability is taken into account. We obtain our theoretical comparisons via a novel higher-order Taylor analysis that dissects the limit theorems of testing evaluations, which applies to model classes that are not amenable to previously known sufficient conditions. Our numerical results demonstrate that plug-in performs indeed no worse than CV in estimating model performance across a wide range of examples.","['cross-validation', 'plug-in', 'uncertainty quantification', 'nonparametric models']",[],"['Garud Iyengar', 'Henry Lam', 'Tianyu Wang']","['Industrial Engineering and Operations Research, Columbia University', 'Columbia University', 'Columbia University']",
https://openreview.net/forum?id=AYDBFxNon4,Fairness & Bias,Linking In-context Learning in Transformers to Human Episodic Memory,"Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.","['in-context learning', 'Transformer', 'induction head', 'episodic memory', 'mechanistic interpretability']",[],"['Li Ji-An', 'Corey Yishan Zhou', 'Marcus K. Benna', 'Marcelo G Mattar']","['University of California, San Diego', 'Amazon', 'University of California, San Diego', 'University of California, San Diego']",
https://openreview.net/forum?id=RQCmMSSzvI,Fairness & Bias,Non-Asymptotic Uncertainty Quantification in High-Dimensional Learning,"Uncertainty quantification (UQ) is a crucial but challenging task in many high-dimensional learning problems to increase the confidence of a given predictor. We develop a new data-driven approach for UQ in regression that applies both to classical optimization approaches such as the LASSO as well as to neural networks. One of the most notable UQ techniques is the debiased LASSO, which modifies the LASSO to allow for the construction of asymptotic confidence intervals by decomposing the estimation error into a Gaussian and an asymptotically vanishing bias component. However, in real-world problems with finite-dimensional data, the bias term is often too significant to disregard, resulting in overly narrow confidence intervals. Our work rigorously addresses this issue and derives a data-driven adjustment that corrects the confidence intervals for a large class of predictors by estimating the means and variances of the bias terms from training data, exploiting high-dimensional concentration phenomena. This gives rise to non-asymptotic confidence intervals, which can help avoid overestimating certainty in critical applications such as MRI diagnosis. Importantly, our analysis extends beyond sparse regression to data-driven predictors like neural networks, enhancing the reliability of model-based deep learning. Our findings bridge the gap between established theory and the practical applicability of such methods.","['high-dimensional regression', 'uncertainty quantification', 'model-based deep learning', 'debiased estimator', 'inverse problems']",[],"['Frederik Hoppe', 'Claudio Mayrink Verdun', 'Hannah Laus', 'Felix Krahmer', 'Holger Rauhut']","['Rheinisch Westfälische Technische Hochschule Aachen', 'Harvard University', 'Mathematics, Technische Universität München', 'Technische Universität München', 'Department of Mathematics, Ludwig-Maximilians-Universität München']",
https://openreview.net/forum?id=ez7w0Ss4g9,Fairness & Bias,How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self Distillation Networks,"Two competing paradigms exist for self-supervised learning of data representations.      Joint Embedding Predictive Architectures (JEPAs) is a class of architectures in which semantically similar inputs are encoded into representations that are predictive of each other. A recent successful approach that falls under the JEPA framework is self-distillation, where an online encoder is trained to predict the output of the target encoder, sometimes with a lightweight predictor network. This is contrasted with the Masked Auto Encoder (MAE) paradigm, where an encoder and decoder are trained to reconstruct missing parts of the input in ambient space rather than its latent representation. A common motivation for using the JEPA approach over MAE is that the JEPA objective prioritizes abstract features over fine-grained pixel information (which can be unpredictable and uninformative).     In this work, we seek to understand the mechanism behind this empirical observation by analyzing deep linear models. We uncover a surprising mechanism: in a simplified linear setting where both approaches learn similar representations, JEPAs are biased to learn high influence features, or features characterized by having high regression coefficients. Our results point to a distinct implicit bias of predicting in latent space that may shed light on its success in practice.","['SSL', 'JEPA']",[],"['Etai Littwin', 'Omid Saremi', 'Madhu Advani', 'Vimal Thilak', 'Preetum Nakkiran', 'Chen Huang', 'Joshua M. Susskind']","['Apple', 'Apple', 'Apple', 'Machine Learning Engineering, Apple', 'Apple', 'Apple', 'Cognitive Science, Apple']",
https://openreview.net/forum?id=eNM94i7R3A,Transparency & Explainability,Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning,"While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this *rich feature learning regime* remain elusive, with much of our theoretical understanding stemming from the opposing *lazy regime*. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced *layer-specific* initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs from balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.","['feature learning', 'rich regime', 'lazy regime', 'exact solutions', 'conserved quantities', 'balanced initialization', 'neural tangent kernel', 'grokking']",[],"['Daniel Kunin', 'Allan Raventos', 'Clémentine Carla Juliette Dominé', 'Feng Chen', 'David Klindt', 'Andrew M Saxe', 'Surya Ganguli']","['Stanford University', 'Stanford University', 'University College London, University of London', 'Stanford University', '', 'University College London, University of London', 'Stanford University']",
https://openreview.net/forum?id=eP9auEJqFg,Security,Representation Noising: A Defence Mechanism Against Harmful Finetuning,"Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (\textsf{\small RepNoise}), a defence mechanism that operates even when attackers have access to the weights. \textsf{\small RepNoise} works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process as long as they are drawn from the same distribution of the attack set. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its ``depth'': the degree to which information about harmful representations is removed across {\em all layers} of the LLM. We also find areas where \textsf{\small RepNoise} still remains ineffective and highlight how those limitations can inform future research.","['Harmful Fine-tuning', 'LLM Security', 'Domain Authorization']",[],"['Domenic Rosati', 'Jan Wehner', 'Kai Williams', 'Lukasz Bartoszcze', 'Robie Gonzales', 'carsten maple', 'Subhabrata Majumdar', 'Hassan Sajjad', 'Frank Rudzicz']","['Dalhousie University', 'Electrical Engineering, Mathematics and Computer Science, Delft University of Technology', 'Swarthmore College', 'WMG, University of Warwick', 'Computer Science, Dalhousie University', 'Alan Turing Institute', 'Vijil', 'Faculty of Computer Science, Dalhousie University', 'Dalhousie University']",
https://openreview.net/forum?id=RB1F2h5YEx,Fairness & Bias,Parseval Regularization for Continual Reinforcement Learning,"Plasticity loss, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.","['Reinforcement Learning', 'Continual Learning', 'Plasticity', 'Optimization']",[],"['Wesley Chung', 'Lynn Cherif', 'Doina Precup', 'David Meger']","['McGill University', 'Mila - Quebec Artificial Intelligence Institute', 'DeepMind', 'McGill University']",
https://openreview.net/forum?id=2n1Ysn1EDl,Transparency & Explainability,MambaLRP: Explaining Selective State Space Sequence Models,"Recent sequence modeling approaches using selective state space sequence models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.","['Explainable AI', 'explainability', 'interpretability', 'state space models', 'Mamba', 'long-range dependency']",[],"['Farnoush Rezaei Jafari', 'Grégoire Montavon', 'Klaus Robert Muller', 'Oliver Eberle']","['Technische Universität Berlin', '', 'Google', 'Technische Universität Berlin']",
https://openreview.net/forum?id=lpXDZKiAnt,Security,Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack,"The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable  cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users  fine-tuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from  un-sanitized user data in the fine-tuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at  https://github.com/git-disl/Vaccine.","['Larger language model', 'safety alignment', 'perturbation-aware alignment', 'harmful finetuning attack']",[],"['Tiansheng Huang', 'Sihao Hu', 'Ling Liu']","['Georgia Institute of Technology', 'College of Computing, Georgia Institute of Technology', '']",
https://openreview.net/forum?id=LXz1xIEBkF,Transparency & Explainability,"STL: Still Tricky Logic (for System Validation, Even When Showing Your Work)","As learned control policies become increasingly common in autonomous systems, there is increasing need to ensure that they are interpretable and can be checked by human stakeholders. Formal specifications have been proposed as ways to produce human-interpretable policies for autonomous systems that can still be learned from examples. Previous work showed that despite claims of interpretability, humans are unable to use formal specifications presented in a variety of ways to validate even simple robot behaviors. This work uses active learning, a standard pedagogical method, to attempt to improve humans' ability to validate policies in signal temporal logic (STL). Results show that overall validation accuracy is not high, at 65\% $\pm$ 15% (mean $\pm$ standard deviation), and that the three conditions of no active learning, active learning, and active learning with feedback do not significantly differ from each other. Our results suggest that the utility of formal specifications for human interpretability is still unsupported but point to other avenues of development which may enable improvements in system validation.","['Explainability', 'Formal Methods', 'Human Experiments', 'Robotics']",[],"['Isabelle Hurley', 'Rohan R Paleja', 'Ashley Suh', 'Jaime Daniel Pena', 'Ho Chit Siu']","['AI Technology Group, MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'AI Technology & Systems, MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology']",
https://openreview.net/forum?id=ZYNYhh3ocW,Fairness & Bias,Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework,"Logic Synthesis (LS) aims to generate an optimized logic circuit satisfying a given functionality, which generally consists of circuit translation and optimization. It is a challenging and fundamental combinatorial optimization problem in integrated circuit design. Traditional LS approaches rely on manually designed heuristics to tackle the LS task, while machine learning recently offers a promising approach towards next-generation logic synthesis by neural circuit generation and optimization. In this paper, we first revisit the application of differentiable neural architecture search (DNAS) methods to circuit generation and found from extensive experiments that existing DNAS methods struggle to exactly generate circuits, scale poorly to large circuits, and exhibit high sensitivity to hyper-parameters. Then we provide three major insights for these challenges from extensive empirical analysis: 1) DNAS tends to overfit to too many skip-connections, consequently wasting a significant portion of the network's expressive capabilities; 2) DNAS suffers from the structure bias between the network architecture and the circuit inherent structure, leading to inefficient search; 3) the learning difficulty of different input-output examples varies significantly, leading to severely imbalanced learning. To address these challenges in a systematic way, we propose a novel regularized triangle-shaped circuit network generation framework, which leverages our key insights for completely accurate and scalable circuit generation. Furthermore, we propose an evolutionary algorithm assisted by reinforcement learning agent restarting technique for efficient and effective neural circuit optimization. Extensive experiments on four different circuit benchmarks demonstrate that our method can precisely generate circuits with up to 1200 nodes. Moreover, our synthesized circuits significantly outperform the state-of-the-art results from several competitive winners in IWLS 2022 and 2023 competitions.","['Electronic Design Automation', 'Logic Synthesis', 'Neural Architecture Search']",[],"['Zhihai Wang', 'Jie Wang', 'Qingyue Yang', 'Yinqi Bai', 'Xing Li', 'Lei Chen', 'Jianye HAO', 'Mingxuan Yuan', 'Bin Li', 'Yongdong Zhang', 'Feng Wu']","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Tianjin University', 'Hong Kong Research Center, Huawei Technologies Ltd.', 'School of Information Science and Technology, University of Science and Technology of China', 'School of Information Science And Technology, University of Science and Technology of China', 'University of Science and Technology of China']",
https://openreview.net/forum?id=SCEdoGghcw,Transparency & Explainability,Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models,"What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features which we expect good SAEs to identify. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on Chess and Othello transcripts. These settings carry natural collections of interpretable features—for example, “there is a knight on F3”—which we leverage into metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, $p$-annealing, which demonstrates improved performance on our metric.","['Language models', 'interpretability', 'dictionary learning']",[],"['Adam Karvonen', 'Benjamin Wright', 'Can Rager', 'Rico Angell', 'Jannik Brinkmann', 'Logan Riggs Smith', 'Claudio Mayrink Verdun', 'David Bau', 'Samuel Marks']","['Galois', 'Massachusetts Institute of Technology', 'Universität Hamburg', 'Center for Data Science, New York University', 'Center for Data Science, New York University', 'Mississippi State University', 'Harvard University', 'Northeastern University', 'Northeastern University']",
https://openreview.net/forum?id=3MW44iNdrD,Fairness & Bias,FairQueue: Rethinking Prompt Learning for Fair Text-to-Image Generation,"Recently, prompt learning has emerged as the state-of-the-art (SOTA) for fair text-to-image (T2I) generation. Specifically, this approach leverages readily available reference images to learn inclusive prompts for each target Sensitive Attribute (tSA), allowing for fair image generation. In this work, we first reveal that this prompt learning-based approach results in degraded sample quality. Our analysis shows that the approach's training objective--which aims to align the embedding differences of learned prompts and reference images-- could be sub-optimal, resulting in distortion of the learned prompts and degraded generated images. To further substantiate this claim, **as our major contribution**, we deep dive into the denoising subnetwork of the T2I model to track down the effect of these learned prompts by analyzing the cross-attention maps. In our analysis, we propose a novel prompt switching analysis: I2H and H2I. Furthermore, we propose new quantitative characterization of cross-attention maps. Our analysis reveals abnormalities in the early denoising steps, perpetuating improper global structure that results in degradation in the generated samples. Building on insights from our analysis, we propose two ideas: (i) *Prompt Queuing* and (ii) *Attention Amplification* to address the quality issue. Extensive experimental results on a wide range of tSAs show that our proposed method outperforms SOTA approach's image generation quality, while achieving competitive fairness. More resources at FairQueue Project site: https://sutd-visual-computing-group.github.io/FairQueue","['fairness', 'generative modelling', 'text-to-image models', 'bias mitigation', 'prompt learning']",[],"['Christopher T.H Teo', 'Milad Abdollahzadeh', 'Xinda Ma', 'Ngai-man Cheung']","['Carnegie Mellon University', 'ISTD, Singapore University of Technology and Design', 'Singapore University of Technology and Design', 'Singapore University of Technology and Design']",
https://openreview.net/forum?id=8271eFxojN,Transparency & Explainability,Identifiability Analysis of Linear ODE Systems with Hidden Confounders,"The identifiability analysis of linear Ordinary Differential Equation (ODE) systems is a necessary prerequisite for making reliable causal inferences about these systems. While identifiability has been well studied in scenarios where the system is fully observable, the conditions for identifiability remain unexplored when latent variables interact with the system. This paper aims to address this gap by presenting a systematic analysis of identifiability in linear ODE systems incorporating hidden confounders. Specifically, we investigate two cases of such systems. In the first case, latent confounders exhibit no causal relationships, yet their evolution adheres to specific functional forms, such as polynomial functions of time $t$. Subsequently, we extend this analysis to encompass scenarios where hidden confounders exhibit causal dependencies, with the causal structure of latent variables described by a Directed Acyclic Graph (DAG). The second case represents a more intricate variation of the first case, prompting a more comprehensive identifiability analysis. Accordingly, we conduct detailed identifiability analyses of the second system under various observation conditions, including both continuous and discrete observations from single or multiple trajectories. To validate our theoretical results, we perform a series of simulations, which support and substantiate our findings.","['Linear ODEs', 'Identifiability analysis', 'Hidden confounders', 'Causality']",[],"['Yuanyuan Wang', 'Biwei Huang', 'Wei Huang', 'Xi Geng', 'Mingming Gong']","['University of Melbourne', 'University of California, San Diego', 'School of Mathematics and Statistics, University of Melbourne', '', 'School of mathematics and statistics, University of Melbourne']",
https://openreview.net/forum?id=lHcvjsQFQq,Fairness & Bias,Mitigating Covariate Shift in Behavioral Cloning via Robust Stationary Distribution Correction,"We consider offline imitation learning (IL), which aims to train an agent to imitate from the dataset of expert demonstrations without online interaction with the environment. Behavioral Cloning (BC) has been a simple yet effective approach to offline IL, but it is also well-known to be vulnerable to the covariate shift resulting from the mismatch between the state distributions induced by the learned policy and the expert policy. Moreover, as often occurs in practice, when expert datasets are collected from an arbitrary state distribution instead of a stationary one, these shifts become more pronounced, potentially leading to substantial failures in existing IL methods. Specifically, we focus on covariate shift resulting from arbitrary state data distributions, such as biased data collection or incomplete trajectories, rather than shifts induced by changes in dynamics or noisy expert actions. In this paper, to mitigate the effect of the covariate shifts in BC, we propose DrilDICE, which utilizes a distributionally robust BC objective by employing a stationary distribution correction ratio estimation (DICE) to derive a feasible solution. We evaluate the effectiveness of our method through an extensive set of experiments covering diverse covariate shift scenarios. The results demonstrate the efficacy of the proposed approach in improving the robustness against the shifts, outperforming existing offline IL methods in such scenarios.","['Imitation Learning', 'Behavioral Cloning', 'Robust Learning']",[],"['Seokin Seo', 'Byung-Jun Lee', 'Jongmin Lee', 'HyeongJoo Hwang', 'Hongseok Yang', 'Kee-Eung Kim']","['Korea Advanced Institute of Science and Technology', 'Department of Artificial Intelligence, Korea University', 'University of California, Berkeley', 'Graduate School of AI, Korea Advanced Institute of Science & Technology', 'School of Computing, Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']",
https://openreview.net/forum?id=zxSWIdyW3A,Privacy & Data Governance,Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging,"Existing reconstruction models in snapshot compressive imaging systems (SCI) are trained with a single well-calibrated hardware instance, making their perfor- mance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different hardware (e.g., coded apertures). Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware con- figurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous settings. Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems. Data and code are aveilable at https://github.com/Jiamian-Wang/FedHP-Snapshot-Compressive-Imaging.git","['snapshot compressive imaging', 'hyperpectral imaging', 'prompt learning', 'federated learning']",[],"['Jiamian Wang', 'Zongliang Wu', 'Yulun Zhang', 'Xin Yuan', 'Tao Lin', 'ZHIQIANG TAO']","['Rochester Institute of Technology', '', 'School of Electronics, Information and Electrical Engineering, Shanghai Jiaotong University', 'AI, Westlake University', 'School of Engineering, Westlake University', 'Rochester Institute of Technology']",
https://openreview.net/forum?id=xse8QMGnyM,Fairness & Bias,Toward Approaches to Scalability in 3D Human Pose Estimation,"In the field of 3D Human Pose Estimation (HPE), scalability and generalization across diverse real-world scenarios remain significant challenges. This paper addresses two key bottlenecks to scalability: limited data diversity caused by 'popularity bias' and increased 'one-to-many' depth ambiguity arising from greater pose diversity. We introduce the Biomechanical Pose Generator (BPG), which leverages biomechanical principles, specifically the normal range of motion, to autonomously generate a wide array of plausible 3D poses without relying on a source dataset, thus overcoming the restrictions of popularity bias. To address depth ambiguity, we propose the Binary Depth Coordinates (BDC), which simplifies depth estimation into a binary classification of joint positions (front or back). This method decomposes a 3D pose into three core elements—2D pose, bone length, and binary depth decision—substantially reducing depth ambiguity and enhancing model robustness and accuracy, particularly in complex poses. Our results demonstrate that these approaches increase the diversity and volume of pose data while consistently achieving performance gains, even amid the complexities introduced by increased pose diversity.","['3D Human Pose Estimation', 'Data generation', 'Pose Decompression']",[],"['Jun-Hee Kim', 'Seong-Whan Lee']","['Korea University', 'Korea University']",
https://openreview.net/forum?id=yiXZZC5qDI,Security,From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models,"While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to data poisoning attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the art necessitates modifications to the diffusion training and sampling procedures. Unlike the prior work, we investigate whether BadNets-like data poisoning methods can directly degrade the generation by DMs. In other words, if only the training dataset is contaminated (without manipulating the diffusion process), how will this affect the performance of learned DMs? In this setting, we uncover bilateral data poisoning effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for defense in classification tasks against poisoning attacks). We show that a BadNets-like data poisoning attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions). Meanwhile, poisoned DMs exhibit an increased ratio of triggers, a phenomenon we refer to as 'trigger amplification', among the generated images. This insight can be then used to enhance the detection of poisoned training data. In addition, even under a low poisoning ratio, studying the poisoning effects of DMs is also valuable for designing robust image classifiers against such attacks. Last but not least, we establish a meaningful linkage between data poisoning and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. Code is available at https://github.com/OPTML-Group/BiBadDiff.","['Diffusion model', 'data poisoning', 'data replication', 'diffusion classifier']",[],"['Zhuoshi Pan', 'Yuguang Yao', 'Gaowen Liu', 'Bingquan Shen', 'H. Vicky Zhao', 'Ramana Rao Kompella', 'Sijia Liu']","['Tsinghua University, Tsinghua University', 'Michigan State University', 'Cisco Systems', 'National University of Singapore', 'Department of ECE, Tsinghua University, Tsinghua University', 'Cisco Research, Cisco', 'CSE, Michigan State University']",
https://openreview.net/forum?id=wdGvRud1LS,Transparency & Explainability,Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios,"The cortico-spinal neural pathway is fundamental for motor control and movement execution, and in humans it is typically studied using concurrent electroencephalography (EEG) and electromyography (EMG) recordings. However, current approaches for capturing high-level and contextual connectivity between these recordings have important limitations. Here, we present a novel application of statistical dependence estimators based on orthonormal decomposition of density ratios to model the relationship between cortical and muscle oscillations. Our method extends from traditional scalar-valued measures by learning eigenvalues, eigenfunctions, and projection spaces of density ratios from realizations of the signal, addressing the interpretability, scalability, and local temporal dependence of cortico-muscular connectivity. We experimentally demonstrate that eigenfunctions learned from cortico-muscular connectivity can accurately classify movements and subjects. Moreover, they reveal channel and temporal dependencies that confirm the activation of specific EEG channels during movement.","['EEG-EMG fusion', 'statistical dependence', 'orthonormal decomposition', 'cortico-muscular connectivity']",[],"['Shihan Ma', 'Bo Hu', 'Tianyu Jia', 'Alexander Kenneth Clarke', 'Blanka Zicher', 'Arnault H. Caillet', 'Dario Farina', 'Jose C Principe']","['Facebook', 'University of Florida', 'Bioengineering, Imperial College London', 'Imperial College London', 'Imperial College London', 'Imperial College London', 'Imperial College London', 'University of Florida']",
https://openreview.net/forum?id=vunJCq9PwU,Transparency & Explainability,GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models,"Current studies on adversarial robustness mainly focus on aggregating \textit{local} robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true \textit{global} robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called \textit{GREAT Score}, for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In particular, we show high correlation and significantly reduced computation cost of GREAT Score when compared to the attack-based model ranking on RobustBench \cite{croce2021robustbench}. (2) The use of generative models facilitates the approximation of the unknown data distribution. In our ablation study with different generative adversarial networks (GANs), we observe consistency between global robustness evaluation and the quality of GANs. (3) GREAT Score can be used for remote auditing of privacy-sensitive black-box models, as demonstrated by our robustness evaluation on several online facial recognition services.",['Adversarial Robustness'],[],"['ZAITANG LI', 'Pin-Yu Chen', 'Tsung-Yi Ho']","['Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'International Business Machines', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong']",
https://openreview.net/forum?id=utMOhsgXzB,Fairness & Bias,BendVLM: Test-Time Debiasing of Vision-Language Embeddings,"Vision-language (VL) embedding models have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. Due to their wide-spread adoption for various tasks ranging from few-shot classification to text-guided image generation, debiasing VL models is crucial. Debiasing approaches that fine-tune the VL model often suffer from catastrophic forgetting. On the other hand, fine-tuning-free methods typically utilize a ``one-size-fits-all"" approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs. In this work, we propose a nonlinear, fine-tuning-free approach for VL embedding model debiasing that tailors the debiasing operation to each unique input. This allows for a more flexible debiasing approach. Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online tasks such as retrieval and text guided image generation.","['vision language models', 'embedding models', 'multimodal models', 'debias', 'fairness']",[],"['Walter Gerych', 'Haoran Zhang', 'Kimia Hamidieh', 'Eileen Pan', 'Maanas Sharma', 'Thomas Hartvigsen', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'EECS, Massachusetts Institute of Technology', 'Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology', 'University of Virginia, Charlottesville', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=uBVCPAMDGk,Fairness & Bias,Enhancing Consistency-Based Image Generation via Adversarialy-Trained Classification and Energy-Based Discrimination,"The recently introduced Consistency models pose an efficient alternative to diffusion algorithms, enabling rapid and good quality image synthesis. These methods overcome the slowness of diffusion models by directly mapping noise to data, while maintaining a (relatively) simpler training. Consistency models enable a fast one- or few-step generation, but they typically fall somewhat short in sample quality when compared to their diffusion origins.  In this work we propose a novel and highly effective technique for post-processing Consistency-based generated images, enhancing their perceptual quality. Our approach utilizes a joint classifier-discriminator model, in which both portions are trained adversarially. While the classifier aims to grade an image based on its assignment to a designated class, the discriminator portion of the very same network leverages the softmax values to assess the proximity of the input image to the targeted data manifold, thereby serving as an Energy-based Model. By employing example-specific projected gradient iterations under the guidance of this joint machine, we refine synthesized images and achieve an improved FID scores on the ImageNet 64x64 dataset for both Consistency-Training and Consistency-Distillation techniques.",['Adversarial Training; Robustness; Energy-Based Models; Classification;'],[],"['Shelly Golan', 'Roy Ganz', 'Michael Elad']","['Computer Science Department, Technion - Israel Institute of Technology', 'Technion - Israel Institute of Technology, Technion', 'Computer Science Department, Technion - Israel Institute of Technology']",
https://openreview.net/forum?id=shYQXpnBLB,Fairness & Bias,Association of Objects May Engender Stereotypes: Mitigating Association-Engendered Stereotypes in Text-to-Image Generation,"Text-to-Image (T2I) has witnessed significant advancements, demonstrating superior performance for various generative tasks. However, the presence of stereotypes in T2I introduces harmful biases that require urgent attention as the T2I   technology becomes more prominent. Previous work for stereotype mitigation mainly concentrated on mitigating stereotypes engendered with individual objects within images, which failed to address stereotypes engendered by the association of multiple objects, referred to as *Association-Engendered Stereotypes*. For example, mentioning  ''black people'' and ''houses''  separately in prompts may not exhibit stereotypes. Nevertheless, when these two objects are associated in prompts, the association of ''black people'' with ''poorer houses'' becomes more pronounced. To tackle this issue, we propose a novel framework, MAS, to Mitigate Association-engendered Stereotypes. This framework models the stereotype problem as a probability distribution alignment problem, aiming to align the stereotype probability distribution of the generated image with the stereotype-free distribution. The MAS framework primarily consists of the *Prompt-Image-Stereotype CLIP* (*PIS CLIP*) and *Sensitive Transformer*. The *PIS CLIP* learns the association between prompts, images, and stereotypes, which can establish the mapping of prompts to stereotypes. The *Sensitive Transformer* produces the sensitive constraints, which guide the stereotyped image distribution to align with the stereotype-free probability distribution. Moreover, recognizing that existing metrics are insufficient for accurately evaluating association-engendered stereotypes, we propose a novel metric, *Stereotype-Distribution-Total-Variation*(*SDTV*), to evaluate stereotypes in T2I. Comprehensive experiments demonstrate that our framework effectively mitigates association-engendered stereotypes.","['Stereotypes', 'Diffusion Model', 'Text-to-Image']",[],"['Junlei Zhou', 'Jiashi Gao', 'Xiangyu Zhao', 'Xin Yao', 'Xuetao Wei']","['Southern University of Science and Technology', 'Department of Computer Science and Engineering, Southern University of Science and Technology', 'Department of Data Science, City University of Hong Kong', 'Southern University of Science and Technology', 'Computer Science and Engineering , Southern University of Science and Technology']",
https://openreview.net/forum?id=s63dtq0mwA,Transparency & Explainability,Understanding Information Storage and Transfer in Multi-Modal Large Language Models,"Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by \emph{the director in this photo} has won a \emph{Golden Globe}?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) \emph{VQA-Constraints}, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks. We will publicly release our dataset and code.","['interpretability', 'multimodal generative models', 'VQA']",[],"['Samyadeep Basu', 'Martin Grayson', 'Cecily Morrison', 'Besmira Nushi', 'Soheil Feizi', 'Daniela Massiceti']","['Computer Science, University of Maryland, College Park', '', 'Research, Microsoft', 'Microsoft', 'University of Maryland, College Park', 'Research, Microsoft']",
https://openreview.net/forum?id=rniiAVjHi5,Fairness & Bias,"Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction","We present adaptive gradient methods (both basic and accelerated) for solving convex composite optimization problems in which the main part is approximately smooth (a.k.a. $(\delta, L)$-smooth) and can be accessed only via a (potentially biased) stochastic gradient oracle. This setting covers many interesting examples including Hölder smooth problems and various inexact computations of the stochastic gradient. Our methods use AdaGrad stepsizes and are adaptive in the sense that they do not require knowing any problem-dependent constants except an estimate of the diameter of the feasible set but nevertheless achieve the best possible convergence rates as if they knew the corresponding constants. We demonstrate that AdaGrad stepsizes work in a variety of situations by proving, in a unified manner, three types of new results. First, we establish efficiency guarantees for our methods in the classical setting where the oracle's variance is uniformly bounded. We then show that, under more refined assumptions on the variance, the same methods without any modifications enjoy implicit variance reduction properties allowing us to express their complexity estimates in terms of the variance only at the minimizer. Finally, we show how to incorporate explicit SVRG-type variance reduction into our methods and obtain even faster algorithms. In all three cases, we present both basic and accelerated algorithms achieving state-of-the-art complexity bounds. As a direct corollary of our results, we obtain universal stochastic gradient methods for Hölder smooth problems which can be used in all situations.","['convex optimization', 'stochastic optimization', 'adaptive methods', 'universal algorithms', 'acceleration', 'variance reduction', 'AdaGrad', 'SVRG', 'weakly smooth functions', 'Hölder condition', 'inexact oracle', 'complexity estimates']",[],"['Anton Rodomanov', 'Xiaowen Jiang', 'Sebastian U Stich']","['CISPA', 'CISPA Helmholtz Center for Information Security', 'CISPA Helmholtz Center for Information Security']",
https://openreview.net/forum?id=rPKCrzdqJx,Security,Regret Minimization in Stackelberg Games with Side Information,"Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader commits to a (context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve good performance (measured by regret) in the full adversarial setting.  Motivated by our impossibility result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which the sequence of contexts is stochastic and the sequence of followers is chosen by an adversary.","['Stackelberg game', 'context', 'online learning']",[],"['Keegan Harris', 'Steven Wu', 'Maria Florina Balcan']","['Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University']",
https://openreview.net/forum?id=pyqPUf36D2,Security,Pseudo-Private Data Guided Model Inversion Attacks,"In model inversion attacks (MIAs), adversaries attempt to recover private training data by exploiting access to a well-trained target model. Recent advancements have improved MIA performance using a two-stage generative framework. This approach first employs a generative adversarial network to learn a fixed distributional prior, which is then used to guide the inversion process during the attack. However, in this paper, we observed a phenomenon that such a fixed prior would lead to a low probability of sampling actual private data during the inversion process due to the inherent distribution gap between the prior distribution and the private data distribution, thereby constraining attack performance. To address this limitation, we propose increasing the density around high-quality pseudo-private data—recovered samples through model inversion that exhibit characteristics of the private training data—by slightly tuning the generator. This strategy effectively increases the probability of sampling actual private data that is close to these pseudo-private data during the inversion process. After integrating our method, the generative model inversion pipeline is strengthened, leading to improvements over state-of-the-art MIAs. This paves the way for new research directions in generative MIAs.",['Model Inversion Attacks'],[],"['Xiong Peng', 'Bo Han', 'Feng Liu', 'Tongliang Liu', 'Mingyuan Zhou']","['Hong Kong Baptist University', 'Department of Computer Science, HKBU', 'Computing and Information Systems, University of Melbourne', 'University of Sydney', 'Google']",
https://openreview.net/forum?id=mfvKEdJ4zW,Transparency & Explainability,Latent Functional Maps: a spectral framework for representation alignment,"Neural models learn data representations that lie on low-dimensional manifolds, yet modeling the relation between these representational spaces is an ongoing challenge. By integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks.  To this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces. We validate our framework on various applications, ranging from stitching to retrieval tasks, and on multiple modalities, demonstrating that Latent Functional Maps can serve as a swiss-army knife for representation alignment.",['Representation alignment; Spectral methods;'],[],"['Marco Fumero', 'Marco Pegoraro', 'Valentino Maiorca', 'Francesco Locatello', 'Emanuele Rodolà']","['Institute of Science and Technology Austria(ISTA)', 'University of Roma ""La Sapienza""', 'Computer Science, University of Roma ""La Sapienza""', 'Institute of Science and Technology', 'Sapienza University of Rome']",
https://openreview.net/forum?id=m296WJXyzQ,Security,Scanning Trojaned Models Using Out-of-Distribution Samples,"Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named TRODO (TROjan scanning by Detection of adversarial shifts in Out-of-distribution samples). TRODO leverages the concept of ""blind spots""—regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy.","['Trojan Scanning Method', 'Trojan Post-Training Defense', 'Backdoor Attacks', 'Out-of-Distribution Samples', 'Adversarially Perturbed Out-of-Distribution Samples']",[],"['Hossein Mirzaei', 'Ali Ansari', 'Bahar Dibaei Nia', 'Mojtaba Nafez', 'Moein Madadi', 'Sepehr Rezaee', 'Zeinab Sadat Taghavi', 'Arad Maleki', 'Kian Shamsaie', 'Mahdi Hajialilue', 'Jafar Habibi', 'Mohammad Sabokrou', 'Mohammad Hossein Rohban']","['EPFL - EPF Lausanne', 'Sharif University of Technology', 'Computer Engineering, Sharif University of Technology', 'Computer Engineering, Sharif University of Technology', 'Computer Engineering, Sharif University of Technology', 'Computer Science, The National University of Iran', 'The Center for Information and Language Processing (CIS), Ludwig-Maximilians-Universität München', 'Computer Engineering, Sharif University of Technology', 'Sharif University of Technology', 'Mathematical Sciences, Sharif University of Technology', 'Computer engineering, Sharif University of Technology', 'Okinawa Institute of Science and Technology (OIST)', 'Computer Engineering, Sharif University of Technology']",
https://openreview.net/forum?id=jBf3eIyD2x,Security,Query-Based Adversarial Prompt Generation,"Recent work has shown it is possible to construct adversarial examples that cause aligned language models to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through _transferability_: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a _query-based_ attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the OpenAI and Llama Guard safety classifiers with nearly 100% probability.","['adversarial examples', 'large language models', 'black box']",[],"['Jonathan Hayase', 'Ema Borevković', 'Nicholas Carlini', 'Florian Tramèr', 'Milad Nasr']","['Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Faculty of Science, Department of mathematics, University of Zagreb', 'Google', 'ETHZ - ETH Zurich', 'Google']",
https://openreview.net/forum?id=jMJVFP4BH6,Transparency & Explainability,Towards Neuron Attributions in Multi-Modal Large Language Models,"As Large Language Models (LLMs) demonstrate impressive capabilities, demystifying their internal mechanisms becomes increasingly vital. Neuron attribution, which attributes LLM outputs to specific neurons to reveal the semantic properties they learn, has emerged as a key interpretability approach. However, while neuron attribution has made significant progress in deciphering text-only LLMs, its application to Multimodal LLMs (MLLMs) remains less explored. To address this gap, we propose a novel Neuron Attribution method tailored for MLLMs, termed NAM. Specifically, NAM not only reveals the modality-specific semantic knowledge learned by neurons within MLLMs, but also highlights several intriguing properties of neurons, such as cross-modal invariance and semantic sensitivity. These properties collectively elucidate the inner workings mechanism of MLLMs, providing a deeper understanding of how MLLMs process and generate multi-modal content. Through theoretical analysis and empirical validation, we demonstrate the efficacy of NAM and the valuable insights it offers. Furthermore, leveraging NAM, we introduce a multi-modal knowledge editing paradigm, underscoring the practical significance of our approach for downstream applications of MLLMs.","['Neuron Attribution', 'Large Language Models', 'Multimodality']",[],"['Junfeng Fang', 'Zac Bi', 'Ruipeng Wang', 'Houcheng Jiang', 'Yuan Gao', 'Kun Wang', 'An Zhang', 'Jie Shi', 'Xiang Wang', 'Tat-Seng Chua']","['USTC, University of Science and Technology of China', 'TaoTian Group, Alibaba Group', '电子信息工程, University of Science and Technology of China', 'LDS, University of Science and Technology of China', 'Alibaba Group', 'Nanyang Technological University', 'National University of Singapore', 'Huawei International.', 'University of Science and Technology of China', 'Department of Computer Science, National University of Singapore']",
https://openreview.net/forum?id=jz5ZMeN9He,Transparency & Explainability,DRIP: Unleashing Diffusion Priors for Joint Foreground and Alpha Prediction in Image Matting,"Recovering the foreground color and opacity/alpha matte from a single image (i.e., image matting) is a challenging and ill-posed problem where data priors play a critical role in achieving precise results. Traditional methods generally predict the alpha matte and then extract the foreground through post-processing, often failing to produce high-fidelity foreground color. This failure stems from the models' difficulty in learning robust color predictions from limited matting datasets. To address this, we explore the potential of leveraging vision priors embedded in pre-trained latent diffusion models (LDM) for estimating foreground RGBA values in challenging scenarios and rare objects. We introduce Drip, a novel approach for image matting that harnesses the rich prior knowledge of LDM models. Our method incorporates a switcher and a cross-domain attention mechanism to extend the original LDM for joint prediction of the foreground color and opacity. This setup facilitates mutual information exchange and ensures high consistency across both modalities. To mitigate the inherent reconstruction errors of the LDM's VAE decoder, we propose a latent transparency decoder to align the RGBA prediction with the input image, thereby reducing discrepancies. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in foreground and alpha predictions and shows remarkable generalizability across various benchmarks.","['Image Matting', 'Diffusion Model', 'Foreground Estimation']",[],"['Xiaodi Li', 'Zongxin Yang', 'Ruijie Quan', 'Yi Yang']","['', 'Harvard Medical School, Harvard University', 'Nanyang Technological University', 'College of Computer Science and Technology, Zhejiang University']",
https://openreview.net/forum?id=ioKQzb8SMr,Fairness & Bias,Guided Trajectory Generation with Diffusion Models for Offline Model-based Optimization,"Optimizing complex and high-dimensional black-box functions is ubiquitous in science and engineering fields. Unfortunately, the online evaluation of these functions is restricted due to time and safety constraints in most cases. In offline model-based optimization (MBO), we aim to find a design that maximizes the target function using only a pre-existing offline dataset. While prior methods consider forward or inverse approaches to address the problem, these approaches are limited by conservatism and the difficulty of learning highly multi-modal mappings. Recently, there has been an emerging paradigm of learning to improve solutions with synthetic trajectories constructed from the offline dataset. In this paper, we introduce a novel conditional generative modeling approach to produce trajectories toward high-scoring regions. First, we construct synthetic trajectories toward high-scoring regions using the dataset while injecting locality bias for consistent improvement directions. Then, we train a conditional diffusion model to generate trajectories conditioned on their scores. Lastly, we sample multiple trajectories from the trained model with guidance to explore high-scoring regions beyond the dataset and select high-fidelity designs among generated trajectories with the proxy function. Extensive experiment results demonstrate that our method outperforms competitive baselines on Design-Bench and its practical variants. The code is publicly available in \url{https://github.com/dbsxodud-11/GTG}.","['Offline Model-based Optimization', 'Diffusion Models', 'Decision Making']",[],"['Taeyoung Yun', 'Sujin Yun', 'Jaewoo Lee', 'Jinkyoo Park']","['Korea Advanced Institute of Science & Technology', 'Graduate School of Data Science, Korea Advanced Institute of Science & Technology', 'Industrial System Engineering , Korea Advanced Institute of Science & Technology', 'Korea Advanced Institute of Science and Technology']",
https://openreview.net/forum?id=hkEwwAqmCk,Fairness & Bias,DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object Detection,"Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. As the visual-language models (VLMs) can provide essential general knowledge on unseen images, freezing the visual encoder and inserting a domain-agnostic adapter can learn domain-invariant knowledge for DAOD. However, the domain-agnostic adapter is inevitably biased to the source domain. It discards some beneficial knowledge discriminative on the unlabelled domain, \ie domain-specific knowledge of the target domain. To solve the issue, we propose a novel Domain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is exploiting domain-specific knowledge between the essential general knowledge and domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter (DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter (DSA) for injecting the domain-specific knowledge from the information discarded by the visual encoder. Comprehensive experiments over multiple DAOD tasks show that DA-Ada can efficiently infer a domain-aware visual encoder for boosting domain adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Ada.",['Unsupervised domain adaptation; Object detection; Visual-language model'],[],"['Haochen Li', 'Rui Zhang', 'Hantao Yao', 'Xin Zhang', 'Yifan Hao', 'Xinkai Song', 'Xiaqing Li', 'Yongwei Zhao', 'Yunji Chen', 'Ling Li']","['Institute of Software, Chinese Academy of Sciences', 'Institute of Computing Technology, CAS', ',Institute of automation, Chinese academy of science', 'Institute of Computing Technology, Chinese Academy of Sciences', 'SKLP, Institute of Computing Technology, Chinese Academy of Sciences', '', 'Institute of computing, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Software, CAS']",
https://openreview.net/forum?id=hocAc3Qit7,Transparency & Explainability,Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals,"Grid cells in the medial entorhinal cortex create remarkable periodic maps of explored space during navigation. Recent studies show that they form similar maps of abstract cognitive spaces. Examples of such abstract environments include auditory tone sequences in which the pitch is continuously varied or images in which abstract features are continuously deformed (e.g., a cartoon bird whose legs stretch and shrink). Here, we hypothesize that the brain generalizes how it maps spatial domains to mapping abstract spaces.  To sidestep the computational cost of learning representations for each high-dimensional sensory input, the brain extracts self-consistent, low-dimensional descriptions of displacements across abstract spaces, leveraging the spatial velocity integration of grid cells to efficiently build maps of different domains. Our neural network model for abstract velocity extraction factorizes the content of these abstract domains from displacements within the domains to generate content-independent and self-consistent, low-dimensional velocity estimates.  Crucially, it uses a self-supervised geometric consistency constraint that requires displacements along closed loop trajectories to sum to zero, an integration that is itself performed by the downstream grid cell circuit over learning. This process results in high fidelity estimates of velocities and allowed transitions in abstract domains, a crucial prerequisite for efficient map generation in these high-dimensional environments. We also show how our method outperforms traditional dimensionality reduction and deep-learning based motion extraction networks on the same set of tasks. This is the first neural network model to explain how grid cells can flexibly represent different abstract spaces and makes the novel prediction that they should do so while maintaining their population correlation and manifold structure across domains. Fundamentally, our model sheds light on the mechanistic origins of cognitive flexibility and transfer of representations across vastly different domains in brains, providing a potential self-supervised learning (SSL) framework for leveraging similar ideas in transfer learning and data-efficient generalization in machine learning and robotics.","['grid cells', 'cognitive mapping', 'cognitive maps', 'self-supervised learning', 'entorhinal cortex', 'path integration', 'neuroscience', 'dimensionality reduction']",[],"['Abhiram Iyer', 'Sarthak Chandra', 'Sugandha Sharma', 'Ila R Fiete']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'NVIDIA', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=hdUCZiMkFO,Fairness & Bias,Happy: A Debiased Learning Framework for Continual Generalized Category Discovery,"Constantly discovering novel concepts is crucial in evolving environments. This paper explores the underexplored task of Continual Generalized Category Discovery (C-GCD), which aims to incrementally discover new classes from *unlabeled* data while maintaining the ability to recognize previously learned classes. Although several settings are proposed to study the C-GCD task, they have limitations that do not reflect real-world scenarios. We thus study a more practical C-GCD setting, which includes more new classes to be discovered over a longer period, without storing samples of past classes. In C-GCD, the model is initially trained on labeled data of known classes, followed by multiple incremental stages where the model is fed with unlabeled data containing both old and new classes. The core challenge involves two conflicting objectives: discover new classes and prevent forgetting old ones. We delve into the conflicts and identify that models are susceptible to *prediction bias* and *hardness bias*. To address these issues, we introduce a debiased learning framework, namely **Happy**, characterized by **H**ardness-**a**ware **p**rototype sampling and soft entro**py** regularization. For the *prediction bias*, we first introduce clustering-guided initialization to provide robust features. In addition, we propose soft entropy regularization to assign appropriate probabilities to new classes, which can significantly enhance the clustering performance of new classes. For the *harness bias*, we present the hardness-aware prototype sampling, which can effectively reduce the forgetting issue for previously seen classes, especially for difficult classes. Experimental results demonstrate our method proficiently manages the conflicts of C-GCD and achieves remarkable performance across various datasets, e.g., 7.5% overall gains on ImageNet-100. Our code is publicly available at https://github.com/mashijie1028/Happy-CGCD.","['novel category discovery', 'generalized category discovery', 'continual category discovery', 'continual generalized category discovery']",[],"['Shijie Ma', 'Fei Zhu', 'Zhun Zhong', 'Wenzhuo Liu', 'Xu-Yao Zhang', 'Cheng-Lin Liu']","['Institute of automation, Chinese academy of science, Chinese Academy of Sciences', 'Centre for Artificial Intelligence and Robotics Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences', 'University of Nottingham', 'Microsoft', '', 'Institute of automation, Chinese academy of science, Chinese Academy of Sciences']",
https://openreview.net/forum?id=hilGwNabqB,Privacy & Data Governance,A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings,"Federated learning (FL), through its privacy-preserving collaborative learning approach, has significantly empowered decentralized devices. However,  constraints in either data and/or computational resources among participating clients introduce several challenges in learning, including the inability to train large model architectures, heightened risks of overfitting, and more. In this work, we present a novel FL framework grounded in Bayesian learning to address these challenges. Our approach involves training personalized Bayesian models at each client tailored to the unique complexities of the clients' datasets and efficiently collaborating across these clients. By leveraging Bayesian neural networks and their uncertainty quantification capabilities, our local training procedure robustly learns from small datasets. And the novel collaboration procedure utilizing priors in the functional (output) space of the networks facilitates collaboration across models of varying sizes, enabling the framework to adapt well in heterogeneous data and computational settings. Furthermore, we present a differentially private version of the algorithm, accompanied by formal differential privacy guarantees that apply without any assumptions on the learning algorithm. Through experiments on popular FL datasets, we demonstrate that our approach outperforms strong baselines in both homogeneous and heterogeneous settings, and under strict privacy constraints.","['Federated Learning', 'Bayesian Learning']",[],"['Disha Makhija', 'Joydeep Ghosh', 'Nhat Ho']","['University of Texas at Austin', 'Electrical & Computer Engr, University of Texas, Austin', 'University of Texas, Austin']",
https://openreview.net/forum?id=h1iMVi2iEM,Privacy & Data Governance,A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs,"As a popular paradigm for juggling data privacy and collaborative training, federated learning (FL) is flourishing to distributively process the large scale of heterogeneous datasets on edged clients. Due to bandwidth limitations and security considerations, it ingeniously splits the original problem into multiple subproblems to be solved in parallel, which empowers primal dual solutions to great application values in FL. In this paper, we review the recent development of classical federated primal dual methods and point out a serious common defect of such methods in non-convex scenarios, which we say is a ``dual drift'' caused by dual hysteresis of those longstanding inactive clients under partial participation training. To further address this problem, we propose a novel Aligned Federated Primal Dual (A-FedPD) method, which constructs virtual dual updates to align global consensus and local dual variables for those protracted unparticipated local clients. Meanwhile, we provide a comprehensive analysis of the optimization and generalization efficiency for the A-FedPD method on smooth non-convex objectives, which confirms its high efficiency and practicality. Extensive experiments are conducted on several classical FL setups to validate the effectiveness of our proposed method.","['Federated primal dual methods', 'dual drift', 'virtual dual update', 'generalization']",[],"['Yan Sun', 'Li Shen', 'Dacheng Tao']","['Computer Science, University of Sydney', 'Sun Yat-Sen University', '']",
https://openreview.net/forum?id=h1grUs6CjN,Fairness & Bias,The Price of Implicit Bias in Adversarially Robust Generalization,"We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and its connection with robust generalization.  In classification settings under adversarial perturbations with linear models, we study what type of regularization should ideally be applied for a given perturbation set to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM can significantly affect the robustness of the model and identify two ways this can happen; either through the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.","['Adversarial Robustness', 'Robust Generalization Gap', 'Implicit Bias', 'Optimisation', 'Generalization']",[],"['Nikolaos Tsilivis', 'Natalie Frank', 'Nathan Srebro', 'Julia Kempe']","['Facebook', 'New York University', 'Toyota Technological Institute at Chicago', '']",
https://openreview.net/forum?id=gzh9nTUtsY,Fairness & Bias,Least Squares Regression Can Exhibit Under-Parameterized Double Descent,"The relationship between the number of training data points, the number of parameters, and the generalization capabilities of models has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime and that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work.","['Learning Theory', 'Generalization', 'Random Matrix Theory', 'High Dimensional Statistics']",[],"['Xinyue Li', 'Rishi Sonthalia']","['Applied Mathematics, Yale University', 'Math, Boston College']",
https://openreview.net/forum?id=gJbZyKGfd6,Transparency & Explainability,HyperLogic: Enhancing Diversity and Accuracy in Rule Learning with HyperNets,"Exploring the integration of if-then logic rules within neural network architectures  presents an intriguing area. This integration seamlessly transforms the rule learning task into neural network training using backpropagation and stochastic gradient descent. From a well-trained sparse and shallow neural network, one can interpret each layer and neuron through the language of logic rules, and a global explanatory rule set can be directly extracted. However, ensuring interpretability may impose constraints on the flexibility, depth, and width of neural networks. In this paper, we propose HyperLogic: a novel framework leveraging hypernetworks to generate weights of the main network. HyperLogic can unveil multiple diverse rule sets, each capable of capturing heterogeneous patterns in data. This provides a simple yet effective method to increase model flexibility and preserve interpretability. We theoretically analyzed the benefits of the HyperLogic by examining the approximation error and generalization capabilities under two types of regularization terms: sparsity and diversity regularizations. Experiments on real data demonstrate that our method can learn more diverse, accurate, and concise rules.","['Differentiable rule learning', 'Heterogeneous logic rule', 'Hypernet']",[],"['Yang Yang', 'Wendi Ren', 'Shuang Li']","['The Chinese University of Hong Kong,Shenzhen', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong (Shenzhen)']",
https://openreview.net/forum?id=gC3BzNwqQp,Fairness & Bias,Online Learning of Delayed Choices,"Choice models are essential for understanding decision-making processes in domains like online advertising, product recommendations, and assortment optimization. The Multinomial Logit (MNL) model is particularly versatile in selecting products or advertisements for display. However, challenges arise with unknown MNL parameters and delayed feedback, requiring sellers to learn customers’ choice behavior and make dynamic decisions with biased knowledge due to delays. We address these challenges by developing an algorithm that handles delayed feedback, balancing exploration and exploitation using confidence bounds and optimism. We first consider a censored setting where a threshold for considering feedback is imposed by business requirements. Our algorithm demonstrates a $\tilde{O}(\sqrt{NT})$ regret, with a matching lower bound up to a logarithmic term. Furthermore, we extend our analysis to environments with non-thresholded delays, achieving a $\tilde{O}(\sqrt{NT})$ regret. To validate our approach, we conduct experiments that confirm the effectiveness of our algorithm.","['choice models', 'multinomial logit', 'delayed feedback']",[],['Recep Yusuf Bekci'],"['Management Sciences, University of Waterloo']",
https://openreview.net/forum?id=fkf0OquD3Q,Privacy & Data Governance,Private Online Learning via Lazy Algorithms,"We study the problem of private online learning, specifically, online prediction from experts (OPE) and online convex optimization (OCO).     We propose a new transformation that transforms lazy online learning algorithms into private algorithms. We apply our transformation for differentially private OPE and OCO using existing lazy algorithms for these problems. Our final algorithms obtain regret which significantly improves the regret in the high privacy regime $\varepsilon \ll 1$, obtaining $\sqrt{T \log d} + T^{1/3} \log(d)/\varepsilon^{2/3}$ for DP-OPE and $\sqrt{T} + T^{1/3} \sqrt{d}/\varepsilon^{2/3}$ for DP-OCO. We also complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms.","['Differential Privacy', 'Online Optimization']",[],"['Hilal Asi', 'Tomer Koren', 'Daogao Liu', 'Kunal Talwar']","['Apple', 'Tel Aviv University', 'University of Washington, Seattle', 'Apple']",
https://openreview.net/forum?id=gLoe70Tn8V,Fairness & Bias,Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random,"In most practical applications such as recommendation systems, display advertising, and so forth, the collected data often contains missing values and those missing values are generally missing-not-at-random, which deteriorates the prediction performance of models. Some existing estimators and regularizers attempt to achieve unbiased estimation to improve the predictive performance. However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness. In this paper, we first theoretically reveal that limitations of regularization techniques. Besides, we further illustrate that, for more general estimators, unbiasedness will inevitably lead to unbounded variance. These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off. Instead, it involves a quantitative joint optimization of bias and variance. Then, we develop a systematic fine-grained dynamic learning framework to jointly optimize bias and variance, which adaptively selects an appropriate estimator for each user-item pair according to the predefined objective function. With this operation, the generalization bounds and variances of models are reduced and bounded with theoretical guarantees. Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework.","['Advertising recommendation systems', 'Quantifying bias-variance joint optimization', 'Limitations of regularization']",[],"['Mingming Ha', 'Taoxuewen', 'Wenfang Lin', 'QIONGXU MA', 'Wujiang Xu', 'Linxun Chen']","['Ant Group', 'Alibaba Group', 'Alibaba Group', 'MYbank，Ant Group', 'Rutgers University', 'Ant Group']",
https://openreview.net/forum?id=g8wnC1E1OS,Security,Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning,"Backdoor attacks pose a serious threat to federated systems, where malicious clients optimize on the triggered distribution to mislead the global model towards a predefined target. Existing backdoor defense methods typically require either homogeneous assumption, validation datasets, or client optimization conflicts. In our work, we observe that benign heterogeneous distributions and malicious triggered distributions exhibit distinct parameter importance degrees. We introduce the Fisher Discrepancy Cluster and Rescale (FDCR) method, which utilizes Fisher Information to calculate the degree of parameter importance for local distributions. This allows us to reweight client parameter updates and identify those with large discrepancies as backdoor attackers. Furthermore, we prioritize rescaling important parameters to expedite adaptation to the target distribution, encouraging significant elements to contribute more while diminishing the influence of trivial ones. This approach enables FDCR to handle backdoor attacks in heterogeneous federated learning environments. Empirical results on various heterogeneous federated scenarios under backdoor attacks demonstrate the effectiveness of our method.","['Federated Learning', 'Backdoor Attack']",[],"['Wenke Huang', 'Mang Ye', 'Zekun Shi', 'Guancheng Wan', 'He Li', 'Bo Du']","['Wuhan University, Wuhan University', 'Wuhan University', 'Xiaomi Corporation', 'Emory University, Emory University', 'School of Computer Science, Wuhan University', 'School of Computer Science, Wuhan University']",
https://openreview.net/forum?id=fjLCqicn64,Fairness & Bias,Long-range Brain Graph Transformer,"Understanding communication and information processing among brain regions of interest (ROIs) is highly dependent on long-range connectivity, which plays a crucial role in facilitating diverse functional neural integration across the entire brain. However, previous studies generally focused on the short-range dependencies within brain networks while neglecting the long-range dependencies, limiting an integrated understanding of brain-wide communication. To address this limitation, we propose Adaptive Long-range aware TransformER (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs utilizing biased random walk. Specifically, we present a novel long-range aware strategy to explicitly capture long-range dependencies between brain ROIs. By guiding the walker towards the next hop with higher correlation value, our strategy simulates the real-world brain-wide communication. Furthermore, by employing the transformer framework, ALERT adaptively integrates both short- and long-range dependencies between brain ROIs, enabling an integrated understanding of multi-level communication across the entire brain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis.","['long-range dependencies', 'brain graph transformer', 'brain disease prediction', 'graph learning', 'graph neural networks']",[],"['Shuo Yu', 'Shan Jin', 'Ming Li', 'Tabinda Sarwar', 'Feng Xia']","['School of Computer Science and Technology, Dalian University of Technology', 'Dalian University of Technology', 'Zhejiang Normal University', 'Royal Melbourne Institute of Technology', 'School of Computing Technologies, Royal Melbourne Institute of Technology']",
https://openreview.net/forum?id=ejWvCpLuwu,Transparency & Explainability,RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks,"Graph regression is a fundamental task that has gained significant attention in various graph learning tasks. However, the inference process is often not easily interpretable. Current explanation techniques are limited to understanding Graph Neural Network (GNN) behaviors in classification tasks, leaving an explanation gap for graph regression models. In this work, we propose a novel explanation method to interpret the graph regression models (XAIG-R). Our method addresses the distribution shifting problem and continuously ordered decision boundary issues that hinder existing methods away from being applied in regression tasks. We introduce a novel objective based on the graph information bottleneck theory (GIB) and a new mix-up framework, which can support various GNNs and explainers in a model-agnostic manner. Additionally, we present a self-supervised learning strategy to tackle the continuously ordered labels in regression tasks. We evaluate our proposed method on three benchmark datasets and a real-life dataset introduced by us, and extensive experiments demonstrate its effectiveness in interpreting GNN models in regression tasks.","['graph neural network', 'explainability', 'data augmentation']",[],"['Jiaxing Zhang', 'Zhuomin Chen', 'hao mei', 'Longchao Da', 'Dongsheng Luo', 'Hua Wei']","['YWCC, New Jersey Institute of Technology', '', 'Arizona State University', 'Arizona State University', '', 'Arizona State University']",
https://openreview.net/forum?id=eXNyq8FGSz,Fairness & Bias,Active Learning of General Halfspaces: Label Queries vs Membership Queries,"We study the problem of learning general (i.e., not necessarily homogeneous)  halfspaces under the Gaussian distribution on $\mathbb{R}^d$  in the presence of some form of query access.  In the classical pool-based active learning model, where the algorithm is allowed to make adaptive label queries to previously sampled points,  we establish a strong information-theoretic lower bound ruling out non-trivial improvements over the passive setting. Specifically, we show that any active learner requires label complexity of  $\tilde{\Omega}(d/(\log(m)\epsilon))$, where $m$ is the number of unlabeled examples.  Specifically, to beat the passive label complexity of $\tilde{O}(d/\epsilon)$,  an active learner requires a pool of $2^{\mathrm{poly}(d)}$ unlabeled samples. On the positive side, we show that this lower bound  can be circumvented with membership query access,  even in the agnostic model. Specifically, we give a computationally efficient  learner with query complexity of $\tilde{O}(\min(1/p, 1/\epsilon) + d\mathrm{polylog}(1/\epsilon))$ achieving error guarantee of $O(\mathrm{opt}+\epsilon)$. Here $p \in [0, 1/2]$  is the bias and $\mathrm{opt}$ is the 0-1 loss of the optimal halfspace.  As a corollary, we obtain a strong separation  between the active and membership query models.  Taken together, our results characterize the complexity of learning  general halfspaces under Gaussian marginals in these models.","['Active Learning', 'Membership Query', 'Linear Separator']",[],"['Ilias Diakonikolas', 'Daniel Kane', 'Mingchen Ma']","['Computer Sciences, University of Wisconsin - Madison', 'University of California, San Diego', 'University of Wisconsin - Madison']",
https://openreview.net/forum?id=eGIzeTmAtE,Security,ColJailBreak: Collaborative Generation and Editing for Jailbreaking Text-to-Image Deep Generation,"The commercial text-to-image deep generation models (e.g. DALL·E) can produce high-quality images based on input language descriptions. These models incorporate a black-box safety filter to prevent the generation of unsafe or unethical content, such as violent, criminal, or hateful imagery. Recent jailbreaking methods generate adversarial prompts capable of bypassing safety filters and producing unsafe content, exposing vulnerabilities in influential commercial models. However, once these adversarial prompts are identified, the safety filter can be updated to prevent the generation of unsafe images. In this work, we propose an effective, simple, and difficult-to-detect jailbreaking solution: generating safe content initially with normal text prompts and then editing the generations to embed unsafe content. The intuition behind this idea is that the deep generation model cannot reject safe generation with normal text prompts, while the editing models focus on modifying the local regions of images and do not involve a safety strategy. However, implementing such a solution is non-trivial, and we need to overcome several challenges: how to automatically confirm the normal prompt to replace the unsafe prompts, and how to effectively perform editable replacement and naturally generate unsafe content. In this work, we propose the collaborative generation and editing for jailbreaking text-to-image deep generation (ColJailBreak), which comprises three key components: adaptive normal safe substitution, inpainting-driven injection of unsafe content, and contrastive language-image-guided collaborative optimization. We validate our method on three datasets and compare it to two baseline methods. Our method could generate unsafe content through two commercial deep generation models including GPT-4 and DALL·E 2.","['Text-to-Image Models', 'jailbreak attack']",[],"['Yizhuo Ma', 'Shanmin Pang', 'Qi Guo', 'Tianyu Wei', 'Qing Guo']","[""School of Software Engineering, Xi'an Jiaotong University"", ""Schol of Software Engineering, Xi'an Jiaotong University"", ""Xi'an Jiaotong University"", ""School of Software Engineering, Xi'an Jiaotong University"", 'National University of Singapore']",
https://openreview.net/forum?id=e2R4WNHHGQ,Fairness & Bias,Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium,"The persistent challenge of bias in machine learning models necessitates robust solutions to ensure parity and equal treatment across diverse groups, particularly in classification tasks. Current methods for mitigating bias often result in information loss and an inadequate balance between accuracy and fairness. To address this, we propose a novel methodology grounded in bilevel optimization principles. Our deep learning-based approach concurrently optimizes for both accuracy and fairness objectives, and under certain assumptions, achieving proven Pareto optimal solutions while mitigating bias in the trained model. Theoretical analysis indicates that the upper bound on the loss incurred by this method is less than or equal to the loss of the Lagrangian approach, which involves adding a regularization term to the loss function. We demonstrate the efficacy of our model primarily on tabular datasets such as UCI Adult and Heritage Health. When benchmarked against state-of-the-art fairness methods, our model exhibits superior performance, advancing fairness-aware machine learning solutions and bridging the accuracy-fairness gap. The implementation of FairBiNN is available on https://github.com/yazdanimehdi/FairBiNN.","['Fairness', 'Bilevel optimization', 'Pareto optimal', 'Stackelberg', 'Demographic parity', 'Fairness-accuracy trade-off']",[],"['Mehdi Yazdani-Jahromi', 'Ali Khodabandeh Yalabadi', 'Amirarsalan Rajabi', 'Aida Tayebi', 'Ivan Garibay', 'Ozlem Garibay']","['University of Central Florida', 'University of Central Florida', 'Integral Ad Science', 'Industrial Engineering, University of Central Florida', 'University of Central Florida', 'Industrial Engineering and Management Systsems, University of Central Florida']",
https://openreview.net/forum?id=dGQtja9X2C,Fairness & Bias,Thinking Forward: Memory-Efficient Federated Finetuning of Language Models,"Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. In this paper, we introduce Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using forward-mode AD that are closer estimations of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We formally prove that the global gradients in Spry are unbiased estimators of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1$\times$ in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings.  Spry reduces the convergence time by 1.2-20.3$\times$ and achieves 5.2-13.5\% higher accuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices. Our source code is available for replication at https://github.com/Astuary/Spry.","['Federated Learning', 'Large Language Models', 'Forward-mode Automatic Differentiation', 'Forward-mode AD', 'Memory-efficient Finetuning', 'Memory-efficiency', 'Data Heterogeneity']",[],"['Kunjal Panchal', 'Nisarg Parikh', 'Sunav Choudhary', 'Lijun Zhang', 'Yuriy Brun', 'Hui Guan']","['Department of Computer Science, University of Massachusetts at Amherst', 'University of Massachusetts at Amherst', '', 'Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts, Amherst']",
https://openreview.net/forum?id=cIBSsXowMr,Security,Boosting Transferability and Discriminability for Time Series Domain Adaptation,"Unsupervised domain adaptation excels in transferring knowledge from a labeled source domain to an unlabeled target domain, playing a critical role in time series applications. Existing time series domain adaptation methods either ignore frequency features or treat temporal and frequency features equally, which makes it challenging to fully exploit the advantages of both types of features. In this paper, we delve into transferability and discriminability, two crucial properties in transferable representation learning. It's insightful to note that frequency features are more discriminative within a specific domain, while temporal features show better transferability across domains. Based on the findings, we propose **A**dversarial **CO**-learning **N**etworks (**ACON**), to enhance transferable representation learning through a collaborative learning manner in three aspects: (1) Considering the multi-periodicity in time series, multi-period frequency feature learning is proposed to enhance the discriminability of frequency features; (2) Temporal-frequency domain mutual learning is proposed to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain; (3) Domain adversarial learning is conducted in the correlation subspaces of temporal-frequency features instead of original feature spaces to further enhance the transferability of both features. Extensive experiments conducted on a wide range of time series datasets and five common applications demonstrate the state-of-the-art performance of ACON. Code is available at <https://github.com/mingyangliu1024/ACON>.",['Time-series; Domain Adaptation; Transferability; Discriminability; Frequency domain'],[],"['Mingyang Liu', 'Xinyang Chen', 'Yang Shu', 'Xiucheng Li', 'Weili Guan', 'Liqiang Nie']","['School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen', 'School of computer science and technology, Harbin Institute of Technology, Shenzhen', 'East China Normal University', 'Harbin Institute of Technology', 'Harbin Institute of Technology (Shenzhen)', 'Harbin Institute of Technology (Shenzhen)']",
https://openreview.net/forum?id=chLoLUHnai,Fairness & Bias,Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization,"The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition. We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize. Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors. If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps. Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk. Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes.","['Deep Learning Theory', 'Large stepsize', 'Optimization', 'Implicit Bias']",[],"['Yuhang Cai', 'Jingfeng Wu', 'Song Mei', 'Michael Lindsey', 'Peter Bartlett']","['Math, University of California, Berkeley', 'University of California, Berkeley', 'University of California Berkeley', 'University of California, Berkeley', 'University of California - Berkeley']",
https://openreview.net/forum?id=c3OZBJpN7M,Privacy & Data Governance,FedGMKD: An Efficient Prototype Federated Learning Framework through Knowledge Distillation and Discrepancy-Aware Aggregation,"Federated Learning (FL) faces significant challenges due to data heterogeneity across distributed clients. To address this, we propose FedGMKD, a novel framework that combines knowledge distillation and differential aggregation for efficient prototype-based personalized FL without the need for public datasets or server-side generative models. FedGMKD introduces Cluster Knowledge Fusion, utilizing Gaussian Mixture Models to generate prototype features and soft predictions on the client side, enabling effective knowledge distillation while preserving data privacy. Additionally, we implement a Discrepancy-Aware Aggregation Technique that weights client contributions based on data quality and quantity, enhancing the global model's generalization across diverse client distributions. Theoretical analysis confirms the convergence of FedGMKD. Extensive experiments on benchmark datasets, including SVHN, CIFAR-10, and CIFAR-100, demonstrate that FedGMKD outperforms state-of-the-art methods, significantly improving both local and global accuracy in non-IID data settings.","['Federated Learning', 'Clustering', 'Aggragation method', 'Knowledge Distillation', 'Prototype', 'Heterogeneous Data Environments']",[],"['Jianqiao Zhang', 'Caifeng Shan', 'Jungong Han']","['Computer Science, University of Wales, Aberystwyth', 'Nanjing University', 'The University of Sheffield']",
https://openreview.net/forum?id=bf0MdFlz1i,Transparency & Explainability,Optimistic Verifiable Training by Controlling Hardware Nondeterminism,"The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning and backdoors, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which can be difficult to scale, and ``optimistic'' methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for  nondeterminism. Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2 (117M) models. Our  verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems.","['auditing', 'training', 'security', 'verification', 'safety', 'hardware', 'nondeterminism']",[],"['Megha Srivastava', 'Simran Arora', 'Dan Boneh']","['Stanford University', 'Stanford University', 'Computer Sciece, Stanford University']",
https://openreview.net/forum?id=aetbfmCcwg,Fairness & Bias,Debiasing Synthetic Data Generated by Deep Generative Models,"While synthetic data hold great promise for privacy protection, their statistical analysis poses significant challenges that necessitate innovative solutions. The use of deep generative models (DGMs) for synthetic data generation is known to induce considerable bias and imprecision into synthetic data analyses, compromising their inferential utility as opposed to original data analyses. This bias and uncertainty can be substantial enough to impede statistical convergence rates, even in seemingly straightforward analyses like mean calculation. The standard errors of such estimators then exhibit slower shrinkage with sample size than the typical 1 over root-$n$ rate. This complicates fundamental calculations like p-values and confidence intervals, with no straightforward remedy currently available. In response to these challenges, we propose a new strategy that targets synthetic data created by DGMs for specific data analyses. Drawing insights from debiased and targeted machine learning, our approach accounts for biases, enhances convergence rates, and facilitates the calculation of estimators with easily approximated large sample variances. We exemplify our proposal through a simulation study on toy data and two case studies on real-world data, highlighting the importance of tailoring DGMs for targeted data analysis. This debiasing strategy contributes to advancing the reliability and applicability of synthetic data in statistical inference.","['deep generative model', 'efficient influence function', 'inferential utility', 'synthetic data']",[],"['Alexander Decruyenaere', 'Heidelinde Dehaene', 'Paloma Rabaey', 'Johan Decruyenaere', 'Christiaan Polet', 'Thomas Demeester', 'Stijn Vansteelandt']","['Universiteit Gent', 'Universiteit Gent', 'Universiteit Gent', 'Universiteit Gent', 'Universiteit Gent', 'Ghent University - imec', 'Universiteit Gent']",
https://openreview.net/forum?id=aXYL24yhjN,Fairness & Bias,Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making,"As society increasingly relies on AI-based tools for decision-making in socially sensitive domains, investigating fairness and equity of such automated systems has become a critical field of inquiry. Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline. For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice).  In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness. We introduce the notion of a margin complement, which measures how much a prediction score $S$ changes due to a thresholding operation. We then demonstrate that the marginal difference in the optimal 0/1 predictor $\widehat Y$ between groups, written $P(\hat y \mid x_1) - P(\hat y \mid x_0)$, can be causally decomposed into the influences of $X$ on the $L_2$-optimal prediction score $S$ and the influences of $X$ on the margin complement $M$, along different causal pathways (direct, indirect, spurious). We then show that under suitable causal assumptions, the influences of $X$ on the prediction score $S$ are equal to the influences of $X$ on the true outcome $Y$. This yields a new decomposition of the disparity in the predictor $\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself. This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied. We apply our method to three real-world datasets and derive new insights on bias amplification in prediction and decision-making.","['Causal Fairness', 'Fair Machine Learning', 'Causal Inference', 'Trustworthy AI']",[],"['Drago Plecko', 'Elias Bareinboim']","['Columbia University', 'Columbia University']",
https://openreview.net/forum?id=ZvQ4Bn75kN,Transparency & Explainability,Video Diffusion Models are Training-free Motion Interpreter and Controller,"Video generation primarily aims to model authentic and customized motion across frames, making understanding and controlling the motion a crucial topic. Most diffusion-based studies on video motion focus on motion customization with training-based paradigms, which, however, demands substantial training resources and necessitates retraining for diverse models. Crucially, these approaches do not explore how video diffusion models encode cross-frame motion information in their features, lacking interpretability and transparency in their effectiveness. To answer this question, this paper introduces a novel perspective to understand, localize, and manipulate motion-aware features in video diffusion models. Through analysis using Principal Component Analysis (PCA), our work discloses that robust motion-aware feature already exists in video diffusion models. We present a new MOtion FeaTure (MOFT) by eliminating content correlation information and filtering motion channels. MOFT provides a distinct set of benefits, including the ability to encode comprehensive motion information with clear interpretability, extraction without the need for training, and generalizability across diverse architectures. Leveraging MOFT, we propose a novel training-free video motion control framework. Our method demonstrates competitive performance in generating natural and faithful motion, providing architecture-agnostic insights and applicability in a variety of downstream tasks.","['video generation', 'motion control']",[],"['Zeqi Xiao', 'Yifan Zhou', 'Shuai Yang', 'Xingang Pan']","['Nanyang Technological University', 'SCSE, Nanyang Technological University', 'WICT, Peking University', 'School of Computer Science and Engineering, Nanyang Technological University']",
https://openreview.net/forum?id=aJDGfynRw7,Fairness & Bias,IWBVT: Instance Weighting-based Bias-Variance Trade-off for Crowdsourcing,"In recent years, a large number of algorithms for label integration and noise correction have been proposed to infer the unknown true labels of instances in crowdsourcing. They have made great advances in improving the label quality of crowdsourced datasets. However, due to the presence of intractable instances, these algorithms are usually not as significant in improving the model quality as they are in improving the label quality. To improve the model quality, this paper proposes an instance weighting-based bias-variance trade-off (IWBVT) approach. IWBVT at first proposes a novel instance weighting method based on the complementary set and entropy, which mitigates the impact of intractable instances and thus makes the bias and variance of trained models closer to the unknown true results. Then, IWBVT performs probabilistic loss regressions based on the bias-variance decomposition, which achieves the bias-variance trade-off and thus reduces the generalization error of trained models. Experimental results indicate that IWBVT can serve as a universal post-processing approach to significantly improving the model quality of existing state-of-the-art label integration algorithms and noise correction algorithms.","['Crowdsourcing', 'Instance weighting', 'Bias-variance decomposition', 'Bias-variance trade-off']",[],"['Wenjun Zhang', 'Liangxiao Jiang', 'Chaoqun Li']","['School of Computer Science, China University of Geosciences', 'School of Computer Science, China University of Geosciences', 'School of Mathematics and Physics, China University of Geosciences']",
https://openreview.net/forum?id=ZmIAd3JaZN,Privacy & Data Governance,Truthful High Dimensional Sparse Linear Regression,"We study the problem of fitting the high dimensional sparse linear regression model, where the data are provided by strategic or self-interested agents (individuals) who prioritize their privacy of data disclosure. In contrast to the classical setting, our focus is on designing mechanisms that can effectively incentivize most agents to truthfully report their data while preserving the privacy of individual reports. Simultaneously, we seek an estimator which should be close to the underlying parameter.  We attempt to solve the problem by deriving a novel private estimator that has a closed-form expression.  Based on the estimator, we propose a mechanism which has the following properties via some appropriate design of the computation and payment scheme:  (1) the mechanism is $(o(1), O(n^{-\Omega({1})}))$-jointly differentially private, where $n$ is the number of agents; (2) it is an $o(\frac{1}{n})$-approximate Bayes Nash equilibrium for a $(1-o(1))$-fraction of agents to truthfully report their data; (3) the output could achieve an error of $o(1)$ to the underlying parameter; (4) it is individually rational for a $(1-o(1))$ fraction of agents in the mechanism; (5) the payment budget required from the analyst to run the mechanism is $o(1)$. To the best of our knowledge, this is the first study on designing truthful (and privacy-preserving) mechanisms for high dimensional sparse linear regression.","['truthful mechanism', 'Bayesian game', 'linear regression', 'differential privacy']",[],"['Liyang Zhu', 'Amina Manseur', 'Meng Ding', 'Jinyan Liu', 'Jinhui Xu', 'Di Wang']","['Duke University', ""Ecole Nationale de la Statistique et de l'Administration Economique"", 'State University of New York at Buffalo', 'Beijing Institute of Technology', 'Computer Science and Engineering, University at Buffalo, State University of New York', '']",
https://openreview.net/forum?id=YfQA78gEFA,Security,Collaboration! Towards Robust Neural Methods for Routing Problems,"Despite enjoying desirable efficiency and reduced reliance on domain expertise, existing neural methods for vehicle routing problems (VRPs) suffer from severe robustness issues — their performance significantly deteriorates on clean instances with crafted perturbations. To enhance robustness, we propose an ensemble-based *Collaborative Neural Framework (CNF)* w.r.t. the defense of neural VRP methods, which is crucial yet underexplored in the literature. Given a neural VRP method, we adversarially train multiple models in a collaborative manner to synergistically promote robustness against attacks, while boosting standard generalization on clean instances. A neural router is designed to adeptly distribute training instances among models, enhancing overall load balancing and collaborative efficacy. Extensive experiments verify the effectiveness and versatility of CNF in defending against various attacks across different neural VRP methods. Notably, our approach also achieves impressive out-of-distribution generalization on benchmark instances.","['Combinatorial Optimization', 'Vehicle Routing Problem', 'Ensemble', 'Robustness']",[],"['Jianan Zhou', 'Yaoxin Wu', 'Zhiguang Cao', 'Wen Song', 'Jie Zhang', 'Zhiqi Shen']","['Nanyang Technological University', 'Industrial Engineering and Innovation Sciences, Eindhoven University of Technology', 'School of Computing and Information Systems, Singapore Management University', 'Shandong University', 'Nanyang Technological University', 'College of Computing and Data Science, Nanyang Technological University']",
https://openreview.net/forum?id=WJAiaslhin,Transparency & Explainability,A Functional Extension of Semi-Structured Networks,"Semi-structured networks (SSNs) merge the structures familiar from additive models with deep neural networks, allowing the modeling of interpretable partial feature effects while capturing higher-order non-linearities at the same time. A significant challenge in this integration is maintaining the interpretability of the additive model component. Inspired by large-scale biomechanics datasets, this paper explores extending SSNs to functional data. Existing methods in functional data analysis are promising but often not expressive enough to account for all interactions and non-linearities and do not scale well to large datasets. Although the SSN approach presents a compelling potential solution, its adaptation to functional data remains complex. In this work, we propose a functional SSN method that retains the advantageous properties of classical functional regression approaches while also improving scalability. Our numerical experiments demonstrate that this approach accurately recovers underlying signals, enhances predictive performance, and performs favorably compared to competing methods.","['Functional Data', 'Neural Networks', 'Biomechanics']",[],"['David Rügamer', 'Bernard X.W. Liew', 'Zainab Altai', 'Almond Stöcker']","['Statistics, Ludwig-Maximilians-Universität München', 'School of Sport, Rehabilitation and Exercise Sciences,, University of Essex', 'School of Sport Rehabilitation and Exercise sciences, University of Essex', 'Institute of Mathematics, EPFL - EPF Lausanne']",
https://openreview.net/forum?id=KIrZmlTA92,Transparency & Explainability,Data-Driven Discovery of Dynamical Systems in Pharmacology using Large Language Models,"The discovery of dynamical systems is crucial across a range of fields, including pharmacology, epidemiology, and physical sciences. *Accurate* and *interpretable* modeling of these systems is essential for understanding complex temporal processes, optimizing interventions, and minimizing adverse effects. In pharmacology, for example, precise modeling of drug dynamics is vital to maximize therapeutic efficacy while minimizing patient harm, as in chemotherapy. However, current models, often developed by human experts, are limited by high cost, lack of scalability, and restriction to existing human knowledge. In this paper, we present the **Data-Driven Discovery (D3)** framework, a novel approach leveraging Large Language Models (LLMs) to iteratively discover and refine interpretable models of dynamical systems, demonstrated here with pharmacological applications. Unlike traditional methods, D3 enables the LLM to propose, acquire, and integrate new features, validate, and compare dynamical systems models, uncovering new insights into pharmacokinetics. Experiments on a pharmacokinetic Warfarin dataset reveal that D3 identifies a new plausible model that is well-fitting, highlighting its potential for precision dosing in clinical applications.","['ODE Discovery', 'LLM Discovery', 'LLM']",[],"['Samuel Holt', 'Zhaozhi Qian', 'Tennison Liu', 'Jim Weatherall', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'AstraZeneca', 'University of Cambridge']",
https://openreview.net/forum?id=WcmqdY2AKu,Transparency & Explainability,Boosting Graph Pooling with Persistent Homology,"Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.","['graph pooling', 'persistent homology', 'graph learning']",[],"['Chaolong Ying', 'Xinjian Zhao', 'Tianshu Yu']","['School of Data Science, The Chinese University of Hong Kong, Shenzhen', 'School of Data Science, Chinese University of Hong Kong (Shenzhen)', 'School of Data Science, Chinese University of Hong Kong (Shenzhen)']",
https://openreview.net/forum?id=UJ9k3j93MD,Fairness & Bias,Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics,"The deep equilibrium model (DEQ) generalizes the conventional feedforward neural network by fixing the same weights for each layer block and extending the number of layers to infinity. This novel model directly finds the fixed points of such a forward process as features for prediction.  Despite empirical evidence showcasing its efficacy  compared to feedforward neural networks, a theoretical understanding for its separation and bias is still limited.  In this paper, we take a step by proposing some separations and studying the bias of DEQ in its expressive power and learning dynamics.   The results include: (1) A general separation is proposed, showing the existence of a width-$m$ DEQ that any fully connected neural networks (FNNs) with depth $O(m^{\alpha})$ for $\alpha \in (0,1)$ cannot approximate unless its width is sub-exponential in $m$; (2) DEQ with polynomially bounded size and magnitude can efficiently approximate certain steep functions (which has very large derivatives) in $L^{\infty}$ norm, whereas FNN with bounded depth and exponentially bounded width cannot unless its weights magnitudes are exponentially large; (3) The implicit regularization caused by gradient flow from a diagonal linear DEQ is characterized, with specific examples showing the benefits brought by such regularization.  From the overall study, a high-level conjecture from our analysis and empirical validations is that DEQ has potential advantages in learning certain high-frequency components.","['Deep Equilibrium Models', 'separation', 'bias', 'expressivity', 'learning dynamics']",[],"['Zhoutong Wu', 'Yimu Zhang', 'Cong Fang', 'Zhouchen Lin']","['Center for Data Science, Peking University', 'Electronic Engineering, Tsinghua University, Tsinghua University', 'Peking University', 'School of Intelligence Science and Technology, Peking University']",
https://openreview.net/forum?id=Tj5wJslj0R,Fairness & Bias,Task Confusion and Catastrophic Forgetting in Class-Incremental Learning: A Mathematical Framework for Discriminative and Generative Modelings,"In class-incremental learning (class-IL), models must classify all previously seen classes at test time without task-IDs, leading to task confusion. Despite being a key challenge, task confusion lacks a theoretical understanding. We present a novel mathematical framework for class-IL and prove the Infeasibility Theorem, showing optimal class-IL is impossible with discriminative modeling due to task confusion. However, we establish the Feasibility Theorem, demonstrating that generative modeling can achieve optimal class-IL by overcoming task confusion. We then assess popular class-IL strategies, including regularization, bias-correction, replay, and generative classifier, using our framework. Our analysis suggests that adopting generative modeling, either for generative replay or direct classification (generative classifier), is essential for optimal class-IL.","['Catastrophic Forgetting', 'Class-Incremental Learning', 'Continual Learning', 'Task Confusion.']",[],"['Milad Khademi Nori', 'IL MIN KIM']","['ECE, Toronto Metropolitan University', ""Electrical and Computer Engineering, Queen's University""]",
https://openreview.net/forum?id=TcCorXxNJQ,Privacy & Data Governance,FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations,"The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning. Federated learning (FL) further enhances fine-tuning in a privacy-aware manner by utilizing clients' local data through in-situ computation, eliminating the need for data movement. However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL. Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters. This approach led to mathematically inaccurate aggregation noise, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs. In this work, we first highlight the mathematical incorrectness of LoRA aggregation in existing federated fine-tuning methods. We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method. Our approach is noise-free and seamlessly supports heterogeneous LoRAs. Extensive experiments demonstrate FLoRA's superior performance in both homogeneous and heterogeneous settings, surpassing state-of-the-art methods. We envision this work as a milestone for efficient, privacy-preserving, and accurate federated fine-tuning of LLMs.","['Federated Learning', 'Large Language Models', 'LoRA']",[],"['Ziyao Wang', 'Zheyu Shen', 'Yexiao He', 'Guoheng Sun', 'Hongyi Wang', 'Lingjuan Lyu', 'Ang Li']","['ECE, University of Maryland, College Park', 'Electrical and Computer Engineering, University of Maryland, College Park', 'Electrical and Computer Engineering, University of Maryland, College Park', 'Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Computer Science Department, Rutgers University', 'Sony Research, Sony', 'University of Maryland, College Park']",
https://openreview.net/forum?id=TJsknGasMy,Privacy & Data Governance,Differentially Private Stochastic Gradient Descent with Fixed-Size Minibatches: Tighter RDP Guarantees with or without Replacement,"Differentially private stochastic gradient descent (DP-SGD) has been instrumental in privately training deep learning models by providing a framework to control and track the privacy loss incurred during training.  At the core of this computation lies a subsampling method that uses a privacy amplification lemma to enhance the privacy guarantees provided by the additive noise. Fixed size subsampling is appealing for its constant memory usage, unlike the variable sized minibatches in Poisson subsampling. It is also of interest in addressing class imbalance and federated learning. Current computable guarantees for fixed-size subsampling are not tight and do not consider both add/remove and replace-one adjacency relationships. We present a new and holistic Rényi differential privacy (RDP)  accountant for DP-SGD with fixed-size subsampling without replacement (FSwoR) and with replacement (FSwR). For FSwoR we consider both add/remove and replace-one adjacency, where we improve on the best current computable bound by a factor of $4$. We also show for the first time that the widely-used Poisson subsampling and FSwoR with replace-one adjacency have the same privacy to leading order in the sampling probability. Our work suggests that FSwoR is often preferable to Poisson subsampling due to constant memory usage. Our FSwR accountant includes explicit non-asymptotic upper and lower bounds and, to the  authors' knowledge, is the first such RDP analysis of fixed-size  subsampling with replacement  for DP-SGD. We analytically and empirically compare fixed size and Poisson subsampling, and show that DP-SGD gradients in a fixed-size subsampling regime exhibit lower variance in practice in addition to memory usage benefits.","['privacy preserving machine learning', 'differential privacy', 'differentially private stochastic gradient descent', 'fixed-size subsampled mechanisms', 'privacy amplification lemma']",[],"['Jeremiah Birrell', 'Mohammadreza Ebrahimi', 'Rouzbeh Behnia', 'Jason Pacheco']","['Texas State University', 'University of South Florida', 'University of South Florida', 'Computer Science, University of Arizona']",
https://openreview.net/forum?id=RxXdokK2qz,Fairness & Bias,Computing the Bias of Constant-step Stochastic Approximation with Markovian Noise,"We study stochastic approximation algorithms with Markovian noise and constant step-size $\alpha$. We develop a method based on infinitesimal generator comparisons to study the bias of the algorithm, which is the expected difference between $\theta_n$ ---the value at iteration $n$--- and $\theta^*$ ---the unique equilibrium of the corresponding ODE. We show that, under some smoothness conditions, this bias is of order $O(\alpha)$. Furthermore, we show that the time-averaged bias is equal to $\alpha V + O(\alpha^2)$, where $V$ is a constant characterized by a Lyapunov equation, showing that $E[\bar{\theta}_n] \approx \theta^*+V\alpha + O(\alpha^2)$, where $\bar{\theta}_n$ is the Polyak-Ruppert average.  We also show that $\bar{\theta}_n$ converges with high probability around $\theta^*+\alpha V$. We illustrate how to combine this with Richardson-Romberg extrapolation to derive an iterative scheme with a bias of order $O(\alpha^2)$.","[""Stochastic approximation; Polyak-Ruppert averaging; Stein's method""]",[],"['Sebastian Allmeier', 'Nicolas Gast']","['INRIA', 'INRIA']",
https://openreview.net/forum?id=RJG8ar4wHA,Transparency & Explainability,Improving Generalization of Dynamic Graph Learning via Environment Prompt,"Out-of-distribution (OOD) generalization issue is a well-known challenge within deep learning tasks. In dynamic graphs, the change of temporal environments is regarded as the main cause of data distribution shift. While numerous OOD studies focusing on environment factors have achieved remarkable performance, they still fail to systematically solve the two issue of environment inference and utilization. In this work, we propose a novel dynamic graph learning model named EpoD based on prompt learning and structural causal model to comprehensively enhance both environment inference and utilization. Inspired by the superior performance of prompt learning in understanding underlying semantic and causal associations, we first design a self-prompted  learning  mechanism to infer unseen environment factors. We then rethink the role of environment variable within spatio-temporal causal structure model, and introduce a novel causal pathway where dynamic subgraphs serve as mediating variables. The extracted dynamic subgraph can effectively capture the data distribution shift  by incorporating the inferred environment variables into the node-wise dependencies. Theoretical discussions and intuitive analysis support the generalizability and interpretability of EpoD. Extensive experiments on seven real-world datasets across domains showcase the superiority of EpoD against baselines, and toy example experiments further verify the powerful interpretability and rationality of our EpoD.","['Dynamic graph', 'spatio-temporal graph learning', 'out-of-distribution generalization', 'causal theory', 'prompt learning', 'subgraph learning.']",[],"['Kuo Yang', 'Zhengyang Zhou', 'Qihe Huang', 'Limin Li', 'Yuxuan Liang', 'Yang Wang']","['School of Artificial Intelligence and Data Science, University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'University of Science and Technology of China', 'INTR & DSA, The Hong Kong University of Science and Technology (Guangzhou)', 'University of Science and Technology of China']",
https://openreview.net/forum?id=QyxE3W9Yni,Privacy & Data Governance,Faster Differentially Private Top-$k$ Selection: A Joint Exponential Mechanism with Pruning,"We study the differentially private top-$k$ selection problem, aiming to identify a sequence of $k$ items with approximately the highest scores from $d$ items. Recent work by Gillenwater et al. (2022) employs a direct sampling approach from the vast collection of $O(d^k)$ possible length-$k$ sequences, showing superior empirical accuracy compared to previous pure or approximate differentially private methods. Their algorithm has a time and space complexity of $\tilde{O}(dk)$.   In this paper, we present an improved algorithm that achieves time and space complexity of $\tilde{O}(d + k^2)$. Experimental results show that our algorithm runs orders of magnitude faster than their approach, while achieving similar empirical accuracy.","['Differential Privacy', 'Top-k Selection', 'Exponential Mechanism']",[],"['Hao WU', 'Hanwen Zhang']","['University of Waterloo', 'Computer Science Department, Copenhagen University']",
https://openreview.net/forum?id=PcyioHOmjq,Fairness & Bias,What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A Controlled Study for Transferable Insights,"Severe data imbalance naturally exists among web-scale vision-language datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations. With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training. This isolates the bias from dominant classes and implicitly balances the learning signal. Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning. Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community. The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks. Code and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.","['CLIP', 'vision-language pre-training', 'robustness', 'long-tail', 'data imbalance', 'uncurated data']",[],"['Xin Wen', 'Bingchen Zhao', 'Yilun Chen', 'Jiangmiao Pang', 'XIAOJUAN QI']","['Department of Electrical and Electronic Engineering, The University of Hong Kong', 'University of Edinburgh, University of Edinburgh', 'Embodied AI Center, Shanghai Artificial Intelligence Laboratory', 'Shanghai AI Laboratory', 'Department of Eletrical and Electronic Engineering, University of Hong Kong']",
https://openreview.net/forum?id=QCINh3O9q6,Fairness & Bias,Cross-video Identity Correlating for Person Re-identification Pre-training,"Recent researches have proven that pre-training on large-scale person images extracted from internet videos is an effective way in learning better representations for person re-identification. However, these researches are mostly confined to pre-training at the instance-level or single-video tracklet-level. They ignore the identity-invariance in images of the same person across different videos, which is a key focus in person re-identification. To address this issue, we propose a Cross-video Identity-cOrrelating pre-traiNing (CION) framework. Defining a noise concept that comprehensively considers both intra-identity consistency and inter-identity discrimination, CION seeks the identity correlation from cross-video images by modeling it as a progressive multi-level denoising problem. Furthermore, an identity-guided self-distillation loss is proposed to implement better large-scale pre-training by mining the identity-invariance within person images. We conduct extensive experiments to verify the superiority of our CION in terms of efficiency and performance. CION achieves significantly leading performance with even fewer training samples. For example, compared with the previous state-of-the-art ISR, CION with the same ResNet50-IBN achieves higher mAP of 93.3% and 74.3% on Market1501 and MSMT17, while only utilizing 8% training samples. Finally, with CION demonstrating superior model-agnostic ability, we contribute a model zoo named ReIDZoo to meet diverse research and application needs in this field. It contains a series of CION pre-trained models with spanning structures and parameters, totaling 32 models with 10 different structures, including GhostNet, ConvNext, RepViT, FastViT and so on. The code and models will be open-sourced.","['Person Re-identification', 'Pre-training', 'Self-supervised Representation Learning']",[],"['Jialong Zuo', 'Ying Nie', 'Hanyu Zhou', 'Huaxin Zhang', 'Haoyu Wang', 'Tianyu Guo', 'Nong Sang', 'Changxin Gao']","['Huazhong University of Science and Technology', ""Huawei Noah's Ark Lab"", 'Schoole of Artificial Intelegence and Automation, Huazhong University of Science and Technology', 'Huazhong University of Science and Technology', 'Huawei Technologies Ltd.', 'Huawei Technologies Ltd.', 'Huazhong University of Science and Technology', 'School of Artificial Intelligence and Automation, Huazhong University of Science and Technology']",
https://openreview.net/forum?id=OJximyClit,Fairness & Bias,Enhancing Zero-Shot Vision Models by Label-Free Prompt Distribution Learning and Bias Correcting,"Vision-language models, such as CLIP, have shown impressive generalization capacities when using appropriate text descriptions. While optimizing prompts on downstream labeled data has proven effective in improving performance, these methods entail labor costs for annotations and are limited by their quality. Additionally, since CLIP is pre-trained on highly imbalanced Web-scale data, it suffers from inherent label bias that leads to suboptimal performance.   To tackle the above challenges, we propose a label-**F**ree p**ro**mpt distribution **l**earning and b**i**as **c**orrection framework, dubbed as **Frolic**, which boosts zero-shot performance without the need for labeled data. Specifically, our Frolic learns distributions over prompt prototypes to capture diverse visual representations and adaptively fuses these with the original CLIP through confidence matching. This fused model is further enhanced by correcting label bias via a label-free logit adjustment. Notably, our method is not only training-free but also circumvents the necessity for hyper-parameter tuning. Extensive experimental results across 16 datasets demonstrate the efficacy of our approach, particularly outperforming the state-of-the-art by an average of $2.6\%$ on 10 datasets with CLIP ViT-B/16 and achieving an average margin of $1.5\%$ on ImageNet and its five distribution shifts with CLIP ViT-B/16. Codes are available in [https://github.com/zhuhsingyuu/Frolic](https://github.com/zhuhsingyuu/Frolic).","['vision-language model', 'zero-shot classification', 'logit adjustment']",[],"['Xingyu Zhu', 'Beier Zhu', 'Yi Tan', 'Shuo Wang', 'Yanbin Hao', 'Hanwang Zhang']","['University of Science and Technology of China', 'Nanyang Technological University', 'Information science, University of Science and Technology of China', 'University of Science and Technology of China', 'Hefei University of Technology', 'computer science, Nanyang Technological University']",
https://openreview.net/forum?id=PzG7xVlYqm,Privacy & Data Governance,On the Computational Complexity of Private High-dimensional Model Selection,"We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. We propose a differentially private (DP) best subset selection method with strong statistical utility properties by adopting the well-known exponential mechanism for selecting the best model. To achieve computational expediency, we propose an efficient Metropolis-Hastings algorithm and under certain regularity conditions, we establish that it enjoys polynomial mixing time to its stationary distribution. As a result, we also establish both approximate differential privacy and statistical utility for the estimates of the mixed Metropolis-Hastings chain. Finally, we perform some illustrative experiments on simulated data showing that our algorithm can quickly identify active features under reasonable privacy budget constraints.","['Best Subset Selection', 'Differential Privacy', 'Exponential Mechanism', 'Metropolis-Hastings', 'Model Consistency', 'Variable Selection.']",[],"['Saptarshi Roy', 'Zehua Wang', 'Ambuj Tewari']","['University of Texas at Austin', 'University of Michigan - Ann Arbor', 'University of Michigan - Ann Arbor']",
https://openreview.net/forum?id=O97BzlN9Wh,Fairness & Bias,"GDeR: Safeguarding Efficiency, Balancing, and Robustness via Prototypical Graph Pruning","Training high-quality deep models necessitates vast amounts of data, resulting in overwhelming computational and memory demands. Recently, data pruning, distillation, and coreset selection have been developed to streamline data volume by \textit{retaining}, \textit{synthesizing}, or \textit{selecting} a small yet informative subset from the full set. Among these methods, data pruning incurs the least additional training cost and offers the most practical acceleration benefits. However, it is the most vulnerable, often suffering significant performance degradation with imbalanced or biased data schema, thus raising concerns about its accuracy and reliability in on-device deployment. Therefore, there is a looming need for a new data pruning paradigm that maintains the efficiency of previous practices while ensuring balance and robustness. Unlike the fields of computer vision and natural language processing, where mature solutions have been developed to address these issues, graph neural networks (GNNs) continue to struggle with increasingly large-scale, imbalanced, and noisy datasets, lacking a unified dataset pruning solution.  To achieve this, we introduce a novel dynamic soft-pruning method, \ourmethod, designed to update the training ``basket'' during the process using trainable prototypes. \ourmethod first constructs a well-modeled graph embedding hypersphere and then samples \textit{representative, balanced, and unbiased subsets} from this embedding space, which achieves the goal we called {\fontfamily{lmtt}\selectfont \textbf{Graph Training Debugging}}. Extensive experiments on four datasets across three GNN backbones, demonstrate that \ourmethod (I) achieves or surpasses the performance of the full dataset with $30\%\sim50\%$ fewer training samples, (II) attains up to a $2.81\times$ lossless training speedup, and (III) outperforms state-of-the-art pruning methods in imbalanced training and noisy training scenarios by $0.3\%\sim4.3\%$ and $3.6\%\sim7.8\%$, respectively.","['data pruning', 'graph pruning', 'graph neural networks']",[],"['Guibin Zhang', 'Haonan Dong', 'Yuchen Zhang', 'Zhixun Li', 'Dingshuo Chen', 'Kai Wang', 'Tianlong Chen', 'Yuxuan Liang', 'Dawei Cheng', 'Kun Wang']","['', 'Tongji University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'Institute of automation, Chinese Academy of Sciences', 'soc, national university of singaore, National University of Singapore', 'University of North Carolina at Chapel Hill', 'INTR & DSA, The Hong Kong University of Science and Technology (Guangzhou)', 'Tongji University', 'Nanyang Technological University']",
https://openreview.net/forum?id=NhyDfZXjQX,Transparency & Explainability,A Local Method for Satisfying Interventional Fairness with Partially Known Causal Graphs,"Developing fair automated machine learning algorithms is critical in making safe and trustworthy decisions. Many causality-based fairness notions have been proposed to address the above issues by quantifying the causal connections between sensitive attributes and decisions, and when the true causal graph is fully known, certain algorithms that achieve interventional fairness have been proposed. However, when the true causal graph is unknown, it is still challenging to effectively and efficiently exploit partially directed acyclic graphs (PDAGs) to achieve interventional fairness. To exploit the PDAGs for achieving interventional fairness, previous methods have been built on variable selection or causal effect identification, but limited to reduced prediction accuracy or strong assumptions. In this paper, we propose a general min-max optimization framework that can achieve interventional fairness with promising prediction accuracy and can be extended to maximally oriented PDAGs (MPDAGs) with added background knowledge. Specifically, we first estimate all possible treatment effects of sensitive attributes on a given prediction model from all possible adjustment sets of sensitive attributes via an efficient local approach. Next, we propose to alternatively update the prediction model and possible estimated causal effects, where the prediction model is trained via a min-max loss to control the worst-case fairness violations. Extensive experiments on synthetic and real-world datasets verify the superiority of our methods. To benefit the research community, we have released our project at https://github.com/haoxuanli-pku/NeurIPS24-Interventional-Fairness-with-PDAGs.","['Interventional fairness', 'PDAG', 'MPDAG', 'Causal effect']",[],"['Haoxuan Li', 'Yue Liu', 'Zhi Geng', 'Kun Zhang']","['Center for Data Science, Peking University', '', 'School of Matematics asn Statistics, Beijing Technology and Business University', 'Mohamed bin Zayed University of Artificial Intelligence']",
https://openreview.net/forum?id=NIcIdhyfQX,Fairness & Bias,Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model,"``Distribution shift'' is the primary obstacle to the success of offline reinforcement learning. As a learning policy may take actions beyond the knowledge of the behavior policy (referred to as Out-of-Distribution (OOD) actions), the Q-values of these OOD actions can be easily overestimated. Consequently, the learning policy becomes biasedly optimized using the incorrect recovered Q-value function. One commonly used idea to avoid the overestimation of Q-value is to make a pessimistic adjustment. Our key idea is to penalize the Q-values of OOD actions that correspond to high uncertainty. In this work, we propose Q-Distribution guided Q-learning (QDQ) which  pessimistic Q-value on OOD regions based on uncertainty estimation. The uncertainty measure is based on the conditional Q-value distribution, which is learned via a high-fidelity and efficient consistency model. On the other hand, to avoid the overly conservative problem, we introduce an uncertainty-aware optimization objective to update the Q-value function. The proposed QDQ demonstrates solid theoretical guarantees for the accuracy of Q-value distribution learning and uncertainty measurement, as well as the performance of the learning policy. QDQ consistently exhibits strong performance in the D4RL benchmark and shows significant improvements for many tasks. Our code can be found at <code link>.","['offline Reinforcement Learning', 'Q value distribution', 'Uncertainty measure', 'pessimistic Q learning', 'Consistency model']",[],"['Jing Zhang', 'Linjiajie Fang', 'Kexin Shi', 'Wenjia Wang', 'Bingyi Jing']","['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology', 'HKUST (GZ)', 'Department of Statistics and Data Science, South University of Science and Technology']",
https://openreview.net/forum?id=MZ47wPr6C3,Transparency & Explainability,On Sparse Canonical Correlation Analysis,"The classical Canonical Correlation Analysis (CCA) identifies the correlations between two sets of multivariate variables based on their covariance, which has been widely applied in diverse fields such as computer vision, natural language processing, and speech analysis. Despite its popularity, CCA can encounter challenges in explaining correlations between two variable sets within high-dimensional data contexts. Thus, this paper studies Sparse Canonical Correlation Analysis (SCCA) that enhances the interpretability of CCA. We first show that SCCA generalizes three well-known sparse optimization problems, sparse PCA, sparse SVD, and sparse regression, which are all classified as NP-hard problems. This result motivates us to develop strong formulations and efficient algorithms. Our main contributions include (i) the introduction of a combinatorial formulation that captures the essence of SCCA and allows the development of exact and approximation algorithms; (ii) the establishment of the complexity results for two low-rank special cases of SCCA; and (iii) the derivation of an equivalent mixed-integer semidefinite programming model that facilitates a specialized branch-and-cut algorithm with analytical cuts. The effectiveness of our proposed formulations and algorithms is validated through numerical experiments.","['Sparse canonical correlation analysis', 'low rank', 'exact and approximation algorithms']",[],"['Yongchun Li', 'Santanu Dey', 'Weijun Xie']","['University of Tennessee, Knoxville', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=MjD9Y05Q6i,Transparency & Explainability,LG-CAV: Train Any Concept Activation Vector with Language Guidance,"Concept activation vector (CAV) has attracted broad research interest in explainable AI, by elegantly attributing model predictions to specific concepts. However, the training of CAV often necessitates a large number of high-quality images, which are expensive to curate and thus limited to a predefined set of concepts. To address this issue, we propose Language-Guided CAV (LG-CAV) to harness the abundant concept knowledge within the certain pre-trained vision-language models (e.g., CLIP). This method allows training any CAV without labeled data, by utilizing the corresponding concept descriptions as guidance. To bridge the gap between vision-language model and the target model, we calculate the activation values of concept descriptions on a common pool of images (probe images) with vision-language model and utilize them as language guidance to train the LG-CAV. Furthermore, after training high-quality LG-CAVs related to all the predicted classes in the target model, we propose the activation sample reweighting (ASR), serving as a model correction technique, to improve the performance of the target model in return. Experiments on four datasets across nine architectures demonstrate that LG-CAV achieves significantly superior quality to previous CAV methods given any concept, and our model correction method achieves state-of-the-art performance compared to existing concept-based methods. Our code is available at https://github.com/hqhQAQ/LG-CAV.","['Explainability for Computer Vision', 'Concept Activation Vector', 'Cross-Modal Model']",[],"['Qihan Huang', 'Jie Song', 'Mengqi Xue', 'Haofei Zhang', 'Bingde Hu', 'Huiqiong Wang', 'Hao Jiang', 'Xingen Wang', 'Mingli Song']","['Computer Science and Technology, Zhejiang University', 'School of Software Technology, Zhejiang University', 'School of Computer and Computing Science, Hangzhou City University', 'National University of Singapore', 'Zhejiang University', 'Zhejiang University', 'Alibaba Group', 'Computer Science & Technology, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University']",
https://openreview.net/forum?id=LUsx0chTsL,Transparency & Explainability,Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models,"Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank *communication channels* (Elhage et al., 2021) between layers. A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. That is, the model has trouble copying the correct information from context when many items ``crowd"" this limited space. By decomposing attention heads with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices alone. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20\%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.","['mechanistic interpretability', 'interpretability', 'attention', 'subspaces', 'circuit', 'LLM', 'LM', 'language model', 'transformer']",[],"['Jack Merullo', 'Carsten Eickhoff', 'Ellie Pavlick']","['Computer Science, Brown University', 'Eberhard-Karls-Universität Tübingen', 'Brown University']",
https://openreview.net/forum?id=LLuSjg59an,Transparency & Explainability,Where does In-context Learning Happen in Large Language Models?,"Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from recognizing the task to performing the task. Through a series of layer-wise context-masking experiments on GPTNeo2.7B, Bloom3B, Starcoder2-7B, Llama3.1-8B, Llama3.1-8B-Instruct, on Machine Translation and Code generation, we demonstrate evidence of a ""task recognition"" point where the task is encoded into the input representations and attention to context is no longer necessary. Taking advantage of this redundancy results in 45% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation. Our findings also have implications for resource and parameter efficient fine-tuning; we observe a correspondence between strong fine-tuning performance of individual LoRA layers and the task recognition layers.","['Explainability', 'self-attention layers', 'redundancy', 'LLM', 'in-context learning', 'machine translation', 'text to code']",[],"['Suzanna Sia', 'David Mueller', 'Kevin Duh']","['', 'Johns Hopkins University', 'Johns Hopkins University']",
https://openreview.net/forum?id=J6zHcScAo0,Transparency & Explainability,Transcoders find interpretable LLM feature circuits,"A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features—such as those found by sparse autoencoders (SAEs)—are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore **transcoders**, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms. We then successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the ""greater-than circuit"" in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder_circuits/","['mechanistic interpretability', 'transcoders', 'sparse autoencoders', 'circuit analysis']",[],"['Jacob Dunefsky', 'Philippe Chlenski', 'Neel Nanda']","['Department of Computer Science, Yale University', 'Columbia University', 'Google DeepMind']",
https://openreview.net/forum?id=KvAaIJhqhI,Security,Style Adaptation and Uncertainty Estimation for Multi-Source Blended-Target Domain Adaptation,"Blended-target domain adaptation (BTDA), which implicitly mixes multiple sub-target domains into a fine domain, has attracted more attention in recent years. Most previously developed BTDA approaches focus on utilizing a single source domain, which makes it difficult to obtain sufficient feature information for learning domain-invariant representations. Furthermore, different feature distributions derived from different domains may increase the uncertainty of models. To overcome these issues, we propose a style adaptation and uncertainty estimation (SAUE) approach for multi-source blended-target domain adaptation (MBDA). Specifically, we exploit the extra knowledge acquired from the blended-target domain, where a similarity factor is adopted to select more useful target style information for augmenting the source features. \!Then, to mitigate the negative impact of the domain-specific attributes, we devise a function to estimate and mitigate uncertainty in category prediction. Finally, we construct a simple and lightweight adversarial learning strategy for MBDA, effectively aligning multi-source and blended-target domains without the requirements of domain labels of the target domains. Extensive experiments conducted on several challenging DA benchmarks, including the ImageCLEF-DA, Office-Home, VisDA 2017, and DomainNet datasets, demonstrate the superiority of our method over the state-of-the-art (SOTA) approaches.","['Domain adaptation', 'Transfer learning', 'Multi-source and blended-target', 'Style transfer', 'Uncertainty estimation']",[],"['Yuwu Lu', 'Haoyu Huang', 'Xue Hu']","['School of Artificial Intelligence, South China Normal University', 'School of Artificial Intelligence, South China Normal University', 'School of Artificial Intelligence, South China Normal University']",
https://openreview.net/forum?id=GQrk0WGNiC,Privacy & Data Governance,Pre-training Differentially Private Models with Limited Public Data,"The superior performance of large foundation models can be attributed to the use of massive amounts of high-quality data. However, such datasets often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method used to gauge the degree of security provided to large foundation models, its application in large foundation models has been met with limited success because there are often significant performance compromises when applying DP during the pre-training phase. Consequently, DP is more commonly implemented during the model fine-tuning stage, hence not capable of protecting a substantial portion of the data used during the initial pre-training process. In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration improvement of loss through the lens of the Hessian. We observe that DP optimizers' deceleration can be significantly mitigated by the use of limited public data, and thus propose the DP continual pre-training strategy. Our DP continual pre-training on vision models, using only 10% of public data, have achieved DP accuracy of 41.5% on ImageNet-21k (with epsilon=8) and non-DP accuracy of 55.7% on Places365 and 60.0% on iNaturalist-2021, which are on par with state-of-the-art standard pre-training and outperform existing DP pertained models. Our DP pre-trained models are released in *fastDP* library (https://github.com/awslabs/fast-differential-privacy/releases/tag/v2.1)","['Differential Privacy', 'Foundation Models', 'Pre-training']",[],"['Zhiqi Bu', 'Xinwei Zhang', 'Sheng Zha', 'Mingyi Hong', 'George Karypis']","['Amazon', 'University of Southern California', 'Amazon', 'AGI, Amazon', 'University of Minnesota, Minneapolis']",
https://openreview.net/forum?id=G522UpazH3,Security,Transferability Bound Theory: Exploring Relationship between Adversarial Transferability and Flatness,"A prevailing belief in attack and defense community is that the higher flatness of adversarial examples enables their better cross-model transferability, leading to a growing interest in employing sharpness-aware minimization and its variants. However, the theoretical relationship between the transferability of adversarial examples and their flatness has not been well established, making the belief questionable. To bridge this gap, we embark on a theoretical investigation and, for the first time, derive a theoretical bound for the transferability of adversarial examples with few practical assumptions. Our analysis challenges this belief by demonstrating that the increased flatness of adversarial examples does not necessarily guarantee improved transferability. Moreover, building upon the theoretical analysis, we propose TPA, a Theoretically Provable Attack that optimizes a surrogate of the derived bound to craft adversarial examples. Extensive experiments across widely used benchmark datasets and various real-world applications show that TPA can craft more transferable adversarial examples compared to state-of-the-art baselines. We hope that these results can recalibrate preconceived impressions within the community and facilitate the development of stronger adversarial attack and defense mechanisms.",['Adversarial examples; transferability; transfer-based attack; theoretical analysis'],[],"['Mingyuan Fan', 'Xiaodan Li', 'Cen Chen', 'Wenmeng Zhou', 'Yaliang Li']","['', 'Alibaba Group', 'School of Data Science and Engineering, East China Normal University', '', 'Alibaba Group']",
https://openreview.net/forum?id=FbUSCraXEB,Privacy & Data Governance,Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously,"Availability attacks provide a tool to prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and crafting unlearnable examples before release.  Ideally, the obtained unlearnability can prevent algorithms from training usable models.  When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection. Through evaluation, we have found that most existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection by availability attacks. Different from recent methods based on contrastive learning, we employ contrastive-like data augmentations in supervised learning frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications.  The code is available at https://github.com/EhanW/AUE-AAP.","['availability Attacks', 'indiscriminate attack', 'unlearnable examles', 'contrastive learning']",[],"['Yihan Wang', 'Yifan Zhu', 'Xiao-Shan Gao']","['University of Waterloo', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences']",
https://openreview.net/forum?id=FDfrPugkGU,Privacy & Data Governance,DoFIT: Domain-aware Federated Instruction Tuning with Alleviated Catastrophic Forgetting,"Federated Instruction Tuning (FIT) advances collaborative training on decentralized data, crucially enhancing model's capability and safeguarding data privacy. However, existing FIT methods are dedicated to handling data heterogeneity across different clients (i.e., client-aware data heterogeneity), while ignoring the variation between data from different domains (i.e., domain-aware data heterogeneity). When scarce data needs supplementation from related fields, these methods lack the ability to handle domain heterogeneity in cross-domain training. This leads to domain-information catastrophic forgetting in collaborative training and therefore makes model perform sub-optimally on the individual domain. To address this issue, we introduce DoFIT, a new Domain-aware FIT framework that alleviates catastrophic forgetting through two new designs. First, to reduce interference information from the other domain, DoFIT finely aggregates overlapping weights across domains on the inter-domain server side. Second, to retain more domain information, DoFIT initializes intra-domain weights by incorporating inter-domain information into a less-conflicted parameter space. Experimental results on diverse datasets consistently demonstrate that DoFIT excels in cross-domain collaborative training and exhibits significant advantages over conventional FIT methods in alleviating catastrophic forgetting. Code is available at [this link](https://github.com/1xbq1/DoFIT).","['Federated Instruction Tuning', 'LLM', 'LoRA']",[],"['Binqian Xu', 'Xiangbo Shu', 'Haiyang Mei', 'Zechen Bai', 'Basura Fernando', 'Mike Zheng Shou', 'Jinhui Tang']","['Nanjing University of Science and Technology', 'School of Computer Science and Engineering, Nanjing University of Science and Technology', 'Electrical and Computer Engineering, National University of Singapore', 'National University of Singapore', 'Nanyang Technological University', 'National University of Singapore', 'Nanjing University of Science and Technology']",
https://openreview.net/forum?id=Eyyt3ZmNV6,Privacy & Data Governance,ZeroMark: Towards Dataset Ownership Verification without Disclosing Watermark,"High-quality public datasets significantly prompt the prosperity of deep neural networks (DNNs). Currently, dataset ownership verification (DOV), which consists of dataset watermarking and ownership verification, is the only feasible solution to protect their copyright by preventing unauthorized use. In this paper, we revisit existing DOV methods and find that they all mainly focused on the first stage by designing different types of dataset watermarks and directly exploiting watermarked samples as the verification samples for ownership verification. As such, their success relies on an underlying assumption that verification is a \emph{one-time} and \emph{privacy-preserving} process, which does not necessarily hold in practice. To alleviate this problem, we propose \emph{ZeroMark} to conduct ownership verification without disclosing dataset-specified watermarks. Our method is inspired by our empirical and theoretical findings of the intrinsic property of DNNs trained on the watermarked dataset. Specifically, ZeroMark first generates the closest boundary version of given benign samples and calculates their boundary gradients under the label-only black-box setting. After that, it examines whether the given suspicious method has been trained on the protected dataset by performing a hypothesis test, based on the cosine similarity measured on the boundary gradients and the watermark pattern. Extensive experiments on benchmark datasets verify the effectiveness of our ZeroMark and its resistance to potential adaptive attacks. The codes for reproducing our main experiments are publicly available at \href{https://github.com/JunfengGo/ZeroMark.git}{GitHub}.","['Dataset Ownership Verification', 'Ownership Verification', 'Copyright Protection', 'AI Security']",[],"['Junfeng Guo', 'Yiming Li', 'Ruibo Chen', 'Yihan Wu', 'Chenxi Liu', 'Heng Huang']","['Computer Science, University of Maryland Institute for Advanced Computer Studies, University of Maryland, College Park', 'Nanyang Technological University', 'Computer Science, University of Maryland, College Park', 'CS, University of Maryland, College Park', 'Computer Science, University of Maryland, College Park', 'Department of Computer Science, University of Maryland, College Park']",
https://openreview.net/forum?id=Ex3rPvEct8,Transparency & Explainability,Towards a Scalable Reference-Free Evaluation of Generative Models,"While standard evaluation scores for generative models are mostly reference-based, a reference-dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets. Recently, the reference-free entropy scores, VENDI and RKE, have been proposed to evaluate the diversity of generated data. However, estimating these scores from data leads to significant computational costs for large-scale generative models. In this work, we leverage the random Fourier features framework to reduce the metrics' complexity and propose the *Fourier-based Kernel Entropy Approximation (FKEA)* method. We utilize FKEA's approximated eigenspectrum of the kernel matrix to efficiently estimate the mentioned entropy scores. Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the method's identified modes in evaluating the diversity of produced samples. We provide a stochastic implementation of the FKEA assessment algorithm with a complexity $O(n)$ linearly growing with sample size $n$. We extensively evaluate FKEA's numerical performance in application to standard image, text, and video datasets. Our empirical results indicate the method's scalability and interpretability applied to large-scale generative models.  The codebase is available at [https://github.com/aziksh-ospanov/FKEA](https://github.com/aziksh-ospanov/FKEA).","['Evaluation of Generative Models', 'Scalable Algorithms', 'Kernel Methods', 'Random Fourier Features']",[],"['Azim Ospanov', 'Jingwei Zhang', 'Mohammad Jalali', 'Xuenan Cao', 'Andrej Bogdanov', 'Farzan Farnia']","['Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Department of Computer Science and Engineering, The Chinese University of Hong Kong', 'Cultural and Religious Studies, The Chinese University of Hong Kong', 'EECS, University of Ottawa', 'Computer Science and Engineering, The Chinese University of Hong Kong']",
https://openreview.net/forum?id=KcDcaVOW1S,Fairness & Bias,Conformalized Time Series with Semantic Features,"Conformal prediction is a powerful tool for uncertainty quantification, but its application to time-series data is constrained by the violation of the exchangeability assumption. Current solutions for time-series prediction typically operate in the output space and rely on manually selected weights to address distribution drift, leading to overly conservative predictions. To enable dynamic weight learning in the semantically rich latent space, we introduce a novel approach called Conformalized Time Series with Semantic Features (CT-SSF). CT-SSF utilizes the inductive bias in deep representation learning to dynamically adjust weights, prioritizing semantic features relevant to the current prediction. Theoretically, we show that CT-SSF surpasses previous methods defined in the output space. Experiments on synthetic and benchmark datasets demonstrate that CT-SSF significantly outperforms existing state-of-the-art (SOTA) conformal prediction techniques in terms of prediction efficiency while maintaining a valid coverage guarantee.","['conformal prediction', 'time series', 'neural network', 'uncertainty qualification']",[],"['Baiting Chen', 'Zhimei Ren', 'Lu Cheng']","['UCLA, University of California, Los Angeles', 'The Wharton School, University of Pennsylvania', 'University of Illinois at Chicago']",
https://openreview.net/forum?id=pMPBxMf8T3,Fairness & Bias,The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing,"Models are expected to engage in invariance learning, which involves distinguishing the core relations that remain consistent across varying environments to ensure the predictions are safe, robust and fair. While existing works consider specific algorithms to realize invariance learning, we show that model has the potential to learn invariance through standard training procedures. In other words, this paper studies the implicit bias of Stochastic Gradient Descent (SGD) over heterogeneous data and shows that the implicit bias drives the model learning towards an invariant solution. We call the phenomenon the implicit invariance learning. Specifically, we theoretically investigate the multi-environment low-rank matrix sensing problem where in each environment, the signal comprises (i) a lower-rank invariant part shared across all environments; and (ii) a significantly varying environment-dependent spurious component. The key insight is, through simply employing the large step size large-batch SGD sequentially in each environment without any explicit regularization, the oscillation caused by heterogeneity can provably prevent model learning spurious signals.  The model reaches the invariant solution after certain iterations. In contrast, model learned using pooled SGD over all data would simultaneously learn both the invariant and spurious signals. Overall, we unveil another implicit bias that is a result of the symbiosis between the heterogeneity of data and modern algorithms, which is, to the best of our knowledge, first in the literature.","['implicit bias', 'matrix sensing', 'invariance learning', 'non-convex optimization']",[],"['Yang Xu', 'Yihong Gu', 'Cong Fang']","['Peking University', 'Princeton University', 'Peking University']",
https://openreview.net/forum?id=EMV8nIDZJn,Fairness & Bias,Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment,"Mixed time series (MiTS) comprising both continuous variables (CVs) and discrete variables (DVs) are frequently encountered yet under-explored in time series analysis. Essentially, CVs and DVs exhibit different temporal patterns and distribution types. Overlooking these heterogeneities would lead to insufficient and imbalanced representation learning, bringing biased results. This paper addresses the problem with two insights: 1) DVs may originate from intrinsic latent continuous variables (LCVs), which lose fine-grained information due to extrinsic discretization; 2) LCVs and CVs share similar temporal patterns and interact spatially. Considering these similarities and interactions, we propose a general MiTS analysis framework MiTSformer, which recovers LCVs behind DVs for sufficient and balanced spatial-temporal modeling by designing two essential inductive biases: 1) hierarchically aggregating multi-scale temporal context information to enrich the information granularity of DVs; 2) adaptively learning the aggregation processes via the adversarial guidance from CVs. Subsequently, MiTSformer captures complete spatial-temporal dependencies within and across LCVs and CVs via cascaded self- and cross-attention blocks. Empirically, MiTSformer achieves consistent SOTA on five mixed time series analysis tasks, including classification, extrinsic regression, anomaly detection, imputation, and long-term forecasting. The code is available at https://github.com/chunhuiz/MiTSformer.","['Mixed Time Series', 'General Time Series Modeling', 'Spatial-Temporal Heterogeneity', 'Latent Continuity Recovery', 'Adversarial Learning']",[],"['Jiawei Chen', 'Chunhui Zhao']","['College of Control Science and Engineerig, Zhejiang University', 'Control Science and Engineering, Zhejiang University']",
https://openreview.net/forum?id=DHucngOEe3,Transparency & Explainability,Pre-Trained Multi-Goal Transformers with Prompt Optimization for Efficient Online Adaptation,"Efficiently solving unseen tasks remains a challenge in reinforcement learning (RL), especially for long-horizon tasks composed of multiple subtasks.  Pre-training policies from task-agnostic datasets has emerged as a promising approach, yet existing methods still necessitate substantial interactions via RL to learn new tasks. We introduce MGPO, a method that leverages the power of Transformer-based policies to model sequences of goals, enabling efficient online adaptation through prompt optimization. In its pre-training phase, MGPO utilizes hindsight multi-goal relabeling and behavior cloning. This combination equips the policy to model diverse long-horizon behaviors that align with varying goal sequences. During online adaptation, the goal sequence, conceptualized as a prompt, is optimized to improve task performance. We adopt a multi-armed bandit framework for this process, enhancing prompt selection based on the returns from online trajectories. Our experiments across various environments demonstrate that MGPO holds substantial advantages in sample efficiency, online adaptation performance, robustness, and interpretability compared with existing methods.","['reinforcement learning', 'pre-training', 'Transformers']",[],"['Haoqi Yuan', 'Yuhui Fu', 'Feiyang Xie', 'Zongqing Lu']","['Peking University', '', 'Peking University', '']",
https://openreview.net/forum?id=CyCDqnrymT,Transparency & Explainability,Spike-based Neuromorphic Model for Sound Source Localization,"Biological systems possess remarkable sound source localization (SSL) capabilities that are critical for survival in complex environments. This ability arises from the collaboration between the auditory periphery, which encodes sound as precisely timed spikes, and the auditory cortex, which performs spike-based computations. Inspired by these biological mechanisms, we propose a novel neuromorphic SSL framework that integrates spike-based neural encoding and computation. The framework employs Resonate-and-Fire (RF) neurons with a phase-locking coding (RF-PLC) method to achieve energy-efficient audio processing. The RF-PLC method leverages the resonance properties of RF neurons to efficiently convert audio signals to time-frequency representation and encode interaural time difference (ITD) cues into discriminative spike patterns. In addition, biological adaptations like frequency band selectivity and short-term memory effectively filter out many environmental noises, enhancing SSL capabilities in real-world settings. Inspired by these adaptations, we propose a spike-driven multi-auditory attention (MAA) module that significantly improves both the accuracy and robustness of the proposed SSL framework. Extensive experimentation demonstrates that our SSL framework achieves state-of-the-art accuracy in SSL tasks. Furthermore, it shows exceptional noise robustness and maintains high accuracy even at very low signal-to-noise ratios. By mimicking biological hearing, this neuromorphic approach contributes to the development of high-performance and explainable artificial intelligence systems capable of superior performance in real-world environments.","['Spiking Neural Networks', 'Resonate-and-Fire Neurons', 'Bio-inspired Neuromorphic System', 'Sound Source Localization']",[],"['Dehao Zhang', 'Shuai Wang', 'Ammar Belatreche', 'Wenjie Wei', 'Yichen Xiao', 'Haorui Zheng', 'Zijian Zhou', 'Malu Zhang', 'Yang Yang']","['School of Computer Science & Engineering, University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'Computer and Information Sciences, Northumbria University', '', 'University of Electronic Science and Technology of China', 'Peking University', 'School of Computer Science and Engineering, University of Electronic Science and Technology of China', 'University of Electronic Science and Technology of China', 'School of Computer Science and Engineering, University of Electronic Science and Technology of China']",
https://openreview.net/forum?id=CLxcLPfARc,Security,Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space,"Current research in adversarial robustness of LLMs focuses on \textit{discrete} input manipulations in the natural language space, which can be directly transferred to \textit{closed-source} models. However, this approach neglects the steady progression of \textit{open-source} models. As open-source models advance in capability, ensuring their safety becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the \textit{embedding space attack}, which directly attacks the \textit{continuous} embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Additionally, we demonstrate that models compromised by embedding attacks can be used to create discrete jailbreaks in natural language. Lastly, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source LLMs.","['Adversarial Attacks', 'Large Language Models']",[],"['Leo Schwinn', 'David Dobre', 'Sophie Xhonneux', 'Gauthier Gidel', 'Stephan Günnemann']","['Technical University of Munich', 'Mila - Quebec Artificial Intelligence Institute', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'Computer Science and Operational Research, University of Montreal', 'Technical University Munich']",
https://openreview.net/forum?id=CAdBTYBlOv,Fairness & Bias,Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes,"Scaling hyperparameter optimisation to very large datasets remains an open problem in the Gaussian process community. This paper focuses on iterative methods, which use linear system solvers, like conjugate gradients, alternating projections or stochastic gradient descent, to construct an estimate of the marginal likelihood gradient. We discuss three key improvements which are applicable across solvers: (i) a pathwise gradient estimator, which reduces the required number of solver iterations and amortises the computational cost of making predictions, (ii) warm starting linear system solvers with the solution from the previous step, which leads to faster solver convergence at the cost of negligible bias, (iii) early stopping linear system solvers after a limited computational budget, which synergises with warm starting, allowing solver progress to accumulate over multiple marginal likelihood steps. These techniques provide speed-ups of up to $72\times$ when solving to tolerance, and decrease the average residual norm by up to $7\times$ when stopping early.","['Gaussian process', 'marginal likelihood', 'iterative linear system solver', 'pathwise', 'warm start', 'conjugate gradient', 'alternating projection', 'stochastic gradient descent']",[],"['Jihao Andreas Lin', 'Shreyas Padhy', 'Bruno Kacper Mlodozeniec', 'Javier Antoran', 'José Miguel Hernández-Lobato']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'University of Cambridge']",
https://openreview.net/forum?id=APBq3KAmFa,Fairness & Bias,A New Neural Kernel Regime: The Inductive Bias of Multi-Task Learning,"This paper studies the properties of solutions to multi-task shallow ReLU neural network learning problems, wherein the network is trained to fit a dataset with minimal sum of squared weights. Remarkably, the solutions learned for each individual task resemble those obtained by solving a kernel regression problem, revealing a novel connection between neural networks and kernel methods. It is known that single-task neural network learning problems are equivalent to a minimum norm interpolation problem in a non-Hilbertian Banach space, and that the solutions of such problems are generally non-unique. In contrast, we prove that the solutions to univariate-input, multi-task neural network interpolation problems are almost always unique, and coincide with the solution to a minimum-norm interpolation problem in a Sobolev (Reproducing Kernel) Hilbert Space. We also demonstrate a similar phenomenon in the multivariate-input case; specifically, we show that neural network learning problems with large numbers of tasks are approximately equivalent to an $\ell^2$ (Hilbert space) minimization problem over a fixed kernel determined by the optimal neurons.","['neural networks', 'weight decay', 'multitask learning', 'regularization', 'kernels']",[],"['Julia B Nakhleh', 'Joseph Shenouda', 'Robert D Nowak']","['Computer Sciences, University of Wisconsin - Madison', 'Electrical and Computer Engineering, University of Wisconsin - Madison', 'University of Wisconsin, Madison']",
https://openreview.net/forum?id=9622QfVSAb,Transparency & Explainability,Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs,"Large Language Models (LLMs) have demonstrated impressive performance on multimodal tasks, without any multimodal finetuning. They are the de facto building block for Large Multimodal Models (LMMs), yet, we still lack a proper understanding of their success. In this work, we expose frozen LLMs to image, video, audio and text inputs and analyse their internal representation with the attempt to understand their generalization beyond textual inputs. Our work provides the following **findings.** Perceptual tokens (1) are easily distinguishable from textual ones inside LLMs, with significantly different representations (e.g. live in different narrow cones), and complete translation to textual tokens does not exists. Yet, (2) both perceptual and textual tokens activate similar LLM weights. Despite their differences, (3) perceptual tokens are implicitly aligned to textual tokens inside LLMs, we call this the implicit multimodal alignment effect (IMA), and argue that this is linked to architectural design, helping LLMs to generalize. This provide more evidence to believe that the generalization of LLMs to multimodal inputs is mainly due to their architecture.  These findings lead to several **implications.** This work provides several implications. (1) We find a positive correlation between the implicit alignment score and the task performance, suggesting that this could act as a proxy metric for model evaluation and selection. (2) A negative correlation exists regarding hallucinations (e.g. describing non-existing objects in images), revealing that this problem is mainly due to misalignment between the internal perceptual and textual representations. (3) Perceptual tokens change slightly throughout the model, thus, we propose different approaches to skip computations (e.g. in FFN layers), and significantly reduce the inference cost. (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork (called alpha-SubNet) that works well across a wide range of multimodal tasks. The code is available here: https://github.com/mshukor/ima-lmms.","['Large multimodal models', 'LLMs', 'cross-modal alignment', 'efficiency', 'safety', 'interpretability']",[],"['Mustafa Shukor', 'Matthieu Cord']","['Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Sorbonne Université']",
https://openreview.net/forum?id=9jgODkdH0F,Fairness & Bias,Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion,"Matrix factorization models have been extensively studied as a valuable test-bed for understanding the implicit biases of overparameterized models. Although both low nuclear norm and low rank regularization have been studied for these models, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive. In this work, we systematically investigate the implicit regularization of matrix factorization for solving matrix completion problems. We empirically discover that the connectivity of observed data plays a key role in the implicit bias, with a transition from low nuclear norm to low rank as data shifts from disconnected to connected with increased observations. We identify a hierarchy of intrinsic invariant manifolds in the loss landscape that guide the training trajectory to evolve from low-rank to higher-rank solutions. Based on this finding, we theoretically characterize the training trajectory as following the hierarchical invariant manifold traversal process, generalizing the characterization of Li et al.(2020)  to include the disconnected case. Furthermore, we establish conditions that guarantee minimum nuclear norm, closely aligning with our experimental findings, and we provide a dynamics characterization condition for ensuring minimum rank. Our work reveals the intricate interplay between data connectivity, training dynamics, and implicit regularization in matrix factorization models.",['matrix completion; implicit regularization; training dynamics; low nuclear norm; low rank;'],[],"['Zhiwei Bai', 'Jiajie Zhao', 'Yaoyu Zhang']","['Shanghai Jiao Tong University, Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University']",
https://openreview.net/forum?id=7uWzoGn4kv,Transparency & Explainability,HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model,"Current video-language models (VLMs) rely extensively on instance-level alignment between video and language modalities, which presents two major limitations: (1) visual reasoning disobeys the natural perception that humans do in first-person perspective, leading to a lack of reasoning interpretation; and (2) learning is limited in capturing inherent fine-grained relationships between two modalities.  In this paper, we take an inspiration from human perception and explore a compositional approach for egocentric video representation. We introduce HENASY (Hierarchical ENtities ASsemblY), which includes a spatiotemporal token grouping mechanism to explicitly assemble dynamically evolving scene entities through time and model their relationship for video representation. By leveraging compositional structure understanding, HENASY possesses strong interpretability via visual grounding with free-form text queries. We further explore a suite of multi-grained contrastive losses to facilitate entity-centric understandings. This comprises three alignment types: video-narration, noun-entity, verb-entities alignments.  Our method demonstrates strong interpretability in both quantitative and qualitative experiments; while maintaining competitive performances on five downstream tasks via zero-shot transfer or as video/text representation, including video/text retrieval, action recognition, multi-choice query, natural language query, and moments query.  Project page: https://uark-aicv.github.io/HENASY","['egocentric', 'video understanding', 'vision langauge models']",[],"['Khoa Vo', 'Thinh Phan', 'Kashu Yamazaki', 'Minh Tran', 'Ngan Hoang Le']","['University of Arkansas - Fayetteville', 'University of Arkansas - Fayetteville', 'LTI, CMU, Carnegie Mellon University', 'EECS, University of Arkansas - Fayetteville', 'EECS, University of Arkansas, Fayetteville']",
https://openreview.net/forum?id=7qJFkuZdYo,Security,Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization,"Researchers have been studying approaches to steer the behavior of Large Language Models (LLMs) and build personalized LLMs tailored for various applications. While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM.  Recent endeavors have introduced more lightweight strategies, focusing on extracting ``steering vectors'' to guide the model's output toward desired behaviors by adjusting activations within specific layers of the LLM's transformer architecture. However, such steering vectors are directly extracted from the activations of human preference data and thus often lead to suboptimal results and occasional failures, especially in alignment-related scenarios. In this work, we propose an innovative approach that could produce more effective steering vectors through bi-directional preference optimization.  Our method is designed to allow steering vectors to directly influence the generation probability of contrastive human preference data pairs, thereby offering a more precise representation of the target behavior. By carefully adjusting the direction and magnitude of the steering vector, we enabled personalized control over the desired behavior across a spectrum of intensities. Extensive experimentation across various open-ended generation tasks, particularly focusing on steering AI personas, has validated the efficacy of our approach.  Moreover, we comprehensively investigate critical alignment-concerning scenarios, such as managing truthfulness, mitigating hallucination, and addressing jailbreaking attacks alongside their respective defenses. Remarkably, our method can still demonstrate outstanding steering effectiveness across these scenarios. Furthermore, we showcase the transferability of our steering vectors across different models/LoRAs and highlight the synergistic benefits of applying multiple vectors simultaneously. These findings significantly broaden the practicality and versatility of our proposed method.","['personalized steering of LLMs', 'steering vector', 'LLM alignment']",[],"['Yuanpu Cao', 'Tianrong Zhang', 'Bochuan Cao', 'Ziyi Yin', 'Lu Lin', 'Fenglong Ma', 'Jinghui Chen']","['Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University', 'College of Information Sciences and Technology, Pennsylvania State University', 'Pennsylvania State University']",
https://openreview.net/forum?id=6CFHg7exjY,Fairness & Bias,Addressing Hidden Confounding with Heterogeneous Observational Datasets for Recommendation,"The collected data in recommender systems generally suffers selection bias. Considerable works are proposed to address selection bias induced by observed user and item features, but they fail when hidden features (e.g., user age or salary) that affect both user selection mechanism and feedback exist, which is called hidden confounding. To tackle this issue, methods based on sensitivity analysis and leveraging a few randomized controlled trial (RCT) data for model calibration are proposed. However, the former relies on strong assumptions of hidden confounding strength, whereas the latter relies on the expensive RCT data, thereby limiting their applicability in real-world scenarios. In this paper, we propose to employ heterogeneous observational data to address hidden confounding, wherein some data is subject to hidden confounding while the remaining is not. We argue that such setup is more aligned with practical scenarios, especially when some users do not have complete personal information (thus assumed with hidden confounding), while others do have (thus assumed without hidden confounding). To achieve unbiased learning, we propose a novel meta-learning based debiasing method called MetaDebias. This method explicitly models oracle error imputation and hidden confounding bias, and utilizes bi-level optimization for model training. Extensive experiments on three public datasets validate our method achieves state-of-the-art performance in the presence of hidden confounding, regardless of RCT data availability.","['Recommender System', 'Causal Inference', 'Bias', 'Hidden Confounding']",[],"['Yanghao Xiao', 'Haoxuan Li', 'Yongqiang Tang', 'Wensheng Zhang']","['University of Chinese Academy of Sciences', 'Center for Data Science, Peking University', '', 'School of Computer Science and Cyber Engineering, Guangzhou University']",
https://openreview.net/forum?id=6ArNmbMpKF,Privacy & Data Governance,Noisy Dual Mirror Descent: A Near Optimal Algorithm for Jointly-DP Convex Resource Allocation,"We study convex resource allocation problems with $m$ hard constraints under $(\varepsilon,\delta)$-joint differential privacy (Joint-DP or JDP) in an offline setting. To approximately solve the problem, we propose a generic algorithm called Noisy Dual Mirror Descent. The algorithm applies noisy Mirror Descent to a dual problem from relaxing the hard constraints for private shadow prices, and then uses the shadow prices to coordinate allocations in the primal problem. Leveraging weak duality theory, we show that the optimality gap is upper bounded by $\mathcal{O}(\frac{\sqrt{m\ln(1/\delta)}}{\varepsilon})$, and constraint violation is no more than $\mathcal{O}(\frac{\sqrt{m\ln(1/\delta)}}{\varepsilon})$ per constraint. When strong duality holds, both preceding results can be improved to $\widetilde{\mathcal{O}}(\frac{\sqrt{\ln(1/\delta)}}{\varepsilon})$ by better utilizing the geometric structure of the dual space, which is neglected by existing works. To complement our results under strong duality, we derive a minimax lower bound $\Omega(\frac{m}{\varepsilon})$ for any JDP algorithm outputting feasible allocations. The lower bound matches our upper bounds up to some logarithmic factors for $\varepsilon\geq \max(1, 1/(n\gamma))$, where $n\gamma$ is the available resource level. Numerical studies further confirm the effectiveness of our algorithm.","['joint differential privacy', 'resource allocation', 'near optimal algorithm', 'mirror descent']",[],"['Du Chen', 'Geoffrey A. Chua']","['Nanyang Technological University', 'Nanyang Technological University']",
https://openreview.net/forum?id=5IRtAcVbiC,Security,e-COP : Episodic Constrained Optimization of Policies,"In this paper, we present the e-COP algorithm, the first policy optimization algorithm for constrained Reinforcement Learning (RL) in episodic (finite horizon) settings. Such formulations are applicable when there are separate sets of optimization criteria and constraints on a system's behavior. We approach this problem by first establishing a policy difference lemma for the episodic setting, which provides the theoretical foundation for the algorithm. Then, we propose to combine a set of established and novel solution ideas to yield the e-COP  algorithm that is easy to implement and numerically stable, and provide a theoretical guarantee on optimality under certain scaling assumptions. Through extensive empirical analysis using benchmarks in the Safety Gym suite, we show that our algorithm has similar or better performance than SoTA (non-episodic) algorithms adapted for the episodic setting. The scalability of the algorithm  opens the door to its application in safety-constrained Reinforcement Learning from Human Feedback for Large Language or Diffusion Models.","['reinforcement learning', 'policy optimization', 'constrained MDPs']",[],"['Akhil Agnihotri', 'Rahul Jain', 'Deepak Ramachandran', 'Sahil Singla']","['University of Southern California', 'DeepMind, Google DeepMind', 'Google', 'Google Deepmind, Google']",
https://openreview.net/forum?id=4jn7KWPHSD,Security,Improving Robustness of 3D Point Cloud Recognition from a Fourier Perspective,"Although 3D point cloud recognition has achieved substantial progress on standard benchmarks, the typical models are vulnerable to point cloud corruptions, leading to security threats in real-world applications. To improve the corruption robustness, various data augmentation methods have been studied, but they are mainly limited to the spatial domain. As the point cloud has low information density and significant spatial redundancy, it is challenging to analyze the effects of corruptions. In this paper, we focus on the frequency domain to observe the underlying structure of point clouds and their corruptions. Through graph Fourier transform (GFT), we observe a correlation between the corruption robustness of point cloud recognition models and their sensitivity to different frequency bands, which is measured by the GFT spectrum of the model’s Jacobian matrix. To reduce the sensitivity and improve the corruption robustness, we propose Frequency Adversarial Training (FAT) that adopts frequency-domain adversarial examples as data augmentation to train robust point cloud recognition models against corruptions. Theoretically, we provide a guarantee of FAT on its out-of-distribution generalization performance. Empirically, we conduct extensive experiments with various network architectures to validate the effectiveness of FAT, which achieves the new state-of-the-art results.","['3D point cloud recognition', 'corruption robustness', 'Frequency Adversarial Training']",[],"['Yibo Miao', 'Yinpeng Dong', 'Jinlai Zhang', 'Lijia Yu', 'Xiao Yang', 'Xiao-Shan Gao']","['Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences', 'Tsinghua University, Tsinghua University', 'Changsha University of Science and Technology', 'Institute of Software, Chinese Academy of Sciences', 'Tsinghua University, Tsinghua University', 'Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Chinese Academy of Sciences']",
https://openreview.net/forum?id=3pWHKxK1sC,Fairness & Bias,Conformal Classification with Equalized Coverage for Adaptively Selected Groups,This paper introduces a conformal inference method to evaluate uncertainty in classification by generating prediction sets with valid coverage conditional on adaptively chosen features. These features are carefully selected to reflect potential model limitations or biases. This can be useful to find a practical compromise between efficiency---by providing informative predictions---and algorithmic fairness---by ensuring equalized coverage for the most sensitive groups. We demonstrate the validity and effectiveness of this method on simulated and real data sets.,"['Classification', 'Conformal Prediction', 'Equalized Coverage', 'Fairness']",[],"['Yanfei Zhou', 'Matteo Sesia']","['Data Sciences and Operations, University of Southern California', 'Data Sciences and Operations, University of Southern California']",
https://openreview.net/forum?id=3uDEmsf3Jf,Security,OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning,"Offline safe reinforcement learning (RL) aims to train a policy that satisfies con- straints using a pre-collected dataset. Most current methods struggle with the mismatch between imperfect demonstrations and the desired safe and rewarding performance. In this paper, we mitigate this issue from a data-centric perspective and introduce OASIS (cOnditionAl diStributIon Shaping), a new paradigm in offline safe RL designed to overcome these critical limitations. OASIS utilizes a conditional diffusion model to synthesize offline datasets, thus shaping the data dis- tribution toward a beneficial target domain. Our approach makes compliance with safety constraints through effective data utilization and regularization techniques to benefit offline safe RL training. Comprehensive evaluations on public benchmarks and varying datasets showcase OASIS’s superiority in benefiting offline safe RL agents to achieve high-reward behavior while satisfying the safety constraints, out- performing established baselines. Furthermore, OASIS exhibits high data efficiency and robustness, making it suitable for real-world applications, particularly in tasks where safety is imperative and high-quality demonstrations are scarce. More details are available at the website https://sites.google.com/view/saferl-oasis/home.","['Safe Reinforcement Learning', 'Offline Reinforcement Learning']",[],"['Yihang Yao', 'Zhepeng Cen', 'Wenhao Ding', 'Haohong Lin', 'Shiqi Liu', 'Tingnan Zhang', 'Wenhao Yu', 'Ding Zhao']","['Carnegie Mellon University', 'CMU, Carnegie Mellon University', 'NVIDIA Research', 'Control Science and Engineering, Carnegie Mellon University', 'Carnegie Mellon University', 'Google', 'Google', 'Carnegie Mellon University']",
https://openreview.net/forum?id=3Ds5vNudIE,Transparency & Explainability,LLM Circuit Analyses Are Consistent Across Training and Scale,"Most currently deployed LLMs undergo continuous training or additional finetuning. By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings. Existing studies of mechanisms over time focus on encoder-only or toy models, which differ significantly from most deployed models. In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters. We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale. Moreover, although such components may be implemented by different attention heads over time, the overarching algorithm that they implement remains. Surprisingly, both these algorithms and the types of components involved therein tend to replicate across model scale. Finally, we find that circuit size correlates with model size and can fluctuate considerably over time even when the same algorithm is implemented. These results suggest that circuit analyses conducted on small models at the end of pre-training can provide insights that still apply after additional training and over model scale.","['training dynamics', 'interpretability', 'mechanistic interpretability']",[],"['Curt Tigges', 'Michael Hanna', 'Qinan Yu', 'Stella Biderman']","['Decode Research', 'Institute for Logic, Language, and Computation, University of Amsterdam', 'Stanford University', 'EleutherAI']",
https://openreview.net/forum?id=2cQ3lPhkeO,Fairness & Bias,Provably Mitigating Overoptimization in RLHF: Your SFT Loss is Implicitly an Adversarial Regularizer,"Aligning generative models with human preference via RLHF typically suffers from overoptimization, where an imperfectly learned reward model can misguide the generative model to output even undesired responses. We investigate this problem in a principled manner by identifying the source of the issue as the distributional shift and uncertainty of human preference in dataset. To mitigate overoptimization, we first propose a theoretical algorithm which optimizes the policy against an adversarially chosen reward model, one that simultaneously minimizes its MLE loss and a reward penalty term. The penalty pessimistically biases the uncertain rewards so as to prevent the policy from choosing actions with spursiouly high proxy rewards, resulting in provable sample efficiency of the algorithm under a partial coverage style condition. Moving from theory to practice, the proposed algorithm further enjoys an equivalent but surprisingly easy to implement form. With a clever usage of the equivalence between reward models and the corresponding optimal policy, the algorithm features a simple objective that combines (i) a preference optimization loss that directly aligns the policy with human preference, and (ii) a supervised learning loss which explicitly imitates the policy with a baseline distribution. In the context of aligning large language models (LLM), this objective fuses the direct preference optimization (DPO) loss with the supervised fune-tuning (SFT) loss to help mitigate the overoptimization towards undesired responses, for which we name the algorithm Regularized Preference Optimization (RPO). Experiments of aligning LLMs demonstrate the improved performance of our method when compared with DPO baselines.  Our work sheds light on the interplay between preference optimization and SFT in tuning LLMs with both theoretical guarantees and empirical evidence.",['Alignment; Reinforcement Learning from Human Feedback'],[],"['Zhihan Liu', 'Miao Lu', 'Shenao Zhang', 'Boyi Liu', 'Hongyi Guo', 'Yingxiang Yang', 'Jose Blanchet', 'Zhaoran Wang']","['Northwestern University', 'Stanford University', 'Northwestern University', 'Seed-Foundation-Code, ByteDance Inc.', 'Northwestern University, Northwestern University', 'ByteDance Inc', 'Stanford University', 'Northwestern University']",
https://openreview.net/forum?id=1sLdprsbmk,Security,Can Models Learn Skill Composition from Examples?,"As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization---the capacity to combine learned skills in novel ways not encountered during training---has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the Skill-Mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$.  In this paper, we employ a setup akin to Skill-Mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills---including rhetorical, literary, reasoning, theory of mind, and common sense---GPT was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: (1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.  This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.","['Skill Composition', 'Large Language Model']",[],"['Haoyu Zhao', 'Simran Kaur', 'Dingli Yu', 'Anirudh Goyal', 'Sanjeev Arora']","['Princeton University', 'Princeton University', 'Microsoft Research, Microsoft', 'Google DeepMind', 'Princeton University']",
https://openreview.net/forum?id=0Lb8vZT1DB,Fairness & Bias,Reliable Learning of Halfspaces under Gaussian Marginals,"We study the problem of PAC learning halfspaces in the  reliable agnostic model of Kalai et al. (2012). The reliable PAC model   captures learning scenarios where one type of error is  costlier than the others. Our main positive result is a  new algorithm for reliable learning  of Gaussian halfspaces on  $\mathbb{R}^d$ with sample and computational complexity  $d^{O(\log (\min\{1/\alpha, 1/\epsilon\}))}\min (2^{\log(1/\epsilon)^{O(\log (1/\alpha))}},2^{\mathrm{poly}(1/\epsilon)})$,  where $\epsilon$ is the excess error and $\alpha$  is the bias of the optimal halfspace. We complement our upper bound with  a Statistical Query lower bound  suggesting that the $d^{\Omega(\log (1/\alpha))}$ dependence is best possible.  Conceptually, our results imply a strong computational separation  between reliable agnostic learning and standard agnostic  learning of halfspaces in the Gaussian setting.","['reliable learning', 'agnostic learning', 'halfspace', 'proper leaning', 'statistical query']",[],"['Ilias Diakonikolas', 'Lisheng Ren', 'Nikos Zarifis']","['Computer Sciences, University of Wisconsin - Madison', 'University of Wisconsin - Madison', 'University of Wisconsin, Madison']",
https://openreview.net/forum?id=vCIc9BXzze,Fairness & Bias,Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors. Despite their effectiveness, a notable drawback of LLMs is their inconsistent moral behavior, which raises ethical concerns. This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios. Our extensive investigation of twelve popular LLMs reveals that their assessed consistency scores are influenced by position bias and selection bias rather than their intrinsic abilities. We propose a new framework tSMC, which gauges the effects of these biases and effectively mitigates the bias impact based on the Kullback–Leibler divergence to pinpoint LLMs' mitigated Symmetric Moral Consistency. We find that the ability of LLMs to maintain consistency varies across different moral scenarios. Specifically, LLMs show more consistency in scenarios with clear moral answers compared to those where no choice is morally perfect. The average consistency score of 12 LLMs ranges from $60.7\%$ in high-ambiguity moral scenarios to $84.8\%$ in low-ambiguity moral scenarios.","['Large Language Model', 'Moral Consistency', 'Evaluation', 'Ethics']",[],"['Ziyi Zhou', 'Xinwei Guo', 'Jiashi Gao', 'Xiangyu Zhao', 'Shiyao Zhang', 'Xin Yao', 'Xuetao Wei']","['Southern University of Science and Technology', 'Southern University of Science and Technology', 'Department of Computer Science and Engineering, Southern University of Science and Technology', 'Department of Data Science, City University of Hong Kong', 'Southern University of Science and Technology', 'Southern University of Science and Technology', 'Computer Science and Engineering , Southern University of Science and Technology']",
https://openreview.net/forum?id=x7pjdDod6Z,Fairness & Bias,MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model,"Open-world 3D reconstruction models have recently garnered significant attention. However, without sufficient 3D inductive bias, existing methods typically entail expensive training costs and struggle to extract high-quality 3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. Specifically, instead of using a triplane representation, we store features in 3D sparse voxels and combine transformers with 3D convolutions to leverage an explicit 3D structure and projective bias. In addition to sparse-view RGB input, we require the network to take input and generate corresponding normal maps. The input normal maps can be predicted by 2D diffusion models, significantly aiding in the guidance and refinement of the geometry's learning. Moreover, by combining Signed Distance Function (SDF) supervision with surface rendering, we directly learn to generate high-quality meshes without the need for complex multi-stage training processes. By incorporating these explicit 3D biases, MeshFormer can be trained efficiently and deliver high-quality textured meshes with fine-grained geometric details. It can also be integrated with 2D diffusion models to enable fast single-image-to-3D and text-to-3D tasks. **Videos are available at https://meshformer3d.github.io/**","['sparse view 3D reconstruction', '3D generation', '3D AIGC', 'reconstruction model']",[],"['Minghua Liu', 'Chong Zeng', 'Xinyue Wei', 'Ruoxi Shi', 'Linghao Chen', 'Chao Xu', 'Mengqi Zhang', 'Zhaoning Wang', 'Xiaoshuai Zhang', 'Isabella Liu', 'Hongzhi Wu', 'Hao Su']","['CSE, University of California, San Diego', 'Zhejiang University', 'ECE, University of California, San Diego', 'University of California, San Diego', 'Zhejiang University', 'University of California, Los Angeles', 'Georgia Institute of Technology', 'Hillbot Inc.', 'Hillbot', 'University of California, San Diego', 'Zhejiang University', 'University of California, San Diego']",
https://openreview.net/forum?id=hhnkH8ex5d,Fairness & Bias,Precipitation Downscaling with Spatiotemporal Video Diffusion,"In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it against six state-of-the-art baselines. Our analysis, capturing CRPS, MSE, precipitation distributions, and qualitative aspects using California and the Himalayas as examples, establishes our method as a new standard for data-driven precipitation downscaling.","['precipitation downscaling', 'diffusion models']",[],"['Prakhar Srivastava', 'Ruihan Yang', 'Gavin Kerrigan', 'Gideon Dresdner', 'Jeremy J McGibbon', 'Christopher S. Bretherton', 'Stephan Mandt']","['University of California, Irvine', 'Computer Science, University of California, Irvine', 'Statistics, University of Oxford', 'Allen Institute for Artificial Intelligence', 'Climate Modeling, Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'CS, University of California, Irvine']",
https://openreview.net/forum?id=ZOZjMs3JTs,Fairness & Bias,User-item fairness tradeoffs in recommendations,"In the basic recommendation paradigm, the most (predicted) relevant item is recommended to each user. This may result in some items receiving lower exposure than they ""should""; to counter this, several algorithmic approaches have been developed to ensure *item fairness*. These approaches necessarily degrade recommendations for some users to improve outcomes for items, leading to *user fairness* concerns. In turn, a recent line of work has focused on developing algorithms for multi-sided fairness, to jointly optimize user fairness, item fairness, and overall recommendation quality. This induces the question: *what is the tradeoff between these objectives, and what are the characteristics of (multi-objective) optimal solutions?* Theoretically, we develop a model of recommendations with user and item fairness objectives and characterize the solutions of fairness-constrained optimization. We identify two phenomena: (a) when user preferences are diverse, there is ""free"" item and user fairness; and (b) users whose preferences are misestimated can be *especially* disadvantaged by item fairness constraints. Empirically, we prototype a recommendation system for preprints on arXiv and implement our framework, measuring the phenomena in practice and showing how these phenomena inform the *design* of markets with recommendation systems-intermediated matching.","['recommendation systems', 'algorithmic fairness']",[],"['Sophie Greenwood', 'Sudalakshmee Chiniah', 'Nikhil Garg']","['', 'Operations Research and Information Engineering, Cornell University', 'Cornell University']",
https://openreview.net/forum?id=TFAG9UznPv,Security,On the Scalability of Certified Adversarial Robustness with Generated Data,"Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses but also reveal notable differences in the scaling behavior between certified and empirical methods. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and $\ell_{\infty}$ ($\epsilon = 8/255$) threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively. Furthermore, we report similar improvements for CIFAR-100.","['certified robustness', 'adversarial robustness', 'scaling', 'generated data']",[],"['Thomas Altstidl', 'David Dobre', 'Arthur Kosmala', 'Bjoern Eskofier', 'Gauthier Gidel', 'Leo Schwinn']","['Friedrich-Alexander Universität Erlangen-Nürnberg', 'Mila - Quebec Artificial Intelligence Institute', 'School of Computation, Information and Technology, Technische Universität München', '', 'Computer Science and Operational Research, University of Montreal', 'Technical University of Munich']",
https://openreview.net/forum?id=ZVrrPNqHFw,Fairness & Bias,A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective,"Learning generalized models from biased data is an important undertaking toward fairness in deep learning. To address this issue, recent studies attempt to identify and leverage bias-conflicting samples free from spurious correlations without prior knowledge of bias or an unbiased set. However, spurious correlation remains an ongoing challenge, primarily due to the difficulty in correctly detecting these samples. In this paper, inspired by the similarities between mislabeled samples and bias-conflicting samples, we approach this challenge from a novel perspective of mislabeled sample detection. Specifically, we delve into Influence Function, one of the standard methods for mislabeled sample detection, for identifying bias-conflicting samples and propose a simple yet effective remedy for biased models by leveraging them. Through comprehensive analysis and experiments on diverse datasets, we demonstrate that our new perspective can boost the precision of detection and rectify biased models effectively. Furthermore, our approach is complementary to existing methods, showing performance improvement even when applied to models that have already undergone recent debiasing techniques.","['Debiasing', 'Spurious correlation', 'Robust learning', 'Dataset bias']",[],"['Yeonsung Jung', 'Jaeyun Song', 'June Yong Yang', 'Jin-Hwa Kim', 'Sung-Yub Kim', 'Eunho Yang']","['Korea Advanced Institute of Science and Technology', 'Graduate School of AI, Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology', 'Seoul National University', 'Samsung', 'Korea Advanced Institute of Science & Technology']",
https://openreview.net/forum?id=G7L65B2P0y,Transparency & Explainability,An effective framework for estimating individualized treatment rules,"Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications. Traditional ITR estimation methods rely on inverse probability weighting (IPW) to address confounding factors and $L_{1}$-penalization for simplicity and interpretability. However, IPW can introduce statistical bias without precise propensity score modeling, while $L_1$-penalization makes the objective non-smooth, leading to computational bias and requiring subgradient methods. In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem. The optimal ITR can be robustly and effectively computed by projected gradient descent. Our comprehensive theoretical analysis reveals that weights that balance the spectrum of a `weighted design matrix' improve both the optimization and likelihood landscapes, yielding improved computational and statistical estimation guarantees. In particular, this is achieved by distributional covariate balancing weights, which are model-free alternatives to IPW. Extensive simulations and applications demonstrate that our framework achieves significant gains in both robustness and effectiveness for ITR learning against existing methods.","['Covariate Balancing', 'Multicategory Treatment', 'Precision Medicine', 'Projected Gradient Descent', 'Convergence Analysis', 'Constrained Optimization']",[],"['Joowon Lee', 'Jared Davis Huling', 'Guanhua Chen']","['Statistics, University of Wisconsin - Madison', 'University of Minnesota - Twin Cities', 'University of Wisconsin - Madison']",
https://openreview.net/forum?id=4ZH48aGD60,Security,"Active, anytime-valid risk controlling prediction sets","Rigorously establishing the safety of black-box machine learning models with respect to critical risk measures is important for providing guarantees about the behavior of the model. Recently, a notion of a risk controlling prediction set (RCPS) has been introduced by Bates et. al. (JACM '24) for producing prediction sets that are statistically guaranteed to have low risk from machine learning models. Our method extends this notion to the sequential setting, where we provide guarantees even when the data is collected adaptively, and ensures the risk guarantee is anytime-valid, i.e., simultaneously holds at all time steps. Further, we propose a framework for constructing RCPSes for active labeling, i.e., allowing one to use a labeling policy that chooses whether to query the true label for each received data point, and ensures the expected proportion data points whose labels are queried are below a predetermined label budget. We also describe how to use predictors (e.g., the machine learning model we are providing risk control guarantees for) to further improve the utility of our RCPSes by estimating the expected risk conditioned on the covariates. We characterize the optimal choices of label policy under a fixed label budget, and predictor, and show a regret result that relates the estimation error of the optimal labeling policy and predictor to the wealth process that underlies our RCPSes. Lastly, we present practical ways of formulating label policies and we empirically show that our label policies use fewer labels to reach higher utility than naive baseline labeling strategies on both simulations and real data.","['distribution free', 'conformal prediction', 'e-process', 'confidence sequence']",[],"['Ziyu Xu', 'Nikos Karampatziakis', 'Paul Mineiro']","['Carnegie Mellon University', 'Microsoft', 'University of California, San Diego']",
https://openreview.net/forum?id=yKvHJJE9le,Security,Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel,"Ensuring safety is a key aspect in sequential decision making problems, such as robotics or process control. The complexity of the underlying systems often makes finding the optimal decision challenging, especially when the safety-critical system is time-varying. Overcoming the problem of optimizing an unknown time-varying reward subject to unknown time-varying safety constraints, we propose TVSAFEOPT, a new algorithm built on Bayesian optimization with a spatio-temporal kernel. The algorithm is capable of safely tracking a time-varying safe region without the need for explicit change detection. Optimality guarantees are also provided for the algorithm when the optimization problem becomes stationary. We show that TVSAFEOPT compares favorably against SAFEOPT on synthetic data, both regarding safety and optimality. Evaluation on a realistic case study with gas compressors confirms that TVSAFEOPT ensures safety when solving time-varying optimization problems with unknown reward and safety functions.","['Safe learning', 'Bayesian optimization', 'Time-varying optimization']",[],"['Jialin Li', 'Marta Zagorowska', 'Giulia De Pasquale', 'Alisa Rupenyan', 'John Lygeros']","['Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign', '', 'Eindhoven University of Technology', 'ZHAW - Zürcher Hochschule für Angewandte Wissenschaften', 'ETHZ - ETH Zurich']",
https://openreview.net/forum?id=yMS7ansbr6,Security,Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes,"In recent years, DeepFake technology has achieved unprecedented success in high-quality video synthesis, but these methods also pose potential and severe security threats to humanity. DeepFake can be bifurcated into entertainment applications like face swapping and illicit uses such as lip-syncing fraud. However, lip-forgery videos, which neither change identity nor have discernible visual artifacts, present a formidable challenge to existing DeepFake detection methods. Our preliminary experiments have shown that the effectiveness of the existing methods often drastically decrease or even fail when tackling lip-syncing videos. In this paper, for the first time, we propose a novel approach dedicated to lip-forgery identification that exploits the inconsistency between lip movements and audio signals. We also mimic human natural cognition by capturing subtle biological links between lips and head regions to boost accuracy. To better illustrate the effectiveness and advances of our proposed method, we create a high-quality LipSync dataset, AVLips, by employing the state-of-the-art lip generators. We hope this high-quality and diverse dataset could be well served the further research on this challenging and interesting field. Experimental results show that our approach gives an average accuracy of more than 95.3% in spotting lip-syncing videos, significantly outperforming the baselines. Extensive experiments demonstrate the capability to tackle deepfakes and the robustness in surviving diverse input transformations. Our method achieves an accuracy of up to 90.2% in real-world scenarios (e.g., WeChat video call) and shows its powerful capabilities in real scenario deployment. To facilitate the progress of this research community, we release all resources at https://github.com/AaronComo/LipFD.","['DeepFake Detection', 'LipSync Detection', 'AI security']",[],"['Weifeng Liu', 'Tianyi She', 'Jiawei Liu', 'Boheng Li', 'Dongyu Yao', 'Ziyou Liang', 'Run Wang']","['Wuhan University', 'Wuhan University', 'Cyber security Academy, Wuhan University', 'Nanyang Technological University', 'School of Computer Science, Carnegie Mellon University', 'Wuhan University', 'Wuhan University']",
https://openreview.net/forum?id=w67vRHZF13,Fairness & Bias,Unified Generative and Discriminative Training for Multi-modal Large Language Models,"In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM’s hidden state. This approach enhances the MLLM’s ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling.","['vision-language', 'multi-modal understanding']",[],"['Wei Chow', 'Juncheng Li', 'Qifan Yu', 'Kaihang Pan', 'Hao Fei', 'Zhiqi Ge', 'Shuai Yang', 'Siliang Tang', 'Hanwang Zhang', 'Qianru Sun']","['Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'Zhejiang University', 'National University of Singapore', 'Computer Science, Zhejiang University', 'college of information science and electronic engineering, Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'computer science, Nanyang Technological University', 'School of Computing and Information Systems, Singapore Management University']",
https://openreview.net/forum?id=uHs6RJFDsg,Fairness & Bias,MoVA: Adapting Mixture of Vision Experts to Multimodal Context,"As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks.","['Multimodal large language model', 'Vision encoder', 'Mixture-of-expert']",[],"['Zhuofan Zong', 'Bingqi Ma', 'Dazhong Shen', 'Guanglu Song', 'Hao Shao', 'Dongzhi Jiang', 'Hongsheng Li', 'Yu Liu']","['The Chinese University of Hong Kong', 'Sensetime Group Limmited', 'College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics', 'Sensetime', 'The Chinese University of Hong Kong, The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'SenseTime']",
https://openreview.net/forum?id=tZRpvLXevU,Fairness & Bias,Latent Representation Matters: Human-like Sketches in One-shot Drawing Tasks,"Humans can effortlessly draw new categories from a single exemplar, a feat that has long posed a challenge for generative models. However, this gap has started to close with recent advances in diffusion models. This one-shot drawing task requires powerful inductive biases that have not been systematically investigated. Here, we study how different inductive biases shape the latent space of Latent Diffusion Models (LDMs). Along with standard LDM regularizers (KL and vector quantization), we explore supervised regularizations (including classification and prototype-based representation) and contrastive inductive biases (using  SimCLR and redundancy reduction objectives). We demonstrate that LDMs with redundancy reduction and prototype-based regularizations produce near-human-like drawings (regarding both samples' recognizability and originality) -- better mimicking human perception (as evaluated psychophysically). Overall, our results suggest that the gap between humans and machines in one-shot drawings is almost closed.","['Neuroscience', 'Cognitive Science', 'One-Shot Generative Models', 'Latent Diffusion Models', 'Human Machine alignment', 'Human-Machine comparison']",[],"['Victor Boutin', 'Rishav Mukherji', 'Aditya Agrawal', 'Sabine Muzellec', 'Thomas FEL', 'Thomas Serre', 'Rufin VanRullen']","['CerCo, CNRS', 'School of Computer Science, Carnegie Mellon University', 'Birla Institute of Tchnology and Science - KK Birla Goa Campus', '', 'Kempner Institute, Harvard University', 'Brown University', 'CNRS']",
https://openreview.net/forum?id=pR37AmwbOt,Security,Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning,"Diffusion models (DMs) have demonstrated remarkable proficiency in producing images based on textual prompts. Numerous methods have been proposed to ensure these models generate safe images. Early methods attempt to incorporate safety filters into models to mitigate the risk of generating harmful images but such external filters do not inherently detoxify the model and can be easily bypassed. Hence, model unlearning and data cleaning are the most essential methods for maintaining the safety of models, given their impact on model parameters. However, malicious fine-tuning can still make models prone to generating harmful or undesirable images even with these methods. Inspired by the phenomenon of catastrophic forgetting, we propose a training policy using contrastive learning to increase the latent space distance between clean and harmful data distribution, thereby protecting models from being fine-tuned to generate harmful images due to forgetting. The experimental results demonstrate that our methods not only maintain clean image generation capabilities before malicious fine-tuning but also effectively prevent DMs from producing harmful images after malicious fine-tuning. Our method can also be combined with other safety methods to maintain their safety against malicious fine-tuning further.","['Diffusion Model', 'AI Safety']",[],"['Jiadong Pan', 'Hongcheng Gao', 'Zongyu Wu', 'taihang Hu', 'Li Su', 'Qingming Huang', 'Liang Li']","['Institute of Computing Technology, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'Pennsylvania State University', 'Nankai University', 'School of Computer Science and Technology, University of Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'Computer Science, Institute of Computing Technology, Chinese Academy of Sciences']",
https://openreview.net/forum?id=kamAXSJxGV,Privacy & Data Governance,Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy,"When releasing outputs from confidential data, agencies need to balance the analytical usefulness of the released data with the obligation to protect data subjects' confidentiality. For releases satisfying differential privacy, this balance is reflected by the privacy budget, $\varepsilon$. We provide a framework for setting $\varepsilon$ based on its relationship with Bayesian posterior probabilities of disclosure. The agency responsible for the data release decides how much posterior risk it is willing to accept at various levels of prior risk, which implies a unique $\varepsilon$. Agencies can evaluate different risk profiles to determine one that leads to an acceptable trade-off in risk and utility.","['confidentiality', 'disclosure', 'risk', 'semantics', 'utility']",[],"['Zeki Kazan', 'Jerome Reiter']","['Statistical Science, Duke University', 'Statistical Science, Duke University']",
https://openreview.net/forum?id=n5R6TvBVcX,Security,WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models,"We introduce WildTeaming, an automatic red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes selections of multiple mined tactics for systematic exploration of novel and even more challenging jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with large language models (LLMs), our work investigates jailbreaks from chatbot users in-the-wild who were not specifically instructed to break the system.  WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in more diverse and successful adversarial attacks compared to state-of-the-art jailbreaking methods.   While there exist many datasets for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed among all frontier models even when their weights are open. Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. In order to mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (both vanilla and adversarial) and 2) benign queries that resemble harmful queries in form but contain no harmful intent. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive model training and evaluations, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of both vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All the components of WildJailbreak contribute to achieving balanced safety behaviors of models","['Red-teaming', 'AI Safety', 'Safety Training', 'LLM Defense', 'Safety Training Data', 'Adversarial Training', 'Adversarial Attacks', 'Jailbreak']",[],"['Liwei Jiang', 'Kavel Rao', 'Seungju Han', 'Allyson Ettinger', 'Faeze Brahman', 'Sachin Kumar', 'Niloofar Mireshghallah', 'Ximing Lu', 'Maarten Sap', 'Yejin Choi', 'Nouha Dziri']","['University of Washington', 'Department of Computer Science, University of Washington', 'NVIDIA', 'Allen Institute for Artificial Intelligence', 'Allen Institute for AI', 'Computer Science and Engineering, Ohio State University, Columbus', 'University of Washington', 'Department of Computer Science, University of Washington', 'Carnegie Mellon University', 'Computer Science, Computer Science Department, Stanford University', '']",
https://openreview.net/forum?id=kW30LbNwdV,Fairness & Bias,Improving Adversarial Robust Fairness via Anti-Bias Soft Label  Distillation,"Adversarial Training (AT) has been widely proved to be an effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD) has demonstrated its superior performance in improving the robustness of small student models with the guidance of large teacher models. However, both AT and ARD encounter the robust fairness problem: these models exhibit strong robustness when facing part of classes (easy class), but weak robustness when facing others (hard class). In this paper, we give an in-depth analysis of the potential factors and argue that the smoothness degree of samples' soft labels for different classes (i.e., hard class or easy class) will affect the robust fairness of DNNs from both empirical observation and theoretical analysis. Based on the above finding, we propose an Anti-Bias Soft Label Distillation (ABSLD) method to mitigate the adversarial robust fairness problem within the framework of Knowledge Distillation (KD). Specifically, ABSLD adaptively reduces the student's error risk gap between different classes to achieve fairness by adjusting the class-wise smoothness degree of samples' soft labels during the training process, and the smoothness degree of soft labels is controlled by assigning different temperatures in KD to different classes. Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT, ARD, and robust fairness methods in the comprehensive metric (Normalized Standard Deviation) of robustness and fairness.","['Adversarial Robustness', 'Robust Fairness', 'Knowledge Distillation']",[],"['Shiji Zhao', 'Ranjie Duan', 'xizhewang', 'Xingxing Wei']","['Institute of Artificial Intelligence, Beijing University of Aeronautics and Astronautics', 'Security, Alibaba Group', 'Beijing University of Aeronautics and Astronautics', 'Beihang University']",
https://openreview.net/forum?id=hVGAGU4TKk,Fairness & Bias,NeuRodin: A Two-stage Framework for High-Fidelity Neural Surface Reconstruction,"Signed Distance Function (SDF)-based volume rendering has demonstrated significant capabilities in surface reconstruction. Although promising, SDF-based methods often fail to capture detailed geometric structures, resulting in visible defects. By comparing SDF-based volume rendering to density-based volume rendering, we identify two main factors within the SDF-based approach that degrade surface quality: SDF-to-density representation and geometric regularization. These factors introduce challenges that hinder the optimization of the SDF field. To address these issues, we introduce NeuRodin, a novel two-stage neural surface reconstruction framework that not only achieves high-fidelity surface reconstruction but also retains the flexible optimization characteristics of density-based methods.    NeuRodin incorporates innovative strategies that facilitate transformation of arbitrary topologies and reduce artifacts associated with density bias.   Extensive evaluations on the Tanks and Temples and ScanNet++ datasets demonstrate the superiority of NeuRodin, showing strong reconstruction capabilities for both indoor and outdoor environments using solely posed RGB captures. Project website: https://open3dvlab.github.io/NeuRodin/",['SDF representation; Volume Rendering; High-Fidelity Surface Reconstruction'],[],"['Yifan Wang', 'Di Huang', 'Weicai Ye', 'Guofeng Zhang', 'Wanli Ouyang', 'Tong He']","[""School of Electronic and Information Engineering, Xi'an Jiaotong University"", '', 'KwaiVGI, Kuaishou Technology', 'Zhejiang University', 'Shanghai AI Lab', 'AI Lab, Shanghai AI lab']",
https://openreview.net/forum?id=hNlk9cIGo9,Privacy & Data Governance,Faster Algorithms for User-Level Private Stochastic Convex Optimization,"We study private stochastic convex optimization (SCO) under user-level differential privacy (DP) constraints. In this setting, there are $n$ users (e.g., cell phones), each possessing $m$ data items (e.g., text messages), and we need to protect the privacy of each user's entire collection of data items. Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least $(mn)^{3/2}$ gradient computations for smooth losses and $(mn)^3$ computations for non-smooth losses. To address these limitations, we provide novel user-level DP algorithms with state-of-the-art excess risk and runtime guarantees, without stringent assumptions. First, we develop a linear-time algorithm with state-of-the-art excess risk (for a non-trivial linear-time algorithm) under a mild smoothness assumption. Our second algorithm applies to arbitrary smooth losses and achieves optimal excess risk in $\approx (mn)^{9/8}$ gradient computations. Third, for non-smooth loss functions, we obtain optimal excess risk in $n^{11/8} m^{5/4}$ gradient computations. Moreover, our algorithms do not require the number of users to grow polynomially with the dimension.","['differential privacy', 'user-level privacy', 'stochastic convex optimization', 'private optimization']",[],"['Andrew Lowy', 'Daogao Liu', 'Hilal Asi']","['University of Wisconsin - Madison', 'University of Washington, Seattle', 'Apple']",
https://openreview.net/forum?id=eAqcVZx30k,Transparency & Explainability,Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization,"An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures.  In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales.  This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants.  Code: \url{https://github.com/jugechengzi/Rationalization-MRD}.","['interpretability', 'data cleaning', 'causality', 'mutual information']",[],"['Wei Liu', 'Zhiying Deng', 'Zhongyu Niu', 'Jun Wang', 'Haozhao Wang', 'YuanKai Zhang', 'Ruixuan Li']","['', '', 'University of Science and Technology of China', 'iWudao', 'Computer Science an Technology, Huazhong University of Science and Technology', '', 'Huazhong University of Science and Technology']",
https://openreview.net/forum?id=bN5PA3HHo8,Security,Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization,"Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content. This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which has been shown to successfully jailbreak multiple open-source LLMs. Drawing inspiration from the difficulties of discrete token optimization, our method relaxes the discrete jailbreak optimization into a continuous optimization process while gradually increasing the sparsity of the optimizing vectors. This technique effectively bridges the gap between discrete and continuous space optimization. Experimental results demonstrate that our method is more effective and efficient than state-of-the-art token-level methods. On Harmbench, our approach achieves the highest attack success rate on seven out of eight LLMs compared to the latest jailbreak methods. \textcolor{red}{Trigger Warning: This paper contains model behavior that can be offensive in nature.}","['large language model', 'AI safety', 'jailbreak']",[],"['Kai Hu', 'Weichen Yu', 'Yining Li', 'Tianjun Yao', 'Xiang Li', 'Wenhe Liu', 'Lijun Yu', 'Zhiqiang Shen', 'Kai Chen', 'Matt Fredrikson']","['Carnegie Mellon University', 'ECE, Carnegie Mellon University', 'Shanghai AI Laboratory', '', '', 'LTI, Carnegie Mellon University', 'Google DeepMind', '', 'Shanghai AI Laboratory', 'Gray Swan AI']",
https://openreview.net/forum?id=bmoS6Ggw4j,Fairness & Bias,Can Graph Learning Improve Planning in LLM-based Agents?,"Task planning in language agents is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests in natural language into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. The performance gain increases with a larger task graph size.","['Task Planning', 'Language Agents', 'Graph Learning', 'Graph Neural Networks', 'Language Model']",[],"['Xixi Wu', 'Yifei Shen', 'Caihua Shan', 'Kaitao Song', 'Siwei Wang', 'Bohang Zhang', 'Jiarui Feng', 'Hong Cheng', 'Wei Chen', 'Yun Xiong', 'Dongsheng Li']","['The Chinese University of Hong Kong', 'Microsoft Research Asia', 'Microsoft', 'Microsoft', 'Microsoft', 'Peking University', 'Computer Sciencen and Engineering Department, Washington University, Saint Louis', 'The Chinese University of Hong Kong', 'Microsoft Research', 'School of Computer Science, Fudan University', 'Microsoft Research Asia']",
https://openreview.net/forum?id=VTJvTa41D0,Fairness & Bias,Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics,"Inverse problems describe the process of estimating the causal factors from a set of measurements or data.  Mapping of often incomplete or degraded data to parameters is ill-posed, thus data-driven iterative solutions are required, for example when reconstructing clean images from poor signals.  Diffusion models have shown promise as potent generative tools for solving inverse problems due to their superior reconstruction quality and their compatibility with iterative solvers. However, most existing approaches are limited to linear inverse problems represented as Stochastic Differential Equations (SDEs). This simplification falls short of addressing the challenging nature of real-world problems, leading to amplified cumulative errors and biases.  We provide an explanation for this gap through the lens of measure-preserving dynamics of Random Dynamical Systems (RDS) with which we analyse Temporal Distribution Discrepancy and thus introduce a theoretical framework based on RDS for SDE diffusion models. We uncover several strategies that inherently enhance the stability and generalizability of diffusion models for inverse problems and introduce a novel score-based diffusion framework, the Dynamics-aware SDE Diffusion Generative Model (D^3GM). The Measure-preserving property can return the degraded measurement to the original state despite complex degradation with the RDS concept of stability. Our extensive experimental results corroborate the effectiveness of D^3GM across multiple benchmarks including a prominent application for inverse problems, magnetic resonance imaging.","['Diffusion modeling', 'Random dynamical systems', 'Magnetic resonance imaging']",[],"['Weitong Zhang', 'Chengqi Zang', 'Liu Li', 'Sarah Cechnicka', 'Cheng Ouyang', 'Bernhard Kainz']","['', 'Economics, University of Tokyo', 'Imperial College London', 'Computer Science, Imperial College London', '', 'AIBE']",
https://openreview.net/forum?id=TutGINeJzZ,Fairness & Bias,A Huber Loss Minimization Approach to Mean Estimation under User-level Differential Privacy,"Privacy protection of users' entire contribution of samples is important in distributed systems. The most effective approach is the two-stage scheme, which finds a small interval first and then gets a refined estimate by clipping samples into the interval. However, the clipping operation induces bias, which is serious if the sample distribution is heavy-tailed. Besides, users with large local sample sizes can make the sensitivity much larger, thus the method is not suitable for imbalanced users. Motivated by these challenges, we propose a Huber loss minimization approach to mean estimation under user-level differential privacy. The connecting points of Huber loss can be adaptively adjusted to deal with imbalanced users. Moreover, it avoids the clipping operation, thus significantly reducing the bias compared with the two-stage approach. We provide a theoretical analysis of our approach, which gives the noise strength needed for privacy protection, as well as the bound of mean squared error. The result shows that the new method is much less sensitive to the imbalance of user-wise sample sizes and the tail of sample distributions. Finally, we perform numerical experiments to validate our theoretical analysis.","['differential privacy', 'robustness', 'huber loss']",[],"['Puning Zhao', 'Lifeng Lai', 'Li Shen', 'Qingming Li', 'Jiafei Wu', 'Zhe Liu']","['Zhejiang Lab', 'University of California, Davis', 'Sun Yat-Sen University', 'College of Computer Science & Technology, Zhejiang University', 'Zhejiang Lab', 'Zhejiang Lab']",
https://openreview.net/forum?id=Osh7u2E1kC,Fairness & Bias,Leveraging Separated World Model for Exploration in Visually Distracted Environments,"Model-based unsupervised reinforcement learning (URL) has gained prominence for reducing environment interactions and learning general skills using intrinsic rewards. However, distractors in observations can severely affect intrinsic reward estimation, leading to a biased exploration process, especially in environments with visual inputs like images or videos. To address this challenge, we propose a bi-level optimization framework named Separation-assisted eXplorer (SeeX). In the inner optimization, SeeX trains a separated world model to extract exogenous and endogenous information, minimizing uncertainty to ensure task relevance. In the outer optimization, it learns a policy on imaginary trajectories generated within the endogenous state space to maximize task-relevant uncertainty. Evaluations on multiple locomotion and manipulation tasks demonstrate SeeX's effectiveness.","['unsupervised RL', 'separate world model', 'visual inputs with distractors', 'minimax optimization']",[],"['Kaichen Huang', 'Shenghua Wan', 'Minghao Shao', 'Hai-Hang Sun', 'Le Gan', 'Shuai Feng', 'De-Chuan Zhan']","['School of Artificial Intelligence, nanjing university', '', 'School of Artificial Intelligence, Nanjing University', 'School of Artificial Intelligence, nanjing university', 'Nanjing University', 'School of Cyberspace Science and Technology, Beijing Institute of Technology', 'School of AI, Nanjing University']",
https://openreview.net/forum?id=NhqZpst42I,Transparency & Explainability,Understanding Visual Feature Reliance through the Lens of Complexity,"Recent studies suggest that deep learning models' inductive bias towards favoring simpler features may be an origin of shortcut learning. Yet, there has been limited focus on understanding the complexities of the myriad features that models learn. In this work, we introduce a new metric for quantifying feature complexity, based on V-information and capturing whether a feature requires complex computational transformations to be extracted. Using this V-information metric, we analyze the complexities of 10,000 features—represented as directions in the penultimate layer—that were extracted from a standard ImageNet-trained vision model. Our study addresses four key questions:  First, we ask what features look like as a function of complexity, and find a spectrum of simple-to-complex features present within the model. Second, we ask when features are learned during training. We find that simpler features dominate early in training, and more complex features emerge gradually. Third, we investigate where within the network simple and complex features ""flow,"" and find that simpler features tend to bypass the visual hierarchy via residual connections. Fourth, we explore the connection between features' complexity and their importance for driving the network's decision. We find that complex features tend to be less important. Surprisingly, important features become accessible at earlier layers during training, like a ""sedimentation process,"" allowing the model to build upon these foundational elements.",['Explainability'],[],"['Thomas FEL', 'Louis Béthune', 'Andrew Kyle Lampinen', 'Thomas Serre', 'Katherine Hermann']","['Kempner Institute, Harvard University', 'Machine Learning Research, Apple', 'Google DeepMind', 'Brown University', 'Google']",
https://openreview.net/forum?id=Jf40H5pRW0,Privacy & Data Governance,Open LLMs are Necessary for Current Private Adaptations and Outperform their Closed Alternatives,"While open Large Language Models (LLMs) have made significant progress, they still fall short of matching the performance of their closed, proprietary counterparts, making the latter attractive even for the use on highly *private* data.  Recently, various new methods have been proposed to adapt closed LLMs to private data without leaking private information to third parties and/or the LLM provider.  In this work, we analyze the privacy protection and performance of the four most recent methods for private adaptation of closed LLMs.  By examining their threat models and thoroughly comparing their performance under different privacy levels according to differential privacy (DP), various LLM architectures, and multiple datasets for classification and generation tasks, we find that: (1) all the methods leak query data, i.e., the (potentially sensitive) user data that is queried at inference time, to the LLM provider, (2) three out of four methods also leak large fractions of private training data to the LLM provider while the method that protects private data requires a local open LLM, (3) all the methods exhibit lower performance compared to three private gradient-based adaptation methods for *local open LLMs*, and (4) the private adaptation methods for closed LLMs incur higher monetary training and query costs than running the alternative methods on local open LLMs. This yields the conclusion that, to achieve truly *privacy-preserving LLM adaptations* that yield high performance and more privacy at lower costs, taking into account current methods and models, one should use open LLMs.","['LLM', 'privacy', 'differential privacy', 'dpsgd', 'LoRA', 'private fine-tuning', 'PromptPATE', 'PromptDPSGD', 'Adaptations', 'soft prompt', 'prefix tuning', 'hard prompts']",[],"['Vincent Hanke', 'Tom Blanchard', 'Franziska Boenisch', 'Iyiola Emmanuel Olatunji', 'Michael Backes', 'Adam Dziedzic']","['CISPA Helmholtz Center for Information Security', 'CISPA Helmholtz Center for Information Security', '', 'CISPA Helmholtz Center for Information Security', '', '']",
https://openreview.net/forum?id=GproaSYZk5,Transparency & Explainability,Universal In-Context Approximation By Prompting Fully Recurrent Models,"Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve be universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.","['prompting', 'universal approximation', 'in-context learning', 'recurrent models', 'rnn', 'ssm']",[],"['Aleksandar Petrov', 'Tom A. Lamb', 'Alasdair Paren', 'Philip Torr', 'Adel Bibi']","['Google DeepMind', 'Engineering Science, University of Oxford', 'Engineering Science, University of Oxford', 'University of Oxford', 'Engineering Science, University of Oxford']",
https://openreview.net/forum?id=E7fZOoiEKl,Transparency & Explainability,FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion,"One-shot Federated Learning (OFL) significantly reduces communication costs in FL by aggregating trained models only once. However, the performance of advanced OFL methods is far behind the normal FL. In this work, we provide a causal view to find that this performance drop of OFL methods comes from the isolation problem, which means that local isolatedly trained models in OFL may easily fit to spurious correlations due to the data heterogeneity. From the causal perspective, we observe that the spurious fitting can be alleviated by augmenting intermediate features from other clients. Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL. Specifically, FuseFL decomposes neural networks into several blocks, and progressively trains and fuses each block following a bottom-up manner for feature augmentation, introducing no additional communication costs. Comprehensive experiments demonstrate that FuseFL outperforms existing OFL and ensemble FL by a significant margin. We conduct comprehensive experiments to show that FuseFL supports high scalability of clients, heterogeneous model training, and low memory costs. Our work is the first attempt using causality to analyze and alleviate data heterogeneity of OFL.","['Federated Learning', 'communication efficiency', 'causality']",[],"['Zhenheng Tang', 'Yonggang Zhang', 'Peijie Dong', 'Yiu-ming Cheung', 'Amelie Chi Zhou', 'Bo Han', 'Xiaowen Chu']","['The Hong Kong University of Science and Technology', 'Hong Kong Baptist University', 'The Hong Kong University of Science and Technology (Guang Zhou)', '', 'Department of Computer Science, Hong Kong Baptist University', 'Department of Computer Science, HKBU', 'Data Science and Analytics, Hong Kong University of Science and Technology (Guangzhou)']",
https://openreview.net/forum?id=Actjv6Wect,Transparency & Explainability,Proportional Fairness in Non-Centroid Clustering,"We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points that are large and cohesive. Prior work applies this framework to centroid-based clustering, where points are partitioned into clusters, and the cost to each data point is measured by its distance to a centroid assigned to its cluster. However, real-life applications often do not require such centroids. We extend the theory of proportionally fair clustering to non-centroid clustering by considering a variety of cost functions, both metric and non-metric, for a data point to be placed in a cluster with other data points. Our results indicate that Greedy Capture, a clustering algorithm developed for centroid clustering, continues to provide strong proportional fairness guarantees for non-centroid clustering, although the guarantees are significantly different and establishing them requires novel proof ideas. We also design algorithms for auditing proportional fairness of a given clustering solution. We conduct experiments on real data which suggest that traditional clustering algorithms are highly unfair, while our algorithms achieve strong fairness guarantees with a moderate loss in common clustering objectives.","['clustering', 'proportional fairness', 'core', 'fully justified representation', 'auditing']",[],"['Ioannis Caragiannis', 'Evi Micha', 'Nisarg Shah']","['Aarhus University', 'Computer Science, University of Southern California', 'Computer Science, University of Toronto']",
https://openreview.net/forum?id=8hBc843g1p,Security,Improved Generation of Adversarial Examples Against Safety-aligned LLMs,"Adversarial prompts (or say, adversarial examples) generated using gradient-based methods exhibit outstanding performance in performing automatic jailbreak attacks against safety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the *white-box* setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking *black-box* image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, *i.e.*, Skip Gradient Method and Intermediate Level Attack, into gradient-based adversarial prompt generation and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that 87% of the query-specific adversarial suffixes generated by the developed combination can induce Llama-2-7B-Chat to produce the output that exactly matches the target string on AdvBench. This match rate is 33% higher than that of a very strong baseline known as GCG, demonstrating advanced discrete optimization for adversarial prompt generation against LLMs. In addition, without introducing obvious cost, the combination achieves >30% absolute increase in attack success rates compared with GCG when generating both query-specific (38% ->68%) and universal adversarial prompts (26.68% -> 60.32%) for attacking the Llama-2-7B-Chat model on AdvBench. Code at: https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.","['large language model', 'adversarial attack', 'jailbreak attack']",[],"['Qizhang Li', 'Yiwen Guo', 'Wangmeng Zuo', 'Hao Chen']","['Harbin Institute of Technology', '', 'Harbin Institute of Technology', 'University of California, Davis']",
https://openreview.net/forum?id=7FokMz6U8n,Fairness & Bias,Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data,"One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs $(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to ""connect the dots"" without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.","['NLP', 'LLM', 'GPT', 'generalization', 'out-of-context reasoning', 'capabilities', 'fine-tuning']",[],"['Johannes Treutlein', 'Dami Choi', 'Jan Betley', 'Samuel Marks', 'Cem Anil', 'Roger Baker Grosse', 'Owain Evans']","['', 'Department of Computer Science, University of Toronto', ""Owain Evans' research group"", 'Northeastern University', 'Research, Anthropic', 'Department of Computer Science, University of Toronto', 'Truthful AI']",
https://openreview.net/forum?id=7b2DrIBGZz,Fairness & Bias,Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models,"Large language models based on decoder-only transformers have demonstrated superior text understanding capabilities compared to CLIP and T5-series models. However, the paradigm for utilizing current advanced LLMs in text-to-image diffusion models remains to be explored. We observed an unusual phenomenon: directly using a large language model as the prompt encoder significantly degrades the prompt-following ability in image generation. We identified two main obstacles behind this issue. One is the misalignment between the next token prediction training in LLM and the requirement for discriminative prompt features in diffusion models. The other is the intrinsic positional bias introduced by the decoder-only architecture. To deal with this issue, we propose a novel framework to fully harness the capabilities of LLMs. Through the carefully designed usage guidance, we effectively enhance the text representation capability of the LLM for prompt encoding and eliminate its inherent positional bias. This allows us to flexibly integrate state-of-the-art LLMs into the text-to-image generation model. Furthermore, we also provide an effective manner to fuse multiple LLMs into our framework. Considering the excellent performance and scaling capabilities demonstrated by the transformer architecture, we further design an LLM-Infused Diffusion Transformer (LI-DIT)based on the framework. We conduct extensive experiments to validate LI-DIT across model size and data size. Benefiting from the inherent ability of the LLMs and our innovative designs, the prompt understanding performance of  LI-DIT easily surpasses state-of-the-art open-source models as well as mainstream closed-source commercial models including Stable Diffusion 3, DALL-E 3, and Midjourney V6.",['large language model; text-to-image generation; diffusion model'],[],"['Bingqi Ma', 'Zhuofan Zong', 'Guanglu Song', 'Hongsheng Li', 'Yu Liu']","['Sensetime Group Limmited', 'The Chinese University of Hong Kong', 'Sensetime', 'The Chinese University of Hong Kong', 'SenseTime']",
https://openreview.net/forum?id=7CMNSqsZJt,Security,ContextCite: Attributing Model Generation to Context,"How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of *context attribution*: pinpointing the parts of the context (if any) that *led* a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for ContextCite at https://github.com/MadryLab/context-cite.","['attribution', 'citation', 'generative models', 'large language models']",[],"['Benjamin Cohen-Wang', 'Harshay Shah', 'Kristian Georgiev', 'Aleksander Madry']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",
https://openreview.net/forum?id=3NaqGg92KZ,Fairness & Bias,Training Data Attribution via Approximate Unrolling,"Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set. Methods based on implicit differentiation, such as influence functions, can be made computationally efficient, but fail to account for underspecification, the implicit bias of the optimization algorithm, or multi-stage training pipelines. By contrast, methods based on unrolling address these issues but face scalability challenges. In this work, we connect the implicit-differentiation-based and unrolling-based approaches and combine their benefits by introducing Source, an approximate unrolling-based TDA method that is computed using an influence-function-like formula. While being computationally efficient compared to unrolling-based approaches, Source is suitable in cases where implicit-differentiation-based approaches struggle, such as in non-converged models and multi-stage training pipelines. Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short.","['training data attribution', 'influence functions']",[],"['Juhan Bae', 'Wu Lin', 'Jonathan Lorraine', 'Roger Baker Grosse']","['Department of Computer Science, University of Toronto', 'Vector Institute', 'NVIDIA', 'Department of Computer Science, University of Toronto']",
https://openreview.net/forum?id=T9PfJViMiJ,Transparency & Explainability,HHD-GP: Incorporating Helmholtz-Hodge Decomposition into Gaussian Processes for Learning Dynamical Systems,"Machine learning models provide alternatives for efficiently recognizing complex patterns from data, but the main concern in applying them to modeling physical systems stems from their physics-agnostic design, leading to learning methods that lack interpretability, robustness, and data efficiency. This paper mitigates this concern by incorporating the Helmholtz-Hodge decomposition into a Gaussian process model, leading to a versatile framework that simultaneously learns the curl-free and divergence-free components of a dynamical system. Learning a predictive model in this form facilitates the exploitation of symmetry priors. In addition to improving predictive power, these priors make the model indentifiable, thus the identified features can be linked to comprehensible scientific properties of the system. We show that compared to baseline models, our model achieves better predictive performance on several benchmark dynamical systems while allowing physically meaningful decomposition of the systems from noisy and sparse data.","['Gaussian process', 'Helmholtz-Hodge decomposition', 'dynamical system']",[],"['Hao Xu', 'Jia Pan']","['University of Hong Kong', 'Computer Science, University of Hong Kong']",
https://openreview.net/forum?id=3uUIwMxYbR,Privacy & Data Governance,Revisiting Differentially Private ReLU Regression,"As one of the most fundamental non-convex learning problems, ReLU regression under differential privacy (DP) constraints, especially in high-dimensional settings, remains a challenging area in privacy-preserving machine learning. Existing results are limited to the assumptions of bounded norm $ \|\mathbf{x}\|_2 \leq 1$, which becomes meaningless with increasing data dimensionality. In this work, we revisit the problem of DP ReLU regression in high-dimensional regimes. We propose two innovative algorithms DP-GLMtron and DP-TAGLMtron that outperform the conventional DPSGD.  DP-GLMtron is based on a generalized linear model perceptron approach, integrating adaptive clipping and Gaussian mechanism for enhanced privacy. To overcome the constraints of small privacy budgets in DP-GLMtron, represented by $\widetilde{O}(\sqrt{1/N})$ where $N$ is the sample size, we introduce DP-TAGLMtron, which utilizes a tree aggregation protocol to balance privacy and utility effectively, showing that DP-TAGLMtron achieves comparable performance with only an additional factor of $O(\log N)$ in the utility upper bound. Moreover, our theoretical analysis extends beyond Gaussian-like data distributions to settings with eigenvalue decay, showing how data distribution impacts learning in high dimensions. Notably, our findings suggest that the utility upper bound could be independent of the dimension $d$, even when $d \gg N$.  Experiments on synthetic and real-world datasets also validate our results.","['Differential Privacy', 'Generalized Linear Model']",[],"['Meng Ding', 'Mingxi Lei', 'Liyang Zhu', 'Shaowei Wang', 'Di Wang', 'Jinhui Xu']","['State University of New York at Buffalo', 'State University of New York at Buffalo', 'Duke University', 'Guangzhou University', '', 'Computer Science and Engineering, University at Buffalo, State University of New York']",
https://openreview.net/forum?id=YrAxxscKM2,Fairness & Bias,Why Do We Need Weight Decay in Modern Deep Learning?,"Weight decay is a broadly used technique for training state-of-the-art deep networks from image classification to large language models. Despite its widespread usage and being extensively studied in the classical literature, its role remains poorly understood for deep learning. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For deep networks on vision tasks trained with multipass SGD, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the *loss stabilization mechanism*. In contrast, for large language models trained with nearly one-epoch training, we describe how weight decay balances the *bias-variance tradeoff* in stochastic optimization leading to lower training loss and improved training stability.  Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way.","['Weight decay', 'overparameterization', 'implicit regularization', 'large language models', 'optimization dynamics']",[],"[""Francesco D'Angelo"", 'Maksym Andriushchenko', 'Aditya Varre', 'Nicolas Flammarion']","['EPFL - EPF Lausanne', 'EPFL - EPF Lausanne', 'EPFL - EPF Lausanne', 'IC, Swiss Federal Institute of Technology Lausanne']",
https://openreview.net/forum?id=XgwTH95kCl,Security,Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning,"Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model’s performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.","['Human intention understanding', 'modality missing']",[],"['Mingcheng Li', 'Dingkang Yang', 'Yang Liu', 'Shunli Wang', 'Jiawei Chen', 'Shuaibing Wang', 'Jinjie Wei', 'Yue Jiang', 'Qingyao Xu', 'Xiaolu Hou', 'Mingyang Sun', 'Ziyun Qian', 'Dongliang Kou', 'Lihua Zhang']","['Fudan University', 'Academy for Engineering and Technolog, Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University', 'Fudan University, Fudan University', 'Fudan University, Fudan University', 'Fudan University', 'Academy for Engineering and Technology, Fudan University', 'Fudan University', 'Fudan University, Fudan University', 'Academy for Engineering and Technology, Fudan University']",
https://openreview.net/forum?id=cR2QDzdpEv,Fairness & Bias,Robust Reinforcement Learning from Corrupted Human Feedback,"Reinforcement learning from human feedback (RLHF) provides a principled framework for aligning AI systems with human preference data. For various reasons, e.g., personal bias, context ambiguity, lack of training, etc, human annotators may give incorrect or inconsistent preference labels.  To tackle this challenge, we propose a robust RLHF approach -- $R^3M$, which models the potentially corrupted preference label as sparse outliers. Accordingly, we formulate the robust reward learning as an $\ell_1$-regularized maximum likelihood estimation problem. Computationally, we develop an efficient alternating optimization algorithm, which only incurs negligible computational overhead compared with the standard RLHF approach. Theoretically, we prove that under proper regularity conditions, $R^3M$ can consistently learn the underlying reward and identify outliers, provided that the number of outlier labels scales sublinearly with the preference sample size. Furthermore, we remark that $R^3M$ is versatile and can be extended to various preference optimization methods, including direct preference optimization (DPO). Our experiments on robotic control and natural language generation with large language models (LLMs) show that $R^3M$ improves robustness of the reward  against several types of perturbations to the preference data.","['Reinforcement Learning from Human Feedback', 'Robust Reward Learning']",[],"['Alexander Bukharin', 'Ilgee Hong', 'Haoming Jiang', 'Zichong Li', 'Qingru Zhang', 'Zixuan Zhang', 'Tuo Zhao']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Amazon', 'Isye, Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']",
https://openreview.net/forum?id=bSv0MBDBF2,Transparency & Explainability,Denoising Diffusion Path: Attribution Noise Reduction with An Auxiliary Diffusion Model,"The explainability of deep neural networks (DNNs) is critical for trust and reliability in AI systems. Path-based attribution methods, such as integrated gradients (IG), aim to explain predictions by accumulating gradients along a path from a baseline to the target image. However, noise accumulated during this process can significantly distort the explanation. While existing methods primarily concentrate on finding alternative paths to circumvent noise, they overlook a critical issue: intermediate-step images frequently diverge from the distribution of training data, further intensifying the impact of noise. This work presents a novel Denoising Diffusion Path (DDPath) to tackle this challenge by harnessing the power of diffusionmodels for denoising. By exploiting the inherent ability of diffusion models to progressively remove noise from an image, DDPath constructs a piece-wise linear path. Each segment of this path ensures that samples drawn from a Gaussian distribution are centered around the target image. This approach facilitates a gradual reduction of noise along the path. We further demonstrate that DDPath adheres to essential axiomatic properties for attribution methods and can be seamlessly integrated with existing methods such as IG. Extensive experimental results demonstrate that DDPath can significantly reduce noise in the attributions—resulting in clearer explanations—and achieves better quantitative results than traditional path-based methods.","['Diffusion Models', 'Interpretability', 'Integrated Gradients', 'Path Integration']",[],"['Yiming Lei', 'Zilong Li', 'Junping Zhang', 'Hongming Shan']","['Fudan University', 'Fudan University', 'School of Computer Science, Fudan University', 'Fudan University']",
https://openreview.net/forum?id=Mtsi1eDdbH,Fairness & Bias,A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems,"Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using a d-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS, Hatespeech, and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of learn-to-defer baselines and can control multiple constraint violations at once. The use of d-GNP is beyond learn-to-defer applications and can potentially obtain a solution to decision-making problems with a set of controlled expected performance measures.","['Learn-to-Defer', 'Human-AI Teaming', 'Neyman-Pearson Lemma', 'Hypothesis Testing', 'Bayes Optimal', 'PAC Generalization']",[],"['Mohammad-Amin Charusaie', 'Samira Samadi']","['Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute']",
https://openreview.net/forum?id=mYEjc7qGRA,Transparency & Explainability,Towards Robust Multimodal Sentiment Analysis with Incomplete Data,"The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets (e.g., MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics.","['Multimodal Sentiment Analysis', 'Multimodal Learning', 'Robust MSA']",[],"['Haoyu Zhang', 'Wenbin Wang', 'Tianshu Yu']","['', 'School of Computer Science, Wuhan University', 'School of Data Science, Chinese University of Hong Kong (Shenzhen)']",
https://openreview.net/forum?id=oTv6Qa12G0,Fairness & Bias,A theoretical design of concept sets: improving the predictability of concept bottleneck models,"Concept-based learning, a promising approach in machine learning, emphasizes the value of high-level representations called concepts. However, despite growing interest in concept-bottleneck models (CBMs), there is a lack of clear understanding regarding the properties of concept sets and their impact on model performance. In this work, we define concepts within the machine learning context, highlighting their core properties: 'expressiveness' and 'model-aware inductive bias', and we make explicit the underlying assumption of CBMs. We establish theoretical results for concept-bottleneck models (CBMs), revealing how these properties guide the design of concept sets that optimize model performance. Specifically, we demonstrate that well-chosen concept sets can improve sample efficiency and out-of-distribution robustness in the appropriate regimes. Based on these insights, we propose a method to effectively identify informative and non-redundant concepts. We validate our approach with experiments on CIFAR-10 and MetaShift, showing that concept-bottleneck models outperform the foundational embedding counterpart, particularly in low-data regimes and under distribution shifts. We also examine failure modes and discuss how they can be tackled.","['Representation Learning', 'Embedding Approaches', 'Learning Theory', 'Neural Abstract Machines', 'Nonlinear Dimensionality Reduction and Manifold Learning', 'One-Shot/Low-Shot Learning Approaches']",[],"['Max Ruiz Luyten', 'Mihaela van der Schaar']","['Mathematics, University of Cambridge', 'University of Cambridge']",
https://openreview.net/forum?id=oYyEsVz6DX,Transparency & Explainability,Measuring Per-Unit Interpretability at Scale Without Humans,"In today’s era, whatever we can measure at scale, we can optimize. So far, measuring the interpretability of units in deep neural networks (DNNs) for computer vision still requires direct human evaluation and is not scalable. As a result, the inner workings of DNNs remain a mystery despite the remarkable progress we have seen in their applications. In this work, we introduce the first scalable method to measure the per-unit interpretability in vision DNNs. This method does not require any human evaluations, yet its prediction correlates well with existing human interpretability measurements. We validate its predictive power through an interventional human psychophysics study. We demonstrate the usefulness of this measure by performing previously infeasible experiments: (1) A large-scale interpretability analysis across more than 70 million units from 835 computer vision models, and (2) an extensive analysis of how units transform during training. We find an anticorrelation between a model's downstream classification performance and per-unit interpretability, which is also observable during model training. Furthermore, we see that a layer's location and width influence its interpretability.","['interpretability', 'explainability', 'deep learning', 'neural networks', 'analysis', 'activation maximization', 'alignment', 'evaluation']",[],"['Roland S. Zimmermann', 'David Klindt', 'Wieland Brendel']","['Google DeepMind', '', 'ELLIS Institute Tübingen']",
https://openreview.net/forum?id=wm9JZq7RCe,Transparency & Explainability,An Analysis of Tokenization: Transformers under Markov Data,"While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al. 2022, Xue et al. 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple $k^{\text{th}}$-order Markov processes for $k > 1$, transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically are incredibly slow or fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al. 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\text{th}}$-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.","['Tokenization', 'LLMs', 'interpretability']",[],"['Nived Rajaraman', 'Jiantao Jiao', 'Kannan Ramchandran']","['University of California Berkeley', 'University of California Berkeley', 'EECS, University of California, Berkeley']",
https://openreview.net/forum?id=xW6ga9i4eA,Security,pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning,"Federated learning, a pioneering paradigm, enables collaborative model training without exposing users’ data to central servers. Most existing federated learning systems necessitate uniform model structures across all clients, restricting their practicality. Several methods have emerged to aggregate diverse client models; however, they either lack the ability of personalization, raise privacy and security concerns, need prior knowledge, or ignore the capability and functionality of personalized models. In this paper, we present an innovative approach, named pFedClub, which addresses these challenges. pFedClub introduces personalized federated learning through the substitution of controllable neural network blocks/layers. Initially, pFedClub dissects heterogeneous client models into blocks and organizes them into functional groups on the server. Utilizing the designed CMSR (Controllable Model Searching and Reproduction) algorithm, pFedClub generates a range of personalized candidate models for each client. A model matching technique is then applied to select the optimal personalized model, serving as a teacher model to guide each client’s training process. We conducted extensive experiments across three datasets, examining both IID and non-IID settings. The results demonstrate that pFedClub outperforms baseline approaches, achieving state-of-the-art performance. Moreover, our model insight analysis reveals that pFedClub generates personalized models of reasonable size in a controllable manner, significantly reducing computational costs.",['Heterogenous federated learning'],[],"['Jiaqi Wang', 'Qi Li', 'Lingjuan Lyu', 'Fenglong Ma']","['', 'Iowa State University', 'Sony Research, Sony', 'College of Information Sciences and Technology, Pennsylvania State University']",
https://openreview.net/forum?id=xqrlhsbcwN,Transparency & Explainability,Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient,"Neural networks (NN) are extensively studied in cutting-edge soft sensor models due to their  feature extraction and function approximation capabilities. Current research into network-based methods primarily focuses on models' offline accuracy. Notably, in industrial soft sensor context, online optimizing stability and interpretability are prioritized, followed by accuracy. This requires a clearer understanding of network's training process. To bridge this gap, we propose a novel NN named the Approximated Orthogonal Projection Unit (AOPU) which has solid mathematical basis and presents superior training stability. AOPU truncates the gradient backpropagation at dual parameters, optimizes the trackable parameters updates, and enhances the robustness of training. We further prove that AOPU attains minimum variance estimation in NN, wherein the truncated gradient approximates the natural gradient. Empirical results on two chemical process datasets clearly show that AOPU outperforms other models in achieving stable convergence, marking a significant advancement in soft sensor field.","['Neural networks', ""network's structure design"", 'minimum variance estimation', 'online learning', 'training stability', 'natural gradient', 'soft sensor']",[],"['ShaoQi Wang', 'Chunjie Yang', 'Siwei Lou']","['Control Science and Engineering, Zhejiang University', 'Zhejiang University', 'Zhejiang University']",
https://openreview.net/forum?id=95VyH4VxN9,Security,Autonomous Driving with Spiking Neural Networks,"Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability. We present Spiking Autonomous Driving (SAD), the first unified Spiking Neural Network (SNN) to address the energy challenges faced by autonomous driving systems through its event-driven and energy-efficient nature. SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort. Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs. This work highlights the potential of neuromorphic computing to be applied to energy-efficient autonomous driving, a critical step toward sustainable and safety-critical automotive technology. Our code is available at [https://github.com/ridgerchu/SAD](https://github.com/ridgerchu/SAD).","['Spiking Neural Networks', 'Neuromorphic Computing', 'Brain-inspired Computing']",[],"['Rui-Jie Zhu', 'Ziqing Wang', 'Leilani H. Gilpin', 'Jason Eshraghian']","['University of California, Santa Cruz', 'Northwestern University', 'University of California, Santa Cruz', 'University of California, Santa Cruz']",
https://openreview.net/forum?id=SKY1ScUTwA,Transparency & Explainability,The Intelligible and Effective Graph Neural Additive Network,"Graph Neural Networks (GNNs) have emerged as the predominant approach for learning over graph-structured data.  However, most GNNs operate as black-box models and require post-hoc explanations, which may not suffice in high-stakes scenarios where transparency is crucial. In this paper, we present a GNN that is interpretable by design.  Our model,  Graph Neural Additive Network (GNAN), is a novel extension of the interpretable class of Generalized Additive Models,  and can be visualized and fully understood by humans. GNAN is designed to be fully interpretable, offering both global and local explanations at the feature and graph levels through direct visualization of the model. These visualizations describe exactly how the model uses the relationships between the target variable, the features, and the graph. We demonstrate the intelligibility of GNANs in a series of examples on different tasks and datasets. In addition, we show that the accuracy of GNAN is on par with black-box GNNs, making it suitable for critical applications where transparency is essential, alongside high accuracy.","['Explainable AI', 'Graph Neural Networks', 'Graph Learning', 'Interpretability.']",[],"['Maya Bechler-Speicher', 'Amir Globerson', 'Ran Gilad-Bachrach']","['Tel Aviv University', 'Tel Aviv University', 'Tel Aviv University']",
https://openreview.net/forum?id=rle9X7DQuH,Fairness & Bias,OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning,"Semi-supervised learning (SSL) offers a robust framework for harnessing the potential of unannotated data. Traditionally, SSL mandates that all classes possess labeled instances. However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes. This scenario leads to misclassification of unseen classes as known ones, consequently undermining classification accuracy. To overcome this challenge, this study revisits two methodologies from self-supervised and semi-supervised learning, self-labeling and consistency, tailoring them to address the OwSSL problem. Specifically, we propose an effective framework called _OwMatch_, combining conditional self-labeling and open-world hierarchical thresholding.  Theoretically, we analyze the estimation of class distribution on unlabeled data through rigorous statistical analysis, thus demonstrating that OwMatch can ensure the unbiasedness of the label assignment estimator with reliability.  Comprehensive empirical analyses demonstrate that our method yields substantial performance enhancements across both known and unknown classes in comparison to previous studies. Code is available at [https://github.com/niusj03/OwMatch](https://github.com/niusj03/OwMatch).","['Open-world Semi-Supervised Learning', 'self-labeling', 'consistency loss']",[],"['Shengjie Niu', 'Lifan Lin', 'Jian Huang', 'Chao Wang']","['Hong Kong Polytechnic University', 'Department of Statistics and Data Science, Southern University of Science and Technology', 'Department of Data Science and AI, Hong Kong Polytechnic University', 'Southern University of Science and Technology']",
https://openreview.net/forum?id=R8SolCx62K,Fairness & Bias,Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering,"Graph Contrastive Learning (GCL) has emerged as a powerful approach for generating graph representations without the need for manual annotation. Most advanced GCL methods fall into three main frameworks: node discrimination, group discrimination, and bootstrapping schemes, all of which achieve comparable performance. However, the underlying mechanisms and factors that contribute to their effectiveness are not yet fully understood. In this paper, we revisit these frameworks and reveal a common mechanism—representation scattering—that significantly enhances their performance. Our discovery highlights an essential feature of GCL and unifies these seemingly disparate methods under the concept of representation scattering. To leverage this insight, we introduce Scattering Graph Representation Learning (SGRL), a novel framework that incorporates a new representation scattering mechanism designed to enhance representation diversity through a center-away strategy. Additionally, consider the interconnected nature of graphs, we develop a topology-based constraint  mechanism that integrates graph structural properties with representation scattering to prevent excessive scattering. We extensively evaluate SGRL across various downstream tasks on benchmark datasets, demonstrating its efficacy and superiority over existing GCL methods. Our findings underscore the significance of representation scattering in GCL and provide a structured framework for harnessing this mechanism to advance graph representation learning. The code of SGRL is at https://github.com/hedongxiao-tju/SGRL.",['Graph Contrastive Learning'],[],"['Dongxiao He', 'Lianze Shan', 'Jitao Zhao', 'Hengrui Zhang', 'Zhen Wang', 'Weixiong Zhang']","['Computer Science, Tianjin University', 'Tianjin University', 'College of Intelligence and Computing, Tianjin University', 'University of Illinois, Chicago', 'school of cybersecurity, Northwestern Polytechnical University', 'Washington University, Saint Louis']",
https://openreview.net/forum?id=YYJojVBCcd,Fairness & Bias,Fairness without Harm: An Influence-Guided Active Sampling Approach,"The pursuit of fairness in machine learning (ML), ensuring that the models do not exhibit biases toward protected demographic groups, typically results in a compromise scenario. This compromise can be explained by a Pareto frontier where given certain resources (e.g., data), reducing the fairness violations often comes at the cost of lowering the model accuracy.  In this work, we aim to train models that mitigate group fairness disparity without causing harm to model accuracy. Intuitively, acquiring more data is a natural and promising approach to achieve this goal by reaching a better Pareto frontier of the fairness-accuracy tradeoff. The current data acquisition methods, such as fair active learning approaches, typically require annotating sensitive attributes. However, these sensitive attribute annotations should be protected due to privacy and safety concerns. In this paper, we propose a tractable active data sampling algorithm that does not rely on training group annotations, instead only requiring group annotations on a small validation set. Specifically, the algorithm first scores each new example by its influence on fairness and accuracy evaluated on the validation dataset, and then selects a certain number of examples for training.  We theoretically analyze how acquiring more data can improve fairness without causing harm, and validate the possibility of our sampling approach in the context of risk disparity. We also provide the upper bound of generalization error and risk disparity as well as the corresponding connections. Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm. Our code is available at [github.com/UCSC-REAL/FairnessWithoutHarm](https://github.com/UCSC-REAL/FairnessWithoutHarm).",['Fairness; Sampling'],[],"['Jinlong Pang', 'Jialu Wang', 'Zhaowei Zhu', 'Yuanshun Yao', 'Chen Qian', 'Yang Liu']","['', 'University of California, Santa Cruz', 'Docta.ai', 'Meta GenAI', 'University of California-Santa Cruz', 'Computer Science and Engineering, University of California, Santa Cruz']",
https://openreview.net/forum?id=8Dy42ThoNe,Security,Large Language Model Unlearning,"We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in the standard alignment process. (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",['Large Language Model; LLM Alignment; Machine Unlearning; AI Privacy; AI Security'],[],"['Yuanshun Yao', 'Xiaojun Xu', 'Yang Liu']","['Meta GenAI', 'ByteDance Inc.', 'Computer Science and Engineering, University of California, Santa Cruz']",