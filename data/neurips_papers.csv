link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://openreview.net/forum?id=nkzSE5KkCA,Fairness & Bias,Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning,"Despite advancements in Text-to-Video (T2V) generation, producing videos with realistic motion remains challenging. Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text. This issue stems from the internal biases in text encoding which overlooks motions, and inadequate conditioning mechanisms in T2V generation models. To address this, we propose a novel framework called DEcomposed MOtion (DEMO), which enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, alongside separate content and motion conditioning mechanisms. Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Evaluations on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior ability to produce videos with enhanced motion dynamics while maintaining high visual quality. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions. Project page: https://PR-Ryan.github.io/DEMO-project/","['Text-to-Video Generation', 'Diffusion Models']",,"['Penghui Ruan', 'Pichao WANG', 'Divya Saxena', 'Jiannong Cao', 'Yuhui Shi']","['Department of Computing, Hong Kong Polytechnic University', 'Amazon', 'The Hong Kong Polytechnic University, Hong Kong Polytechnic University', 'Department of Computing, Hong Kong Polytechnic University', 'Southern University of Science and Technology']",
https://openreview.net/forum?id=Oo7dlLgqQX,Fairness & Bias,Questioning the Survey Responses of Large Language Models,"Surveys have recently gained popularity as a tool to study large language models. By comparing models’ survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter “A”. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.","['large language models', 'surveys']",,"['Ricardo Dominguez-Olmedo', 'Moritz Hardt', 'Celestine Mendler-Dünner']","['Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'ELLIS Institute Tübingen']",
https://openreview.net/forum?id=GQNvvQquO0,Privacy & Data Governance,Differentially Private Set Representations,"We study the problem of differentially private (DP) mechanisms for representing sets of size $k$ from a large universe. Our first construction creates $(\epsilon,\delta)$-DP representations with error probability of $1/(e^\epsilon + 1)$ using space at most $1.05 k \epsilon \cdot \log(e)$ bits where the time to construct a representation is $O(k \log(1/\delta))$ while decoding time is $O(\log(1/\delta))$. We also present a second algorithm for pure $\epsilon$-DP representations with the same error using space at most $k \epsilon \cdot \log(e)$ bits, but requiring large decoding times. Our algorithms match the lower bounds on privacy-utility trade-offs (including constants but ignoring $\delta$ factors) and we also present a new space lower bound matching our constructions up to small constant factors. To obtain our results, we design a new approach embedding sets into random linear systems deviating from most prior approaches that inject noise into non-private solutions.","['Differential Privacy', 'Data Structure']",,"['Sarvar Patel', 'Giuseppe Persiano', 'Joon Young Seo', 'Kevin Yeo']","['Google', 'University of Salerno', 'Google', 'Google']",
