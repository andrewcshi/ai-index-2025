link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/1809.06061,Transparency & Explainability,Transparency and Explanation in Deep Reinforcement Learning Neural Networks,"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.",[],[],"['Rahul Iyer', 'Yuezhang Li', 'Huao Li', 'Michael Lewis', 'Ramitha Sundar', 'Katia Sycara']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Google Inc., Mountain View, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'University of Pittsburgh, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1712.08197,Fairness & Bias,Fair Forests: Regularized Tree Induction to Minimize Model Bias,"The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees. We show that our ""Fair Forest"" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both ""group fairness"" and ""individual fairness.'"" We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.",[],[],"['Edward Raff', 'Jared Sylvester', 'Steven Mills']",[],[]
https://arxiv.org/abs/1711.09050,Fairness & Bias,Ethical Challenges in Data-Driven Dialogue Systems,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",[],[],"['Peter Henderson', 'Koustuv Sinha', 'Nicolas Angelard-Gontier', 'Nan Rosemary Ke', 'Genevieve Fried', 'Ryan Lowe', 'Joelle Pineau']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Polytechnique Montréal, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://arxiv.org/abs/1801.07593,Fairness & Bias,Mitigating Unwanted Biases with Adversarial Learning,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",[],[],"['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']","['Stanford University, Stanford, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1712.00846,Fairness & Bias,Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection,"Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.",[],[],"['Kyle Hundman', 'Thamme Gowda', 'Mayank Kejriwal', 'Benedikt Boecking']",[],[]
https://arxiv.org/abs/1711.07111,Fairness & Bias,Modeling Epistemological Principles for Bias Mitigation in AI Systems: An Illustration in Hiring Decisions,"Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.",[],[],"['Marisa Vasconcelos', 'Carlos Cardonha', 'Bernardo Gonçalves']",[],[]
https://arxiv.org/abs/1803.02852,Security,"Value Alignment, Fair Play, and the Rights of Service Robots","Ethics and safety research in artificial intelligence is increasingly framed in terms of ""alignment"" with human values and interests. I argue that Turing's call for ""fair play for machines"" is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on ""fair play"" motivate a novel interpretation of Turing's notorious ""imitation game"" as a condition not of intelligence but instead of value alignment: a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is ""fair"" in precisely the sense that it encourages an alignment of interests between humans and machines.",[],[],['Daniel Estrada'],[],[]
https://arxiv.org/abs/1711.09050,Security,Ethical Challenges in Data-Driven Dialogue Systems,"The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",[],[],"['Peter Henderson', 'Koustuv Sinha', 'Nicolas Angelard-Gontier', 'Nan Rosemary Ke', 'Genevieve Fried', 'Ryan Lowe', 'Joelle Pineau']","['McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'Polytechnique Montréal, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada', 'McGill University, Montreal, PQ, Canada']","['Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada', 'Canada']"
https://arxiv.org/abs/1801.07593,Security,Mitigating Unwanted Biases with Adversarial Learning,"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",[],[],"['Brian Hu Zhang', 'Blake Lemoine', 'Margaret Mitchell']","['Stanford University, Stanford, CA, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA']","['US', 'US', 'US']"