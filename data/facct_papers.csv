link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://doi.org/10.1145/3630106.3658537,Transparency & Explainability,Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations,"Decision processes of computer vision models—especially deep neural networks—are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are—as the explainability methods themselves—often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.","['eXplainable AI', 'XAI', 'saliency maps', 'saliency metrics', 'heatmaps', 'quantitative evaluation', 'psychometric testing', 'validity', 'reliability', 'objective XAI evaluation']",Computing methodologies → Interest point and salient region detections General and reference → Metrics Computing methodologies → Computer vision Human-centered computing~Human computer interaction (HCI),"['Benjamin Fresz', 'Lena Lörcher', 'Marco Huber']","['Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany']",[]
https://doi.org/10.1145/3630106.3658538,Fairness & Bias,Designing Long-term Group Fair Policies in Dynamical Systems,"Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term—even if fairness considerations were taken into account in the policy design process. In this paper, we propose a novel framework for studying long-term group fairness in dynamical systems, in which current decisions may affect an individual’s features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long-term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov Chain Convergence Theorem to ensure unique convergence. Our framework enables the utilization of historical temporal data to tackle challenges associated with delayed feedback when learning long-term fair policies in practice. Importantly, our framework shows that interventions on the data distribution (e.g., subsidies) can be used to achieve policy learning that is both short- and long-term fair. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. In semi-synthetic simulations based on real-world datasets, we show how our approach facilitates identifying effective interventions for long-term fairness.","['fairness', 'long-term', 'dynamical system', 'equilibrium', 'policy learning']",Computing methodologies → Machine learning Social and professional topics,"['Miriam Rateike', 'Isabel Valera', 'Patrick Forré']","['Saarland University, Germany', 'Saarland University, Germany and Max Planck Institute for Software Systems, Germany', 'AI4Science Lab, AMLab, Informatics Institute, University of Amsterdam, Germany']",[]
https://doi.org/10.1145/3630106.3658539,Fairness & Bias,Learning Fairness from Demonstrations via Inverse Reinforcement Learning,"Defining fairness in algorithmic contexts is challenging, particularly when adapting to new domains. Our research introduces a novel method for learning and applying group fairness preferences across different classification domains, without the need for manual fine-tuning. Utilizing concepts from inverse reinforcement learning (IRL), our approach enables the extraction and application of fairness preferences from human experts or established algorithms. We propose the first technique for using IRL to recover and adapt group fairness preferences to new domains, offering a low-touch solution for implementing fair classifiers in settings where expert-established fairness tradeoffs are not yet defined.",[],,"['Jack Blandin', 'Ian A. Kash']","['Department of Computer Science, University of Illinois at Chicago, United States of America', 'Department of Computer Science, University of Illinois at Chicago, United States of America']",[]
