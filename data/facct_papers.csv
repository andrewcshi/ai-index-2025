link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://doi.org/10.1145/3630106.3658537,Transparency & Explainability,Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations,"Decision processes of computer vision models—especially deep neural networks—are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are—as the explainability methods themselves—often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.","['eXplainable AI', 'XAI', 'saliency maps', 'saliency metrics', 'heatmaps', 'quantitative evaluation', 'psychometric testing', 'validity', 'reliability', 'objective XAI evaluation']",Computing methodologies → Interest point and salient region detections General and reference → Metrics Computing methodologies → Computer vision Human-centered computing~Human computer interaction (HCI),"['Benjamin Fresz', 'Lena Lörcher', 'Marco Huber']","['Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany']",[]
https://doi.org/10.1145/3630106.3658538,Fairness & Bias,Designing Long-term Group Fair Policies in Dynamical Systems,"Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term—even if fairness considerations were taken into account in the policy design process. In this paper, we propose a novel framework for studying long-term group fairness in dynamical systems, in which current decisions may affect an individual’s features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long-term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov Chain Convergence Theorem to ensure unique convergence. Our framework enables the utilization of historical temporal data to tackle challenges associated with delayed feedback when learning long-term fair policies in practice. Importantly, our framework shows that interventions on the data distribution (e.g., subsidies) can be used to achieve policy learning that is both short- and long-term fair. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. In semi-synthetic simulations based on real-world datasets, we show how our approach facilitates identifying effective interventions for long-term fairness.","['fairness', 'long-term', 'dynamical system', 'equilibrium', 'policy learning']",Computing methodologies → Machine learning Social and professional topics,"['Miriam Rateike', 'Isabel Valera', 'Patrick Forré']","['Saarland University, Germany', 'Saarland University, Germany and Max Planck Institute for Software Systems, Germany', 'AI4Science Lab, AMLab, Informatics Institute, University of Amsterdam, Germany']",[]
https://doi.org/10.1145/3630106.3658539,Fairness & Bias,Learning Fairness from Demonstrations via Inverse Reinforcement Learning,"Defining fairness in algorithmic contexts is challenging, particularly when adapting to new domains. Our research introduces a novel method for learning and applying group fairness preferences across different classification domains, without the need for manual fine-tuning. Utilizing concepts from inverse reinforcement learning (IRL), our approach enables the extraction and application of fairness preferences from human experts or established algorithms. We propose the first technique for using IRL to recover and adapt group fairness preferences to new domains, offering a low-touch solution for implementing fair classifiers in settings where expert-established fairness tradeoffs are not yet defined.",[],,"['Jack Blandin', 'Ian A. Kash']","['Department of Computer Science, University of Illinois at Chicago, United States of America', 'Department of Computer Science, University of Illinois at Chicago, United States of America']",[]
https://doi.org/10.1145/3630106.3658540,Fairness & Bias,Using Property Elicitation to Understand the Impacts of Fairness Regularizers,"Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.",[],,['Jessie Finocchiaro'],"['Center for Research on Computation and Society, Harvard University, United States of America']",[]
https://doi.org/10.1145/3630106.3658543,Fairness & Bias,Data Feminism for AI,"This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.","['feminism', 'data feminism', 'data justice', 'ai ethics', 'responsible ai']",,[],[],[]
https://doi.org/10.1145/3630106.3658547,Transparency & Explainability,"Why is ""Problems"" Predictive of Positive Sentiment? A Case Study of Explaining Unintuitive Features in Sentiment Classification","Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions. To this end, many approaches explain which input features are most predictive of a target label. However, such explanations can still be puzzling to users (e.g., in product reviews, the word “problems” is predictive of positive sentiment). If left unexplained, puzzling explanations can have negative impacts. Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research. We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study. We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive. Results from a crowdsourced study (N = 300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification.",[],,"['Jiaming Qu', 'Jaime Arguello', 'Yue Wang']","['School of Information and Library Science, University of North Carolina at Chapel Hill, United States', 'School of Information and Library Science, University of North Carolina at Chapel Hill, USA', 'School of Information and Library Science, University of North Carolina at Chapel Hill, USA']",[]
https://doi.org/10.1145/3630106.3658548,Transparency & Explainability,"Regulating AI-Based Remote Biometric Identification. Investigating the Public Demand for Bans, Audits, and Public Database Registrations","AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but at the same time they are criticised for inheriting biases and violating fundamental human rights. As a result, the use of RBI poses risks to society. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a broad consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI for the Common Good. As a possible counterweight, public opinion can have a decisive influence on policymakers (e.g. through voter demands) to establish boundaries and conditions under which AI systems should be used – if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems.","['Regulation', 'Trust', 'Discrimination Perception', 'Survey Research', 'Artificial Intelligence', 'Remote Biometric Identification']",Human-centered computing → Empirical studies in HCI Social and professional topics → Governmental regulations Applied computing → Sociology Applied computing → Law,[],[],[]
https://doi.org/10.1145/3630106.3658542,N/A,Power Hungry Processing: Watts Driving the Cost of AI Deployment?,"The authors have requested minor, non-substantive changes to the VoR and, in accordance with ACM policies, a Corrected VoR was published on November 6, 2024. For reference purposes the VoR may still be accessed via the Supplemental Material section on this page.",[],Computing methodologies → Machine learning Computing methodologies → Neural networks Hardware → Impact on the environment Hardware → Power estimation and optimization,"['Sasha Luccioni', 'Yacine Jernite', 'Emma Strubell']","['Hugging Face, Canada', 'Hugging Face, United States', 'Carnegie Mellon, Allen AI Institute, USA']",[]
https://doi.org/10.1145/3630106.3658544,N/A,Reliability Gaps Between Groups in COMPAS Dataset,"This paper investigates the inter-rater reliability of risk assessment instruments (RAIs). The main question is whether different, socially salient groups are affected differently by a lack of inter-rater reliability of RAIs, that is, whether mistakes with respect to different groups affects them differently. The question is investigated with a simulation study of the COMPAS dataset. A controlled degree of noise is injected into the input data of a predictive model; the noise can be interpreted as a synthetic rater that makes mistakes. The main finding is that there are systematic differences in output reliability between groups in the COMPAS dataset. The sign of the difference depends on the kind of inter-rater statistic that is used (Cohen’s Kappa, Byrt’s PABAK, ICC), and in particular whether or not the statistic corrects for prediction prevalences of the groups.",[],,['Tim Räz'],"['University of Bern, Switzerland']",[]
https://doi.org/10.1145/3630106.3658545,N/A,Mapping AI ethics: a meso-scale analysis of its charters and manifestos,"The recent years have seen a surge of initiatives with the goal of defining what “ethical” artificial intelligence would or should entail, resulting in the publication of various charters and manifestos discussing AI ethics; these documents originate from academia, AI industry companies, non-profits, regulatory institutions, and the civil society. The contents of such documents vary wildly, from short, vague position statements to verbatims of democratic debates or impact assessment studies. As such, they are a marker of the social world of artificial intelligence, outlining the tenets of different actors, the consensus and dissensus on important goals, and so on.",[],,"['Mélanie Gornet', 'Simon Delarue', 'Maria Boritchev', 'Tiphaine Viard']","['i3, SES, Télécom Paris, Institut Polytechnique de Paris, France', 'LTCI, Télécom Paris, Institut Polytechnique de Paris, France', 'LTCI, Télécom Paris, Institut Polytechnique de Paris, France', 'i3, SES, Télécom Paris, Institut Polytechnique de Paris, Télécom Paris, France']",[]
https://doi.org/10.1145/3630106.3658546,N/A,“I Searched for a Religious Song in Amharic and Got Sexual Content Instead’’: Investigating Online Harm in Low-Resourced Languages on YouTube.,"Online social media platforms such as YouTube have a wide, global reach. However, little is known about the experience of low-resourced language speakers on such platforms; especially in how they experience and navigate harmful content. To better understand this, we (1) conducted semi-structured interviews (n=15) and (2) analyzed search results (n=9313), recommendations (n=3336), channels (n=120) and comments (n=406) of policy-violating sexual content on YouTube focusing on the Amharic language. Our findings reveal that – although Amharic-speaking YouTube users find the platform crucial for several aspects of their lives – participants reported unplanned exposure to policy-violating sexual content when searching for benign, popular queries. Furthermore, malicious content creators seem to exploit under-performing language technologies and content moderation to further target vulnerable groups of speakers, including migrant domestic workers, diaspora, and local Ethiopians. Overall, our study sheds light on how failures in low-resourced language technology may lead to exposure to harmful content and suggests implications for stakeholders in minimizing harm. Content Warning: This paper includes discussions of NSFW topics and harmful content (hate, abuse, sexual harassment, self-harm, misinformation). The authors do not support the creation or distribution of harmful content.","['Online Harm', 'Recommendation Systems', 'Low-Resourced Languages', 'Community Guidelines', 'User Experience', 'Search Engines', 'Low-Resourced NLP', 'Policy']","Human-centered computing → Empirical studies in HCI Human-centered computing → HCI theory, concepts and models Social and professional topics → Cultural characteristics Social and professional topics → Geographic characteristics","['Hellina Hailu Nigatu', 'Inioluwa Deborah Raji']","['UC Berkeley, United States of America', 'UC Berkeley, USA']",[]
https://doi.org/10.1145/3630106.3658549,N/A,,,[],,[],[],[]
