link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://doi.org/10.1145/3630106.3658537,Transparency & Explainability,Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations,"Decision processes of computer vision models—especially deep neural networks—are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are—as the explainability methods themselves—often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.","['eXplainable AI', 'XAI', 'saliency maps', 'saliency metrics', 'heatmaps', 'quantitative evaluation', 'psychometric testing', 'validity', 'reliability', 'objective XAI evaluation']",Computing methodologies → Interest point and salient region detections General and reference → Metrics Computing methodologies → Computer vision Human-centered computing~Human computer interaction (HCI),"['Benjamin Fresz', 'Lena Lörcher', 'Marco Huber']","['Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany', 'Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Germany and Institute of Industrial Manufacturing and Management IFF, University of Stuttgart, Germany']",[]
https://doi.org/10.1145/3630106.3658538,Fairness & Bias,Designing Long-term Group Fair Policies in Dynamical Systems,"Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term—even if fairness considerations were taken into account in the policy design process. In this paper, we propose a novel framework for studying long-term group fairness in dynamical systems, in which current decisions may affect an individual’s features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long-term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov Chain Convergence Theorem to ensure unique convergence. Our framework enables the utilization of historical temporal data to tackle challenges associated with delayed feedback when learning long-term fair policies in practice. Importantly, our framework shows that interventions on the data distribution (e.g., subsidies) can be used to achieve policy learning that is both short- and long-term fair. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. In semi-synthetic simulations based on real-world datasets, we show how our approach facilitates identifying effective interventions for long-term fairness.","['fairness', 'long-term', 'dynamical system', 'equilibrium', 'policy learning']",Computing methodologies → Machine learning Social and professional topics,"['Miriam Rateike', 'Isabel Valera', 'Patrick Forré']","['Saarland University, Germany', 'Saarland University, Germany and Max Planck Institute for Software Systems, Germany', 'AI4Science Lab, AMLab, Informatics Institute, University of Amsterdam, Germany']",[]
https://doi.org/10.1145/3630106.3658539,Fairness & Bias,Learning Fairness from Demonstrations via Inverse Reinforcement Learning,"Defining fairness in algorithmic contexts is challenging, particularly when adapting to new domains. Our research introduces a novel method for learning and applying group fairness preferences across different classification domains, without the need for manual fine-tuning. Utilizing concepts from inverse reinforcement learning (IRL), our approach enables the extraction and application of fairness preferences from human experts or established algorithms. We propose the first technique for using IRL to recover and adapt group fairness preferences to new domains, offering a low-touch solution for implementing fair classifiers in settings where expert-established fairness tradeoffs are not yet defined.",[],,"['Jack Blandin', 'Ian A. Kash']","['Department of Computer Science, University of Illinois at Chicago, United States of America', 'Department of Computer Science, University of Illinois at Chicago, United States of America']",[]
https://doi.org/10.1145/3630106.3658540,Fairness & Bias,Using Property Elicitation to Understand the Impacts of Fairness Regularizers,"Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.",[],,['Jessie Finocchiaro'],"['Center for Research on Computation and Society, Harvard University, United States of America']",[]
https://doi.org/10.1145/3630106.3658543,Fairness & Bias,Data Feminism for AI,"This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.","['feminism', 'data feminism', 'data justice', 'ai ethics', 'responsible ai']",,[],[],[]
