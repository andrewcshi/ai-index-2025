link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/2309.13965,Transparency & Explainability,Understanding Decision Subjects’ Fairness Perceptions and Retention in repeated Interactions with AI-Based Decision Systems,"Research in explainable AI (XAI) aims to provide insights into the decision-making process of opaque AI models. To date, most XAI methods offer one-off and static explanations, which cannot cater to the diverse backgrounds and understanding levels of users. With this paper, we investigate if free-form conversations can enhance users' comprehension of static explanations, improve acceptance and trust in the explanation methods, and facilitate human-AI collaboration. Participants are presented with static explanations, followed by a conversation with a human expert regarding the explanations. We measure the effect of the conversation on participants' ability to choose, from three machine learning models, the most accurate one based on explanations and their self-reported comprehension, acceptance, and trust. Empirical results show that conversations significantly improve comprehension, acceptance, trust, and collaboration. Our findings highlight the importance of customized model explanations in the format of free-form conversations and provide insights for the future design of conversational explanations.",[],[],"['Tong Zhang', 'X. Jessie Yang', 'Boyang Li']",[],[]
https://arxiv.org/abs/2203.02013,Transparency & Explainability,DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations,"The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment. Code for our experiments can be found at this https URL.",[],[],"['Yiwei Lyu', 'Paul Pu Liang', 'Zihao Deng', 'Ruslan Salakhutdinov', 'Louis-Philippe Morency']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2206.04737,Transparency & Explainability,Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,"Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of interventions that allow for the effective participation of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",[],[],"['Inioluwa Deborah Raji', 'Peggy Xu', 'Colleen Honigsberg', 'Daniel E. Ho']","['University of California, Berkeley, Berkeley, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2108.06907,Transparency & Explainability,Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression(GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",[],[],"['Aditya Saini', 'Ranjitha Prasad']","['Indraprastha Institute of Information Technology, Delhi, New Delhi, India', 'Indraprastha Institute of Information Technology, Delhi, New Delhi, India']","['India', 'India']"
https://arxiv.org/abs/2205.05126,Transparency & Explainability,A Meta-Analysis of the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making,"Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.",[],[],"['Max Schemmer', 'Patrick Hemmer', 'Maximilian Nitsche', 'Niklas Kühl', 'Michael Vössing']","['Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany', 'Karlsruhe Institute of Technology, Karlsruhe, Germany']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2204.00302,Transparency & Explainability,Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes,"Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.",[],[],"['Stelios Triantafyllou', 'Adish Singla', 'Goran Radanovic']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","['Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2205.15258,Transparency & Explainability," Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: a UK Case Study","We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",[],[],"['Miri Zilka', 'Holli Sargeant', 'Adrian Weller']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2205.03931,Fairness & Bias,Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations,"Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.",[],[],"['Hammaad Adam', 'Ming Ying Yang', 'Kenrick Cato', 'Ioana Baldini', 'Charles Senteio', 'Leo Anthony Celi', 'Jiaming Zeng', 'Moninder Singh', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology, Cambridge, MA, USA', 'Massachusetts Institute of Technology, Cambridge, MA, USA', 'Columbia University, New York, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Rutgers University, New Brunswick, NJ, USA', 'Massachusetts Institute of Technology & Beth Israel Deaconess Medical Center, Cambridge, MA, USA', 'IBM Research, Cambridge, MA, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Massachusetts Institute of Technology & Vector Institute, Cambridge, MA, USA']","['US', 'US', 'US', 'US', 'US', 'Israel', 'US', 'US', 'US']"
https://arxiv.org/abs/2203.16432,Fairness & Bias,Long-term Dynamics of Fairness Intervention in Connection Recommender Systems,"Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Pólya urn model.",[],[],"['Nil-Jana Akpinar', 'Cyrus DiCiccio', 'Preetam Nandy', 'Kinjal Basu']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Work done at LinkedIn, Sunnyvale, CA, USA', 'LinkedIn, Sunnyvale, CA, USA', 'LinkedIn, Sunnyvale, CA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2207.01510,Fairness & Bias,Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",[],[],"['Alejandra Bringas Colmenarejo', 'Luca Nannini', 'Alisa Rieger', 'Kristen M. Scott', 'Xuan Zhao', 'Gourab K. Patro', 'Gjergji Kasneci', 'Katharina Kinder-Kurlanda']","['University of Southampton, Southampton, United Kingdom', 'Minsait - Indra Sistemas & Universidade de Santiago de Compostela, Santiago de Compostela, Spain', 'Delft University of Technology, Delft, Netherlands', 'Katholieke Universiteit Leuven, Leuven, Belgium', 'SCHUFA Holding AG & University of Tuebingen, Tuebingen, Germany', 'IIT Kharagpur & L3S Research Center, Kharagpur, India', 'SCHUFA Holding AG & University of Tuebingen, Tuebingen, Germany', 'University of Klagenfurt, Klagenfurt, Austria']","['United Kingdom', 'Spain', 'Netherlands', 'Belgium', 'Germany', 'India', 'Germany', 'Austria']"
https://arxiv.org/abs/2203.11771,Fairness & Bias,Racial Disparities in the Enforcement of Marijuana Violations in the US,"Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.",[],[],"['Bradley Butcher', 'Chris Robinson', 'Miri Zilka', 'Riccardo Fogliato', 'Carolyn Ashurst', 'Adrian Weller']","['University of Sussex, Brighton, United Kingdom', 'University of Sussex, Brighton, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'The Alan Turing Institute, London, United Kingdom', 'University of Cambridge & The Alan Turing Institute, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'US', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2206.03390,Fairness & Bias," Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics","The statistical regularities in language corpora encode well-known social biases into word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). Using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence.",[],[],"['Aylin Caliskan', 'Pimparkar Parth Ajay', 'Tessa Charlesworth', 'Robert Wolfe', 'Mahzarin R. Banaji']","['University of Washington, Seattle, WA, USA', 'Birla Institute of Technology and Science, Pilani, India', 'Harvard University, Cambridge, MA, USA', 'University of Washington, Seattle, WA, USA', 'Harvard University, Cambridge, MA, USA']","['US', 'India', 'US', 'US', 'US']"
https://arxiv.org/abs/2205.07277,Fairness & Bias,Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations,"As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across various population subgroups including the minority groups. For instance, it should not be the case that explanations associated with instances belonging to a particular gender subgroup (e.g., female) are less accurate than those associated with other genders. However, there is little to no research that assesses if there exist such group-based disparities in the quality of the explanations output by state-of-the-art explanation methods. In this work, we address the aforementioned gaps by initiating the study of identifying group-based disparities in explanation quality. To this end, we first outline the key properties which constitute explanation quality and where disparities can be particularly problematic. We then leverage these properties to propose a novel evaluation framework which can quantitatively measure disparities in the quality of explanations output by state-of-the-art methods. Using this framework, we carry out a rigorous empirical analysis to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and highly non-linear. In addition, we also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit the aforementioned disparities. To the best of our knowledge, this work is the first to highlight and study the problem of group-based disparities in explanation quality. In doing so, our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.",[],[],"['Jessica Dai', 'Sohini Upadhyay', 'Ulrich Aivodji', 'Stephen H. Bach', 'Himabindu Lakkaraju']","['Brown University, Providence, RI, USA', 'Harvard University, Cambridge, MA, USA', 'Université du Québec à Montréal, Montreal, PQ, Canada', 'Brown University, Providence, RI, USA', 'Harvard University, Cambridge, MA, USA']","['US', 'US', 'Canada', 'US', 'US']"
https://arxiv.org/abs/2106.07057,Fairness & Bias,FairCanary: Rapid Continuous Explainable Fairness,"Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems. We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.",[],[],"['Avijit Ghosh', 'Aalok Shanbhag', 'Christo Wilson']","['Northeastern University, Boston, MA, USA', 'Snap Inc., Mountain View, CA, USA', 'Northeastern University, Boston, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2203.06498,Fairness & Bias,The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning,"Recent arguments that machine learning (ML) is facing a reproducibility and replication crisis suggest that some published claims in ML research cannot be taken at face value. These concerns inspire analogies to the replication crisis affecting the social and medical sciences. They also inspire calls for the integration of statistical approaches to causal inference and predictive modeling. A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimental science puts the new concerns in perspective, and helps researchers avoid ""the worst of both worlds,"" where ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exemplified in psychology versus predictive modeling as exemplified in ML. We identify themes that re-occur in reform discussions, like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often impossible to refute due to undisclosed sources of variance in the learning pipeline. In particular, errors being acknowledged in ML expose cracks in long-held beliefs that optimizing predictive accuracy using huge datasets absolves one from having to consider a true data generating process or formally represent uncertainty in performance claims. We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform.",[],[],"['Jessica Hullman', 'Sayash Kapoor', 'Priyanka Nanayakkara', 'Andrew Gelman', 'Arvind Narayanan']","['Northwestern University, Evanston, IL, USA', 'Princeton University, Princeton, NJ, USA', 'Northwestern University, Evanston, IL, USA', 'Columbia University, New York, NY, USA', 'Princeton University, Princeton, NJ, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2206.01691,Fairness & Bias,Measuring Gender Bias in Word Embeddings of Gendered Languages Requires Disentangling Grammatical Gender Signals,"Does the grammatical gender of a language interfere when measuring the semantic gender information captured by its word embeddings? A number of anomalous gender bias measurements in the embeddings of gendered languages suggest this possibility. We demonstrate that word embeddings learn the association between a noun and its grammatical gender in grammatically gendered languages, which can skew social gender bias measurements. Consequently, word embedding post-processing methods are introduced to quantify, disentangle, and evaluate grammatical gender signals. The evaluation is performed on five gendered languages from the Germanic, Romance, and Slavic branches of the Indo-European language family. Our method reduces the strength of grammatical gender signals, which is measured in terms of effect size (Cohen's d), by a significant average of d = 1.3 for French, German, and Italian, and d = 0.56 for Polish and Spanish. Once grammatical gender is disentangled, the association between over 90% of 10,000 inanimate nouns and their assigned grammatical gender weakens, and cross-lingual bias results from the Word Embedding Association Test (WEAT) become more congruent with country-level implicit bias measurements. The results further suggest that disentangling grammatical gender signals from word embeddings may lead to improvement in semantic machine learning tasks.",[],[],"['Shiva Omrani Sabbaghi', 'Aylin Caliskan']","['George Washington University, Washington, DC, USA', 'University of Washington, Seattle, WA, USA']","['US', 'US']"
https://arxiv.org/abs/2108.06907,Fairness & Bias,Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression(GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",[],[],"['Aditya Saini', 'Ranjitha Prasad']","['Indraprastha Institute of Information Technology, Delhi, New Delhi, India', 'Indraprastha Institute of Information Technology, Delhi, New Delhi, India']","['India', 'India']"
https://arxiv.org/abs/2206.07555,Fairness & Bias,Respect as a Lens for the Design of AI Systems,"Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",[],[],"['William Seymour', 'Max Van Kleek', 'Reuben Binns', 'Dave Murray-Rust']",[],[]
https://arxiv.org/abs/2207.00691,Fairness & Bias,American == White in Multimodal Language-and-Image AI,"Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, but never remarks upon race for White individuals. Finally, provided with an initialization image from the CFD and the text ""an American person,"" a synthetic image generator (VQGAN) using the text-based guidance of CLIP lightens the skin tone of individuals of all races (by 35% for Black individuals, based on pixel brightness). The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.",[],[],"['Robert Wolfe', 'Aylin Caliskan']","['University of Washington, Seattle, WA, USA', 'University of Washington, Seattle, WA, USA']","['US', 'US']"
https://arxiv.org/abs/2205.15258,Fairness & Bias,"Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: a UK Case Study","We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",[],[],"['Miri Zilka', 'Holli Sargeant', 'Adrian Weller']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2209.10604,Security,Current and Near-Term AI as a Potential Existential Risk Factor,"There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.",[],[],"['Benjamin S. Bucknall', 'Shiri Dori-Hacohen']","['Uppsala University, Uppsala, Sweden', 'University of Connecticut, Storrs, CT, USA']","['Sweden', 'US']"
https://arxiv.org/abs/2206.04737,Security,Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,"Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of interventions that allow for the effective participation of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",[],[],"['Inioluwa Deborah Raji', 'Peggy Xu', 'Colleen Honigsberg', 'Daniel E. Ho']","['University of California, Berkeley, Berkeley, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2206.07555,Security,Respect as a Lens for the Design of AI Systems,"Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",[],[],"['William Seymour', 'Max Van Kleek', 'Reuben Binns', 'Dave Murray-Rust']","['Oregon State University, Corvallis, OR, USA', 'Oregon State University, Corvallis, OR, USA']","['US', 'US']"