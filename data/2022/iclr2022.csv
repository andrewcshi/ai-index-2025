link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2022/poster/6601,Transparency & Explainability,Huber Additive Models for Non-stationary Time Series Analysis,"Sparse additive models have shown promising ﬂexibility and interpretability in processing time series data. However, existing methods usually assume the time series data to be stationary and the innovation is sampled from a Gaussian distribution. Both assumptions are too stringent for heavy-tailed and non-stationary time series data that frequently arise in practice, such as ﬁnance and medical ﬁelds. To address these problems, we propose an adaptive sparse Huber additive model for robust forecasting in both non-Gaussian data and (non)stationary data. In theory, the generalization bounds of our estimator are established for both stationary and nonstationary time series data, which are independent of the widely used mixing conditions in learning theory of dependent observations. Moreover, the error bound for non-stationary time series contains a discrepancy measure for the shifts of the data distributions over time. Such a discrepancy measure can be estimated empirically and used as a penalty in our method. Experimental results on both synthetic and real-world benchmark datasets validate the effectiveness of the proposed method. The code is available at https://github.com/xianruizhong/SpHAM.",[],[],"['Yingjie Wang', 'Xianrui Zhong', 'Fengxiang He', 'Hong Chen', 'Dacheng Tao']","['China University of Petroleum', 'University of Illinois, Urbana Champaign', 'University of Edinburgh', 'Huazhong Agricultural University', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6461,Transparency & Explainability,Optimization inspired Multi-Branch Equilibrium Models,"Works have shown the strong connections between some implicit models and optimization problems. However, explorations on such relationships are limited. Most works pay attention to some common mathematical properties, such as sparsity. In this work, we propose a new type of implicit model inspired by the designing of the systems' hidden objective functions, called the Multi-branch Optimization induced Equilibrium networks~(MOptEqs). The model architecture is designed based on modelling the hidden objective function for the multi-resolution recognition task. Furthermore, we also propose a new training strategy inspired by our understandings of the hidden objective function. In this manner, the proposed model can better utilize the hierarchical patterns for recognition tasks and retain the abilities for interpreting the whole structure as trying to obtain the minima of the problem's goal. Comparing with the state-of-the-art models, our MOptEqs not only enjoys better explainability but are also superior to MDEQ with less parameter consumption and better performance on practical tasks. Furthermore, we also implement various experiments to demonstrate the effectiveness of our new methods and explore the applicability of the model's hidden objective function.",[],[],"['Mingjie Li', 'Yisen Wang', 'Xingyu Xie', 'Zhouchen Lin']","['Peking University', 'Peking University', 'National University of Singapore', 'Peking University']",[]
https://iclr.cc/virtual/2022/poster/6680,Transparency & Explainability,Granger causal inference on DAGs identifies genomic loci regulating transcription,"When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables. However, traditional Granger causal inference has limited utility in domains where the dynamics need to be represented as directed acyclic graphs (DAGs) rather than as a linear sequence, such as with cell differentiation trajectories. Here, we present GrID-Net, a framework based on graph neural networks with lagged message passing for Granger causal inference on DAG-structured systems. Our motivating application is the analysis of single-cell multimodal data to identify genomic loci that mediate the regulation of specific genes. To our knowledge, GrID-Net is the first single-cell analysis tool that accounts for the temporal lag between a genomic locus becoming accessible and its downstream effect on a target gene's expression. We applied GrID-Net on multimodal single-cell assays that profile chromatin accessibility (ATAC-seq) and gene expression (RNA-seq) in the same cell and show that it dramatically outperforms existing methods for inferring regulatory locus-gene links, achieving up to 71% greater agreement with independent population genetics-based estimates. By extending Granger causality to DAG-structured dynamical systems, our work unlocks new domains for causal analyses and, more specifically, opens a path towards elucidating gene regulatory interactions relevant to cellular differentiation and complex human diseases at unprecedented scale and resolution.","['Granger causality', 'Directed Acyclic Graphs', 'causal inference', 'graph neural networks']",[],"['Alexander P Wu', 'Rohit Singh', 'Bonnie Berger']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6645,Transparency & Explainability,Modeling Label Space Interactions in Multi-label Classification using Box Embeddings,"Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018).  Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles.  Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels.  Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance.","['embeddings', 'multi-label classification', 'representation learning']",[],"['Dhruvesh Patel', 'Pavitra Dangati', 'Jay-Yoon Lee', 'Michael Boratko', 'Andrew McCallum']","['College of Information and Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'Seoul National University', 'Google', 'Department of Computer Science, University of Massachusetts, Amherst']",[]
https://iclr.cc/virtual/2022/poster/6555,Transparency & Explainability,Discovering Invariant Rationales for Graph Neural Networks,"Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features --- rationale --- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.","['graph neural networks', 'interpretability']",[],"['Shirley Wu', 'Xiang Wang', 'An Zhang', 'Xiangnan He', 'Tat-Seng Chua']","['Computer Science Department, Stanford University', 'University of Science and Technology of China', 'National University of Singapore', 'University of Science and Technology of China', 'National University of Singapore']",[]
https://iclr.cc/virtual/2022/poster/6442,Transparency & Explainability,Meta Discovery: Learning to Discover Novel Classes given Very Limited Data,"In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, NCD is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as NCD. Thus, we can empirically solve the NCD problem by meta-learning algorithms after slight modifications. This meta-learning-based methodology significantly reduces the amount of unlabeled data needed for training and makes it more practical, as demonstrated in experiments. The use of very limited data is also justified by the application scenario of NCD: since it is unnatural to label only seen-class data, NCD is sampling instead of labeling in causality. Therefore, unseen-class data should be collected on the way of collecting seen-class data, which is why they are novel and first need to be clustered.",[],[],"['Haoang Chi', 'Feng Liu', 'Wenjing Yang', 'Long Lan', 'Tongliang Liu', 'Bo Han', 'Gang Niu', 'Mingyuan Zhou', 'Masashi Sugiyama']","['National University of Defense Technology', 'University of Melbourne', 'National University of Defense Technology', 'National University of Defense Technology', 'University of Sydney', 'HKBU', 'RIKEN', 'The University of Texas at Austin', 'RIKEN']",[]
https://iclr.cc/virtual/2022/poster/6367,Transparency & Explainability,DISSECT: Disentangled Simultaneous Explanations via Concept Traversals,"Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent ""notion"" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.","['explainability', 'generative adversarial network', 'variational autoencoder', 'interpretability']",[],"['Asma Ghandeharioun', 'Been Kim', 'Chun-Liang Li', 'Brendan Jou', 'Brian Eoff', 'Rosalind Picard']","['Google', 'Google DeepMind', 'Google', 'Google', 'Google', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6355,Transparency & Explainability,NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning,"Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we are able to improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.","['generalized additive model', 'interpretability']",[],"['Chun-Hao Chang', 'Rich Caruana', 'Anna Goldenberg']","['University of Toronto', 'School of Computer Science, Carnegie Mellon University', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6146,Transparency & Explainability,Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing,"Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., ""red apple'') instead of just individual attributes (""red'' and ""apple'' independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: https://github.com/iro-cp/ssl-qrp.","['computer vision', 'representation learning', 'interpretability']",[],"['Iro Laina', 'Yuki M Asano', 'Andrea Vedaldi']","['University of Oxford', 'University of Amsterdam', 'University of Oxford']",[]
https://iclr.cc/virtual/2022/poster/5926,Transparency & Explainability,Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph,"This paper addresses the unsupervised learning of content-style decomposed representation. We first give a definition of style and then model the content-style representation as a token-level bipartite graph. An unsupervised framework, named Retriever, is proposed to learn such representations. First, a cross-attention module is employed to retrieve permutation invariant (P.I.) information, defined as style, from the input data. Second, a vector quantization (VQ) module is used, together with man-induced constraints, to produce interpretable content tokens. Last, an innovative link attention module serves as the decoder to reconstruct data from the decomposed content and style, with the help of the linking keys. Being modal-agnostic, the proposed Retriever is evaluated in both speech and image domains. The state-of-the-art zero-shot voice conversion performance confirms the disentangling ability of our framework. Top performance is also achieved in the part discovery task for images, verifying the interpretability of our representation. In addition, the vivid part-based style transfer quality demonstrates the potential of Retriever to support various fascinating generative tasks. Project page at https://ydcustc.github.io/retriever-demo/.","['transformer', 'style transfer', 'unsupervised learning']",[],"['Dacheng Yin', 'Xuanchi Ren', 'Chong Luo', 'Yuwang Wang', 'Zhiwei Xiong', 'Wenjun Zeng']","['University of Science and Technology of China', 'University of Toronto', 'Microsoft Research Asia', 'Tsinghua University, Tsinghua University', 'USTC', 'Eastern Institute for Advanced Study']",[]
https://iclr.cc/virtual/2022/poster/6330,Transparency & Explainability,Adversarial Robustness Through the Lens of Causality,"The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial attacks and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From the causal perspective, we study the distinction between the natural and adversarial distribution and conclude that the origin of adversarial vulnerability is the focus of models on spurious correlations. Inspired by the causal understanding, we propose the \emph{Causal}-inspired \emph{Adv}ersarial distribution alignment method, CausalAdv, to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method. Our work is the first attempt towards using causality to understand and mitigate the adversarial vulnerability.","['adversarial examples', 'causality']",[],"['Yonggang Zhang', 'Mingming Gong', 'Tongliang Liu', 'Gang Niu', 'Xinmei Tian', 'Bo Han', 'Kun Zhang']","['Hong Kong Baptist University', 'University of Melbourne', 'University of Sydney', 'RIKEN', 'University of Science and Technology of China', 'HKBU', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2022/poster/5962,Transparency & Explainability,Attention-based Interpretability with Concept Transformers,"Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks.One additional notable aspect of attention is that it conveniently exposes the ``reasoning'' behind each particular output generated by the model.Specifically, attention scores over input regions or intermediate features have been interpreted as a measure of the contribution of the attended element to the model inference.While the debate in regard to the interpretability of attention is still not settled, researchers have pointed out the existence of architectures and scenarios that afford a meaningful interpretation of the attention mechanism.Here we propose the generalization of attention from low-level input features to high-level concepts as a mechanism to ensure the interpretability of attention scores within a given application domain.In particular, we design the ConceptTransformer, a deep learning module that exposes explanations of the output of a model in which it is embedded in terms of attention over user-defined high-level concepts.Such explanations are \emph{plausible} (i.e.\ convincing to the human user) and \emph{faithful} (i.e.\ truly reflective of the reasoning process of the model).Plausibility of such explanations is obtained by construction by training the attention heads to conform with known relations between inputs, concepts and outputs dictated by domain knowledge.Faithfulness is achieved by design by enforcing a linear relation between the transformer value vectors that represent the concepts and their contribution to the classification log-probabilities.We validate our ConceptTransformer module on established explainability benchmarks and show how it can be used to infuse domain knowledge into classifiers to improve accuracy, and conversely to extract concept-based explanations of classification outputs. Code to reproduce our results is available at: \url{https://github.com/ibm/concept_transformer}.","['transformer', 'attention', 'interpretability']",[],"['Mattia Rigotti', 'Christoph Miksovic', 'Ioana Giurgiu', 'Thomas Gschwind', 'Paolo Scotton']","['International Business Machines', 'International Business Machines', 'International Business Machines', 'IBM Research', 'International Business Machines']",[]
https://iclr.cc/virtual/2022/poster/6744,Transparency & Explainability,Programmatic Reinforcement Learning without Oracles,"Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable.","['reinforcement learning', 'program synthesis']",[],"['Wenjie Qiu', 'He Zhu']","['Rutgers University', 'Rutgers University']",[]
https://iclr.cc/virtual/2022/poster/7027,Transparency & Explainability,Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis,"Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model’s ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset (5,499 EEGs), we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types (e.g. 47 points increase in combined tonic seizure accuracy over baselines). Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.","['self-supervision', 'interpretability', 'electroencephalography', 'graph neural network', 'visualization', 'time series', 'neuroscience']",[],"['Siyi Tang', 'Jared Dunnmon', 'Khaled Kamal Saab', 'Xuan Zhang', 'Qianying Huang', 'Florian Dubost', 'Daniel Rubin', 'Christopher Lee-Messer']","['Artera Inc', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Liminal Sciences Inc', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2022/poster/6188,Transparency & Explainability,CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability,"What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation.  The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions.  In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass: the CLEVA-Compass. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape.  In addition to promoting compact specification in the spirit of recent replication trends, it thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison.","['lifelong learning', 'continual learning']",[],"['Martin Mundt', 'Steven Braun', 'Quentin Delfosse', 'Kristian Kersting']","['Technische Universität Darmstadt & hessian.AI', 'TU Darmstadt', 'CS Department, TU Darmstadt, TU Darmstadt', 'German Research Center for AI']",[]
https://iclr.cc/virtual/2022/poster/5977,Transparency & Explainability,Interpretable Unsupervised Diversity Denoising and Artefact Removal,"Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts  and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves state-of-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions.Additionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image.",['unsupervised image denoising'],[],"['Mangal Prakash', 'Mauricio Delbracio', 'Peyman Milanfar', 'Florian Jug']","['Johnson & Johnson', 'Google', 'Google Research', 'Fondation Human Technopole']",[]
https://iclr.cc/virtual/2022/poster/6983,Transparency & Explainability,"Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning","Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate ""fairness"" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.","['Shapley values', 'similarity learning', 'metric learning', 'information retrieval']",[],"['Mark Hamilton', 'Scott M Lundberg', 'Stephanie Fu', 'Lei Zhang', 'William T. Freeman']","['Massachusetts Institute of Technology', 'Microsoft', 'University of California, Berkeley', 'International Digital Economy Academy', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6404,Transparency & Explainability,You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction,"Predicting the future trajectory of a moving agent can be easy when the past trajectory continues smoothly but is challenging when complex interactions with other agents are involved. Recent deep learning approaches for trajectory prediction show promising performance and partially attribute this to successful reasoning about agent-agent interactions.  However, it remains unclear which features such black-box models actually learn to use for making predictions. This paper proposes a procedure that quantifies the contributions of different cues to model performance based on a variant of Shapley values. Applying this procedure to state-of-the-art trajectory prediction methods on standard benchmark datasets shows that they are, in fact, unable to reason about interactions. Instead, the past trajectory of the target is the only feature used for predicting its future. For a task with richer social interaction patterns, on the other hand, the tested models do pick up such interactions to a certain extent, as quantified by our feature attribution method. We discuss the limits of the proposed method and its links to causality.","['causality', 'trajectory prediction', 'Feature Attribution', 'Shapley values']",[],"['Osama Makansi', 'Julius von Kügelgen', 'Francesco Locatello', 'Peter Vincent Gehler', 'Dominik Janzing', 'Thomas Brox']","['Universität Freiburg', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Institute of Science and Technology', 'Zalando SE', 'Amazon', 'University of Freiburg']",[]
https://iclr.cc/virtual/2022/poster/6731,Transparency & Explainability,Explanations of Black-Box Models based on Directional Feature Interactions,"As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature.  We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph.  Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features.  We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.","['Shapley values', 'explainability', 'interpretability']",[],"['Aria Masoomi', 'Zhonghui Xu', 'Craig P Hersh', 'Edwin K. Silverman', 'Peter J. Castaldi', 'Stratis Ioannidis', 'Jennifer Dy']","['Northeastern University', 'Harvard University', ""Brigham and Women's Hospital"", ""Brigham and Women's Hospital"", 'Harvard University', 'Northeastern University', 'Northeastern University']",[]
https://iclr.cc/virtual/2022/poster/5988,Transparency & Explainability,Natural Language Descriptions of Deep Features,"Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.",[],[],"['Evan Hernandez', 'Sarah Schwettmann', 'David Bau', 'Teona Bagashvili', 'Antonio Torralba', 'Jacob Andreas']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Northeastern University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7045,Transparency & Explainability,Fair Normalizing Flows,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.",['fairness'],[],"['Mislav Balunovic', 'Anian Ruoss', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'DeepMind', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6741,Transparency & Explainability,Symbolic Learning to Optimize: Towards Interpretability and Scalability,"Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limits their applicability to optimizing larger tasks; (2) interpretability: it is unclear what each L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can ""kill two birds by one stone"", by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.","['symbolic regression', 'learning to optimize', 'interpretability']",[],"['Wenqing Zheng', 'Tianlong Chen', 'Ting-Kuei Hu', 'Zhangyang Wang']","['University of Texas, Austin', 'Massachusetts Institute of Technology', 'Texas A&M', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2022/poster/6786,Transparency & Explainability,Model Agnostic Interpretability for Multiple Instance Learning,"In Multiple Instance Learning (MIL), models are trained using bags of instances, where only a single label is provided for each bag. A bag label is often only determined by a handful of key instances within a bag, making it difficult to interpret what information a classifier is using to make decisions. In this work, we establish the key requirements for interpreting MIL models. We then go on to develop several model-agnostic approaches that meet these requirements. Our methods are compared against existing inherently interpretable MIL models on several datasets, and achieve an increase in interpretability accuracy of up to 30%. We also examine the ability of the methods to identify interactions between instances and scale to larger datasets, improving their applicability to real-world problems.",['interpretability'],[],"['Joseph Early', 'Christine Evers', 'SArvapali Ramchurn']","['Helsing', 'University of Southampton', 'University of Southampton']",[]
https://iclr.cc/virtual/2022/poster/6556,Transparency & Explainability,Fast Generic Interaction Detection for Model Interpretability and Compression,"The ability of discovering feature interactions in a black-box model is vital to explainable deep learning. We propose a principled, global interaction detection method by casting our target as a multi-arm bandits problem and solving it swiftly with the UCB algorithm. This adaptive method is free of ad-hoc assumptions and among the cutting-edge methods with outstanding detection accuracy and stability. Based on the detection outcome, a lightweight and interpretable deep learning model (called ParaACE) is further built using the alternating conditional expectation (ACE) method. Our proposed ParaACE improves the prediction performance by 26 % and reduces the model size by 100+ times as compared to its Teacher model over various datasets. Furthermore, we show the great potential of our method for scientific discovery through interpreting various real datasets in the economics and smart medicine sectors. The code is available at https://github.com/zhangtj1996/ParaACE.",[],[],"['Tianjian Zhang', 'Feng Yin', 'Zhi-Quan Luo']","['The Chinese University of Hong Kong, Shenzhen', 'The Chinese University of Hong Kong', 'The Chinese University of Hong Kong, Shenzhen']",[]
https://iclr.cc/virtual/2022/poster/6978,Transparency & Explainability,A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease,"We propose a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. Our genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The hierarchical network implicitly tracks the convergence of genetic risk across well-established biological pathways, while an attention mechanism automatically identifies the salient edges of this network at the subject level. In parallel, our imaging network projects multimodal data onto a set of latent embeddings. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. We couple the imaging and genetic embeddings with a predictor network, to ensure that the learned representations are linked to phenotype. We evaluate our framework on a schizophrenia dataset that includes two functional MRI paradigms and gene scores derived from Single Nucleotide Polymorphism data. Using repeated 10-fold cross-validation, we show that our imaging-genetics fusion achieves the better classification performance than state-of-the-art baselines. In an exploratory analysis, we further show that the biomarkers identified by our model are reproducible and closely associated with deficits in schizophrenia.",[],[],"['Sayan Ghosal', 'Qiang Chen', 'Giulio Pergola', 'Aaron L Goldman', 'William Ulrich', 'Daniel R Weinberger', 'Archana Venkataraman']","['Johns Hopkins University', 'Lieber Institute for Brain Development', 'University of Bari', 'Lieber Institute for Brain Development', 'Lieber Institute', 'Johns Hopkins University', 'Johns Hopkins University']",[]
https://iclr.cc/virtual/2022/poster/6367,Fairness & Bias,DISSECT: Disentangled Simultaneous Explanations via Concept Traversals,"Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore ""what-if"" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent ""notion"" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions.","['explainability', 'generative adversarial network', 'variational autoencoder', 'interpretability']",[],"['Asma Ghandeharioun', 'Been Kim', 'Chun-Liang Li', 'Brendan Jou', 'Brian Eoff', 'Rosalind Picard']","['Google', 'Google DeepMind', 'Google', 'Google', 'Google', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6140,Fairness & Bias,SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search,"One-shot Neural Architecture Search (NAS) usually constructs an over-parameterized network, which we call a supernet, and typically adopts sharing parameters among the sub-models to improve computational efficiency. One-shot NAS often repeatedly samples sub-models from the supernet and trains them to optimize the shared parameters. However, this training strategy suffers from multi-model forgetting. Training a sampled sub-model overrides the previous knowledge learned by the other sub-models, resulting in an unfair performance evaluation between the sub-models. We propose Supernet with Unbiased Meta-Features for Neural Architecture Search (SUMNAS), a supernet learning strategy based on meta-learning to tackle the knowledge forgetting issue. During the training phase, we explicitly address the multi-model forgetting problem and help the supernet learn unbiased meta-features, independent from the sampled sub-models. Once training is over, sub-models can be instantly compared to get the overall ranking or the best sub-model. Our evaluation on the NAS-Bench-201 and MobileNet-based search space demonstrate that SUMNAS shows improved ranking ability and finds architectures whose performance is on par with existing state-of-the-art NAS algorithms.",['neural architecture search'],[],"['Hyeonmin Ha', 'Ji-Hoon Kim', 'Semin Park', 'Byung-Gon Chun']","['Seoul National University', 'NAVER', 'Yonsei University', 'Seoul National University']",[]
https://iclr.cc/virtual/2022/poster/6112,Fairness & Bias,Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning,"Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving  generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose \textit{\textbf{finDML}}, the \textit{\textbf{f}}airness \textit{\textbf{i}}n \textit{\textbf{n}}on-balanced \textit{\textbf{DML}} benchmark to characterize representation fairness. Utilizing \textit{finDML}, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (\textit{\textbf{\pad}}) to disentangle feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics.","['representation learning', 'fairness']",[],"['Natalie Dullerud', 'Karsten Roth', 'Kimia Hamidieh', 'Nicolas Papernot', 'Marzyeh Ghassemi']","['Stanford University', 'University of Tuebingen', 'Massachusetts Institute of Technology', 'University of Toronto', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6666,Fairness & Bias,Fairness Guarantees under Demographic Shift,"Recent studies have demonstrated that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes. To address this challenge, recent machine learning algorithms have been designed to limit the likelihood such unfair behaviors will occur. However, these approaches typically assume the data used for training is representative of what will be encountered once the model is deployed, thus limiting their usefulness. In particular, if certain subgroups of the population become more or less probable after the model is deployed (a phenomenon we call demographic shift), the fair-ness assurances provided by prior algorithms are often invalid. We consider the impact of demographic shift and present a class of algorithms, called Shifty algorithms, that provide high-confidence behavioral guarantees that hold under demographic shift. Shifty is the first technique of its kind and demonstrates an effective strategy for designing algorithms to overcome the challenges demographic shift poses. We evaluate Shifty-ttest, an implementation of Shifty based on Student’s 𝑡-test, and, using a real-world data set of university entrance exams and subsequent student success, show that the models output by our algorithm avoid unfair bias under demo-graphic shift, unlike existing methods. Our experiments demonstrate that our algorithm’s high-confidence fairness guarantees are valid in practice and that our algorithm is an effective tool for training models that are fair when demographic shift occurs.",['machine learning'],[],"['Stephen Giguere', 'Blossom Metevier', 'Bruno Castro da Silva', 'Yuriy Brun', 'Philip S. Thomas', 'Scott Niekum']","['University of Texas, Austin', 'University of Massachusetts, Amherst', 'University of Massachusetts, Amherst', 'University of Massachusetts Amherst', 'College of Information and Computer Science, University of Massachusetts, Amherst', 'University of Massachusetts at Amherst']",[]
https://iclr.cc/virtual/2022/poster/6820,Fairness & Bias,MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining,"Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \& recall by 4.12\% \& 3.01\% and decreases gender bias by 41.2\%, without requiring labels or retraining.","['data augmentation', 'fairness']",[],"['Ahmed Imtiaz Humayun', 'Randall Balestriero', 'Richard Baraniuk']","['Google', 'Facebook', 'William Marsh Rice University']",[]
https://iclr.cc/virtual/2022/poster/7085,Fairness & Bias,Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations,"Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable, easy-to-use, and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N (jointly we call them CIFAR-N), equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels we collected from Amazon Mechanical Turk. We quantitatively and qualitatively show that real-world noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g.,  class-dependent label noise). We then initiate an effort to benchmarking a subset of the existing solutions using  CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are available at http://noisylabels.com.","['benchmark', 'Learning with noisy labels']",[],"['Jiaheng Wei', 'Zhaowei Zhu', 'Hao Cheng', 'Tongliang Liu', 'Gang Niu', 'Yang Liu']","['University of California, Santa Cruz', 'Docta.ai', 'University of California, Santa Cruz', 'University of Sydney', 'RIKEN', 'University of California, Santa Cruz']",[]
https://iclr.cc/virtual/2022/poster/6347,Fairness & Bias,FairCal: Fairness Calibration for Face Verification,"Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models, thus avoiding the cost of retraining. However, they still have drawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages.","['calibration', 'bias', 'clustering', 'fairness']",[],"['Tiago Salvador', 'Stephanie Cairns', 'Vikram Voleti', 'Noah Marshall', 'Adam Oberman']","['McGill University', 'McGill University', 'Stability AI', 'McGill University', 'McGill University']",[]
https://iclr.cc/virtual/2022/poster/6839,Fairness & Bias,Generalized Demographic Parity for Group Fairness,"This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation. Current fairness metrics for continuous sensitive attributes largely rely on intractable statistical independence between variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information. Statistical fairness metrics estimation relying on either tractable bounds or neural network approximation, however, are not sufficiently trustful to rank algorithms prediction bias due to lack of estimation accuracy guarantee. To make fairness metrics trustable, we propose \textit{\underline{G}eneralized \underline{D}emographic \underline{P}arity} (GDP), a group fairness metric for continuous and discrete attributes. We show the understanding of GDP from the probability perspective and theoretically reveal the connection between GDP regularizer and adversarial debiasing. To estimate GDP, we adopt hard and soft group strategies via the one-hot or the soft group indicator, representing the membership of each sample in different groups of the sensitive attribute. We provably and numerically show that the soft group strategy achieves a faster estimation error convergence rate. Experiments show the better bias mitigation performance of GDP regularizer, compared with adversarial debiasing, for regression and classification tasks in tabular and graph benchmarks.",[],[],"['Zhimeng Jiang', 'Xiaotian Han', 'Chao Fan', 'Fan Yang', 'Ali Mostafavi', 'Xia Hu']","['Texas A&M University', 'Texas A&M University', 'Clemson University', 'Wake Forest University', 'Texas A&M', 'Rice University']",[]
https://iclr.cc/virtual/2022/poster/6276,Fairness & Bias,Visual Representation Learning Does Not Generalize Strongly Within the Same Domain,"An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world.In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark.In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets.Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization.","['generalization', 'Composition', 'disentanglement']",[],"['Julius von Kügelgen', 'Frederik Träuble', 'Peter Vincent Gehler', 'Chris Russell', 'Matthias Bethge', 'Francesco Locatello', 'Wieland Brendel']","['Max Planck Institute for Intelligent Systems, Max-Planck Institute', ', Max Planck Institute for Intelligent Systems', 'Zalando SE', 'Amazon', 'University of Tuebingen', 'Institute of Science and Technology', 'ELLIS Institute Tübingen']",[]
https://iclr.cc/virtual/2022/poster/6557,Fairness & Bias,Controlling Directions Orthogonal to a Classifier,"We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier",['invariance'],[],"['Yilun Xu', 'Hao He', 'Tianxiao Shen', 'Tommi Jaakkola']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Washington', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/7045,Fairness & Bias,Fair Normalizing Flows,"Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets.",['fairness'],[],"['Mislav Balunovic', 'Anian Ruoss', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'DeepMind', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6796,Privacy & Data Governance,"Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How","Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question remains yet open: how to model and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) itself? To close such a gap, the prerequisite is to consider the optimizers as sampled from a distribution, rather than a few prefabricated and fixed update rules. We first take the novel angle to consider the algorithmic space of optimizers, and provide definitions for the optimizer prior and likelihood, that intrinsically determine the posterior and therefore uncertainty. We then leverage the recent advance of learning to optimize (L2O) for the space parameterization, with the end-to-end training pipeline built via variational inference, referred to as uncertainty-aware L2O (UA-L2O). Our study represents the first effort to recognize and quantify the uncertainty of the optimization algorithm. The extensive numerical results show that, UA-L2O achieves superior uncertainty calibration with accurate confidence estimation and tight confidence intervals, suggesting the improved posterior estimation thanks to considering optimizer uncertainty. Intriguingly, UA-L2O even improves optimization performances for two out of three test functions, the loss function in data privacy attack, and four of five cases of the energy function in protein docking. Our codes are released at https://github.com/Shen-Lab/Bayesian-L2O.",[],[],"['Yuning You', 'Yue Cao', 'Tianlong Chen', 'Zhangyang Wang', 'Yang Shen']","['Texas A&M University', 'Texas A&M', 'Massachusetts Institute of Technology', 'University of Texas at Austin', 'Texas A&M University - College Station']",[]
https://iclr.cc/virtual/2022/poster/6513,Privacy & Data Governance,Increasing the Cost of Model Extraction with Calibrated Proof of Work,"In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.","['Model stealing', 'deep learning', 'model extraction', 'adversarial machine learning']",[],"['Muhammad Ahmad Kaleem', 'Yu Shen Lu', 'Nicolas Papernot']","['University of Toronto', 'Toronto University', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6471,Privacy & Data Governance,Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters,"The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.",[],[],"['Qiang Meng', 'Feng Zhou', 'Hainan Ren', 'Guochao Liu', 'Yuanqing Lin']","['DiDi', 'NEC Labs', 'Aibee', 'DiDi', 'Aibee Inc.']",[]
https://iclr.cc/virtual/2022/poster/6094,Privacy & Data Governance,A Zest of LIME: Towards Architecture-Independent Model Distances,"Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because it implies similarity of predictions in the limit, it suffers from being inapplicable to comparing models with different architectures. On the other hand, the similarity of predictions is broadly applicable but depends heavily on the choice of model inputs during comparison. In this paper, we instead propose to compute distance between black-box models by comparing their Local Interpretable Model-Agnostic Explanations (LIME). To compare two models, we take a reference dataset, and locally approximate the models on each reference point with linear models trained by LIME. We then compute the cosine distance between the concatenated weights of the linear models. This yields an approach that is both architecture-independent and possesses the benefits of comparing models in weight space. We empirically show that our method, which we call Zest, can be applied to two problems that require measurements of model similarity: detecting model stealing and machine unlearning.",['Model stealing'],[],"['Hengrui Jia', 'Hongyu Chen', 'Jonas Guan', 'Ali Shahin Shamsabadi', 'Nicolas Papernot']","['Toronto University', 'Cohere', 'Department of Computer Science, University of Toronto', 'Brave Software', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6035,Privacy & Data Governance,Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning,"The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \url{https://github.com/fshp971/robust-unlearnable-examples}.","['adversarial training', 'Unlearnable Examples', 'privacy']",[],"['Shaopeng Fu', 'Fengxiang He', 'Yang Liu', 'Li Shen', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'Tsinghua University, Tsinghua University', 'JD Explore Academy', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6934,Privacy & Data Governance,Bayesian Framework for Gradient Leakage,"Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem.","['privacy', 'federated learning']",[],"['Mislav Balunovic', 'Dimitar Iliev Dimitrov', 'Robin Staab', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6703,Privacy & Data Governance,Shuffle Private Stochastic Convex Optimization,"In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means, histograms, and counts. In this work, we present interactive shuffle protocols for stochastic convex optimization. Our optimization protocols rely on a new noninteractive protocol for summing vectors of bounded $\ell_2$ norm. By combining this sum subroutine with techniques including mini-batch stochastic gradient descent, accelerated gradient descent, and Nesterov's smoothing method, we obtain loss guarantees for a variety of convex loss functions that significantly improve on those of the local model and sometimes match those of the central model.",['differential privacy'],[],"['Albert Cheu', 'Matthew Joseph', 'Jieming Mao', 'Binghui Peng']","['Google', 'Google', 'Google', 'Columbia University']",[]
https://iclr.cc/virtual/2022/poster/6020,Privacy & Data Governance,Knowledge Removal in Sampling-based Bayesian Inference,"The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, {\it i.e.}, Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an {\it MCMC influence function} is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at \url{https://github.com/fshp971/mcmc-unlearning}.","['Markov chain Monte Carlo', 'bayesian inference']",[],"['Shaopeng Fu', 'Fengxiang He', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6746,Privacy & Data Governance,Hyperparameter Tuning with Renyi Differential Privacy,"For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.",['differential privacy'],[],"['Nicolas Papernot', 'Thomas Steinke']","['University of Toronto', 'Google']",[]
https://iclr.cc/virtual/2022/poster/6796,Security,"Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How","Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question remains yet open: how to model and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) itself? To close such a gap, the prerequisite is to consider the optimizers as sampled from a distribution, rather than a few prefabricated and fixed update rules. We first take the novel angle to consider the algorithmic space of optimizers, and provide definitions for the optimizer prior and likelihood, that intrinsically determine the posterior and therefore uncertainty. We then leverage the recent advance of learning to optimize (L2O) for the space parameterization, with the end-to-end training pipeline built via variational inference, referred to as uncertainty-aware L2O (UA-L2O). Our study represents the first effort to recognize and quantify the uncertainty of the optimization algorithm. The extensive numerical results show that, UA-L2O achieves superior uncertainty calibration with accurate confidence estimation and tight confidence intervals, suggesting the improved posterior estimation thanks to considering optimizer uncertainty. Intriguingly, UA-L2O even improves optimization performances for two out of three test functions, the loss function in data privacy attack, and four of five cases of the energy function in protein docking. Our codes are released at https://github.com/Shen-Lab/Bayesian-L2O.",[],[],"['Yuning You', 'Yue Cao', 'Tianlong Chen', 'Zhangyang Wang', 'Yang Shen']","['Texas A&M University', 'Texas A&M', 'Massachusetts Institute of Technology', 'University of Texas at Austin', 'Texas A&M University - College Station']",[]
https://iclr.cc/virtual/2022/poster/6513,Security,Increasing the Cost of Model Extraction with Calibrated Proof of Work,"In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and model utility for legitimate users. Instead, we propose to impede model extraction by requiring users to complete a proof-of-work before they can read the model's predictions. This deters attackers by greatly increasing (even up to 100x) the computational effort needed to leverage query access for model extraction. Since we calibrate the effort required to complete the proof-of-work to each query, this only introduces a slight overhead for regular users (up to 2x). To achieve this, our calibration applies tools from differential privacy to measure the information revealed by a query. Our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.","['Model stealing', 'deep learning', 'model extraction', 'adversarial machine learning']",[],"['Muhammad Ahmad Kaleem', 'Yu Shen Lu', 'Nicolas Papernot']","['University of Toronto', 'Toronto University', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6471,Security,Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters,"The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly  due to the particularity of the task, \textit{i.e.},  broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo leads to more discriminative features. The proposed schemes are mathematically proved to be differential private, introduce a lightweight overhead as well as yield prominent performance boosts (\textit{e.g.}, +9.63\% and +10.26\% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method.",[],[],"['Qiang Meng', 'Feng Zhou', 'Hainan Ren', 'Guochao Liu', 'Yuanqing Lin']","['DiDi', 'NEC Labs', 'Aibee', 'DiDi', 'Aibee Inc.']",[]
https://iclr.cc/virtual/2022/poster/6094,Security,A Zest of LIME: Towards Architecture-Independent Model Distances,"Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because it implies similarity of predictions in the limit, it suffers from being inapplicable to comparing models with different architectures. On the other hand, the similarity of predictions is broadly applicable but depends heavily on the choice of model inputs during comparison. In this paper, we instead propose to compute distance between black-box models by comparing their Local Interpretable Model-Agnostic Explanations (LIME). To compare two models, we take a reference dataset, and locally approximate the models on each reference point with linear models trained by LIME. We then compute the cosine distance between the concatenated weights of the linear models. This yields an approach that is both architecture-independent and possesses the benefits of comparing models in weight space. We empirically show that our method, which we call Zest, can be applied to two problems that require measurements of model similarity: detecting model stealing and machine unlearning.",['Model stealing'],[],"['Hengrui Jia', 'Hongyu Chen', 'Jonas Guan', 'Ali Shahin Shamsabadi', 'Nicolas Papernot']","['Toronto University', 'Cohere', 'Department of Computer Science, University of Toronto', 'Brave Software', 'University of Toronto']",[]
https://iclr.cc/virtual/2022/poster/6035,Security,Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning,"The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \url{https://github.com/fshp971/robust-unlearnable-examples}.","['adversarial training', 'Unlearnable Examples', 'privacy']",[],"['Shaopeng Fu', 'Fengxiang He', 'Yang Liu', 'Li Shen', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'Tsinghua University, Tsinghua University', 'JD Explore Academy', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6934,Security,Bayesian Framework for Gradient Leakage,"Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem.","['privacy', 'federated learning']",[],"['Mislav Balunovic', 'Dimitar Iliev Dimitrov', 'Robin Staab', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology', 'ETHZ - ETH Zurich', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2022/poster/6703,Security,Shuffle Private Stochastic Convex Optimization,"In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means, histograms, and counts. In this work, we present interactive shuffle protocols for stochastic convex optimization. Our optimization protocols rely on a new noninteractive protocol for summing vectors of bounded $\ell_2$ norm. By combining this sum subroutine with techniques including mini-batch stochastic gradient descent, accelerated gradient descent, and Nesterov's smoothing method, we obtain loss guarantees for a variety of convex loss functions that significantly improve on those of the local model and sometimes match those of the central model.",['differential privacy'],[],"['Albert Cheu', 'Matthew Joseph', 'Jieming Mao', 'Binghui Peng']","['Google', 'Google', 'Google', 'Columbia University']",[]
https://iclr.cc/virtual/2022/poster/6020,Security,Knowledge Removal in Sampling-based Bayesian Inference,"The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, {\it i.e.}, Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an {\it MCMC influence function} is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at \url{https://github.com/fshp971/mcmc-unlearning}.","['Markov chain Monte Carlo', 'bayesian inference']",[],"['Shaopeng Fu', 'Fengxiang He', 'Dacheng Tao']","['King Abdullah University of Science and Technology', 'University of Edinburgh', 'University of Sydney']",[]
https://iclr.cc/virtual/2022/poster/6746,Security,Hyperparameter Tuning with Renyi Differential Privacy,"For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.",['differential privacy'],[],"['Nicolas Papernot', 'Thomas Steinke']","['University of Toronto', 'Google']",[]