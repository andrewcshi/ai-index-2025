link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://doi.org/10.1145/3531146.3533237,Transparency & Explainability,How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India,"Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.","['algorithmic accountability', 'algorithmic fairness', 'human-ai interaction', 'instant loans', 'socio-technical systems']","['Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability']","['Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan']","['Computer Science and Engineering, University of Michigan, Ann Arbor', 'School of Information, University of Michigan, Ann Arbor', 'Google Research', 'Unaffiliated']","['USA', 'USA', 'India', 'USA']"
https://doi.org/10.1145/3531146.3534639,Transparency & Explainability,A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods,"The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.","['explainability', 'interpretability', 'explainable artificial intelligence', 'XAI', 'transparency', 'taxonomy', 'review']","['General and reference _ Surveys and overviews', 'Computing methodologies _ Artificial intelligence']",['Timo Speith'],"['Institute of Philosophy and Department of Computer Science, Saarland University']",['Germany']
https://doi.org/10.1145/3531146.3533201,Transparency & Explainability,Accountable Data: The Politics and Pragmatics of Disclosure Datasets,"This paper attends specifically to what I call “disclosure datasets” - tabular datasets produced in accordance with laws requiring various kinds of disclosure. For the purposes of this paper, the most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Through a series of case studies of disclosure datasets in the United States, I specifically draw attention to two concerns with disclosure datasets: First, for disclosure datasets, there is often political and social mobilization around the definitions that determine reporting thresholds, which in turn implicates what observations end up in the dataset. Changes in reporting thresholds can be traced along changes in political party power as the aims to promote accountability through mandated disclosure often get pitted against the aims to reduce regulatory burden. Second, for disclosure datasets, the observational unit – what is ultimately being counted in the data – is often not a person, institution, or action but instead a form that the reporting institution is required by law to fill out. Forms infrastructure the information that ends up in the dataset in notable ways. This work contributes to recent calls to promote the transparency and accountability of data science work through improved inquiry into and documentation of the social lineages of source datasets. The analysis of disclosure datasets presented in this paper poses important questions regarding what ultimately gets documented in the data, along with the representativeness and usefulness of these accountability mechanisms.","['disclosure', 'accountability', 'infrastructure', 'data provenance']","['Theory of computation _ Data provenance', 'Theory of computation _ Incomplete', 'inconsistent', 'and uncertain databases', 'Information systems _ Data dictionaries']",['Lindsay Poirier'],"['Statistical and Data Sciences, Smith College']",['USA']
https://doi.org/10.1145/3531146.3533780,Transparency & Explainability,AI Ethics Statements: Analysis and Lessons Learnt from NeurIPS Broader Impact Statements,"Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.","['ethics statements', 'broader impacts', 'research ethics', 'conference policies', 'AI governance', 'NeurIPS policies']",['Human-centered computing _ HCI design and evaluation methods'],"['Carolyn Ashurst', 'Emmie Hine', 'Paul Sedille', 'Alexis Carlier']","['Alan Turing Institute', 'Oxford Internet Institute', 'Harvard Kennedy School', 'Centre for the Governance of AI']","['United Kingdom', 'USA', 'USA', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533204,Transparency & Explainability,Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",[],[],"['Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho']","['Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533212,Transparency & Explainability,Algorithms Off-limits?: If digital trade law restricts access to source code of software then accountability will suffer,"Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.","['Software', 'Source code', 'Computer algorithms', 'Application Programming Interface', 'International trade law', 'Digital policy', 'Transparency', 'Fairness', 'Accountability']","['Social and professional topics', 'Computing / technology policy', 'Government technology policy', 'Governmental regulations']",['Kristina Irion'],"['Institute for Information Law, University of Amsterdam']",['Netherlands']
https://doi.org/10.1145/3531146.3533102,Transparency & Explainability,An Outcome Test of Discrimination for Ranked Lists,"This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1",[],[],"['Jonathan Roth', 'Guillaume Saint-Jacques', 'YinYin Yu']","['Brown University', 'Apple', 'LinkedIn']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533174,Transparency & Explainability,Auditing for Gerrymandering by Identifying Disenfranchised Individuals,"Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina’s spatial structure.","['gerrymandering', 'fairness', 'audit', 'simulated annealing']","['Theory of computation _ Random search heuristics', 'Applied computing _ Law']","['Jerry Lin', 'Carolyn Chen', 'Marc Chmielewski', 'Samia Zaman', 'Brandon Fain']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Computer Science, Duke University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533176,Transparency & Explainability,Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world,"Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.","['brain computer interfaces', 'human rights', 'neurological privacy', 'autonomy', 'identity']",[],['Marietjie Wilhelmina Maria Botes'],"['Computer Sciences, SnT Interdisciplinary Centre for Security Reliability and Trust']",['Luxembourg']
https://doi.org/10.1145/3531146.3533219,Transparency & Explainability,Characterizing Properties and Trade-offs of Centralized Delegation Mechanisms in Liquid Democracy,"Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.","['liquid democracy', 'voting', 'computational social choice', 'fairness', 'accountability', 'transparency']","['Applied computing _ Voting / election technologies', 'Applied computing _ E-government', 'Theory of computation _ Algorithmic game theory and mechanism design']","['Brian Brubach', 'Audrey Ballarin', 'Heeba Nazeer']","['Computer Science Department, Wellesley College', 'Wellesley College', 'Wellesley College']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533133,Transparency & Explainability,Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability,"There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims.  This paper argues for a paradigm shift towards reconceptualising disclosures as ‘interfaces’ – designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.","['transparency', 'accountability', 'GDPR', 'document engineering', 'interfaces', 'data rights', 'usability']","['Security and privacy _ Human and societal aspects of security and privacy', 'Human-centered computing', 'Human-centered computing _ Interaction design']","['Chris Norval', 'Kristin Cornelius', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group, University of Cambridge', 'Informatics Department, UCLA, Thousand Oaks, California, United States', 'Compliant & Accountable Systems Group, University of Cambridge', 'Compliant & Accountable Systems Group, University of Cambridge']","['United Kingdom', 'USA', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533112,Transparency & Explainability,Equi-explanation Maps: Concise and Informative Global Summary Explanations,"We attempt to summarize the model logic of a black-box classification model in order to generate concise and informative global explanations. We propose equi-explanation maps, a new explanation data-structure that presents the region of interest as a union of equi-explanation subspaces along with their explanation vectors. We then propose E-Map, a method to generate equi-explanation maps. We demonstrate the broad utility of our approach by generating equi-explanation maps for various binary classification models (Logistic Regression, SVM, MLP, and XGBoost) on the UCI Heart disease dataset and the Pima Indians diabetes dataset. Each subspace in our generated map is the union of d-dimensional hyper-cuboids which can be compactly represented for the sake of interpretability. For each of these subspaces, we present linear explanations assigning a weight to each explanation feature. We justify the use of equi-explanation maps in comparison to other global explanation methods by evaluating in terms of interpretability, fidelity, and informativeness. A user study further corroborates the use of equi-explanation maps to generate compact and informative global explanations.","['explainability', 'subspace interpretability', 'global explanations', 'explaining classifiers', 'model-logic subspaces']","['Computer systems organization _ Embedded systems', 'Redundancy', 'Computer systems organization~Robotics', 'Networks~Network reliability']","['Tanya Chowdhury', 'Razieh Rahimi', 'James Allan']","['CIIR, CICS, UMass Amherst', 'CIIR, CICS, UMass Amherst', 'CIIR, CICS, UMass Amherst']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533076,Transparency & Explainability,FAccT-Check on AI regulation: Systematic Evaluation of AI Regulation on the Example of the Legislation on the Use of AI in the Public Sector in the German Federal State of Schleswig-Holstein,"In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein “IT Deployment Law” (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.","['AI regulation', 'AI fairness', 'AI transparency']",['Computing methodologies _ Artificial intelligence'],['Katharina Simbeck'],['HTW Berlin'],['Germany']
https://doi.org/10.1145/3531146.3533074,Transparency & Explainability,Fairness Indicators for Systematic Assessments of Visual Feature Extractors,"Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds.  Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models.  To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.","['Fairness', 'Computer Vision', 'benchmarks', 'metrics']","['Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Computer vision']","['Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier']","['Meta', 'Meta', 'Meta', 'Meta', 'Meta']","['USA', 'Canada', 'USA', 'France', 'France']"
https://doi.org/10.1145/3531146.3533156,Transparency & Explainability,German AI Start-Ups and �AI Ethics�: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation,"The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context.  This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency.  In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines.  Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.","['AI ethics', 'start-ups', 'social practice', 'organizations', 'fairness', 'accountability', 'transparency', 'innovation', 'regulation', 'socio-cultural history']","['Social and professional topics', 'Social and professional topics _ Codes of ethics', 'Socio-technical systems', 'Socio-technical systems']","['Mona Sloane', 'Janina Zakrzewski']","['University of Tübingen, Germany and New York University', 'University of Tübingen']","['USA', 'Germany']"
https://doi.org/10.1145/3531146.3533202,Transparency & Explainability,How Explainability Contributes to Trust in AI,"We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as “explainability fosters trust in AI,” that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called “trust as anti-monitoring,” that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that “explainability fosters trust in AI” if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user’s trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.","['artificial intelligence', 'explainable artificial intelligence', 'trust', 'healthcare', 'trustworthiness', 'ethics of artificial intelligence']","['Human-centered computing _ HCI theory', 'concepts and models', 'Applied computing _ Sociology', 'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Artificial intelligence']","['Andrea Ferrario', 'Michele Loi']","['ETH Zurich', 'Politecnico di Milano']","['Switzerland', 'Italy']"
https://doi.org/10.1145/3531146.3533108,Transparency & Explainability,Interactive Model Cards: A Human-Centered Approach to Model Documentation,"Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability & interpretability; sensemaking & skepticism; and trust & safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.","['model cards', 'human centered design', 'interactive data visualization']","['Computing methodologies _ Natural language processing', 'Human-centered computing _ Visualization', 'Human-centered computing _ Human computer interaction (HCI)', 'Interaction design process and methods']","['Anamaria Crisan', 'Margaret Drouhard', 'Jesse Vig', 'Nazneen Rajani']","['Tableau Research', 'Tableau Software', 'Salesforce Research', 'Salesforce Research']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533090,Transparency & Explainability,It�s Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy,"To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque.  Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!","['machine learning', 'explainability', 'public policy', 'responsible AI']","['Human-centered computing _ Human computer interaction (HCI)', 'Computing methodologies _ Machine learning']","['Andrew Bell', 'Ian Solano-Kamaiko', 'Oded Nov', 'Julia Stoyanovich']","['New York University', 'New York University', 'New York University', 'New York University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533205,Transparency & Explainability,"Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms","Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.","['algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice']",[],"['Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour']","['University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3534630,Transparency & Explainability,Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting,"Counterfactual explanations (CFEs) highlight changes to a model’s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo","['XAI', 'Counterfactual Explanations', 'Quantitative User Study', 'Algorithm Evaluation', 'Human Factors']",[],"['Ulrike Kuhl', 'André Artelt', 'Barbara Hammer']","['HammerLab for Machine Learning, Bielefeld University', 'HammerLab for Machine Learning, Bielefeld University', 'HammerLab for Machine Learning, Bielefeld University']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533779,Transparency & Explainability,Limits and Possibilities for �Ethical AI� in Open Source: A Study of Deepfakes,"Open source software communities are a significant site of AI development, but “Ethical AI” discourses largely focus on the problems that arise in software produced by private companies. Design, policy and tooling interventions to encourage “Ethical AI” based on studies in private companies risk being ill-suited for an open source context, which operates under radically different organizational structures, cultural norms, and incentives.  In this paper, we show that significant and understudied harms and possibilities originate from differing practices of transparency and accountability in the open source community. We conducted an interview study of an AI-enabled open source Deepfake project to understand how members of that community reason about the ethics of their work. We found that notions of the “Freedom 0” to use code without any restriction, alongside beliefs about technology neutrality and technological inevitability, were central to how community members framed their responsibilities, and the actions they believed were and were not available to them. We propose a continuum between harms resulting from how a system is implemented versus how it is used, and show how commitments to radical transparency in open source allow great ethical scrutiny for harms wrought by implementation bugs, but allow harms through (mis)use to proliferate, requiring a deeper toolbox for disincentivizing harmful use. We discuss how an assumption of control over downstream uses is often implicit in discourses of “Ethical AI”, but outline alternative possibilities for action in cases such as open source where this assumption may not hold.","['deepfakes', 'ethics', 'open source', 'free software', 'agency', 'responsibility', 'interview']","['Human-centered computing _ Empirical studies in HCI', 'Social and professional topics _ Socio-technical systems', 'Security and privacy _ Social aspects of security and privacy']","['David Gray Widder', 'Dawn Nafus', 'Laura Dabbish', 'James Herbsleb']","['School of Computer Science, Carnegie Mellon University', 'Intel Labs, Intel Corporation', 'School of Computer Science, Carnegie Mellon University', 'School of Computer Science, Carnegie Mellon University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533235,Transparency & Explainability,Model Explanations with Differential Privacy,"Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.","['Differential Privacy', 'Model Explainations']",[],"['Neel Patel', 'Reza Shokri', 'Yair Zick']","['Viterbi School of engineering, University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst']","['USA', 'Singapore', 'USA']"
https://doi.org/10.1145/3531146.3533149,Transparency & Explainability,"Model Multiplicity: Opportunities, Concerns, and Solutions","Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?","['Model multiplicity', 'predictive multiplicity', 'procedural multiplicity', 'fairness', 'discrimination', 'recourse', 'arbitrariness']","['Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Machine learning', 'Theory of computation _ Machine learning theory', 'General and reference _ Evaluation', 'General and reference _ Performance']","['Emily Black', 'Manish Raghavan', 'Solon Barocas']","['Carngie Mellon University', 'Harvard SEAS', 'Microsoft Research']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533224,Transparency & Explainability,NeuroView-RNN: It�s About Time,"Recurrent Neural Networks (RNNs) are important tools for processing sequential data such as time-series or video. Interpretability is defined as the ability to be understood by a person and is different from explainability, which is the ability to be explained in a mathematical formulation. A key interpretability issue with RNNs is that it is not clear how each hidden state per time step contributes to the decision-making process in a quantitative manner. We propose NeuroView-RNN as a family of new RNN architectures that explains how all the time steps are used for the decision-making process. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier. The global linear classifier has all the hidden states as the input, so the weights of the classifier have a linear mapping to the hidden states. Hence, from the weights, NeuroView-RNN can quantify how important each time step is to a particular decision. As a bonus, NeuroView-RNN also offers higher accuracy in many cases compared to the RNNs and their variants. We showcase the benefits of NeuroView-RNN by evaluating on a multitude of diverse time-series datasets.","['Recurrent neural networks', 'interpretability', 'time series']",[],"['Cj Barberan', 'Sina Alemmohammad', 'Naiming Liu', 'Randall Balestriero', 'Richard Baraniuk']","['Rice University', 'Rice University', 'Rice University', 'Rice University', 'Rice University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533077,Transparency & Explainability,News from Generative Artificial Intelligence Is Believed Less,"Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.","['generative artificial intelligence', 'algorithmic transparency', 'fairness', 'news', 'news generation']","['Human-centered computing _ Empirical studies in HCI', 'Computing methodologies~Cognitive science', 'General and reference _ Empirical studies', 'Applied computing _ Psychology']","['Chiara Longoni', 'Andrey Fradkin', 'Luca Cian', 'Gordon Pennycook']","['Boston University, Questrom School of Business', 'Boston University, Questrom School of Business', 'University of Virginia, Darden School of Business', 'Hill/Levene Schools of Business, University of Regina']","['USA', 'USA', 'USA', 'Canada']"
https://doi.org/10.1145/3531146.3533232,Transparency & Explainability,On the Existence of Simpler Machine Learning Models,"It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.","['Rashomon Set', 'Model Multiplicity', 'Simplicity', 'Generalization', 'Interpretable Machine Learning']","['Computing methodologies _ Supervised learning by classification', 'Computing methodologies _ Classification and regression trees', 'Human-centered computing _ Empirical studies in visualization']","['Lesia Semenova', 'Cynthia Rudin', 'Ronald Parr']","['Duke University', 'Duke University', 'Duke University']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533099,Transparency & Explainability,Measuring Representational Harms in Image Captioning,"Previous work has largely considered the fairness of image captioning systems through the underspecified lens of “bias.” In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.","['fairness measurement', 'image captioning', 'harm propagation']","['Social and professional topics _ User characteristics', 'Computing methodologies _ Computer vision problems', 'Computing methodologies _ Natural language processing']","['Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach']","['Princeton University', 'Microsoft Research', 'Microsoft', 'Microsoft Research']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533153,Transparency & Explainability,Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts,"Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to “explain”. Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law’s objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying “explainability” obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union’s draft Artificial Intelligence Act.","['Explainability', 'Transparency', 'Regulation', 'Artificial Intelligence Act', 'GDPR', 'Counterfactual Explanations', 'SHAP', 'LIME']",[],"['Sebastian Bordt', 'Michèle Finck', 'Eric Raidl', 'Ulrike von Luxburg']","['Department of Computer Science, University of Tübingen', 'Law Faculty, University of Tübingen', 'Ethics and Philosophy Lab, University of Tübingen', 'Department of Computer Science, University of Tübingen']","['Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533151,Transparency & Explainability,Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications,"Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies.  Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.","['Communication Analysis', 'Visual Analytics', 'Intelligence Analysis', 'Ethic Awareness', 'Science & Technology Studies', 'Critical Algorithm Studies', 'Critical Data Studies', 'Machine Learning', 'Interdisciplinary Research']","['Computing methodologies _ Visual analytics', 'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Natural language processing']","['Maximilian T. Fischer', 'Simon David Hirsbrunner', 'Wolfgang Jentner', 'Matthias Miller', 'Daniel A. Keim', 'Paula Helm']","['University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of Tübingen', 'University of Konstanz', 'University of Konstanz', 'University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of Tübingen']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533170,Transparency & Explainability,Rational Shapley Values,"Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce rational Shapley values, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons.","['Explainable artificial intelligence', 'Interpretable machine learning', 'Shapley values', 'Counterfactuals', 'Decision theory']","['Theory of computation _ Machine learning theory', 'Mathematics of computing _ Probability and statistics']",['David Watson'],"['Department of Statistical Science, University College London']",['United Kingdom']
https://doi.org/10.1145/3531146.3533138,Transparency & Explainability,Robots Enact Malignant Stereotypes,"Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",[],[],"['Andrew Hundt', 'William Agnew', 'Vicky Zeng', 'Severin Kacianka', 'Matthew Gombolay']","['School of Interactive Computing, Georgia Institute of Technology', 'University of Washington', 'Johns Hopkins University', 'Technical University of Munich', 'School of Interactive Computing, Georgia Institute of Technology']","['USA', 'USA', 'USA', 'Germany', 'USA']"
https://doi.org/10.1145/3531146.3533135,Transparency & Explainability,Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory,"Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact—an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick’s sensemaking theory, which focuses on who the explanation is intended for. Recent work has advocated for the importance of understanding stakeholders’ needs—we build on this by providing concrete properties (e.g., identity, social context, environmental cues, etc.) that shape human understanding. We use an application of sensemaking in organizations as a template for discussing design guidelines for sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.","['interpretability', 'explainability', 'sensemaking', 'organizations']","['Human-centered computing _ HCI theory', 'concepts and models', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning']","['Harmanpreet Kaur', 'Eytan Adar', 'Eric Gilbert', 'Cliff Lampe']","['University of Michigan', 'University of Michigan', 'University of Michigan', 'University of Michigan']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533194,Transparency & Explainability,Confronting Power and Corporate Capture at the FAccT Conference,"Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.","['pymetrics', 'conflict of interest', 'corporate capture', 'industry engagement', 'research funding', 'agonism']","['Social and professional topics _ Codes of ethics', 'Social and professional topics _ Funding', 'Political speech', 'Governmental regulations']","['Meg Young', 'Michael Katell', 'P.M. Krafft']","['Cornell Tech', 'The Alan Turing Institute', 'University of the Arts London']","['USA', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533186,Transparency & Explainability,The Algorithmic Imprint,"When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.","['Algorithmic Imprint', 'Algorithmic Impact Assessment', 'Situated Fairness', 'Infrastructure', 'Global South', 'Folk Theories of Algorithms', 'User Perceptions']",['Human-centered computing _ Collaborative and social computing'],"['Upol Ehsan', 'Ranjit Singh', 'Jacob Metcalf', 'Mark Riedl']","['Georgia Institute of Technology', 'AI on the Ground Initiative, Data & Society Research Institute', 'AI on the Ground Initiative, Data & Society Research Institute', 'Georgia Institute of Technology']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3534628,Transparency & Explainability,The Conflict Between Explainable and Accountable Decision-Making Algorithms,"Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility.","['Responsibility', 'Accountability', 'Explainability', 'Artificial Intelligence', 'AI', 'Decision-Making', 'Algorithms', 'Blame', 'Designers', 'Patients', 'Users']","['Applied computing _ Law', 'Applied computing _ Psychology', 'Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence']","['Gabriel Lima', 'Nina Grgić-Hlača', 'Jin Keun Jeong', 'Meeyoung Cha']","['School of Computing, KAIST, Republic of Korea and Data Science Group, Institute for Basic Science', 'Max Planck Institute for Software Systems, Germany and Max Planck Institute for Research on Collective Goods', 'School of Law, Kangwon National University', 'Data Science Group, Institute for Basic Science, Republic of Korea and School of Computing, KAIST']","['Republic of Korea', 'Germany', 'Republic of Korea', 'Republic of Korea']"
https://doi.org/10.1145/3531146.3533179,Transparency & Explainability,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","['explainability', 'machine learning', 'fairness']","['Computing methodologies _ Machine learning', 'Human-centred computing _ explanations']","['Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute']","['USA', 'USA', 'Canada', 'USA', 'Canada', 'Canada']"
https://doi.org/10.1145/3531146.3533118,Transparency & Explainability,Towards a multi-stakeholder value-based assessment framework for algorithmic systems,"In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers’ and practitioners’ input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.","['values', 'ML development and deployment pipeline', 'algorithm assessment', 'multi-stakeholder']","['General and reference _ Evaluation', 'Human-centered computing _ Human computer interaction (HCI)', 'Social and professional topics _ User characteristics']","['Mireia Yurrita', 'Dave Murray-Rust', 'Agathe Balayn', 'Alessandro Bozzon']","['Human Information Communication Design, TU Delft', 'Human Information Communication Design, TU Delft', 'Web Information Systems, TU Delft', 'Knowledge and Intelligence Design, TU Delft']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://doi.org/10.1145/3531146.3533182,Transparency & Explainability,Designing for Responsible Trust in AI Systems: A Communication Perspective,"Current literature and public discourse on “trust in AI” are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments. Besides AI-generated content, we highlight transparency and interaction as AI systems’ affordances that present a wide range of trustworthiness cues to users. By bringing to light the variety of users’ cognitive processes to make trust judgments and their potential limitations, we urge technology creators to make conscious decisions in choosing reliable trustworthiness cues for target users and, as an industry, to regulate this space and prevent malicious use. Towards these goals, we define the concepts of warranted trustworthiness cues and expensive trustworthiness cues, and propose a checklist of requirements to help technology creators identify appropriate cues to use. We present a hypothetical use case to illustrate how practitioners can use MATCH to design AI systems responsibly, and discuss future directions for research and industry efforts aimed at promoting responsible trust in AI.","['Trust in AI', 'human-AI interaction', 'human-centered AI', 'AI design']",[],"['Q.Vera Liao', 'S. Shyam Sundar']","['Microsoft Research', 'Pennsylvania State University']","['Canada', 'USA']"
https://doi.org/10.1145/3531146.3533132,Transparency & Explainability,Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection,"Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide — gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process — with quantitative, qualitative and participatory steps — focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",[],[],"['Harini Suresh', 'Rajiv Movva', 'Amelia Lee Dogan', 'Rahul Bhargava', 'Isadora Cruxen', 'Angeles Martinez Cuba', 'Guilia Taurino', 'Wonyoung So', ""Catherine D'Ignazio""]","['Data + Feminism Lab, Massachusetts Institute of Technology, USA and CSAIL, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'School of Journalism, Northeastern University', 'School of Business and Management, Queen Mary University of London', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Khoury College of Computer Sciences, Northeastern University', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'United Kingdom', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3534638,Transparency & Explainability,Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat,"Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.","['machine translation', 'human-AI interaction', 'computer-mediated communication', 'explainable machine learning']",[],"['Samantha Robertson', 'Mark Díaz']","['University of California, Berkeley', 'Google']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533189,Transparency & Explainability,Why Am I Not Seeing It? Understanding Users� Needs for Counterfactual Explanations in Everyday Recommendations,"Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.",['Explainable recommender system; Counterfactual explanations; User studies'],['Information systems _ Recommender systems'],"['Ruoxi Shang', 'K. J. Kevin Feng', 'Chirag Shah']","['University of Washington', 'University of Washington', 'University of Washington']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533213,Transparency & Explainability,Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,"Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.","['AI audit', 'algorithm audit', 'audit', 'ethical AI', 'AI bias', 'AI harm', 'AI policy', 'algorithmic accountability']","['Social and professional topics _ Computing / technology policy', 'Human-centered computing']","['Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini']","['Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533237,Fairness & Bias,How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India,"Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.","['algorithmic accountability', 'algorithmic fairness', 'human-ai interaction', 'instant loans', 'socio-technical systems']","['Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability']","['Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan']","['Computer Science and Engineering, University of Michigan, Ann Arbor', 'School of Information, University of Michigan, Ann Arbor', 'Google Research', 'Unaffiliated']","['USA', 'USA', 'India', 'USA']"
https://doi.org/10.1145/3531146.3533072,Fairness & Bias,#FuckTheAlgorithm: algorithmic imaginaries and political resistance,"This paper applies and extends the concept of algorithmic imaginaries in the context of political resistance to sociotechnical injustice. Focusing on the 2020 UK OfQual protests, the role of the ”fuck the algorithm” chant is examined as an imaginary of resistance to confront power in sociotechnical systems. The protest is analysed as a shift in algorithmic imaginaries amidst evolving uses of #FuckTheAlgorithm on social media as part of everyday practices of resistance.",[],[],['Garfield Benjamin'],"['Sociology, Solent University']",['United Kingdom']
https://doi.org/10.1145/3531146.3533071,Fairness & Bias,A Data-driven analysis of the interplay between Criminological theory and predictive policing algorithms,"Previous studies have focused on the biases and feedback loops that occur in predictive policing algorithms. These studies show how systemically and institutionally biased data leads to these feedback loops when predictive policing algorithms are applied in real life. We take a step back, and show that the choice in algorithm can be embedded in a specific criminological theory, and that the choice of a model on its own even without biased data can create biased feedback loops. By synthesizing “historical” data, in which we control the relationships between crimes, location and time, we show that the current predictive policing algorithms create biased feedback loops even with completely random data. We then review the process of creation and deployment of these predictive systems, and highlight when good practices, such as fitting a model to data, “go bad” within the context of larger system development and deployment. Using best practices from previous work on assessing and mitigating the impact of new technologies, we highlight where the design of these algorithms has broken down. The study also found that multidisciplinary analysis of such systems is vital for uncovering these issues and shows that any study of equitable AI should involve a systematic and holistic analysis of their design rationalities.","['model design', 'impact of data on algorithms']","['Applied computing _ Law', 'social and behavioral sciences', 'Software and its engineering _ Designing software', 'Theory of computation _ Models of computation']","['Adriane Chapman', 'Philip Grylls', 'Pamela Ugwudike', 'David Gammack', 'Jacqui Ayling']","['University of Southampton', 'University of Southampton', 'University of Southampton', 'University of Southampton', 'University of Southampton']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533211,Fairness & Bias,ABCinML: Anticipatory Bias Correction in Machine Learning Applications,"The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.","['classification', 'fairness', 'algorithmic bias']",['Computing methodologies _ Batch learning'],"['Abdulaziz A. Almuzaini', 'Chidansh A. Bhatt', 'David M. Pennock', 'Vivek K. Singh']","['Rutgers University', 'IBM', 'Rutgers University', 'Rutgers University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533136,Fairness & Bias,Achieving Fairness via Post-Processing in Web-Scale Recommender Systems_,"Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally “qualified” (or “unqualified”) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.",[],[],"['Preetam Nandy', 'Cyrus DiCiccio', 'Divya Venugopalan', 'Heloise Logan', 'Kinjal Basu', 'Noureddine El Karoui']","['LinkedIn Corporation', 'While at LinkedIn Corporation', 'LinkedIn Corporation', 'LinkedIn Corporation', 'LinkedIn Corporation', 'While at LinkedIn Corporation']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533228,Fairness & Bias,Adversarial Scrutiny of Evidentiary Statistical Software,"The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights.  Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.","['evidentiary software', 'statistical software', 'adversarial scrutiny', 'black-box software', 'robust machine learning']","['Applied computing _ Law', 'Computing methodologies _ Machine learning', 'Software and its engineering']","['Rediet Abebe', 'Moritz Hardt', 'Angela Jin', 'John Miller', 'Ludwig Schmidt', 'Rebecca Wexler']","['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Max Planck Institute for Intelligent Systems', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Department of Electrical Engineering and Computer Sciences, University of California, Berkeley', 'Paul G. Allen School of Computer Science, University of Washington', 'School of Law, University of California, Berkeley']","['USA', 'Germany', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533115,Fairness & Bias,Affirmative Algorithms: Relational Equality as Algorithmic Fairness,"Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.","['fairness', 'algorithmic fairness', 'philosophy', 'relational equality', 'affirmative algorithms', 'criminal justice', 'pretrial risk assessments']","['Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law', 'social and behavioral sciences']",['Marilyn Zhang'],['Stanford University'],['USA']
https://doi.org/10.1145/3531146.3533204,Fairness & Bias,Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",[],[],"['Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho']","['Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533172,Fairness & Bias,An Algorithmic Framework for Bias Bounties,"We propose and analyze an algorithmic framework for “bias bounties” — events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1","['bias bounty', 'subgroup fairness', 'multigroup fairness']",['Theory of computation _ Machine learning theory'],"['Ira Globus-Harris', 'Michael Kearns', 'Aaron Roth']","['University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533102,Fairness & Bias,An Outcome Test of Discrimination for Ranked Lists,"This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1",[],[],"['Jonathan Roth', 'Guillaume Saint-Jacques', 'YinYin Yu']","['Brown University', 'Apple', 'LinkedIn']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533216,Fairness & Bias,Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus,"Content Warning: This paper contains content considered profane, hateful, and offensive.  Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator’s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument’s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator’s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.","['annotation', 'hate speech', 'annotator sensitivity', 'item response theory', 'differential rater functioning']","['Human-centered computing _ Empirical studies in collaborative and social computing', 'Human-centered computing _ Social media']","['Pratik S. Sachdeva', 'Renata Barreto', 'Claudia von Vacano', 'Chris J. Kennedy']","['D-Lab, University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'Harvard Medical School']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533121,Fairness & Bias,Best vs. All: Equity and Accuracy of Standardized Test Score Reporting,"We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type.  The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as “super-scoring”) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported.  We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student’s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can—at their option—either report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.","['algorithm fairness', 'screen accuracy', 'score reporting policies']","['Social and professional topics _ User characteristics', 'Theory of computation _ Theory and algorithms for application domains']","['Mingzi Niu', 'Sampath Kannan', 'Aaron Roth', 'Rakesh Vohra']","['Rice University', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533160,Fairness & Bias,Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US,"Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.","['fairness', 'mortgage lending', 'housing', 'racial wealth gap', 'reparations']","['Theory of computation _ Design and analysis of algorithms', 'Applied computing _ Sociology']","['Wonyoung So', 'Pranay Lohia', 'Rakesh Pimplikar', 'A.E. Hosoi', ""Catherine D'Ignazio""]","['Department of Urban Studies and Planning, Massachusetts Institute of Technology', 'Microsoft', 'IBM Research AI', 'Institute for Data, Systems, and Society, Massachusetts Institute of Technology', 'Department of Urban Studies and Planning, Massachusetts Institute of Technology']","['USA', 'India', 'India', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533089,Fairness & Bias,Bias in Automated Speaker Recognition,"Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.","['speaker recognition', 'speaker verification', 'bias', 'fairness', 'audit', 'evaluation']","['General and reference _ Evaluation', 'Security and privacy _ Biometrics', 'Computing methodologies _ Speech recognition', 'Computing methodologies~Machine learning']","['Wiebke Toussaint Hutiri', 'Aaron Yi Ding']","['Technology, Policy & Management / Engineering Systems & Services / Cyber Physical Intelligence Lab, Delft University of Technology', 'Technology, Policy & Management/Engineering Systems & Services / Cyber Physical Intelligence Lab, Delft University of Technology']","['Netherlands', 'Netherlands']"
https://doi.org/10.1145/3531146.3533241,Fairness & Bias,CounterFAccTual: How FAccT Undermines Its Organizing Principles,"This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles. We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‘metadata maximalism’. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems. In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.","['metadata maximalism', 'algorithmic realism', 'failures of FAccT', 'accountability infrastructures']","['Applied computing _ Law', 'Applied computing _ Sociology', 'General and reference _ Evaluation', 'Software and its engineering _ Documentation', 'Software and its engineering _ System administration']","['Ben Gansky', 'Sean McDonald']","['Human and Social Dimensions in Science and Technology, Arizona State University', 'Digital Public']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3534644,Fairness & Bias,Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems,"Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.","['computer vision', 'fairness', 'algorithmic bias', 'AI ethics', 'violence detection', 'law enforcement technology']","['Computing methodologies', 'Machine learning', 'Social and professional topics_Computing / technology policy', 'Surveillance_Governmental surveillance']","['Ioannis Pastaltzidis', 'Nikolaos Dimitriou', 'Katherine Quezada-Tavarez', 'Stergios Aidinlis', 'Thomas Marquenie', 'Agata Gurzawska', 'Dimitrios Tzovaras']","['Information Technologies Institute, Centre for Research and Technology Hellas', 'Information Technologies Institute, Centre for Research and Technology Hellas', 'Centre for IT and IP Law, KU Leuven', 'Trilateral Research', 'Centre for IT and IP Law, KU Leuven', 'Trilateral Research', 'Information Technologies Institute, Centre for Research and Technology Hellas']","['Greece', 'Greece', 'Belgium', 'United Kingdom', 'Belgium', 'United Kingdom', 'Greece']"
https://doi.org/10.1145/3531146.3533105,Fairness & Bias,De-biasing �bias� measurement,"When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.",[],"['Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence']","['Kristian Lum', 'Yunfeng Zhang', 'Amanda Bower']","['Twitter', 'Twitter', 'Twitter']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533226,Fairness & Bias,Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness,"Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.","['demographic data', 'sensitive data', 'categorization', 'fairness', 'discrimination', 'identity', 'race', 'gender', 'sexuality', 'measurement']","['Security and privacy _ Social aspects of security and privacy', 'Privacy protections', 'Economics of security and privacy', 'Social and professional topics _ User characteristics', 'Gender', 'Sexual orientation']","['McKane Andrus', 'Sarah Villeneuve']","['Partnership on AI', 'Partnership on AI']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533129,Fairness & Bias,Don�t let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution to the Hiring Paradox,"Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department’s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action.  In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a bias-aware technique can be used to diagnose and mitigate “built-in” headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating “people as numbers” to treating people as individuals—a property that is sought after by Title VII in the context of employment.","['anti-discrimination laws', 'hiring', 'resume screening', 'bias', 'uncertainty']","['Applied computing _ Law', 'Mathematics of computing _ Discrete mathematics']","['Jad Salem', 'Deven Desai', 'Swati Gupta']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533199,Fairness & Bias,Don�t Throw it Away! The Utility of Unlabeled Data in Fair Decision Making,"Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.","['fairness', 'decision making', 'label bias', 'selection bias', 'variational autoencoder', 'fair representation']","['Computing methodologies _ Machine learning algorithms', 'Computing methodologies _ Online learning settings', 'Social and professional topics']","['Miriam Rateike', 'Ayan Majumdar', 'Olga Mineeva', 'Krishna P. Gummadi', 'Isabel Valera']","['Max Planck Institute of Intelligent Systems, Tübingen, Germany and Saarland University', 'Max Planck Institute for Software Systems, Germany and Saarland University', 'ETH Zürich, Switzerland and Max Planck Institute for Intelligent Systems, Tübingen', 'Max Planck Institute for Software Systems', 'Saarland University, Germany and Max Planck Institute for Software Systems']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3534645,Fairness & Bias,Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency,"Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two).  We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity.  We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.","['algorithmic fairness', 'prediction-based decision making', 'constrained utility optimization', 'sufficiency', 'machine learning', 'group fairness metrics', 'fairness trade-offs']","['Computing methodologies _ Machine learning', 'Applied computing _ Decision analysis']","['Joachim Baumann', 'Anikó Hannák', 'Christoph Heitz']","['Universitiy of Zurich, Switzerland and Zurich University of Applied Sciences', 'Universitiy of Zurich', 'Zurich University of Applied Sciences']","['Switzerland', 'Switzerland', 'Switzerland']"
https://doi.org/10.1145/3531146.3533185,Fairness & Bias,Evidence for Hypodescent in Visual Semantic AI,"We examine the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with ”person,” with Pearson’s ρ as high as 0.82, p < 10− 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson’s ρ = 0.48, p < 10− 90 for 21,000 Black-White multiracial male images, and ρ = 0.41, p < 10− 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.","['multimodal', 'bias in AI', 'visual semantics', 'language-image models', 'racial bias', 'hypodescent']",['Computing methodologies _ Artificial intelligence'],"['Robert Wolfe', 'Mahzarin R. Banaji', 'Aylin Caliskan']","['University of Washington', 'Harvard University', 'University of Washington']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533113,Fairness & Bias,Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits,"Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.",[],[],"['Wesley Hanwen Deng', 'Manish Nagireddy', 'Michelle Seng Ah Lee', 'Jatinder Singh', 'Zhiwei Steven Wu', 'Kenneth Holstein', 'Haiyi Zhu']","['Human-Computer Interaction Institute, Carnegie Mellon University', 'Carnegie Mellon University', 'University of Cambridge', 'University of Cambridge', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","['USA', 'USA', 'United Kingdom', 'United Kingdom', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533144,Fairness & Bias,Exploring the Role of Grammar and Word Choice in Bias Toward African American English (AAE) in Hate Speech Classification,"Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.","['Natural Language Processing', 'Linguistics', 'Fairness', 'African American English', 'Hate Speech', 'Social Media']","['Computing methodologies _ Natural language processing', 'Computing methodologies _ Information extraction', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Race and ethnicity']","['Camille Harris', 'Matan Halevy', 'Ayanna Howard', 'Amy Bruckman', 'Diyi Yang']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'The Ohio State University', 'Georgia Institute of Technology', 'Georgia Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533167,Fairness & Bias,FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes,"Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.","['fairness', 'counterfactual', 'ensemble learning', 'semiparametric']",['Computing methodologies _ Ensemble methods'],"['Alan Mishler', 'Edward H. Kennedy']","['JP Morgan AI Research', 'Department of Statistics & Data Science, Carnegie Mellon University']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533126,Fairness & Bias,Fairness for AUC via Feature Augmentation,"We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.","['Classification', 'Area under the ROC curve (AUC)', 'feature augmentation', 'data collection and curation']",['Computing methodologies _ Supervised learning by classification'],"['Hortense Fong', 'Vineet Kumar', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'Yale University', 'Yale University', 'Yale University']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533074,Fairness & Bias,Fairness Indicators for Systematic Assessments of Visual Feature Extractors,"Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds.  Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models.  To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.","['Fairness', 'Computer Vision', 'benchmarks', 'metrics']","['Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Computer vision']","['Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier']","['Meta', 'Meta', 'Meta', 'Meta', 'Meta']","['USA', 'Canada', 'USA', 'France', 'France']"
https://doi.org/10.1145/3531146.3533225,Fairness & Bias,Fairness-aware Model-agnostic Positive and Unlabeled Learning,"With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.","['Fairness', 'Machine Learning', 'Positive and Unlabeled Learning']",[],"['Ziwei Wu', 'Jingrui He']","['School of Information Sciences, University of Illinois at Urbana-Champaign', 'School of Information Sciences, University of Illinois at Urbana-Champaign']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533159,Fairness & Bias,"Female, white, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces","Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.","['affective computing', 'action units', 'categorical emotions', 'metadata post-annotation', 'bias', 'fairness', 'data evaluation', 'algorithm evaluation']","['Computing methodologies _ Computer vision', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Age', 'Social and professional topics _ Race and ethnicity', 'Social and professional topics~Gender']","['Jaspar Pahl', 'Ines Rieger', 'Anna Möller', 'Thomas Wittenberg', 'Ute Schmid']","['Multimodal Human Sensing, Fraunhofer-Institute for Integrated Circuits IIS, Germany and Cognitive Systems Group, University of Bamberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Cognitive Systems Group, University of Bamberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Chair for Visual Computing, Friedrich-Alexander-Universität Erlangen-Nürnberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Chair for Visual Computing, Friedrich-Alexander-Universität Erlangen-Nürnberg', 'Cognitive Systems Group, University of Bamberg, Germany and Project Group Comprehensible AI, Fraunhofer-Institute for Integrated Circuits IIS']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533104,Fairness & Bias,Flipping the Script on Criminal Justice Risk Assessment: An actuarial model for assessing the risk the federal sentencing system poses to defendants,"In the criminal justice system, algorithmic risk assessment instruments are used to predict the risk a defendant poses to society; examples include the risk of recidivating or the risk of failing to appear at future court dates. However, defendants are also at risk of harm from the criminal justice system. To date, there exists no risk assessment instrument that considers the risk the system poses to the individual. We develop a risk assessment instrument that “flips the script.” Using data about U.S. federal sentencing decisions, we build a risk assessment instrument that predicts the likelihood an individual will receive an especially lengthy sentence given factors that should be legally irrelevant to the sentencing decision. To do this, we develop a two-stage modeling approach. Our first-stage model is used to determine which sentences were “especially lengthy.” We then use a second-stage model to predict the defendant’s risk of receiving a sentence that is flagged as especially lengthy given factors that should be legally irrelevant. The factors that should be legally irrelevant include, for example, race, court location, and other socio-demographic information about the defendant. Our instrument achieves comparable predictive accuracy to risk assessment instruments used in pretrial and parole contexts. We discuss the limitations of our modeling approach and use the opportunity to highlight how traditional risk assessment instruments in various criminal justice settings also suffer from many of the same limitations and embedded value systems of their creators.","['risk assessment', 'criminal justice', 'heteroscedastic Bayesian additive regression trees', 'LASSO', 'two-stage model', 'perspective reversal']",['Applied computing _ Law'],"['Mikaela Meyer', 'Aaron Horowitz', 'Erica Marshall', 'Kristian Lum']","['Department of Statistics & Data Science and Heinz College, Carnegie Mellon University', 'American Civil Liberties Union', 'Idaho Justice Project', 'Department of Computer and Information Science, University of Pennsylvania']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533184,Fairness & Bias,Gender and Racial Bias in Visual Question Answering Datasets,"Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.","['visual question answering', 'gender stereotype', 'racial stereotype', 'datasets']","['Social and professional topics _ Gender', 'Social and professional topics _ Race and ethnicity', 'Computing methodologies _ Computer vision', 'Computing methodologies _ Natural language processing']","['Yusuke Hirota', 'Yuta Nakashima', 'Noa Garcia']","['Osaka University', 'Osaka University', 'Osaka University']","['Japan', 'Japan', 'Japan']"
https://doi.org/10.1145/3531146.3533094,Fairness & Bias,GetFair: Generalized Fairness Tuning of Classification Models,"We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem.  The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric.  We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.","['Fair classification', 'fairness metrics', 'sensitive attribute', 'classifier models']","['Computing methodologies _ Machine learning', 'Social and professional topics _ User characteristics']","['Sandipan Sikdar', 'Florian Lemmerich', 'Markus Strohmaier']","['RWTH Aachen University', 'University of Passau', 'University of Mannheim, Germany and GESIS - Leibniz Institute for the Social Sciences']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533097,Fairness & Bias,How Different Groups Prioritize Ethical Values for Responsible AI,"Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”","['Responsible AI', 'value-sensitive design', 'empirical ethics']","['Human-centered computing _ Empirical studies in HCI', 'Social and professional topics _ Cultural characteristics', 'Social and professional topics _ Codes of ethics']","['Maurice Jakesch', 'Zana Buçinca', 'Saleema Amershi', 'Alexandra Olteanu']","['Cornell University', 'Harvard University', 'Microsoft Research', 'Microsoft Research']","['USA', 'USA', 'USA', 'Canada']"
https://doi.org/10.1145/3531146.3533140,Fairness & Bias,Imperfect Inferences: A Practical Assessment,"Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed.  We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels.  We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset.  Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person’s name might be best detected by inferences that rely on names.  In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.","['inference', 'demographics', 'race', 'discrimination', 'civil rights', 'fairness']",['Social and professional topics _ Race and ethnicity'],"['Aaron Rieke', 'Vincent Southerland', 'Dan Svirsky', 'Mingwei Hsu']","['Upturn', 'New York University School of Law', 'Uber Technologies, Inc', 'Upturn']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533245,Fairness & Bias,Is calibration a fairness requirement?: An argument from the point of view of moral philosophy and decision theory,"In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term “fairness” has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with “prima facie wrongful discrimination” in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.","['calibration', 'fairness', 'equalized odds', 'opportunity', 'prediction']",['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence'],"['Michele Loi', 'Christoph Heitz']","['Department of Mathematics, Politecnico di Milano', 'Institute of Data Analysis and Process, Zurich University of Applied Sciences']","['Italy', 'Switzerland']"
https://doi.org/10.1145/3531146.3533205,Fairness & Bias,"Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms","Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.","['algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice']",[],"['Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour']","['University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533117,Fairness & Bias,Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition,"All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.","['algorithmic bias', 'speech and language technologies', 'language variation', 'speech recognition']","['Computing methodologies _ Natural language processing', 'Computing methodologies _ Speech recognition']",['Nina Markl'],['University of Edinburgh'],['United Kingdom']
https://doi.org/10.1145/3531146.3534646,Fairness & Bias,Locality of Technical Objects and the Role of Structural Interventions for Systemic Change,"Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.","['sociotechnical systems', 'ethics', 'fairness', 'system dynamics', 'interventions', 'lending']","['Social and professional topics _ User characteristics', 'Computing methodologies _ Causal reasoning and diagnostics', 'Human-centered computing', 'Applied computing _ Sociology']","['Efrén Cruz Cortés', 'Sarah Rajtmajer', 'Debashis Ghosh']","['Michigan Institute of Data Science, Center for the Study of Complex Systems, University of Michigan', 'College of Information Sciences and Technology, Pennsylvania State University', 'Department of Biostatistics and Informatics, University of Colorado School of Public Health']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533183,Fairness & Bias,Markedness in Visual Semantic AI,"We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.","['multimodal', 'bias in AI', 'visual semantics', 'language-and-vision AI', 'markedness', 'age bias']",['Computing methodologies _ Artificial intelligence'],"['Robert Wolfe', 'Aylin Caliskan']","['University of Washington', 'University of Washington']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533236,Fairness & Bias,Marrying Fairness and Explainability in Supervised Learning,"Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.","['machine learning', 'explainability', 'algorithmic fairness', 'discrimination', 'supervised learning']","['Computing methodologies _ Machine learning algorithms', 'Applied computing~Law', 'social and behavioral sciences', 'Computing methodologies _ Supervised learning']","['Przemyslaw A. Grabowicz', 'Nicholas Perello', 'Aarshee Mishra']","['College of Information and Computer Sciences, University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3534641,Fairness & Bias,Measuring Fairness of Rankings under Noisy Sensitive Information,"Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",[],"['Information systems _ Information retrieval', 'Computing methodologies _ Machine learning']","['Azin Ghazimatin', 'Matthaus Kleindessner', 'Chris Russell', 'Ziawasch Abedjan', 'Jacek Golebiowski']","['Spotify', 'Amazon Web Services', 'Amazon Web Services', 'Leibniz Universitat Hannover, Germany and Amazon Search', 'Amazon Search']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533081,Fairness & Bias,Minimax Demographic Group Fairness in Federated Learning,"Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.","['Federated Learning', 'Algorithmic Fairness', 'Minimax Group Fairness']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Cooperation and coordination']","['Afroditi Papadaki', 'Natalia Martinez', 'Martin Bertran', 'Guillermo Sapiro', 'Miguel Rodrigues']","['University College London', 'Duke University', 'Duke University', 'Duke University, USA and Apple Inc.', 'University College London']","['United Kingdom', 'USA', 'USA', 'USA', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533149,Fairness & Bias,"Model Multiplicity: Opportunities, Concerns, and Solutions","Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?","['Model multiplicity', 'predictive multiplicity', 'procedural multiplicity', 'fairness', 'discrimination', 'recourse', 'arbitrariness']","['Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Machine learning', 'Theory of computation _ Machine learning theory', 'General and reference _ Evaluation', 'General and reference _ Performance']","['Emily Black', 'Manish Raghavan', 'Solon Barocas']","['Carngie Mellon University', 'Harvard SEAS', 'Microsoft Research']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533178,Fairness & Bias,Multi Stage Screening: Enforcing Fairness and Maximizing Efficiency in a Pre-Existing Pipeline,"Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others.  Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage.  We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‘price of’ adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.",[],[],"['Avrim Blum', 'Kevin Stangl', 'Ali Vakilian']","['TTIC', 'TTIC', 'TTIC']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533154,Fairness & Bias,Multi-disciplinary fairness considerations in machine learning for clinical trials,"While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.","['clinical trials', 'adaptive clinical trials', 'health informatics', 'machine learning for healthcare']","['Computing methodologies _ Machine learning', 'Applied computing _ Health informatics']","['Isabel Chien', 'Nina Deliu', 'Richard Turner', 'Adrian Weller', 'Sofia Villar', 'Niki Kilbertus']","['Machine Learning Group, University of Cambridge', 'MRC Biostatistics Unit, University of Cambridge, United Kingdom and MEMOTEF Department, Sapienza University of Rome', 'Machine Learning Group, University of Cambridge', 'Machine Learning Group, University of Cambridge, United Kingdom and The Alan Turing Institute', 'MRC Biostatistics Unit, University of Cambridge', 'Technical University of Munich, Germany and Helmholtz Munich']","['United Kingdom', 'Italy', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'Germany']"
https://doi.org/10.1145/3531146.3533166,Fairness & Bias,"Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare","A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.","['healthcare', 'fairness', 'cardiovascular disease']",['Applied computing _ Health informatics'],"['Stephen Pfohl', 'Yizhe Xu', 'Agata Foryciarz', 'Nikolaos Ignatiadis', 'Julian Genkins', 'Nigam Shah']","['Stanford University, USA and Google', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533152,Fairness & Bias,On the Fairness of Machine-Assisted Human Decisions,"When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.","['Protected Classes', 'Human Computer Interaction', 'Decision Support Systems', 'Fairness', 'Machine Learning']","['Human-centered computing _ HCI theory', 'concepts and models', 'Applied computing~Law', 'Applied computing~Economics']","['Bryce McLaughlin', 'Jann Spiess', 'Talia Gillis']","['Stanford Graduate School of Business', 'Stanford Graduate School of Business', 'Columbia Law School']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533209,Fairness & Bias,On the Power of Randomization in Fair Classification and Representation,"Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.",['fairness; demographic parity; equal opportunity; randomization; machine learning; classification; representation'],"['Theory of computation _ Unsupervised learning and clustering', 'Computing methodologies _ Supervised learning by classification', 'Learning latent representations']","['Sushant Agarwal', 'Amit Deshpande']","['University of Waterloo', 'Microsoft Research']","['Canada', 'India']"
https://doi.org/10.1145/3531146.3533099,Fairness & Bias,Measuring Representational Harms in Image Captioning,"Previous work has largely considered the fairness of image captioning systems through the underspecified lens of “bias.” In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.","['fairness measurement', 'image captioning', 'harm propagation']","['Social and professional topics _ User characteristics', 'Computing methodologies _ Computer vision problems', 'Computing methodologies _ Natural language processing']","['Angelina Wang', 'Solon Barocas', 'Kristen Laird', 'Hanna Wallach']","['Princeton University', 'Microsoft Research', 'Microsoft', 'Microsoft Research']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3534643,Fairness & Bias,People are not coins: Morally distinct types of predictions necessitate different fairness constraints,"In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices.” We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices.” Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.","['fairness metrics', 'algorithmic decision making', 'fair prediction', 'moral principles']",['Computing methodologies _ Artificial intelligence _ Philosophical/theoretical foundations of artificial intelligence'],"['Eleonora Viganò', 'Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']","['Institute of Biomedical Ethics and History of Medicine, University of Zurich', 'University of Zurich, Zurich University of Applied Sciences', 'Zurich University of Applied Sciences', 'Politecnico di Milano']","['Switzerland', 'Switzerland', 'Switzerland', 'Italy']"
https://doi.org/10.1145/3531146.3533155,Fairness & Bias,Prediction as Extraction of Discretion,"I argue that data-driven predictions work primarily as instruments for systematic extraction of discretionary power – the practical capacity to make everyday decisions and define one's situation. This extractive relation reprises a long historical pattern, in which new methods of producing knowledge generate a redistribution of epistemic power: who declares what kind of truth about me, to count for what kinds of decisions? I argue that prediction as extraction of discretion is normal and fundamental to the technology, rather than isolated cases of bias or error. Synthesising critical observations across anthropology, history of technology and critical data studies, the paper demonstrates this dynamic in two contemporary domains: (1) crime and policing demonstrates how predictive systems are extractive by design. Rather than neutral models led astray by garbage data, pre-existing interests thoroughly shape how prediction conceives of its object, its measures, and most importantly, what it does not measure and in doing so devalues. (2) I then examine the prediction of productivity in the long tradition of extracting discretion as a means to extract labour power. Making human behaviour more predictable for the client of prediction (the manager, the corporation, the police officer) often means making life and work more unpredictable for the target of prediction (the employee, the applicant, the citizen).",[],[],['Sun-ha Hong'],['Simon Fraser University'],['Canada']
https://doi.org/10.1145/3531146.3533151,Fairness & Bias,Promoting Ethical Awareness in Communication Analysis: Investigating Potentials and Limits of Visual Analytics for Intelligence Applications,"Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies.  Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science & Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.","['Communication Analysis', 'Visual Analytics', 'Intelligence Analysis', 'Ethic Awareness', 'Science & Technology Studies', 'Critical Algorithm Studies', 'Critical Data Studies', 'Machine Learning', 'Interdisciplinary Research']","['Computing methodologies _ Visual analytics', 'Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Natural language processing']","['Maximilian T. Fischer', 'Simon David Hirsbrunner', 'Wolfgang Jentner', 'Matthias Miller', 'Daniel A. Keim', 'Paula Helm']","['University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of Tübingen', 'University of Konstanz', 'University of Konstanz', 'University of Konstanz', 'International Center for Ethics in the Sciences (IZEW), University of Tübingen']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533079,Fairness & Bias,Providing Item-side Individual Fairness for Deep Recommender Systems,"Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.","['Individual fairness', 'deep recommender systems', 'algorithmic fairness in machine learning']","['Computing methodologies _ Machine learning', 'Information systems _ Data analytics']","['Xiuling Wang', 'Wendy Hui Wang']","['Stevens institute of technology', 'Stevens institute of technology']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533138,Fairness & Bias,Robots Enact Malignant Stereotypes,"Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",[],[],"['Andrew Hundt', 'William Agnew', 'Vicky Zeng', 'Severin Kacianka', 'Matthew Gombolay']","['School of Interactive Computing, Georgia Institute of Technology', 'University of Washington', 'Johns Hopkins University', 'Technical University of Munich', 'School of Interactive Computing, Georgia Institute of Technology']","['USA', 'USA', 'USA', 'Germany', 'USA']"
https://doi.org/10.1145/3531146.3533124,Fairness & Bias,Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints,"In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality.","['Implicit bias', 'selection', 'Intersectionality', 'Intersectional biases', 'Rooney Rule']",[],"['Anay Mehrotra', 'Bary S. R. Pradelski', 'Nisheeth K. Vishnoi']","['Yale University', 'National Centre for Scientific Research (CNRS)', 'Yale University']","['USA', 'France', 'USA']"
https://doi.org/10.1145/3531146.3533175,Fairness & Bias,Smallset Timelines: A Visual Representation of Data Preprocessing Decisions,"Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.","['data preprocessing', 'visualization', 'communication', 'open-source software', 'reflexivity']",['Human-centered computing _ Visualization toolkits'],"['Lydia R. Lucchesi', 'Petra M. Kuhnert', 'Jenny L. Davis', 'Lexing Xie']","[""Australian National University, Australia and CSIRO's Data61"", ""CSIRO's Data61, Australia and Australian National University"", 'Australian National University', ""Australian National University, Australia and CSIRO's Data61""]","['Australia', 'Australia', 'Australia', 'Australia']"
https://doi.org/10.1145/3531146.3533095,Fairness & Bias,Social Inclusion in Curated Contexts: Insights from Museum Practices,"Artificial intelligence literature suggests that minority and fragile communities in society can be negatively impacted by machine learning algorithms due to inherent biases in the design process, which lead to socially exclusive decisions and policies. Faced with similar challenges in dealing with an increasingly diversified audience, the museum sector has seen changes in theory and practice, particularly in the areas of representation and meaning-making. While rarity and grandeur used to be at the centre stage of the early museum practices, folk life and museums’ relationships with the diverse communities they serve become a widely integrated part of the contemporary practices. These changes address issues of diversity and accessibility in order to offer more socially inclusive services. Drawing on these changes and reflecting back on the AI world, we argue that the museum experience provides useful lessons for building AI with socially inclusive approaches, especially in situations in which both a collection and access to it will need to be curated or filtered, as frequently happens in search engines, recommender systems and digital libraries. We highlight three principles: (1) Instead of upholding the value of neutrality, practitioners are aware of the influences of their own backgrounds and those of others on their work. By not claiming to be neutral but practising cultural humility, the chances of addressing potential biases can be increased. (2) There should be room for situational interpretation beyond the stages of data collection and machine learning. Before applying models and predictions, the contexts in which relevant parties exist should be taken into account. (3) Community participation serves the needs of communities and has the added benefit of bringing practitioners and communities together.","['Curation', 'museums', 'libraries', 'social inclusion', 'diversity']","['General and reference _ Design', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Applied computing _ Arts and humanities']","['Han-Yin Huang', 'Cynthia C. S. Liem']","['Multimedia Computing Group, Delft University of Technology', 'Multimedia Computing Group, Delft University of Technology']","['Netherlands', 'Netherlands']"
https://doi.org/10.1145/3531146.3533128,Fairness & Bias,Subverting Fair Image Search with Generative Adversarial Perturbations,"In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model [75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation.  We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.","['Information Retrieval', 'Fair Ranking', 'Adversarial Machine Learning', 'Demographic Inference']","['Information systems _ Retrieval models and ranking', 'Security and privacy']","['Avijit Ghosh', 'Matthew Jagielski', 'Christo Wilson']","['Khoury College of Computer Sciences, Northeastern University', 'Google Brain', 'Khoury College of Computer Sciences, Northeastern University']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533169,Fairness & Bias,"Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis","Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.","['ethics of discrimination', 'persons with disabilities', 'reasonable accommodation', 'Artificial Intelligence Act', 'equality law', 'data protection law', 'automated hiring systems', 'algorithmic discrimination', 'social justice']","['Social and professional topics _ People with disabilities', 'Governmental regulations', 'Computing methodologies _ Artificial intelligence']","['Maarten Buyl', 'Christina Cociancig', 'Cristina Frattone', 'Nele Roekens']","['Ghent University', 'University of Bremen', 'Roma Tre University', 'Unia']","['Belgium', 'Germany', 'Italy', 'Belgium']"
https://doi.org/10.1145/3531146.3533186,Fairness & Bias,The Algorithmic Imprint,"When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.","['Algorithmic Imprint', 'Algorithmic Impact Assessment', 'Situated Fairness', 'Infrastructure', 'Global South', 'Folk Theories of Algorithms', 'User Perceptions']",['Human-centered computing _ Collaborative and social computing'],"['Upol Ehsan', 'Ranjit Singh', 'Jacob Metcalf', 'Mark Riedl']","['Georgia Institute of Technology', 'AI on the Ground Initiative, Data & Society Research Institute', 'AI on the Ground Initiative, Data & Society Research Institute', 'Georgia Institute of Technology']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533134,Fairness & Bias,The Death of the Legal Subject: How Predictive Algorithms Are (Re)constructing Legal Subjectivity,"This paper explores the epistemological differences between the socio-political legal subject of Western liberalism, and the algorithmic subject of informational capitalism. It argues that the increasing use of predictive algorithms in judicial decision-making is reconstructing both the nature and experience of legal subjectivity in a manner that is incompatible with law's normative commitments to individualized justice. Whereas algorithmic subjectivity derives its epistemic authority from population-level insights, legal subjectivity has historically derived credibility from its close approximation of the underlying individual, through careful evaluation of their mental and physical autonomy, prior to any assignment of legal liability. With the introduction of predictive algorithms in judicial decision-making, knowledge about the legal subject is increasingly algorithmically produced, in a manner that discounts, and effectively displaces, qualitative knowledge about the legal subject's intentions, motivations, and moral capabilities. This results in the death of the legal subject, or the emergence of new, algorithmic practices of signification that no longer require the input of the underlying individual. As algorithms increasingly guide judicial decision-making, the shifting epistemology of legal subjectivity has long-term consequences for the legitimacy of legal institutions.",[],[],['Katrina Geddes'],['New York University'],['USA']
https://doi.org/10.1145/3531146.3534629,Fairness & Bias,The Effects of Crowd Worker Biases in Fact-Checking Tasks,"Due to the increasing amount of information shared online every day, the need for sound and reliable ways of distinguishing between trustworthy and non-trustworthy information is as present as ever. One technique for performing fact-checking at scale is to employ human intelligence in the form of crowd workers. Although earlier work has suggested that crowd workers can reliably identify misinformation, cognitive biases of crowd workers may reduce the quality of truthfulness judgments in this context. We performed a systematic exploratory analysis of publicly available crowdsourced data to identify a set of potential systematic biases that may occur when crowd workers perform fact-checking tasks. Following this exploratory study, we collected a novel data set of crowdsourced truthfulness judgments to validate our hypotheses. Our findings suggest that workers generally overestimate the truthfulness of statements and that different individual characteristics (i.e., their belief in science) and cognitive biases (i.e., the affect heuristic and overconfidence) can affect their annotations. Interestingly, we find that, depending on the general judgment tendencies of workers, their biases may sometimes lead to more accurate judgments.","['Truthfulness', 'Crowdsourcing', 'Misinformation', 'Explainability', 'Bias']","['Information systems _ Crowdsourcing', 'General and reference _ Estimation']","['Tim Draws', 'David La Barbera', 'Michael Soprano', 'Kevin Roitero', 'Davide Ceolin', 'Alessandro Checco', 'Stefano Mizzaro']","['Delft University of Technology', 'University of Udine', 'University of Udine', 'University of Udine', 'CWI', 'University of Rome La Sapienza', 'University of Udine']","['Netherlands', 'Italy', 'Italy', 'Italy', 'Netherlands', 'Italy', 'Italy']"
https://doi.org/10.1145/3531146.3533157,Fairness & Bias,The Forgotten Margins of AI Ethics,"How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.","['AI Ethics', 'Trends', 'Justice', 'FAccT', 'AIES']","['Social and professional topics', 'Computing methodologies _ Artificial intelligence']","['Abeba Birhane', 'Elayne Ruane', 'Thomas Laurent', 'Matthew S. Brown', 'Johnathan Flowers', 'Anthony Ventresque', 'Christopher L. Dancy']","['Mozilla Foundation, USA and School of Computer Science, University College Dublin', 'School of Computer Science, University College Dublin', 'School of Computer Science, University College Dublin', 'School of Computer Science, Bucknell University', 'Worcester State University', 'School of Computer Science, University College Dublin', 'Dept of Industrial and Manufacturing Engineering & Dept of Computer Science and Engineering, Pennsylvania State University']","['Ireland', 'Ireland', 'Ireland', 'USA', 'USA', 'Ireland', 'USA']"
https://doi.org/10.1145/3531146.3533179,Fairness & Bias,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","['explainability', 'machine learning', 'fairness']","['Computing methodologies _ Machine learning', 'Human-centred computing _ explanations']","['Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute']","['USA', 'USA', 'Canada', 'USA', 'Canada', 'Canada']"
https://doi.org/10.1145/3531146.3534627,Fairness & Bias,Theories of �Gender� in NLP Bias Research,"The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time.  We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.","['natural language processing', 'gender bias', 'gender studies']",[],"['Hannah Devinney', 'Jenny Björklund', 'Henrik Björklund']","['Umeå University', 'Uppsala University', 'Umeå University']","['Sweden', 'Sweden', 'Sweden']"
https://doi.org/10.1145/3531146.3533118,Fairness & Bias,Towards a multi-stakeholder value-based assessment framework for algorithmic systems,"In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers’ and practitioners’ input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.","['values', 'ML development and deployment pipeline', 'algorithm assessment', 'multi-stakeholder']","['General and reference _ Evaluation', 'Human-centered computing _ Human computer interaction (HCI)', 'Social and professional topics _ User characteristics']","['Mireia Yurrita', 'Dave Murray-Rust', 'Agathe Balayn', 'Alessandro Bozzon']","['Human Information Communication Design, TU Delft', 'Human Information Communication Design, TU Delft', 'Web Information Systems, TU Delft', 'Knowledge and Intelligence Design, TU Delft']","['Netherlands', 'Netherlands', 'Netherlands', 'Netherlands']"
https://doi.org/10.1145/3531146.3533197,Fairness & Bias,Towards Fair Unsupervised Learning,"Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (“fGLRM”) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.","['Unsupervised Learning', 'PCA', 'Clustering', 'Fairness']","['Theory of computation _ Unsupervised learning and clustering', 'Social and professional topics _ User characteristics']","['Francois Buet-Golfouse', 'Islam Utyagulov']","['University College London', 'Independent']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533132,Fairness & Bias,Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection,"Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide — gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process — with quantitative, qualitative and participatory steps — focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",[],[],"['Harini Suresh', 'Rajiv Movva', 'Amelia Lee Dogan', 'Rahul Bhargava', 'Isadora Cruxen', 'Angeles Martinez Cuba', 'Guilia Taurino', 'Wonyoung So', ""Catherine D'Ignazio""]","['Data + Feminism Lab, Massachusetts Institute of Technology, USA and CSAIL, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'School of Journalism, Northeastern University', 'School of Business and Management, Queen Mary University of London', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Khoury College of Computer Sciences, Northeastern University', 'Data + Feminism Lab, Massachusetts Institute of Technology', 'Data + Feminism Lab, Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'United Kingdom', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533087,Fairness & Bias,Treatment Effect Risk: Bounds and Inference,"Since the average treatment effect (ATE) measures the change in social welfare, even if positive, there is a risk of negative effect on, say, some 10% of the population. Assessing such risk is difficult, however, because any one individual treatment effect (ITE) is never observed so the 10% worst-affected cannot be identified, while distributional treatment effects only compare the first deciles within each treatment group, which does not correspond to any 10%-subpopulation. In this paper we consider how to nonetheless assess this important risk measure, formalized as the conditional value at risk (CVaR) of the ITE-distribution. We leverage the availability of pre-treatment covariates and characterize the tightest-possible upper and lower bounds on ITE-CVaR given by the covariate-conditional average treatment effect (CATE) function. We then proceed to study how to estimate these bounds efficiently from data and construct confidence intervals. This is challenging even in randomized experiments as it requires understanding the distribution of the unknown CATE function, which can be very complex if we use rich covariates so as to best control for heterogeneity. We develop a debiasing method that overcomes this and prove it enjoys favorable statistical properties even when CATE and other nuisances are estimated by black-box machine learning or even inconsistently. Studying a hypothetical change to French job-search counseling services, our bounds and inference demonstrate a small social benefit entails a negative impact on a substantial subpopulation.","['Program evaluation', 'Individual treatment effect', 'Conditional average treatment effect', 'Conditional value at risk', 'Partial identification', 'Debiased machine learning']",[],['Nathan Kallus'],"['Cornell University, USA and Netflix']",['USA']
https://doi.org/10.1145/3531146.3533145,Fairness & Bias,Trucks Don�t Mean Trump: Diagnosing Human Error in Image Analysis,"Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.","['image analysis', 'human error', 'diagnosing bias']","['Human-centered computing _ HCI design and evaluation methods', 'Computing methodologies _ Machine learning approaches']","['J.D. Zamfirescu-Pereira', 'Jerry Chen', 'Emily Wen', 'Allison Koenecke', 'Nikhil Garg', 'Emma Pierson']","['UC Berkeley', 'Stanford University', 'Stanford University', 'Cornell University', 'Cornell Tech', 'Cornell University']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533243,Fairness & Bias,Uncertainty and the Social Planner�s Problem: Why Sample Complexity Matters,"Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner’s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.",[],[],['Cyrus Cousins'],"['Department of Computer Science, Brown University']",['USA']
https://doi.org/10.1145/3531146.3533242,Fairness & Bias,What is Proxy Discrimination?,"The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.","['proxy', 'discrimination', 'conceptual analysis']",['Social and professional topics _ Computing / technology policy'],['Michael Carl Tschantz'],['International Computer Science Institute'],['USA']
https://doi.org/10.1145/3531146.3533080,Fairness & Bias,What People Think AI Should Infer From Faces,"Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.","['artificial intelligence', 'computer vision', 'human faces', 'participatory AI ethics']","['Computing methodologies _ Computer vision tasks', 'Social and professional topics _ User characteristics', 'Security and privacy _ Social aspects of security and privacy']","['Severin Engelmann', 'Chiara Ullstein', 'Orestis Papakyriakopoulos', 'Jens Grossklags']","['Chair of Cyber Trust, Department of Informatics, Technical University Munich', 'Chair of Cyber Trust, Department of Informatics, Technical University Munich', 'Princeton University', 'Chair of Cyber Trust, Department of Informatics, Technical University Munich']","['Germany', 'Germany', 'USA', 'Germany']"
https://doi.org/10.1145/3531146.3533078,Fairness & Bias,When learning becomes impossible,"We formally analyze an epistemic bias we call interpretive blindness (IB), in which under certain conditions a learner will be incapable of learning. IB is now common in our society, but it is a natural consequence of Bayesian inference and what we argue are mild assumptions about the relation between belief and evidence. IB a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a codependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic of contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices.","['learning', 'bias', 'agent modeling', 'echo chambers', 'Bayesian learning', 'philosophical foundations']",[],"['Nicholas Asher', 'Julie Hunter']","['Institut de Recherche en Informatique de Toulouse, Centre Nationale de Recherche Scientifique', 'LINAGORA Labs']","['France', 'France']"
https://doi.org/10.1145/3531146.3533213,Fairness & Bias,Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,"Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.","['AI audit', 'algorithm audit', 'audit', 'ethical AI', 'AI bias', 'AI harm', 'AI policy', 'algorithmic accountability']","['Social and professional topics _ Computing / technology policy', 'Human-centered computing']","['Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini']","['Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533176,Privacy,Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world,"Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.","['brain computer interfaces', 'human rights', 'neurological privacy', 'autonomy', 'identity']",[],['Marietjie Wilhelmina Maria Botes'],"['Computer Sciences, SnT Interdisciplinary Centre for Security Reliability and Trust']",['Luxembourg']
https://doi.org/10.1145/3531146.3534637,Privacy,Data Governance in the Age of Large-Scale Data-Driven Language Technology,"The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.","['datasets', 'technology governance', 'data rights', 'language data']","['Social and professional topics', 'Social and professional topics _ Information system economics', 'Social and professional topics _ Digital rights management']","['Yacine Jernite', 'Huu Nguyen', 'Stella Biderman', 'Anna Rogers', 'Maraim Masoud', 'Valentin Danchev', 'Samson Tan', 'Alexandra Sasha Luccioni', 'Nishant Subramani', 'Isaac Johnson', 'Gerard Dupont', 'Jesse Dodge', 'Kyle Lo', 'Zeerak Talat', 'Dragomir Radev', 'Aaron Gokaslan', 'Somaieh Nikpoor', 'Peter Henderson', 'Rishi Bommasani', 'Margaret Mitchell']","['Hugging Face', 'Ontocord', 'EleutherAI', 'University of Copenhagen', 'Independent researcher', 'University of Essex', 'AWS AI Research & Education', 'Hugging Face', 'Allen Institute for Artificial Intelligence', 'Wikimedia', 'Independent', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Simon Fraser University', 'Yale University', 'Cornell University', 'CAIDP', 'Stanford University', 'Stanford University', 'Hugging Face']","['USA', 'USA', 'USA', 'Denmark', 'Ireland', 'United Kingdom', 'USA', 'Canada', 'USA', 'USA', 'France', 'USA', 'USA', 'Canada', 'USA', 'USA', 'Canada', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533226,Privacy,Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness,"Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.","['demographic data', 'sensitive data', 'categorization', 'fairness', 'discrimination', 'identity', 'race', 'gender', 'sexuality', 'measurement']","['Security and privacy _ Social aspects of security and privacy', 'Privacy protections', 'Economics of security and privacy', 'Social and professional topics _ User characteristics', 'Gender', 'Sexual orientation']","['McKane Andrus', 'Sarah Villeneuve']","['Partnership on AI', 'Partnership on AI']","['USA', 'USA']"
https://doi.org/10.1145/3531146.3533133,Privacy,Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability,"There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims.  This paper argues for a paradigm shift towards reconceptualising disclosures as ‘interfaces’ – designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.","['transparency', 'accountability', 'GDPR', 'document engineering', 'interfaces', 'data rights', 'usability']","['Security and privacy _ Human and societal aspects of security and privacy', 'Human-centered computing', 'Human-centered computing _ Interaction design']","['Chris Norval', 'Kristin Cornelius', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group, University of Cambridge', 'Informatics Department, UCLA, Thousand Oaks, California, United States', 'Compliant & Accountable Systems Group, University of Cambridge', 'Compliant & Accountable Systems Group, University of Cambridge']","['United Kingdom', 'USA', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3531146.3533148,Privacy,Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization,"Modern machine learning systems are increasingly characterized by extensive personal data collection, despite the diminishing returns and increasing societal costs of such practices. Yet, data minimisation is one of the core data protection principles enshrined in the European Union’s General Data Protection Regulation (’GDPR’) and requires that only personal data that is adequate, relevant and limited to what is necessary is processed. However, the principle has seen limited adoption due to the lack of technical interpretation.  In this work, we build on literature in machine learning and law to propose FIDO, a Framework for Inhibiting Data Overcollection. FIDO learns to limit data collection based on an interpretation of data minimization tied to system performance. Concretely, FIDO provides a data collection stopping criterion by iteratively updating an estimate of the performance curve, or the relationship between dataset size and performance, as data is acquired. FIDO estimates the performance curve via a piecewise power law technique that models distinct phases of an algorithm’s performance throughout data collection separately. Empirical experiments show that the framework produces accurate performance curves and data collection stopping criteria across datasets and feature acquisition algorithms. We further demonstrate that many other families of curves systematically overestimate the return on additional data. Results and analysis from our investigation offer deeper insights into the relevant considerations when designing a data minimization framework, including the impacts of active feature acquisition on individual users and the feasability of user-specific data minimization. We conclude with practical recommendations for the implementation of data minimization.",[],"['Social and professional topics _ Governmental regulations', 'Applied computing _ Law', 'Computing methodologies _ Feature selection', 'Computing methodologies~Online learning settings']","['Divya Shanmugam', 'Fernando Diaz', 'Samira Shabanian', 'Michele Finck', 'Asia Biega']","['MIT', 'Google', 'Microsoft Research, Canada', 'University of Tuebigen', 'Max Planck Institute for Security and Privacy']","['USA', 'USA', 'Canada', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533235,Privacy,Model Explanations with Differential Privacy,"Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.","['Differential Privacy', 'Model Explainations']",[],"['Neel Patel', 'Reza Shokri', 'Yair Zick']","['Viterbi School of engineering, University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst']","['USA', 'Singapore', 'USA']"
https://doi.org/10.1145/3531146.3534642,Privacy,What Does it Mean for a Language Model to Preserve Privacy?,"Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.","['Natural Language Processing', 'Privacy', 'Differential Privacy', 'Data Sanitization']","['Social and professional topics _ Privacy policies', 'Computing methodologies _ Natural language processing']","['Hannah Brown', 'Katherine Lee', 'Fatemehsadat Mireshghallah', 'Reza Shokri', 'Florian Tramèr']","['National University of Singapore', 'Cornell University', 'University of California, San Diego', 'National University of Singapore', 'Google Research']","['Singapore', 'USA', 'USA', 'Singapore', 'Switzerland']"
https://doi.org/10.1145/3531146.3533237,Security,How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India,"Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.","['algorithmic accountability', 'algorithmic fairness', 'human-ai interaction', 'instant loans', 'socio-technical systems']","['Computer systems organization _ Embedded systems', 'Computer systems organization~Robotics', 'Networks~Network reliability']","['Divya Ramesh', 'Vaishnav Kameswaran', 'Ding Wang', 'Nithya Sambasivan']","['Computer Science and Engineering, University of Michigan, Ann Arbor', 'School of Information, University of Michigan, Ann Arbor', 'Google Research', 'Unaffiliated']","['USA', 'USA', 'India', 'USA']"
https://doi.org/10.1145/3531146.3533115,Security,Affirmative Algorithms: Relational Equality as Algorithmic Fairness,"Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.","['fairness', 'algorithmic fairness', 'philosophy', 'relational equality', 'affirmative algorithms', 'criminal justice', 'pretrial risk assessments']","['Social and professional topics _ Computing / technology policy', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law', 'social and behavioral sciences']",['Marilyn Zhang'],['Stanford University'],['USA']
https://doi.org/10.1145/3531146.3533084,Security,AI Opacity and Explainability in Tort Litigation,"A spate of recent accidents and a lawsuit involving Tesla's ‘self-driving’ cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal ‘causes of action’ – the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.","['Explainable AI', 'Law', 'Evidence', 'Expert Evidence', 'AI Opacity', 'Civil Procedure', 'Negligence', 'Product Liability', 'Autonomous Vehicle', 'Accidents', 'Causation', 'Damages']","['Applied computing _ Law', 'Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence', 'Social and professional topics _ Computing / technology policy']","['Henry Fraser', 'Rhyle Simcock', 'Aaron J. Snoswell']","['Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre', 'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre', 'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre']","['Australia', 'Australia', 'Australia']"
https://doi.org/10.1145/3531146.3533204,Security,Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",[],[],"['Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho']","['Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533172,Security,An Algorithmic Framework for Bias Bounties,"We propose and analyze an algorithmic framework for “bias bounties” — events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1","['bias bounty', 'subgroup fairness', 'multigroup fairness']",['Theory of computation _ Machine learning theory'],"['Ira Globus-Harris', 'Michael Kearns', 'Aaron Roth']","['University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533176,Security,Brain Computer Interfaces and Human Rights: Brave new rights for a brave new world,"Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.","['brain computer interfaces', 'human rights', 'neurological privacy', 'autonomy', 'identity']",[],['Marietjie Wilhelmina Maria Botes'],"['Computer Sciences, SnT Interdisciplinary Centre for Security Reliability and Trust']",['Luxembourg']
https://doi.org/10.1145/3531146.3533188,Security,DualCF: Efficient Model Extraction Attack from Counterfactual Explanations,"Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.","['Counterfactual Explanations', 'Model Extraction Attack', 'Decision Boundary Shift', 'Model Security and Privacy']","['Security and privacy _ Software and application security', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Machine learning algorithms', 'Computing methodologies _ Reasoning about belief and knowledge']","['Yongjie Wang', 'Hangwei Qian', 'Chunyan Miao']","['School of Computer Science and Engineering, Nanyang Technological University', 'School of Computer Science and Engineering, Nanyang Technological University', 'School of Computer Science and Engineering, Nanyang Technological University']","['Singapore', 'Singapore', 'Singapore']"
https://doi.org/10.1145/3531146.3533074,Security,Fairness Indicators for Systematic Assessments of Visual Feature Extractors,"Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds.  Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models.  To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.","['Fairness', 'Computer Vision', 'benchmarks', 'metrics']","['Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Computer vision']","['Priya Goyal', 'Adriana Romero Soriano', 'Caner Hazirbas', 'Levent Sagun', 'Nicolas Usunier']","['Meta', 'Meta', 'Meta', 'Meta', 'Meta']","['USA', 'Canada', 'USA', 'France', 'France']"
https://doi.org/10.1145/3531146.3533108,Security,Interactive Model Cards: A Human-Centered Approach to Model Documentation,"Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability & interpretability; sensemaking & skepticism; and trust & safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.","['model cards', 'human centered design', 'interactive data visualization']","['Computing methodologies _ Natural language processing', 'Human-centered computing _ Visualization', 'Human-centered computing _ Human computer interaction (HCI)', 'Interaction design process and methods']","['Anamaria Crisan', 'Margaret Drouhard', 'Jesse Vig', 'Nazneen Rajani']","['Tableau Research', 'Tableau Software', 'Salesforce Research', 'Salesforce Research']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533069,Security,"Interdisciplinarity, Gender Diversity, and Network Structure Predict the Centrality of AI Organizations","Artificial intelligence (AI) research plays an increasingly important role in society, impacting key aspects of human life. From face recognition algorithms aiding national security in airports, to software that advises judges in criminal cases, and medical staff in healthcare, AI research is shaping critical facets of our experience in the world. But who are the people and institutional bodies behind this influential research? What are the predictors of influence of AI researchers and research organizations? We study this question using social network analysis, in an exploration of the structural characteristics, i.e., network topology, of research organizations that shape modern AI. In a sample of 149 organizations with 9,987 affiliated authors of published papers in a major AI conference (NeurIPS) and two major conferences that specifically focus on societal impacts of AI (FAccT and AIES), we find that both industry and academic research organizations with influential authors are more interdisciplinary, have a greater fraction of women, are more hierarchical, and less clustered, even when controlling for the size of the organizations. The influence is operationalized as betweenness centrality in co-authorship networks, i.e., how often an author is on the shortest path connecting any pair of authors, acting as a bridge connecting otherwise distant (or even disconneted) members of the network, such as their own co-authors who are not each other’s co-author themselves. Using this operationalization, we also find that women have less influence in the AI community, determined as lower betweenness centrality in co-authorship networks. These results suggest that while diverse AI institutions are more influential, the individuals contributing to the increased diversity are marginalized in the AI field. We discuss these results in the context of current events with important societal implications.","['organizational structure', 'artificial intelligence', 'gender diversity', 'interdisciplinarity']",[],"['Madalina Vlasceanu', 'Miroslav Dudik', 'Ida Momennejad']","['New York University', 'Microsoft Research', 'Microsoft Research']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533205,Security,"Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms","Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.","['algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice']",[],"['Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour']","['University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533073,Security,Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash,"Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system’s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.1","['perceptual hashing', 'client-side scanning', 'neuralhash', 'neural networks', 'deep learning', 'privacy']","['Computing methodologies _ Machine learning', 'Security and privacy _ Software and application security']","['Lukas Struppek', 'Dominik Hintersdorf', 'Daniel Neider', 'Kristian Kersting']","['Department of Computer Science, Technical University of Darmstadt', 'Department of Computer Science, Technical University of Darmstadt', 'Max Planck Institute for Software Systems, Germany and Safety and Explainability of Learning Systems, Carl von Ossietzky University of Oldenburg', 'Department of Computer Science and Centre for Cognitive Science, Technical University of Darmstadt, Germany and Hessian Center for Artificial Intelligence']","['Germany', 'Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3531146.3533128,Security,Subverting Fair Image Search with Generative Adversarial Perturbations,"In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model [75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation.  We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.","['Information Retrieval', 'Fair Ranking', 'Adversarial Machine Learning', 'Demographic Inference']","['Information systems _ Retrieval models and ranking', 'Security and privacy']","['Avijit Ghosh', 'Matthew Jagielski', 'Christo Wilson']","['Khoury College of Computer Sciences, Northeastern University', 'Google Brain', 'Khoury College of Computer Sciences, Northeastern University']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3531146.3533215,Security,System Safety and Artificial Intelligence,"This article formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social and institutional components of a system. This article honors system safety pioneer Nancy Leveson, by situating her core lessons for today’s AI system safety challenges [2]. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.  The article is a non-archival contribution to FAccT 2022, and will be published as a chapter to The Oxford Handbook of AI Governance [1]. The full article is available as a pre-print on ArXiv via  https://arxiv.org/abs/2202.09292.","['artificial intelligence', 'harms', 'audits', 'culture', 'safety', 'system safety', 'governance', 'policy', 'automation', 'systems and control']","['Computer systems organization _ Embedded and cyber-physical systems', 'Computing methodologies _ Artificial intelligence', 'Social and professional topics _ Government technology policy']",['Roel Dobbe'],"['Technology, Policy and Management, Delft University of Technology']",['Netherlands']
https://doi.org/10.1145/3531146.3533179,Security,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","['explainability', 'machine learning', 'fairness']","['Computing methodologies _ Machine learning', 'Human-centred computing _ explanations']","['Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute']","['USA', 'USA', 'Canada', 'USA', 'Canada', 'Canada']"
https://doi.org/10.1145/3531146.3533213,Security,Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,"Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.","['AI audit', 'algorithm audit', 'audit', 'ethical AI', 'AI bias', 'AI harm', 'AI policy', 'algorithmic accountability']","['Social and professional topics _ Computing / technology policy', 'Human-centered computing']","['Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini']","['Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League']","['USA', 'USA', 'USA']"