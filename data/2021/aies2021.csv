link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://arxiv.org/abs/2011.03654,Transparency & Explainability,Fair Machine Learning under Partial Compliance.,"Typically, fair machine learning research focuses on a single decisionmaker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decisionmakers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does the strategic behavior of decision subjects in partial compliance settings affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance (k% of employers) can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; and (4) partial compliance to local parity measures can induce extreme segregation.",[],[],"['Jessica Dai', 'Sina Fazelpour', 'Zachary C. Lipton']","['Brown University, Providence, RI, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/1907.00164,Transparency & Explainability,On the Privacy Risks of Model Explanations.,"Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",[],[],"['Reza Shokri', 'Martin Strobel', 'Yair Zick']","['National University of Singapore, Singapore, Singapore', 'National University of Singapore, Singapore, Singapore', 'University of Massachusetts, Amherst, Amherst, MA, USA']","['Singapore', 'Singapore', 'US']"
https://arxiv.org/abs/2105.00060,Transparency & Explainability,Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization.,"AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",[],[],"['Michael Anis Mihdi Afnan', 'Cynthia Rudin', 'Vincent Conitzer', 'Julian Savulescu', 'Abhishek Mishra', 'Yanhe Liu', 'Masoud Afnan']","['Imperial College London, London, United Kingdom', 'Duke University, Durham, NC, USA', 'Duke University & Oxford University, Durham, NC, USA', ""Oxford University & Oxford University & Royal Children's Hospital, Oxford, United Kingdom"", 'Oxford University, Oxford, United Kingdom', 'Monash IVF Group & University of Western Australia & Edith Cowan University, Southport, Australia', 'Qingdao United Family Hospital, Qingdao, China']","['United Kingdom', 'US', 'US', 'United Kingdom', 'United Kingdom', 'Australia', 'China']"
https://arxiv.org/abs/2101.02555,Transparency & Explainability,Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study.,"We study whether receiving advice from either a human or algorithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with a No-explanation alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.",[],[],"['Daniel Ben David', 'Yehezkel S. Resheff', 'Talia Tron']","['The Hebrew University of Jerusalem, Jerusalem, Israel', 'Holon Institude of Technology, Holon, Israel', 'Intuit Inc., Tel Aviv, Israel']","['Israel', 'Israel', 'Israel']"
https://arxiv.org/abs/2011.07586,Transparency & Explainability,"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty.","Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",[],[],"['Umang Bhatt', 'Javier Antorán', 'Yunfeng Zhang', 'Q. Vera Liao', 'Prasanna Sattigeri', 'Riccardo Fogliato', 'Gabrielle Gauthier Melançon', 'Ranganath Krishnan', 'Jason Stanley', 'Omesh Tickoo', 'Lama Nachman', 'Rumi Chunara', 'Madhulika Srikumar', 'Adrian Weller', 'Alice Xiang']","['University of Cambridge, Cambridge, United Kingdom', 'University of Cambridge, Cambridge, United Kingdom', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'IBM Research, Yorktown Heights, NY, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'ElementAI, Montreal, PQ, Canada', 'Intel, Portland, OR, USA', 'ElementAI, Montreal, PQ, Canada', 'Intel, Portland, OR, USA', 'Intel, Santa Clara, CA, USA', 'New York University, New York City, NY, USA', 'Partnership on AI, San Francisco, CA, USA', 'University of Cambridge, Cambridge, United Kingdom', 'Sony AI, Seattle, WA, USA']","['United Kingdom', 'United Kingdom', 'US', 'US', 'US', 'US', 'Canada', 'US', 'Canada', 'US', 'US', 'US', 'US', 'United Kingdom', 'US']"
https://arxiv.org/abs/2105.10174,Transparency & Explainability,Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing.,"We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 euros more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",[],[],"['Alessandro Fabris', 'Alan Mishler', 'Stefano Gottardi', 'Mattia Carletti', 'Matteo Daicampi', 'Gian Antonio Susto', 'Gianmaria Silvello']","['University of Padua, Padua, Italy', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'University of Padua, Padua, Italy', 'University of Padua, Padua, Italy', 'University of Udine, Udine, Italy', 'University of Padua, Padua, Italy', 'University of Padua, Padua, Italy']","['Italy', 'US', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy']"
https://arxiv.org/abs/2102.03977,Transparency & Explainability,Learning to Generate Fair Clusters from Demonstrations.,"Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement, leading to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",[],[],"['Sainyam Galhotra', 'Sandhya Saisubramanian', 'Shlomo Zilberstein']","['University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA', 'University of Massachusetts Amherst, Amherst, MA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2011.04917,Transparency & Explainability,Towards Unifying Feature Attribution and Counterfactual Explanations: Different Means to the Same End.,"Feature attributions and counterfactual explanations are popular approaches to explain a ML model. The former assigns an importance score to each input feature, while the latter provides input examples with minimal changes to alter the model's predictions. To unify these approaches, we provide an interpretation based on the actual causality framework and present two key results in terms of their use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which attribution-based methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complementarity of these two approaches. Our evaluation on three benchmark datasets - Adult-Income, LendingClub, and German-Credit - confirms the complementarity. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like Wachter et al. and DiCE often do not agree on feature importance rankings. In addition, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are often neither necessary nor sufficient explanations of a model's prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem",[],[],"['Ramaravind Kommiya Mothilal', 'Divyat Mahajan', 'Chenhao Tan', 'Amit Sharma']","['Microsoft Research India, Bangalore, India', 'Microsoft Research India, Bangalore, India', 'University of Chicago, Chicago, IL, USA', 'Microsoft Research India, Bangalore, India']","['India', 'India', 'US', 'India']"
https://arxiv.org/abs/2105.01434,Transparency & Explainability,Towards Accountability in the Use of Artificial Intelligence for Public Administrations.,"We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer to processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",[],[],"['Michele Loi', 'Matthias Spielkamp']","['University of Zurich, Zurich, Switzerland', 'AlgorithmWatch, Berlin, Germany']","['Switzerland', 'Germany']"
https://arxiv.org/abs/2006.00305,Transparency & Explainability,RelEx: A Model-Agnostic Relational Model Explainer.,"In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art results, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist one work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, a model-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.",[],[],"['Yue Zhang', 'David Defazio', 'Arti Ramesh']","['SUNY Binghamton, Binghamton, NY, USA', 'SUNY Binghamton, Binghamton, NY, USA', 'SUNY Binghamton, Binghamton, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2011.03654,Fairness & Bias,Fair Machine Learning under Partial Compliance.,"Typically, fair machine learning research focuses on a single decisionmaker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decisionmakers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does the strategic behavior of decision subjects in partial compliance settings affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance (k% of employers) can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; and (4) partial compliance to local parity measures can induce extreme segregation.",[],[],"['Jessica Dai', 'Sina Fazelpour', 'Zachary C. Lipton']","['Brown University, Providence, RI, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2011.03108,Fairness & Bias,Minimax Group Fairness: Algorithms and Experiments.,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.",[],[],"['Emily Diana', 'Wesley Gill', 'Michael Kearns', 'Krishnaram Kenthapadi', 'Aaron Roth']","['University of Pennsylvania, Amazon AWS AI, Philadelphia, PA, USA', 'The University of Pennsylvania, Amazon AWS AI, Philadelphia, PA, USA', 'University of Pennsylvania, Amazon AWS AI, Philadelphia, PA, USA', 'Amazon AWS AI, Sunnyvale, CA, USA', 'University of Pennsylvania & Amazon AWS AI, Philadelphia, PA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2105.04953,Fairness & Bias,On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes.,"The risk of re-offense is considered in decision-making at many stages of the criminal justice system, from pre-trial, to sentencing, to parole. To aid decision makers in their assessments, institutions increasingly rely on algorithmic risk assessment instruments (RAIs). These tools assess the likelihood that an individual will be arrested for a new criminal offense within some time window following their release. However, since not all crimes result in arrest, RAIs do not directly assess the risk of re-offense. Furthermore, disparities in the likelihood of arrest can potentially lead to biases in the resulting risk scores. Several recent validations of RAIs have therefore focused on arrests for violent offenses, which are viewed as being more accurate reflections of offending behavior. In this paper, we investigate biases in violent arrest data by analysing racial disparities in the likelihood of arrest for White and Black violent offenders. We focus our study on 2007--2016 incident-level data of violent offenses from 16 US states as recorded in the National Incident Based Reporting System (NIBRS). Our analysis shows that the magnitude and direction of the racial disparities depend on various characteristics of the crimes. In addition, our investigation reveals large variations in arrest rates across geographical locations and offense types. We discuss the implications of the observed disconnect between re-arrest and re-offense in the context of RAIs and the challenges around the use of data from NIBRS to correct for the sampling bias.",[],[],"['Riccardo Fogliato', 'Alice Xiang', 'Zachary Lipton', 'Daniel Nagin', 'Alexandra Chouldechova']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Sony AI, Seattle, WA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2006.03955,Fairness & Bias,Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-Like Biases.,"With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that appear in particular contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD and EIBD achieve high accuracy when detecting the intersectional and emergent biases of African American females and Mexican American females. Our results indicate that biases at the intersection of race and gender associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",[],[],"['Wei Guo', 'Aylin Caliskan']","['George Washington University, Washington, DC, USA', 'George Washington University and Institute for Data, Democracy & Politics, Washington, DC, USA']","['US', 'US']"
https://arxiv.org/abs/2102.13004,Fairness & Bias,Towards Unbiased and Accurate Deferral to Multiple Experts.,"Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on ""deferral systems"" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed learning framework outperforms baselines on this real-world dataset as well.",[],[],"['Vijay Keswani', 'Matthew Lease', 'Krishnaram Kenthapadi']","['Yale University, New Haven, CT, USA', 'University of Texas at Austin, Austin, TX, USA', 'Amazon AWS AI, East Palo Alto, CA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2012.03063,Fairness & Bias,FairOD: Fairness-Aware Outlier Detection.,"Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. However, when being a minority (as defined by protected variables, such as race/ethnicity/sex/age) does not reflect positive-class membership (such as criminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focuses on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector that has the following desirable properties: FairOD (1) exhibits treatment parity at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.",[],[],"['Shubhranshu Shekhar', 'Neil Shah', 'Leman Akoglu']","['Carnegie Mellon University, Pittsburgh, PA, USA', 'Snap Inc., Seattle, WA, USA', 'Carnegie Mellon University, Pittsburgh, PA, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2102.04257,Fairness & Bias,Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities.,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness--frequently, race and legal gender--can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",[],[],"['Nenad Tomasev', 'Kevin R. McKee', 'Jackie Kay', 'Shakir Mohamed']","['DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind & University College London, London, United Kingdom', 'DeepMind, London, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2009.13676,Fairness & Bias,"The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity.","As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.",[],[],"['Mohamed Abdalla', 'Moustafa Abdalla']","['University of Toronto, Toronto, ON, Canada', 'Harvard Medical School, Cambridge, MA, USA']","['Canada', 'US']âIâm Covered in Bloodâ: Persistent Anti-Muslim Bias in Large Language Models."
https://arxiv.org/abs/2103.03417,Fairness & Bias,Measuring Model Biases in the Absence of Ground Truth.,"The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice. We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a bag of words, we rank the biases that a model has learned with respect to different identity labels. We use (man, woman) as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most ""gender biased"" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.",[],[],"['Osman Aka', 'Ken Burke', 'Alex Bäuerle', 'Christina Greer', 'Margaret Mitchell']","['Max Planck Institute for Software Systems, Saarbrücken, Germany', 'Max Planck Institute for Informatics, Saarbrücken, Germany', 'Max Planck Institute for Software Systems, Saarbrücken, Germany']","['Germany', 'Germany', 'Germany']"
https://arxiv.org/abs/2105.04249,Fairness & Bias,Accounting for Model Uncertainty in Algorithmic Discrimination.,"Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize ``total'' error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further propose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets.",[],[],"['Junaid Ali', 'Preethi Lahoti', 'Krishna P. Gummadi']","['CYENS Centre of Excellence, Nicosia, Cyprus', 'CYENS Centre of Excellence, Nicosia, Cyprus', 'Cyprus Center for Algorithmic Transparency & Open University of Cyprus, Latsia, Cyprus', 'Cyprus Center for Algorithmic Transparency & Open University of Cyprus, Latsia, Cyprus']","['Cyprus', 'Cyprus', 'Cyprus', 'Cyprus']"
https://arxiv.org/abs/2011.07586,Fairness & Bias,"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty.","Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",[],[],"['Umang Bhatt', 'Javier Antorán', 'Yunfeng Zhang', 'Q. Vera Liao', 'Prasanna Sattigeri', 'Riccardo Fogliato', 'Gabrielle Gauthier Melançon', 'Ranganath Krishnan', 'Jason Stanley', 'Omesh Tickoo', 'Lama Nachman', 'Rumi Chunara', 'Madhulika Srikumar', 'Adrian Weller', 'Alice Xiang']","['Microsoft Research, Bangalore, India', 'Indian Institute of Science, Bangalore, India', 'Microsoft Research, Bangalore, India', 'Indian Institute of Science, Bangalore, India']","['India', 'India', 'India', 'India']"
https://arxiv.org/abs/2105.14890,Fairness & Bias,Rawlsian Fair Adaptation of Deep Learning Classifiers.,"Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.",[],[],"['Kulin Shah', 'Pooja Gupta', 'Amit Deshpande', 'Chiranjib Bhattacharyya']","['Harvard University, Cambridge, MA, USA', 'Microsoft Corporation, Redmond, WA, USA']","['US', 'US']"
https://arxiv.org/abs/2005.03474,Fairness & Bias,Ensuring Fairness under Prior Probability Shifts.,"In this paper, we study the problem of fair classification in the presence of prior probability shifts, where the training set distribution differs from the test set. This phenomenon can be observed in the yearly records of several real-world datasets, such as recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we propose a method, called CAPE, which provides a comprehensive solution to the aforementioned problem. CAPE makes novel use of prevalence estimation techniques, sampling and an ensemble of classifiers to ensure fair predictions under prior probability shifts. We introduce a metric, called prevalence difference (PD), which CAPE attempts to minimize in order to ensure PE-fairness. We theoretically establish that this metric exhibits several desirable properties. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several popular fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures PE-fair predictions, while performing well on other performance metrics.",[],[],"['Arpita Biswas', 'Suvam Mukherjee']","['Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA', 'Harvard University, Cambridge, MA, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2105.01774,Fairness & Bias,Envisioning Communities: A Participatory Approach towards AI for Social Good.,"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",[],[],"['Elizabeth Bondi', 'Lily Xu', 'Diana Acosta-Navas', 'Jackson A. Killian']","['Florida International University, Miami, FL, USA', 'Tulane University, New Orleans, LA, USA', 'Case Western Reserve University, Cleveland, OH, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.10174,Fairness & Bias,Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing.,"We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 euros more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",[],[],"['Alessandro Fabris', 'Alan Mishler', 'Stefano Gottardi', 'Mattia Carletti', 'Matteo Daicampi', 'Gian Antonio Susto', 'Gianmaria Silvello']",[],[]
https://arxiv.org/abs/2102.03977,Fairness & Bias,Learning to Generate Fair Clusters from Demonstrations.,"Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement, leading to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",[],[],"['Sainyam Galhotra', 'Sandhya Saisubramanian', 'Shlomo Zilberstein']","['University at Buffalo, Buffalo, NY, USA', 'University at Buffalo, Buffalo, NY, USA', 'University at Buffalo, Buffalo, NY, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.04452,Fairness & Bias,"Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation.","Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the ""Who,"" ""What,"" and ""How"" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the ""Who"" and ""What,"" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.",[],[],"['Jacqueline Hannan', 'Huei-Yen Winnie Chen', 'Kenneth Joseph']","['University of Maryland, Baltimore County, Baltimore, MD, USA', 'University of Maryland, Baltimore County, Baltimore, MD, USA', 'University of Maryland, Baltimore County, Baltimore, MD, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.06604,Fairness & Bias,Towards Equity and Algorithmic Fairness in Student Grade Prediction.,"Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",[],[],"['Weijie Jiang', 'Zachary A. Pardos']","['University of Toronto & Australian National University, Canberra, Australia', 'Australian National University, Canberra, Australia']","['Australia', 'Australia']"
https://arxiv.org/abs/2104.11757,Fairness & Bias,Becoming Good at AI for Good.,"AI for good (AI4G) projects involve developing and applying artificial intelligence (AI) based solutions to further goals in areas such as sustainability, health, humanitarian aid, and social justice. Developing and deploying such solutions must be done in collaboration with partners who are experts in the domain in question and who already have experience in making progress towards such goals. Based on our experiences, we detail the different aspects of this type of collaboration broken down into four high-level categories: communication, data, modeling, and impact, and distill eleven takeaways to guide such projects in the future. We briefly describe two case studies to illustrate how some of these takeaways were applied in practice during our past collaborations.",[],[],"['Meghana Kshirsagar', 'Caleb Robinson', 'Siyu Yang', 'Shahrzad Gholami', 'Ivan Klyuzhin', 'Sumit Mukherjee', 'Md Nasir', 'Anthony Ortiz', 'Felipe Oviedo', 'Darren Tanner', 'Anusua Trivedi', 'Yixi Xu', 'Ming Zhong', 'Bistra Dilkina', 'Rahul Dodhia', 'Juan M. Lavista Ferres']",[],[]
https://arxiv.org/abs/2104.03909,Fairness & Bias,RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity.,"We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of our system with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from the biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.",[],[],"['David Liu', 'Zohair Shafi', 'William Fleisher', 'Tina Eliassi-Rad', 'Scott Alfeld']",[],[]
https://arxiv.org/abs/2010.07343,Fairness & Bias,Causal Multi-Level Fairness.,"Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.",[],[],"['Vishwali Mhasawade', 'Rumi Chunara']","['Northwestern University, Evanston, IL, USA', 'Northwestern University, Evanston, IL, USA', 'Northwestern University, Evanston, IL, USA']","['US', 'US', 'US']"
https://arxiv.org/abs/2105.04760,Fairness & Bias,Unpacking the Expressed Consequences of AI Research in Broader Impact Statements.,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals.",[],[],"['Priyanka Nanayakkara', 'Jessica Hullman', 'Nicholas Diakopoulos']","['University of New South Wales, Sydney, NSW, Australia', 'University of New South Wales, Sydney, NSW, Australia', 'University of New South Wales, Sydney, NSW, Australia', 'Macquarie University, Sydney, NSW, Australia', 'University of New South Wales, Sydney, NSW, Australia']","['Australia', 'Australia', 'Australia', 'Australia', 'Australia']"
https://arxiv.org/abs/2102.00753,Fairness & Bias,Quantum Fair Machine Learning.,"In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within $\epsilon$-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.",[],[],['Elija Perrier'],"['Amazon Web Services, Berlin, Germany', 'Amazon Web Services, Berlin, Germany', 'Amazon Web Services, Berlin, Germany', 'Carnegie Mellon University, Pittsburgh, PA, USA', 'Amazon Web Services, Palo Alto, CA, USA', 'Amazon Web Services, Berlin, Germany']","['Germany', 'Germany', 'Germany', 'US', 'US', 'Germany']"
https://arxiv.org/abs/2006.05109,Fairness & Bias,Fair Bayesian Optimization.,"Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",[],[],"['Valerio Perrone', 'Michele Donini', 'Muhammad Bilal Zafar', 'Robin Schmucker', 'Krishnaram Kenthapadi', 'Cédric Archambeau']","['Google, New York, NY, USA', 'Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, San Bruno, CA, USA', 'Google, Mountain View, CA, USA', 'Google, New York, NY, USA', 'Google, Mountain View, CA, USA', 'Google, Mountain View, CA, USA', 'Google, New York, NY, USA']","['US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2105.09985,Fairness & Bias,Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective.,"In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts $\gamma$ and $\epsilon$. The first part $\gamma$ depends solely on the performance of the proxy such as precision and recall, whereas the second part $\epsilon$ captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by $\gamma$ via a linear dependence, whereas the dependence on the correlations $\epsilon$ only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.",[],[],"['Flavien Prost', 'Pranjal Awasthi', 'Nick Blumm', 'Aditee Kumthekar', 'Trevor Potter', 'Li Wei', 'Xuezhi Wang', 'Ed H. Chi', 'Jilin Chen', 'Alex Beutel']","['University of Texas at Austin, Austin, TX, USA', 'University of Texas at Austin, Austin, TX, USA', 'University of Texas at Austin, Austin, TX, USA', 'University of Texas at Austin, Austin, TX, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2010.06113,Fairness & Bias,FaiR-N: Fair and Robust Neural Networks for Structured Data.,"Fairness in machine learning is crucial when individuals are subject to automated decisions made by models in high-stake domains. Organizations that employ these models may also need to satisfy regulations that promote responsible and ethical A.I. While fairness metrics relying on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias, fairness in terms of the equalized ability to achieve recourse for different protected attribute groups has been relatively unexplored. We present a novel formulation for training neural networks that considers the distance of data points to the decision boundary such that the new objective: (1) reduces the average distance to the decision boundary between two groups for individuals subject to a negative outcome in each group, i.e. the network is more fair with respect to the ability to obtain recourse, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that training with this loss yields more fair and robust neural networks with similar accuracies to models trained without it. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse capabilities across groups are considered to train fairer neural networks, and a relation between error rates based fairness and recourse based fairness is investigated.",[],[],"['Shubham Sharma', 'Alan H. Gee', 'David Paydarfar', 'Joydeep Ghosh']","['Technische Universität Berlin, Berlin, Germany', 'Harvard University, Boston, MA, USA', 'Harvard University, Boston, MA, USA']","['Germany', 'US', 'US']"
https://arxiv.org/abs/2012.00423,Fairness & Bias,Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring.,"Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice. In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a largescale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",[],[],"['Tom Sühr', 'Sophie Hilgard', 'Himabindu Lakkaraju']","['Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA', 'Stanford University, Stanford, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/1907.00164,Privacy & Data Governance,On the Privacy Risks of Model Explanations.,"Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",[],[],"['Reza Shokri', 'Martin Strobel', 'Yair Zick']","['DeepMind, London, United Kingdom', 'DeepMind, London, United Kingdom', 'DeepMind & University College London, London, United Kingdom', 'DeepMind, London, United Kingdom']","['United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://arxiv.org/abs/2102.04257,Privacy & Data Governance,Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities.,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness--frequently, race and legal gender--can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",[],[],"['Nenad Tomasev', 'Kevin R. McKee', 'Jackie Kay', 'Shakir Mohamed']","['Clemson University, Clemson, SC, USA', 'Clemson University, Clemson, SC, USA', 'Clemson University, Clemson , SC, USA', 'Clemson University, Clemson , SC, USA']","['US', 'US', 'US', 'US']"
https://arxiv.org/abs/2105.06604,Privacy & Data Governance,Towards Equity and Algorithmic Fairness in Student Grade Prediction.,"Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",[],[],"['Weijie Jiang', 'Zachary A. Pardos']","['The University of Texas at Austin, Austin, TX, USA', 'The University of Texas at Austin, Austin, TX, USA', 'The University of Texas at Austin, Austin, TX, USA', 'The University of Texas at Austin, Austin, TX, USA', 'University of California, San Diego, San Diego, CA, USA', 'University of California, San Diego, San Diego, CA, USA']","['US', 'US', 'US', 'US', 'US', 'US']"
https://arxiv.org/abs/2109.06309,Security,Fairness and Data Protection Impact Assessments.,"In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice.",[],[],"['Atoosa Kasirzadeh', 'Damian Clifford']","['Tel-Aviv University, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel', 'Bar-Ilan University, Ramat Gan, Israel', 'Aarhus University, Aarhus, Denmark', 'IISc Bangalore, Bangalore, Denmark', 'Bar-Ilan University, Ramat Gan, Israel']","['Israel', 'Israel', 'Israel', 'Denmark', 'Denmark', 'Israel']"
https://arxiv.org/abs/2009.01534,Security,Fairness in the Eyes of the Data: Certifying Machine-Learning Models.,"We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.",[],[],"['Shahar Segal', 'Yossi Adi', 'Benny Pinkas', 'Carsten Baum', 'Chaya Ganesh', 'Joseph Keshet']",[],[]
https://arxiv.org/abs/2103.14068,Security,Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation.,"Normalizing flow models have risen as a popular solution to the problem of density estimation, enabling high-quality synthetic data generation as well as exact probability density evaluation. However, in contexts where individuals are directly associated with the training data, releasing such a model raises privacy concerns. In this work, we propose the use of normalizing flow models that provide explicit differential privacy guarantees as a novel approach to the problem of privacy-preserving density estimation. We evaluate the efficacy of our approach empirically using benchmark datasets, and we demonstrate that our method substantially outperforms previous state-of-the-art approaches. We additionally show how our algorithm can be applied to the task of differentially private anomaly detection.",[],[],"['Chris Waites', 'Rachel Cummings']",[],[]