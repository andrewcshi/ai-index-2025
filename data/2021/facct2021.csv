link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://dl.acm.org/doi/10.1145/3442188.3445899,Transparency & Explainability,Algorithmic Recourse: from Counterfactual Explanations to Consequential Interventions,"As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -""how the world would have (had) to be different for a desirable outcome to occur""- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.","['algorithmic recourse', 'explainable artificial intelligence', 'causal inference', 'counterfactual explanations', 'contrastive explanations', 'consequential recommendations', 'minimal interventions']","['Computing methodologies _ Causal reasoning and diagnostics', 'Human-centered computing']","['Amir-Hossein Karimi', 'Bernhard Schölkopf', 'Isabel Valera']","['MPI-IS, Germany, ETH Zürich', 'MPI-IS', 'MPI-IS, Germany, Saarland University']","['Switzerland', 'Germany', 'Germany']"
https://dl.acm.org/doi/10.1145/3442188.3445903,Transparency & Explainability,High Dimensional Model Explanations: An Axiomatic Approach,"Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets. We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model. Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.",[],[],"['Neel Patel', 'Martin Strobel', 'Yair Zick']","['University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst']","[None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445938,Transparency & Explainability,An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","['Participatory design', 'participatory action research', 'accountability', 'algorithmic equity', 'algorithmic justice', 'surveillance', 'regulation']","['Social and professional topics _ Surveillance', 'Governmental regulations', 'Computing literacy', 'Human-centered computing _ Participatory design', 'Computing methodologies _ Artificial intelligence']","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']","['Creative Computing Institute, University of Arts London', 'Digital Life Initiative, Cornell Tech', 'Public Policy Programme, Alan Turing Institute', 'ACLU of Washington', 'MIRA', 'Coveillance Collective', 'Human Centered Design & Engineering, University of Washington', 'eScience Institute, University of Washington', 'Evans School of Public Policy & Governance, University of Washington', 'Department of Sociology, West Virginia University', 'Department of Computer Science, Middlebury College', 'School of Information, University of Michigan', 'Albers School of Business & Economics, Seattle University', 'Oxford Department of International Development, University of Oxford', 'ACLU of Washington', 'ACLU of Washington']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445919,Transparency & Explainability,"Fairness, Equality, and Power in Algorithmic Decision-Making","Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.","['Algorithmic fairness', 'inequality', 'power', 'auditing', 'empirical economics']","['Computing methodologies_Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ Computing / technology policy']","['Maximilian Kasy', 'Rediet Abebe']","['University of Oxford, Department of Economics', 'University of California, Berkeley, Department of Electrical Engineering & Computer Sciences']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445944,Transparency & Explainability,When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces,"Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","['Recommendation', 'e-commerce marketplace', 'algorithmic auditing']",['Human-centered computing_Empirical studies in collaborative and social computing'],"['Abhisek Dash', 'Abhijnan Chakraborty', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Krishna P. Gummadi']","['Indian Institute of Technology, Kharagpur', 'Indian Institute of Technology, Delhi', 'Indian Institute of Technology, Kharagpur', 'Indian Institute of Technology, Kharagpur', 'Max Planck Institute for Software, Systems']","['India', 'India', 'India', 'India', 'Germany']"
https://dl.acm.org/doi/10.1145/3442188.3445941,Transparency & Explainability,How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations,"There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.","['XAI', 'Evaluation', 'Explainability', 'LIME', 'SHAP', 'User Study']","['General and reference _ Experimentation', 'Evaluation', 'Computing methodologies _ Machine learning']","['Sérgio Jesus', 'Catarina Belém', 'Vladimir Balayan', 'João Bento', 'Pedro Saleiro', 'Pedro Bizarro', 'João Gama']","['Feedzai, DCC-FCUP, Universidade do Porto', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'LIAAD, INESCTEC, Universidade do Porto']","[None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445928,Transparency & Explainability,Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.","['algorithm auditing', 'four-fifths rule', 'adverse impact testing', 'fairness']","['Social and professional topics _ Gender', 'Race and ethnicity', 'Employment issues', 'Codes of ethics']","['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.']","[None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445881,Transparency & Explainability,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",[],[],"['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445910,Transparency & Explainability,Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness",[],"['Computing methodologies _ Neural networks', 'General and reference _ Evaluation']","['Vedant Nanda', 'Samuel Dooley', 'Sahil Singla', 'Soheil Feizi', 'John P. Dickerson']","['University of Maryland MPI-SWS', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445927,Transparency & Explainability,A Statistical Test for Probabilistic Fairness,"Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.","['fairness', 'algorithmic bias', 'equal opportunity', 'equalized odds', 'Wasserstein distance']","['Applied computing _ IT governance', 'Law', 'Social and professional topics _ Race and ethnicity', 'Geographic characteristics', 'Sexual orientation', 'Gender', 'Age', 'Theory of computation _ Mathematical optimization']","['Bahar Taskesen', 'Jose Blanchet', 'Daniel Kuhn', 'Viet Anh Nguyen']","['Ecole Polytechnique Fédérale de Lausanne', 'Stanford University', 'Ecole Polytechnique Fédérale de Lausanne', 'Stanford University']","['Switzerland', 'USA', 'Switzerland', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445890,Transparency & Explainability,The Sanction of Authority: Promoting Public Trust in AI,"Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the underdevelopment of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Trusted AI research which underscores the need to develop nothing less than a comprehensive and visibly functioning regulatory ecosystem. We elaborate the pivotal role of externally auditable AI documentation within this model and the work to be done to ensure it is effective, and outline a number of actions that would promote public trust in AI. We discuss how existing efforts to develop AI documentation within organizations---both to inform potential adopters of AI components and support the deliberations of risk and ethics review boards---is necessary but insufficient assurance of the trustworthiness of AI. We argue that being accountable to the public in ways that earn their trust, through elaborating rules for AI and developing resources for enforcing these rules, is what will ultimately make AI trustworthy enough to be woven into the fabric of our society.","['Trust', 'trustworthiness', 'artificial intelligence', 'institutional trust', 'facework', 'structuration theory']","['Human-centered computing _ HCI theory', 'concepts and models', 'Social and professional topics _ Computing profession']","['Bran Knowles', 'John T. Richards']","['Lancaster University, Lancaster', 'TJ Watson Research Center, IBM Yorktown Heights, New York']","['UK', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445866,Transparency & Explainability,"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable AI","The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.","['Explainable AI', 'Explainable Artificial Intelligence', 'Explainable Machine Learning', 'Interpretable Machine Learning', 'Ethics of AI', 'Ethical AI', 'Machine learning', 'Philosophy of Explanation', 'Philosophy of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning', 'Human-centered computing']",['Atoosa Kasirzadeh'],['University of Toronto Australian National University'],[None]
https://dl.acm.org/doi/10.1145/3442188.3445940,Transparency & Explainability,Detecting discriminatory risk through data annotation based on Bayesian inferences,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.","['human annotation', 'data ethics', 'race discrimination', 'sampling bias', 'data labeling', 'machine learning']",['Human-centered computing _ Visualization application domains'],"['Elena Beretta', 'Antonio Vetrò', 'Bruno Lepri', 'Juan Carlos De Martin']","['Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy Fondazione Bruno Kessler Trento', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin', 'Fondazione Bruno Kessler, Trento', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin']","['Italy', 'Italy', 'Italy', 'Italy']"
https://dl.acm.org/doi/10.1145/3442188.3445871,Transparency & Explainability,Fifty Shades of Grey: In Praise of a Nuanced Approach Towards Trustworthy Design,"Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs---easily accessible online environments provisioning access to datasets, analysis and visualisations---are socio-technical systems which, if carefully designed, might address these challenges and promote trust in a variety of ways. In addition to various system properties that can be utilised in support of effective collaboration, certain features which are commonly seen to benefit trust---transparency and provenance in particular---appear applicable to promoting trust in and through Virtual Labs. Attempting to realise these features in their design reveals, however, that their implementation is more nuanced and complex than it would appear. Using the lens of affordances, we argue for the need to carefully articulate these features, with consideration of multiple stakeholder needs on balance, so that these Virtual Labs do in fact promote trust. We argue that these features not be conceived as widgets that can be imported into a given context to promote trust; rather, whether they promote trust is a function of how systematically designers consider various (potentially conflicting) stakeholder trust needs.","['Trust', 'Affordances', 'Environmental Data Science', 'Virtual Research Environments']","['Human-centered computing _ Computer supported cooperative work', 'HCI theory', 'concepts and models']","['Lauren Thornton', 'Bran Knowles', 'Gordon Blair']","['Lancaster University, Lancaster', 'Lancaster University, Lancaster', 'Lancaster University, Lancaster']","['UK', 'UK', 'UK']"
https://dl.acm.org/doi/10.1145/3442188.3445900,Transparency & Explainability,A semiotically-based epistemic tool to reason about ethical issues in digital technology design and development,"One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.","['semiotic engineering', 'ethics', 'epistemic tool']","['Human-centered computing _ HCI theory', 'concepts and models', 'Computing methodologies _ Artificial intelligence']","['Simone Diniz Junqueira Barbosa', 'Gabriel Diniz Junqueira Barbosa', 'Clarisse Sieckenius de Souza', 'Carla Faria Leitão']","['Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Informatics, PUC-Rio Rio de Janeiro, RJ', 'Department of Psychology, PUC-Rio Rio de Janeiro, RJ']","[None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445918,Transparency & Explainability,Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,"Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.","['datasets', 'requirements engineering', 'machine learning']","['Information systems _ Data management systems', 'Computing methodologies _ Machine learning']","['Ben Hutchinson', 'Andrew Smart', 'Alex Hanna', 'Emily Denton', 'Christina Greer', 'Oddur Kjartansson', 'Parker Barnes', 'Margaret Mitchell']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445943,Transparency & Explainability,Epistemic values in feature importance methods: Lessons from feminist epistemology,"As the public seeks greater accountability and transparency from machine learning algorithms, the research literature on methods to explain algorithms and their outputs has rapidly expanded. Feature importance methods form a popular class of explanation methods. In this paper, we apply the lens of feminist epistemology to recent feature importance research. We investigate what epistemic values are implicitly embedded in feature importance methods and how or whether they are in conflict with feminist epistemology. We offer some suggestions on how to conduct research on explanations that respects feminist epistemic values, taking into account the importance of social context, the epistemic privileges of subjugated knowers, and adopting more interactional ways of knowing","['explanation', 'philosophy', 'epistemology', 'machine learning', 'feature importance', 'feminism', 'methodology']",[],"['Leif Hancox-Li', 'I. Elizabeth Kumar']","['Capital One, New York, New York', 'University of Utah, Salt Lake City, UT']","['USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445937,Transparency & Explainability,Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.","['traceability', 'accountability', 'transparency', 'AI principles', 'AI ethics']","['Software and its engineering _ Traceability', 'Software version control', 'Computer systems organization _ Maintainability and maintenance']",['Joshua A. Kroll'],"['Naval Postgraduate School, Monterey, CA']",[None]
https://dl.acm.org/doi/10.1145/3442188.3445917,Transparency & Explainability,Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.","['Explainability', 'AI', 'Machine Learning', 'GDPR', 'Black-Box', 'Automated Decision-Making', 'Data Protection']","['Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Machine learning']","['Ronan Hamon', 'Henrik Junklewitz', 'Gianclaudio Malgieri', 'Paul De Hert', 'Laurent Beslay', 'Ignacio Sanchez']","['European Commission, Joint Research Centre, Ispra', 'European Commission, Joint Research Centre, Ispra', 'Augmented Law Institute, EDHEC Business School, Lille', 'Law Science Technology & Society, Vrije Universiteit Brussel, Brussels', 'European Commission, Joint Research Centre, Ispra', 'European Commission, Joint Research Centre, Ispra']","['Italy', 'Italy', 'France', 'Belgium', 'Italy', 'Italy']"
https://dl.acm.org/doi/10.1145/3442188.3445886,Transparency & Explainability,The Use and Misuse of Counterfactuals in Fair Machine Learning,"The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.","['Ethics of AI', 'Ethical AI', 'Counterfactuals', 'Machine learning', 'Fairness', 'Algorithmic Fairness', 'Explanation', 'Explainable AI', 'Philosophy', 'Social ontology', 'Social category', 'Social kind', 'Philosophy of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning', 'Social and professional topics _ Socio-technical systems', 'Race and ethnicity']","['Atoosa Kasirzadeh', 'Andrew Smart']","['University of Toronto, Australian National University', 'Google']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445929,Transparency & Explainability,Black Feminist Musings on Algorithmic Oppression,"This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.","['Algorithmic oppression', 'Black feminism', 'abolition']",[],['Lelia Marie Hampton'],"['EECS and CSAIL, MIT, Cambridge, MA']",['USA']
https://dl.acm.org/doi/10.1145/3442188.3445925,Transparency & Explainability,TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering,"In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do. We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty. Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.","['Data transparency', 'GDPR', 'data protection', 'privacy', 'privacy by design', 'legal tech', 'privacy engineering', 'web privacy', 'privacy law']","['Applied computing _ Law', 'Information systems _ Information systems applications', 'Web data description languages', 'Software and its engineering _ Formal language definitions', 'Context specific languages', 'Security and privacy _ Privacy protections']","['Elias Grünewald', 'Frank Pallas']","['Technische Universität Berlin, Information Systems Engineering Research Group, Berlin', 'Technische Universität Berlin, Information Systems Engineering Research Group, Berlin']","['Germany', 'Germany']"
https://dl.acm.org/doi/10.1145/3442188.3445891,Fairness & Bias,Algorithmic Fairness in Predicting Opioid Use Disorder using Machine Learning,"There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.","['causality', 'fairness', 'algorithm development', 'social and organizational processes', 'auditing', 'evaluations', 'ethics', 'algorithmic impacts']","['Applied computing _ Economics', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Government technology policy']",['Angela E. Kilby'],"['Northeastern University Boston, Massachusetts']",['USA']
https://dl.acm.org/doi/10.1145/3442188.3445887,Fairness & Bias,Mitigating Bias in Set Selection with Noisy Protected Attributes,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.",[],[],"['Anay Mehrotra', 'L. Elisa Celis']","['Yale University', 'Yale University']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445883,Fairness & Bias,Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately,"Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.",[],[],"['Fereshte Khani', 'Percy Liang']","['Stanford University', 'Stanford University']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445906,Fairness & Bias,Socially Fair k-Means Clustering,"We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.",[],[],"['Mehrdad Ghadiri', 'Samira Samadi', 'Santosh Vempala']","['Georgia Tech', 'MPI for Intelligent Systems', 'Georgia Tech']","['USA', 'Germany', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445902,Fairness & Bias,Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds,"In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed. The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria. In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.","['fairness', 'risk assessment', 'post-processing', 'counterfactual']",['Computing methodologies _ Machine learning'],"['Alan Mishler', 'Edward H. Kennedy', 'Alexandra Chouldechova']","['Department of Statistics, Carnegie Mellon University', 'Department of Statistics, Carnegie Mellon University', 'Heinz College Carnegie Mellon University']","[None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445889,Fairness & Bias,Standardized Tests and Affirmative Action: The Role of Bias and Variance,"The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies---such as affirmative action---on admitted class composition. This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action. We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool. Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement. The full paper can be found at https://arxiv.org/abs/2010.04396.",[],[],"['Nikhil Garg', 'Hannah Li', 'Faidra Monachou']","['UC Berkeley, Berkeley', 'Stanford University, Stanford', 'Stanford University, Stanford']","['USA', 'USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445915,Fairness & Bias,Fair Classification with Group-Dependent Label Noise,"This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.","['machine learning', 'algorithmic fairness', 'learning with noisy and biased labels']","['Computing methodologies_Machine learning', 'Philosophical/theoretical foundations of artificial intelligence']","['Jialu Wang', 'Yang Liu', 'Caleb Levy']","['UC Santa Cruz, Santa Cruz, CA', 'UC Santa Cruz, Santa Cruz, CA', 'UC Santa Cruz Santa Cruz, CA']","['USA', 'USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445878,Fairness & Bias,Towards Fair Deep Anomaly Detection,"Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.","['machine learning', 'algorithmic fairness', 'anomaly detection', 'deep learning', 'adversarial learning']",['Computing methodologies_Machine learning algorithms'],"['Hongjing Zhang', 'Ian Davidson']","['University of California, Davis', 'University of California, Davis']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445875,Fairness & Bias,Differential Tweetment: Mitigating Racial Dialect Bias in Harmful Tweet Detection,"Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.","['bias', 'fairness', 'racial disparities', 'dialect', 'content moderation']","['Social and professional topics _ Race and ethnicity', 'Censorship', 'Computing methodologies _ Machine learning', 'Natural language processing']","['Ari Ball-Burack', 'Michelle Seng Ah Lee', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group University of Cambridge', 'Compliant & Accountable Systems Group University of Cambridge', 'Compliant & Accountable Systems Group University of Cambridge', 'Compliant & Accountable Systems Group University of Cambridge']","['UK', 'UK', 'UK', 'UK']"
https://dl.acm.org/doi/10.1145/3442188.3445912,Fairness & Bias,Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",[],[],"['Jessie Finocchiaro', 'Roland Maio', 'Faidra Monachou', 'Gourab K Patro', 'Manish Raghavan', 'Ana-Andreea Stoica', 'Stratis Tsirtsis']","['CU Boulder', 'Columbia University', 'Stanford University', 'IIT Kharagpur', 'Cornell University', 'Columbia University', 'Max Planck Institute for Software Systems']","[None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445924,Fairness & Bias,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation,"Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.","['Fairness', 'natural language generation']",['Computing methodologies _ Natural language generation'],"['Jwala Dhamala', 'Tony Sun', 'Varun Kumar', 'Satyapriya Krishna', 'Yada Pruksachatkun', 'Kai-Wei Chang', 'Rahul Gupta']","['Amazon Alexa AI-NU', 'UC Santa Barbara', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445919,Fairness & Bias,"Fairness, Equality, and Power in Algorithmic Decision-Making","Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same ""merit."" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by ""merit;"" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.","['Algorithmic fairness', 'inequality', 'power', 'auditing', 'empirical economics']","['Computing methodologies_Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ Computing / technology policy']","['Maximilian Kasy', 'Rediet Abebe']","['University of Oxford, Department of Economics', 'University of California, Berkeley, Department of Electrical Engineering & Computer Sciences']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445944,Fairness & Bias,When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces,"Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","['Recommendation', 'e-commerce marketplace', 'algorithmic auditing']",['Human-centered computing_Empirical studies in collaborative and social computing'],"['Abhisek Dash', 'Abhijnan Chakraborty', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Krishna P. Gummadi']","['Indian Institute of Technology, Kharagpur', 'Indian Institute of Technology, Delhi', 'Indian Institute of Technology, Kharagpur', 'Indian Institute of Technology, Kharagpur', 'Max Planck Institute for Software, Systems']","['India', 'India', 'India', 'India', 'Germany']"
https://dl.acm.org/doi/10.1145/3442188.3445928,Fairness & Bias,Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.","['algorithm auditing', 'four-fifths rule', 'adverse impact testing', 'fairness']","['Social and professional topics _ Gender', 'Race and ethnicity', 'Employment issues', 'Codes of ethics']","['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.']","[None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445932,Fairness & Bias,Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases,"Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.","['implicit bias', 'unsupervised learning', 'computer vision']","['Computing methodologies _ Unsupervised learning', 'Transfer learning', 'Machine learning']","['Ryan Steed', 'Aylin Caliskan']","['Carnegie Mellon University, Pittsburgh, Pennsylvania', 'George Washington University, Washington']","['USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445881,Fairness & Bias,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",[],[],"['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445916,Fairness & Bias,Censorship of Online Encyclopedias and its Political Effect on NLP Models,"While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.","['word embeddings', 'censorship', 'training data', 'machine learning']",[],"['Eddie Yang', 'Margaret E. Roberts']","['University of California, San Diego La Jolla, California', 'University of California, San Diego La Jolla, California']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445892,Fairness & Bias,Avoiding Disparity Amplification under Different Worldviews,"We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.","['fairness', 'worldview', 'disparity amplification', 'demographic parity', 'equalized odds', 'predictive parity', 'calibration']","['Social and professional topics _ Socio-technical systems', 'Mathematics of computing _ Probability and statistics']","['Samuel Yeom', 'Michael Carl Tschantz']","['Carnegie Mellon University', 'International Computer Science Institute']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445893,Fairness & Bias,"Spoken Corpora Data, Automatic Speech Recognition, and Bias Against African American Language: The case of Habitual ‘Be’","Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in ""I be in my office by 7:30am"", paraphrasable as ""I am usually in my office by 7:30"" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora -- Switchboard, Fisher, TIMIT, and LibriSpeech -- to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL -- the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.","['automatic speech recognition', 'spoken corpora', 'datasets', 'linguistic bias', 'racial bias', 'African American Language']","['Social and professional topics_Race and ethnicity', 'Humancentered computing _ Natural language interfaces']",['Joshua L Martin'],"['University of Florida Gainesville, Florida']",[None]
https://dl.acm.org/doi/10.1145/3442188.3445910,Fairness & Bias,Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness",[],"['Computing methodologies _ Neural networks', 'General and reference _ Evaluation']","['Vedant Nanda', 'Samuel Dooley', 'Sahil Singla', 'Soheil Feizi', 'John P. Dickerson']","['University of Maryland MPI-SWS', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445907,Fairness & Bias,Towards Cross-Lingual Generalization of Translation Gender Bias,"Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.","['machine translation', 'gender bias', 'evaluation', 'cross-linguality']","['Computing methodologies _ Machine translation', 'Model verification and validation']","['Won Ik Cho', 'Jiwon Kim', 'Jaeyeong Yang', 'Nam Soo Kim']","['Dept. of ECE and INMC, Seoul National University, Seoul', 'Independent Researcher, Daegu', 'Dept. of Linguistics, Seoul National University, Seoul', 'Dept. of ECE and INMC, Seoul National University, Seoul']","['Korea', 'Korea', 'Korea', 'Korea']"
https://dl.acm.org/doi/10.1145/3442188.3445920,Fairness & Bias,"One Label, One Billion Faces: Usage & Consistency of Racial Categories in Computer Vision","Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.","['datasets', 'bias', 'fairness', 'faces', 'computer vision', 'race']","['Social and professional topics _ Race and ethnicity', 'Computing methodologies _ Computer vision']","['Zaid Khan', 'Yun Fu']","['Northeastern University', 'Northeastern University']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445884,Fairness & Bias,Evaluating Fairness of Machine Learning Models Under Uncertain and Incomplete Information,"Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives. We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.","['Model Auditing', 'Bias Estimation', 'Active Learning']",['Computing methodologies_Machine learning approaches'],"['Pranjal Awasthi', 'Alex Beutel', 'Matthäus Kleindessner', 'Jamie Morgenstern', 'Xuezhi Wang']","['Rutgers University & Google', 'Google', 'Amazon', 'University of Washington & Google', 'Google']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445927,Fairness & Bias,A Statistical Test for Probabilistic Fairness,"Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.","['fairness', 'algorithmic bias', 'equal opportunity', 'equalized odds', 'Wasserstein distance']","['Applied computing _ IT governance', 'Law', 'Social and professional topics _ Race and ethnicity', 'Geographic characteristics', 'Sexual orientation', 'Gender', 'Age', 'Theory of computation _ Mathematical optimization']","['Bahar Taskesen', 'Jose Blanchet', 'Daniel Kuhn', 'Viet Anh Nguyen']","['Ecole Polytechnique Fédérale de Lausanne', 'Stanford University', 'Ecole Polytechnique Fédérale de Lausanne', 'Stanford University']","['Switzerland', 'USA', 'Switzerland', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445894,Fairness & Bias,Leave-one-out Unfairness,"We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.",[],[],"['Emily Black', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445864,Fairness & Bias,Price Discrimination with Fairness Constraints,"Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints. In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations. We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature. Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.","['fairness', 'price discrimination', 'personalization', 'social welfare']","['Applied computing _ Economics', 'Law', 'Information systems _ Personalization', 'Social and professional topics _ Pricing and resource allocation']","['Maxime C. Cohen', 'Adam N. Elmachtoub', 'Xiao Lei']","['Desautels Faculty of Management, McGill University, Montreal, Quebec', 'Department of Industrial Engineering and Operations Research & Data Science Institute, Columbia University, New York, New York', 'Department of Industrial Engineering and Operations Research, Columbia University, New York, New York']","['Canada', 'USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445869,Fairness & Bias,Biases in Generative Art---A Causal Look from the Lens of Art History,"With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.","['generative art', 'style transfer', 'biases', 'AI', 'socio-cultural impacts']","['Social and professional topics', 'Computing methodologies _ Artificial intelligence']","['Ramya Srinivasan', 'Kanji Uchino']","['Fujitsu Laboratories of America', 'Fujitsu Laboratories of America']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445931,Fairness & Bias,"""I agree with the decision, but they didn't deserve this"": Future Developers' Perception of Fairness in Algorithmic Decisions","While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.","['algorithmic fairness', 'algorithmic transparency', 'algorithmic accountability', 'algorithmic decision-making']",['Human-centered computing _ Empirical studies in HCI'],"['Maria Kasinidou', 'Styliani Kleanthous', 'Pınar Barlas', 'Jahna Otterbacher']","['Cyprus Center for Algorithmic Transparency, Open University of Cyprus, Nicosia', 'Cyprus Center for Algorithmic Transparency, Open University of Cyprus Nicosia', 'Research Centre on Interactive Media, Smart Systems and Emerging Technologies, Nicosia', 'Cyprus Center for Algorithmic Transparency, Open University of Cyprus, Nicosia']","['Cyprus', 'Cyprus', 'Cyprus', 'Cyprus']"
https://dl.acm.org/doi/10.1145/3442188.3445866,Fairness & Bias,"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable AI","The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.","['Explainable AI', 'Explainable Artificial Intelligence', 'Explainable Machine Learning', 'Interpretable Machine Learning', 'Ethics of AI', 'Ethical AI', 'Machine learning', 'Philosophy of Explanation', 'Philosophy of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning', 'Human-centered computing']",['Atoosa Kasirzadeh'],['University of Toronto Australian National University'],[None]
https://dl.acm.org/doi/10.1145/3442188.3445940,Fairness & Bias,Detecting discriminatory risk through data annotation based on Bayesian inferences,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.","['human annotation', 'data ethics', 'race discrimination', 'sampling bias', 'data labeling', 'machine learning']",['Human-centered computing _ Visualization application domains'],"['Elena Beretta', 'Antonio Vetrò', 'Bruno Lepri', 'Juan Carlos De Martin']","['Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy Fondazione Bruno Kessler Trento', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin', 'Fondazione Bruno Kessler, Trento', 'Nexa Center for Internet & Society, Department of Control and Computer Engineering, Politecnico di Torino, Turin']","['Italy', 'Italy', 'Italy', 'Italy']"
https://dl.acm.org/doi/10.1145/3442188.3445896,Fairness & Bias,Re-imagining Algorithmic Fairness in India and Beyond,"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","['India', 'algorithmic fairness', 'caste', 'gender', 'religion', 'ability', 'class', 'feminism', 'decoloniality', 'anti-caste politics', 'critical algorithmic studies']",['Human-centered computing _ Empirical studies in HCI'],"['Nithya Sambasivan', 'Erin Arnesen', 'Ben Hutchinson', 'Tulsee Doshi', 'Vinodkumar Prabhakaran']","['Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA', 'Google Research Mountain View, CA']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445926,Fairness & Bias,"From Papers to Programs: Courts, Corporations, Clinics and the Battle over Computerized Psychological Testing","This paper examines the role of technology firms in computerizing personality tests from the early 1960s to late 1980s. It focuses on the National Computer Systems (NCS) and their development of an automated interpretation for the Minnesota Multiphasic Personality inventory (MMPI). NCS trumpeted their computerized interpretation as a way to free up clerical labor and mitigate human bias. Yet psychologists cautioned that proprietary algorithms risked obscuring decision rules. I show how clinics, courtrooms, and businesses all had competing interests in the use of computerized personality tests. As I argue, the development of computerized psychological tests was shaped both by business concerns about intellectual property and profits and psychologists' concerns with validity and access to algorithms. Across these domains, the common claim was that computerized psychological testing could provide a technical fix for bias. This paper contributes to histories of computing emphasizing the importance of IP, the relationship between labor, technology, and expertise, and to histories of algorithms.",[],"['Social and professional topics _ History of computing', 'Intellectual property']",['Kira Lussier'],['University of Toronto'],[None]
https://dl.acm.org/doi/10.1145/3442188.3445942,Fairness & Bias,"The Algorithmic Leviathan: Arbitrariness, Unfairness, and Opportunity in Algorithmic Decision Making Systems","Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern? We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.","['opportunity', 'arbitrariness', 'fairness', 'machine learning', 'algorithmic decision making', 'automated hiring']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Law', 'Computing methodologies _ Artificial intelligence']","['Kathleen Creel', 'Deborah Hellman']","['Stanford University, Palo Alto, California', 'University of Virginia, Charlottesville, Virginia']","['USA', 'USA']"
https://dl.acm.org/doi/10.1145/3442188.3445936,Fairness & Bias,On the Moral Justification of Statistical Parity,"A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews ""What You See Is What You Get"" (WYSIWYG) and ""We're All Equal"" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.","['fairness', 'independence', 'statistical parity', 'distributive justice', 'bias']","['Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Socio-technical systems']","['Corinna Hertweck', 'Christoph Heitz', 'Michele Loi']","['Zurich University of Applied Sciences, University of Zurich', 'Zurich University of Applied Sciences', 'University of Zurich']","[None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445876,Fairness & Bias,Group Fairness: Independence Revisited,"This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.","['fairness', 'independence', 'statistical parity', 'demographic parity', 'sufficiency', 'separation', 'affirmative action', 'accuracy']","['Social and professional topics_User characteristics', 'Computing methodologies _ Machine learning', 'Applied computing _ Arts and humanities']",['Tim Räz'],"['Institute of Philosophy, University of Bern, Switzerland Institute of Biomedical Ethics and History of Medicine, University of Zürich']",['Switzerland']
https://dl.acm.org/doi/10.1145/3442188.3445886,Fairness & Bias,The Use and Misuse of Counterfactuals in Fair Machine Learning,"The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.","['Ethics of AI', 'Ethical AI', 'Counterfactuals', 'Machine learning', 'Fairness', 'Algorithmic Fairness', 'Explanation', 'Explainable AI', 'Philosophy', 'Social ontology', 'Social category', 'Social kind', 'Philosophy of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning', 'Social and professional topics _ Socio-technical systems', 'Race and ethnicity']","['Atoosa Kasirzadeh', 'Andrew Smart']","['University of Toronto, Australian National University', 'Google']","[None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445930,Fairness & Bias,The Effect of the Rooney Rule on Implicit Bias in the Long Term,"The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change. In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist---independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.",[],[],"['L. Elisa Celis', 'Chris Hays', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'Yale University', 'Yale University', 'Yale University']","[None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445888,Fairness & Bias,"""What We Can’t Measure, We Can’t Understand"": Challenges to Demographic Data Procurement in the Pursuit of Fairness","As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.","['demographic data', 'sensitive data', 'special category data', 'data privacy', 'fairness', 'anti-discrimination']","['Social and professional topics _ Privacy policies', 'Systems planning', 'Socio-technical systems']","['McKane Andrus', 'Elena Spitzer', 'Jeffrey Brown', 'Alice Xiang']","['Partnership on AI', 'Partnership on AI', 'Partnership on AI, Minnesota State University, Mankato', 'Sony AI, Partnership on AI']","[None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445873,Fairness & Bias,"The Distributive Effects of Risk Prediction in Environmental Compliance: Algorithmic Design, Environmental Justice, and Public Policy","Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.","['risk models', 'government', 'environmental protection', 'fairness', 'environmental justice']","['Social and professional topics_Governmental regulations', 'Applied computing _ Law', 'Computing in government', 'Computing methodologies_Machine learning', 'Human-centered computing _ Interaction design']","['Elinor Benami', 'Reid Whitaker', 'Vincent La', 'Hongjin Lin', 'Brandon R. Anderson', 'Daniel E. Ho']","['Virginia Tech', 'University of California Berkeley', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445934,Privacy & Data Governance,Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings,"Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.","['machine learning', 'health care', 'privacy', 'fairness', 'robustness']","['Applied computing _ Health informatics', 'Security and privacy _ Usability in security and privacy', 'Computing methodologies _ Machine learning']","['Vinith M. Suriyakumar', 'Nicolas Papernot', 'Anna Goldenberg', 'Marzyeh Ghassemi']","['University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute', 'University of Toronto, Vector Institute']","[None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445879,Privacy & Data Governance,Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness,"The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.",[],[],"['Victoria Cheng', 'Vinith M. Suriyakumar', 'Natalie Dullerud', 'Shalmali Joshi', 'Marzyeh Ghassemi']","['Vector Institute, University of Toronto, Snap Inc.', 'Vector Institute, University of Toronto', 'Vector Institute, University of Toronto', 'Vector Institute', 'Vector Institute, University of Toronto Canadian CIFAR AI Chair']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445917,Privacy & Data Governance,Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.","['Explainability', 'AI', 'Machine Learning', 'GDPR', 'Black-Box', 'Automated Decision-Making', 'Data Protection']","['Applied computing _ Law', 'social and behavioral sciences', 'Computing methodologies _ Machine learning']","['Ronan Hamon', 'Henrik Junklewitz', 'Gianclaudio Malgieri', 'Paul De Hert', 'Laurent Beslay', 'Ignacio Sanchez']","['European Commission, Joint Research Centre, Ispra', 'European Commission, Joint Research Centre, Ispra', 'Augmented Law Institute, EDHEC Business School, Lille', 'Law Science Technology & Society, Vrije Universiteit Brussel, Brussels', 'European Commission, Joint Research Centre, Ispra', 'European Commission, Joint Research Centre, Ispra']","['Italy', 'Italy', 'France', 'Belgium', 'Italy', 'Italy']"
https://dl.acm.org/doi/10.1145/3442188.3445925,Privacy & Data Governance,TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering,"In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do. We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty. Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.","['Data transparency', 'GDPR', 'data protection', 'privacy', 'privacy by design', 'legal tech', 'privacy engineering', 'web privacy', 'privacy law']","['Applied computing _ Law', 'Information systems _ Information systems applications', 'Web data description languages', 'Software and its engineering _ Formal language definitions', 'Context specific languages', 'Security and privacy _ Privacy protections']","['Elias Grünewald', 'Frank Pallas']","['Technische Universität Berlin, Information Systems Engineering Research Group, Berlin', 'Technische Universität Berlin, Information Systems Engineering Research Group, Berlin']","['Germany', 'Germany']"
https://dl.acm.org/doi/10.1145/3442188.3445938,Security,An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","['Participatory design', 'participatory action research', 'accountability', 'algorithmic equity', 'algorithmic justice', 'surveillance', 'regulation']","['Social and professional topics _ Surveillance', 'Governmental regulations', 'Computing literacy', 'Human-centered computing _ Participatory design', 'Computing methodologies _ Artificial intelligence']","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']","['Creative Computing Institute, University of Arts London', 'Digital Life Initiative, Cornell Tech', 'Public Policy Programme, Alan Turing Institute', 'ACLU of Washington', 'MIRA', 'Coveillance Collective', 'Human Centered Design & Engineering, University of Washington', 'eScience Institute, University of Washington', 'Evans School of Public Policy & Governance, University of Washington', 'Department of Sociology, West Virginia University', 'Department of Computer Science, Middlebury College', 'School of Information, University of Michigan', 'Albers School of Business & Economics, Seattle University', 'Oxford Department of International Development, University of Oxford', 'ACLU of Washington', 'ACLU of Washington']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445941,Security,How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations,"There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.","['XAI', 'Evaluation', 'Explainability', 'LIME', 'SHAP', 'User Study']","['General and reference _ Experimentation', 'Evaluation', 'Computing methodologies _ Machine learning']","['Sérgio Jesus', 'Catarina Belém', 'Vladimir Balayan', 'João Bento', 'Pedro Saleiro', 'Pedro Bizarro', 'João Gama']","['Feedzai, DCC-FCUP, Universidade do Porto', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'Feedzai', 'LIAAD, INESCTEC, Universidade do Porto']","[None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445928,Security,Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.","['algorithm auditing', 'four-fifths rule', 'adverse impact testing', 'fairness']","['Social and professional topics _ Gender', 'Race and ethnicity', 'Employment issues', 'Codes of ethics']","['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.']","[None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445881,Security,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",[],[],"['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","[None, None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445910,Security,Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness",[],"['Computing methodologies _ Neural networks', 'General and reference _ Evaluation']","['Vedant Nanda', 'Samuel Dooley', 'Sahil Singla', 'Soheil Feizi', 'John P. Dickerson']","['University of Maryland MPI-SWS', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland']","[None, None, None, None, None]"
https://dl.acm.org/doi/10.1145/3442188.3445879,Security,Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness,"The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.",[],[],"['Victoria Cheng', 'Vinith M. Suriyakumar', 'Natalie Dullerud', 'Shalmali Joshi', 'Marzyeh Ghassemi']","['Vector Institute, University of Toronto, Snap Inc.', 'Vector Institute, University of Toronto', 'Vector Institute, University of Toronto', 'Vector Institute', 'Vector Institute, University of Toronto Canadian CIFAR AI Chair']","[None, None, None, None, None]"