link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,author_countries
https://iclr.cc/virtual/2021/poster/3366,Transparency & Explainability,A Geometric Analysis of Deep Generative Image Models and Its Applications,"Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs.","['feature visualization', 'Model Inversion', 'optimization', 'Differential Geometry', 'deep generative model', 'gan', 'interpretability']",[],"['Binxu Wang', 'Carlos R Ponce']","['Harvard University', 'Harvard University']",[]
https://iclr.cc/virtual/2021/poster/3361,Transparency & Explainability,Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels,"Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method not only optimises the architecture in a highly data-efficient manner, but also affords interpretability by discovering useful network features and their corresponding impact on the network performance. Moreover, our method is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. We demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.",[],[],"['Binxin Ru', 'Xingchen Wan', 'Xiaowen Dong', 'Michael A Osborne']","['University of Oxford', 'Google', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/3350,Transparency & Explainability,Representation Learning via Invariant Causal Mechanisms,"Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework.  We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method,  and provide an alternative theoretical explanation for the  success  of  these  methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games.","['causality', 'Contrastive Methods', 'self-supervised learning', 'representation learning']",[],"['Jovana Mitrovic', 'Brian McWilliams', 'Jacob C Walker', 'Lars Holger Buesing', 'Charles Blundell']","['DeepMind', 'Google', 'Carnegie Mellon University', 'Deepmind', 'DeepMind']",[]
https://iclr.cc/virtual/2021/poster/3269,Transparency & Explainability,Evaluating the Disentanglement of Deep Generative Models through Manifold Topology,"Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make our code publicly available at https://github.com/stanfordmlgroup/disentanglement.","['evaluation', 'disentanglement', 'generative models']",[],"['Sharon Zhou', 'Eric Zelikman', 'Andrew Y. Ng', 'Stefano Ermon']","['Stanford University', 'Stanford University', 'Computer Science Department', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/3197,Transparency & Explainability,Representation learning for improved interpretability and classification accuracy of clinical factors from EEG,"Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using $\beta$-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.","['beta-vae', 'depression', 'electroencephalography', 'ERP', 'EEG', 'representation learning', 'disentanglement']",[],"['Garrett Honke', 'Irina Higgins']","['State University of New York, Binghamton', 'DeepMind']",[]
https://iclr.cc/virtual/2021/poster/3189,Transparency & Explainability,Trajectory Prediction using Equivariant Continuous Convolution,"Trajectory prediction is a critical part of many AI applications, for example, the safe operation of autonomous vehicles. However, current methods are prone to making inconsistent and physically unrealistic predictions. We leverage insights from  fluid dynamics to overcome this limitation by considering internal symmetry in real-world trajectories. We propose a novel model, Equivariant Continous COnvolution (ECCO) for improved trajectory prediction.  ECCO uses rotationally-equivariant continuous convolutions to embed the symmetries of the system. On both vehicle and pedestrian trajectory datasets, ECCO attains competitive accuracy  with significantly fewer parameters. It is also more sample efficient, generalizing automatically from few data points in any orientation.  Lastly, ECCO improves generalization with equivariance, resulting in more physically consistent predictions.   Our method provides a fresh perspective towards increasing trust and transparency in deep learning models. Our code and data can be found at https://github.com/Rose-STL-Lab/ECCO.","['argoverse', 'continuous convolution', 'trajectory prediction', 'equivariant', 'symmetry']",[],"['Robin Walters', 'Jinxi Li', 'Rose Yu']","['Northeastern University ', 'Northeastern University', 'University of California, San Diego']",[]
https://iclr.cc/virtual/2021/poster/3173,Transparency & Explainability,The role of Disentanglement in Generalisation,"Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation.","['generalisation', 'compositional generalization', 'variational autoencoders', 'compositionality', 'disentanglement', 'generative models']",[],"['Milton L. Montero', 'Casimir JH Ludwig', 'Rui Ponte Costa', 'Gaurav Malhotra', 'Jeffrey Bowers']","['IT University of Copenhagen', 'University of Bristol', 'University of Bristol', 'University of Bristol', 'University of Bristol']",[]
https://iclr.cc/virtual/2021/poster/3126,Transparency & Explainability,Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting,"Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.","['hybrid systems', 'spatio-temporal forecasting', 'physics', 'differential equations', 'deep learning']",[],"['Yuan Yin', 'Vincent LE GUEN', 'Jérémie DONA', 'Ibrahim Ayed', 'Nicolas THOME', 'patrick gallinari']","['Sorbonne Université, CNRS, ISIR', 'Conservatoire National des Arts et Métiers', 'InstaDeep', 'LIP6', 'Université Pierre et Marie Curie - Paris 6, Sorbonne Université - Faculté des Sciences (Paris VI)', 'Criteo AI Lab']",[]
https://iclr.cc/virtual/2021/poster/3107,Transparency & Explainability,Prototypical Representation Learning for Relation Extraction,"Recognizing relations between entities is a pivotal task of relational learning.Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise and complicated expressions in human language.This paper aims to learn predictive, interpretable, and robust relation representations from distantly-labeled data that are effective in different settings, including supervised, distantly supervised, and few-shot learning. Instead of solely relying on the supervision from noisy labels, we propose to learn prototypes for each relation from contextual information to best explore the intrinsic semantics of relations. Prototypes are representations in the feature space abstracting the essential semantics of relations between entities in sentences. We learn prototypes based on objectives with clear geometric interpretation, where the prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered at the end of their corresponding prototype vectors on the surface of the ball. This approach allows us to learn meaningful, interpretable prototypes for the final classification. Results on several relation learning tasks show that our model significantly outperforms the previous state-of-the-art models. We further demonstrate the robustness of the encoder and the interpretability of prototypes with extensive experiments.","['Relation Extraction', 'nlp', 'representation learning']",[],"['Ning Ding', 'Xiaobin Wang', 'Yao Fu', 'Guangwei Xu', 'Rui Wang', 'Pengjun Xie', 'Ying Shen', 'Fei Huang', 'Hai-Tao Zheng']","['Tsinghua University, Tsinghua University', 'Soochow University, China', 'University of Edinburgh', 'Alibaba Group', 'nyonic', 'Alibaba Group', 'SUN YAT-SEN UNIVERSITY', 'Alibaba Group', 'Tsinghua University, Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/3095,Transparency & Explainability,Multi-timescale Representation Learning in LSTM Language Models,"Language models must capture statistical dependencies between words at timescales ranging from very short to very long. Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory (LSTM) language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency (rare) words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.","['timescales', 'language model', 'lstm']",[],"['Vy A. Vo', 'Javier S. Turek', 'Alexander Huth']","['Intel', 'Intel', 'The University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/3057,Transparency & Explainability,BERTology Meets Biology: Interpreting Attention in Protein Language Models,"Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at https://github.com/salesforce/provis.","['Computational Biology', 'visualization', 'transformers', 'black box', 'attention', 'representation learning', 'natural language processing', 'interpretability']",[],"['Jesse Vig', 'Ali Madani', 'Lav R. Varshney', 'Caiming Xiong', 'richard socher', 'Nazneen Rajani']","['Salesforce Research', 'Profluent Bio', 'University of Illinois at Urbana-Champaign', 'Salesforce Research', 'SalesForce.com', 'Salesforce']",[]
https://iclr.cc/virtual/2021/poster/2950,Transparency & Explainability,Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting,"The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \textit{evidence} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has ``the right reasons'' for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at \url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.","['XAI', 'lifelong learning', 'catastrophic forgetting', 'continual learning', 'explainability']",[],"['Sayna Ebrahimi', 'Suzanne Petryk', 'William Gan', 'Joseph E. Gonzalez', 'Marcus Rohrbach', 'Trevor Darrell']","['Google', 'University of California Berkeley', 'University of California Berkeley', 'University of California - Berkeley', 'Technische Universität Darmstadt', 'Electrical Engineering & Computer Science Department']",[]
https://iclr.cc/virtual/2021/poster/2943,Transparency & Explainability,Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability,"Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding $p_{\theta} ( y\mid \mathbf{x} )$, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work, we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: given that input-gradients can be arbitrary, why are they highly structured and explanatory in standard models? In this work, we re-interpret the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional generative model $p_{\theta}(\mathbf{x} \mid y)$ implicit in the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model $p_{\theta}(\mathbf{x} \mid y)$ with that of the ground truth data distribution $p_{\text{data}} (\mathbf{x} \mid y)$. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this density alignment, we use an algorithm called score-matching, and propose novel approximations to this algorithm to enable training large-scale models. Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. This also leads us to conjecture that unintended density alignment in standard neural network training may explain the highly structured nature of input-gradients observed in practice. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models.","['score-matching', 'saliency maps', 'interpretability']",[],"['Suraj Srinivas', 'François Fleuret']","['School of Engineering and Applied Sciences, Harvard University', 'University of Geneva']",[]
https://iclr.cc/virtual/2021/poster/2906,Transparency & Explainability,On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning,"Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\textit{meta-initialization}$ of model parameters (that we call $\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\textit{how}$ robust regularization can $\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.",[],[],"['Ren Wang', 'Kaidi Xu', 'Sijia Liu', 'Pin-Yu Chen', 'Tsui-Wei Weng', 'Chuang Gan', 'Meng Wang']","['Illinois Institute of Technology', 'Drexel University', 'Michigan State University', 'International Business Machines', 'University of California, San Diego', 'MIT-IBM Watson AI Lab', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2021/poster/2856,Transparency & Explainability,Shapley explainability on the data manifold,"Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model’s features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While “off-manifold” Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.",[],[],"['Christopher Frye', 'Damien De Mijolla', 'Tom Begley', 'Laurence Cowton', 'Megan Stanley', 'Ilya Feige']","['Faculty', 'Faculty AI', 'Faculty', 'University of Cambridge', 'Microsoft Research Cambridge', 'University College London']",[]
https://iclr.cc/virtual/2021/poster/2854,Transparency & Explainability,"Learning ""What-if"" Explanations for Sequential Decision-Making","Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior--i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function--is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ``""what if'' outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making---where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.","['explaining decision-making', 'counterfactuals', 'preference learning']",[],"['Ioana Bica', 'Daniel Jarrett', 'Alihan Hüyük', 'Mihaela van der Schaar']","['DeepMind', 'DeepMind', 'University of Cambridge', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2780,Transparency & Explainability,Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning,"Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker’s policy is challenging—with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision- making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning (“Interpole”) that jointly estimates an agent’s (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer’s disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, quantifying, and understanding human decision-making behavior.","['understanding decision-making', 'interpretable policy learning']",[],"['Alihan Hüyük', 'Daniel Jarrett', 'Cem Tekin', 'Mihaela van der Schaar']","['University of Cambridge', 'DeepMind', 'Bilkent University', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2741,Transparency & Explainability,Getting a CLUE: A  Method for Explaining Uncertainty Estimates,"Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.","['explainability', 'uncertainty', 'interpretability']",[],"['Javier Antoran', 'Umang Bhatt', 'Tameem Adel', 'Adrian Weller', 'José Miguel Hernández-Lobato']","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'Alan Turing Institute', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2719,Transparency & Explainability,"Physics-aware, probabilistic model order reduction with guaranteed stability","Given (small amounts of) time-series' data from  a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions. We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics  of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations. Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. The generative framework proposed achieves  the aforementioned desiderata by  employing a flexible prior on the complex plane for the latent, slow processes, and  an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require  the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation. We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced.","['long-term stability', 'slowness', 'model order reduction', 'probabilistic generative models', 'state-space models', 'inductive bias']",[],"['Sebastian Kaltenbach', 'Phaedon Stelios Koutsourelakis']","['ETHZ - ETH Zurich', 'Technische Universität München']",[]
https://iclr.cc/virtual/2021/poster/2667,Transparency & Explainability,Monotonic Kronecker-Factored Lattice,"It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice ($\mathrm{KFL}$), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The $\mathrm{KFL}$ function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of $M$ base $\mathrm{KFL}$ models strictly increases as $M$ increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that $\mathrm{KFL}$ trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.","['evaluation', 'Matrix and Tensor Factorization', 'algorithms', 'theory', 'efficiency', 'fairness', 'regularization', 'regression', 'classification', 'machine learning']",[],"['William Taylor Bakst', 'Nobuyuki Morioka', 'Erez Louidor']","['Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/3262,Transparency & Explainability,Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization,"Dimensionality reduction methods for count data are critical to a wide range of applications in medical informatics and other fields where model interpretability is paramount. For such data, hierarchical Poisson matrix factorization (HPF) and other sparse probabilistic non-negative matrix factorization (NMF) methods are considered to be interpretable generative models. They consist of sparse transformations for decoding their learned representations into predictions. However, sparsity in representation decoding does not necessarily imply sparsity in the encoding of representations from the original data features.  HPF is often incorrectly interpreted in the literature as if it possesses encoder sparsity. The distinction between decoder sparsity and encoder sparsity is subtle but important. Due to the lack of encoder sparsity, HPF does not possess the column-clustering property of classical NMF -- the factor loading matrix does not sufficiently define how each factor is formed from the original features. We address this deficiency by self-consistently enforcing encoder sparsity, using a generalized additive model  (GAM), thereby allowing one to relate each representation coordinate to a subset of the original data features. In doing so, the method also gains the ability to perform feature selection. We demonstrate our method on simulated data and give an example of how encoder sparsity is of practical use in a concrete application of representing inpatient comorbidities in Medicare patients.","['factor analysis', 'probabilistic matrix factorization', 'poisson matrix factorization', 'bayesian', 'sparse coding', 'generalized additive model', 'interpretability']",[],"['Joshua C Chang', 'Patrick Allen Fletcher', 'Bart Desmet', 'Ayah Zirikly']","['National Institutes of Health', 'NIH', 'Universiteit Gent', 'Johns Hopkins University']",[]
https://iclr.cc/virtual/2021/poster/3310,Transparency & Explainability,NBDT: Neural-Backed Decision Tree,"Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.","['explainability', 'computer vision', 'interpretability']",[],"['Alvin Wan', 'Lisa Dunlap', 'Daniel Ho', 'Jihan Yin', 'Scott Lee', 'Suzanne Petryk', 'Sarah Adel Bargal', 'Joseph E. Gonzalez']","['Apple', 'University of California, Berkeley', 'University of California Berkeley', 'University of California Berkeley', 'University of California Berkeley', 'University of California Berkeley', 'Georgetown University', 'University of California - Berkeley']",[]
https://iclr.cc/virtual/2021/poster/2605,Transparency & Explainability,Interpretable Models for Granger Causality Using Self-explaining Neural Networks,"Exploratory analysis of time series data can yield a better understanding of complex dynamical systems. Granger causality is a practical framework for analysing interactions in sequential data, applied in a wide range of domains. In this paper, we propose a novel framework for inferring multivariate Granger causality under nonlinear dynamics based on an extension of self-explaining neural networks. This framework is more interpretable than other neural-network-based techniques for inferring Granger causality, since in addition to relational inference, it also allows detecting signs of Granger-causal effects and inspecting their variability over time. In comprehensive experiments on simulated data, we show that our framework performs on par with several powerful baseline methods at inferring Granger causality and that it achieves better performance at inferring interaction signs. The results suggest that our framework is a viable and more interpretable alternative to sparse-input neural networks for inferring Granger causality.","['Granger causality', 'time series', 'inference', 'neural networks', 'interpretability']",[],"['Ričards Marcinkevičs', 'Julia E Vogt']","['Department of Computer Science, Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/3200,Transparency & Explainability,Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks,"Neural networks (NNs) whose subnetworks implement reusable functions are expected to offer numerous advantages, including compositionality through efficient recombination of functional building blocks, interpretability, preventing catastrophic interference, etc. Understanding if and how NNs are modular could provide insights into how to improve them. Current inspection methods, however, fail to link modules to their functionality. In this paper, we present a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. Using this powerful tool, we contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets. We demonstrate how common NNs fail to reuse submodules and offer new insights into the related issue of systematic generalization on language tasks.","['systematic generalization', 'modularity', 'compositionality']",[],"['Róbert Csordás', 'Sjoerd van Steenkiste', 'Jürgen Schmidhuber']","['IDSIA', 'Google', 'King Abdullah University of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2706,Transparency & Explainability,A Learning Theoretic Perspective on Local Explainability,"In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the test-time predictive accuracy of a model using a notion of how locally explainable it is.  Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice.","['Local Explanations', 'learning theory', 'generalization', 'interpretability']",[],"['Jeffrey Li', 'Vaishnavh Nagarajan', 'Gregory Plumb', 'Ameet Talwalkar']","['Department of Computer Science, University of Washington', 'Google', 'Carnegie Mellon University', 'University of California-Los Angeles']",[]
https://iclr.cc/virtual/2021/poster/3257,Transparency & Explainability,Disentangled Recurrent Wasserstein Autoencoder,"Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively.","['Recurrent Generative Model', 'Sequential  Representation Learning', 'disentanglement']",[],"['Jun Han', 'Martin Renqiang Min', 'Ligong Han', 'Li Erran Li', 'Xuan Zhang']","['PCG, Tencent', 'NEC Laboratories America', 'Rutgers University', 'Amazon', 'Texas A&M']",[]
https://iclr.cc/virtual/2021/poster/3153,Transparency & Explainability,Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization,"Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. (2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants' performance, and is the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations ($82\pm4\%$ accuracy; chance would be $50\%$). However, natural images --- originally intended to be a baseline --- outperform these synthetic images by a wide margin ($92\pm2\%$). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of the feature visualizations by Olah et al. (2017) are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images ($65\pm5\%$ vs. $73\pm4\%$). In summary, synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this simple baseline.","['explanation method', 'understanding CNNs', 'human psychophysics', 'activation maximization', 'feature visualization', 'evaluation of interpretability']",[],"['Judy Borowski', 'Roland S. Zimmermann', 'Robert Geirhos', 'Thomas S. A. Wallis', 'Matthias Bethge', 'Wieland Brendel']","['University of Tuebingen', 'Eberhard-Karls-Universität Tübingen', 'Google DeepMind', 'TU Darmstadt', 'University of Tuebingen', 'ELLIS Institute Tübingen']",[]
https://iclr.cc/virtual/2021/poster/2637,Transparency & Explainability,Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders,"Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.","['Noise model', 'Unsupervised denoising', 'Diversity denoising', 'variational autoencoders']",[],"['Mangal Prakash', 'Alexander Krull', 'Florian Jug']","['Johnson & Johnson', 'Birmingham University', 'Fondation Human Technopole']",[]
https://iclr.cc/virtual/2021/poster/2841,Transparency & Explainability,Influence Functions in Deep Learning Are Fragile,"Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not well-understood in the context of deep learning with non-convex loss functions.  In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups.","['influence functions', 'interpretability']",[],"['Samyadeep Basu', 'Phil Pope', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2987,Transparency & Explainability,Task-Agnostic Morphology Evolution,"Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution .","['evolution', 'morphology', 'empowerment', 'information theory', 'unsupervised']",[],"['Joey Hejna', 'Pieter Abbeel', 'Lerrel Pinto']","['Stanford University', 'Covariant', 'New York University']",[]
https://iclr.cc/virtual/2021/poster/2679,Fairness & Bias,Tilted Empirical Risk Minimization,"Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.","['fairness', 'label noise robustness', 'models of learning and generalization', 'exponential tilting']",[],"['Tian Li', 'Ahmad Beirami', 'Maziar Sanjabi', 'Virginia Smith']","['University of Chicago', 'Google Research', 'Meta', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/2966,Fairness & Bias,Statistical inference for individual fairness,"As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating unwanted social biases has come to the fore of the public's and the research community's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial loss. The tools allow practitioners to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the adversarial loss and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.",[],[],"['Subha Maity', 'Songkai Xue', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan, Ann Arbor', 'University of Michigan', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2777,Fairness & Bias,SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness,"In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.","['Algorithmic fairness', 'invariance']",[],"['Mikhail Yurochkin', 'Yuekai Sun']","['International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2749,Fairness & Bias,Individually Fair Gradient Boosting,"We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern. At a high level, our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.","['non-smooth models', 'boosting', 'Algorithmic fairness']",[],"['Alexander Vargo', 'Fan Zhang', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan', 'ShanghaiTech University', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2652,Fairness & Bias,FairBatch: Batch Selection for Model Fairness,"Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts.  Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.","['batch selection', 'model fairness', 'bilevel optimization']",[],"['Yuji Roh', 'Kangwook Lee', 'Steven Euijong Whang', 'Changho Suh']","['Korea Advanced Institute of Science and Technology', 'University of Wisconsin, Madison', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/2627,Fairness & Bias,Individually Fair Rankings,We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.,"['Learning to Rank', 'Algorithmic fairness', 'optimal transport']",[],"['Amanda Bower', 'Mikhail Yurochkin', 'Yuekai Sun']","['Twitter', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/2612,Fairness & Bias,Fair Mixup: Fairness via Interpolation,"Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples  between the groups. We use mixup, a powerful data augmentation strategy  to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.","['fairness', 'data augmentation']",[],"['Ching-Yao Chuang', 'Youssef Mroueh']","['Meta', 'IBM']",[]
https://iclr.cc/virtual/2021/poster/2555,Fairness & Bias,FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders,"Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.","['Pretrained Text Encoders', 'contrastive learning', 'fairness', 'mutual information']",[],"['Pengyu Cheng', 'Weituo Hao', 'Siyang Yuan', 'Shijing Si', 'Lawrence Carin']","['Tencent', 'TikTok Inc.', 'Duke University', 'Shanghai International Studies University', 'Duke University']",[]
https://iclr.cc/virtual/2021/poster/3241,Fairness & Bias,On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections,"Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic design. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, \textbf{FairAdj}, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff.","['graph-structured data', 'Algorithmic fairness']",[],"['Peizhao Li', 'Yifei Wang', 'Han Zhao', 'Pengyu Hong', 'Hongfu Liu']","['Brandeis University', 'Brandeis University', 'University of Illinois, Urbana Champaign', 'Brandeis University', 'Brandeis University']",[]
https://iclr.cc/virtual/2021/poster/3250,Privacy & Data Governance,Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning,"Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms ""vanilla"" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.","['Stepsize optimization', 'ensemble', 'Out-of-domain', 'adversarial training', 'uncertainty', 'meta-learning', 'few-shot learning']",[],"['Namyeong Kwon', 'Hwidong Na', 'Gabriel Huang', 'Simon Lacoste-Julien']","['Samsung Advanced Institute of Technology', 'Samsung', 'University of Montreal', 'University of Montreal']",[]
https://iclr.cc/virtual/2021/poster/2846,Privacy & Data Governance,FedBN: Federated Learning on Non-IID Features via Local Batch Normalization,"The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data and hence improving data privacy. In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups. Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence. Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city), where local clients store examples with different distributions compared to other clients, which we denote as feature shift non-iid. In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments. These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg. Code is available at https://github.com/med-air/FedBN.","['batch normalization', 'Non-IID', 'federated learning']",[],"['Xiaoxiao Li', 'Meirui Jiang', 'Xiaofei Zhang', 'Michael Kamp', 'Qi Dou']","['University of British Columbia', 'The Chinese University of Hong Kong', 'Zhongnan University of Economics and Law', 'Institute for AI in Medicine IKIM', 'The Chinese University of Hong Kong']",[]
https://iclr.cc/virtual/2021/poster/2834,Privacy & Data Governance,Do not Let Privacy Overbill Utility:  Gradient Embedding Perturbation for Private Learning,"The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters.  In this paper, we propose an algorithm  \emph{Gradient Embedding Perturbation (GEP)} towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with low computational cost and modest privacy guarantee for deep models.  Especially, with privacy bound $\epsilon=8$, we achieve $74.9\%$ test accuracy on CIFAR10 and $95.1\%$ test accuracy on  SVHN, significantly improving over existing results.","['gradient redundancy', 'differentially private deep learning', 'privacy preserving machine learning']",[],"['Da Yu', 'Huishuai Zhang', 'Wei Chen', 'Tie-Yan Liu']","['SUN YAT-SEN UNIVERSITY', 'Microsoft Research Asia', ' Chinese Academy of Sciences', 'Microsoft']",[]
https://iclr.cc/virtual/2021/poster/2786,Privacy & Data Governance,CaPC Learning: Confidential and Private Collaborative Learning,"Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.","['mpc', 'homomorphic encryption', 'confidentiality', 'security', 'machine learning', 'differential privacy', 'privacy', 'deep learning']",[],"['Christopher A. Choquette-Choo', 'Natalie Dullerud', 'Yunxiang Zhang', 'Somesh Jha', 'Nicolas Papernot', 'Xiao Wang']","['Google Research, Brain team', 'Stanford University', 'New York University', 'Department of Computer Science, University of Wisconsin, Madison', 'University of Toronto', 'Northwestern University']",[]
https://iclr.cc/virtual/2021/poster/2710,Privacy & Data Governance,Information Laundering for Model Privacy,"In this work, we propose information laundering, a novel framework for enhancing model privacy. Unlike data privacy that concerns the protection of raw data information, model privacy aims to protect an already-learned model that is to be deployed for public use. The private model can be obtained from general learning methods, and its deployment means that it will return a deterministic or random response for a given input query. An information-laundered model consists of probabilistic components that deliberately maneuver the intended input and output for queries of the model, so the model's adversarial acquisition is less likely. Under the proposed framework, we develop an information-theoretic principle to quantify the fundamental tradeoffs between model utility and privacy leakage and derive the optimal design.","['security', 'Privacy-utility tradeoff', 'Model privacy', 'adversarial attack', 'machine learning']",[],"['Xinran Wang', 'Yu Xiang', 'Jun Gao', 'Jie Ding']","['University of Minnesota - Twin Cities', 'University of Utah', 'Meta ', 'University of Minnesota, Minneapolis']",[]
https://iclr.cc/virtual/2021/poster/2729,Privacy & Data Governance,Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning,"Federated learning (FL) is a distributed machine learning architecture that leverages a large number of workers to jointly learn a model with decentralized data. FL has received increasing attention in recent years thanks to its data privacy protection, communication efficiency and a linear speedup for convergence in training (i.e., convergence performance increases linearly with respect to the number of workers). However, existing studies on linear speedup for convergence are only limited to the assumptions of i.i.d. datasets across workers and/or full worker participation, both of which rarely hold in practice. So far, it remains an open question whether or not the linear speedup for convergence is achievable under non-i.i.d. datasets with partial worker participation in FL.  In this paper, we show that the answer is affirmative. Specifically, we show that the federated averaging (FedAvg) algorithm (with two-sided learning rates) on non-i.i.d. datasets in non-convex settings achieves a convergence rate $\mathcal{O}(\frac{1}{\sqrt{mKT}} + \frac{1}{T})$ for full worker participation and a convergence rate $\mathcal{O}(\frac{1}{\sqrt{nKT}} + \frac{1}{T})$ for partial worker participation, where $K$ is the number of local steps, $T$ is the number of total communication rounds, $m$ is the total worker number and $n$ is the worker number in one communication round if for partial worker participation. Our results also reveal that the local steps in FL could help the convergence and show that the maximum number of local steps can be improved to $T/m$. We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results.",[],[],"['Haibo Yang', 'Minghong Fang', 'Jia Liu']","['Rochester Institute of Technology', 'Duke University', 'The Ohio State University']",[]
https://iclr.cc/virtual/2021/poster/2867,Security,Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks,"Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set --- by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to both subset aggregation, a well-studied ensemble method in classical machine learning, as well as to randomized smoothing, a popular provable defense against evasion (inference) attacks. Our defense against label-flipping poison attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unlabeled training set in addition to the labels for a partition. SS-DPA significantly outperforms the existing certified defense for label-flipping attacks (Rosenfeld et al., 2020) on both MNIST and CIFAR-10: provably tolerating, for at least half of test images, over 600 label flips (vs. < 200 label flips) on MNIST and over 300 label flips (vs. 175 label flips) on CIFAR-10. Against general poisoning attacks where no prior certified defenses exists, DPA can certify $\geq$ 50% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against general and label-flipping poison attacks. Code is available at https://github.com/alevine0/DPA","['smoothing', 'poisoning', 'certificate', 'ensemble', 'bagging', 'robustness']",[],"['Alexander Levine', 'Soheil Feizi']","['University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2561,Security,Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching,"Data Poisoning attacks modify training data to maliciously control a model trained on such data. In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a particularly malicious poisoning attack that is bothfrom scratch"" andclean label"", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets. The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset. Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.","['clean-label', 'from-scratch', 'Backdoor Attacks', 'gradient alignment', 'Large-scale', 'Data Poisoning', 'imagenet', 'security']",[],"['Jonas Geiping', 'Liam H Fowl', 'W Ronny Huang', 'Gavin Taylor', 'Michael Moeller', 'Tom Goldstein']","['ELLIS Institute Tübingen', 'Google', 'Google', 'US Naval Academy', 'University of Siegen', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2969,Security,"On InstaHide, Phase Retrieval, and Sparse Matrix Factorization","In this work, we examine the security of InstaHide, a scheme recently proposed by \cite{hsla20} for preserving the security of private datasets in the context of distributed learning. To generate a synthetic training example to be shared among the distributed learners, InstaHide takes a convex combination of private feature vectors and randomly flips the sign of each entry of the resulting vector with probability 1/2. A salient question is whether this scheme is secure in any provable sense, perhaps under a plausible complexity-theoretic assumption.The answer to this turns out to be quite subtle and closely related to the average-case complexity of a multi-task, missing-data version of the classic problem of phase retrieval that is interesting in its own right. Motivated by this connection, under the standard distributional assumption that the public/private feature vectors are isotropic Gaussian, we design an algorithm that can actually recover a private vector using only the public vectors and a sequence of synthetic vectors generated by InstaHide.","['phase retrieval', 'InstaHide', 'distributed learning', 'matrix factorization']",[],"['Sitan Chen', 'Xiaoxiao Li', 'Zhao Song', 'Danyang Zhuo']","['University of California Berkeley', 'University of British Columbia', 'Adobe Research', 'Duke University']",[]
https://iclr.cc/virtual/2021/poster/2966,Security,Statistical inference for individual fairness,"As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating unwanted social biases has come to the fore of the public's and the research community's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial loss. The tools allow practitioners to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the adversarial loss and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.",[],[],"['Subha Maity', 'Songkai Xue', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan, Ann Arbor', 'University of Michigan', 'International Business Machines', 'University of Michigan']",[]
https://iclr.cc/virtual/2021/poster/3369,Security,ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity,"Adversarial attacks pose a major challenge for modern deep neural networks. Recent advancements show that adversarially robust generalization requires a large amount of labeled data for training. If annotation becomes a burden, can unlabeled data help bridge the gap? In this paper, we propose ARMOURED, an adversarially robust training method based on semi-supervised learning that consists of two components. The first component applies multi-view learning to simultaneously optimize multiple independent networks and utilizes unlabeled data to enforce labeling consistency. The second component reduces adversarial transferability among the networks via diversity regularizers inspired by determinantal point processes and entropy maximization. Experimental results show that under small perturbation budgets, ARMOURED is robust against strong adaptive adversaries. Notably, ARMOURED does not rely on generating adversarial samples during training. When used in combination with adversarial training, ARMOURED yields competitive performance with the state-of-the-art adversarially-robust benchmarks on SVHN and outperforms them on CIFAR-10, while offering higher clean accuracy.","['Entropy Maximization', 'Diversity Regularization', 'Multi-View Learning', 'semi-supervised learning', 'adversarial robustness']",[],"['Kangkang Lu', 'Cuong Manh Nguyen', 'Xun Xu', 'Kiran Krishnamachari', 'Yu Jing Goh', 'Chuan-Sheng Foo']","['A*STAR', 'A*STAR', 'A*STAR', 'National University of Singapore', 'National University of Singapore', 'Centre for Frontier AI Research, A*STAR']",[]
https://iclr.cc/virtual/2021/poster/3351,Security,Fooling a Complete Neural Network Verifier,"The efficient and accurate characterization of the robustness of neural networks to input perturbation is an important open problem. Many approaches exist including heuristic and exact (or complete) methods. Complete methods are expensive but their mathematical formulation guarantees that they provide exact robustness metrics. However, this guarantee is valid only if we assume that the verified network applies arbitrary-precision arithmetic and the verifier is reliable. In practice, however, both the networks and the verifiers apply limited-precision floating point arithmetic. In this paper, we show that numerical roundoff errors can be exploited to craft adversarial networks, in which the actual robustness and the robustness computed by a state-of-the-art complete verifier radically differ. We also show that such adversarial networks can be used to insert a backdoor into any network in such a way that the backdoor is completely missed by the verifier. The attack is easy to detect in its naive form but, as we show, the adversarial network can be transformed to make its detection less trivial. We offer a simple defense against our particular attack based on adding a very small perturbation to the network weights. However, our conjecture is that other numerical attacks are possible, and exact verification has to take into account all the details of the computation executed by the verified networks, which makes the problem significantly harder.","['numerical errors', 'complete verifiers', 'adversarial examples']",[],"['Zombori Dániel', 'Tibor Csendes', 'Istvan Megyeri', 'Márk Jelasity']","['University of Szeged', 'University of Szeged', 'University of Szeged', 'University of Szeged']",[]
https://iclr.cc/virtual/2021/poster/3346,Security,Generating Adversarial Computer Programs using Optimized Obfuscations,"Machine learning (ML) models that learn and predict properties of computer programs are increasingly being adopted and deployed. These models have demonstrated success in applications such as auto-completing code, summarizing large programs, and detecting bugs and malware in programs. In this work, we investigate principled ways to adversarially perturb a computer program to fool such learned models, and thus determine their adversarial robustness. We use program obfuscations, which have conventionally been used to avoid attempts at reverse engineering programs, as adversarial perturbations. These perturbations modify programs in ways that do not alter their functionality but can be crafted to deceive an ML model when making a decision. We provide a general formulation for an adversarial program that allows applying multiple obfuscation transformations to a program in any language. We develop first-order optimization algorithms to  efficiently determine two key aspects -- which parts of the program to transform, and what transformations to use. We show that it is important to optimize both these aspects to generate the best adversarially perturbed program. Due to the discrete nature of this problem, we also propose using randomized smoothing to improve the attack loss landscape to ease optimization. We evaluate our work on Python and Java programs on the problem of program summarization. We show that our best attack proposal achieves a $52\%$ improvement over a state-of-the-art attack generation approach for programs trained on a \textsc{seq2seq} model. We further show that our formulation is better at training models that are robust to adversarial attacks.","['Models for code', 'Differentiable program generator', 'combinatorial optimization', 'Program obfuscation', 'Adversarial computer programs', 'Machine Learning (ML) for Programming Languages (PL)/Software Engineering (SE)']",[],"['Shashank Srikant', 'Sijia Liu', 'Tamara Mitrovska', 'Shiyu Chang', 'Quanfu Fan', 'Gaoyuan Zhang', ""Una-May O'Reilly""]","['Massachusetts Institute of Technology', 'Michigan State University', 'Massachusetts Institute of Technology', 'UC Santa Barbara', 'MIT-IBM Watson AI Lab', 'International Business Machines', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/3344,Security,Stabilized Medical Image Attacks,"Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.","['Biometrics', 'Healthcare']",[],"['Gege Qi', 'Lijun GONG', 'Yibing Song', 'Kai Ma', 'Yefeng Zheng']","['Peking University', 'Sensetime', 'Fudan University', 'Tencent', 'Tencent Jarvis Lab']",[]
https://iclr.cc/virtual/2021/poster/3312,Security,Byzantine-Resilient Non-Convex Stochastic Gradient Descent,"We study adversary-resilient stochastic distributed optimization, in which $m$ machines can independently compute stochastic gradients, and cooperate to jointly optimize over their local objective functions. However, an $\alpha$-fraction of the machines are Byzantine, in that they may behave in arbitrary, adversarial ways. We consider a variant of this procedure in the challenging non-convex case. Our main result is a new algorithm SafeguardSGD, which can provably escape saddle points and find approximate local minima of the non-convex objective. The algorithm is based on a new concentration filtering technique, and its sample and time complexity bounds match the best known theoretical bounds in the stochastic, distributed setting when no Byzantine machines are present. Our algorithm is very practical: it improves upon the performance of all prior methods when training deep neural networks, it is relatively lightweight, and it is the first method to withstand two recently-proposed Byzantine attacks.","['Byzantine resilience', 'robust deep learning', 'distributed deep learning', 'distributed machine learning', 'non-convex optimization']",[],"['Jerry Li', 'Dan Alistarh']","['Microsoft', 'Institute of Science and Technology']",[]
https://iclr.cc/virtual/2021/poster/3283,Security,"A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference","Recent increases in the computational demands of deep neural networks (DNNs), combined with the observation that most input samples require only simple models, have sparked interest in input-adaptive multi-exit architectures, such as MSDNets or Shallow-Deep Networks. These architectures enable faster inferences and could bring DNNs to low-power devices, e.g., in the Internet of Things (IoT). However, it is unknown if the computational savings provided by this approach are robust against adversarial pressure. In particular, an adversary may aim to slowdown adaptive DNNs by increasing their average inference time—a threat analogous to the denial-of-service attacks from the Internet. In this paper, we conduct a systematic evaluation of this threat by experimenting with three generic multi-exit DNNs (based on VGG16, MobileNet, and ResNet56) and a custom multi-exit architecture, on two popular image classification benchmarks (CIFAR-10 and Tiny ImageNet). To this end, we show that adversarial example-crafting techniques can be modified to cause slowdown, and we propose a metric for comparing their impact on different architectures. We show that a slowdown attack reduces the efficacy of multi-exit DNNs by 90–100%, and it amplifies the latency by 1.5–5× in a typical IoT deployment. We also show that it is possible to craft universal, reusable perturbations and that the attack can be effective in realistic black-box scenarios, where the attacker has limited knowledge about the victim. Finally, we show that adversarial training provides limited protection against slowdowns. These results suggest that further research is needed for defending multi-exit architectures against this emerging threat. Our code is available at https://github.com/sanghyun-hong/deepsloth.","['input-adaptive multi-exit neural networks', 'Slowdown attacks', 'efficient inference', 'adversarial examples']",[],"['Sanghyun Hong', 'Yigitcan Kaya', 'Ionut-Vlad Modoranu', 'Tudor Dumitras']","['Oregon State University', 'University of California, Santa Barbara', 'Institute of Science and Technology Austria', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/3279,Security,Provably robust classification of adversarial examples with detection,"Adversarial attacks against deep networks can be defended against either by building robust classifiers or, by creating classifiers that can \emph{detect} the presence of adversarial perturbations.  Although it may intuitively seem easier to simply detect attacks rather than build a robust classifier, this has not bourne out in practice even empirically, as most detection methods have subsequently been broken by adaptive attacks, thus necessitating \emph{verifiable} performance for detection mechanisms.  In this paper, we propose a new method for jointly training a provably robust classifier and detector.  Specifically, we show that by introducing an additional ""abstain/detection"" into a classifier, we can modify existing certified defense mechanisms to allow the classifier to either robustly classify \emph{or} detect adversarial attacks.  We extend the common interval bound propagation (IBP) method for certified robustness under $\ell_\infty$ perturbations to account for our new robust objective, and show that the method outperforms traditional IBP used in isolation, especially for large perturbation sizes.  Specifically, tests on MNIST and CIFAR-10 datasets exhibit promising results, for example with provable robust error less than $63.63\%$ and $67.92\%$, for $55.6\%$ and $66.37\%$ natural error, for $\epsilon=8/255$ and $16/255$ on the CIFAR-10 dataset, respectively.","['adversarial robustness', 'robust deep learning']",[],"['Fatemeh Sheikholeslami', 'Ali Lotfi', 'J Zico Kolter']","['Amazon', 'University of Texas, Austin', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/3264,Security,PAC Confidence Predictions for Deep Neural Network Classifiers,"A key challenge for deploying deep neural networks (DNNs) in safety critical settings is the need to provide rigorous ways to quantify their uncertainty. In this paper, we propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees. Our approach uses Clopper-Pearson confidence intervals for the Binomial distribution in conjunction with the histogram binning approach to calibrated prediction. In addition, we demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings: (i) fast DNN inference, where we demonstrate how to compose a fast but inaccurate DNN with an accurate but slow DNN in a rigorous way to improve performance without sacrificing accuracy, and (ii) safe planning, where we guarantee safety when using a DNN to predict whether a given action is safe based on visual observations. In our experiments, we demonstrate that our approach can be used to provide guarantees for state-of-the-art DNNs.","['safe planning', 'fast DNN inference', 'probably approximated correct guarantee', 'calibration', 'classification']",[],"['Sangdon Park', 'Shuo Li', 'Insup Lee', 'Osbert Bastani']","['POSTECH', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']",[]
https://iclr.cc/virtual/2021/poster/3228,Security,Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models,"The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive transformation is capable of securing naturally-trained classifiers, and our method is the first to validate a post-training defense approach that is distinct from current successful defenses which modify classifier training.The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples for effective purification, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambiguities for evaluating stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarial training on CIFAR-10, SVHN, and CIFAR-100. Our code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.","['Langevin sampling', 'Markov chain Monte Carlo', 'Energy-based model', 'adversarial defense', 'adversarial robustness', 'adversarial attack']",[],"['Mitch Hill', 'Jonathan Craig Mitchell', 'Song-Chun Zhu']","['InnoPeak Technology', 'University of California, Los Angeles', 'Beijing Institute for General Artificial Intelligence']",[]
https://iclr.cc/virtual/2021/poster/3194,Security,R-GAP: Recursive Gradient Attack on Privacy,"Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.","['collaborative learning', 'privacy leakage from gradients', 'federated learning']",[],"['Junyi Zhu', 'Matthew B. Blaschko']","['KU Leuven', 'KU Leuven']",[]
https://iclr.cc/virtual/2021/poster/3088,Security,LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition,"Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike.  These systems are typically built by scraping social media profiles for user images.  Adversarial perturbations have been proposed for bypassing facial recognition systems.  However, existing methods fail on full-scale systems and commercial APIs.  We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases.  Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.","['facial recognition', 'adversarial attacks']",[],"['Valeriia Cherepanova', 'Micah Goldblum', 'John P Dickerson', 'Gavin Taylor', 'Tom Goldstein']","['University of Maryland, College Park', 'New York University', 'University of Maryland, College Park', 'US Naval Academy', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/3087,Security,WaNet - Imperceptible Warping-based Backdoor Attack,"With the thriving of deep learning and the widespread practice of using pre-trained networks, backdoor attacks have become an increasing security threat drawing many research interests in recent years. A third-party model can be poisoned in training to work well in normal conditions but behave maliciously when a trigger pattern appears. However, the existing backdoor attacks are all built on noise perturbation triggers, making them noticeable to humans. In this paper, we instead propose using warping-based triggers. The proposed backdoor outperforms the previous methods in a human inspection test by a wide margin, proving its stealthiness. To make such models undetectable by machine defenders, we propose a novel training mode, called the ``noise mode. The trained networks successfully attack and bypass the state-ofthe art defense methods on standard classification datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to network inspection, further proving this novel attack mechanism's efficiency.","['wanet', 'image warping', 'backdoor attack']",[],['Anh Tuan Tran'],['VinAI Research'],[]
https://iclr.cc/virtual/2021/poster/3083,Security,Evaluations and Methods for Explanation through Robustness Analysis,"Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to ""remove"" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.","['adversarial robustness', 'Explanations', 'interpretability']",[],"['Cheng-Yu Hsieh', 'Chih-Kuan Yeh', 'Xuanqing Liu', 'Pradeep Kumar Ravikumar', 'Seungyeon Kim', 'Sanjiv Kumar', 'Cho-Jui Hsieh']","['University of Washington', 'Google', 'University of California, Los Angeles', 'Carnegie Mellon University', 'Google', 'Google', 'Google']",[]
https://iclr.cc/virtual/2021/poster/3066,Security,Benchmarks for Deep Off-Policy Evaluation,"Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.","['benchmarks', 'off-policy evaluation', 'reinforcement learning']",[],"['Justin Fu', 'Mohammad Norouzi', 'Ofir Nachum', 'George Tucker', 'ziyu wang', 'Alexander Novikov', 'Sherry Yang', 'Michael R. Zhang', 'Yutian Chen', 'Aviral Kumar', 'Cosmin Paduraru', 'Sergey Levine', 'Thomas Paine']","['University of California Berkeley', 'Google Brain', 'OpenAI', 'Google Brain', 'Google', 'Deep Mind', 'Google', 'University of Toronto', 'DeepMind', 'University of California Berkeley', 'DeepMind', 'Google', 'Google/DeepMind']",[]
https://iclr.cc/virtual/2021/poster/3051,Security,Average-case Acceleration for Bilinear Games and Normal Matrices,"Advances in generative modeling and adversarial learning have given rise to renewed interest in smooth games. However, the absence of symmetry in the matrix of second derivatives poses challenges that are not present in the classical minimization framework. While a rich theory of average-case analysis has been developed for minimization problems, little is known in the context of smooth games. In this work we take a first step towards closing this gap by developing average-case optimal first-order methods for a subset of smooth games. We make the following three main contributions. First, we show that for zero-sum bilinear games the average-case optimal method is the optimal method for the minimization of the Hamiltonian. Second, we provide an explicit expression for the optimal method corresponding to normal matrices, potentially non-symmetric. Finally, we specialize it to matrices with eigenvalues located in a disk and show a provable speed-up compared to worst-case optimal algorithms. We illustrate our findings through benchmarks with a varying degree of mismatch with our assumptions.","['Orthogonal Polynomials', 'Average-case Analysis', 'Bilinear games', 'acceleration', 'First-order Methods', 'Smooth games']",[],"['Carles Domingo-Enrich', 'Fabian Pedregosa', 'Damien Scieur']","['New York University', 'Google', 'Université Catholique de Louvain']",[]
https://iclr.cc/virtual/2021/poster/3030,Security,Robust Overfitting may be mitigated by properly learned smoothening,"A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\%\sim6.68\%$ and robust accuracy by $0.22\%\sim2 .03\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\ell_{\infty}$ and $\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.","['Robust Overfitting', 'adversarial training', 'adversarial robustness']",[],"['Tianlong Chen', 'Zhenyu Zhang', 'Sijia Liu', 'Shiyu Chang', 'Zhangyang Wang']","['Massachusetts Institute of Technology', 'University of Texas at Austin', 'Michigan State University', 'UC Santa Barbara', 'University of Texas at Austin']",[]
https://iclr.cc/virtual/2021/poster/3015,Security,Uncertainty Estimation in Autoregressive Structured Prediction,"Uncertainty estimation is important for ensuring safety and robustness of AI systems.  While most research in the area has focused on un-structured prediction tasks, limited work has investigated general uncertainty estimation approaches for structured prediction. Thus, this work aims to investigate uncertainty estimation for structured prediction tasks within a single unified and interpretable probabilistic ensemble-based framework.  We consider: uncertainty estimation for sequence data at the token-level and complete sequence-level; interpretations for, and applications of, various measures of uncertainty; and discuss both the theoretical and practical challenges associated with obtaining them. This work also provides baselines for token-level and sequence-level error detection, and sequence-level out-of-domain input detection on the WMT’14 English-French and WMT’17 English-German translation and LibriSpeech speech recognition datasets.","['speech recognition.', 'structures prediction', 'knowledge uncertainty', 'ensembles', 'autoregressive models', 'uncertainty estimation', 'information theory', 'machine translation']",[],"['Andrey Malinin', 'Mark Gales']","['Isomorphic Labs', 'University of Cambridge']",[]
https://iclr.cc/virtual/2021/poster/2978,Security,Calibration of Neural Networks using Splines,"Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spline-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures. Code is available online at https://github.com/kartikgupta-at-anu/spline-calibration.","['calibration measure', 'neural network calibration', 'uncertainty']",[],"['Thalaiyasingam Ajanthan', 'Thomas Mensink', 'Cristian Sminchisescu', 'Richard Hartley']","['Amazon', 'Google Research', 'Lund University', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2955,Security,How Benign is Benign Overfitting ?,"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji & Long, 2020).  However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don’t fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness.","['Memorization', 'benign overfitting', 'adversarial robustness', 'generalization']",[],"['Amartya Sanyal', 'Puneet K. Dokania', 'Varun Kanade', 'Philip Torr']","['Max-Planck Institute', 'University of Oxford', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/2924,Security,Towards Robustness Against Natural Language Word Substitutions,"Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either l_2-ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel Adversarial Sparse Convex Combination (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on  ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, i.e., sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.","['adversarial defense', 'natural language processing']",[],"['Xinshuai Dong', 'Anh Tuan Luu', 'Rongrong Ji', 'Hong Liu']","['Carnegie Mellon University', 'Nanyang Technological University', 'Xiamen University', 'Osaka University, Tokyo Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2906,Security,On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning,"Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a $\textit{meta-initialization}$ of model parameters (that we call $\textit{meta-model}$) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how $\textit{adversarial robustness}$ can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study $\textit{when}$ a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate $\textit{how}$ robust regularization can $\textit{efficiently}$ be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.",[],[],"['Ren Wang', 'Kaidi Xu', 'Sijia Liu', 'Pin-Yu Chen', 'Tsui-Wei Weng', 'Chuang Gan', 'Meng Wang']","['Illinois Institute of Technology', 'Drexel University', 'Michigan State University', 'International Business Machines', 'University of California, San Diego', 'MIT-IBM Watson AI Lab', 'Rensselaer Polytechnic Institute']",[]
https://iclr.cc/virtual/2021/poster/2899,Security,Enforcing robust control guarantees within neural network policies,"When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: constructing a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, our approach entails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several domains, improving in average-case performance over existing robust control methods and in worst-case stability over (non-robust) deep RL methods.","['differentiable optimization', 'robust control', 'reinforcement learning']",[],"['Priya L. Donti', 'Melrose Roderick', 'J Zico Kolter']","['Massachusetts Institute of Technology', 'Mila, University of Montreal', 'Carnegie Mellon University']",[]
https://iclr.cc/virtual/2021/poster/2898,Security,Contrastive Divergence Learning is a Time Reversal Adversarial Game,"Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).","['noise contrastive estimation', 'contrastive divergence', 'energy based model', 'unsupervised learning', 'adversarial learning']",[],"['Omer Yair', 'Tomer Michaeli']","['Technion, Technion', 'Technion, Technion']",[]
https://iclr.cc/virtual/2021/poster/2896,Security,Domain-Robust Visual Imitation Learning with Mutual Information Constraints,"Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.","['Domain Adaption', 'Third-Person Imitation', 'Observational Imitation', 'imitation learning', 'mutual information', 'reinforcement learning', 'machine learning']",[],"['Edoardo Cetin', 'Oya Celiktutan']","[""King's College London"", ""King's College London, University of London""]",[]
https://iclr.cc/virtual/2021/poster/2886,Security,Private Image Reconstruction from System Side Channels Using Generative Models,"System side channels denote effects imposed on the underlying system and hardware when running a program, such as its accessed CPU cache lines. Side channel analysis (SCA) allows attackers to infer program secrets based on observed side channel signals. Given the ever-growing adoption of machine learning as a service (MLaaS), image analysis software on cloud platforms has been exploited by reconstructing private user images from system side channels. Nevertheless, to date, SCA is still highly challenging, requiring technical knowledge of victim software's internal operations. For existing SCA attacks, comprehending such internal operations requires heavyweight program analysis or manual efforts.This research proposes an attack framework to reconstruct private user images processed by media software via system side channels. The framework forms an effective workflow by incorporating convolutional networks, variational autoencoders, and generative adversarial networks. Our evaluation of two popular side channels shows that the reconstructed images consistently match user inputs, making privacy leakage attacks more practical. We also show surprising results that even one-bit data read/write pattern side channels, which are deemed minimally informative, can be used to reconstruct quality images using our framework.",['side channel analysis'],[],"['Yuanyuan Yuan', 'Shuai Wang', 'Junping Zhang']","['Department of Computer Science and Engineering, The Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology', 'Fudan University']",[]
https://iclr.cc/virtual/2021/poster/2870,Security,Shape-Texture Debiased Neural Network Training,"Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously.Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.","['debiased training', 'data augmentation', 'representation learning']",[],"['Yingwei Li', 'Qihang Yu', 'Mingxing Tan', 'Jieru Mei', 'Peng Tang', 'Wei Shen', 'Alan Yuille', 'Cihang Xie']","['Waymo LLC', 'ByteDance', 'Google', 'Johns Hopkins University', 'Amazon', 'Shanghai Jiao Tong University', 'Johns Hopkins University', 'University of California, Santa Cruz']",[]
https://iclr.cc/virtual/2021/poster/2860,Security,Perceptual Adversarial Robustness: Defense Against Unseen Threat Models,"A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to get around this issue by considering restrictive adversarial threat models such as those bounded by $L_2$ or $L_\infty$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks: $L_2$, $L_\infty$, spatial, recoloring, and JPEG. We find that PAT achieves state-of-the-art robustness against the union of these five attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial defense with this property. Code and data are available at https://github.com/cassidylaidlaw/perceptual-advex",[],[],"['Cassidy Laidlaw', 'Sahil Singla', 'Soheil Feizi']","['University of California Berkeley', 'University of Maryland, College Park', 'University of Maryland, College Park']",[]
https://iclr.cc/virtual/2021/poster/2859,Security,Deep Neural Network Fingerprinting by Conferrable Adversarial Examples,"In Machine Learning as a Service, a provider trains a deep neural network and gives many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a surrogate model from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call conferrable adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the irremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches a ROC AUC of 1.0 in verifying surrogates, compared to a ROC AUC of 0.63 by previous fingerprints.","['Conferrability', 'Transferability', 'Fingerprinting', 'adversarial examples']",[],"['Nils Lukas', 'Yuxuan Zhang', 'Florian Kerschbaum']","['University of Waterloo', 'Princeton University', 'University of Waterloo']",[]
https://iclr.cc/virtual/2021/poster/2848,Security,Meta-Learning with Neural Tangent Kernels,"Model Agnostic Meta-Learning (MAML) has emerged as a standard framework for meta-learning, where a meta-model is learned with the ability of fast adapting to new tasks. However, as a double-looped optimization problem, MAML needs to differentiate through the whole inner-loop optimization path for every outer-loop training step, which may lead to both computational inefficiency and sub-optimal solutions. In this paper, we generalize MAML to allow meta-learning to be defined in function spaces, and propose the first meta-learning paradigm in the Reproducing Kernel Hilbert Space (RKHS) induced by the meta-model's Neural Tangent Kernel (NTK). Within this paradigm, we introduce two meta-learning algorithms in the RKHS, which no longer need a sub-optimal iterative inner-loop adaptation as in the MAML framework. We achieve this goal by 1) replacing the adaptation with a fast-adaptive regularizer in the RKHS; and 2) solving the adaptation analytically based on the NTK theory. Extensive experimental studies demonstrate advantages of our paradigm in both efficiency and quality of solutions compared to related meta-learning algorithms. Another interesting feature of our proposed methods is that they are demonstrated to be more robust to adversarial attacks and out-of-distribution adaptation than popular baselines, as demonstrated in our experiments.","['neural tangent kernel', 'meta-learning']",[],"['Yufan Zhou', 'Zhenyi Wang', 'Jiayi Xian', 'Changyou Chen', 'Jinhui Xu']","['Adobe ', 'University of Maryland, College Park', 'State University of New York, Buffalo', 'State University of New York, Buffalo', 'State University of New York, Buffalo']",[]
https://iclr.cc/virtual/2021/poster/2823,Security,Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples,"To craft black-box adversarial examples, adversaries need to query the victim model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision (i.e., the hard-label prediction) of the victim model is available.  In this paper, we propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for reproducing our results are available at https://github.com/ZiangYan/pda.pytorch","['hard-label attack', 'black-box attack', 'adversarial attack', 'reinforcement learning']",[],"['Ziang Yan', 'Yiwen Guo', 'Jian Liang', 'Changshui Zhang']","['Tsinghua University', 'ByteDance', 'Alibaba Group', 'Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/2788,Security,"Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds","In the present work we study classifiers' decision boundaries via Brownian motion processes in ambient data space and associated probabilistic techniques. Intuitively, our ideas correspond to placing a heat source at the decision boundary and observing how effectively the sample points warm up. We are largely motivated by the search for a soft measure that sheds further light on the decision boundary's geometry. En route, we  bridge aspects of potential theory and geometric analysis (Maz'ya 2011, Grigor'Yan and Saloff-Coste 2002) with active fields of ML research such as adversarial examples and generalization bounds. First, we focus on the geometric behavior of decision boundaries in the light of adversarial attack/defense mechanisms. Experimentally, we observe a certain capacitory trend over different adversarial defense strategies: decision boundaries locally become flatter as measured by isoperimetric inequalities (Ford et al 2019); however, our more sensitive heat-diffusion metrics  extend this analysis and further reveal that some non-trivial geometry invisible to plain distance-based methods is still preserved. Intuitively, we provide evidence that the decision boundaries nevertheless retain many persistent ""wiggly and fuzzy"" regions on a finer scale. Second, we show how Brownian hitting probabilities translate to soft generalization bounds which are in turn connected to compression and noise stability (Arora et al 2018), and these bounds are significantly stronger if the decision boundary has controlled geometric features.","['adversarial attacks/defenses', 'curvature estimates', 'decision boundary geometry', 'Brownian motion', 'generalization bounds', 'deep learning theory']",[],"['Bogdan Georgiev', 'Lukas Franken', 'Mayukh Mukherjee']","['Fraunhofer IAIS', 'Fraunhofer Institute IAIS, Fraunhofer IAIS', 'Indian Institute of Technology Bombay']",[]
https://iclr.cc/virtual/2021/poster/2760,Security,Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks,"Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make the incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks,  NAD can effectively erase the backdoor triggers using only 5\% clean training data without causing obvious performance degradation on clean examples. Our code is available at https://github.com/bboylyg/NAD.","['Neural Attention Distillation', 'Backdoor Defense', 'deep neural networks']",[],"['Yige Li', 'Xixiang Lyu', 'Nodens Koren', 'Lingjuan Lyu', 'Bo Li', 'Xingjun Ma']","['Xidian University', ""Xi'an University of Electronic Science and Technology"", 'ETHZ - ETH Zurich', 'Sony Research', 'University of Illinois, Urbana Champaign', 'Fudan University']",[]
https://iclr.cc/virtual/2021/poster/2746,Security,Contemplating Real-World Object Classification,"Deep object recognition models have been very successful over benchmark datasets such as ImageNet. How accurate and robust are they to distribution shifts arising from natural and synthetic variations in datasets? Prior research on this problem has primarily focused on ImageNet variations (e.g., ImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies, we take a different approach. Specifically, we reanalyze the ObjectNet dataset recently proposed by Barbu et al. containing objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding the generalization ability of deep models, we take a second look at their analysis. We find that applying deep models to the isolated objects, rather than the entire scene as is done in the original paper, results in around 20-30% performance improvement. Relative to the numbers reported in Barbu et al., around 10-15% of the performance loss is recovered, without any test time data augmentation. Despite this gain, however, we conclude that deep models still suffer drastically on the ObjectNet dataset. We also investigate the robustness of models against synthetic image perturbations such as geometric transformations (e.g., scale, rotation, translation), natural image distortions (e.g., impulse noise, blur) as well as adversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting the object area as much as possible (i.e., from the entire image to the bounding box to the segmentation mask) leads to consistent improvement in accuracy and robustness. Finally, through a qualitative analysis of ObjectNet data, we find that i) a large number of images in this dataset are hard to recognize even for humans, and ii) easy (hard) samples for models match with easy (hard) samples for humans. Overall, our analysis shows that ObjecNet is still a challenging test platform that can be used to measure the generalization ability of models. The code and data are available in [masked due to blind review].","['robustness', 'ObjectNet', 'object recognition', 'deep learning']",[],['ali borji'],['PrimerAI'],[]
https://iclr.cc/virtual/2021/poster/2734,Security,How Does Mixup Help With Robustness and Generalization?,"Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However,  it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.","['MixUp', 'adversarial robustness', 'generalization']",[],"['Linjun Zhang', 'Zhun Deng', 'Kenji Kawaguchi', 'Amirata Ghorbani', 'James Zou']","['Rutgers University', 'Columbia University', 'National University of Singapore', 'Stanford University', 'Stanford University']",[]
https://iclr.cc/virtual/2021/poster/2665,Security,Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics,"Poisoning attacks on Reinforcement Learning (RL) systems could take advantage of RL algorithm’s vulnerabilities and cause failure of the learning. However, prior works on poisoning RL usually either unrealistically assume the attacker knows the underlying Markov Decision Process (MDP), or directly apply the poisoning methods in supervised learning to RL. In this work, we build a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. Without any prior knowledge of the MDP, we propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. VA2C-P uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agents and multiple environments show that our poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget.","['deep RL', 'vulnerability of RL', 'poisoning attack', 'policy gradient']",[],"['Yanchao Sun', 'Da Huo', 'Furong Huang']","['J.P. Morgan AI Research', 'Shanghai Jiao Tong University', 'Department of Computer Science, University of Maryland']",[]
https://iclr.cc/virtual/2021/poster/2636,Security,Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent,"Byzantine-resilient Stochastic Gradient Descent (SGD) aims at shielding model training from Byzantine faults, be they ill-labeled training datapoints, exploited software/hardware vulnerabilities, or malicious worker nodes in a distributed setting. Two recent attacks have been challenging state-of-the-art defenses though, often successfully precluding the model from even fitting the training set. The main identified weakness in current defenses is their requirement of a sufficiently low variance-norm ratio for the stochastic gradients. We propose a practical method which, despite increasing the variance, reduces the variance-norm ratio, mitigating the identified weakness. We assess the effectiveness of our method over 736 different training configurations, comprising the 2 state-of-the-art attacks and 6 defenses. For confidence and reproducibility purposes, each configuration is run 5 times with specified seeds (1 to 5), totalling 3680 runs. In our experiments, when the attack is effective enough to decrease the highest observed top-1 cross-accuracy by at least 20% compared to the unattacked run, our technique systematically increases back the highest observed accuracy, and is able to recover at least 20% in more than 60% of the cases.","['momentum', 'Distributed ML', 'Byzantine SGD']",[],"['El-Mahdi El-Mhamdi', 'Rachid Guerraoui', 'Sébastien Rouault']","['Calicarpa', 'Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne']",[]
https://iclr.cc/virtual/2021/poster/2631,Security,Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits,"To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.","['weight attack', 'bit-flip', 'targeted attack']",[],"['Jiawang Bai', 'Baoyuan Wu', 'Yong Zhang', 'Yiming Li', 'Zhifeng Li', 'Shu-Tao Xia']","['Tsinghua University, Tsinghua University', 'The Chinese University of Hong Kong, Shenzhen', 'Tencent AI Lab', 'Zhejiang University', 'Tencent', 'Shenzhen International Graduate School, Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/2629,Security,Efficient Certified Defenses Against Patch Attacks on Image Classifiers,"Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.","['aversarial examples', 'adversarial patch', 'certified defense', 'robustness']",[],"['Jan Hendrik Metzen', 'Maksym Yatsura']","['Bosch Center Artificial Intelligence', 'Robert Bosch GmbH, Bosch']",[]
https://iclr.cc/virtual/2021/poster/2624,Security,Robust Reinforcement Learning on State Observations with Learned Optimal Adversary,"We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLArobustRL.","['adversarial defense', 'robustness', 'adversarial attacks', 'reinforcement learning']",[],"['Huan Zhang', 'Hongge Chen', 'Duane S Boning', 'Cho-Jui Hsieh']","['University of Illinois at Urbana-Champaign', 'Cruise AI Research', 'Massachusetts Institute of Technology', 'Google']",[]
https://iclr.cc/virtual/2021/poster/2600,Security,Conservative Safety Critics for Exploration,"Safe exploration presents a major challenge in reinforcement learning (RL): when active data collection requires deploying partially trained policies, we must ensure that these policies avoid catastrophically unsafe regions, while still enabling trial and error learning. In this paper, we target the problem of safe exploration in RL, by learning a conservative safety estimate of environment states through a critic, and provably upper bound the likelihood of catastrophic failures at every training iteration. We theoretically characterize the tradeoff between safety and policy improvement, show that the safety constraints are satisfied with high probability during training, derive provable convergence guarantees for our approach which is no worse asymptotically then standard RL, and empirically demonstrate the efficacy of the proposed approach on a suite of challenging navigation, manipulation, and locomotion tasks. Our results demonstrate that the proposed approach can achieve competitive task performance, while incurring significantly lower catastrophic failure rates during training as compared to prior methods. Videos are at this URL https://sites.google.com/view/conservative-safety-critics/","['Safe exploration', 'reinforcement learning']",[],"['Homanga Bharadhwaj', 'Aviral Kumar', 'Nicholas Rhinehart', 'Sergey Levine', 'Florian Shkurti', 'Animesh Garg']","['Facebook', 'University of California Berkeley', 'Waymo Research', 'Google', 'Department of Computer Science, University of Toronto', 'Georgia Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2594,Security,Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors,"Predictive uncertainty estimation is an essential next step for the reliable deployment of deep object detectors in safety-critical tasks. In this work, we focus on estimating predictive distributions for bounding box regression output with variance networks. We show that in the context of object detection, training variance networks with negative log likelihood (NLL) can lead to high entropy predictive distributions regardless of the correctness of the output mean. We propose to use the energy score as a non-local proper scoring rule and find that when used for training, the energy score leads to better calibrated and lower entropy predictive distributions than NLL. We also address the widespread use of non-proper scoring metrics for evaluating predictive distributions from deep object detectors by proposing an alternate evaluation approach founded on proper scoring rules. Using the proposed evaluation tools, we show that although variance networks can be used to produce high quality predictive distributions, ad-hoc approaches used by seminal object detectors for choosing regression targets during training do not provide wide enough data support for reliable variance learning. We hope that our work helps shift evaluation in probabilistic object detection to better align with predictive uncertainty evaluation in other machine learning domains. Code for all models, evaluation, and datasets is available at: https://github.com/asharakeh/probdet.git.","['Energy Score', 'Variance Networks', 'Proper Scoring Rules', 'Predictive Uncertainty Estimation', 'computer vision', 'object detection']",[],"['Ali Harakeh', 'Steven L. Waslander']","['Montreal Institute for Learning Algorithms', 'University of Toronto']",[]
https://iclr.cc/virtual/2021/poster/2585,Security,Improving VAEs' Robustness to Adversarial Attack,"Variational autoencoders (VAEs) have recently been shown to be vulnerable to adversarial attacks, wherein they are fooled into reconstructing a chosen target image. However, how to defend against such attacks remains an open problem. We make significant advances in addressing this issue by introducing methods for producing adversarially robust VAEs. Namely, we first demonstrate that methods proposed to obtain disentangled latent representations produce VAEs that are more robust to these attacks. However, this robustness comes at the cost of reducing the quality of the reconstructions. We ameliorate this by applying disentangling methods to hierarchical VAEs. The resulting models produce high--fidelity autoencoders that are also adversarially robust. We confirm their capabilities on several different datasets and with current state-of-the-art VAE adversarial attacks, and also show that they increase the robustness of downstream tasks to attack.","['adversarial attack', 'variational autoencoders', 'robustness', 'deep generative models']",[],"['Matthew Willetts', 'Alexander Camuto', 'Tom Rainforth', 'S Roberts', 'Christopher C. Holmes']","['University College London', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford']",[]
https://iclr.cc/virtual/2021/poster/3226,Security,Beyond Categorical Label Representations for Image Classification,"We find that the way we choose to represent data labels can have a profound effect on the quality of trained models. For example, training an image classifier to regress audio labels rather than traditional categorical probabilities produces a more reliable classification. This result is surprising, considering that audio labels are more complex than simpler numerical probabilities or text. We hypothesize that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. We support this hypothesis with evidence from various label representations including constant matrices, spectrograms, shuffled spectrograms, Gaussian mixtures, and uniform random matrices of various dimensionalities. Our experiments reveal that high dimensional, high entropy labels achieve comparable accuracy to text (categorical) labels on standard image classification tasks, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data. These results suggest that label representation may play a more important role than previously thought.","['image classification', 'Label Representation', 'representation learning']",[],"['Boyuan Chen', 'Sunand Raghupathi', 'Hod Lipson']","['Duke University', 'Columbia University', 'Columbia University']",[]
https://iclr.cc/virtual/2021/poster/3123,Security,InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective,"Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust ﬁne-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.","['QA', 'NLI', 'BERT', 'adversarial robustness', 'information theory', 'adversarial training']",[],"['Boxin Wang', 'Shuohang Wang', 'Yu Cheng', 'Zhe Gan', 'Ruoxi Jia', 'Bo Li', 'Jingjing Liu']","['NVIDIA', 'Microsoft', 'Microsoft Research', 'Apple', 'Virginia Tech', 'University of Illinois, Urbana Champaign', 'Tsinghua University']",[]
https://iclr.cc/virtual/2021/poster/2745,Security,Dataset Inference: Ownership Resolution in Machine Learning,"With increasingly more data and computation involved in their training,  machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient:  the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $\textit{dataset inference}$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.","['MLaaS', 'model extraction', 'model ownership']",[],"['Pratyush Maini', 'Mohammad Yaghini', 'Nicolas Papernot']","['Carnegie Mellon University', 'University of Toronto, Vector Institute', 'University of Toronto']",[]
https://iclr.cc/virtual/2021/poster/2820,Security,Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates,"We study the multi-agent safe control problem where agents should avoid collisions to static obstacles and collisions with each other while reaching their goals. Our core idea is to learn the multi-agent control policy jointly with  learning the control barrier functions as safety certificates. We propose a new joint-learning framework that can be implemented in a decentralized fashion, which can adapt to an arbitrarily large number of agents. Building upon this framework, we further improve the scalability by  incorporating neural network architectures  that are invariant to the quantity and permutation of neighboring agents. In addition, we propose a new spontaneous policy refinement method to further enforce the certificate condition during testing. We provide extensive experiments to demonstrate that our method significantly outperforms other leading multi-agent control approaches in terms of maintaining safety and completing original tasks. Our approach also shows substantial generalization capability in that the control policy can be trained with 8 agents in one scenario, while being used on other scenarios with up to 1024 agents in complex multi-agent environments and dynamics. Videos and source code can be found at https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates.","['control barrier function', 'safe', 'multi-agent', 'reinforcement learning']",[],"['Zengyi Qin', 'Kaiqing Zhang', 'Jingkai Chen', 'Chuchu Fan']","['Massachusetts Institute of Technology', 'University of Maryland, College Park', 'Amazon Robotics', 'Massachusetts Institute of Technology']",[]
https://iclr.cc/virtual/2021/poster/2696,Security,Protecting DNNs from Theft using an Ensemble of Diverse Models,"Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to $39.7\%$). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.","['Model stealing', 'machine learning security']",[],"['Sanjay Kariyappa', 'Atul Prakash', 'Moinuddin K Qureshi']","['J.P. Morgan Chase', 'University of Michigan', 'Georgia Institute of Technology']",[]