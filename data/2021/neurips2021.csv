link,category,title,abstract,keywords,ccs_concepts,author_names,author_affiliations,affiliated_countries
https://nips.cc/virtual/2021/poster/27382,Transparency & Explainability,Cockpit: A Practical Debugging Tool for the Training of Deep Neural Networks,"When engineers train deep learning models, they are very much ""flying blind"". Commonly used methods for real-time training diagnostics, such as monitoring the train/test loss, are limited. Assessing a network's training process solely through these performance indicators is akin to debugging software without access to internal states through a debugger. To address this, we present Cockpit, a collection of instruments that enable a closer look into the inner workings of a learning machine, and a more informative and meaningful status report for practitioners. It facilitates the identification of learning phases and failure modes, like ill-chosen hyperparameters. These instruments leverage novel higher-order information about the gradient distribution and curvature, which has only recently become efficiently accessible. We believe that such a debugging tool, which we open-source for PyTorch, is a valuable help in troubleshooting the training process. By revealing new insights, it also more generally contributes to explainability and interpretability of deep nets.","['Deep Learning', 'Optimization', 'Interpretability']",[],"['Frank Schneider', 'Felix Dangel', 'Philipp Hennig']","['University of Tübingen', 'Vector Institute, Toronto', 'University of Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26675,Transparency & Explainability,Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language,"In this work, we propose a unified framework, called Visual Reasoning with Differ-entiable Physics (VRDP), that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, performs differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by fitting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing benefits.  More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efficient: physical parameters can be optimized from very few videos, and even a single video can be sufficient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from a few examples.",['Interpretability'],[],"['Mingyu Ding', 'Zhenfang Chen', 'Tao Du', 'Ping Luo', 'Joshua B. Tenenbaum', 'Chuang Gan']","['University of California, Berkeley', 'MIT-IBM Watson AI lab', 'Tsinghua University', 'The University of', 'Massachusetts Institute of Technology', 'MIT-IBM Watson AI Lab']","[None, None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26485,Transparency & Explainability,A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration,"Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation,  and the lack of accurate ground-truth ""clean"" hyperspectral signals acquired on the spot makes restoration tasks challenging.  In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and  significantly outperforms the state of the art.","['Deep Learning', 'Interpretability']",[],"['Theo Bodrito', 'Alexandre Zouaoui', 'Jocelyn Chanussot', 'Julien Mairal']","['INRIA', 'INRIA', 'INRIA', 'Inria']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27060,Transparency & Explainability,Regulating algorithmic filtering on social media,"By filtering the content that users see, social media platforms have the ability to influence users' perceptions and decisions, from their dining choices to their voting preferences. This influence has drawn scrutiny, with many calling for regulations on filtering algorithms, but designing and enforcing regulations remains challenging. In this work, we examine three questions. First, given a regulation, how would one design an audit to enforce it? Second, does the audit impose a performance cost on the platform? Third, how does the audit affect the content that the platform is incentivized to filter? In response to these questions, we propose a method such that, given a regulation, an auditor can test whether that regulation is met with only black-box access to the filtering algorithm. We then turn to the platform's perspective. The platform's goal is to maximize an objective function while meeting regulation. We find that there are conditions under which the regulation does not place a high performance cost on the platform and, notably, that content diversity can play a key role in aligning the interests of the platform and regulators.",[],[],"['Sarah Cen', 'Devavrat Shah']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/26454,Transparency & Explainability,Passive attention in artificial neural networks predicts human visual selectivity,"Developments in machine learning interpretability techniques over the past decade have provided new tools to observe the image regions that are most informative for classification and localization in artificial neural networks (ANNs). Are the same regions similarly informative to human observers? Using data from 79 new experiments and 7,810 participants, we show that passive attention techniques reveal a significant overlap with human visual selectivity estimates derived from 6 distinct behavioral tasks including visual discrimination, spatial localization, recognizability, free-viewing, cued-object search, and saliency search fixations. We find that input visualizations derived from relatively simple ANN architectures probed using guided backpropagation methods are the best predictors of a shared component in the joint variability of the human measures. We validate these correlational results with causal manipulations using recognition experiments. We show that images masked with ANN attention maps were easier for humans to classify than control masks in a speeded recognition experiment. Similarly, we find that recognition performance in the same ANN models was likewise influenced by masking input images using human visual selectivity maps. This work contributes a new approach to evaluating the biological and psychological validity of leading ANNs as models of human vision: by examining their similarities and differences in terms of their visual selectivity to the information contained in images.","['Deep Learning', 'Vision', 'Interpretability', 'Machine Learning']",[],"['Thomas A Langlois', 'Haicheng Charles Zhao', 'Erin Grant', 'Ishita Dasgupta', 'Thomas L. Griffiths', 'Nori Jacoby']","['Princeton University', 'Princeton University', 'University College London', 'DeepMind', 'Princeton University', 'Max-Planck Institute']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28837,Transparency & Explainability,Learning interaction rules from multi-animal trajectories via augmented behavioral models,"Extracting the interaction rules of biological agents from movement sequences pose challenges in various domains. Granger causality is a practical framework for analyzing the interactions from observed time-series data; however, this framework ignores the structures and assumptions of the generative process in animal behaviors, which may lead to interpretational problems and sometimes erroneous assessments of causality. In this paper, we propose a new framework for learning Granger causality from multi-animal trajectories via augmented theory-based behavioral models with interpretable data-driven models. We adopt an approach for augmenting incomplete multi-agent behavioral models described by time-varying dynamical systems with neural networks. For efficient and interpretable learning, our model leverages theory-based architectures separating navigation and motion processes, and the theory-guided regularization for reliable behavioral modeling. This can provide interpretable signs of Granger-causal effects over time, i.e., when specific others cause the approach or separation. In experiments using synthetic datasets, our method achieved better performance than various baselines. We then analyzed multi-animal datasets of mice, flies, birds, and bats, which verified our method and obtained novel biological insights.","['Theory', 'Deep Learning', 'Causality', 'Interpretability']",[],"['Keisuke Fujii', 'Naoya Takeishi', 'Kazushi Tsutsui', 'Emyo Fujioka', 'Nozomi Nishiumi', 'Ryooya Tanaka', 'Kaoru Ide', 'Hiroyoshi Kohno', 'Ken Yoda', 'Susumu Takahashi', 'Shizuko Hiryu', 'Yoshinobu Kawahara']","['Nagoya University', 'The University of Tokyo', 'Nagoya University', 'Doshisha University', 'National Institute for Basic Biology', 'Nagoya University', 'Doshisha University', 'Tokai University', 'Nagoya University', 'Doshisha University', 'Doshisha University', 'Osaka University']","[None, None, None, None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28813,Transparency & Explainability,Scalable Rule-Based Representation Learning for Interpretable Classification,"Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.","['Machine Learning', 'Optimization', 'Interpretability', 'Representation Learning']",[],"['Zhuo Wang', 'Wei Zhang', 'Ning Liu', 'Jianyong Wang']","['Tsinghua University, Tsinghua University', 'East  Normal University', 'Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, 'China', None, None]"
https://nips.cc/virtual/2021/poster/28801,Transparency & Explainability,TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification,"Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03% and 98.82% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is available at: https://github.com/szc19990412/TransMIL.","['Transformers', 'Vision', 'Interpretability', 'Machine Learning']",[],"['Hao Bian', 'Yang Chen', 'Yifeng Wang', 'Jian Zhang', 'Xiangyang Ji', 'Yongbing Zhang']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'national university of singaore, National University of', 'Peking University', 'Tsinghua University', 'Harbin Institute of Technology']","[None, None, 'Singapore', None, None, None]"
https://nips.cc/virtual/2021/poster/28602,Transparency & Explainability,"CROCS: Clustering and Retrieval of Cardiac Signals Based on Patient Disease Class, Sex, and Age","The process of manually searching for relevant instances in, and extracting information from, clinical databases underpin a multitude of clinical tasks. Such tasks include disease diagnosis, clinical trial recruitment, and continuing medical education. This manual search-and-extract process, however, has been hampered by the growth of large-scale clinical databases and the increased prevalence of unlabelled instances. To address this challenge, we propose a supervised contrastive learning framework, CROCS, where representations of cardiac signals associated with a set of patient-specific attributes (e.g., disease class, sex, age) are attracted to learnable embeddings entitled clinical prototypes. We exploit such prototypes for both the clustering and retrieval of unlabelled cardiac signals based on multiple patient attributes. We show that CROCS outperforms the state-of-the-art method, DTC, when clustering and also retrieves relevant cardiac signals from a large database. We also show that clinical prototypes adopt a semantically meaningful arrangement based on patient attributes and thus confer a high degree of interpretability.","['Clustering', 'Interpretability', 'Contrastive Learning']",[],"['Dani Kiyasseh', 'Tingting Zhu', 'David A. Clifton']","['California Institute of Technology', 'University of Oxford', 'University of Oxford']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28351,Transparency & Explainability,Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention,"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin","['Transformers', 'Deep Learning', 'Graph Learning', 'Interpretability']",[],"['Byung-Hoon Kim', 'Jong Chul Ye', 'Jae-Jin Kim']","['Massachusetts General Hospital, Harvard University', 'Korea Advanced Institute of Science and Technology', 'Yonsei University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28229,Transparency & Explainability,Neural Additive Models: Interpretable Machine Learning with Neural Nets,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.","['Deep Learning', 'Interpretability', 'Machine Learning']",[],"['Rishabh Agarwal', 'Levi Melnick', 'Nicholas Frosst', 'Xuezhou Zhang', 'Ben Lengerich', 'Rich Caruana', 'Geoffrey Hinton']","['Google Research, Brain Team', 'Microsoft', 'Google', 'Boston University, Boston University', 'Massachusetts Institute of Technology', 'School of Computer Science, Carnegie Mellon University', 'Google']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28215,Transparency & Explainability,Object-Aware Regularization for Addressing Causal Confusion in Imitation Learning,"Behavioral cloning has proven to be effective for learning sequential decision-making policies from expert demonstrations. However, behavioral cloning often suffers from the causal confusion problem where a policy relies on the noticeable effect of expert actions due to the strong correlation but not the cause we desire. This paper presents Object-aware REgularizatiOn (OREO), a simple technique that regularizes an imitation policy in an object-aware manner. Our main idea is to encourage a policy to uniformly attend to all semantic objects, in order to prevent the policy from exploiting nuisance variables strongly correlated with expert actions. To this end, we introduce a two-stage approach: (a) we extract semantic objects from images by utilizing discrete codes from a vector-quantized variational autoencoder, and (b) we randomly drop the units that share the same discrete code together, i.e., masking out semantic objects. Our experiments demonstrate that OREO significantly improves the performance of behavioral cloning, outperforming various other regularization and causality-based methods on a variety of Atari environments and a self-driving CARLA environment. We also show that our method even outperforms inverse reinforcement learning methods trained with a considerable amount of environment interaction.","['Reinforcement Learning and Planning', 'Causality']",[],"['Jongjin Park', 'Younggyo Seo', 'Chang Liu', 'Li Zhao', 'Tao Qin', 'Jinwoo Shin', 'Tie-Yan Liu']","['Korea Advanced Institute of Science and Technology', 'Dyson', 'Microsoft', 'Tsinghua University', 'Microsoft Research Asia', 'Korea Advanced Institute of Science and Technology', 'Microsoft']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28189,Transparency & Explainability,Curriculum Disentangled Recommendation with Noisy Multi-feedback,"Learning disentangled representations for user intentions from multi-feedback (i.e., positive and negative feedback) can enhance the accuracy and explainability of recommendation algorithms. However, learning such disentangled representations from multi-feedback data is challenging because  i) multi-feedback is complex: there exist complex relations among different types of feedback (e.g., click, unclick, and dislike, etc) as well as various user intentions, and ii) multi-feedback is noisy: there exists noisy (useless) information both in features and labels, which may deteriorate the recommendation performance.  Existing works on disentangled representation learning only focus on positive feedback, failing to handle the complex relations and noise hidden in multi-feedback data. To solve this problem, in this work we propose a Curriculum Disentangled Recommendation (CDR) model that is capable of efficiently learning disentangled representations from complex and noisy multi-feedback for better recommendation. Concretely, we design a co-filtering dynamic routing mechanism that simultaneously captures the complex relations among different behavioral feedback and user intentions as well as denoise the representations in the feature level. We then present an adjustable self-evaluating curriculum that is able to evaluate sample difficulties for better model training and conduct denoising in the label level via disregarding useless information. Our extensive experiments on several real-world datasets demonstrate that the proposed CDR model can significantly outperform several state-of-the-art methods in terms of recommendation accuracy.","['Interpretability', 'Representation Learning']",[],"['Hong Chen', 'Yudong Chen', 'Xin Wang', 'Ruobing Xie', 'Rui Wang', 'Feng Xia', 'Wenwu Zhu']","['Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tencent', 'Tencent Wechat Group', 'Institute of Computing Technology, CAS', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28162,Transparency & Explainability,Explaining Latent Representations with a Corpus of Examples,"Modern machine learning models are complicated. Most of them rely on convoluted latent representations of their input to issue a prediction. To achieve greater transparency than a black-box that connects inputs to predictions, it is necessary to gain a deeper understanding of these latent representations. To that aim, we propose SimplEx: a user-centred method that provides example-based explanations with reference to a freely selected set of examples, called the corpus. SimplEx uses the corpus to improve the user’s understanding of the latent space with post-hoc explanations answering two questions: (1) Which corpus examples explain the prediction issued for a given test example? (2) What features of these corpus examples are relevant for the model to relate them to the test example? SimplEx provides an answer by reconstructing the test latent representation as a mixture of corpus latent representations. Further, we propose a novel approach, the integrated Jacobian, that allows SimplEx to make explicit the contribution of each corpus feature in the mixture. Through experiments on tasks ranging from mortality prediction to image classification, we demonstrate that these decompositions are robust and accurate. With illustrative use cases in medicine, we show that SimplEx empowers the user by highlighting relevant patterns in the corpus that explain model representations. Moreover, we demonstrate how the freedom in choosing the corpus allows the user to have personalized explanations in terms of examples that are meaningful for them.","['Vision', 'Interpretability', 'Machine Learning']",[],"['Jonathan Crabbé', 'Zhaozhi Qian', 'Fergus Imrie', 'Mihaela van der Schaar']","['University of Cambridge', 'University of Cambridge', 'University of California, Los Angeles', 'University of Cambridge']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28083,Transparency & Explainability,Counterfactual Maximum Likelihood Estimation for Training Deep Networks,"Although deep learning models have driven state-of-the-art performance on a wide array of tasks, they are prone to spurious correlations that should not be learned as predictive clues. To mitigate this problem, we propose a causality-based training framework to reduce the spurious correlations caused by observed confounders. We give theoretical analysis on the underlying general Structural Causal Model (SCM) and propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely Counterfactual Maximum Likelihood Estimation (CMLE). As the interventional distribution, in general, is hidden from the observational data, we then derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. We conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations.","['Deep Learning', 'Causality', 'Domain Adaptation', 'Language']",[],"['Xinyi Wang', 'Wenhu Chen', 'Michael Saxon', 'William Yang Wang']","['UC Santa Barbara', 'University of Waterloo', 'UC Santa Barbara', 'UC Santa Barbara']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28027,Transparency & Explainability,Adaptive wavelet distillation from neural networks through interpretations,"Recent deep-learning models have achieved impressive prediction performance, but often sacrifice interpretability and computational efficiency. Interpretability is crucial in many disciplines, such as science and medicine, where models must be carefully vetted or where interpretation is the goal itself. Moreover, interpretable models are concise and often yield computational efficiency. Here, we propose adaptive wavelet distillation (AWD), a method which aims to distill information from a trained neural network into a wavelet transform. Specifically, AWD penalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predictive, concise, computationally efficient, and has properties (such as a multi-scale structure) which make it easy to interpret. In close collaboration with domain experts, we showcase how AWD addresses challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases, AWD yields a scientifically interpretable and concise model which gives predictive performance better than state-of-the-art neural networks. Moreover, AWD identifies predictive features that are scientifically meaningful in the context of respective domains. All code and models are released in a full-fledged package available on Github.","['Deep Learning', 'Interpretability']",[],"['Wooseok Ha', 'Chandan Singh', 'Francois Lanusse', 'Srigokul Upadhyayula', 'Bin Yu']","['AWS AI Labs', 'Microsoft research', 'CNRS', 'University of California Berkeley', 'University of California - Berkeley']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28034,Transparency & Explainability,Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery,"More recently, there has been a surge of interest in employing machine learning approaches to expedite the drug discovery process where virtual screening for hit discovery and ADMET prediction for lead optimization play essential roles. One of the main obstacles to the wide success of machine learning approaches in these two tasks is that the number of compounds labeled with activities or ADMET properties is too small to build an effective predictive model. This paper seeks to remedy the problem by transferring the knowledge from previous assays, namely in-vivo experiments, by different laboratories and against various target proteins. To accommodate these wildly different assays and capture the similarity between assays, we propose a functional rationalized meta-learning algorithm FRML for such knowledge transfer. FRML constructs the predictive model with layers of neural sub-networks or so-called functional regions. Building on this, FRML shares an initialization for the weights of the predictive model across all assays, while customizes it to each assay with a region localization network choosing the pertinent regions. The compositionality of the model improves the capacity of generalization to various and even out-of-distribution tasks. Empirical results on both virtual screening and ADMET prediction validate the superiority of FRML over state-of-the-art baselines powered with interpretability in assay relationship.","['Meta Learning', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Huaxiu Yao', 'Ying Wei', 'Long-Kai Huang', 'Ding Xue', 'Junzhou Huang']","['Department of Computer Science, University of North Carolina at Chapel Hill', 'Nanyang Technological University', 'Tencent AI Lab', 'Tencent AI Lab', 'University of Texas, Arlington']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27987,Transparency & Explainability,Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression,"Models like LASSO and ridge regression are extensively used in practice due to their interpretability, ease of use, and strong theoretical guarantees. Cross-validation (CV) is widely used for hyperparameter tuning in these models, but do practical methods minimize the true out-of-sample loss? A recent line of research promises to show that the optimum of the CV loss matches the optimum of the out-of-sample loss (possibly after simple corrections). It remains to show how tractable it is to minimize the CV loss. In the present paper, we show that, in the case of ridge regression, the CV loss may fail to be quasiconvex and thus may have multiple local optima. We can guarantee that the CV loss is quasiconvex in at least one case: when the spectrum of the covariate matrix is nearly flat and the noise in the observed responses is not too high. More generally, we show that quasiconvexity status is independent of many properties of the observed data (response norm, covariate-matrix right singular vectors and singular-value scaling) and has a complex dependence on the few that remain. We empirically confirm our theory using simulated experiments.","['Theory', 'Optimization', 'Interpretability']",[],"['William T. Stephenson', 'Zachary Frangella', 'Madeleine Udell', 'Tamara Broderick']","['MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Stanford University', 'Stanford University', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27910,Transparency & Explainability,Discerning Decision-Making Process of Deep Neural Networks with Hierarchical Voting Transformation,"Neural network based deep learning techniques have shown great success for numerous applications. While it is expected to understand their intrinsic decision-making processes, these deep neural networks often work in a black-box way. To this end, in this paper, we aim to discern the decision-making processes of neural networks through a hierarchical voting strategy by developing an explainable deep learning model, namely Voting Transformation-based Explainable Neural Network (VOTEN). Specifically, instead of relying on massive feature combinations, VOTEN creatively models expressive single-valued voting functions between explicitly modeled latent concepts to achieve high fitting ability. Along this line, we first theoretically analyze the major components of VOTEN and prove the relationship and advantages of VOTEN compared with Multi-Layer Perceptron (MLP), the basic structure of deep neural networks. Moreover, we design efficient algorithms to improve the model usability by explicitly showing the decision processes of VOTEN. Finally, extensive experiments on multiple real-world datasets clearly validate the performances and explainability of VOTEN.","['Deep Learning', 'Interpretability']",[],"['Ying Sun', 'Hengshu Zhu', 'Chuan Qin', 'Fuzhen Zhuang', 'Qing He', 'Hui Xiong']","['University of Science and Technology (Guangzhou)', 'BOSS Zhipin', 'BOSS Zhipin', 'Beihang University', 'Institute of Computing Technology, CAS', 'University of Science and Technology']","['Hong Kong', None, None, None, None, 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27871,Transparency & Explainability,Reliable Post hoc Explanations: Modeling Uncertainty in Explainability,"As black box explanations are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which  output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence.  This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.","['Robustness', 'Interpretability']",[],"['Dylan Z Slack', 'Sophie Hilgard', 'Sameer Singh', 'Himabindu Lakkaraju']","['University of California, Irvine', 'Harvard University', 'University of California, Irvine', 'Harvard University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27847,Transparency & Explainability,Designing Counterfactual Generators using Deep Model Inversion,"Explanation techniques that synthesize small, interpretable changes to a given image while producing desired changes in the model prediction have become popular for introspecting black-box models. Commonly referred to as counterfactuals, the synthesized explanations are required to contain discernible changes (for easy interpretability) while also being realistic (consistency to the data manifold). In this paper, we focus on the case where we have access only to the trained deep classifier and not the actual training data. While the problem of inverting deep models to synthesize images from the training distribution has been explored, our goal is to develop a deep inversion approach to generate counterfactual explanations for a given query image. Despite their effectiveness in conditional image synthesis, we show that existing deep inversion methods are insufficient for producing meaningful counterfactuals. We propose DISC (Deep Inversion for Synthesizing Counterfactuals) that improves upon deep inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. We find that, in addition to producing visually meaningful explanations, the counterfactuals from DISC are effective at learning classifier decision boundaries and are robust to unknown test-time corruptions.","['Optimization', 'Interpretability', 'Representation Learning']",[],"['Jayaraman J. Thiagarajan', 'Vivek Narayanaswamy', 'Deepta Rajan', 'Jia Liang', 'Akshay Chaudhari']","['Lawrence Livermore National Labs', 'Lawrence Livermore National Labs', 'IBM, International Business Machines', 'Stanford University', 'Stanford University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27836,Transparency & Explainability,Emergent Communication of Generalizations,"To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.",['Interpretability'],[],"['Jesse Mu', 'Noah Goodman']","['Stanford University', 'Stanford University']","[None, None]"
https://nips.cc/virtual/2021/poster/27832,Transparency & Explainability,Sampling  with Trusthworthy Constraints:  A Variational Gradient Framework,"Sampling-based inference and learning techniques, especially Bayesian inference, provide an essential approach to handling uncertainty in machine learning (ML). As these techniques are increasingly used in daily life, it becomes essential to safeguard the ML systems with various trustworthy-related constraints, such as fairness, safety, interpretability. Mathematically, enforcing these constraints in probabilistic inference can be cast into sampling from intractable distributions subject to general nonlinear constraints, for which practical efficient algorithms are still largely missing. In this work, we propose a family of constrained sampling algorithms which generalize Langevin Dynamics (LD) and Stein Variational Gradient Descent (SVGD) to incorporate a moment constraint specified by a general nonlinear function. By exploiting the gradient flow structure of LD and SVGD, we derive two types of algorithms for handling constraints, including a primal-dual gradient approach and the constraint controlled gradient descent approach. We investigate the continuous-time mean-field limit of these algorithms and show that they have O(1/t) convergence under mild conditions. Moreover, the LD variant converges linearly assuming that a log Sobolev like inequality holds. Various numerical experiments are conducted to demonstrate the efficiency of our algorithms in trustworthy settings.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Xingchao Liu', 'Xin Tong', 'qiang liu']","['University of Texas, Austin', 'National University of', 'Dartmouth College']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27764,Transparency & Explainability,Learning Riemannian metric for disease progression modeling,"Linear mixed-effect models provide a natural baseline for estimating disease progression using longitudinal data. They provide interpretable models at the cost of modeling assumptions on the progression profiles and their variability across subjects. A significant improvement is to embed the data in a Riemannian manifold and learn patient-specific trajectories distributed around a central geodesic. A few interpretable parameters characterize subject trajectories at the cost of a prior choice of the metric, which determines the shape of the trajectories. We extend this approach by learning the metric from the data allowing more flexibility while keeping the interpretability. Specifically, we learn the metric as the push-forward of the Euclidean metric by a diffeomorphism. This diffeomorphism is estimated iteratively as the composition of radial basis functions belonging to a reproducible kernel Hilbert space. The metric update allows us to improve the forecasting of imaging and clinical biomarkers in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) cohort. Our results compare favorably to the 56 methods benchmarked in the TADPOLE challenge.",['Interpretability'],[],"['Samuel Gruffaz', 'Pierre-Emmanuel Poulet', 'Etienne Maheux', 'Bruno Michel Jedynak', 'Stanley Durrleman']","['Ecole Normale Superieure', 'INRIA', 'INRIA', 'Portland State University', 'Inria']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26456,Transparency & Explainability,Foundations of Symbolic Languages for Model Interpretability,"Several queries and scores have recently been proposed to explain individual predictions over ML models. Examples include queries based on “anchors”, which are parts of an instance that are sufficient to justify its classification, and “feature-perturbation” scores such as SHAP. Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, which allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over two classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated.  We also present a prototype implementation of FOIL wrapped in a high-level declarative language and perform experiments showing that such a language can be used in practice.","['Interpretability', 'Machine Learning']",[],"['Marcelo Arenas', 'Daniel Báez', 'Pablo Barcelo', 'Jorge Pérez', 'Bernardo Subercaseaux']","['RelationalAI', 'Universidad de', 'Pontificia Universidad Católica', 'Universidad de', 'Carnegie Mellon University']","[None, 'Chile', None, 'Chile', None]"
https://nips.cc/virtual/2021/poster/27651,Transparency & Explainability,Garment4D: Garment Reconstruction from Point Cloud Sequences,"Learning to reconstruct 3D garments is important for dressing 3D human bodies of different shapes in different poses. Previous works typically rely on 2D images as input, which however suffer from the scale and pose ambiguities. To circumvent the problems caused by 2D images, we propose a principled framework, Garment4D, that uses 3D point cloud sequences of dressed humans for garment reconstruction. Garment4D has three dedicated steps: sequential garments registration, canonical garment estimation, and posed garment reconstruction. The main challenges are two-fold: 1) effective 3D feature learning for fine details, and 2) capture of garment dynamics caused by the interaction between garments and the human body, especially for loose garments like skirts. To unravel these problems, we introduce a novel Proposal-Guided Hierarchical Feature Network and Iterative Graph Convolution Network, which integrate both high-level semantic features and low-level geometric features for fine details reconstruction. Furthermore, we propose a Temporal Transformer for smooth garment motions capture. Unlike non-parametric methods, the reconstructed garment meshes by our method are separable from the human body and have strong interpretability, which is desirable for downstream tasks. As the first attempt at this task, high-quality reconstruction results are qualitatively and quantitatively illustrated through extensive experiments. Codes are available at https://github.com/hongfz16/Garment4D.","['Transformers', 'Graph Learning', 'Interpretability']",[],"['Fangzhou Hong', 'Liang Pan', 'Zhongang Cai', 'Ziwei Liu']","['Nanyang Technological University', 'Shanghai AI Lab', 'Nanyang Technological University', 'Nanyang Technological University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27559,Transparency & Explainability,CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions,"In recent years there has been a considerable amount of research on local post hoc explanations for neural networks. However, work on building interpretable neural architectures has been relatively sparse. In this paper, we present a novel neural architecture, CoFrNet, inspired by the form of continued fractions which are known to have many attractive properties in number theory, such as fast convergence of approximations to real numbers. We show that CoFrNets can be efficiently trained as well as interpreted leveraging their particular functional form. Moreover, we prove that such architectures are universal approximators based on a proof strategy that is different than the typical strategy used to prove universal approximation results for neural networks based on infinite width (or depth), which is likely to be of independent interest. We experiment on nonlinear synthetic functions and are able to accurately model as well as estimate feature attributions and even higher order terms in some cases, which is a testament to the representational power as well as interpretability of such architectures. To further showcase the power of CoFrNets, we experiment on seven real datasets spanning tabular, text and image modalities, and show that they are either comparable or significantly better than other interpretable models and multilayer perceptrons, sometimes approaching the accuracies of state-of-the-art models.","['Theory', 'Deep Learning', 'Interpretability']",[],"['Isha Puri', 'Amit Dhurandhar', 'Tejaswini Pedapati', 'Karthikeyan Shanmugam', 'Dennis Wei', 'Kush R. Varshney']","['Harvard University', 'International Business Machines', 'International Business Machines', 'Google', 'International Business Machines', 'International Business Machines']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27493,Transparency & Explainability,Hard-Attention for Scalable Image Classification,"Can we leverage high-resolution information without the unsustainable quadratic complexity to input scale? We propose Traversal Network (TNet), a novel multi-scale hard-attention architecture, which traverses image scale-space in a top-down fashion, visiting only the most informative image regions along the way. TNet offers an adjustable trade-off between accuracy and complexity, by changing the number of attended image locations. We compare our model against hard-attention baselines on ImageNet, achieving higher accuracy with less resources (FLOPs, processing time and memory). We further test our model on fMoW dataset, where we process satellite images of size up to $896 \times 896$ px, getting up to $2.5$x faster processing compared to baselines operating on the same resolution, while achieving higher accuracy as well. TNet is modular, meaning that most classification models could be adopted as its backbone for feature extraction, making the reported performance gains orthogonal to benefits offered by existing optimized deep models. Finally, hard-attention guarantees a degree of interpretability to our model's predictions, without any extra cost beyond inference.","['Vision', 'Interpretability', 'Machine Learning']",[],"['Athanasios Papadopoulos', 'Pawel Korus', 'Nasir Memon']","['New York University', 'New York University', 'New York University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27794,Transparency & Explainability,Understanding Instance-based Interpretability of Variational Auto-Encoders,"Instance-based interpretation methods have been widely studied for supervised learning methods as they help explain how black box neural networks predict. However, instance-based interpretations remain ill-understood in the context of unsupervised learning. In this paper, we investigate influence functions [Koh and Liang, 2017], a popular instance-based interpretation method, for a class of deep generative models called variational auto-encoders (VAE). We formally frame the counter-factual question answered by influence functions in this setting, and through theoretical analysis, examine what they reveal about the impact of training samples on classical unsupervised learning methods. We then introduce VAE- TracIn, a computationally efficient and theoretically sound solution based on Pruthi et al. [2020], for VAEs. Finally, we evaluate VAE-TracIn on several real world datasets with extensive quantitative and qualitative analysis.","['Deep Learning', 'Self-Supervised Learning', 'Interpretability', 'Generative Model']",[],"['Zhifeng Kong', 'Kamalika Chaudhuri']","['NVIDIA', 'UC San Diego, University of California, San Diego']","[None, None]"
https://nips.cc/virtual/2021/poster/27327,Transparency & Explainability,Fair Sortition Made Transparent,"Sortition is an age-old democratic paradigm, widely manifested today through the random selection of citizens' assemblies. Recently-deployed algorithms select assemblies \textit{maximally fairly}, meaning that subject to demographic quotas, they give all potential participants as equal a chance as possible of being chosen.  While these fairness gains can bolster the legitimacy of citizens' assemblies and facilitate their uptake, existing algorithms remain limited by their lack of transparency. To overcome this hurdle, in this work we focus on panel selection by uniform lottery, which is easy to realize in an observable way. By this approach, the final assembly is selected by uniformly sampling some pre-selected set of $m$ possible assemblies. We provide theoretical guarantees on the fairness attainable via this type of uniform lottery, as compared to the existing maximally fair but opaque algorithms, for two different fairness objectives. We complement these results with experiments on real-world instances that demonstrate the viability of the uniform lottery approach as a method of selecting assemblies both fairly and transparently.","['Fairness', 'Graph Learning']",[],"['Bailey Flanigan', 'Gregory Kehne', 'Ariel D. Procaccia']","['CMU, Carnegie Mellon University', 'Harvard University', 'Harvard University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27250,Transparency & Explainability,Do Input Gradients Highlight Discriminative Features?,"Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach: 1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A). 2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models. 3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A). Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github.com/harshays/inputgradients.","['Vision', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Interpretability']",[],"['Harshay Shah', 'Prateek Jain', 'Praneeth Netrapalli']","['Massachusetts Institute of Technology', 'Google', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27169,Transparency & Explainability,Explaining Hyperparameter Optimization via Partial Dependence Plots,"Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models. However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance. This lack of explainability makes it difficult to trust and understand the automated HPO process and its results. We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO with Bayesian optimization (BO). BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias. Hence, many IML techniques, such as the partial dependence plot (PDP), carry the risk of generating biased interpretations. By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands. We propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions. In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.","['Optimization', 'Interpretability', 'Machine Learning']",[],"['Julia Moosbauer', 'Julia Herbinger', 'Giuseppe Casalicchio', 'Marius Lindauer', 'Bernd Bischl']","['Department of Statistics', 'Institut für Statistik', 'Ludwig-Maximilians-Universität München', 'Leibniz Universität Hannover', 'LMU']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27172,Transparency & Explainability,Learning Causal Semantic Representation for Out-of-Distribution Prediction,"Conventional supervised learning methods, especially deep ones, are found to be sensitive to out-of-distribution (OOD) examples, largely because the learned representation mixes the semantic factor with the variation factor due to their domain-specific correlation, while only the semantic factor causes the output. To address the problem, we propose a Causal Semantic Generative model (CSG) based on a causal reasoning so that the two factors are modeled separately, and develop methods for OOD prediction from a single training domain, which is common and challenging. The methods are based on the causal invariance principle, with a novel design in variational Bayes for both efficient learning and easy prediction. Theoretically, we prove that under certain conditions, CSG can identify the semantic factor by fitting training data, and this semantic-identification guarantees the boundedness of OOD generalization error and the success of adaptation. Empirical study shows improved OOD performance over prevailing baselines.","['Generative Model', 'Representation Learning', 'Domain Adaptation']",[],"['Chang Liu', 'Xinwei Sun', 'Jindong Wang', 'Haoyue Tang', 'Tao Li', 'Tao Qin', 'Wei Chen', 'Tie-Yan Liu']","['Microsoft', 'Fudan University', 'Microsoft Research', 'Meta AI', 'Peking University', 'Microsoft Research Asia', ' Chinese Academy of Sciences', 'Microsoft']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27117,Transparency & Explainability,On Locality of Local Explanation Models,"Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of  neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse  feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers.","['Robustness', 'Interpretability']",[],"['Lucile Ter-Minassian', 'Karla DiazOrdaz', 'Christopher C. Holmes']","['University of Oxford', 'LSHTM', 'University of Oxford']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27104,Transparency & Explainability,Neural Population Geometry Reveals the Role of Stochasticity in Robust Perception,"Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrate that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.","['Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Neuroscience']",[],"['Joel Dapello', 'Jenelle Feather', 'Hang Le', 'Tiago Marques', 'David Daniel Cox', 'Josh Mcdermott', 'James J. DiCarlo', 'SueYeon Chung']","['Altos Labs', 'Flatiron Institute', 'Massachusetts Institute of Technology', 'Champalimaud Foundation', 'International Business Machines', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'New York University']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27019,Transparency & Explainability,The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations,"Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model confidence caused by removing certain input features at test time. For example, in the standard Sufficiency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to remove features from an input when creating or evaluating explanations: the fact that these counterfactual inputs are out-of-distribution (OOD) to models implies that the resulting explanations are socially misaligned. The crux of the problem is that the model prior and random weight initialization influence the explanations (and explanation metrics) in unintended ways. To resolve this issue, we propose a simple alteration to the model training process, which results in more socially aligned explanations and metrics. Second, we compare among five approaches for removing features from model inputs. We find that some methods produce more OOD counterfactuals than others, and we make recommendations for selecting a feature-replacement function. Finally, we introduce four search-based methods for identifying FI explanations and compare them to strong baselines, including LIME, Anchors, and Integrated Gradients. Through experiments with six diverse text classification datasets, we find that the only method that consistently outperforms random search is a Parallel Local Search (PLS) that we introduce. Improvements over the second best method are as large as 5.4 points for Sufficiency and 17 points for Comprehensiveness.","['Interpretability', 'Machine Learning']",[],"['Peter Hase', 'Harry Xie', 'Mohit Bansal']","['University of North Carolina, Chapel Hill', 'Department of Computer Science, University of North Carolina, Chapel Hill', 'University of North Carolina at Chapel Hill']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26995,Transparency & Explainability,The Utility of Explainable AI in Ad Hoc Human-Machine Teaming,"Recent advances in machine learning have led to growing interest in Explainable AI (xAI) to enable humans to gain insight into the decision-making of machine learning models. Despite this recent interest, the utility of xAI techniques has not yet been characterized in human-machine teaming. Importantly, xAI offers the promise of enhancing team situational awareness (SA) and shared mental model development, which are the key characteristics of effective human-machine teams. Rapidly developing such mental models is especially critical in ad hoc human-machine teaming, where agents do not have a priori knowledge of others' decision-making strategies. In this paper, we present two novel human-subject experiments quantifying the benefits of deploying xAI techniques within a human-machine teaming scenario. First, we show that xAI techniques can support SA ($p<0.05)$. Second, we examine how different SA levels induced via a collaborative AI policy abstraction affect ad hoc human-machine teaming performance. Importantly, we find that the benefits of xAI are not universal, as there is a strong dependence on the composition of the human-machine team. Novices benefit from xAI providing increased SA ($p<0.05$) but are susceptible to cognitive overhead ($p<0.05$). On the other hand, expert performance degrades with the addition of xAI-based support ($p<0.05$), indicating that the cost of paying attention to the xAI outweighs the benefits obtained from being provided additional information to enhance SA. Our results demonstrate that researchers must deliberately design and deploy the right xAI techniques in the right scenario by carefully considering human-machine team composition and how the xAI method augments SA.","['Interpretability', 'Machine Learning']",[],"['Rohan R Paleja', 'Muyleng Ghuy', 'Nadun Ranawaka Arachchige', 'Reed Jensen', 'Matthew Gombolay']","['Institute of Technology', 'Institute of Technology', 'Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'College of Computing,  Institute of Technology']","['Georgia', 'Georgia', 'Georgia', None, 'Georgia']"
https://nips.cc/virtual/2021/poster/26936,Transparency & Explainability,Sparsely Changing Latent States for Prediction and Planning in Partially Observable Domains,"A common approach to prediction and planning in partially observable domains is to use recurrent neural networks (RNNs), which ideally develop and maintain a latent memory about hidden, task-relevant factors. We hypothesize that many of these hidden factors in the physical world are constant over time, changing only sparsely. To study this hypothesis, we propose Gated $L_0$ Regularized Dynamics (GateL0RD), a novel recurrent architecture that incorporates the inductive bias to maintain stable, sparsely changing latent states.  The bias is implemented by means of a novel internal gating function and a penalty on the $L_0$ norm of latent state changes. We demonstrate that GateL0RD can compete with or outperform state-of-the-art RNNs in a variety of partially observable prediction and control tasks. GateL0RD tends to encode the underlying generative factors of the environment, ignores spurious temporal dependencies, and generalizes better, improving sampling efficiency and overall performance in model-based planning and reinforcement learning tasks. Moreover, we show that the developing latent states can be easily interpreted, which is a step towards better explainability in RNNs.","['Reinforcement Learning and Planning', 'Deep Learning', 'Interpretability']",[],"['Christian Gumbsch', 'Martin V. Butz', 'Georg Martius']","['University of Tuebingen', 'University of Tuebingen', 'Eberhard-Karls-Universität Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28176,Transparency & Explainability,Reinforcement Learning Enhanced Explainer for Graph Neural Networks,"Graph neural networks (GNNs) have recently emerged as revolutionary technologies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most influential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer, which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability.","['Reinforcement Learning and Planning', 'Optimization', 'Machine Learning', 'Graph Learning', 'Deep Learning', 'Interpretability']",[],"['Caihua Shan', 'Yifei Shen', 'Yao Zhang', 'Xiang Li', 'Dongsheng Li']","['Microsoft', 'Microsoft Research Asia', 'eBay Inc.', 'East  Normal University', 'Microsoft Research Asia']","[None, None, None, 'China', None]"
https://nips.cc/virtual/2021/poster/26873,Transparency & Explainability,IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers,"The self-attention-based model, transformer, is recently becoming the leading backbone in the field of computer vision. In spite of the impressive success made by transformers in a variety of vision tasks, it still suffers from heavy computation and intensive memory costs. To address this limitation, this paper presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$). We start by observing a large amount of redundant computation, mainly spent on uncorrelated input patches, and then introduce an interpretable module to dynamically and gracefully drop these redundant patches. This novel framework is then extended to a hierarchical structure, where uncorrelated tokens at different stages are gradually removed, resulting in a considerable shrinkage of computational cost. We include extensive experiments on both image and video tasks, where our method could deliver up to 1.4x speed-up for state-of-the-art models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy. More importantly, contrary to other acceleration approaches, our method is inherently interpretable with substantial visual evidence, making vision transformer closer to a more human-understandable architecture while being lighter. We demonstrate that the interpretability that naturally emerged in our framework can outperform the raw attention learned by the original visual transformer, as well as those generated by off-the-shelf interpretation methods, with both qualitative and quantitative results. Project Page: http://people.csail.mit.edu/bpan/ia-red/.","['Transformers', 'Deep Learning', 'Vision', 'Interpretability']",[],"['Bowen Pan', 'Rameswar Panda', 'Yifan Jiang', 'Zhangyang Wang', 'Rogerio Feris']","['Massachusetts Institute of Technology', 'MIT-IBM Watson AI Lab', 'University of Texas, Austin', 'University of Texas at Austin', 'International Business Machines']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26687,Transparency & Explainability,Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer,"Dealing with severe class imbalance poses a major challenge for many real-world applications, especially when the accurate classification and generalization of minority classes are of primary interest. In computer vision and NLP, learning from datasets with long-tail behavior is a recurring theme, especially for naturally occurring labels. Existing solutions mostly appeal to sampling or weighting adjustments to alleviate the extreme imbalance, or impose inductive bias to prioritize generalizable associations. Here we take a novel perspective to promote sample efficiency and model generalization based on the invariance principles of causality. Our contribution posits a meta-distributional scenario, where the causal generating mechanism for label-conditional features is invariant across different labels. Such causal assumption enables efficient knowledge transfer from the dominant classes to their under-represented counterparts, even if their feature distributions show apparent disparities. This allows us to leverage a causal data augmentation procedure to enlarge the representation of minority classes. Our development is orthogonal to the existing imbalanced data learning techniques thus can be seamlessly integrated. The proposed approach is validated on an extensive set of synthetic and real-world tasks against state-of-the-art solutions.","['Vision', 'Machine Learning', 'Contrastive Learning', 'Causality', 'Representation Learning']",[],"['Junya Chen', 'Zidi Xiu', 'Benjamin Goldstein', 'Ricardo Henao', 'Lawrence Carin', 'Chenyang Tao']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26633,Transparency & Explainability,Explicable Reward Design for Reinforcement Learning Agents,"We study the design of explicable reward functions for a reinforcement learning agent while guaranteeing that an optimal policy induced by the function belongs to a set of target policies. By being explicable, we seek to capture two properties: (a) informativeness so that the rewards speed up the agent's convergence, and (b) sparseness as a proxy for ease of interpretability of the rewards. The key challenge is that higher informativeness typically requires dense rewards for many learning tasks, and existing techniques do not allow one to balance these two properties appropriately. In this paper, we investigate the problem from the perspective of discrete optimization and introduce a novel framework, ExpRD, to design explicable reward functions. ExpRD builds upon an informativeness criterion that captures the (sub-)optimality of target policies at different time horizons in terms of actions taken from any given starting state. We provide a  mathematical analysis of ExpRD, and show its connections to existing reward design techniques, including potential-based reward shaping. Experimental results on two navigation tasks demonstrate the effectiveness of ExpRD in designing explicable reward functions.","['Reinforcement Learning and Planning', 'Optimization', 'Interpretability']",[],"['Rati Devidze', 'Goran Radanovic', 'Parameswaran Kamalaruban', 'Adish Singla']","['MPI-SWS', 'MPI-SWS', 'Alan Turing Institute', 'MPI-SWS']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26595,Transparency & Explainability,Differentiable Synthesis of Program Architectures,"Differentiable programs have recently attracted much interest due to their interpretability, compositionality, and their efficiency to leverage differentiable training. However, synthesizing differentiable programs requires optimizing over a combinatorial, rapidly exploded space of program architectures. Despite the development of effective pruning heuristics, previous works essentially enumerate the discrete search space of program architectures, which is inefficient. We propose to encode program architecture search as learning the probability distribution over all possible program derivations induced by a context-free grammar. This allows the search algorithm to efficiently prune away unlikely program derivations to synthesize optimal program architectures. To this end, an efficient gradient-descent based method is developed to conduct program architecture search in a continuous relaxation of the discrete space of grammar rules. Experiment results on four sequence classification tasks demonstrate that our program synthesizer excels in discovering program architectures that lead to differentiable programs with higher F1 scores, while being more efficient than state-of-the-art program synthesis methods.","['Optimization', 'Interpretability', 'Machine Learning']",[],"['Guofeng Cui', 'He Zhu']","['Rutgers University', 'Rutgers University']","[None, None]"
https://nips.cc/virtual/2021/poster/26541,Transparency & Explainability,Invariance Principle Meets Information Bottleneck for Out-of-Distribution Generalization,"The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label.  Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufficient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. In contrast to the linear regression tasks, we show that for linear classification tasks we need much stronger restrictions on the distribution shifts, or otherwise OOD generalization is impossible.  Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufficient. We prove that a form of the information bottleneck constraint along with invariance helps address the key failures when invariant features capture all the information about the label and also retains the existing success when they do not. We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments.","['Theory', 'Causality', 'Machine Learning']",[],"['Kartik Ahuja', 'Ethan Caballero', 'Dinghuai Zhang', 'Jean-Christophe Gagnon-Audet', 'Yoshua Bengio', 'Ioannis Mitliagkas', 'Irina Rish']","['FAIR (Meta)', 'Mila', 'Mila, University of Montreal', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'University of Montreal', 'Université de Montréal', 'University of Montreal']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26496,Transparency & Explainability,Compositional Transformers for Scene Generation,"We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.","['Transformers', 'Generative Model', 'Interpretability']",[],"['Drew Arad Hudson', 'C. Lawrence Zitnick']","['Google DeepMind', 'Meta']","[None, None]"
https://nips.cc/virtual/2021/poster/26410,Transparency & Explainability,Improving Deep Learning Interpretability by Saliency Guided Training,"Saliency methods have been widely used to highlight important input features in model predictions. Most existing methods use backpropagation on a modified gradient function to generate saliency maps. Thus, noisy gradients can result in unfaithful feature attributions. In this paper, we tackle this issue and introduce a {\it saliency guided training} procedure for neural networks to reduce noisy gradients used in predictions while retaining the predictive performance of the model. Our saliency guided training procedure iteratively masks features with small and potentially noisy gradients while maximizing the similarity of model outputs for both masked and unmasked inputs. We apply the saliency guided training procedure to various synthetic and real data sets from computer vision, natural language processing, and time series across diverse neural architectures, including Recurrent Neural Networks, Convolutional Networks, and Transformers. Through qualitative and quantitative evaluations, we show that saliency guided training procedure significantly improves model interpretability across various domains while preserving its predictive performance.","['Transformers', 'Vision', 'Language', 'Deep Learning', 'Interpretability']",[],"['Aya Abdelsalam Ismail', 'Hector Corrada Bravo', 'Soheil Feizi']","['Genentech', 'Genentech', 'University of Maryland, College Park']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26401,Transparency & Explainability,Multi-Agent Reinforcement Learning for Active Voltage Control on Power Distribution Networks,"This paper presents a problem in power networks that creates an exciting and yet challenging real-world scenario for application of multi-agent reinforcement learning (MARL). The emerging trend of decarbonisation is placing excessive stress on power distribution networks. Active voltage control is seen as a promising solution to relieve power congestion and improve voltage quality without extra hardware investment, taking advantage of the controllable apparatuses in the network, such as roof-top photovoltaics (PVs) and static var compensators (SVCs). These controllable apparatuses appear in a vast number and are distributed in a wide geographic area, making MARL a natural candidate. This paper formulates the active voltage control problem in the framework of Dec-POMDP and establishes an open-source environment. It aims to bridge the gap between the power community and the MARL community and be a drive force towards real-world applications of MARL algorithms. Finally, we analyse the special characteristics of the active voltage control problems that cause challenges (e.g. interpretability) for state-of-the-art MARL approaches, and summarise the potential directions.","['Reinforcement Learning and Planning', 'Graph Learning']",[],"['Jianhong Wang', 'Wangkun Xu', 'Wenbin Song']","['University of Manchester', 'Imperial College London', 'Shanghaitech University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26438,Transparency & Explainability,A Causal Lens for Controllable Text Generation,"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of  rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.","['Causality', 'Language']",[],"['Zhiting Hu', 'Li Erran Li']","['University of California, San Diego', 'Amazon']","[None, None]"
https://nips.cc/virtual/2021/poster/25949,Transparency & Explainability,Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi,"Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance.","['Reinforcement Learning and Planning', 'Deep Learning', 'Interpretability']",[],"['Ho Chit Siu', 'Jaime Daniel Pena', 'Edenna Chen', 'Yutai Zhou', 'Victor Lopez', 'Kyle Palko', 'Kimberlee Chestnut Chang', 'Ross Emerson Allen']","['MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'MIT Lincoln Laboratory, Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25922,Transparency & Explainability,Contrastively Disentangled Sequential  Variational Autoencoder,"Self-supervised disentangled representation learning is a critical task in sequence modeling. The learnt representations contribute to better model interpretability as well as the data generation, and improve the sample efficiency for downstream tasks. We propose a novel sequence representation learning method, named Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE), to extract and separate the static (time-invariant) and dynamic (time-variant) factors in the latent space. Different from previous sequential variational autoencoder methods, we use a novel evidence lower bound which maximizes the mutual information between the input and the latent factors, while penalizes the mutual information between the static and dynamic factors. We leverage contrastive estimations of the mutual information terms in training, together with simple yet effective augmentation techniques, to introduce additional inductive biases. Our experiments show that C-DSVAE significantly outperforms the previous state-of-the-art methods on multiple metrics.","['Self-Supervised Learning', 'Contrastive Learning', 'Generative Model', 'Interpretability', 'Representation Learning']",[],"['Junwen Bai', 'Weiran Wang', 'Carla P Gomes']","['Google', 'Google', 'Cornell University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26500,Transparency & Explainability,"Independent mechanism analysis, a new concept?","Independent component analysis provides a principled framework for unsupervised representation learning, with solid theory on the identifiability of the latent code that generated the data, given only observations of mixtures thereof. Unfortunately, when the mixing is nonlinear, the model is provably nonidentifiable, since statistical independence alone does not sufficiently constrain the problem. Identifiability can be recovered in settings where additional, typically observed variables are included in the generative process. We investigate an alternative path and consider instead including assumptions reflecting the principle of independent causal mechanisms exploited in the field of causality. Specifically, our approach is motivated by thinking of each source as independently influencing the mixing process. This gives rise to a framework which we term independent mechanism analysis. We provide theoretical and empirical evidence that our approach circumvents a number of nonidentifiability issues arising in nonlinear blind source separation.","['Theory', 'Causality', 'Representation Learning']",[],"['Luigi Gresele', 'Julius Von Kügelgen', 'Vincent Stimper', 'Michel Besserve']","['Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'Max Planck Institute for Intelligent Systems, Max-Planck Institute', 'MPI for Intelligent Systems']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27096,Transparency & Explainability,Auditing Black-Box Prediction Models for Data Minimization Compliance,"In this paper, we focus on auditing black-box prediction models for compliance with the GDPR’s data minimization principle. This principle restricts prediction models to use the minimal information that is necessary for performing the task at hand. Given the challenge of the black-box setting, our key idea is to check if each of the prediction model’s input features is individually necessary by assigning it some constant value (i.e., applying a simple imputation) across all prediction instances, and measuring the extent to which the model outcomes would change. We introduce a metric for data minimization that is based on model instability under simple imputations. We extend the applicability of this metric from a finite sample model to a distributional setting by introducing a probabilistic data minimization guarantee, which we derive using a Bayesian approach. Furthermore, we address the auditing problem under a constraint on the number of queries to the prediction system. We formulate the problem of allocating a budget of system queries to feasible simple imputations (for investigating model instability) as a multi-armed bandit framework with probabilistic success metrics. We define two bandit problems for providing a probabilistic data minimization guarantee at a given confidence level: a decision problem given a data minimization level, and a measurement problem given a fixed query budget. We design efficient algorithms for these auditing problems using novel exploration strategies that expand classical bandit strategies. Our experiments with real-world prediction systems show that our auditing algorithms significantly outperform simpler benchmarks in both measurement and decision problems.","['Reinforcement Learning and Planning', 'Bandits', 'Privacy']",[],"['Bashir Rastegarpanah', 'Krishna P. Gummadi', 'Mark Crovella']","['Boston University', 'MPI-SWS', 'Boston University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28445,Transparency & Explainability,Learning Tree Interpretation from Object Representation for Deep Reinforcement Learning,"Interpreting Deep Reinforcement Learning (DRL) models is important to enhance trust and comply with transparency regulations. Existing methods typically explain a DRL model by visualizing the importance of low-level input features with super-pixels, attentions, or saliency maps. Our approach provides an interpretation based on high-level latent object features derived from a disentangled representation. We propose a Represent And Mimic (RAMi) framework for training 1) an identifiable latent representation to capture the independent factors of variation for the objects and 2) a mimic tree that extracts the causal impact of the latent features on DRL action values. To jointly optimize both the fidelity and the simplicity of a mimic tree, we derive a novel Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. Based on this objective, we describe a Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to find the IB-optimal mimic tree. Experiments show that our mimic tree achieves strong approximation performance with significantly fewer nodes than baseline models. We demonstrate the interpretability of our mimic tree by showing latent traversals, decision rules, causal impacts, and human evaluation results.","['Reinforcement Learning and Planning', 'Interpretability']",[],"['Guiliang Liu', 'Oliver Schulte', 'Pascal Poupart']","['The Chinese University of , Shenzhen', 'Simon Fraser University', 'University of Waterloo']","['Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/28627,Transparency & Explainability,Multilingual Pre-training with Universal Dependency Learning,"The pre-trained language model (PrLM) demonstrates domination in downstream natural language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low-resource languages. Despite its successes, the performance of multilingual PrLM is still unsatisfactory, when multilingual PrLMs only focus on plain text and ignore obvious universal linguistic structure clues. Existing PrLMs have shown that monolingual linguistic structure knowledge may bring about better performance. Thus we propose a novel multilingual PrLM that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in our model, which brings unprecedented PrLM interpretability and convenience in downstream task use. Our model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross-lingual modeling capabilities of our approach.","['Interpretability', 'Language']",[],"['Kailai Sun', 'Zuchao Li', 'hai zhao']","['Shanghai Jiao Tong University, Shanghai Jiao Tong University', 'Wuhan University', 'Shanghai Jiao Tong University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27775,Transparency & Explainability,How Well do Feature Visualizations Support Causal Understanding of CNN Activations?,"A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task ($68 \pm 4$% accuracy; baseline performance without any visualizations is $60 \pm 3$%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance ($66\pm3$% to $67 \pm3$% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better ""causal understanding"" of unit activations than simple alternative visualizations.",['Interpretability'],[],"['Roland Simon Zimmermann', 'Judy Borowski', 'Robert Geirhos', 'Matthias Bethge', 'Thomas S. A. Wallis', 'Wieland Brendel']","['Eberhard-Karls-Universität Tübingen', 'University of Tuebingen', 'Google DeepMind', 'University of Tuebingen', 'TU Darmstadt', 'ELLIS Institute Tübingen']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28825,Transparency & Explainability,Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models,"While probabilistic models are an important tool for studying causality, doing so suffers from the intractability of inference. As a step towards tractable causal models, we consider the problem of learning interventional distributions using sum-product networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. Providing an arbitrarily intervened causal graph as input, effectively subsuming Pearl's do-operator, the gate function predicts the parameters of the SPN. The resulting interventional SPNs are motivated and illustrated by a structural causal model themed around personal health. Our empirical evaluation against competing methods from both generative and causal modelling demonstrates that interventional SPNs indeed are both expressive and causally adequate.","['Deep Learning', 'Graph Learning', 'Generative Model', 'Causality']",[],"['Matej Zecevic', 'Devendra Singh Dhami', 'Athresh Karanam', 'Sriraam Natarajan', 'Kristian Kersting']","['TU Darmstadt', 'Eindhoven University of Technology', 'University of Texas, Dallas', 'University of Texas at Dallas', 'German Research Center for AI']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27533,Transparency & Explainability,Self-Interpretable Model with Transformation Equivariant Interpretation,"With the proliferation of machine learning applications in the real world, the demand for explaining machine learning predictions continues to grow especially in high-stakes fields. Recent studies have found that interpretation methods can be sensitive and unreliable, where the interpretations can be disturbed by perturbations or transformations of input data. To address this issue, we propose to learn robust interpretation through transformation equivariant regularization in a self-interpretable model. The resulting model is capable of capturing valid interpretation that is equivariant to geometric transformations. Moreover, since our model is self-interpretable, it enables faithful interpretations that reflect the true predictive mechanism. Unlike existing self-interpretable models, which usually sacrifice expressive power for the sake of interpretation quality, our model preserves the high expressive capability comparable to the state-of-the-art deep learning models in complex tasks, while providing visualizable and faithful high-quality interpretation. We compare with various related methods and validate the interpretation quality and consistency of our model.","['Deep Learning', 'Vision', 'Machine Learning']",[],"['Yipei Wang', 'Xiaoqian Wang']","['Purdue University', 'Purdue University']","[None, None]"
https://nips.cc/virtual/2021/poster/26692,Transparency & Explainability,Deep Synoptic Monte-Carlo Planning in Reconnaissance Blind Chess,"This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large imperfect information games. The algorithm constructs a belief state with an unweighted particle filter and plans via playouts that start at samples drawn from the belief state. The algorithm accounts for uncertainty by performing inference on ""synopses,"" a novel stochastic abstraction of information states. DSMCP is the basis of the program Penumbra, which won the official 2020 reconnaissance blind chess competition versus 33 other programs. This paper also evaluates algorithm variants that incorporate caution, paranoia, and a novel bandit algorithm. Furthermore, it audits the synopsis features used in Penumbra with per-bit saliency statistics.","['Reinforcement Learning and Planning', 'Deep Learning', 'Bandits']",[],['Gregory Clark'],['Google'],[None]
https://nips.cc/virtual/2021/poster/27050,Transparency & Explainability,Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling,"Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.","['Deep Learning', 'Generative Model', 'Interpretability', 'Machine Learning']",[],"['Naoya Takeishi', 'Alexandros Kalousis']","['The University of Tokyo', 'University Of Geneva,']","[None, 'Switzerland']"
https://nips.cc/virtual/2021/poster/28517,Transparency & Explainability,Towards Multi-Grained Explainability for Graph Neural Networks,"When a graph neural network (GNN) made a prediction, one raises question about explainability: “Which fraction of the input graph is most inﬂuential to the model’s decision?” Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the ﬂexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and ﬁne-tuning idea to develop our explainer and generate multi-grained explanations. Speciﬁcally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the ﬁne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classiﬁcation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.","['Deep Learning', 'Graph Learning', 'Interpretability']",[],"['Xiang Wang', 'Yingxin Wu', 'An Zhang', 'Xiangnan He', 'Tat-seng Chua']","['University of Science and Technology of', 'Computer Science Department, Stanford University', 'National University of', 'University of Science and Technology of', 'National University of']","['China', None, 'Singapore', 'China', 'Singapore']"
https://nips.cc/virtual/2021/poster/27877,Transparency & Explainability,Counterfactual Explanations Can Be Manipulated,"Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions.  As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust.  Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation.  We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.",['Robustness'],[],"['Dylan Z Slack', 'Sophie Hilgard', 'Himabindu Lakkaraju', 'Sameer Singh']","['University of California, Irvine', 'Harvard University', 'Harvard University', 'University of California, Irvine']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27763,Transparency & Explainability,Nonparametric estimation of continuous DPPs with kernel methods,"Determinantal Point Process (DPPs) are statistical models for repulsive point patterns. Both sampling and inference are tractable for DPPs, a rare feature among models with negative dependence that explains their popularity in machine learning and spatial statistics. Parametric and nonparametric inference methods have been proposed in the finite case, i.e. when the point patterns live in a finite ground set. In the continuous case, only parametric methods have been investigated, while nonparametric maximum likelihood for DPPs -- an optimization problem over trace-class operators -- has remained an open question. In this paper, we show that a restricted version of this maximum likelihood (MLE) problem falls within the scope of a recent representer theorem for nonnegative functions in an RKHS. This leads to a finite-dimensional problem, with strong statistical ties to the original MLE. Moreover, we propose, analyze, and demonstrate a fixed point algorithm to solve this finite-dimensional problem. Finally, we also provide a controlled estimate of the correlation kernel of the DPP, thus providing more interpretability.","['Optimization', 'Interpretability', 'Machine Learning', 'Kernel Methods']",[],"['Michaël Fanuel', 'Rémi Bardenet']","['CNRS, Univ. de Lille', 'CNRS & Univ. Lille']","[None, None]"
https://nips.cc/virtual/2021/poster/27702,Transparency & Explainability,Topic Modeling Revisited: A Document Graph-based Neural Network Perspective,"Most topic modeling approaches are based on the bag-of-words assumption, where each word is required to be conditionally independent in the same document. As a result, both of the generative story and the topic formulation have totally ignored the semantic dependency among words, which is important for improving the semantic comprehension and model interpretability. To this end, in this paper, we revisit the task of topic modeling by transforming each document into a directed graph with word dependency as edges between word nodes, and develop a novel approach, namely Graph Neural Topic Model (GNTM). Specifically, in GNTM, a well-defined probabilistic generative story is designed to model both the graph structure and word sets with multinomial distributions on the vocabulary and word dependency edge set as the topics. Meanwhile, a Neural Variational Inference (NVI) approach is proposed to learn our model with graph neural networks to encode the document graphs. Besides, we theoretically demonstrate that Latent Dirichlet Allocation (LDA) can be derived from GNTM as a special case with similar objective functions. Finally, extensive experiments on four benchmark datasets have clearly demonstrated the effectiveness and interpretability of GNTM compared with state-of-the-art baselines.","['Deep Learning', 'Graph Learning', 'Interpretability', 'Generative Model']",[],"['Dazhong Shen', 'Chuan Qin', 'Chao Wang', 'Zheng Dong', 'Hengshu Zhu', 'Hui Xiong']","['Shanghai Artificial Intelligence Laboratory', 'BOSS Zhipin', 'HKUST Fok Ying Tung Research Institute, The  University of Science and Technology\xa0(Guangzhou)', 'University of British Columbia', 'BOSS Zhipin', 'University of Science and Technology']","[None, None, 'Hong Kong', None, None, 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27437,Fairness & Bias,Fairness in Ranking under Uncertainty,"Fairness has emerged as an important consideration in algorithmic decision making. Unfairness occurs when an agent with higher merit obtains a worse outcome than an agent with lower merit. Our central point is that a primary cause of unfairness is uncertainty. A principal or algorithm making decisions never has access to the agents' true merit, and instead uses proxy features that only imperfectly predict merit (e.g., GPA, star ratings, recommendation letters). None of these ever fully capture an agent's merit; yet existing approaches have mostly been defining fairness notions directly based on observed features and outcomes. Our primary point is that it is more principled to acknowledge and model the uncertainty explicitly. The role of observed features is to give rise to a posterior distribution of the agents' merits. We use this viewpoint to define a notion of approximate fairness in ranking. We call an algorithm $\phi$-fair (for $\phi \in [0,1]$) if it has the following property for all agents $x$ and all $k$: if agent $x$ is among the top $k$ agents with respect to merit with probability at least $\rho$ (according to the posterior merit distribution), then the algorithm places the agent among the top $k$ agents in its ranking with probability at least $\phi \rho$. We show how to compute rankings that optimally trade off approximate fairness against utility to the principal. In addition to the theoretical characterization, we present an empirical analysis of the potential impact of the approach in simulation studies. For real-world validation, we applied the approach in the context of a paper recommendation system that we built and fielded at the KDD 2020 conference.",['Fairness'],[],"['Ashudeep Singh', 'David Kempe', 'Thorsten Joachims']","['Pinterest, Inc.', 'University of Southern California', 'Amazon']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27339,Fairness & Bias,Fair Exploration via Axiomatic Bargaining,"Motivated by the consideration of fairly sharing the cost of exploration between multiple groups in learning problems, we develop the Nash bargaining solution in the context of multi-armed bandits. Specifically, the 'grouped' bandit associated with any multi-armed bandit problem associates, with each time step, a single group from some finite set of groups. The utility gained by a given group under some learning policy is naturally viewed as the reduction in that group's regret relative to the regret that group would have incurred 'on its own'. We derive policies that yield the Nash bargaining solution relative to the set of incremental utilities possible under any policy. We show that on the one hand, the 'price of fairness' under such policies is limited, while on the other hand, regret optimal policies are arbitrarily unfair under generic conditions. Our theoretical development is complemented by a case study on contextual bandits for warfarin dosing where we are concerned with the cost of exploration across multiple races and age groups.","['Reinforcement Learning and Planning', 'Fairness', 'Bandits']",[],"['Jackie Baek', 'Vivek Farias']","['New York University', 'Massachusetts Institute of Technology']","[None, None]"
https://nips.cc/virtual/2021/poster/27065,Fairness & Bias,Characterizing the risk of fairwashing,"Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of fairwashing attacks by analyzing their fidelity-unfairness trade-offs. In particular, we show that fairwashed explanation models can generalize beyond the suing group (i.e., data points that are being explained), meaning that a fairwashed explainer can be used to rationalize subsequent unfair decisions of a black-box model. We also demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions. This generalization and transferability of fairwashing attacks imply that their detection will be difficult in practice. Finally, we propose an approach to quantify the risk of fairwashing, which is based on the computation of the range of the unfairness of high-fidelity explainers.",['Fairness'],[],"['Ulrich Aïvodji', 'Hiromi Arai', 'Sébastien Gambs', 'Satoshi Hara']","['École de technologie supérieure, Université du Québec', 'RIKEN', 'Université du Québec à Montréal', 'Osaka University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26918,Fairness & Bias,Fairness via Representation Neutralization,"Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing spurious correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.","['Fairness', 'Interpretability', 'Machine Learning']",[],"['Mengnan Du', 'Subhabrata Mukherjee', 'Guanchu Wang', 'Ruixiang Tang', 'Ahmed Hassan Awadallah', 'Xia Hu']","['New  Institute of Technology', 'Hippocratic AI', 'Rice University', 'Rice University', 'Microsoft Research', 'Rice University']","['Jersey', None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28526,Fairness & Bias,Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem,"In this paper, we study the problem of fair sparse regression on a biased dataset where bias depends upon a hidden binary attribute. The presence of a hidden attribute adds an extra layer of complexity to the problem by combining sparse regression and clustering with unknown binary labels. The corresponding optimization problem is combinatorial, but we propose a novel relaxation of it as an invex optimization problem. To the best of our knowledge, this is the first invex relaxation for a combinatorial problem. We show that the inclusion of the debiasing/fairness constraint in our model has no adverse effect on the performance. Rather, it enables the recovery of the hidden attribute. The support of our recovered regression parameter vector matches exactly with the true parameter vector. Moreover, we simultaneously solve the clustering problem by recovering the exact value of the hidden attribute for each sample. Our method uses carefully constructed primal dual witnesses to provide theoretical guarantees for the combinatorial problem. To that end, we show that the sample complexity of our method is logarithmic in terms of the dimension of the regression parameter vector.","['Theory', 'Fairness', 'Optimization', 'Clustering']",[],['Jean Honorio'],['University of Melbourne'],[None]
https://nips.cc/virtual/2021/poster/28357,Fairness & Bias,Are My Deep Learning Systems Fair? An Empirical Study of Fixed-Seed Training,"Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, configuration, software, and hardware) with a fixed seed produce different models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the first empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and five baselines reveals up to 12.6% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artificial intelligence (AI) related conferences, only 34.4% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their results’ validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.","['Deep Learning', 'Fairness']",[],"['Shangshu Qian', 'Hung Viet Pham', 'Thibaud Lutellier', 'Zeou Hu', 'Jungwon Kim', 'Yaoliang Yu', 'Jiahao Chen', 'Sameena Shah']","['Purdue University', 'University of Waterloo', 'University of Waterloo', 'University of Waterloo', 'Purdue University', 'University of Waterloo', 'Responsible AI LLC', 'J.P. Morgan Chase']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28153,Fairness & Bias,Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning,"Federated learning (FL) has gain growing interests for its capability of learning from distributed data sources collectively without the need of accessing the raw data samples across different sources. So far FL research has mostly focused on improving the performance, how the algorithmic disparity will be impacted for the model learned from FL and the impact of algorithmic disparity on the utility inconsistency are largely unexplored. In this paper, we propose an FL framework to jointly consider performance consistency and algorithmic fairness across different local clients (data sources). We derive our framework from a constrained multi-objective optimization perspective, in which we learn a model satisfying fairness constraints on all clients with consistent performance. Specifically, we treat the algorithm prediction loss at each local client as an objective and maximize the worst-performing client with fairness constraints through optimizing a surrogate maximum function with all objectives involved. A gradient-based procedure is employed to achieve the Pareto optimality of this optimization problem. Theoretical analysis is provided to prove that our method can converge to a Pareto solution that achieves the min-max performance with fairness constraints on all clients. Comprehensive experiments on synthetic and real-world datasets demonstrate the superiority that our approach over baselines and its effectiveness in achieving both fairness and consistency across all local clients.","['Federated Learning', 'Fairness', 'Optimization']",[],"['Sen Cui', 'Weishen Pan', 'Jian Liang', 'Changshui Zhang', 'Fei Wang']","['Tsinghua University, Tsinghua University', 'Weill Cornell Medicine, Cornell University', 'Alibaba Group', 'Tsinghua University', 'Cornell University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28125,Fairness & Bias,Assessing Fairness in the Presence of Missing Data,"Missing data are prevalent and present daunting challenges in real data analysis. While there is a growing body of literature on fairness in analysis of fully observed data, there has been little theoretical work on investigating fairness in analysis of incomplete data. In practice, a popular analytical approach for dealing with missing data is to use only the set of complete cases, i.e., observations with all features fully observed to train a prediction algorithm. However, depending on the missing data mechanism, the distribution of complete cases and the distribution of the complete data may be substantially different. When the goal is to develop a fair algorithm in the complete data domain where there are no missing values, an algorithm that is fair in the complete case domain may show disproportionate bias towards some marginalized groups in the complete data domain. To fill this significant gap, we study the problem of estimating fairness in the complete data domain for an arbitrary model evaluated merely using complete cases. We provide upper and lower bounds on the fairness estimation error and conduct numerical experiments to assess our theoretical results. Our work provides the first known theoretical results on fairness guarantee in analysis of incomplete data.","['Fairness', 'Domain Adaptation']",[],"['Yiliang Zhang', 'Qi Long']","['University of Pennsylvania', 'University of Pennsylvania']","[None, None]"
https://nips.cc/virtual/2021/poster/27908,Fairness & Bias,Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial,"In this paper, we answer the question of when inserting label noise (less informative labels) can instead return us more accurate and fair models. We are primarily inspired by three observations: 1) In contrast to reducing label noise rates, increasing the noise rates is easy to implement; 2) Increasing a certain class of instances' label noise to balance the noise rates (increasing-to-balancing) results in an easier learning problem; 3) Increasing-to-balancing improves fairness guarantees against label bias. In this paper, we first quantify the trade-offs introduced by increasing a certain group of instances' label noise rate w.r.t. the loss of label informativeness and the lowered learning difficulties. We analytically demonstrate when such an increase is beneficial, in terms of either improved generalization power or the fairness guarantees. Then we present a method to insert label noise properly for the task of learning with noisy labels, either without or with a fairness constraint. The primary technical challenge we face is due to the fact that we would not know which data instances are suffering from higher noise, and we would not have the ground truth labels to verify any possible hypothesis. We propose a detection method that informs us which group of labels might suffer from higher noise without using ground truth labels. We formally establish the effectiveness of the proposed solution and demonstrate it with extensive experiments.","['Fairness', 'Machine Learning']",[],"['Yang Liu', 'Jialu Wang']","['University of California, Santa Cruz', 'University of California, Santa Cruz']","[None, None]"
https://nips.cc/virtual/2021/poster/27604,Fairness & Bias,An Information-theoretic Approach to Distribution Shifts,"Safely deploying machine learning models to the real world is often a challenging process. For example, models trained with data obtained from a specific geographic location tend to fail when queried with data obtained elsewhere, agents trained in a simulation can struggle to adapt when deployed in the real world or novel environments, and neural networks that are fit to a subset of the population might carry some selection bias into their decision process. In this work, we describe the problem of data shift from an information-theoretic perspective by (i) identifying and describing the different sources of error, (ii) comparing some of the most promising objectives explored in the recent domain generalization and fair classification literature. From our theoretical analysis and empirical evaluation, we conclude that the model selection procedure needs to be guided by careful considerations regarding the observed data, the factors used for correction, and the structure of the data-generating process.","['Graph Learning', 'Domain Adaptation', 'Machine Learning', 'Theory', 'Deep Learning', 'Representation Learning']",[],"['Marco Federici', 'Ryota Tomioka']","['University of Amsterdam', 'Microsoft Research Cambridge']","[None, None]"
https://nips.cc/virtual/2021/poster/27530,Fairness & Bias,An Axiomatic Theory of Provably-Fair Welfare-Centric Machine Learning,"We address an inherent difficulty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justified alternative setting, and studying the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objectives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimization tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We define a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justification via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply defining utility as negative loss and maximizing welfare. Building upon these concepts, we define fair-PAC learning, where a fair-PAC learner is an algorithm that learns an ε-δ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justified) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on firm theoretical ground, as it yields statistical — and in some cases computational — efficiency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees.","['Theory', 'Machine Learning']",[],['Cyrus Cousins'],['Brown University'],[None]
https://nips.cc/virtual/2021/poster/26854,Fairness & Bias,Retiring Adult: New Datasets for Fair Machine Learning,"Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.","['Fairness', 'Graph Learning', 'Machine Learning']",[],"['Frances Ding', 'Moritz Hardt', 'John Miller', 'Ludwig Schmidt']","['University of California Berkeley', 'Max-Planck-Institute for Intelligent Systems, Max-Planck Institute', 'University of California Berkeley', 'University of Washington']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26620,Fairness & Bias,Better Algorithms for Individually Fair $k$-Clustering,"We study data clustering problems with $\ell_p$-norm objectives (e.g. \textsc{$k$-Median} and \textsc{$k$-Means}) in the context of individual fairness. The dataset consists of $n$ points, and we want to find $k$ centers such that (a) the objective is minimized, while (b) respecting the individual fairness constraint that every point $v$ has a center within a distance at most $r(v)$, where $r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz [FORC 2020] introduced this concept and designed a clustering algorithm with provable (approximate) fairness and objective guarantees for the $\ell_\infty$ or \textsc{$k$-Center} objective.  Mahabadi and Vakilian [ICML 2020] revisited this problem to give a local-search algorithm for all $\ell_p$-norms. Empirically, their algorithms outperform Jung et. al.'s by a large margin in terms of cost (for \textsc{$k$-Median} and \textsc{$k$-Means}), but they incur a reasonable loss in fairness. In this paper, our main contribution is to use Linear Programming (LP) techniques to obtain better algorithms for this problem, both in theory and in practice. We prove that by modifying known LP rounding techniques, one gets a worst-case guarantee on the objective which is much better than in MV20, and empirically, this objective is extremely close to the optimal.  Furthermore, our theoretical fairness guarantees are comparable with MV20 in theory, and empirically, we obtain noticeably fairer solutions. Although solving the LP {\em exactly} might be prohibitive, we demonstrate that in practice, a simple sparsification technique drastically improves the run-time of our algorithm.","['Theory', 'Fairness', 'Self-Supervised Learning', 'Clustering']",[],"['Maryam Negahbani', 'Deeparnab Chakrabarty']","['Dartmouth College', 'Dartmouth College']","[None, None]"
https://nips.cc/virtual/2021/poster/26547,Fairness & Bias,Learning to See by Looking at Noise,"Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from procedural noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. In particular, we study statistical image models, randomly initialized deep generative models, and procedural graphics models. Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations.","['Graph Learning', 'Generative Model', 'Representation Learning', 'Privacy']",[],"['Manel Baradad', 'Jonas Wulff', 'Tongzhou Wang', 'Phillip Isola', 'Antonio Torralba']","['Massachusetts Institute of Technology', 'Xyla Inc', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26490,Fairness & Bias,Can Information Flows Suggest Targets for Interventions in Neural Circuits?,"Motivated by neuroscientific and clinical applications, we empirically examine whether observational measures of information flow can suggest interventions. We do so by performing experiments on artificial neural networks in the context of fairness in machine learning, where the goal is to induce fairness in the system through interventions. Using our recently developed M-information flow framework, we measure the flow of information about the true label (responsible for accuracy, and hence desirable), and separately, the flow of information about a protected attribute (responsible for bias, and hence undesirable) on the edges of a trained neural network. We then compare the flow magnitudes against the effect of intervening on those edges by pruning. We show that pruning edges that carry larger information flows about the protected attribute reduces bias at the output to a greater extent. This demonstrates that M-information flow can meaningfully suggest targets for interventions, answering the title's question in the affirmative. We also evaluate bias-accuracy tradeoffs for different intervention strategies, to analyze how one might use estimates of desirable and undesirable information flows (here, accuracy and bias flows) to inform interventions that preserve the former while reducing the latter.","['Machine Learning', 'Fairness', 'Theory', 'Deep Learning', 'Interpretability', 'Neuroscience']",[],"['Praveen Venkatesh', 'Sanghamitra Dutta', 'Neil Mehta', 'Pulkit Grover']","['Allen Institute', 'University of Maryland, College Park', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26483,Fairness & Bias,Fair Clustering Under a Bounded Cost,"Clustering is a fundamental unsupervised learning problem where a dataset is partitioned into clusters that consist of nearby points in a metric space. A recent variant, fair clustering, associates a color with each point representing its group membership and requires that each color has (approximately) equal representation in each cluster to satisfy group fairness. In this model, the cost of the clustering objective increases due to enforcing fairness in the algorithm. The relative increase in the cost, the ```````''price of fairness,'' can indeed be unbounded. Therefore, in this paper we propose to treat an upper bound on the clustering objective as a constraint on the clustering problem, and to maximize equality of representation subject to it. We consider two fairness objectives: the group utilitarian objective and the group egalitarian objective, as well as the group leximin objective which generalizes the group egalitarian objective. We derive fundamental lower bounds on the approximation of the utilitarian and egalitarian objectives and introduce algorithms with provable guarantees for them. For the leximin objective we introduce an effective heuristic algorithm. We further derive impossibility results for other natural fairness objectives. We conclude with experimental results on real-world datasets that demonstrate the validity of our algorithms.","['Clustering', 'Fairness', 'Self-Supervised Learning']",[],"['Seyed A. Esmaeili', 'Brian Brubach', 'Aravind Srinivasan', 'John P Dickerson']","['Simons Laufer Mathematical Sciences Institute', 'Wellesley College', 'Amazon', 'University of Maryland, College Park']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26346,Fairness & Bias,Two-sided fairness in rankings via Lorenz dominance,"We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility.",['Fairness'],[],"['Virginie Do', 'Sam Corbett-Davies', 'Jamal Atif', 'Nicolas Usunier']","['Université Paris Dauphine - PSL', 'Facebook', 'Université Paris-Dauphine', 'Facebook']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26342,Fairness & Bias,Refining Language Models with Compositional Explanations,"Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.","['Fairness', 'Machine Learning', 'Language']",[],"['Huihan Yao', 'Ying Chen', 'Qinyuan Ye', 'Xisen Jin', 'Xiang Ren']","['Peking University', 'Tsinghua University', 'University of Southern California', 'University of Southern California', 'University of Southern California']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26311,Fairness & Bias,Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models,"The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models \textit{should} learn - whether they should reflect or correct for existing inequalities.",['Language'],[],"['Hannah Rose Kirk', 'Yennie Jun', 'Filippo Volpin', 'Haider Iqbal', 'Elias Benussi', 'Aleksandar Shtedritski', 'Yuki Asano']","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Amsterdam']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26987,Fairness & Bias,Post-processing for Individual Fairness,"Post-processing in algorithmic fairness is a versatile approach for correcting bias in ML systems that are already used in production. The main appeal of post-processing is that it avoids expensive retraining. In this work, we propose general post-processing algorithms for individual fairness (IF). We consider a setting where the learner only has access to the predictions of the original model and a similarity graph between individuals, guiding the desired fairness constraints. We cast the IF post-processing problem as a graph smoothing problem corresponding to graph Laplacian regularization that preserves the desired ""treat similar individuals similarly"" interpretation. Our theoretical results demonstrate the connection of the new objective function to a local relaxation of the original individual fairness. Empirically, our post-processing algorithms correct individual biases in large-scale NLP models such as BERT, while preserving accuracy.","['Fairness', 'Graph Learning']",[],"['Felix Petersen', 'Debarghya Mukherjee', 'Yuekai Sun', 'Mikhail Yurochkin']","['Stanford University', 'Boston University, Boston University', 'University of Michigan', 'International Business Machines']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26438,Fairness & Bias,A Causal Lens for Controllable Text Generation,"Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of  rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.","['Causality', 'Language']",[],"['Zhiting Hu', 'Li Erran Li']","['University of California, San Diego', 'Amazon']","[None, None]"
https://nips.cc/virtual/2021/poster/26129,Fairness & Bias,Sample Selection for Fair and Robust Training,"Fairness and robustness are critical elements of Trustworthy AI that need to be addressed together. Fairness is about learning an unbiased model while robustness is about learning from corrupted data, and it is known that addressing only one of them may have an adverse affect on the other. In this work, we propose a sample selection-based algorithm for fair and robust training. To this end, we formulate a combinatorial optimization problem for the unbiased selection of samples in the presence of data corruption. Observing that solving this optimization problem is strongly NP-hard, we propose a greedy algorithm that is efficient and effective in practice. Experiments show that our method obtains fairness and robustness that are better than or comparable to the state-of-the-art technique, both on synthetic and benchmark real datasets. Moreover, unlike other fair and robust training baselines, our algorithm can be used by only modifying the sampling step in batch selection without changing the training algorithm or leveraging additional clean data.","['Robustness', 'Optimization', 'Fairness']",[],"['Yuji Roh', 'Kangwook Lee', 'Steven Euijong Whang', 'Changho Suh']","['Korea Advanced Institute of Science and Technology', 'University of Wisconsin, Madison', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27552,Fairness & Bias,DECAF:  Generating Fair Synthetic Data Using Causally-Aware Generative Networks,"Machine learning models have been criticized for reflecting unfair biases in the training data.  Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data - while remaining truthful to the underlying data-generating process (DGP) - is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data.  With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents.  This procedure enables inference time debiasing, where biased edges can be strategically removed for satisfying user-defined fairness requirements. The DECAF framework is versatile and compatible with several popular definitions of fairness. In our experiments, we show that DECAF successfully removes undesired bias and - in contrast to existing methods - is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator's convergence and the fairness of downstream models.","['Fairness', 'Generative Model', 'Machine Learning', 'Causality']",[],"['Boris van Breugel', 'Trent Kyono', 'Jeroen Berrevoets', 'Mihaela van der Schaar']","['University of Cambridge', 'University of California, Los Angeles', 'University of Cambridge', 'University of Cambridge']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28457,Fairness & Bias,Fair Sequential Selection Using Supervised Learning Models,"We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classification problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfies the common fairness notions, the selection outcomes may still be biased against certain demographic groups. This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions.  We introduce a new fairness notion, ``Equal Selection (ES),'' suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the \textit{perfect} ES fairness can still be attained under certain conditions.","['Fairness', 'Graph Learning', 'Machine Learning', 'Privacy']",[],"['Mohammad Mahdi Khalili', 'Xueru Zhang', 'Mahed Abroshan']","['Yahoo! Research', 'Ohio State University', 'Optum AI']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26952,Fairness & Bias,Does enforcing fairness mitigate biases caused by subpopulation shift?,"Many instances of algorithmic bias are caused by subpopulation shifts. For example, ML models often perform worse on demographic groups that are underrepresented in the training data. In this paper, we study whether enforcing algorithmic fairness during training improves the performance of the trained model in the \emph{target domain}. On one hand, we conceive scenarios in which enforcing fairness does not improve performance in the target domain. In fact, it may even harm performance. On the other hand, we derive necessary and sufficient conditions under which enforcing algorithmic fairness leads to the Bayes model in the target domain. We also illustrate the practical implications of our theoretical results in simulations and on real data.","['Fairness', 'Graph Learning', 'Domain Adaptation']",[],"['Subha Maity', 'Debarghya Mukherjee', 'Mikhail Yurochkin', 'Yuekai Sun']","['University of Michigan, Ann Arbor', 'Boston University, Boston University', 'International Business Machines', 'University of Michigan']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26962,Fairness & Bias,Differentially Private Empirical Risk Minimization under the Fairness Lens,"Differential Privacy (DP) is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals. This paper builds on these important observations and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation and differentially private stochastic gradient descent. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings.","['Fairness', 'Optimization', 'Machine Learning', 'Privacy']",[],"['Cuong Tran', 'My H Dinh', 'Ferdinando Fioretto']","['University of Virginia, Charlottesville', 'University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27877,Fairness & Bias,Counterfactual Explanations Can Be Manipulated,"Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions.  As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust.  Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation.  We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.",['Robustness'],[],"['Dylan Z Slack', 'Sophie Hilgard', 'Himabindu Lakkaraju', 'Sameer Singh']","['University of California, Irvine', 'Harvard University', 'Harvard University', 'University of California, Irvine']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26965,Fairness & Bias,Learning Optimal Predictive Checklists,"Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Haoran Zhang', 'Quaid Morris', 'Berk Ustun', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Memorial Sloan Kettering Cancer Centre', 'University of California, San Diego', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26666,Fairness & Bias,Certifying Robustness to Programmable Data Bias in Decision Trees,"Datasets can be biased due to societal inequities, human biases, under-representation of minorities, etc. Our goal is to certify that models produced by a learning algorithm are pointwise-robust to dataset biases. This is a challenging problem: it entails learning models for a large, or even infinite, number of datasets, ensuring that they all produce the same prediction. We focus on decision-tree learning due to the interpretable nature of the models. Our approach allows programmatically specifying \emph{bias models} across a variety of dimensions (e.g., label-flipping or missing data), composing types of bias, and targeting bias towards a specific group. To certify robustness, we use a novel symbolic technique to evaluate a decision-tree learner on a large, or infinite, number of datasets, certifying that each and every dataset produces the same prediction for a specific test point. We evaluate our approach on datasets that are commonly used in the fairness literature, and demonstrate our approach's viability on a range of bias models.","['Robustness', 'Fairness']",[],"['Anna P. Meyer', 'Aws Albarghouthi', ""Loris D'Antoni""]","['University of Wisconsin - Madison', 'Amazon', 'University of Wisconsin, Madison']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27252,Privacy & Data Governance,FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective,"Federated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. Recent works have demonstrated that FL is vulnerable to model poisoning attacks. Several server-based defense approaches (e.g. robust aggregation), have been proposed to mitigate such attacks. However, we empirically show that under extremely strong attacks, these defensive methods fail to guarantee the robustness of FL. More importantly, we observe that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. In this work, we propose a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model. The key idea of FL-WBC is to identify the parameter space where long-lasting attack effect on parameters resides and perturb that space during local training. Furthermore, we derive a certified robustness guarantee against model poisoning attacks and a convergence guarantee to FedAvg after applying our FL-WBC. We conduct experiments on FasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model poisoning attacks. The results demonstrate that our method can effectively mitigate model poisoning attack impact on the global model within 5 communication rounds with nearly no accuracy drop under both IID and Non-IID settings. Our defense is also complementary to existing server-based robust aggregation approaches and can further improve the robustness of FL under extremely strong attacks.","['Federated Learning', 'Robustness']",[],"['Jingwei Sun', 'Ang Li', 'Louis DiValentin', 'Yiran Chen', 'Hai Li']","['Duke University', 'University of Maryland, College Park', 'Accenture', 'Duke University', 'Duke University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27151,Privacy & Data Governance,TRS: Transferability Reduced Ensemble via Promoting Gradient Diversity and Model Smoothness,"Adversarial Transferability is an intriguing property - adversarial perturbation crafted against one model is also effective against another model, while these models are from different model families or training processes. To better protect ML systems against adversarial attacks, several questions are raised: what are the sufficient conditions for adversarial transferability, and how to bound it? Is there a way to reduce the adversarial transferability in order to improve the robustness of an ensemble ML model? To answer these questions, in this work we first theoretically analyze and outline sufficient conditions for adversarial transferability between models; then propose a practical algorithm to reduce the transferability between base models within an ensemble to improve its robustness. Our theoretical analysis shows that only promoting the orthogonality between gradients of base models is not enough to ensure low transferability; in the meantime, the model smoothness is an important factor to control the transferability. We also provide the lower and upper bounds of adversarial transferability under certain conditions. Inspired by our theoretical analysis, we propose an effective Transferability Reduced Smooth (TRS) ensemble training strategy to train a robust ensemble with low transferability by enforcing both gradient orthogonality and model smoothness between base models. We conduct extensive experiments on TRS and compare with 6 state-of-the-art ensemble baselines against 8 whitebox attacks on different datasets, demonstrating that the proposed TRS outperforms all baselines significantly.","['Adversarial Robustness and Security', 'Robustness']",[],"['Zhuolin Yang', 'Linyi Li', 'Xiaojun Xu', 'Shiliang Zuo', 'Pan Zhou', 'Benjamin I. P. Rubinstein', 'Ce Zhang', 'Bo Li']","['University of Illinois, Urbana Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois, Urbana Champaign', 'Department of Computer Science, University of Illinois at Urbana-Champaign', 'Huazhong University of Science and Technology', 'The University of Melbourne', 'University of Chicago', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27235,Privacy & Data Governance,Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations,"When compared to the image classification models, black-box adversarial attacks against video classification models have been largely understudied. This could be possible because, with video, the temporal dimension poses significant additional challenges in gradient estimation. Query-efficient black-box attacks rely on effectively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Specifically, we design a novel iterative algorithm GEOmetric TRAnsformed Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients into searching for a small group of parameters that define these operations. This group of parameters describes the geometric progression of gradients, resulting in a reduced and structured search space. Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better attack success rates with ~73.55% fewer queries compared to the state-of-the-art method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classification models and achieves new state-of-the-art results under black-box settings on two large datasets.","['Adversarial Robustness and Security', 'Vision', 'Machine Learning']",[],"['Shasha Li', 'Abhishek Aich', 'Shitong Zhu', 'Salman Asif', 'Chengyu Song', 'Amit Roy-Chowdhury', 'Srikanth Krishnamurthy']","['Amazon', 'NEC Laboratories, America', 'Facebook', 'University of California, Riverside', 'University of California, Riverside', 'University of California, Riverside', ', University of California, Riverside']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28025,Privacy & Data Governance,Shift Invariance Can Reduce Adversarial Robustness,"Shift invariance is a critical property of CNNs that improves performance on classification.  However, we show that invariance to circular shifts can also lead to greater sensitivity to adversarial attacks.  We first characterize the margin between classes when a shift-invariant {\em linear} classifier is used. We show that the margin can only depend on the DC component of the signals.  Then, using results about infinitely wide networks, we show that in some simple cases, fully connected and shift-invariant neural networks produce linear decision boundaries.  Using this, we prove that shift invariance in neural networks produces adversarial examples for the simple case of two classes, each consisting of a single image with a black or white dot on a gray background.  This is more than a curiosity; we show empirically that with real datasets and realistic architectures, shift invariance reduces adversarial robustness.  Finally, we describe initial experiments using synthetic data to probe the source of this connection.","['Adversarial Robustness and Security', 'Deep Learning', 'Machine Learning', 'Robustness']",[],"['Vasu Singla', 'Songwei Ge', 'Ronen Basri', 'David Jacobs']","['University of Maryland, College Park', 'University of Maryland, College Park', 'Meta Platforms Inc.', 'University of Maryland']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28058,Privacy & Data Governance,Disrupting Deep Uncertainty Estimation Without Harming Accuracy,"Deep neural networks (DNNs) have proven to be powerful predictors and are widely used for various tasks. Credible uncertainty estimation of their predictions, however, is crucial for their deployment in many risk-sensitive applications. In this paper we present a novel and simple attack, which unlike adversarial attacks, does not cause incorrect predictions but instead cripples the network's capacity for uncertainty estimation. The result is that after the attack, the DNN is more confident of its incorrect predictions than about its correct ones without having its accuracy reduced. We present two versions of the attack. The first scenario focuses on a black-box regime (where the attacker has no knowledge of the target network) and the second scenario attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimations. We demonstrate successful attacks on three of the most popular uncertainty estimation methods: the vanilla softmax score, Deep Ensembles and MC-Dropout. Additionally, we show an attack on SelectiveNet, the selective classification architecture. We test the proposed attack on several contemporary architectures such as MobileNetV2 and EfficientNetB0, all trained to classify ImageNet.","['Adversarial Robustness and Security', 'Deep Learning', 'Machine Learning']",[],"['Ido Galil', 'Ran El-Yaniv']","['Computer Science Departmen, Technion- Institute of Technology', 'Technion']","['Israel', None]"
https://nips.cc/virtual/2021/poster/26485,Privacy & Data Governance,A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration,"Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation,  and the lack of accurate ground-truth ""clean"" hyperspectral signals acquired on the spot makes restoration tasks challenging.  In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and  significantly outperforms the state of the art.","['Deep Learning', 'Interpretability']",[],"['Theo Bodrito', 'Alexandre Zouaoui', 'Jocelyn Chanussot', 'Julien Mairal']","['INRIA', 'INRIA', 'INRIA', 'Inria']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27331,Privacy & Data Governance,Excess Capacity and Backdoor Poisoning,"A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data; however, the model reliably errs on watermarked examples. To gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classification problems. We then use this to analyze important statistical and computational issues surrounding these attacks. On the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks. From a computational standpoint, we show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor filtering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors.","['Theory', 'Robustness', 'Adversarial Robustness and Security', 'Machine Learning']",[],"['Naren Sarayu Manoj', 'Avrim Blum']","['EPFL - EPF Lausanne', 'Toyota Technological Institute at Chicago']","[None, None]"
https://nips.cc/virtual/2021/poster/26136,Privacy & Data Governance,Clustering Effect of Adversarial Robust Models,"Adversarial robustness has received increasing attention along with the study of adversarial examples. So far, existing works show that robust models not only obtain robustness against various adversarial attacks but also boost the performance in some downstream tasks. However, the underlying mechanism of adversarial robustness is still not clear. In this paper, we interpret adversarial robustness from the perspective of linear components, and find that there exist some statistical properties for comprehensively robust models. Specifically, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, maximum pooling, or activation layers). Based on these observations, we propose a novel understanding of adversarial robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of our proposed clustering strategy. Our code is available at https://github.com/bymavis/Adv_Weight_NeurIPS2021.","['Clustering', 'Robustness', 'Adversarial Robustness and Security']",[],"['Yang Bai', 'Xin Yan', 'Yong Jiang', 'Shu-Tao Xia', 'Yisen Wang']","['Tencent Zhuque Lab', 'Tsinghua University', 'Tsinghua University', 'Shenzhen International Graduate School, Tsinghua University', 'Peking University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27499,Privacy & Data Governance,Accumulative Poisoning Attacks on Real-time Data,"Collecting training data from untrusted sources exposes machine learning services to poisoning adversaries, who maliciously manipulate training data to degrade the model accuracy. When trained on offline datasets, poisoning adversaries have to inject the poisoned data in advance before training, and the order of feeding these poisoned batches into the model is stochastic. In contrast, practical systems are more usually trained/fine-tuned on sequentially captured real-time data, in which case poisoning adversaries could dynamically poison each data batch according to the current model state. In this paper, we focus on the real-time settings and propose a new attacking strategy, which affiliates an accumulative phase with poisoning attacks to secretly (i.e., without affecting accuracy) magnify the destructive effect of a (poisoned) trigger batch. By mimicking online learning and federated learning on MNIST and CIFAR-10, we show that model accuracy significantly drops by a single update step on the trigger batch after the accumulative phase. Our work validates that a well-designed but straightforward attacking strategy can dramatically amplify the poisoning effects, with no need to explore complex techniques.","['Federated Learning', 'Online Learning', 'Machine Learning']",[],"['Tianyu Pang', 'Xiao Yang', 'Yinpeng Dong', 'Hang Su', 'Jun Zhu']","['Sea AI Lab', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'Tsinghua University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27458,Privacy & Data Governance,Visualizing the Emergence of Intermediate Visual Patterns in DNNs,"This paper proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e. the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Furthermore, this method also provides new insights into signal-processing behaviors of existing deep-learning techniques, such as adversarial attacks and knowledge distillation.","['Adversarial Robustness and Security', 'Deep Learning']",[],"['Mingjie Li', 'Shaobo Wang', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Shanghai Jiao Tong University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27065,Privacy & Data Governance,Characterizing the risk of fairwashing,"Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of fairwashing attacks by analyzing their fidelity-unfairness trade-offs. In particular, we show that fairwashed explanation models can generalize beyond the suing group (i.e., data points that are being explained), meaning that a fairwashed explainer can be used to rationalize subsequent unfair decisions of a black-box model. We also demonstrate that fairwashing attacks can transfer across black-box models, meaning that other black-box models can perform fairwashing without explicitly using their predictions. This generalization and transferability of fairwashing attacks imply that their detection will be difficult in practice. Finally, we propose an approach to quantify the risk of fairwashing, which is based on the computation of the range of the unfairness of high-fidelity explainers.",['Fairness'],[],"['Ulrich Aïvodji', 'Hiromi Arai', 'Sébastien Gambs', 'Satoshi Hara']","['École de technologie supérieure, Université du Québec', 'RIKEN', 'Université du Québec à Montréal', 'Osaka University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28776,Privacy & Data Governance,Provably Efficient Black-Box Action Poisoning Attacks Against Reinforcement Learning,"Due to the broad range of applications of reinforcement learning (RL), understanding the effects of adversarial attacks against RL model is essential for the safe applications of this model. Prior theoretical works on adversarial attacks against RL mainly focus on either reward poisoning attacks or environment poisoning attacks. In this paper, we introduce a new class of attacks named action poisoning attacks, where an adversary can change the action signal selected by the agent. Compared with existing attack models, the attacker’s ability in the proposed action poisoning attack model is more restricted, which brings some design challenges. We study the action poisoning attack in both white-box and black-box settings. We introduce an adaptive attack scheme called LCB-H, which works for most RL agents in the black-box setting. We prove that LCB-H attack can force any efficient RL agent, whose dynamic regret scales sublinearly with the total number of steps taken, to choose actions according to a policy selected by the attacker very frequently, with only sublinear cost. In addition, we apply LCB-H attack against a very popular model-free RL algorithm: UCB-H. We show that, even in black-box setting, by spending only logarithm cost, the proposed LCB-H attack scheme can force the UCB-H agent to choose actions according to the policy selected by the attacker very frequently.","['Reinforcement Learning and Planning', 'Adversarial Robustness and Security']",[],"['Guanlin Liu', 'Lifeng Lai']","['University of California, Davis', 'University of California, Davis']","[None, None]"
https://nips.cc/virtual/2021/poster/27822,Privacy & Data Governance,Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks,"Deep neural networks (DNNs) are well-known to be vulnerable to adversarial attacks, where malicious human-imperceptible perturbations are included in the input to the deep network to fool it into making a wrong classification. Recent studies have demonstrated that neural Ordinary Differential Equations (ODEs) are intrinsically more robust against adversarial attacks compared to vanilla DNNs. In this work, we propose a neural ODE with Lyapunov-stable equilibrium points for defending against adversarial attacks (SODEF). By ensuring that the equilibrium points of the ODE solution used as part of SODEF are Lyapunov-stable, the ODE solution for an input with a small perturbation converges to the same solution as the unperturbed input. We provide theoretical results that give insights into the stability of SODEF as well as the choice of regularizers to ensure its stability. Our analysis suggests that our proposed regularizers force the extracted feature points to be within a neighborhood of the Lyapunov-stable equilibrium points of the SODEF ODE. SODEF is compatible with many defense methods and can be applied to any neural network's final regressor layer to enhance its stability against adversarial attacks.","['Adversarial Robustness and Security', 'Deep Learning', 'Machine Learning']",[],"['QIYU KANG', 'Yang Song', 'Qinxu Ding', 'Wee Peng Tay']","['Nanyang Technological University', 'C3 AI', 'University of Social Sciences', 'Nanyang Technological University']","[None, None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/28871,Privacy & Data Governance,Learning Robust Hierarchical Patterns of Human Brain across Many fMRI Studies,"Multi-site fMRI studies face the challenge that the pooling introduces systematic non-biological site-specific variance due to hardware, software, and environment. In this paper, we propose to reduce site-specific variance in the estimation of hierarchical Sparsity Connectivity Patterns (hSCPs) in fMRI data via a simple yet effective matrix factorization while preserving biologically relevant variations. Our method leverages unsupervised adversarial learning to improve the reproducibility of the components. Experiments on simulated datasets display that the proposed method can estimate components with higher accuracy and reproducibility, while preserving age-related variation on a multi-center clinical data set.",['Domain Adaptation'],[],"['Dushyant Sahoo', 'Christos Davatzikos']","['School of Engineering and Applied Science, University of Pennsylvania', 'University of Pennsylvania']","[None, None]"
https://nips.cc/virtual/2021/poster/28868,Privacy & Data Governance,Adversarially Robust Change Point Detection ,"Change point detection is becoming increasingly popular in many application areas. On one hand, most of the theoretically-justified methods are investigated in an ideal setting without model violations, or merely robust against identical heavy-tailed noise distribution across time and/or against isolate outliers; on the other hand, we are aware that there have been exponentially growing attacks from adversaries, who may pose systematic contamination on data to purposely create spurious change points or disguise true change points. In light of the timely need for a change point detection method that is robust against adversaries, we start with, arguably, the simplest univariate mean change point detection problem. The adversarial attacks are formulated through the Huber $\varepsilon$-contamination framework, which in particular allows the contamination distributions to be different at each time point. In this paper, we demonstrate a phase transition phenomenon in change point detection. This detection boundary is a function of the contamination proportion~$\varepsilon$ and is the first time shown in the literature. In addition, we derive the minimax-rate optimal localisation error rate, quantifying the cost of accuracy in terms of the contamination proportion. We propose a computationally feasible method, matching the minimax lower bound under certain conditions, saving for logarithmic factors. Extensive numerical experiments are conducted with comparisons to robust change point detection methods in the existing literature.",['Adversarial Robustness and Security'],[],"['Mengchu Li', 'Yi Yu']","['University of Warwick', 'University of Warwick']","[None, None]"
https://nips.cc/virtual/2021/poster/28827,Privacy & Data Governance,Neural Architecture Dilation for Adversarial Robustness,"With the tremendous advances in the architecture and scale of convolutional neural networks (CNNs) over the past few decades, they can easily reach or even exceed the performance of humans in certain tasks. However, a recently discovered shortcoming of CNNs is that they are vulnerable to adversarial attacks. Although the adversarial robustness of CNNs can be improved by adversarial training, there is a trade-off between standard accuracy and adversarial robustness. From the neural architecture perspective, this paper aims to improve the adversarial robustness of the backbone CNNs that have a satisfactory accuracy. Under a minimal computational overhead, the introduction of a dilation architecture is expected to be friendly with the standard performance of the backbone CNN while pursuing adversarial robustness. Theoretical analyses on the standard and adversarial error bounds naturally motivate the proposed neural architecture dilation algorithm. Experimental results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance the accuracy and adversarial robustness.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Yanxi Li', 'Zhaohui Yang', 'Yunhe Wang', 'Chang Xu']","['University of Sydney, University of Sydney', 'Peking University', ""Huawei Noah's Ark Lab"", 'University of Sydney']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28697,Privacy & Data Governance,Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks,"Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what configurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained DNNs. Specifically, we make the following key observations: 1) more parameters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural configuration for adversarial robustness. We also provide a theoretical analysis explaning why such network configuration can help robustness. These architectural insights can help design adversarially robust DNNs.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Hanxun Huang', 'Yisen Wang', 'Sarah Monazam Erfani', 'Quanquan Gu', 'Xingjun Ma']","['University of Melbourne', 'Peking University', 'The University of Melbourne', 'University of California, Los Angeles', 'Fudan University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28685,Privacy & Data Governance,Morié Attack (MA): A New Potential Risk of Screen Photos,"Images, captured by a camera, play a critical role in training Deep Neural Networks (DNNs). Usually, we assume the images acquired by cameras are consistent with the ones perceived by human eyes. However, due to the different physical mechanisms between human-vision and computer-vision systems, the final perceived images could be very different in some cases, for example shooting on digital monitors. In this paper, we find a special phenomenon in digital image processing, the moiré effect, that could cause unnoticed security threats to DNNs. Based on it, we propose a Moiré Attack (MA) that generates the physical-world moiré pattern adding to the images by mimicking the shooting process of digital devices. Extensive experiments demonstrate that our proposed digital Moiré Attack (MA) is a perfect camouflage for attackers to tamper with DNNs with a high success rate ($100.0\%$ for untargeted and $97.0\%$ for targeted attack with the noise budget $\epsilon=4$), high transferability rate across different models, and high robustness under various defenses. Furthermore, MA owns great stealthiness because the moiré effect is unavoidable due to the camera's inner physical structure, which therefore hardly attracts the awareness of humans. Our code is available at https://github.com/Dantong88/Moire_Attack.","['Adversarial Robustness and Security', 'Deep Learning', 'Vision', 'Robustness']",[],"['Dantong Niu', 'Ruohao Guo', 'Yisen Wang']","['University of California, Berkeley', 'Peking University', 'Peking University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28680,Privacy & Data Governance,Medical Dead-ends and Learning to Identify High-Risk States and Treatments,"Machine learning has successfully framed many sequential decision making problems as either supervised prediction, or optimal decision-making policy identification via reinforcement learning. In data-constrained offline settings, both approaches may fail as they assume fully optimal behavior or rely on exploring alternatives that may not exist. We introduce an inherently different approach that identifies ""dead-ends"" of a state space. We focus on patient condition in the intensive care unit, where a ""medical dead-end"" indicates that a patient will expire, regardless of all potential future treatment sequences. We postulate ""treatment security"" as avoiding treatments with probability proportional to their chance of leading to dead-ends, present a formal proof, and frame discovery as an RL problem. We then train three independent deep neural models for automated state construction, dead-end discovery and confirmation. Our empirical results discover that dead-ends exist in real clinical data among septic patients, and further reveal gaps between secure treatments and those administered.","['Reinforcement Learning and Planning', 'Machine Learning']",[],"['Mehdi Fatemi', 'Taylor W. Killian', 'Jayakumar Subramanian', 'Marzyeh Ghassemi']","['Microsoft', 'Massachusetts Institute of Technology', 'Adobe Systems', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28605,Privacy & Data Governance,Multi-Objective SPIBB: Seldonian Offline Policy Improvement with Safety Constraints in Finite MDPs,"We study the problem of Safe Policy Improvement (SPI) under constraints in the offline Reinforcement Learning (RL) setting. We consider the scenario where: (i) we have a dataset collected under a known baseline policy, (ii) multiple reward signals are received from the environment inducing as many objectives to optimize. We present an SPI formulation for this RL setting that takes into account the preferences of the algorithm’s user for handling the trade-offs for different reward signals while ensuring that the new policy performs at least as well as the baseline policy along each individual objective. We build on traditional SPI algorithms and propose a novel method based on Safe Policy Iteration with Baseline Bootstrapping (SPIBB, Laroche et al., 2019) that provides high probability guarantees on the performance of the agent in the true environment. We show the effectiveness of our method on a synthetic grid-world safety task as well as in a real-world critical care context to learn a policy for the administration of IV fluids and vasopressors to treat sepsis.",['Reinforcement Learning and Planning'],[],"['Harsh Satija', 'Philip S. Thomas', 'Joelle Pineau', 'Romain Laroche']","['McGill University', 'College of Information and Computer Science, University of Massachusetts, Amherst', 'Facebook', 'Microsoft']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28561,Privacy & Data Governance,Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity,"Direct policy search serves as one of the workhorses in modern reinforcement learning (RL), and its applications in continuous control tasks have recently attracted increasing attention. In this work, we investigate the convergence theory of policy gradient (PG) methods for learning the linear risk-sensitive and robust controller. In particular, we develop PG methods that can be implemented in a derivative-free fashion by sampling system trajectories, and establish both global convergence and sample complexity results in the solutions of two fundamental settings in risk-sensitive and robust control: the finite-horizon linear exponential quadratic Gaussian, and the finite-horizon linear-quadratic disturbance attenuation problems. As a by-product, our results also provide the first sample complexity for the global convergence of PG methods on solving zero-sum linear-quadratic dynamic games, a nonconvex-nonconcave minimax optimization problem that serves as a baseline setting in multi-agent reinforcement learning (MARL) with continuous spaces. One feature of our algorithms is that during the learning phase, a certain level of robustness/risk-sensitivity of the controller is preserved, which we termed as the implicit regularization property, and is an essential requirement in safety-critical control systems.","['Reinforcement Learning and Planning', 'Theory', 'Optimization']",[],"['Kaiqing Zhang', 'Xiangyuan Zhang', 'Bin Hu', 'Tamer Basar']","['University of Maryland, College Park', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28530,Privacy & Data Governance,Automated Discovery of Adaptive Attacks on Adversarial Defenses,"Reliable evaluation of adversarial defenses is a challenging task, currently limited to an expert who manually crafts attacks that exploit the defense’s inner workings, or to approaches based on ensemble of fixed attacks, none of which may be effective for the specific defense at hand. Our key observation is that adaptive attacks are composed from a set of reusable building blocks that can be formalized in a search space and used to automatically discover attacks for unknown defenses. We evaluated our approach on 24 adversarial defenses and show that it outperforms AutoAttack, the current state-of-the-art tool for reliable evaluation of adversarial defenses: our tool discovered significantly stronger attacks by producing 3.0%-50.8% additional adversarial examples for 10 models, while obtaining attacks with slightly stronger or similar strength for the remaining models.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Chengyuan Yao', 'Pavol Bielik', 'PETAR TSANKOV', 'Martin Vechev']","['Swiss Federal Institute of Technology', 'LatticeFlow', 'LatticeFlow', 'Swiss Federal Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28523,Privacy & Data Governance,Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness,"We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck as a regularizer for learning an adversarially robust deep neural network classifier. In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful information for output prediction while reducing redundant information. We show that the HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. In particular, we prove that the HSIC bottleneck regularizer reduces the sensitivity of the classifier to adversarial examples. Our experiments on multiple benchmark datasets and architectures demonstrate that incorporating an HSIC bottleneck regularizer attains competitive natural accuracy and improves adversarial robustness, both with and without adversarial examples during training. Our code and adversarially robust models are publicly available.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Zifeng Wang', 'Tong Jian', 'Aria Masoomi', 'Stratis Ioannidis', 'Jennifer Dy']","['Google', 'Analog Devices', 'Northeastern University', 'Northeastern University', 'Northeastern University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28481,Privacy & Data Governance,Are Transformers more robust than CNNs?,"Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks,  recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs.","['Transformers', 'Deep Learning', 'Adversarial Robustness and Security', 'Robustness']",[],"['Yutong Bai', 'Jieru Mei', 'Alan Yuille', 'Cihang Xie']","['Facebook', 'Johns Hopkins University', 'Johns Hopkins University', 'University of California, Santa Cruz']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28470,Privacy & Data Governance,Reliable and Trustworthy Machine Learning for Health Using Dataset Shift Detection,"Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal. In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (e.g., skin cancer, lung sound, and Parkinson's disease) using various input data types (e.g., image, audio, and motion data). We demonstrate that these models show unreasonable predictions on out-of-distribution datasets. We show that Mahalanobis distance- and Gram matrices-based out-of-distribution detection methods are able to detect out-of-distribution data with high accuracy for the health models that operate on different modalities. We then translate the out-of-distribution score into a human interpretable \textsc{confidence score} to investigate its effect on the users' interaction with health ML applications. Our user study shows that the \textsc{confidence score} helped the participants only trust the results with a high score to make a medical decision and disregard results with a low score. Through this work, we demonstrate that dataset shift is a critical piece of information for high-stake ML applications, such as medical diagnosis and healthcare, to provide reliable and trustworthy predictions to the users.","['Deep Learning', 'Machine Learning']",[],"['Chunjong Park', 'Anas Awadalla', 'Shwetak Patel']","['Research, Google', 'Department of Computer Science, University of Washington', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28463,Privacy & Data Governance,On the Algorithmic Stability of Adversarial Training,"The adversarial training is a popular tool to remedy the vulnerability of deep learning models against adversarial attacks, and there is rich theoretical literature on the training loss of adversarial training algorithms. In contrast, this paper studies the algorithmic stability of a generic adversarial training algorithm, which can further help to establish an upper bound for generalization error. By figuring out the stability upper bound and lower bound, we argue that the non-differentiability issue of adversarial training causes worse algorithmic stability than their natural counterparts. To tackle this problem, we consider a noise injection method. While the non-differentiability problem seriously affects the stability of adversarial training, injecting noise enables the training trajectory to avoid the occurrence of non-differentiability with dominating probability, hence enhancing the stability performance of adversarial training. Our analysis also studies the relation between the algorithm stability and numerical approximation error of adversarial attacks.","['Adversarial Robustness and Security', 'Deep Learning']",[],"['Yue Xing', 'Qifan Song', 'Guang Cheng']","['Michigan State University', 'Purdue University', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28450,Privacy & Data Governance,Safe Pontryagin Differentiable Programming,"We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic  framework to solve a broad class of safety-critical learning and control tasks---problems that require the guarantee of safety constraint satisfaction at any stage of the learning and control progress.  In the spirit of interior-point methods,   Safe PDP handles different types of system  constraints on states and inputs by incorporating them into the cost or loss through barrier functions. We prove three fundamentals  of the proposed  Safe PDP:  first, both the  solution and its gradient in the backward pass can be approximated by solving their  more efficient unconstrained counterparts;  second,   the approximation for both the  solution and its gradient can be controlled for arbitrary accuracy by a  barrier parameter;   and third,   importantly, all intermediate results throughout the approximation and optimization  strictly respect the  constraints,  thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of   Safe PDP in solving various safety-critical tasks,  including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.","['Reinforcement Learning and Planning', 'Optimization']",[],"['Wanxin Jin', 'Shaoshuai Mou', 'George J. Pappas']","['Arizona State University', 'Purdue University', 'School of Engineering and Applied Science, University of Pennsylvania']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28435,Privacy & Data Governance,Not All Low-Pass Filters are Robust in Graph Convolutional Networks,"Graph Convolutional Networks (GCNs) are promising deep learning approaches in learning representations for graph-structured data. Despite the proliferation of such methods, it is well known that they are vulnerable to carefully crafted adversarial attacks on the graph structure. In this paper, we first conduct an adversarial vulnerability analysis based on matrix perturbation theory. We prove that the low- frequency components of the symmetric normalized Laplacian, which is usually used as the convolutional filter in GCNs, could be more robust against structural perturbations when their eigenvalues fall into a certain robust interval. Our results indicate that not all low-frequency components are robust to adversarial attacks and provide a deeper understanding of the relationship between graph spectrum and robustness of GCNs. Motivated by the theory, we present GCN-LFR, a general robust co-training paradigm for GCN-based models, that encourages transferring the robustness of low-frequency components with an auxiliary neural network. To this end, GCN-LFR could enhance the robustness of various kinds of GCN-based models against poisoning structural attacks in a plug-and-play manner. Extensive experiments across five benchmark datasets and five GCN-based models also confirm that GCN-LFR is resistant to the adversarial attacks without compromising on performance in the benign situation.","['Graph Learning', 'Robustness', 'Adversarial Robustness and Security', 'Theory', 'Deep Learning', 'Representation Learning']",[],"['Heng Chang', 'Yu Rong', 'Tingyang Xu', 'Yatao Bian', 'Shiji Zhou', 'Xin Wang', 'Junzhou Huang', 'Wenwu Zhu']","['Tsinghua University, Tsinghua University', 'Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab', 'Tsinghua University, Tsinghua University', 'Tsinghua University', 'University of Texas, Arlington', 'Tsinghua University, Tsinghua University']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28421,Privacy & Data Governance,Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection,"Detecting out-of-distribution (OOD) samples is vital for developing machine learning based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the {\em predictive normalized maximum likelihood} (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the regret, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary. Furthermore, we describe how to efficiently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither additional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a significant improvement of up to 15.6% over recent leading methods.","['Theory', 'Deep Learning', 'Machine Learning']",[],"['Koby Bibas', 'Meir Feder', 'Tal Hassner']","['Tel Aviv University, Tel Aviv University', 'Tel Aviv University', 'Meta inc.']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28402,Privacy & Data Governance,Online Selective Classification with Limited Feedback,"Motivated by applications to resource-limited and safety-critical domains, we study selective classification in the online learning model, wherein a predictor may abstain from classifying an instance. For example, this may model an adaptive decision to invoke more resources on this instance. Two salient aspects of the setting we consider are that the data may be non-realisable, due to which abstention may be a valid long-term action, and that feedback is only received when the learner abstains, which models the fact that reliable labels are only available when the resource intensive processing is invoked. Within this framework, we explore strategies that make few mistakes, while not abstaining too many times more than the best-in-hindsight error-free classifier from a given class. That is, the one that makes no mistakes, while abstaining the fewest number of times. We construct simple versioning-based schemes for any $\mu \in (0,1],$ that make most $T^\mu$ mistakes while incurring $\tilde{O}(T^{1-\mu})$ excess abstention against adaptive adversaries. We further show that this dependence on $T$ is tight, and provide illustrative experiments on realistic datasets.","['Online Learning', 'Machine Learning']",[],"['Aditya Gangrade', 'Anil Kag', 'Ashok Cutkosky', 'Venkatesh Saligrama']","['Carnegie Mellon University', 'Snap Inc.', 'Boston University', 'Amazon']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28354,Privacy & Data Governance,A Little Robustness Goes a Long Way: Leveraging Robust Features for Targeted Transfer Attacks,"Adversarial examples for neural network image classifiers are known to be transferable: examples optimized to be misclassified by a source classifier are often misclassified as well by classifiers with different architectures. However, targeted adversarial examples—optimized to be classified as a chosen target class—tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classifier. Here, we show that training the source classifier to be ""slightly robust""—that is, robust to small-magnitude adversarial examples—substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as different as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called ""robust"" classifiers.","['Transformers', 'Optimization', 'Robustness', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Jacob M. Springer', 'Melanie Mitchell', 'Garrett T. Kenyon']","['Carnegie Mellon University', 'Santa Fe Institute', 'Los Alamos National Laboratory']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28240,Privacy & Data Governance,EDGE: Explaining Deep Reinforcement Learning Policies,"With the rapid development of deep reinforcement learning (DRL) techniques, there is an increasing need to understand and interpret DRL policies. While recent research has developed explanation methods to interpret how an agent determines its moves, they cannot capture the importance of actions/states to a game's final result. In this work, we propose a novel self-explainable model that augments a Gaussian process with a customized kernel function and an interpretable predictor. Together with the proposed model, we also develop a parameter learning procedure that leverages inducing points and variational inference to improve learning efficiency. Using our proposed model, we can predict an agent's final rewards from its game episodes and extract time step importance within episodes as strategy-level explanations for that agent. Through experiments on Atari and MuJoCo games, we verify the explanation fidelity of our method and demonstrate how to employ interpretation to understand agent behavior, discover policy vulnerabilities, remediate policy errors, and even defend against adversarial attacks.","['Reinforcement Learning and Planning', 'Kernel Methods', 'Adversarial Robustness and Security', 'Generative Model', 'Interpretability']",[],"['Wenbo Guo', 'Xian Wu', 'Usmann Khan', 'Xinyu Xing']","['University of California, Santa Barbara', 'Pennsylvania State University', 'Institute of Technology', 'Pennsylvania State University']","[None, None, 'Georgia', None]"
https://nips.cc/virtual/2021/poster/28190,Privacy & Data Governance,Adversarial Robustness with Non-uniform Perturbations,"Robustness of machine learning models is critical for security related applications, where real-world adversaries are uniquely focused on evading neural network based detectors. Prior work mainly focus on crafting adversarial examples (AEs) with small uniform norm-bounded perturbations across features to maintain the requirement of imperceptibility. However, uniform perturbations do not result in realistic AEs in domains such as malware, finance, and social networks. For these types of applications, features typically have some semantically meaningful dependencies. The key idea of our proposed approach is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training. We propose using characteristics of the empirical data distribution, both on correlations between the features and the importance of the features themselves. Using experimental datasets for malware classification, credit risk prediction, and spam detection, we show that our approach is more robust to real-world attacks. Finally, we present robustness certification utilizing non-uniform perturbation bounds, and show that non-uniform bounds achieve better certification.","['Optimization', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Ecenaz Erdemir', 'Jeffrey Bickford', 'Luca Melis', 'Sergul Aydore']","['Imperial College London', 'Amazon', 'Meta', 'Amazon']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28144,Privacy & Data Governance,Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee,"The growing literature of Federated Learning (FL) has recently inspired Federated Reinforcement Learning (FRL) to encourage multiple agents to federatively build a better decision-making policy without sharing raw trajectories. Despite its promising applications, existing works on FRL fail to I) provide theoretical analysis on its convergence, and II) account for random system failures and adversarial attacks. Towards this end, we propose the first FRL framework the convergence of which is guaranteed and tolerant to less than half of the participating agents being random system failures or adversarial attackers. We prove that the sample efficiency of the proposed framework is guaranteed to improve with the number of agents and is able to account for such potential failures or attacks. All theoretical results are empirically verified on various RL benchmark tasks.","['Reinforcement Learning and Planning', 'Optimization', 'Adversarial Robustness and Security', 'Federated Learning']",[],"['Flint Xiaofeng Fan', 'Yining Ma', 'Zhongxiang Dai', 'Wei Jing', 'Cheston Tan', 'Bryan Kian Hsiang Low']","['National University of', 'Nanyang Technological University', 'Massachusetts Institute of Technology', 'NetEase, Inc.', 'A*STAR', 'National University of']","['Singapore', None, None, None, None, 'Singapore']"
https://nips.cc/virtual/2021/poster/28131,Privacy & Data Governance,Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints,"Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. The inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model. In this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively finding the sample misclassified with maximum confidence within an $\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to minimize the distance of the current sample to the decision boundary. Extensive experiments show that FMN significantly outperforms existing $\ell_0$, $\ell_1$, and $\ell_\infty$-norm attacks in terms of perturbation size, convergence speed and computation time, while reporting comparable performances with state-of-the-art $\ell_2$-norm attacks. Our open-source code is available at: https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack.","['Vision', 'Robustness', 'Machine Learning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Maura Pintor', 'Fabio Roli', 'Wieland Brendel', 'Battista Biggio']","['University of Cagliari', 'University of Cagliari', 'ELLIS Institute Tübingen', 'University of Cagliari,']","[None, None, None, 'Italy']"
https://nips.cc/virtual/2021/poster/28087,Privacy & Data Governance,Adversarial Teacher-Student Representation Learning for Domain Generalization,"Domain generalization (DG) aims to transfer the learning task from a single or multiple source domains to unseen target domains. To extract and leverage the information which exhibits sufficient generalization ability, we propose a simple yet effective approach of Adversarial Teacher-Student Representation Learning, with the goal of deriving the domain generalizable representations via generating and exploring out-of-source data distributions. Our proposed framework advances Teacher-Student learning in an adversarial learning manner, which alternates between knowledge-distillation based representation learning and novel-domain data augmentation. The former progressively updates the teacher network for deriving domain-generalizable representations, while the latter synthesizes data out-of-source yet plausible distributions. Extensive image classification experiments on benchmark datasets in multiple and single source DG settings confirm that, our model exhibits sufficient generalization ability and performs favorably against state-of-the-art DG methods.","['Vision', 'Machine Learning', 'Representation Learning', 'Domain Adaptation']",[],"['Fu-En Yang', 'Yuan-Chia Cheng', 'Zu-Yun Shiau', 'Yu-Chiang Frank Wang']","['NVIDIA', 'National Taiwan University', 'National Taiwan University', 'NVIDIA']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28007,Privacy & Data Governance,Efficiently Learning One Hidden Layer ReLU Networks From Queries,"While the problem of PAC learning neural networks from samples has received considerable attention in recent years, in certain settings like model extraction attacks, it is reasonable to imagine having more than just the ability to observe random labeled examples. Motivated by this, we consider the following problem: given \emph{black-box query access} to a neural network $F$, recover $F$ up to some error. Formally, we show that if $F$ is an arbitrary one hidden layer neural network with ReLU activations, there is an algorithm with query complexity and runtime polynomial in all parameters which outputs a network $F’$ achieving low square loss relative to $F$ with respect to the Gaussian measure. While a number of works in the security literature have proposed and empirically demonstrated the effectiveness of certain algorithms for this problem, ours is to the best of our knowledge the first provable guarantee in this vein.","['Theory', 'Deep Learning']",[],"['Sitan Chen', 'Adam Klivans', 'Raghu Meka']","['University of California Berkeley', 'University of Texas, Austin', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27569,Privacy & Data Governance,Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations,"Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS), which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates are learned via adversarial training and ensure the policy's safety assuming calibrated learned dynamics. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary.  Prior methods require hundreds of violations to achieve decent rewards on these tasks,  whereas our proposed algorithms incur zero violations.","['Reinforcement Learning and Planning', 'Adversarial Robustness and Security']",[],"['Yuping Luo', 'Tengyu Ma']","['Princeton University', 'Stanford University']","[None, None]"
https://nips.cc/virtual/2021/poster/27974,Privacy & Data Governance,Safe Policy Optimization with Local Generalized Linear Function Approximations,"Safe exploration is a key to applying reinforcement learning (RL) in safety-critical systems. Existing safe exploration methods guaranteed safety under the assumption of regularity, and it has been difficult to apply them to large-scale real problems. We propose a novel algorithm, SPO-LF, that optimizes an agent's policy while learning the relation between a locally available feature obtained by sensors and environmental reward/safety using generalized linear function approximations. We provide theoretical guarantees on its safety and optimality. We experimentally show that our algorithm is 1) more efficient in terms of sample complexity and computational cost and 2) more applicable to large-scale problems than previous safe RL methods with theoretical guarantees, and 3) comparably sample-efficient and safer compared with existing advanced deep RL methods with safety constraints.","['Reinforcement Learning and Planning', 'Theory', 'Optimization']",[],"['Akifumi Wachi', 'Yunyue Wei', 'Yanan Sui']","['LINE', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27929,Privacy & Data Governance,Adversarial Attack Generation Empowered by Min-Max Optimization,"The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness. Nevertheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the adversarial context. In this paper, we show how a general notion of min-max optimization over multiple domains can be leveraged to the design of different types of adversarial attacks. In particular, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as a min-max problem by introducing domain weights that are maximized over the probability simplex of the domain set. We showcase this unified framework in three attack generation problems -- attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations. Extensive experiments demonstrate that our approach leads to substantial attack improvement over the existing heuristic strategies as well as robustness improvement over state-of-the-art defense methods against multiple perturbation types. Furthermore, we find that the self-adjusted domain weights learned from min-max optimization can provide a holistic tool to explain the difficulty level of attack across domains.","['Adversarial Robustness and Security', 'Robustness', 'Optimization']",[],"['Jingkang Wang', 'Tianyun Zhang', 'Sijia Liu', 'Pin-Yu Chen', 'Jiacen Xu', 'Bo Li']","['Waabi', 'Cleveland State University', 'Michigan State University', 'International Business Machines', 'University of California, Irvine', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27909,Privacy & Data Governance,You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership,"Despite tremendous success in many application scenarios, the training and inference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a special sparse subnetwork (i.e., $\textit{winning ticket}$) instead of a full model for both training and inference, that can lower both costs without sacrificing the performance. The main resource bottleneck of LTH is however the extraordinary cost to find the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships, since they take owners' massive/unique resources to develop or train. While existing methods explored encrypted weights or predictions, we investigate a unique way to leverage sparse topological information to perform $\textit{lottery verification}$, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box verification scenarios. Through extensive experiments, we demonstrate the effectiveness of lottery verification in diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our verification is shown to be robust to removal attacks such as model fine-tuning and pruning, as well as several ambiguity attacks. Our codes are available at https://github.com/VITA-Group/NO-stealing-LTH.","['Deep Learning', 'Graph Learning']",[],"['Xuxi Chen', 'Tianlong Chen', 'Zhenyu Zhang', 'Zhangyang Wang']","['University of Texas at Austin', 'Massachusetts Institute of Technology', 'University of Texas at Austin', 'University of Texas at Austin']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27870,Privacy & Data Governance,Relaxing Local Robustness,"Certifiable local robustness, which rigorously precludes small-norm adversarial examples, has received significant attention as a means of addressing security concerns in deep learning. However, for some classification problems, local robustness is not a natural objective, even in the presence of adversaries; for example, if an image contains two classes of subjects, the correct label for the image may be considered arbitrary between the two, and thus enforcing strict separation between them is unnecessary. In this work, we introduce two relaxed safety properties for classifiers that address this observation: (1) relaxed top-k robustness, which serves as the analogue of top-k accuracy; and (2) affinity robustness, which specifies which sets of labels must be separated by a robustness margin, and which can be $\epsilon$-close in $\ell_p$ space. We show how to construct models that can be efficiently certified against each relaxed robustness property, and trained with very little overhead relative to standard gradient descent. Finally, we demonstrate experimentally that these relaxed variants of robustness are well-suited to several significant classification problems, leading to lower rejection rates and higher certified accuracies than can be obtained when certifying ""standard"" local robustness.","['Optimization', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Klas Leino', 'Matt Fredrikson']","['School of Computer Science, Carnegie Mellon University', 'Carnegie Mellon University']","[None, None]"
https://nips.cc/virtual/2021/poster/27832,Privacy & Data Governance,Sampling  with Trusthworthy Constraints:  A Variational Gradient Framework,"Sampling-based inference and learning techniques, especially Bayesian inference, provide an essential approach to handling uncertainty in machine learning (ML). As these techniques are increasingly used in daily life, it becomes essential to safeguard the ML systems with various trustworthy-related constraints, such as fairness, safety, interpretability. Mathematically, enforcing these constraints in probabilistic inference can be cast into sampling from intractable distributions subject to general nonlinear constraints, for which practical efficient algorithms are still largely missing. In this work, we propose a family of constrained sampling algorithms which generalize Langevin Dynamics (LD) and Stein Variational Gradient Descent (SVGD) to incorporate a moment constraint specified by a general nonlinear function. By exploiting the gradient flow structure of LD and SVGD, we derive two types of algorithms for handling constraints, including a primal-dual gradient approach and the constraint controlled gradient descent approach. We investigate the continuous-time mean-field limit of these algorithms and show that they have O(1/t) convergence under mild conditions. Moreover, the LD variant converges linearly assuming that a log Sobolev like inequality holds. Various numerical experiments are conducted to demonstrate the efficiency of our algorithms in trustworthy settings.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Xingchao Liu', 'Xin Tong', 'qiang liu']","['University of Texas, Austin', 'National University of', 'Dartmouth College']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27835,Privacy & Data Governance,Anti-Backdoor Learning: Training Clean Models on Poisoned Data,"Backdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place. In this paper, we introduce the concept of \emph{anti-backdoor learning}, aiming to train \emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \emph{clean} and the \emph{backdoor} portions of data. From this view, we identify two inherent characteristics of backdoor attacks as their weaknesses: 1) the models learn backdoored data much faster than learning with clean data, and the stronger the attack the faster the model converges on backdoored data; 2) the backdoor task is tied to a specific class (the backdoor target class). Based on these two weaknesses, we propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training. ABL introduces a two-stage \emph{gradient ascent} mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Through extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks, we empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data. Code is available at \url{https://github.com/bboylyg/ABL}.",['Deep Learning'],[],"['Yige Li', 'Xixiang Lyu', 'Nodens Koren', 'Lingjuan Lyu', 'Bo Li', 'Xingjun Ma']","['Xidian University', ""Xi'an University of Electronic Science and Technology"", 'ETHZ - ETH Zurich', 'Sony Research', 'University of Illinois, Urbana Champaign', 'Fudan University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27819,Privacy & Data Governance,Sageflow: Robust Federated Learning against Both Stragglers and Adversaries,"While federated learning (FL) allows efficient model training with local data at edge devices, among major issues still to be resolved are: slow devices known as stragglers and malicious attacks launched by adversaries.   While the presence of both of these issues raises serious concerns in practical FL systems, no known schemes or combinations of schemes effectively address them at the same time. We propose Sageflow, staleness-aware grouping with entropy-based filtering and loss-weighted averaging, to handle both stragglers and adversaries simultaneously. Model grouping and weighting according to staleness (arrival delay) provides robustness against stragglers, while entropy-based filtering and loss-weighted averaging, working in a highly complementary fashion at each grouping stage,  counter a wide range of adversary attacks. A theoretical bound is established to provide key insights into the convergence behavior of Sageflow. Extensive experimental results show that Sageflow outperforms various existing methods aiming to handle stragglers/adversaries.","['Federated Learning', 'Robustness']",[],"['Jungwuk Park', 'Dong-Jun Han', 'Minseok Choi', 'Jaekyun Moon']","['Korea Advanced Institute of Science and Technology', 'Purdue University', 'Jeju National University', 'KAIST']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27787,Privacy & Data Governance,Detecting Anomalous Event Sequences with Temporal Point Processes,"Automatically detecting anomalies in event data can provide substantial value in domains such as healthcare, DevOps, and information security. In this paper, we frame the problem of detecting anomalous continuous-time event sequences as out-of-distribution (OOD) detection for temporal point processes (TPPs). First, we show how this problem can be approached using goodness-of-fit (GoF) tests. We then demonstrate the limitations of popular GoF statistics for TPPs and propose a new test that addresses these shortcomings. The proposed method can be combined with various TPP models, such as neural TPPs, and is easy to implement. In our experiments, we show that the proposed statistic excels at both traditional GoF testing, as well as at detecting anomalies in simulated and real-world data.",['Generative Model'],[],"['Oleksandr Shchur', 'Ali Caner Turkmen', 'Tim Januschowski', 'Jan Gasthaus', 'Stephan Günnemann']","['Amazon', 'Amazon', 'Amazon', 'Meta', 'Technical University Munich']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27754,Privacy & Data Governance,Random Noise Defense Against Query-Based Black-Box Attacks,"The query-based black-box attacks have raised serious threats to machine learning models in many real applications. In this work, we study a lightweight defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian noise to each query. We conduct the theoretical analysis about the effectiveness of RND against query-based black-box attacks and the corresponding adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or local search.  The large magnitude ratio leads to the stronger defense performance of RND, and it's also critical for mitigating adaptive attacks. Based on our analysis, we further propose to combine RND with a plausible Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise to each query while maintaining the clean accuracy to obtain a better trade-off between clean accuracy and defense performance. Additionally, RND can be flexibly combined with the existing defense methods to further boost the adversarial robustness, such as adversarial training (AT). Extensive experiments on CIFAR-10 and ImageNet verify our theoretical findings and the effectiveness of RND and RND-GF.","['Adversarial Robustness and Security', 'Robustness', 'Machine Learning']",[],"['Zeyu Qin', 'Yanbo Fan', 'Hongyuan Zha', 'Baoyuan Wu']","['The  University of Science and Technology', 'Tencent AI Lab', 'The Chinese University of , Shenzhen', 'The Chinese University of , Shenzhen']","['Hong Kong', None, 'Hong Kong', 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27717,Privacy & Data Governance,Collaborative Uncertainty in Multi-Agent Trajectory Forecasting,"Uncertainty modeling is critical in trajectory-forecasting systems for both interpretation and safety reasons. To better predict the future trajectories of multiple agents, recent works have introduced interaction modules to capture interactions among agents. This approach leads to correlations among the predicted trajectories. However, the uncertainty brought by such correlations is neglected. To fill this gap, we propose a novel concept, collaborative uncertainty (CU), which models the uncertainty resulting from the interaction module. We build a general CU-based framework to make a prediction model learn the future trajectory and the corresponding uncertainty. The CU-based framework is integrated as a plugin module to current state-of-the-art (SOTA) systems and deployed in two special cases based on multivariate Gaussian and Laplace distributions. In each case, we conduct extensive experiments on two synthetic datasets and two public, large-scale benchmarks of trajectory forecasting. The results are promising: 1) The results of synthetic datasets show that CU-based framework allows the model to nicely rebuild the ground-truth distribution. 2) The results of trajectory forecasting benchmarks demonstrate that the CU-based framework steadily helps SOTA systems improve their performances. Specially, the proposed CU-based framework helps VectorNet improve by 57 cm regarding Final Displacement Error on nuScenes dataset. 3) The visualization results of CU illustrate that the value of CU is highly related to the amount of the interactive information among agents.",['Deep Learning'],[],"['Bohan Tang', 'Yiqi Zhong', 'Gang Wang', 'Siheng Chen', 'Ya Zhang']","['University of Oxford', 'University of Southern California', 'Beijing Institute of Technology', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27698,Privacy & Data Governance,Counterexample Guided RL Policy Refinement Using Bayesian Optimization,"Constructing Reinforcement Learning (RL) policies that adhere to safety requirements is an emerging field of study. RL agents learn via trial and error with an objective to optimize a reward signal. Often policies that are designed to accumulate rewards do not satisfy safety specifications. We present a methodology for counterexample guided refinement of a trained RL policy against a given safety specification. Our approach has two main components. The first component is an approach to discover failure trajectories using Bayesian optimization over multiple parameters of uncertainty from a policy learnt in a model-free setting. The second component selectively modifies the failure points of the policy using gradient-based updates. The approach has been tested on several RL environments, and we demonstrate that the policy can be made to respect the safety specifications through such targeted changes.","['Reinforcement Learning and Planning', 'Optimization']",[],"['Briti Gangopadhyay', 'Pallab Dasgupta']","['Sony Group Coorporation', 'n Institute of Technology Kharagpur']","[None, 'India']"
https://nips.cc/virtual/2021/poster/27590,Privacy & Data Governance,Safe Reinforcement Learning by Imagining the Near Future,"Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.",['Reinforcement Learning and Planning'],[],"['Garrett Thomas', 'Yuping Luo', 'Tengyu Ma']","['Stanford University', 'Princeton University', 'Stanford University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27612,Privacy & Data Governance,Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics,"Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we explore whether this shortcoming may be partly addressed by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We take inspiration from a popular framework in neuroscience: ""predictive coding"". At each layer of the hierarchical model, generative feedback ""predicts"" (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network’s representations across timesteps, and to optimize the network's feedback weights over the natural image dataset--a form of unsupervised training. We show that implementing this strategy into two popular networks, VGG16 and EfficientNetB0, improves their robustness against various corruptions and adversarial attacks. We hypothesize that other feedforward networks could similarly benefit from the proposed framework. To promote research in this direction, we provide an open-sourced PyTorch-based package called \textit{Predify}, which can be used to implement and investigate the impacts of the predictive coding dynamics in any convolutional neural network.","['Vision', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Neuroscience']",[],"['Bhavin Choksi', 'Milad Mozafari', ""Callum Biggs O'May"", 'B. ADOR', 'Andrea Alamia', 'Rufin VanRullen']","['CNRS', 'CNRS', 'Centre de Recherche Cerveau et Cognition', 'Centre de Recherche Cerveau et Cognition', 'CNRS', 'CNRS']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27564,Privacy & Data Governance,Towards a Unified Game-Theoretic View of Adversarial Perturbations and Robustness,"This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing robustness-boosting methods in a principle way. Besides, our findings also make a revision of previous inaccurate understanding of the shape bias of adversarially learned features. Our code is available online at https://github.com/Jie-Ren/A-Unified-Game-Theoretic-Interpretation-of-Adversarial-Robustness.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Jie Ren', 'Die Zhang', 'Yisen Wang', 'Lu Chen', 'Zhanpeng Zhou', 'Yiting Chen', 'Xu Cheng', 'Xin Wang', 'Meng Zhou', 'Jie Shi', 'Quanshi Zhang']","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Peking University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiaotong University', 'Nanjing University of Science and Technology', 'Shanghai Jiao Tong University', 'CMU, Carnegie Mellon University', 'Huawei International.', 'Shanghai Jiao Tong University']","[None, None, None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27544,Privacy & Data Governance,Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks,"Adversarial attacks based on randomized search schemes have obtained state-of-the-art results in black-box robustness evaluation recently. However, as we demonstrate in this work, their efficiency in different query budget regimes depends on manual design and heuristic tuning of the underlying proposal distributions. We study how this issue can be addressed by adapting the proposal distribution online based on the information obtained during the attack. We consider Square Attack, which is a state-of-the-art score-based black-box attack, and demonstrate how its performance can be improved by a learned controller that adjusts the parameters of the proposal distribution online during the attack. We train the controller using gradient-based end-to-end training on a CIFAR10 model with white box access. We demonstrate that plugging the learned controller into the attack consistently improves its black-box robustness estimate in different query regimes by up to 20% for a wide range of different models with black-box access. We further show that the learned adaptation principle transfers well to the other data distributions such as CIFAR100 or ImageNet and to the targeted attack setting.","['Adversarial Robustness and Security', 'Meta Learning', 'Robustness']",[],"['Maksym Yatsura', 'Jan Hendrik Metzen', 'Matthias Hein']","['Robert Bosch GmbH, Bosch', 'Bosch Center Artificial Intelligence', 'University of Tübingen']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27483,Privacy & Data Governance,Alignment Attention by Matching Key and Query Distributions,"The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.","['Graph Learning', 'Robustness', 'Vision', 'Language', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Shujian Zhang', 'XINJIE FAN', 'Huangjie Zheng', 'Korawat Tanwisuth', 'Mingyuan Zhou']","['University of Texas, Austin', 'University of Texas, Austin', 'University of Texas, Austin', 'University of Texas, Austin', 'The University of Texas at Austin']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27413,Privacy & Data Governance,Adversarially robust learning for security-constrained optimal power flow,"In recent years, the ML community has seen surges of interest in both adversarially robust learning and implicit layers, but connections between these two areas have seldom been explored. In this work, we combine innovations from these areas to tackle the problem of N-k security-constrained optimal power flow (SCOPF). N-k SCOPF is a core problem for the operation of electrical grids, and aims to schedule power generation in a manner that is robust to potentially $k$ simultaneous equipment outages. Inspired by methods in adversarially robust training, we frame N-k SCOPF as a minimax optimization problem -- viewing power generation settings as adjustable parameters and equipment outages as (adversarial) attacks -- and solve this problem via gradient-based techniques. The loss function of this minimax problem involves resolving implicit equations representing grid physics and operational decisions, which we differentiate through via the implicit function theorem. We demonstrate the efficacy of our framework in solving N-3 SCOPF, which has traditionally been considered as prohibitively expensive to solve given that the problem size depends combinatorially on the number of potential outages.","['Adversarial Robustness and Security', 'Robustness', 'Optimization']",[],"['Priya L. Donti', 'Aayushya Agarwal', 'Neeraj Vijay Bedmutha', 'Larry Pileggi', 'J Zico Kolter']","['Massachusetts Institute of Technology', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27374,Privacy & Data Governance,Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples,"Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera.","['Deep Learning', 'Privacy']",[],"['Kanghyun Choi', 'Deokki Hong', 'Noseong Park', 'Youngsok Kim', 'Jinho Lee']","['Seoul National University', 'Sapeon Korea', 'Yonsei University', 'Yonsei University', 'Seoul National University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27272,Privacy & Data Governance,Adversarial Robustness with Semi-Infinite Constrained Learning,"Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains.  While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of robust learning, particularly with respect to when and why adversarial training works.  In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Markov Chain Monte Carlo approach for which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10.  Our code is available at: https://github.com/arobey1/advbench.","['Optimization', 'Robustness', 'Theory', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Alexander Robey', 'Luiz F. O. Chamon', 'George J. Pappas', 'Hamed Hassani', 'Alejandro Ribeiro']","['School of Engineering and Applied Science, University of Pennsylvania', 'Universität Stuttgart', 'School of Engineering and Applied Science, University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27251,Privacy & Data Governance,Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time,"In several real world applications, machine learning models are deployed to make predictions on data whose distribution changes gradually along time, leading to a drift between the train and test distributions. Such models are often re-trained on new data periodically, and they hence need to generalize to data not too far into the future. In this context, there is much prior work on enhancing temporal generalization, e.g. continuous transportation of past data, kernel smoothed time-sensitive parameters and more recently, adversarial learning of time-invariant features. However, these methods share several limitations, e.g, poor scalability, training instability, and dependence on unlabeled data from the future. Responding to the above limitations, we propose a simple method that starts with a model with time-sensitive parameters but regularizes its temporal complexity using a Gradient Interpolation  (GI) loss. GI allows the decision boundary to change along time and can still prevent overfitting to the limited training time snapshots  by allowing task-specific control over changes along time. We compare our method to existing baselines on multiple real-world datasets, which show that GI outperforms more complicated generative and adversarial approaches on the one hand, and simpler gradient regularization methods on the other.","['Machine Learning', 'Domain Adaptation']",[],"['Anshul Nasery', 'Soumyadeep Thakur', 'Vihari Piratla', 'Abir De', 'Sunita Sarawagi']","['University of Washington', 'n Institute of Technology, Bombay', 'University of Cambridge', 'n Institute of Technology Bombay,', 'IIT Bombay']","[None, 'India', None, 'India', None]"
https://nips.cc/virtual/2021/poster/27236,Privacy & Data Governance,Evaluating Gradient Inversion Attacks and Defenses in Federated Learning,"Gradient inversion attack (or input recovery from gradient) is an emerging threat to the security and privacy preservation of Federated learning, whereby malicious eavesdroppers or participants in the protocol can recover (partially) the clients' private data. This paper evaluates existing attacks and defenses. We find that some attacks make strong assumptions about the setup. Relaxing such assumptions can substantially weaken these attacks. We then evaluate the benefits of three proposed defense mechanisms against gradient inversion attacks. We show the trade-offs of privacy leakage and data utility of these defense methods, and find that combining them in an appropriate manner makes the attack less effective, even under the original strong assumptions. We also estimate the computation cost of end-to-end recovery of a single image under each evaluated defense. Our findings suggest that the state-of-the-art attacks can currently be defended against with minor data utility loss, as summarized in a list of potential strategies.","['Federated Learning', 'Privacy']",[],"['Yangsibo Huang', 'Samyak Gupta', 'Zhao Song', 'Kai Li', 'Sanjeev Arora']","['Princeton University', 'Princeton University', 'Adobe Research', 'Princeton University', 'Princeton University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27196,Privacy & Data Governance,Robust Deep Reinforcement Learning through Adversarial Loss,"Recent studies have shown that deep reinforcement learning agents are vulnerable to small adversarial perturbations on the agent's inputs, which raises concerns about deploying such agents in the real world. To address this issue, we propose RADIAL-RL, a principled framework to train reinforcement learning agents with improved robustness against $l_p$-norm bounded adversarial attacks. Our framework is compatible with popular deep reinforcement learning algorithms and we demonstrate its performance with deep Q-learning, A3C and PPO. We experiment on three deep RL benchmarks (Atari, MuJoCo and ProcGen) to show the effectiveness of our robust training algorithm. Our RADIAL-RL agents consistently outperform prior methods when tested against attacks of varying strength and are more computationally efficient to train. In addition, we propose a new evaluation method called Greedy-Worst-Case Reward (GWC) to measure attack agnostic robustness of deep RL agents. We show that GWC can be evaluated efficiently and is a good estimate of the reward under the worst possible sequence of adversarial attacks. All code used for our experiments is available at https://github.com/tuomaso/radial_rl_v2.","['Reinforcement Learning and Planning', 'Robustness', 'Adversarial Robustness and Security']",[],"['Tuomas Oikarinen', 'Wang Zhang', 'Alexandre Megretski', 'Luca Daniel', 'Tsui-Wei Weng']","['University of California, San Diego', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of California, San Diego']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27165,Privacy & Data Governance,Manipulating SGD with Data Ordering Attacks,"Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.","['Vision', 'Machine Learning']",[],"['I Shumailov', 'Zakhar Shumaylov', 'Dmitry Kazhdan', 'Yiren Zhao', 'Nicolas Papernot', 'Murat A Erdogdu', 'Ross Anderson']","['Google DeepMind', 'University of Cambridge', 'University of Cambridge', 'Imperial College London', 'University of Toronto', 'University of Toronto', 'University of Edinburgh, University of Edinburgh']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27010,Privacy & Data Governance,SSAL: Synergizing between Self-Training and Adversarial Learning for Domain Adaptive Object Detection,"We study adapting trained object detectors to unseen domains manifesting significant variations of object appearance, viewpoints and backgrounds. Most current methods align domains by either using image or instance-level feature alignment in an adversarial fashion. This often suffers due to the presence of unwanted background and as such lacks class-specific alignment. A common remedy to promote class-level alignment is to use high confidence predictions on the unlabelled domain as pseudo labels. These high confidence predictions are often fallacious since the model is poorly calibrated under domain shift. In this paper, we propose to leverage model’s predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. Specifically, we measure predictive uncertainty on class assignments and the bounding box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-supervision, whereas the ones with higher uncertainty are used to generate tiles for an adversarial feature alignment stage. This synergy between tiling around the uncertain object regions and generating pseudo-labels from highly certain object regions allows us to capture both the image and instance level context during the model adaptation stage. We perform extensive experiments covering various domain shift scenarios. Our approach improves upon existing state-of-the-art methods with visible margins.","['Deep Learning', 'Vision', 'Domain Adaptation']",[],"['Muhammad Akhtar Munir', 'Muhammad Haris Khan', 'M. Saquib Sarfraz', 'Mohsen Ali']","['Mohamed bin Zayed University of Artificial Intelligence', 'MBZUAI', 'Karlsruher Institut für Technologie', 'Information Technology University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26917,Privacy & Data Governance,Antipodes of Label Differential Privacy: PATE and ALIBI,"We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks. While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning. We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis. Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes.","['Semi-Supervised Learning', 'Machine Learning', 'Privacy']",[],"['Mani Malek Esmaeili', 'Ilya Mironov', 'Karthik Prasad', 'Igor Shilov', 'Florian Tramer']","['Facebook', 'Facebook', 'Facebook AI', 'Imperial College London', 'ETHZ - ETH Zurich']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27598,Privacy & Data Governance,Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training,"Delusive attacks aim to substantially deteriorate the test accuracy of the learning model by slightly perturbing the features of correctly labeled training examples. By formalizing this malicious attack as finding the worst-case training data within a specific $\infty$-Wasserstein ball, we show that minimizing adversarial risk on the perturbed data is equivalent to optimizing an upper bound of natural risk on the original data. This implies that adversarial training can serve as a principled defense against delusive attacks. Thus, the test accuracy decreased by delusive attacks can be largely recovered by adversarial training. To further understand the internal mechanism of the defense, we disclose that adversarial training can resist the delusive perturbations by preventing the learner from overly relying on non-robust features in a natural setting. Finally, we complement our theoretical findings with a set of experiments on popular benchmark datasets, which show that the defense withstands six different practical attacks. Both theoretical and empirical results vote for adversarial training when confronted with delusive adversaries.",['Adversarial Robustness and Security'],[],"['Lue Tao', 'Lei Feng', 'Jinfeng Yi', 'Sheng-Jun Huang', 'Songcan Chen']","['Nanjing University', 'Nanyang Technological University', 'JD AI Research', 'Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26781,Privacy & Data Governance,Learning to Simulate Self-driven Particles System with Coordinated Policy Optimization,"Self-Driven Particles (SDP) describe a category of multi-agent systems common in everyday life, such as flocking birds and traffic flows. In a SDP system, each agent pursues its own goal and constantly changes its cooperative or competitive behaviors with its nearby agents. Manually designing the controllers for such SDP system is time-consuming, while the resulting emergent behaviors are often not realistic nor generalizable. Thus the realistic simulation of SDP systems remains challenging. Reinforcement learning provides an appealing alternative for automating the development of the controller for SDP. However, previous multi-agent reinforcement learning (MARL) methods define the agents to be teammates or enemies before hand, which fail to capture the essence of SDP where the role of each agent varies to be cooperative or competitive even within one episode. To simulate SDP with MARL, a key challenge is to coordinate agents' behaviors while still maximizing individual objectives. Taking traffic simulation as the testing bed, in this work we develop a novel MARL method called Coordinated Policy Optimization (CoPO), which incorporates social psychology principle to learn neural controller for SDP. Experiments show that the proposed method can achieve superior performance compared to MARL baselines in various metrics. Noticeably the trained vehicles exhibit complex and diverse social behaviors that improve performance and safety of the population as a whole. Demo video and source code are available at: https://decisionforce.github.io/CoPO/","['Reinforcement Learning and Planning', 'Optimization']",[],"['Zhenghao Peng', 'Quanyi Li', 'Ka Ming Hui', 'Chunxiao Liu', 'Bolei Zhou']","['University of California, Los Angeles', 'University of Edinburgh', 'The Chinese University of', 'Tsinghua University', 'University of California, Los Angeles']","[None, None, 'Hong Kong', None, None]"
https://nips.cc/virtual/2021/poster/26761,Privacy & Data Governance,Backdoor Attack with Imperceptible Input and Latent Modification,"Recent studies have shown that deep neural networks (DNN) are vulnerable to various adversarial attacks. In particular, an adversary can inject a stealthy backdoor into a model such that the compromised model will behave normally without the presence of the trigger. Techniques for generating backdoor images that are visually imperceptible from clean images have also been developed recently, which further enhance the stealthiness of the backdoor attacks from the input space. Along with the development of attacks, defense against backdoor attacks is also evolving. Many existing countermeasures found that backdoor tends to leave tangible footprints in the latent or feature space, which can be utilized to mitigate backdoor attacks. In this paper, we extend the concept of imperceptible backdoor from the input space to the latent representation, which significantly improves the effectiveness against the existing defense mechanisms, especially those relying on the distinguishability between clean inputs and backdoor inputs in latent space. In the proposed framework, the trigger function will learn to manipulate the input by injecting imperceptible input noise while matching the latent representations of the clean and manipulated inputs via a Wasserstein-based regularization of the corresponding empirical distributions. We formulate such an objective as a non-convex and constrained optimization problem and solve the problem with an efficient stochastic alternating optimization procedure. We name the proposed backdoor attack as Wasserstein Backdoor (WB), which achieves a high attack success rate while being stealthy from both the input and latent spaces, as tested in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and TinyImagenet.","['Adversarial Robustness and Security', 'Deep Learning', 'Optimization', 'Generative Model']",[],"['Khoa Doan', 'Yingjie Lao', 'Ping Li']","['VinUniversity', 'Tufts University', 'Rutgers University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26751,Privacy & Data Governance,Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing,"Unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain has attracted much attention in recent years. While many domain adaptation techniques have been proposed for images, the problem of unsupervised domain adaptation in videos remains largely underexplored. In this paper, we introduce Contrast and Mix (CoMix), a new contrastive learning framework that aims to learn discriminative invariant feature representations for unsupervised video domain adaptation. First, unlike existing methods that rely on adversarial learning for feature alignment, we utilize temporal contrastive learning to bridge the domain gap by maximizing the similarity between encoded representations of an unlabeled video at two different speeds as well as minimizing the similarity between different videos played at different speeds. Second, we propose a novel extension to the temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains. Moreover, we also integrate a supervised contrastive learning objective using target pseudo-labels to enhance discriminability of the latent space for video domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed approach over state-of-the-art methods. Project page: https://cvir.github.io/projects/comix.","['Contrastive Learning', 'Domain Adaptation']",[],"['Rutav Shah', 'Rameswar Panda', 'Kate Saenko', 'Abir Das']","['University of Texas at Austin', 'MIT-IBM Watson AI Lab', 'Boston University', 'n Institute of Technology Kharagpur']","[None, None, None, 'India']"
https://nips.cc/virtual/2021/poster/26667,Privacy & Data Governance,Class-Disentanglement and Applications in Adversarial Detection and Defense,"What is the minimum necessary information required by a neural net $D(\cdot)$ from an image $x$ to accurately predict its class? Extracting such information in the input space from $x$ can allocate the areas $D(\cdot)$ mainly attending to and shed novel insights to the detection and defense of adversarial attacks. In this paper, we propose ''class-disentanglement'' that trains a variational autoencoder $G(\cdot)$ to extract this class-dependent information as $x - G(x)$ via a trade-off between reconstructing $x$ by $G(x)$ and classifying $x$ by $D(x-G(x))$, where the former competes with the latter in decomposing $x$ so the latter retains only necessary information for classification in $x-G(x)$. We apply it to both clean images and their adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class-dependent part $x-G(x)$. The decomposition results also provide novel interpretations to classification and attack models. Inspired by these observations, we propose to conduct adversarial detection and adversarial defense respectively on $x - G(x)$ and $G(x)$, which consistently outperform the results on the original $x$. In experiments, this simple approach substantially improves the detection and defense against different types of adversarial attacks.","['Adversarial Robustness and Security', 'Machine Learning']",[],"['Kaiwen Yang', 'Tianyi Zhou', 'Yonggang Zhang', 'Xinmei Tian', 'Dacheng Tao']","['University of Science and Technology of', 'University of Maryland, College Park', 'Baptist University', 'University of Science and Technology of', 'University of Sydney']","['China', None, 'Hong Kong', 'China', None]"
https://nips.cc/virtual/2021/poster/26631,Privacy & Data Governance,Robust and Fully-Dynamic Coreset for Continuous-and-Bounded Learning (With Outliers) Problems,"In many machine learning tasks, a common approach for dealing with large-scale data is to build a small summary, {\em e.g.,} coreset,  that can efficiently represent the original input. However, real-world datasets usually contain outliers and most existing coreset construction methods are not resilient against outliers (in particular, an outlier can be located arbitrarily in the space by an adversarial attacker). In this paper, we propose a novel robust coreset method for the {\em continuous-and-bounded learning} problems (with outliers) which includes a broad range of popular optimization objectives in machine learning, {\em e.g.,} logistic regression and $ k $-means clustering. Moreover, our robust coreset  can be efficiently maintained in fully-dynamic environment. To the best of our knowledge, this is the first robust and fully-dynamic coreset construction method for these optimization problems. Another highlight is that our coreset size can depend on the doubling dimension of the parameter space, rather than the VC dimension of the objective function which could be very large or even challenging to compute. Finally, we conduct the experiments on real-world datasets to evaluate the effectiveness of our proposed robust coreset method.","['Clustering', 'Optimization', 'Adversarial Robustness and Security', 'Machine Learning']",[],"['Zixiu Wang', 'Yiwen Guo', 'Hu Ding']","['Meituan', 'ByteDance', 'University of Science and Technology of']","[None, None, 'China']"
https://nips.cc/virtual/2021/poster/26638,Privacy & Data Governance,A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks,"Poisoning attacks have emerged as a significant security threat to machine learning algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the performance of the output model. Most of these attacks require the full knowledge of training data. This leaves open the possibility of achieving the same attack results using poisoning attacks that do not have the full knowledge of the clean training set. In this work, we initiate a theoretical study of the problem above. Specifically, for the case of feature selection with LASSO, we show that \emph{full information} adversaries (that craft poisoning examples based on the rest of the training data) are provably much more devastating compared to the optimal attacker that is \emph{oblivious} to the training set yet has access to the distribution of the data.  Our separation result shows that the two settings of data-aware and data-oblivious are fundamentally different and we cannot hope to achieve the same attack or defense results in these scenarios.",['Machine Learning'],[],"['Samuel Deng', 'Sanjam Garg', 'Somesh Jha', 'Saeed Mahloujifar', 'Mohammad Mahmoody', 'Abhradeep Guha Thakurta']","['Columbia University', 'University of California Berkeley', 'Department of Computer Science, University of Wisconsin, Madison', 'Meta', 'University of Virginia', 'Google']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26582,Privacy & Data Governance,Learning Stable Deep Dynamics Models for Partially Observed or Delayed Dynamical Systems,"Learning how complex dynamical systems evolve over time is a key challenge in system identification. For safety critical systems, it is often crucial that the learned model is guaranteed to converge to some equilibrium point. To this end, neural ODEs regularized with neural Lyapunov functions are a promising approach when states are fully observed. For practical applications however, {\em partial observations} are the norm. As we will demonstrate, initialization of unobserved augmented states can become a key problem for neural ODEs. To alleviate this issue, we propose to augment the system's state with its history. Inspired by state augmentation in discrete-time systems, we thus obtain {\em neural delay differential equations}. Based on classical time delay stability analysis, we then show how to ensure stability of the learned models, and theoretically analyze our approach. Our experiments demonstrate its applicability to stable system identification of partially observed systems and learning a stabilizing feedback policy in delayed feedback control.","['Deep Learning', 'Machine Learning']",[],"['Andreas Schlaginhaufen', 'Philippe Wenk', 'Andreas Krause', 'Florian Dörfler']","['EPFL - EPF Lausanne', '', 'Swiss Federal Institute of Technology', 'Swiss Federal Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26535,Privacy & Data Governance,Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes,"Quantization is a popular technique that transforms the parameter representation of a neural network from floating-point numbers into lower-precision ones (e.g., 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in behavioral disparities between the model before and after quantization. For example, a quantized model can misclassify some test-time samples that are otherwise classified correctly. It is not known whether such differences lead to a new security vulnerability. We hypothesize that an adversary may control this disparity to introduce specific behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for significant accuracy loss; (ii) a targeted attack against specific samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compromised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor. Lastly, we discuss potential counter-measures and show that only re-training consistently removes the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation","['Federated Learning', 'Deep Learning', 'Adversarial Robustness and Security']",[],"['Sanghyun Hong', 'Michael-Andrei Panaitescu-Liess', 'Yigitcan Kaya', 'Tudor Dumitras']","['Oregon State University', 'University of Maryland, College Park', 'University of California, Santa Barbara', 'University of Maryland, College Park']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26512,Privacy & Data Governance,Subgame solving without common knowledge,"In imperfect-information games, subgame solving is significantly more challenging than in perfect-information games, but in the last few years, such techniques have been developed. They were the key ingredient to the milestone of superhuman play in no-limit Texas hold'em poker. Current subgame-solving techniques analyze the entire common-knowledge closure of the player's current information set, that is, the smallest set of nodes within which it is common knowledge that the current node lies. While this is acceptable in games like poker where the common-knowledge closure is relatively small, many practical games have more complex information structure, which renders the common-knowledge closure impractically large to enumerate or even reasonably approximate. We introduce an approach that overcomes this obstacle, by instead working with only low-order knowledge. Our approach allows an agent, upon arriving at an infoset, to basically prune any node that is no longer reachable, thereby massively reducing the game tree size relative to the common-knowledge subgame. We prove that, as is, our approach can increase exploitability compared to the blueprint strategy. However, we develop three avenues by which safety can be guaranteed. First, safety is guaranteed if the results of subgame solves are incorporated back into the blueprint. Second, we provide a method where safety is achieved by limiting the infosets at which subgame solving is performed. Third, we prove that our approach, when applied at every infoset reached during play, achieves a weaker notion of equilibrium, which we coin affine equilibrium, and which may be of independent interest. We show that affine equilibria cannot be exploited by any Nash strategy of the opponent, so an opponent who wishes to exploit must open herself to counter-exploitation. Even without the safety-guaranteeing additions, experiments on medium-sized games show that our approach always reduced exploitability in practical games even when applied at every infoset, and a depth-limited version of it led to---to our knowledge---the first strong AI for the challenge problem dark chess.",[],[],"['Brian Hu Zhang', 'Tuomas Sandholm']","['Carnegie Mellon University', 'Carnegie Mellon University']","[None, None]"
https://nips.cc/virtual/2021/poster/26508,Privacy & Data Governance,Adversarial Examples Make Strong Poisons,"The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data.  In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. In fact, adversarial examples with labels re-assigned by the crafting network remain effective for training, suggesting that adversarial examples contain useful semantic content, just with the ""wrong"" labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.","['Adversarial Robustness and Security', 'Robustness', 'Machine Learning']",[],"['Liam H Fowl', 'Micah Goldblum', 'Ping-yeh Chiang', 'Jonas Geiping', 'Tom Goldstein']","['Google', 'New York University', 'University of Maryland, College Park', 'ELLIS Institute Tübingen', 'University of Maryland, College Park']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26489,Privacy & Data Governance,Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds,"Certified robustness is a desirable property for deep neural networks in safety-critical applications, and popular training algorithms can certify robustness of a neural network by computing a global bound on its Lipschitz constant. However, such a bound is often loose: it tends to over-regularize the neural network and degrade its natural accuracy. A tighter Lipschitz bound may provide a better tradeoff between natural and certified accuracy, but is generally hard to compute exactly due to non-convexity of the network. In this work, we propose an efficient and trainable \emph{local} Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, we eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global Lipschitz constant of the neural network. Our method can be used as a plug-in module to tighten the Lipschitz bound in many certifiable training algorithms. Furthermore, we propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local Lipschitz bound. Experimentally, we show that our method consistently outperforms state-of-the-art methods in both clean and certified accuracy on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Yujia Huang', 'Huan Zhang', 'Yuanyuan Shi', 'J Zico Kolter', 'Anima Anandkumar']","['California Institute of Technology', 'University of Illinois at Urbana-Champaign', 'University of California, San Diego', 'Carnegie Mellon University', 'California Institute of Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26465,Privacy & Data Governance,Novel Upper Bounds for the Constrained Most Probable Explanation Task,"We propose several schemes for upper bounding the optimal value of the constrained most probable explanation (CMPE) problem. Given a set of discrete random variables, two probabilistic graphical models defined over them and a real number $q$, this problem involves finding an assignment of values to all the variables such that the probability of the assignment is maximized according to the first model and is bounded by $q$ w.r.t. the second model. In prior work, it was shown that CMPE is a unifying problem with several applications and special cases including the nearest assignment problem, the decision preserving most probable explanation task and robust estimation. It was also shown that CMPE is NP-hard even on tractable models such as bounded treewidth networks and is hard for integer linear programming methods because it includes a dense global constraint. The main idea in our approach is to simplify the problem via Lagrange relaxation and decomposition to yield either a knapsack problem or the unconstrained most probable explanation (MPE) problem, and then solving the two problems, respectively using specialized knapsack algorithms and mini-buckets based upper bounding schemes. We evaluate our proposed scheme along several dimensions including quality of the bounds and computation time required on various benchmark graphical models and how it can be used to find heuristic, near-optimal feasible solutions in an example application pertaining to robust estimation and adversarial attacks on classifiers.","['Interpretability', 'Graph Learning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Tahrima Rahman', 'Sara Rouhani', 'Vibhav Giridhar Gogate']","['University of Texas, Dallas', 'University of Texas, Dallas', 'University of Texas at Dallas']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26458,Privacy & Data Governance,Robustness of Graph Neural Networks at Scale,"Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.","['Optimization', 'Robustness', 'Graph Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Simon Geisler', 'Tobias Schmidt', 'Hakan Şirin', 'Daniel Zügner', 'Aleksandar Bojchevski', 'Stephan Günnemann']","['Technical University Munich', 'Technical University Munich', 'Technical University Munich', 'Microsoft', 'Universität Köln', 'Technical University Munich']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26391,Privacy & Data Governance,Drawing Robust Scratch Tickets: Subnetworks with Inborn Robustness Are Found within Randomly Initialized Networks,"Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks, i.e., an imperceptible perturbation to the input can mislead DNNs trained on clean images into making erroneous predictions. To tackle this, adversarial training is currently the most effective defense method, by augmenting the training set with adversarial samples generated on the fly. \textbf{Interestingly, we discover for the first time that there exist subnetworks with inborn robustness, matching or surpassing the robust accuracy of the adversarially trained networks with comparable model sizes, within randomly initialized networks without any model training}, indicating that adversarial training on model weights is not indispensable towards adversarial robustness. We name such subnetworks Robust Scratch Tickets (RSTs), which are also by nature efficient. Distinct from the popular lottery ticket hypothesis, neither the original dense networks nor the identified RSTs need to be trained. To validate and understand this fascinating finding, we further conduct extensive experiments to study the existence and properties of RSTs under different models, datasets, sparsity patterns, and attacks, drawing insights regarding the relationship between DNNs’ robustness and their initialization/overparameterization. Furthermore, we identify the poor adversarial transferability between RSTs of different sparsity ratios drawn from the same randomly initialized dense network, and propose a Random RST Switch (R2S) technique, which randomly switches between different RSTs, as a novel defense method built on top of RSTs. We believe our findings about RSTs have opened up a new perspective to study model robustness and extend the lottery ticket hypothesis.","['Adversarial Robustness and Security', 'Deep Learning', 'Robustness']",[],"['Yonggan Fu', 'Qixuan Yu', 'Yang Zhang', 'Shang Wu', 'Xu Ouyang', 'David Daniel Cox', 'Yingyan Lin']","['Institute of Technology', 'Rice University', 'International Business Machines', 'Northwestern University, Northwestern University', 'University of Virginia, Charlottesville', 'International Business Machines', 'Institute of Technology']","['Georgia', None, None, None, None, None, 'Georgia']"
https://nips.cc/virtual/2021/poster/26325,Privacy & Data Governance,Optimal Best-Arm Identification Methods for Tail-Risk Measures,"Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular tail-risk measures in finance and insurance industries as well as in highly reliable, safety-critical uncertain environments where often the underlying probability distributions are heavy-tailed. We use the multi-armed bandit best-arm identification framework and consider the problem of identifying the arm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum of CVaR and mean. The latter captures the risk-return trade-off common in finance. Our main contribution is an optimal $\delta$-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as $ \delta$ approaches $0$). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. En-route, we develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures.","['Reinforcement Learning and Planning', 'Optimization', 'Bandits']",[],"['Shubhada Agrawal', 'Wouter M Koolen', 'Sandeep Kumar Juneja']","['Institute of Technology', 'Centrum voor Wiskunde en Informatica', 'Tata Institute of Fundamental Research']","['Georgia', None, None]"
https://nips.cc/virtual/2021/poster/26329,Privacy & Data Governance,CAFE: Catastrophic Data Leakage in Vertical Federated Learning,"Recent studies show that private training data can be leaked through the gradients sharing mechanism deployed in distributed machine learning systems, such as federated learning (FL). Increasing batch size to complicate data recovery is often viewed as a promising defense strategy against data leakage. In this paper, we revisit this defense premise and propose an advanced data leakage attack with theoretical justification to efficiently recover batch data from the shared aggregated gradients. We name our proposed method as catastrophic data leakage in vertical federated learning (CAFE). Comparing to existing data leakage attacks, our extensive experimental results on vertical FL settings demonstrate the effectiveness of CAFE to perform large-batch data leakage attack with improved data recovery quality. We also propose a practical countermeasure to mitigate CAFE. Our results suggest that private data participated in standard FL, especially the vertical case, have a high risk of being leaked from the training gradients. Our analysis implies unprecedented and practical data leakage risks in those learning settings. The code of our work is available at https://github.com/DeRafael/CAFE.","['Federated Learning', 'Machine Learning', 'Privacy']",[],"['Xiao Jin', 'Pin-Yu Chen', 'Chia-Yi Hsu', 'Chia-Mu Yu', 'Tianyi Chen']","['Rensselaer Polytechnic Institute', 'International Business Machines', 'National Chiao Tung University', 'National Yang Ming Chiao Tung University', 'Rensselaer Polytechnic Institute']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26328,Privacy & Data Governance,Topological Detection of Trojaned Neural Networks,"Deep neural networks are known to have security issues. One particular threat is the Trojan attack. It occurs when the attackers stealthily manipulate the model's behavior through Trojaned training samples, which can later be exploited. Guided by basic neuroscientific principles, we discover subtle -- yet critical -- structural deviation characterizing Trojaned models. In our analysis we use topological tools. They allow us to model high-order dependencies in the networks, robustly compare different networks, and localize structural abnormalities. One interesting observation is that Trojaned models develop short-cuts from shallow to deep layers. Inspired by these observations, we devise a strategy for robust detection of Trojaned models. Compared to standard baselines it displays better performance on multiple benchmarks.",['Deep Learning'],[],"['Songzhu Zheng', 'Hubert Wagner', 'Chao Chen']","['Morgan Stanley', 'Institute of Science and Technology', 'State University of New York at Stony Brook']","[None, 'Austria', None]"
https://nips.cc/virtual/2021/poster/26314,Privacy & Data Governance,Adversarial Robustness without Adversarial Training: A Teacher-Guided Curriculum Learning Approach,"Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.","['Adversarial Robustness and Security', 'Robustness']",[],"['Anindya Sarkar', 'Anirban Sarkar', 'Sowrya Gali', 'Vineeth N. Balasubramanian']","['Washington University, Saint Louis', 'Cold Spring Harbor Laboratory', 'n Institute of Technology Hyderabad', 'n Institute of Technology Hyderabad']","[None, None, 'India', 'India']"
https://nips.cc/virtual/2021/poster/26297,Privacy & Data Governance,On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms,"Zeroth-order (ZO) optimization is widely used to handle challenging tasks, such as query-based black-box adversarial attacks and reinforcement learning. Various attempts have been made to integrate prior information into the gradient estimation procedure based on finite differences, with promising empirical results. However, their convergence properties are not well understood. This paper makes an attempt to fill up this gap by analyzing the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. We provide a convergence guarantee for the prior-guided random gradient-free (PRGF) algorithms. Moreover, to further accelerate over greedy descent methods, we present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. Finally, our theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks.","['Reinforcement Learning and Planning', 'Optimization', 'Adversarial Robustness and Security']",[],"['Shuyu Cheng', 'Guoqiang Wu', 'Jun Zhu']","['Tsinghua University, Tsinghua University', 'Shandong University', 'Tsinghua University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26270,Privacy & Data Governance,Towards Efficient and Effective Adversarial Training,"The vulnerability of Deep Neural Networks to adversarial attacks has spurred immense interest towards improving their robustness. However, present state-of-the-art adversarial defenses involve the use of 10-step adversaries during training, which renders them computationally infeasible for application to large-scale datasets. While the recent single-step defenses show promising direction, their robustness is not on par with multi-step training methods. In this work, we bridge this performance gap by introducing a novel Nuclear-Norm regularizer on network predictions to enforce function smoothing in the vicinity of data samples.  While prior works consider each data sample independently, the proposed regularizer uses the joint statistics of adversarial samples across a training minibatch to enhance optimization during both attack generation and training, obtaining state-of-the-art results amongst efficient defenses. We achieve further gains by incorporating exponential averaging of network weights over training iterations. We finally introduce a Hybrid training approach that combines the effectiveness of a two-step variant of the proposed defense with the efficiency of a single-step defense. We demonstrate superior results when compared to multi-step defenses such as TRADES and PGD-AT as well, at a significantly lower computational cost.","['Adversarial Robustness and Security', 'Deep Learning', 'Optimization', 'Robustness']",[],"['Gaurang Sriramanan', 'Sravanti Addepalli', 'Arya Baburaj', 'Venkatesh Babu Radhakrishnan']","['University of Maryland, College Park', 'Google', 'n Institute of Science, Bangalore', 'n Institute of Science']","[None, None, 'India', 'India']"
https://nips.cc/virtual/2021/poster/26272,Privacy & Data Governance,Revisiting 3D Object Detection From an Egocentric Perspective,"3D object detection is a key module for safety-critical robotics applications such as autonomous driving. For these applications, we care most about how the detections affect the ego-agent’s behavior and safety (the egocentric perspective). Intuitively, we seek more accurate descriptions of object geometry when it’s more likely to interfere with the ego-agent’s motion trajectory. However, current detection metrics, based on box Intersection-over-Union (IoU), are object-centric and aren’t designed to capture the spatio-temporal relationship between objects and the ego-agent. To address this issue, we propose a new egocentric measure to evaluate 3D object detection,  namely Support Distance Error (SDE). Our analysis based on SDE reveals that the egocentric detection quality is bounded by the coarse geometry of the bounding boxes. Given the insight that SDE would benefit from more accurate geometry descriptions, we propose to represent objects as amodal contours, specifically amodal star-shaped polygons, and devise a simple model, StarPoly, to predict such contours. Our experiments on the large-scale Waymo Open Dataset show that SDE better reflects the impact of detection quality on the ego-agent’s safety compared to IoU; and the estimated contours from StarPoly consistently improve the egocentric detection quality over recent 3D object detectors.",['Vision'],[],"['Boyang Deng', 'Charles R. Qi', 'Mahyar Najibi', 'Thomas Funkhouser', 'Yin Zhou', 'Dragomir Anguelov']","['Stanford University', 'Waymo', 'Apple', 'Princeton University', 'Waymo', 'Waymo']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26087,Privacy & Data Governance,On Success and Simplicity: A Second Look at Transferable Targeted Attacks,"Achieving transferability of targeted attacks is reputed to be remarkably difficult. The current state of the art has resorted to resource-intensive solutions that necessitate training model(s) for each target class with additional data. In our investigation, we find, however, that simple transferable attacks which require neither model training nor additional data can achieve surprisingly strong targeted transferability. This insight has been overlooked until now, mainly because the widespread practice of attacking with only few iterations has largely limited the attack convergence to optimal targeted transferability. In particular, we, for the first time, identify that a very simple logit loss can largely surpass the commonly adopted cross-entropy loss, and yield even better results than the resource-intensive state of the art. Our analysis spans a variety of transfer scenarios, especially including three new, realistic scenarios: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API. Results in these new transfer scenarios demonstrate that the commonly adopted, easy scenarios cannot fully reveal the actual strength of different attacks and may cause misleading comparative results. We also show the usefulness of the simple logit loss for generating targeted universal adversarial perturbations in a data-free manner. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability. Code is available at https://github.com/ZhengyuZhao/Targeted-Tansfer.",['Adversarial Robustness and Security'],[],"['Zhengyu Zhao', 'Zhuoran Liu', 'Martha Larson']","[""Xi'an Jiaotong University"", 'Radboud University', 'Radboud University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28350,Privacy & Data Governance,Variational Model Inversion Attacks,"Given the ubiquity of deep neural networks, it is important that these models do not reveal information about sensitive data that they have been trained on. In model inversion attacks, a malicious user attempts to recover the private dataset used to train a supervised neural network. A successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset. In this work, we provide a probabilistic interpretation of model inversion attacks, and formulate a variational objective that accounts for both diversity and accuracy. In order to optimize this variational objective, we choose a variational family defined in the code space of a deep generative model, trained on a public auxiliary dataset that shares some structural similarity with the target dataset.  Empirically, our method substantially improves performance in terms of target attack accuracy, sample realism, and diversity on datasets of faces and chest X-ray images.","['Deep Learning', 'Generative Model']",[],"['Kuan-Chieh Wang', 'YAN FU', 'Ke Li', 'Ashish J Khisti', 'Richard Zemel', 'Alireza Makhzani']","['Stanford University', 'Toronto University', 'Simon Fraser University', 'Toronto University', 'Department of Computer Science, Columbia University', 'Vector Institute']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26067,Privacy & Data Governance,A PAC-Bayes Analysis of Adversarial Robustness,"We propose the first general PAC-Bayesian generalization bounds for adversarial robustness, that estimate, at test time, how much a model will be invariant to imperceptible perturbations in the input. Instead of deriving a worst-case analysis of the risk of a hypothesis over all the possible perturbations, we leverage the PAC-Bayesian framework to bound the averaged risk on the perturbations for majority votes (over the whole class of hypotheses). Our theoretically founded analysis has the advantage to provide general bounds (i) that are valid for any kind of attacks (i.e., the adversarial attacks), (ii) that are tight thanks to the PAC-Bayesian framework, (iii) that can be directly minimized during the learning phase to obtain a robust model on different attacks at test time.","['Adversarial Robustness and Security', 'Robustness']",[],"['Paul Viallard', 'Guillaume Eric VIDOT', 'Amaury Habrard', 'Emilie Morvant']","['INRIA Paris', 'IRIT', 'Université Saint-Etienne, Laboratoire Hubert Curien', 'University Jean Monnet']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/25987,Privacy & Data Governance,Adversarial Attacks on Graph Classifiers via Bayesian Optimisation,"Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.","['Graph Learning', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Xingchen Wan', 'Henry Kenlay', 'Binxin Ru', 'Arno Blaas', 'Michael Osborne', 'Xiaowen Dong']","['Google', 'Exscientia', 'University of Oxford', 'Apple', 'University of Oxford', 'University of Oxford']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/25959,Privacy & Data Governance,Adversarial Feature Desensitization,"Neural networks are known to be vulnerable to adversarial attacks -- slight but carefully constructed perturbations of the inputs which can drastically impair the network's performance. Many defense methods have been proposed for improving robustness of  deep networks by training them on adversarially perturbed inputs. However, these models often remain vulnerable to new types of attacks not seen during training, and even to slightly stronger versions of previously seen  attacks. In this work, we propose a novel approach to  adversarial robustness, which builds upon the insights from the domain adaptation field. Our method, called Adversarial Feature Desensitization (AFD), aims at learning  features that are invariant towards adversarial perturbations of the inputs. This is achieved through a game where we learn features that are both predictive and robust (insensitive to adversarial attacks), i.e. cannot be used to discriminate between natural and adversarial data. Empirical results on several benchmarks  demonstrate the effectiveness of the proposed approach against a wide range of attack types and attack strengths. Our code is available at https://github.com/BashivanLab/afd.","['Adversarial Robustness and Security', 'Deep Learning', 'Domain Adaptation', 'Robustness']",[],"['Pouya Bashivan', 'Reza Bayat', 'Adam Ibrahim', 'Kartik Ahuja', 'Mojtaba Faramarzi', 'Touraj Laleh', 'Blake Aaron Richards', 'Irina Rish']","['McGill University', 'Université de Montréal', 'University of Montreal', 'FAIR (Meta)', 'Mila', 'Montreal Institute for Learning Algorithms, University of Montreal, University of Montreal', 'McGill University', 'University of Montreal']","[None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27644,Privacy & Data Governance,Learning Transferable Adversarial Perturbations,"While effective, deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, recent work has shown that such attacks could be generated by another deep network, leading to significant speedups over optimization-based perturbations. However, the ability of such generative methods to generalize to different test-time situations has not been systematically studied. In this paper, we, therefore, investigate the transferability of generated perturbations when the conditions at inference time differ from the training ones in terms of the target architecture, target data, and target task. Specifically, we identify the mid-level features extracted by the intermediate layers of DNNs as common ground across different architectures, datasets, and tasks. This lets us introduce a loss function based on such mid-level features to learn an effective, transferable perturbation generator. Our experiments demonstrate that our approach outperforms the state-of-the-art universal and transferable attack strategies.","['Adversarial Robustness and Security', 'Deep Learning', 'Optimization']",[],"['Krishna kanth Nakka', 'Mathieu Salzmann']","['Huawei Technologies Ltd.', 'Swiss Federal Institute of Technology Lausanne']","[None, None]"
https://nips.cc/virtual/2021/poster/27128,Privacy & Data Governance,ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers,"Adversarial patch attacks that craft the pixels in a confined region of the input images show their powerful attack effectiveness in physical environments even with noises or deformations. Existing certified defenses towards adversarial patch attacks work well on small images like MNIST and CIFAR-10 datasets, but achieve very poor certified accuracy on higher-resolution images like ImageNet. It is urgent to design both robust and effective defenses against such a practical and harmful attack in industry-level larger images. In this work, we propose the certified defense methodology that achieves high provable robustness for high-resolution images and largely improves the practicality for real adoption of the certified defense. The basic insight of our work is that the adversarial patch intends to leverage localized superficial important neurons (SIN) to manipulate the prediction results. Hence, we leverage the SIN-based DNN compression techniques to significantly improve the certified accuracy, by reducing the adversarial region searching overhead and filtering the prediction noises. Our experimental results show that the certified accuracy is increased from 36.3%  (the state-of-the-art certified detection)  to 60.4%on the ImageNet dataset, largely pushing the certified defenses for practical use.",['Robustness'],[],"['Husheng Han', 'Kaidi Xu', 'Xing Hu', 'Xiaobing Chen', 'Ling Liang', 'Zidong Du', 'Qi Guo', 'Yanzhi Wang', 'Yunji Chen']","['Institude of Computer Technology, Chinese Academy of Sciences', 'Drexel University', ', Chinese Academy of Sciences', 'ict, cas', 'University of California, Santa Barbara', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Northeastern University', 'Institute of Computing Technology, Chinese Academy of Sciences']","[None, None, None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26092,Privacy & Data Governance,How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?,"The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.","['Adversarial Robustness and Security', 'Robustness', 'Language']",[],"['Xinshuai Dong', 'Anh Tuan Luu', 'Min Lin', 'Shuicheng Yan', 'Hanwang Zhang']","['Carnegie Mellon University', 'Nanyang Technological University', 'Sea AI Lab', 'National University of', 'Nanyang Technological University']","[None, None, None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/27157,Privacy & Data Governance,Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis,"Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First,  we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.","['Privacy', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Generative Model']",[],"['JAEHOON LEE', 'Jihyeon Hyeong', 'Jinsung Jeon', 'Noseong Park', 'Jihoon Cho']","['LG AI RESEARCH', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Samsung SDS']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28047,Privacy & Data Governance,Adversarial Robustness of Streaming Algorithms through Importance Sampling,"Robustness against adversarial attacks has recently been at the forefront of algorithmic design for machine learning tasks. In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates $u_1,\ldots,u_n$ as a data stream. The goal of the algorithm is to compute or approximate some predetermined function for every prefix of the adversarial stream, but the adversary may generate future updates based on previous outputs of the algorithm. In particular, the adversary may gradually learn the random bits internally used by an algorithm to manipulate dependencies in the input. This is especially problematic as many important problems in the streaming model require randomized algorithms, as they are known to not admit any deterministic algorithms that use sublinear space. In this paper, we introduce adversarially robust streaming algorithms for central machine learning and algorithmic tasks, such as regression and clustering, as well as their more general counterparts, subspace embedding, low-rank approximation, and coreset construction. For regression and other numerical linear algebra related tasks, we consider the row arrival streaming model. Our results are based on a simple, but powerful, observation that many importance sampling-based algorithms give rise to adversarial robustness which is in contrast to sketching based algorithms, which are very prevalent in the streaming literature but suffer from adversarial attacks. In addition, we show that the well-known merge and reduce paradigm in streaming is adversarially robust. Since the merge and reduce paradigm allows coreset constructions in the streaming setting, we thus obtain robust algorithms for $k$-means, $k$-median, $k$-center, Bregman clustering, projective clustering, principal component analysis (PCA) and non-negative matrix factorization. To the best of our knowledge, these are the first adversarially robust results for these problems yet require no new algorithmic implementations. Finally, we empirically confirm the robustness of our algorithms on various adversarial attacks and demonstrate that by contrast, some common existing algorithms are not robust.","['Clustering', 'Robustness', 'Adversarial Robustness and Security', 'Machine Learning']",[],"['Vladimir Braverman', 'Mariano Schain', 'Sandeep Silwal', 'Samson Zhou']","['Rice University', 'Google', 'Massachusetts Institute of Technology', 'Texas A&M University - College Station']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27350,Privacy & Data Governance,Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks,"One major problem in black-box adversarial attacks is the high query complexity in the hard-label attack setting, where only the top-1 predicted label is available. In this paper, we propose a novel geometric-based approach called Tangent Attack (TA), which identifies an optimal tangent point of a virtual hemisphere located on the decision boundary to reduce the distortion of the attack. Assuming the decision boundary is locally flat, we theoretically prove that the minimum $\ell_2$ distortion can be obtained by reaching the decision boundary along the tangent line passing through such tangent point in each iteration. To improve the robustness of our method, we further propose a generalized method which replaces the hemisphere with a semi-ellipsoid to adapt to curved decision boundaries. Our approach is free of pre-training. Extensive experiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that our approach can consume only a small number of queries to achieve the low-magnitude distortion. The implementation source code is released online.","['Adversarial Robustness and Security', 'Robustness']",[],"['Chen Ma', 'Xiangyu Guo', 'Li Chen', 'Jun-Hai Yong', 'Yisen Wang']","['Zhejiang University of Technology', 'Meta Platforms, Inc', 'Tsinghua University, Tsinghua University', 'Tsinghua University, Tsinghua University', 'Peking University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26894,Privacy & Data Governance,Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs,"We address the issue of safety in reinforcement learning. We pose the problem in an episodic framework of a constrained Markov decision process. Existing results have shown that it is possible to achieve a reward regret of $\tilde{\mathcal{O}}(\sqrt{K})$ while allowing an $\tilde{\mathcal{O}}(\sqrt{K})$ constraint violation in $K$ episodes. A critical question that arises is whether it is possible to keep the constraint violation even smaller. We show that when a strictly safe policy is known, then one can confine the system to zero constraint violation with arbitrarily high probability while keeping the reward regret of order $\tilde{\mathcal{O}}(\sqrt{K})$. The algorithm which does so employs the principle of optimistic pessimism in the face of uncertainty to achieve safe exploration. When no strictly safe policy is known, though one is known to exist, then it is possible to restrict the system to bounded constraint violation with arbitrarily high probability. This is shown to be realized by a primal-dual algorithm with an optimistic primal estimate and a pessimistic dual update.",['Reinforcement Learning and Planning'],[],"['Tao Liu', 'Ruida Zhou', 'Dileep Kalathil', 'Panganamala Kumar']","['Texas A&M', 'University of California, Los Angeles', 'Texas A&M University', 'Texas A&M']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27461,Privacy & Data Governance,Bandit Quickest Changepoint Detection,"Many industrial and security applications employ a suite of sensors for detecting abrupt changes in temporal behavior patterns. These abrupt changes typically manifest locally, rendering only a small subset of sensors informative. Continuous monitoring of every sensor can be expensive due to resource constraints, and serves as a motivation for the bandit quickest changepoint detection problem, where sensing actions (or sensors) are sequentially chosen, and only measurements corresponding to chosen actions are observed. We derive an information-theoretic lower bound on the detection delay for a general class of finitely parameterized probability distributions. We then propose a computationally efficient online sensing scheme, which seamlessly balances the need for exploration of different sensing options with exploitation of querying informative actions. We derive expected delay bounds for the proposed scheme and show that these bounds match our information-theoretic lower bounds at low false alarm rates, establishing optimality of the proposed method. We then perform a number of experiments on synthetic and real datasets demonstrating the effectiveness of our proposed method.","['Reinforcement Learning and Planning', 'Bandits']",[],"['Aditya Gopalan', 'Braghadeesh Lakshminarayanan', 'Venkatesh Saligrama']","['n Institute of Science', 'KTH Royal Institute of Technology, Stockholm,', 'Amazon']","['India', 'Sweden', None]"
https://nips.cc/virtual/2021/poster/28216,Privacy & Data Governance,Infinite Time Horizon Safety of Bayesian Neural Networks,"Bayesian neural networks (BNNs) place distributions over the weights of a neural network to model uncertainty in the data and the network's prediction. We consider the problem of verifying safety when running a Bayesian neural network policy in a feedback loop with infinite time horizon systems. Compared to the existing sampling-based approaches, which are inapplicable to the infinite time horizon setting, we train a separate deterministic neural network that serves as an infinite time horizon safety certificate. In particular, we show that the certificate network guarantees the safety of the system over a subset of the BNN weight posterior's support. Our method first computes a safe weight set and then alters the BNN's weight posterior to reject samples outside this set. Moreover, we show how to extend our approach to a safe-exploration reinforcement learning setting, in order to avoid unsafe trajectories during the training of the policy. We evaluate our approach on a series of reinforcement learning benchmarks, including non-Lyapunovian safety specifications.","['Reinforcement Learning and Planning', 'Deep Learning']",[],"['Mathias Lechner', 'Đorđe Žikelić', 'Krishnendu Chatterjee', 'Thomas A Henzinger']","['Massachusetts Institute of Technology', 'Management University', 'Institute of Science and Technology', 'Institute of Science and Technology']","[None, 'Singapore', 'Austria', 'Austria']"
https://nips.cc/virtual/2021/poster/26692,Privacy & Data Governance,Deep Synoptic Monte-Carlo Planning in Reconnaissance Blind Chess,"This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large imperfect information games. The algorithm constructs a belief state with an unweighted particle filter and plans via playouts that start at samples drawn from the belief state. The algorithm accounts for uncertainty by performing inference on ""synopses,"" a novel stochastic abstraction of information states. DSMCP is the basis of the program Penumbra, which won the official 2020 reconnaissance blind chess competition versus 33 other programs. This paper also evaluates algorithm variants that incorporate caution, paranoia, and a novel bandit algorithm. Furthermore, it audits the synopsis features used in Penumbra with per-bit saliency statistics.","['Reinforcement Learning and Planning', 'Deep Learning', 'Bandits']",[],['Gregory Clark'],['Google'],[None]
https://nips.cc/virtual/2021/poster/26926,Privacy & Data Governance,Adversarially Robust 3D Point Cloud Recognition Using Self-Supervisions,"3D point cloud data is increasingly used in safety-critical applications such as autonomous driving. Thus, the robustness of 3D deep learning models against adversarial attacks becomes a major consideration. In this paper, we systematically study the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, we study MLP-based (PointNet), convolution-based (DGCNN), and transformer-based (PCT) 3D architectures. Through extensive experimentation, we demonstrate that appropriate applications of self-supervision can significantly enhance the robustness in 3D point cloud recognition, achieving considerable improvements compared to the standard adversarial training baseline. Our analysis reveals that local feature learning is desirable for adversarial robustness in point clouds since it limits the adversarial propagation between the point-level input perturbations and the model's final output. This insight also explains the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial robustness.","['Transformers', 'Robustness', 'Self-Supervised Learning', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Jiachen Sun', 'Yulong Cao', 'Christopher Choy', 'Zhiding Yu', 'Anima Anandkumar', 'Zhuoqing Mao', 'Chaowei Xiao']","['University of Michigan', 'NVIDIA', 'NVIDIA', 'NVIDIA', 'California Institute of Technology', 'University of Michigan', 'University of Wisconsin - Madison']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28031,Privacy & Data Governance,Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning,"Unsupervised domain adaptation (UDA) enables cross-domain learning without target domain labels by transferring knowledge from a labeled source domain whose distribution differs from that of the target. However, UDA is not always successful and several accounts of `negative transfer' have been reported in the literature. In this work, we prove a simple lower bound on the target domain error that complements the existing upper bound. Our bound shows the insufficiency of minimizing source domain error and marginal distribution mismatch for a guaranteed reduction in the target domain error, due to the possible increase of induced labeling function mismatch. This insufficiency is further illustrated through simple distributions for which the same UDA approach succeeds, fails, and may succeed or fail with an equal chance. Motivated from this, we propose novel data poisoning attacks to fool UDA methods into learning representations that produce large target domain errors. We evaluate the effect of these attacks on popular UDA methods using benchmark datasets where they have been previously shown to be successful. Our results show that poisoning can significantly decrease the target domain accuracy, dropping it to almost 0% in some cases, with the addition of only 10% poisoned data in the source domain. The failure of these UDA methods demonstrates their limitations at guaranteeing cross-domain generalization consistent with our lower bound. Thus, evaluating UDA methods in adversarial settings such as data poisoning provides a better sense of their robustness to data distributions unfavorable for UDA.","['Robustness', 'Domain Adaptation']",[],"['Akshay Mehra', 'Bhavya Kailkhura', 'Pin-Yu Chen', 'Jihun Hamm']","['Tulane University', 'Lawrence Livermore National Laboratory', 'International Business Machines', 'Ohio State University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/27159,Privacy & Data Governance,Formalizing Generalization and Adversarial Robustness of Neural Networks to Weight Perturbations,"Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.","['Robustness', 'Machine Learning', 'Theory', 'Adversarial Robustness and Security', 'Deep Learning']",[],"['Yu-Lin Tsai', 'Chia-Yi Hsu', 'Chia-Mu Yu', 'Pin-Yu Chen']","['National Yang Ming Chiao Tung University', 'National Chiao Tung University', 'National Yang Ming Chiao Tung University', 'International Business Machines']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26715,Privacy & Data Governance,Observation-Free Attacks on Stochastic Bandits,"We study data corruption attacks on stochastic multi arm bandit algorithms. Existing attack methodologies assume that the attacker can observe the multi arm bandit algorithm's realized behavior which is in contrast to the adversaries modeled in the robust multi arm bandit algorithms literature. To the best of our knowledge, we develop the first data corruption attack on stochastic multi arm bandit algorithms which works without observing the algorithm's realized behavior. Through this attack, we also discover a sufficient condition for a stochastic multi arm bandit algorithm to be susceptible to adversarial data corruptions. We show that any bandit algorithm that makes decisions just using the empirical mean reward, and the number of times that arm has been pulled in the past can suffer from linear regret under data corruption attacks. We further show that various popular stochastic multi arm bandit algorithms such UCB, $\epsilon$-greedy and Thompson Sampling satisfy this sufficient condition and are thus prone to data corruption attacks. We further analyze the behavior of our attack for these algorithms and show that using only $o(T)$ corruptions, our attack can force these algorithms to select a potentially non-optimal target arm preferred by the attacker for all but $o(T)$ rounds.",['Bandits'],[],"['Yinglun Xu', 'Bhuvesh Kumar', 'Jacob Abernethy']","['University of Illinois, Urbana Champaign', 'TikTok Inc', 'Research, Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26965,Privacy & Data Governance,Learning Optimal Predictive Checklists,"Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.","['Fairness', 'Optimization', 'Interpretability', 'Machine Learning']",[],"['Haoran Zhang', 'Quaid Morris', 'Berk Ustun', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Memorial Sloan Kettering Cancer Centre', 'University of California, San Diego', 'Massachusetts Institute of Technology']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28592,Security,Instance-optimal Mean Estimation Under Differential Privacy,"Mean estimation under differential privacy is a fundamental problem, but worst-case optimal mechanisms do not offer meaningful utility guarantees in practice when the global sensitivity is very large.  Instead, various heuristics have been proposed to reduce the error on real-world data that do not resemble the worst-case instance.  This paper takes a principled approach, yielding a mechanism that is instance-optimal in a strong sense.  In addition to its theoretical optimality, the mechanism is also simple and practical, and adapts to a variety of data characteristics without the need of parameter tuning.  It easily extends to the local and shuffle model as well.",['Privacy'],[],"['Ziyue Huang', 'Yuting Liang', 'Ke Yi']","['Tencent', 'University of Science and Technology', 'The  University of Science and Technology']","[None, 'Hong Kong', 'Hong Kong']"
https://nips.cc/virtual/2021/poster/27635,Security,Parameter-free HE-friendly Logistic Regression,"Privacy in machine learning has been widely recognized as an essential ethical and legal issue, because the data used for machine learning may contain sensitive information. Homomorphic encryption has recently attracted attention as a key solution to preserve privacy in machine learning applications. However, current approaches on the training of encrypted machine learning have relied heavily on hyperparameter selection, which should be avoided owing to the extreme difficulty of conducting validation on encrypted data. In this study, we propose an effective privacy-preserving logistic regression method that is free from the approximation of the sigmoid function and hyperparameter selection. In our framework, a logistic regression model can be transformed into the corresponding ridge regression for the logit function. We provide a theoretical background for our framework by suggesting a new generalization error bound on the encrypted data. Experiments on various real-world data show that our framework achieves better classification results while reducing latency by $\sim68\%$, compared to the previous models.","['Machine Learning', 'Privacy']",[],"['Junyoung Byun', 'Woojin Lee', 'Jaewook Lee']","['Seoul National University', 'Seoul National University', 'Seoul National University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26488,Security,Privately Publishable Per-instance Privacy,"We consider how to privately share the personalized privacy losses incurred by objective perturbation, using per-instance differential privacy (pDP). Standard differential privacy (DP) gives us a worst-case bound that might be orders of magnitude larger than the privacy loss to a particular individual relative to a fixed dataset. The pDP framework provides a more fine-grained analysis of the privacy guarantee to a target individual, but the per-instance privacy loss itself might be a function of sensitive data. In this paper, we analyze the per-instance privacy loss of releasing a private empirical risk minimizer learned via objective perturbation, and propose a group of methods to privately and accurately publish the pDP losses at little to no additional privacy cost.",['Privacy'],[],"['Rachel Emily Redberg', 'Yu-Xiang Wang']","['UC Santa Barbara', 'UC Santa Barbara']","[None, None]"
https://nips.cc/virtual/2021/poster/27315,Security,Analyzing the Confidentiality of Undistillable Teachers in Knowledge Distillation,"Knowledge distillation (KD) has recently been identified as a method that can unintentionally leak private information regarding the details of a teacher model to an unauthorized student. Recent research in developing undistillable nasty teachers that can protect model confidentiality has gained significant attention. However, the level of protection these nasty models offer has been largely untested. In this paper, we show that transferring knowledge to a shallow sub-section of a student can largely reduce a teacher’s influence. By exploring the depth of the shallow subsection, we then present a distillation technique that enables a skeptical student model to learn even from a nasty teacher. To evaluate the efficacy of our skeptical students, we conducted experiments with several models with KD on both training data-available and data-free scenarios for various datasets. While distilling from nasty teachers, compared to the normal student models, skeptical students consistently provide superior classification performance of up to ∼59.5%. Moreover, similar to normal students, skeptical students maintain high classification accuracy when distilled from a normal teacher, showing their efficacy irrespective of the teacher being nasty or not. We believe the ability of skeptical students to largely diminish the KD-immunity of potentially nasty teachers will motivate the research community to create more robust mechanisms for model confidentiality. We have open-sourced the code at https://github.com/ksouvik52/Skeptical2021","['Machine Learning', 'Privacy']",[],"['Souvik Kundu', 'Qirui Sun', 'Yao Fu', 'Massoud Pedram', 'Peter Anthony Beerel']","['Intel', 'City University of', 'University of Southern California', 'University of Southern California', 'University of Southern California']","[None, 'Hong Kong', None, None, None]"
https://nips.cc/virtual/2021/poster/28786,Security,Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms,"Implementations of the exponential mechanism in differential privacy often require sampling from intractable distributions. When approximate procedures like Markov chain Monte Carlo (MCMC) are used, the end result incurs costs to both privacy and accuracy. Existing work has examined these effects asymptotically, but implementable finite sample results are needed in practice so that users can specify privacy budgets in advance and implement samplers with exact privacy guarantees. In this paper, we use tools from ergodic theory and perfect simulation to design exact finite runtime sampling algorithms for the exponential mechanism by introducing an intermediate modified target distribution using artificial atoms. We propose an additional modification of this sampling algorithm that maintains its $\epsilon$-DP guarantee and has improved runtime at the cost of some utility. We then compare these methods in scenarios where we can explicitly calculate a $\delta$ cost (as in $(\epsilon, \delta)$-DP) incurred when using standard MCMC techniques. Much as there is a well known trade-off between privacy and utility, we demonstrate that there is also a trade-off between privacy guarantees and runtime.","['Theory', 'Generative Model', 'Privacy']",[],"['Jeremy Seeman', 'Matthew Reimherr']","['Pennsylvania State University', 'Pennsylvania State University']","[None, None]"
https://nips.cc/virtual/2021/poster/28755,Security,Privately Learning Mixtures of Axis-Aligned Gaussians,"We consider the problem of learning multivariate Gaussians under the constraint of approximate differential privacy. We prove that $\widetilde{O}(k^2 d \log^{3/2}(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\mathbb{R}^d$ to within total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-differential privacy. This is the first result for privately learning mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If the covariance matrices of each of the Gaussians is the identity matrix, we show that $\widetilde{O}(kd/\alpha^2 + kd \log(1/\delta) / \alpha \varepsilon)$ samples are sufficient. To prove our results, we design a new technique for privately learning mixture distributions.  A class of distributions $\mathcal{F}$ is said to be list-decodable if there is an algorithm that, given ""heavily corrupted"" samples from $f \in \mathcal{F}$, outputs a list of distributions one of which approximates $f$. We show that if $\mathcal{F}$ is privately list-decodable then we can learn mixtures of distributions in $\mathcal{F}$. Finally, we show axis-aligned Gaussian distributions are privately list-decodable, thereby proving mixtures of such distributions are privately learnable.",['Privacy'],[],"['Ishaq Aden-Ali', 'Hassan Ashtiani', 'Christopher Liaw']","['University of California, Berkeley', 'McMaster University', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28666,Security,Exploiting Data Sparsity in Secure Cross-Platform Social Recommendation,"Social recommendation has shown promising improvements over traditional systems since it leverages social correlation data as an additional input. Most existing work assumes that all data are available to the recommendation platform. However, in practice, user-item interaction data (e.g.,rating) and user-user social data are usually generated by different platforms, and both of which contain sensitive information.  Therefore, ""How to perform secure and efficient social recommendation across different platforms, where the data are highly-sparse in nature"" remains an important challenge. In this work, we bring secure computation techniques into social recommendation, and propose S3Rec, a sparsity-aware secure cross-platform social recommendation framework. As a result, our model can not only improve the recommendation performance of the rating platform by incorporating the sparse social data on the social platform, but also protect data privacy of both platforms. Moreover, to further improve model training efficiency, we propose two secure sparse matrix multiplication protocols based on homomorphic encryption and private information retrieval. Our experiments on two benchmark datasets demonstrate the effectiveness of S3Rec.","['Machine Learning', 'Privacy']",[],"['Jamie Cui', 'Chaochao Chen', 'Lingjuan Lyu', 'Carl Yang', 'Wang Li']","['Ant Group', 'Zhejiang University', 'Sony Research', 'Emory University', 'Shanghai Jiao Tong University, Tsinghua University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28241,Security,Differentially Private n-gram Extraction,"We revisit the problem of $n$-gram extraction in the differential privacy setting. In this problem, given a corpus of private text data, the goal is to release as many $n$-grams as possible while preserving user level privacy. Extracting $n$-grams is a fundamental subroutine in many NLP applications such as sentence completion, auto response generation for emails, etc. The problem also arises in other applications such as sequence mining, trajectory analysis, etc., and is a generalization of recently studied differentially private set union (DPSU) by Gopi et al. (2020). In this paper, we develop a new differentially private algorithm for this problem which, in our experiments, significantly outperforms the state-of-the-art. Our improvements stem from combining recent advances in DPSU, privacy accounting, and new heuristics for pruning in the tree-based approach initiated by Chen et al. (2012).",['Privacy'],[],"['Kunho Kim', 'Sivakanth Gopi', 'Janardhan Kulkarni', 'Sergey Yekhanin']","['Microsoft', 'Microsoft Research', 'Microsoft', 'Microsoft']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/28151,Security,Photonic Differential Privacy with Direct Feedback Alignment,"Optical Processing Units (OPUs) -- low-power photonic chips dedicated to large scale random projections -- have been used in previous work to train deep neural networks using Direct Feedback Alignment (DFA), an effective alternative to backpropagation. Here, we demonstrate how to leverage the intrinsic noise of optical random projections to build a differentially private DFA mechanism, making OPUs a solution of choice to provide a \emph{private-by-design} training. We provide a theoretical analysis of our adaptive privacy mechanism, carefully measuring how the noise of optical random projections propagates in the process and gives rise to provable Differential Privacy. Finally, we conduct experiments demonstrating the ability of our learning procedure to achieve solid end-task performance.","['Deep Learning', 'Privacy']",[],"['Ruben Ohana', 'Hamlet Jesse Medina Ruiz', 'Julien Launay', 'Alessandro Cappelli', 'Iacopo Poli', 'Liva Ralaivola']","['Flatiron Institute', 'Criteo', 'HuggingFace', 'huggingface', 'LightOn', 'Criteo']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27916,Security,Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods,"We study private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries. We first present an algorithmic framework that unifies a long line of iterative algorithms in the literature. Under this framework, we propose two new methods. The first method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Our second method, generative networks with the exponential mechanism (GEM), circumvents computational bottlenecks in algorithms such as MWEM and PEP by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. We demonstrate that PEP and GEM empirically outperform existing algorithms. Furthermore, we show that GEM nicely incorporates prior information from public data while overcoming limitations of PMW^Pub, the existing state-of-the-art method that also leverages public data.","['Optimization', 'Privacy', 'Machine Learning', 'Deep Learning', 'Generative Model']",[],"['Terrance Liu', 'Giuseppe Vietri', 'Steven Wu']","['Carnegie Mellon University', 'University of Minnesota, Minneapolis', 'Carnegie Mellon University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27850,Security,Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data,"Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.","['Domain Adaptation', 'Privacy', 'Transfer Learning', 'Machine Learning', 'Contrastive Learning']",[],"['Jiaxing Huang', 'Aoran Xiao', 'Shijian Lu']","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27840,Security,G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators,"Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator.  Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($\varepsilon \le 1$). Our code is available at https://github.com/AI-secure/G-PATE.","['Generative Model', 'Machine Learning', 'Privacy']",[],"['Yunhui Long', 'Boxin Wang', 'Zhuolin Yang', 'Bhavya Kailkhura', 'Aston Zhang', 'Carl A. Gunter', 'Bo Li']","['University of Illinois, Urbana Champaign', 'NVIDIA', 'University of Illinois, Urbana Champaign', 'Lawrence Livermore National Laboratory', 'AWS', 'University of Illinois, Urbana Champaign', 'University of Illinois, Urbana Champaign']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27837,Security,On the Sample Complexity of Privately Learning Axis-Aligned Rectangles,"We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a finite grid $X^d\subseteq\mathbb{R}^d$ with differential privacy. Existing results show that the sample complexity of this problem is at most $\min\left\{ d{\cdot}\log|X| \;,\; d^{1.5}{\cdot}\left(\log^*|X| \right)^{1.5}\right\}$. That is, existing constructions either require sample complexity that grows linearly with $\log|X|$, or else it grows super linearly with the dimension $d$.  We present a novel algorithm that reduces the sample complexity to only $\tilde{O}\left\{d{\cdot}\left(\log^*|X|\right)^{1.5}\right\}$,  attaining a dimensionality optimal dependency without requiring the sample complexity to grow with $\log|X|$. The technique used in order to attain this improvement involves the deletion of ""exposed"" data-points on the go, in a fashion designed to avoid the cost of the adaptive composition theorems. The core of this technique may be of individual interest, introducing a new method for constructing statistically-efficient private algorithms.","['Theory', 'Privacy']",[],"['Menachem Sadigurschi', 'Uri Stemmer']","['Ben Gurion University of the Negev, Technion', 'Tel Aviv University']","[None, None]"
https://nips.cc/virtual/2021/poster/27820,Security,Differentially Private Model Personalization,"We study personalization of supervised learning with user-level differential privacy. Consider a setting with many users, each of whom has a training data set drawn from their own distribution $P_i$. Assuming some shared structure among the problems $P_i$, can users collectively learn the shared structure---and solve their tasks better than they could individually---while preserving the privacy of their data? We formulate this question using joint, user-level differential privacy---that is, we control what is leaked about each user's entire data set. We provide algorithms that exploit popular non-private approaches in this domain like the Almost-No-Inner-Loop (ANIL) method, and give strong user-level privacy guarantees for our general approach. When the problems $P_i$ are linear regression problems with each user's regression vector lying in a common, unknown low-dimensional subspace, we show that our efficient algorithms satisfy nearly optimal estimation error guarantees. We also establish a general, information-theoretic upper bound via an exponential mechanism-based algorithm.","['Theory', 'Privacy']",[],"['Prateek Jain', 'J Keith Rush', 'Adam Smith', 'Shuang Song', 'Abhradeep Guha Thakurta']","['Google', 'Google', 'Boston University', 'Google', 'Google']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27733,Security,PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization,"The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data confidentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a significant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain characteristic, such as traffic, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed, which loads a subset of the global model’s parameters rather than loading the entire model used in most previous works. We first validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix. Then we develop PartialFed-Adaptive, which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classification and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Office-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs significantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg.","['Federated Learning', 'Machine Learning', 'Privacy']",[],"['Benyuan Sun', 'Hongxing Huo']","['Huawei Technologies Ltd.', 'Huawei Technologies Ltd.']","[None, None]"
https://nips.cc/virtual/2021/poster/27669,Security,Federated Split Task-Agnostic  Vision Transformer for COVID-19 CXR Diagnosis,"Federated learning, which shares the weights of the neural network across clients, is gaining attention in the healthcare sector as it enables training on a large corpus of decentralized data while maintaining data privacy. For example, this enables neural network training for COVID-19 diagnosis on chest X-ray (CXR) images without collecting patient CXR data across multiple hospitals. Unfortunately, the exchange of the weights quickly consumes the network bandwidth if highly expressive network architecture is employed. So-called split learning partially solves this problem by dividing a neural network into a client and a server part, so that the client part of the network takes up less extensive computation resources and bandwidth. However, it is not clear how to find the optimal split without sacrificing the overall network performance. To amalgamate these methods and thereby maximize their distinct strengths, here we show that the Vision Transformer, a recently developed deep learning architecture with straightforward decomposable configuration, is ideally suitable for split learning without sacrificing performance. Even under the non-independent and identically distributed data distribution which emulates a real collaboration between hospitals using CXR datasets from multiple sources, the proposed framework was able to attain performance comparable to data-centralized training. In addition, the proposed framework along with heterogeneous multi-task clients also improves individual task performances including the diagnosis of COVID-19, eliminating the need for sharing large weights with innumerable parameters. Our results affirm the suitability of Transformer for collaborative learning in medical imaging and pave the way forward for future real-world implementations.","['Transformers', 'Deep Learning', 'Privacy', 'Federated Learning']",[],"['Sangjoon Park', 'Gwanghyun Kim', 'Jeongsol Kim', 'Boah Kim', 'Jong Chul Ye']","['Department of Radiation Oncology, Severance Hospital', 'Seoul National University', 'Korea Advanced Institute of Science and Technology', 'National Institutes of Health', 'Korea Advanced Institute of Science and Technology']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27285,Security,RelaySum for Decentralized Deep Learning on Heterogeneous Data,"In decentralized machine learning, workers compute model updates on their local data. Because the workers only communicate with few neighbors without central coordination, these updates propagate progressively over the network. This paradigm enables distributed training on networks without all-to-all connectivity, helping to protect data privacy as well as to reduce the communication cost of distributed training in data centers. A key challenge, primarily in decentralized deep learning, remains the handling of differences between the workers' local data distributions. To tackle this challenge, we introduce the RelaySum mechanism for information propagation in decentralized learning. RelaySum uses spanning trees to distribute information exactly uniformly across all workers with finite delays depending on the distance between nodes. In contrast, the typical gossip averaging mechanism only distributes data uniformly asymptotically while using the same communication volume per step as RelaySum. We prove that RelaySGD, based on this mechanism, is independent of data heterogeneity and scales to many workers, enabling highly accurate decentralized deep learning on heterogeneous data.","['Deep Learning', 'Optimization', 'Machine Learning', 'Privacy']",[],"['Thijs Vogels', 'Lie He', 'Anastasia Koloskova', 'Sai Praneeth Karimireddy', 'Tao Lin', 'Sebastian U Stich', 'Martin Jaggi']","['Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'Swiss Federal Institute of Technology Lausanne', 'University of California, Berkeley', 'Westlake University', 'CISPA Helmholtz Center for Information Security', 'EPFL']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27268,Security,Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence,"Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efficient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for controlling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images.","['Generative Model', 'Machine Learning', 'Optimal Transport', 'Privacy']",[],"['Tianshi Cao', 'Alex Bie', 'Arash Vahdat', 'Sanja Fidler', 'Karsten Kreis']","['University of Toronto', 'University of Waterloo', 'NVIDIA', 'University of Toronto', 'NVIDIA']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27238,Security,Gradient Inversion with Generative Image Prior,"Federated Learning (FL) is a distributed learning framework, in which the local data never leaves clients’ devices to preserve privacy, and the server trains models on the data via accessing only the gradients of those local data. Without further privacy mechanisms such as differential privacy, this leaves the system vulnerable against an attacker who inverts those gradients to reveal clients’ sensitive data. However, a gradient is often insufficient to reconstruct the user data without any prior knowledge. By exploiting a generative model pretrained on the data distribution, we demonstrate that data privacy can be easily breached. Further, when such prior knowledge is unavailable, we investigate the possibility of learning the prior from a sequence of gradients seen in the process of FL training. We experimentally show that the prior in a form of generative model is learnable from iterative interactions in FL. Our findings demonstrate that additional mechanisms are necessary to prevent privacy leakage in FL.","['Federated Learning', 'Generative Model', 'Privacy']",[],"['Jinwoo Jeon', 'Jaechang Kim', 'Kangwook Lee', 'Sewoong Oh', 'Jungseul Ok']","['POSTECH', 'POSTECH', 'University of Wisconsin, Madison', 'University of Washington', 'POSTECH']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/27211,Security,Local Differential Privacy for Regret Minimization in Reinforcement Learning,"Reinforcement learning algorithms are widely used in domains where it is desirable to provide a personalized service. In these domains it is common that user data contains sensitive information that needs to be protected from third parties. Motivated by this, we study privacy in the context of finite-horizon Markov Decision Processes (MDPs) by requiring information to be obfuscated on the user side. We formulate this notion of privacy for RL by leveraging the local differential privacy (LDP) framework. We establish a lower bound for regret minimization in finite-horizon MDPs with LDP guarantees which shows that guaranteeing privacy has a multiplicative effect on the regret. This result shows that while LDP is an appealing notion of privacy, it makes the learning problem significantly more complex. Finally, we present an optimistic algorithm that simultaneously satisfies $\varepsilon$-LDP requirements, and achieves $\sqrt{K}/\varepsilon$ regret in any finite-horizon MDP after $K$ episodes,  matching the lower bound dependency on the number of episodes $K$.","['Reinforcement Learning and Planning', 'Theory', 'Privacy']",[],"['Evrard Garcelon', 'Vianney Perchet', 'Ciara Pike-Burke', 'Matteo Pirotta']","['Facebook', 'Ensae ParisTech', 'Imperial College London', 'Meta']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26983,Security,Covariance-Aware Private Mean Estimation Without Private Covariance Estimation,"We present two sample-efficient differentially private mean estimators for $d$-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that $\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is the \emph{Mahalanobis distance}. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require $\Omega(d^{3/2})$ samples. Each of our estimators is based on a simple, general approach to designing differentially private mechanisms, but with novel technical steps to make the estimator private and sample-efficient. Our first estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Proving that this mechanism is private requires a novel analysis. Our second estimator perturbs the empirical mean of the data set with noise calibrated to the empirical covariance. Only the mean is released, however; the covariance is only used internally. Its sample complexity guarantees hold more generally for subgaussian distributions, albeit with a slightly worse dependence on the privacy parameter. For both estimators, careful preprocessing of the data is required to satisfy differential privacy.","['Theory', 'Privacy']",[],"['Gavin R Brown', 'Marco Gaboardi', 'Adam Smith', 'Jonathan Ullman', 'Lydia Zakynthinou']","['Boston University', 'Boston University', 'Boston University', 'Northeastern University', 'Northeastern University']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26985,Security,Generalized Linear Bandits with Local Differential Privacy,"Contextual bandit algorithms are useful in personalized online decision-making. However, many applications such as personalized medicine and online advertising require the utilization of individual-specific information for effective learning, while user's data should remain private from the server due to privacy concerns. This motivates the introduction of local differential privacy (LDP), a stringent notion in privacy, to contextual bandits. In this paper, we design LDP algorithms for stochastic generalized linear bandits to achieve the same regret bound as in non-privacy settings. Our main idea is to develop a stochastic gradient-based estimator and update mechanism to ensure LDP. We then exploit the flexibility of stochastic gradient descent (SGD), whose theoretical guarantee for bandit problems is rarely explored, in dealing with generalized linear bandits. We also develop an estimator and update mechanism based on Ordinary Least Square (OLS) for linear bandits. Finally, we conduct experiments with both simulation and real-world datasets to demonstrate the consistently superb performance of our algorithms under LDP constraints with reasonably small parameters $(\varepsilon, \delta)$ to ensure strong privacy protection.","['Optimization', 'Bandits', 'Privacy']",[],"['Zhipeng Liang', 'Yang Wang', 'Jiheng Zhang']","['University of Science and Technology', 'The  University of Science and Technology', 'The  University of Science and Technology']","['Hong Kong', 'Hong Kong', 'Hong Kong']"
https://nips.cc/virtual/2021/poster/26917,Security,Antipodes of Label Differential Privacy: PATE and ALIBI,"We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks. While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning. We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis. Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes.","['Semi-Supervised Learning', 'Machine Learning', 'Privacy']",[],"['Mani Malek Esmaeili', 'Ilya Mironov', 'Karthik Prasad', 'Igor Shilov', 'Florian Tramer']","['Facebook', 'Facebook', 'Facebook AI', 'Imperial College London', 'ETHZ - ETH Zurich']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26906,Security,Deep Learning with Label Differential Privacy,"The Randomized Response (RR) algorithm is a classical technique to improve robustness in survey aggregation, and has been widely adopted in applications with differential privacy guarantees. We propose a novel algorithm, Randomized Response with Prior (RRWithPrior), which can provide more accurate results while maintaining the same level of privacy guaranteed by RR. We then apply RRWithPrior to learn neural networks with label differential privacy (LabelDP), and show that when only the label needs to be protected, the model performance can be significantly improved over the previous state-of-the-art private baselines. Moreover, we study different ways to obtain priors, which when used with RRWithPrior can additionally improve the model performance, further reducing the accuracy gap between private and non-private models. We complement the empirical results with theoretical analysis showing that LabelDP is provably easier than protecting both the inputs and labels.","['Deep Learning', 'Self-Supervised Learning', 'Robustness', 'Privacy']",[],"['Badih Ghazi', 'Noah Golowich', 'Ravi Kumar', 'Pasin Manurangsi', 'Chiyuan Zhang']","['Google', 'Massachusetts Institute of Technology', 'Google', 'Google', 'Google']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26681,Security,Differentially Private Learning with Adaptive Clipping,"Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user's model update by {\em clipping} it to some constant value. However there is no good {\em a priori} setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a fixed clipping norm, one clips to a value at a specified quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy. The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg. Experiments demonstrate that adaptive clipping to the median update norm works well across a range of federated learning tasks, eliminating the need to tune any clipping hyperparameter.","['Federated Learning', 'Deep Learning', 'Privacy']",[],"['Galen Andrew', 'Om Thakkar', 'Hugh Brendan McMahan', 'Swaroop Ramaswamy']","['Google', 'Google', 'Google', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26652,Security,Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning,"We study privacy in a distributed learning framework, where clients collaboratively build a learning model iteratively through interactions with a server from whom we need privacy. Motivated by stochastic optimization and the federated learning (FL) paradigm, we focus on the case where a small fraction of data samples are randomly sub-sampled in each round to participate in the learning process, which also enables privacy amplification.  To obtain even stronger local privacy guarantees, we study this in the shuffle privacy model, where each client randomizes its response using a local differentially private (LDP) mechanism and the server only receives a random permutation (shuffle) of the clients' responses without their association to each client. The principal result of this paper is a privacy-optimization performance trade-off for discrete randomization mechanisms in this sub-sampled shuffle privacy model. This is enabled through a new theoretical technique to analyze the Renyi Differential Privacy (RDP) of the sub-sampled shuffle model.  We numerically demonstrate that, for important regimes, with composition our bound yields significant improvement in privacy guarantee over the state-of-the-art approximate Differential Privacy (DP) guarantee (with strong composition) for sub-sampled shuffled models. We also demonstrate numerically significant improvement in privacy-learning performance operating point using real data sets. Despite these advances, an open question is to bridge the gap between lower and upper privacy bounds in our RDP analysis.","['Federated Learning', 'Optimization', 'Privacy']",[],"['Antonious M. Girgis', 'Deepesh Data', 'Suhas Diggavi']","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26644,Security,Remember What You Want to Forget: Algorithms for Machine Unlearning,"We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset $S$ drawn i.i.d. from an unknown distribution, and outputs a model $\widehat{w}$ that performs well on  unseen samples from the same distribution. However, at some point in the future, any training datapoint $z \in S$ can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees.  We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity. For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This demonstrates a novel separation between differential privacy and machine unlearning.","['Theory', 'Privacy']",[],"['Ayush Sekhari', 'Jayadev Acharya', 'Gautam Kamath', 'Ananda Theertha Suresh']","['Massachusetts Institute of Technology', 'Cornell University', 'University of Waterloo', 'Google']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26628,Security,Numerical Composition of Differential Privacy,"We give a fast algorithm to compose privacy guarantees of differentially private (DP) algorithms to arbitrary accuracy. Our method is based on the notion of privacy loss random variables to quantify the privacy loss of DP algorithms. The running time and memory needed for our algorithm to approximate the privacy curve of a DP algorithm composed with itself $k$ times is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We demonstrate the utility of our algorithm by accurately computing the privacy loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm speeds up the privacy computations by a few orders of magnitude compared to prior work, while maintaining similar accuracy.",['Privacy'],[],"['Sivakanth Gopi', 'Lukas Wutschitz']","['Microsoft Research', 'Microsoft']","[None, None]"
https://nips.cc/virtual/2021/poster/28283,Security,Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation,"Domain adaptation (DA) aims to alleviate the domain shift between source domain and target domain. Most DA methods require access to the source data, but often that is not possible (e.g. due to data privacy or intellectual property). In this paper, we address the challenging source-free domain adaptation (SFDA) problem, where the source pretrained model is adapted to the target domain in the absence of source data. Our method is based on the observation that target data, which might no longer align with the source domain classifier, still forms clear clusters. We capture this intrinsic structure by defining local affinity of the target data, and encourage label consistency among data with high local affinity. We observe that higher affinity should be assigned to reciprocal neighbors, and propose a self regularization loss to decrease the negative impact of noisy neighbors. Furthermore, to aggregate information with more context, we consider expanded neighborhoods with small affinity values. In the experimental results we verify that the inherent structure of the target features is an important source of information for domain adaptation. We demonstrate that this local structure can be efficiently captured by considering the local neighbors, the reciprocal neighbors, and the expanded neighborhood. Finally, we achieve state-of-the-art performance on several 2D image and 3D point cloud recognition datasets. Code is available in https://github.com/Albert0147/SFDA_neighbors.","['Domain Adaptation', 'Privacy']",[],"['Shiqi Yang', 'Yaxing Wang', 'Joost van de weijer', 'Luis Herranz', 'SHANGLING JUI']","['Sony', 'Computer Vision Center, Universitat Autònoma de Barcelona', 'Universitat Autónoma de Barcelona', 'Computer Vision Center, Universitat Autònoma de Barcelona', 'Huawei Technologies Ltd.']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26381,Security,Adaptive Machine Unlearning,"Data deletion algorithms aim to remove the influence of deleted data points from trained models at a cheaper computational cost than fully retraining those models. However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don’t like what the models reveal about them, for example), then the update sequence is adaptive. In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely flexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST.","['Theory', 'Privacy']",[],"['Varun Gupta', 'Christopher Jung', 'Seth Neel', 'Aaron Roth', 'Saeed Sharifi -Malvajerdi', 'Christopher Waites']","['School of Engineering and Applied Science, University of Pennsylvania', 'Stanford University', 'Harvard University', 'Amazon', 'University of Pennsylvania', 'Stanford University']","[None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28853,Security,Locally private online change point detection,"We study online change point detection problems under the constraint of local differential privacy (LDP) where, in particular, the statistician does not have access to the raw data.  As a concrete problem, we study a multivariate nonparametric regression problem.  At each time point $t$, the raw data are assumed to be of the form $(X_t, Y_t)$, where $X_t$ is a $d$-dimensional feature vector and $Y_t$ is a response variable. Our primary aim is to detect changes in the regression function $m_t(x)=\mathbb{E}(Y_t |X_t=x)$ as soon as the change occurs.  We provide algorithms which respect the LDP constraint, which control the false alarm probability, and which detect changes with a minimal (minimax rate-optimal) delay.  To quantify the cost of privacy, we also present the optimal rate in the benchmark, non-private setting.  These non-private results are also new to the literature and thus are interesting \emph{per se}.  In addition, we study the univariate mean online change point detection problem, under privacy constraints.  This serves as the blueprint of studying more complicated private change point detection problems.",['Privacy'],[],"['Thomas Berrett', 'Yi Yu']","['The university of Warwick', 'University of Warwick']","[None, None]"
https://nips.cc/virtual/2021/poster/26102,Security,Differentially Private Federated Bayesian Optimization with Distributed Exploration,"Bayesian optimization (BO) has recently been extended to the federated learning (FL) setting by the federated Thompson sampling (FTS) algorithm, which has promising applications such as federated hyperparameter tuning. However, FTS is not equipped with a rigorous privacy guarantee which is an important consideration in FL. Recent works have incorporated differential privacy (DP) into the training of deep neural networks through a general framework for adding DP to iterative algorithms. Following this general DP framework, our work here integrates DP into FTS to preserve user-level privacy. We also leverage the ability of this general DP framework to handle different parameter vectors, as well as the technique of local modeling for BO, to further improve the utility of our algorithm through distributed exploration (DE). The resulting differentially private FTS with DE (DP-FTS-DE) algorithm is endowed with theoretical guarantees for both the privacy and utility and is amenable to interesting theoretical insights about the privacy-utility trade-off. We also use real-world experiments to show that DP-FTS-DE achieves high utility (competitive performance) with a strong privacy guarantee (small privacy loss) and induces a trade-off between privacy and utility.","['Reinforcement Learning and Planning', 'Optimization', 'Privacy', 'Federated Learning', 'Deep Learning']",[],"['Zhongxiang Dai', 'Bryan Kian Hsiang Low', 'Patrick Jaillet']","['Massachusetts Institute of Technology', 'National University of', 'Massachusetts Institute of Technology']","[None, 'Singapore', None]"
https://nips.cc/virtual/2021/poster/26031,Security,An Uncertainty Principle is a Price of Privacy-Preserving Microdata,"Privacy-protected microdata are often the desired output of a differentially private algorithm since  microdata is familiar and convenient for downstream users. However, there is a statistical price for this kind of convenience. We show that an uncertainty principle governs the trade-off between accuracy for a population of interest (``sum query'') vs. accuracy for its component sub-populations (``point queries''). Compared to differentially private query answering systems that are not required to produce microdata, accuracy can degrade by a logarithmic factor. For example, in the case of pure differential privacy, without the microdata requirement, one can provide noisy answers to the sum query and all point queries while guaranteeing that each answer has squared error $O(1/\epsilon^2)$. With the microdata requirement, one must choose between allowing an additional $\log^2(d)$ factor ($d$ is the number of point queries) for some point queries or allowing an extra $O(d^2)$ factor for the sum query. We present lower bounds for pure, approximate, and concentrated differential privacy. We propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem.",['Privacy'],[],"['John M. Abowd', 'Robert Ashmead', 'Ryan Cumings-Menon', 'Simson L. Garfinkel', 'Daniel Kifer', 'Philip Leclerc', 'William Sexton', 'Ashley E Simpson', 'Christine Task', 'Pavel Zhuravlev']","['U.S. Census Bureau', 'U.S. Census Bureau', 'Census Bureau', 'Massachusetts Institute of Technology', 'Pennsylvania State University', 'U.S. Census Bureau', 'Tumult Labs', 'Knexus Research Corp.', 'Knexus Research Corporation', 'Census Bureau']","[None, None, 'US', None, None, None, None, None, None, 'US']"
https://nips.cc/virtual/2021/poster/26020,Security,User-Level Differentially Private Learning via Correlated Sampling,"Most works in learning with differential privacy (DP) have focused on the setting where each user has a single sample. In this work, we consider the setting where each user holds $m$ samples and the privacy protection is enforced at the level of each user's data.  We show that, in this setting, we may learn with a much fewer number of users. Specifically, we show that, as long as each user receives sufficiently many samples, we can learn any privately learnable class via an $(\epsilon, \delta)$-DP algorithm using only $O(\log(1/\delta)/\epsilon)$ users. For $\epsilon$-DP algorithms, we show that we can learn using only $O_{\epsilon}(d)$ users even in the local model, where $d$ is the probabilistic representation dimension. In both cases, we show a nearly-matching lower bound on the number of users required. A crucial component of our results is a generalization of global stability [Bun, Livni, Moran, FOCS 2020]  that allows the use of public randomness. Under this relaxed notion, we employ a correlated sampling strategy to show that the global stability can be boosted to be arbitrarily close to one, at a polynomial expense in the number of samples.",['Privacy'],[],"['Badih Ghazi', 'Ravi Kumar', 'Pasin Manurangsi']","['Google', 'Google', 'Google']","[None, None, None]"
https://nips.cc/virtual/2021/poster/25962,Security,Locally differentially private estimation of functionals of discrete distributions,"We study the  problem of estimating non-linear functionals of discrete distributions in the context of local differential privacy. The initial data $x_1,\ldots,x_n \in[K]$ are supposed i.i.d. and distributed according to an unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally differentially private (LDP) samples $z_1,...,z_n$ are publicly available, where the term 'local' means that each $z_i$ is produced using one individual attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e. they are allowed to use already published confidential data) or non-interactive. We describe the behavior of the quadratic risk for estimating the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma >0$ as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study twol plug-in type estimators of $F_{\gamma}$, for all $\gamma >0$, that are similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model. However, due to the privacy constraint the rates we attain are slower and similar to those obtained in the Gaussian model by Collier et al. (2020). In the sequentially interactive case, we introduce for all $\gamma >1$ a two-step procedure which attains the parametric rate $(n \alpha^2)^{-1/2}$ when $\gamma \geq 2$.  We give lower bounds results over all $\alpha-$LDP mechanisms and over all estimators using the private samples.",['Privacy'],[],"['Cristina Butucea', 'Yann Issartel']","['CREST, ENSAE, IP Paris', 'Ensae ParisTech']","[None, None]"
https://nips.cc/virtual/2021/poster/27581,Security,The Skellam Mechanism for Differentially Private Federated Learning,"We introduce the multi-dimensional Skellam mechanism, a discrete differential privacy mechanism based on the difference of two independent Poisson random variables. To quantify its privacy guarantees, we analyze the privacy loss distribution via a numerical evaluation and provide a sharp bound on the Rényi divergence between two shifted Skellam distributions. While useful in both centralized and distributed privacy applications, we investigate how it can be applied in the context of federated learning with secure aggregation under communication constraints. Our theoretical findings and extensive experimental evaluations demonstrate that the Skellam mechanism provides the same privacy-accuracy trade-offs as the continuous Gaussian mechanism, even when the precision is low. More importantly, Skellam is closed under summation and sampling from it only requires sampling from a Poisson distribution -- an efficient routine that ships with all machine learning and data analysis software packages. These features, along with its discrete nature and competitive privacy-accuracy trade-offs, make it an attractive practical alternative to the newly introduced discrete Gaussian mechanism.","['Federated Learning', 'Machine Learning', 'Privacy']",[],"['Naman Agarwal', 'Peter Kairouz', 'Ziyu Liu']","['Google', 'Google', 'Stanford University']","[None, None, None]"
https://nips.cc/virtual/2021/poster/27157,Security,Invertible Tabular GANs: Killing Two Birds with One Stone for Tabular Data Synthesis,"Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First,  we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.","['Privacy', 'Robustness', 'Machine Learning', 'Adversarial Robustness and Security', 'Deep Learning', 'Generative Model']",[],"['JAEHOON LEE', 'Jihyeon Hyeong', 'Jinsung Jeon', 'Noseong Park', 'Jihoon Cho']","['LG AI RESEARCH', 'Yonsei University', 'Yonsei University', 'Yonsei University', 'Samsung SDS']","[None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/26338,Security,Differentially Private Sampling from Distributions,"We initiate an investigation of  private sampling from distributions. Given a dataset with $n$ independent observations from an unknown distribution $P$, a sampling algorithm must output a single observation from a distribution that is close in total variation distance to $P$ while satisfying differential privacy. Sampling abstracts the goal of generating small amounts of realistic-looking data. We provide tight upper and lower bounds for the dataset size needed for this task for three natural families of distributions: arbitrary distributions on $\{1,\ldots ,k\}$, arbitrary product distributions on $\{0,1\}^d$, and product distributions on on $\{0,1\}^d$ with bias in each coordinate bounded away from 0 and 1. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer observations than learning a description of $P$ nonprivately; in other regimes, however, private sampling proves to be as difficult as private learning. Notably, for some classes of distributions, the overhead in the number of observations needed for private learning compared to non-private learning is completely captured by the number of observations needed for private sampling.",['Privacy'],[],"['Sofya Raskhodnikova', 'Satchit Sivakumar', 'Adam Smith', 'Marika Swanberg']","['Boston University, Boston University', 'Boston University', 'Boston University', 'Boston University']","[None, None, None, None]"
https://nips.cc/virtual/2021/poster/26585,Security,Learning with User-Level Privacy,"We propose and analyze algorithms to solve a range of learning tasks under user-level differential privacy constraints. Rather than guaranteeing only the privacy of individual samples, user-level DP protects a user's entire contribution ($m \ge 1$ samples), providing more stringent but more realistic protection against information leaks.  We show that for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hypothesis classes with finite metric entropy, the privacy cost decreases as $O(1/\sqrt{m})$ as users provide more samples. In contrast, when increasing the number of users $n$, the privacy cost decreases at a faster $O(1/n)$ rate.  We complement these results with lower bounds showing the minimax optimality of our algorithms for mean estimation and stochastic convex optimization. Our algorithms rely on novel techniques for private mean estimation in arbitrary dimension with error scaling as the concentration radius $\tau$ of the distribution rather than the entire range.","['Optimization', 'Privacy']",[],"['Daniel Asher Nathan Levy', 'Ziteng Sun', 'Kareem Amin', 'Satyen Kale', 'Alex Kulesza', 'Mehryar Mohri', 'Ananda Theertha Suresh']","['Stanford University', 'Google', 'Google', 'Google', 'Google', 'New York University', 'Google']","[None, None, None, None, None, None, None]"
https://nips.cc/virtual/2021/poster/28045,Security,Differential Privacy Over Riemannian Manifolds,"In this work we consider the problem of releasing a differentially private statistical summary that resides on a Riemannian manifold.  We present an extension of the Laplace or K-norm mechanism that utilizes intrinsic distances and volumes on the manifold.  We also consider in detail the specific case where the summary is the Fr\'echet mean of data residing on a manifold.  We demonstrate that our mechanism is rate optimal and depends only on the dimension of the manifold, not on the dimension of any ambient space, while also showing how ignoring the manifold structure can decrease the utility of the sanitized summary.  We illustrate our framework in two examples of particular interest in statistics: the space of symmetric positive definite matrices, which is used for covariance matrices, and the sphere, which can be used as a space for modeling discrete distributions.",['Privacy'],[],"['Matthew Reimherr', 'Karthik Bharath', 'Carlos J Soto']","['Pennsylvania State University', 'University of Nottingham', 'University of Massachusetts at Amherst']","[None, None, None]"
https://nips.cc/virtual/2021/poster/26962,Security,Differentially Private Empirical Risk Minimization under the Fairness Lens,"Differential Privacy (DP) is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals. This paper builds on these important observations and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation and differentially private stochastic gradient descent. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings.","['Fairness', 'Optimization', 'Machine Learning', 'Privacy']",[],"['Cuong Tran', 'My H Dinh', 'Ferdinando Fioretto']","['University of Virginia, Charlottesville', 'University of Virginia, Charlottesville', 'University of Virginia, Charlottesville']","[None, None, None]"
https://nips.cc/virtual/2021/poster/28032,Security,Individual Privacy Accounting via a Rényi Filter,"We consider a sequential setting in which a single dataset of individuals is used to perform adaptively-chosen analyses, while ensuring that the differential privacy loss of each participant does not exceed a pre-specified privacy budget. The standard approach to this problem relies on bounding a worst-case estimate of the privacy loss over all individuals and all possible values of their data, for every single analysis. Yet, in many scenarios this approach is overly conservative, especially for ""typical"" data points which incur little privacy loss by participation in most of the analyses. In this work, we give a method for tighter privacy loss accounting based on the value of a personalized privacy loss estimate for each individual in each analysis. To implement the accounting method we design a filter for Rényi differential privacy. A filter is a tool that ensures that the privacy parameter of a composed sequence of algorithms with adaptively-chosen privacy parameters does not exceed a pre-specified budget. Our filter is simpler and tighter than the known filter for $(\epsilon,\delta)$-differential privacy by Rogers et al. (2016). We apply our results to the analysis of noisy gradient descent and show that personalized accounting can be practical, easy to implement, and can only make the privacy-utility tradeoff tighter.","['Optimization', 'Privacy']",[],"['Vitaly Feldman', 'Tijana Zrnic']","['Apple AI Research', 'Stanford University']","[None, None]"
https://nips.cc/virtual/2021/poster/28210,Security,Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent,"What is the information leakage of an iterative randomized learning algorithm about its training data, when the internal state of the algorithm is \emph{private}? How much is the contribution of each specific training epoch to the information leakage through the released model? We study this problem for noisy gradient descent algorithms, and model the \emph{dynamics} of R\'enyi differential privacy loss throughout the training process.  Our analysis traces a provably \emph{tight} bound on the R\'enyi divergence between the pair of probability distributions over parameters of models trained on neighboring datasets.  We prove that the privacy loss converges exponentially fast, for smooth and strongly convex loss functions, which is a significant improvement over composition theorems (which over-estimate the privacy loss by upper-bounding its total value over all intermediate gradient computations). For Lipschitz, smooth, and strongly convex loss functions, we prove optimal utility with a small gradient complexity for noisy gradient descent algorithms.","['Optimization', 'Privacy']",[],"['Rishav Chourasia', 'Jiayuan Ye', 'Reza Shokri']","['National University of', 'National University of', 'National University of']","['Singapore', 'Singapore', 'Singapore']"